<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Tue  3 Oct 23  to  Wed  4 Oct 23, announced Thu,  5 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item342">Cross-lists</a></li>
<li><a href="#item390">Replacements</a></li>
</ul>
<small>[ total of 595 entries:  <b>1-595</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Thu,  5 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02269" title="Abstract">arXiv:2310.02269</a> [<a href="/pdf/2310.02269" title="Download PDF">pdf</a>, <a href="/format/2310.02269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARRQP: Anomaly Resilient Real-time QoS Prediction Framework with Graph  Convolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Suraj Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Chattopadhyay%2C+S">Soumi Chattopadhyay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 9 Figures, 12 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In the realm of modern service-oriented architecture, ensuring Quality of
Service (QoS) is of paramount importance. The ability to predict QoS values in
advance empowers users to make informed decisions. However, achieving accurate
QoS predictions in the presence of various issues and anomalies, including
outliers, data sparsity, grey-sheep instances, and cold-start scenarios,
remains a challenge. Current state-of-the-art methods often fall short when
addressing these issues simultaneously, resulting in performance degradation.
In this paper, we introduce a real-time QoS prediction framework (called ARRQP)
with a specific emphasis on improving resilience to anomalies in the data.
ARRQP utilizes the power of graph convolution techniques to capture intricate
relationships and dependencies among users and services, even when the data is
limited or sparse. ARRQP integrates both contextual information and
collaborative insights, enabling a comprehensive understanding of user-service
interactions. By utilizing robust loss functions, ARRQP effectively reduces the
impact of outliers during the model training. Additionally, we introduce a
sparsity-resilient grey-sheep detection method, which is subsequently treated
separately for QoS prediction. Furthermore, we address the cold-start problem
by emphasizing contextual features over collaborative features. Experimental
results on the benchmark WS-DREAM dataset demonstrate the framework's
effectiveness in achieving accurate and timely QoS predictions.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02275" title="Abstract">arXiv:2310.02275</a> [<a href="/pdf/2310.02275" title="Download PDF">pdf</a>, <a href="/format/2310.02275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MuSe-GNN: Learning Unified Gene Representation From Multimodal  Biological Graph Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuge Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+R">Rex Ying</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hongyu Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Genomics (q-bio.GN)

</div>
<p class="mathjax">Discovering genes with similar functions across diverse biomedical contexts
poses a significant challenge in gene representation learning due to data
heterogeneity. In this study, we resolve this problem by introducing a novel
model called Multimodal Similarity Learning Graph Neural Network, which
combines Multimodal Machine Learning and Deep Graph Neural Networks to learn
gene representations from single-cell sequencing and spatial transcriptomic
data. Leveraging 82 training datasets from 10 tissues, three sequencing
techniques, and three species, we create informative graph structures for model
training and gene representations generation, while incorporating
regularization with weighted similarity learning and contrastive learning to
learn cross-data gene-gene relationships. This novel design ensures that we can
offer gene representations containing functional similarity across different
contexts in a joint space. Comprehensive benchmarking analysis shows our
model's capacity to effectively capture gene function similarity across
multiple modalities, outperforming state-of-the-art methods in gene
representation learning by up to 97.5%. Moreover, we employ bioinformatics
tools in conjunction with gene representations to uncover pathway enrichment,
regulation causal networks, and functions of disease-associated or
dosage-sensitive genes. Therefore, our model efficiently produces unified gene
representations for the analysis of gene functions, tissue functions, diseases,
and species evolution.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02277" title="Abstract">arXiv:2310.02277</a> [<a href="/pdf/2310.02277" title="Download PDF">pdf</a>, <a href="/format/2310.02277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights  through Sparsity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+L">Lu Yin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jaiswal%2C+A">Ajay Jaiswal</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Souvik Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The traditional notion of "Junk DNA" has long been linked to non-coding
segments within the human genome, constituting roughly 98% of its composition.
However, recent research has unveiled the critical roles some of these
seemingly non-functional DNA sequences play in cellular processes.
Intriguingly, the weights within deep neural networks exhibit a remarkable
similarity to the redundancy observed in human genes. It was believed that
weights in gigantic models contained excessive redundancy, and could be removed
without compromising performance. This paper challenges this conventional
wisdom by presenting a compelling counter-argument. We employ sparsity as a
tool to isolate and quantify the nuanced significance of low-magnitude weights
in pre-trained large language models (LLMs). Our study demonstrates a strong
correlation between these weight magnitudes and the knowledge they encapsulate,
from a downstream task-centric angle. we raise the "Junk DNA Hypothesis" backed
by our in-depth investigation: while small-magnitude weights may appear
"useless" for simple tasks and suitable for pruning, they actually encode
crucial knowledge necessary for solving more difficult downstream tasks.
Removing these seemingly insignificant weights can lead to irreversible
knowledge forgetting and performance damage in difficult tasks. These findings
offer fresh insights into how LLMs encode knowledge in a task-sensitive manner,
pave future research direction in model pruning, and open avenues for
task-aware conditional computation during inference.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02279" title="Abstract">arXiv:2310.02279</a> [<a href="/pdf/2310.02279" title="Download PDF">pdf</a>, <a href="/format/2310.02279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistency Trajectory Models: Learning Probability Flow ODE Trajectory  of Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dongjun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+C">Chieh-Hsin Lai</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+W">Wei-Hsiang Liao</a>, 
<a href="/search/cs?searchtype=author&query=Murata%2C+N">Naoki Murata</a>, 
<a href="/search/cs?searchtype=author&query=Takida%2C+Y">Yuhta Takida</a>, 
<a href="/search/cs?searchtype=author&query=Uesaka%2C+T">Toshimitsu Uesaka</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yutong He</a>, 
<a href="/search/cs?searchtype=author&query=Mitsufuji%2C+Y">Yuki Mitsufuji</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion
model sampling at the cost of sample quality but lack a natural way to
trade-off quality for speed. To address this limitation, we propose Consistency
Trajectory Model (CTM), a generalization encompassing CM and score-based models
as special cases. CTM trains a single neural network that can -- in a single
forward pass -- output scores (i.e., gradients of log-density) and enables
unrestricted traversal between any initial and final time along the Probability
Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables
the efficient combination of adversarial training and denoising score matching
loss to enhance performance and achieves new state-of-the-art FIDs for
single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at
64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes,
both deterministic and stochastic, involving long jumps along the ODE solution
trajectories. It consistently improves sample quality as computational budgets
increase, avoiding the degradation seen in CM. Furthermore, CTM's access to the
score accommodates all diffusion model inference techniques, including exact
likelihood computation.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02280" title="Abstract">arXiv:2310.02280</a> [<a href="/pdf/2310.02280" title="Download PDF">pdf</a>, <a href="/format/2310.02280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expert enhanced dynamic time warping based anomaly detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kloska%2C+M">Matej Kloska</a>, 
<a href="/search/cs?searchtype=author&query=Grmanova%2C+G">Gabriela Grmanova</a>, 
<a href="/search/cs?searchtype=author&query=Rozinajova%2C+V">Viera Rozinajova</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Expert Systems with Applications: An International Journal, Vol.
  225, Elsevier 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Dynamic time warping (DTW) is a well-known algorithm for time series elastic
dissimilarity measure. Its ability to deal with non-linear time distortions
makes it helpful in variety of data mining tasks. Such a task is also anomaly
detection which attempts to reveal unexpected behaviour without false detection
alarms. In this paper, we propose a novel anomaly detection method named Expert
enhanced dynamic time warping anomaly detection (E-DTWA). It is based on DTW
with additional enhancements involving human-in-the-loop concept. The main
benefits of our approach comprise efficient detection, flexible retraining
based on strong consideration of the expert's detection feedback while
retaining low computational and space complexity.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02282" title="Abstract">arXiv:2310.02282</a> [<a href="/pdf/2310.02282" title="Download PDF">pdf</a>, <a href="/format/2310.02282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SWMLP: Shared Weight Multilayer Perceptron for Car Trajectory Speed  Prediction using Road Topographical Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carneiro%2C+S+A">Sarah Almeida Carneiro</a> (LIGM, IFPEN), 
<a href="/search/cs?searchtype=author&query=Chierchia%2C+G">Giovanni Chierchia</a> (LIGM), 
<a href="/search/cs?searchtype=author&query=Charl%C3%A9ty%2C+J">Jean Charl&#xe9;ty</a> (IFPEN), 
<a href="/search/cs?searchtype=author&query=Chataignon%2C+A">Aur&#xe9;lie Chataignon</a> (IFPEN), 
<a href="/search/cs?searchtype=author&query=Najman%2C+L">Laurent Najman</a> (LIGM)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Models and Technologies for
  Intelligent Transportation Systems, Jun 2023, Nice, France. pp.1-6
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Although traffic is one of the massively collected data, it is often only
available for specific regions. One concern is that, although there are studies
that give good results for these data, the data from these regions may not be
sufficiently representative to describe all the traffic patterns in the rest of
the world. In quest of addressing this concern, we propose a speed prediction
method that is independent of large historical speed data. To predict a
vehicle's speed, we use the trajectory road topographical features to fit a
Shared Weight Multilayer Perceptron learning model. Our results show
significant improvement, both qualitative and quantitative, over standard
regression analysis. Moreover, the proposed framework sheds new light on the
way to design new approaches for traffic analysis.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02284" title="Abstract">arXiv:2310.02284</a> [<a href="/pdf/2310.02284" title="Download PDF">pdf</a>, <a href="/format/2310.02284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PASTA: PArallel Spatio-Temporal Attention with spatial auto-correlation  gating for fine-grained crowd flow prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chung Park</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Junui Hong</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Cheonbok Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taesan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+M">Minsung Choi</a>, 
<a href="/search/cs?searchtype=author&query=Choo%2C+J">Jaegul Choo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD) 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Understanding the movement patterns of objects (e.g., humans and vehicles) in
a city is essential for many applications, including city planning and
management. This paper proposes a method for predicting future city-wide crowd
flows by modeling the spatio-temporal patterns of historical crowd flows in
fine-grained city-wide maps. We introduce a novel neural network named PArallel
Spatio-Temporal Attention with spatial auto-correlation gating (PASTA) that
effectively captures the irregular spatio-temporal patterns of fine-grained
maps. The novel components in our approach include spatial auto-correlation
gating, multi-scale residual block, and temporal attention gating module. The
spatial auto-correlation gating employs the concept of spatial statistics to
identify irregular spatial regions. The multi-scale residual block is
responsible for handling multiple range spatial dependencies in the
fine-grained map, and the temporal attention gating filters out irrelevant
temporal information for the prediction. The experimental results demonstrate
that our model outperforms other competing baselines, especially under
challenging conditions that contain irregular spatial regions. We also provide
a qualitative analysis to derive the critical time information where our model
assigns high attention scores in prediction.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02286" title="Abstract">arXiv:2310.02286</a> [<a href="/pdf/2310.02286" title="Download PDF">pdf</a>, <a href="/format/2310.02286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparison of Mesh-Free Differentiable Programming and Data-Driven  Strategies for Optimal Control under PDE Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nzoyem%2C+R+D">Roussel Desmond Nzoyem</a>, 
<a href="/search/cs?searchtype=author&query=Barton%2C+D+A+W">David A.W. Barton</a>, 
<a href="/search/cs?searchtype=author&query=Deakin%2C+T">Tom Deakin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
<p class="mathjax">The field of Optimal Control under Partial Differential Equations (PDE)
constraints is rapidly changing under the influence of Deep Learning and the
accompanying automatic differentiation libraries. Novel techniques like
Physics-Informed Neural Networks (PINNs) and Differentiable Programming (DP)
are to be contrasted with established numerical schemes like Direct-Adjoint
Looping (DAL). We present a comprehensive comparison of DAL, PINN, and DP using
a general-purpose mesh-free differentiable PDE solver based on Radial Basis
Functions. Under Laplace and Navier-Stokes equations, we found DP to be
extremely effective as it produces the most accurate gradients; thriving even
when DAL fails and PINNs struggle. Additionally, we provide a detailed
benchmark highlighting the limited conditions under which any of those methods
can be efficiently used. Our work provides a guide to Optimal Control
practitioners and connects them further to the Deep Learning community.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02294" title="Abstract">arXiv:2310.02294</a> [<a href="/pdf/2310.02294" title="Download PDF">pdf</a>, <a href="/format/2310.02294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond-Accuracy: A Review on Diversity, Serendipity and Fairness in  Recommender Systems Based on Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duricic%2C+T">Tomislav Duricic</a>, 
<a href="/search/cs?searchtype=author&query=Kowald%2C+D">Dominik Kowald</a>, 
<a href="/search/cs?searchtype=author&query=Lacic%2C+E">Emanuel Lacic</a>, 
<a href="/search/cs?searchtype=author&query=Lex%2C+E">Elisabeth Lex</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 1 figure, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">By providing personalized suggestions to users, recommender systems have
become essential to numerous online platforms. Collaborative filtering,
particularly graph-based approaches using Graph Neural Networks (GNNs), have
demonstrated great results in terms of recommendation accuracy. However,
accuracy may not always be the most important criterion for evaluating
recommender systems' performance, since beyond-accuracy aspects such as
recommendation diversity, serendipity, and fairness can strongly influence user
engagement and satisfaction. This review paper focuses on addressing these
dimensions in GNN-based recommender systems, going beyond the conventional
accuracy-centric perspective. We begin by reviewing recent developments in
approaches that improve not only the accuracy-diversity trade-off but also
promote serendipity and fairness in GNN-based recommender systems. We discuss
different stages of model development including data preprocessing, graph
construction, embedding initialization, propagation layers, embedding fusion,
score computation, and training methodologies. Furthermore, we present a look
into the practical difficulties encountered in assuring diversity, serendipity,
and fairness, while retaining high accuracy. Finally, we discuss potential
future research directions for developing more robust GNN-based recommender
systems that go beyond the unidimensional perspective of focusing solely on
accuracy. This review aims to provide researchers and practitioners with an
in-depth understanding of the multifaceted issues that arise when designing
GNN-based recommender systems, setting our work apart by offering a
comprehensive exploration of beyond-accuracy dimensions.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02295" title="Abstract">arXiv:2310.02295</a> [<a href="/pdf/2310.02295" title="Download PDF">pdf</a>, <a href="/format/2310.02295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Complex Semi-Binary Matrix Factorization for Activation  Sequence Recovery of Quasi-Stationary Sources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delabeye%2C+R">Romain Delabeye</a> (QUARTZ, ISAE-Supm&#xe9;ca), 
<a href="/search/cs?searchtype=author&query=Ghienne%2C+M">Martin Ghienne</a> (QUARTZ, ISAE-Supm&#xe9;ca), 
<a href="/search/cs?searchtype=author&query=Penas%2C+O">Olivia Penas</a> (QUARTZ, ISAE-Supm&#xe9;ca), 
<a href="/search/cs?searchtype=author&query=Dion%2C+J">Jean-Luc Dion</a> (QUARTZ, ISAE-Supm&#xe9;ca)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Advocating for a sustainable, resilient and human-centric industry, the three
pillars of Industry 5.0 call for an increased understanding of industrial
processes and manufacturing systems, as well as their energy sustainability.
One of the most fundamental elements of comprehension is knowing when the
systems are operated, as this is key to locating energy intensive subsystems
and operations. Such knowledge is often lacking in practice. Activation
statuses can be recovered from sensor data though. Some non-intrusive sensors
(accelerometers, current sensors, etc.) acquire mixed signals containing
information about multiple actuators at once. Despite their low cost as regards
the fleet of systems they monitor, additional signal processing is required to
extract the individual activation sequences. To that end, sparse regression
techniques can extract leading dynamics in sequential data. Notorious
dictionary learning algorithms have proven effective in this regard. This paper
considers different industrial settings in which the identification of binary
subsystem activation sequences is sought. In this context, it is assumed that
each sensor measures an extensive physical property, source signals are
periodic, quasi-stationary and independent, albeit these signals may be
correlated and their noise distribution is arbitrary. Existing methods either
restrict these assumptions, e.g., by imposing orthogonality or noise
characteristics, or lift them using additional assumptions, typically using
nonlinear transforms.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02296" title="Abstract">arXiv:2310.02296</a> [<a href="/pdf/2310.02296" title="Download PDF">pdf</a>, <a href="/format/2310.02296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIP Is Also a Good Teacher: A New Learning Framework for Inductive  Zero-shot Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jialei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Deguchi%2C+D">Daisuke Deguchi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenkai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Murase%2C+H">Hiroshi Murase</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing Generalized Zero-shot Semantic Segmentation (GZLSS) methods apply
either finetuning the CLIP paradigm or formulating it as a mask classification
task, benefiting from the Vision-Language Models (VLMs). However, the
fine-tuning methods are restricted with fixed backbone models which are not
flexible for segmentation, and mask classification methods heavily rely on
additional explicit mask proposers. Meanwhile, prevalent methods utilize only
seen categories which is a great waste, i.e., neglecting the area exists but
not annotated. To this end, we propose CLIPTeacher, a new learning framework
that can be applied to various per-pixel classification segmentation models
without introducing any explicit mask proposer or changing the structure of
CLIP, and utilize both seen and ignoring areas. Specifically, CLIPTeacher
consists of two key modules: Global Learning Module (GLM) and Pixel Learning
Module (PLM). Specifically, GLM aligns the dense features from an image encoder
with the CLS token, i.e., the only token trained in CLIP, which is a simple but
effective way to probe global information from the CLIP models. In contrast,
PLM only leverages dense tokens from CLIP to produce high-level pseudo
annotations for ignoring areas without introducing any extra mask proposer.
Meanwhile, PLM can fully take advantage of the whole image based on the pseudo
annotations. Experimental results on three benchmark datasets: PASCAL VOC 2012,
COCO-Stuff 164k, and PASCAL Context show large performance gains, i.e., 2.2%,
1.3%, and 8.8%
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02298" title="Abstract">arXiv:2310.02298</a> [<a href="/pdf/2310.02298" title="Download PDF">pdf</a>, <a href="/format/2310.02298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Audios Using Acoustic Properties For Emotion Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhamyal%2C+H">Hira Dhamyal</a>, 
<a href="/search/cs?searchtype=author&query=Elizalde%2C+B">Benjamin Elizalde</a>, 
<a href="/search/cs?searchtype=author&query=Deshmukh%2C+S">Soham Deshmukh</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huaming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Rita Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2211.07737">arXiv:2211.07737</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Emotions lie on a continuum, but current models treat emotions as a finite
valued discrete variable. This representation does not capture the diversity in
the expression of emotion. To better represent emotions we propose the use of
natural language descriptions (or prompts). In this work, we address the
challenge of automatically generating these prompts and training a model to
better learn emotion representations from audio and prompt pairs. We use
acoustic properties that are correlated to emotion like pitch, intensity,
speech rate, and articulation rate to automatically generate prompts i.e.
'acoustic prompts'. We use a contrastive learning objective to map speech to
their respective acoustic prompts. We evaluate our model on Emotion Audio
Retrieval and Speech Emotion Recognition. Our results show that the acoustic
prompts significantly improve the model's performance in EAR, in various
Precision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on
the Ravdess dataset.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02299" title="Abstract">arXiv:2310.02299</a> [<a href="/pdf/2310.02299" title="Download PDF">pdf</a>, <a href="/format/2310.02299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in  3D Physical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Walters%2C+R">Robin Walters</a>, 
<a href="/search/cs?searchtype=author&query=Smidt%2C+T+E">Tess E.Smidt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep equivariant models use symmetries to improve sample efficiency and
generalization. However, the assumption of perfect symmetry in many of these
models can sometimes be restrictive, especially when the data does not
perfectly align with such symmetries. Thus, we introduce relaxed octahedral
group convolution for modeling 3D physical systems in this paper. This flexible
convolution technique provably allows the model to both maintain the highest
level of equivariance that is consistent with data and discover the subtle
symmetry-breaking factors in the physical systems. Empirical results validate
that our approach can not only provide insights into the symmetry-breaking
factors in phase transitions but also achieves superior performance in fluid
super-resolution tasks.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02304" title="Abstract">arXiv:2310.02304</a> [<a href="/pdf/2310.02304" title="Download PDF">pdf</a>, <a href="/format/2310.02304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zelikman%2C+E">Eric Zelikman</a>, 
<a href="/search/cs?searchtype=author&query=Lorch%2C+E">Eliana Lorch</a>, 
<a href="/search/cs?searchtype=author&query=Mackey%2C+L">Lester Mackey</a>, 
<a href="/search/cs?searchtype=author&query=Kalai%2C+A+T">Adam Tauman Kalai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Several recent advances in AI systems (e.g., Tree-of-Thoughts and
Program-Aided Language Models) solve problems by providing a "scaffolding"
program that structures multiple calls to language models to generate better
outputs. A scaffolding program is written in a programming language such as
Python. In this work, we use a language-model-infused scaffolding program to
improve itself. We start with a seed "improver" that improves an input program
according to a given utility function by querying a language model several
times and returning the best solution. We then run this seed improver to
improve itself. Across a small set of downstream tasks, the resulting improved
improver generates programs with significantly better performance than its seed
improver. Afterward, we analyze the variety of self-improvement strategies
proposed by the language model, including beam search, genetic algorithms, and
simulated annealing. Since the language models themselves are not altered, this
is not full recursive self-improvement. Nonetheless, it demonstrates that a
modern language model, GPT-4 in our proof-of-concept experiments, is capable of
writing code that can call itself to improve itself. We critically consider
concerns around the development of self-improving technologies and evaluate the
frequency with which the generated code bypasses a sandbox.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02324" title="Abstract">arXiv:2310.02324</a> [<a href="/pdf/2310.02324" title="Download PDF">pdf</a>, <a href="/format/2310.02324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALT-Pilot: Autonomous navigation with Language augmented Topometric maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Omama%2C+M">Mohammad Omama</a>, 
<a href="/search/cs?searchtype=author&query=Inani%2C+P">Pranav Inani</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+P">Pranjal Paul</a>, 
<a href="/search/cs?searchtype=author&query=Yellapragada%2C+S+C">Sarat Chandra Yellapragada</a>, 
<a href="/search/cs?searchtype=author&query=Jatavallabhula%2C+K+M">Krishna Murthy Jatavallabhula</a>, 
<a href="/search/cs?searchtype=author&query=Chinchali%2C+S">Sandeep Chinchali</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+M">Madhava Krishna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We present an autonomous navigation system that operates without assuming HD
LiDAR maps of the environment. Our system, ALT-Pilot, relies only on publicly
available road network information and a sparse (and noisy) set of crowdsourced
language landmarks. With the help of onboard sensors and a language-augmented
topometric map, ALT-Pilot autonomously pilots the vehicle to any destination on
the road network. We achieve this by leveraging vision-language models
pre-trained on web-scale data to identify potential landmarks in a scene,
incorporating vision-language features into the recursive Bayesian state
estimation stack to generate global (route) plans, and a reactive trajectory
planner and controller operating in the vehicle frame. We implement and
evaluate ALT-Pilot in simulation and on a real, full-scale autonomous vehicle
and report improvements over state-of-the-art topometric navigation systems by
a factor of 3x on localization accuracy and 5x on goal reachability
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02339" title="Abstract">arXiv:2310.02339</a> [<a href="/pdf/2310.02339" title="Download PDF">pdf</a>, <a href="/format/2310.02339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe and Robust Robot Behavior Planning via Constraint Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vermaelen%2C+J">Jan Vermaelen</a>, 
<a href="/search/cs?searchtype=author&query=Holvoet%2C+T">Tom Holvoet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings AREA 2023, <a href="/abs/2310.00333">arXiv:2310.00333</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 391, 2023, pp. 26-41
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">The safe operation of an autonomous system is a complex endeavor, one pivotal
element being its decision-making. Decision-making logic can formally be
analyzed using model checking or other formal verification approaches. Yet, the
non-deterministic nature of realistic environments makes these approaches
rather troublesome and often impractical. Constraint-based planning approaches
such as Tumato have been shown to be capable of generating policies for a
system to reach a stated goal and abiding safety constraints, with guarantees
of soundness and completeness by construction. However, uncertain outcomes of
actions in the environment are not explicitly modeled or accounted for,
severely limiting the expressiveness of Tumato.
<br />In this work, we extend Tumato with support for non-deterministic outcomes of
actions. Actions have a specific intended result yet can be modeled to have
alternative outcomes that may realistically occur. The adapted solver generates
a policy that enables reaching the goals in a safe manner, even when
alternative outcomes of actions occur. Furthermore, we introduce a purely
declarative way of defining safety in Tumato, increasing its expressiveness.
Finally, the addition of cost or duration values to actions enables the solver
to restore safety when necessary, in the most preferred way.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02341" title="Abstract">arXiv:2310.02341</a> [<a href="/pdf/2310.02341" title="Download PDF">pdf</a>, <a href="/format/2310.02341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Runtime Verification for Trustworthy Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abela%2C+R">Robert Abela</a> (University of Malta), 
<a href="/search/cs?searchtype=author&query=Colombo%2C+C">Christian Colombo</a> (University of Malta), 
<a href="/search/cs?searchtype=author&query=Curmi%2C+A">Axel Curmi</a> (University of Malta), 
<a href="/search/cs?searchtype=author&query=Fenech%2C+M">Mattea Fenech</a> (University of Malta), 
<a href="/search/cs?searchtype=author&query=Vella%2C+M">Mark Vella</a> (University of Malta), 
<a href="/search/cs?searchtype=author&query=Ferrando%2C+A">Angelo Ferrando</a> (University of Genoa)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings AREA 2023, <a href="/abs/2310.00333">arXiv:2310.00333</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 391, 2023, pp. 49-62
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Autonomous and robotic systems are increasingly being trusted with sensitive
activities with potentially serious consequences if that trust is broken.
Runtime verification techniques present a natural source of inspiration for
monitoring and enforcing the desirable properties of the communication
protocols in place, providing a formal basis and ways to limit intrusiveness. A
recently proposed approach, RV-TEE, shows how runtime verification can enhance
the level of trust to the Rich Execution Environment (REE), consequently adding
a further layer of protection around the Trusted Execution Environment (TEE).
<br />By reflecting on the implication of deploying RV in the context of
trustworthy computing, we propose practical solutions to two threat models for
the RV-TEE monitoring process: one where the adversary has gained access to the
system without elevated privileges, and another where the adversary gains all
privileges to the host system but fails to steal secrets from the TEE.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02342" title="Abstract">arXiv:2310.02342</a> [<a href="/pdf/2310.02342" title="Download PDF">pdf</a>, <a href="/ps/2310.02342" title="Download PostScript">ps</a>, <a href="/format/2310.02342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of Strategies and Information in Model Checking for  Multi-Agent Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malvone%2C+V">Vadim Malvone</a> (LTCI, T&#xe9;l&#xe9;com Paris, Institut Polytechnique de Paris, Palaiseau, France)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings AREA 2023, <a href="/abs/2310.00333">arXiv:2310.00333</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 391, 2023, pp. 63-70
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">System correctness is one of the most crucial and challenging objectives in
software and hardware systems. With the increasing evolution of connected and
distributed systems, ensuring their correctness requires the use of formal
verification for multi-agent systems. In this paper, we present a summary of
certain results on model checking for multi-agent systems that derive from the
selection of strategies and information for agents. Additionally, we discuss
some open directions for future research.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02343" title="Abstract">arXiv:2310.02343</a> [<a href="/pdf/2310.02343" title="Download PDF">pdf</a>, <a href="/format/2310.02343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Application Behaviour for Robot Swarms using Mixed-Criticality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Signer%2C+S">Sven Signer</a> (University of York), 
<a href="/search/cs?searchtype=author&query=Gray%2C+I">Ian Gray</a> (University of York)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings AREA 2023, <a href="/abs/2310.00333">arXiv:2310.00333</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 391, 2023, pp. 71-82
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Communication is a vital component for all swarm robotics applications, and
even simple swarm robotics behaviours often break down when this communication
is unreliable. Since wireless communications are inherently subject to
interference and signal degradation, real-world swarm robotics applications
will need to be able handle such scenarios. This paper argues for tighter
integration of application level and network layer behaviour, so that the
application can alter its behaviour in response to a degraded network. This is
systematised through the implementation of a mixed-criticality system model. We
compare a static application behaviour with that of an application that is able
to alter its behaviour in response to the current criticality level of a
mixed-criticality wireless protocol. Using simulation results we show that
while a static approach is sufficient if the network conditions are known a
priori, a mixed-criticality system model is able to adapt application behaviour
to better support unseen or unpredictable conditions.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02344" title="Abstract">arXiv:2310.02344</a> [<a href="/pdf/2310.02344" title="Download PDF">pdf</a>, <a href="/format/2310.02344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous Systems&#x27; Safety Cases for use in UK Nuclear Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anderson%2C+C+R">Christopher R. Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Dennis%2C+L+A">Louise A. Dennis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings AREA 2023, <a href="/abs/2310.00333">arXiv:2310.00333</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 391, 2023, pp. 83-88
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">An overview of the process to develop a safety case for an autonomous robot
deployment on a nuclear site in the UK is described and a safety case for a
hypothetical robot incorporating AI is presented. This forms a first step
towards a deployment, showing what is possible now and what may be possible
with development of tools. It forms the basis for further discussion between
nuclear site licensees, the Office for Nuclear Regulation (ONR), industry and
academia.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02345" title="Abstract">arXiv:2310.02345</a> [<a href="/pdf/2310.02345" title="Download PDF">pdf</a>, <a href="/format/2310.02345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rollout Heuristics for Online Stochastic Contingent Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blumenthal%2C+O">Oded Blumenthal</a>, 
<a href="/search/cs?searchtype=author&query=Shani%2C+G">Guy Shani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings AREA 2023, <a href="/abs/2310.00333">arXiv:2310.00333</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 391, 2023, pp. 89-101
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Partially observable Markov decision processes (POMDP) are a useful model for
decision-making under partial observability and stochastic actions. Partially
Observable Monte-Carlo Planning is an online algorithm for deciding on the next
action to perform, using a Monte-Carlo tree search approach, based on the UCT
(UCB applied to trees) algorithm for fully observable Markov-decision
processes. POMCP develops an action-observation tree, and at the leaves, uses a
rollout policy to provide a value estimate for the leaf. As such, POMCP is
highly dependent on the rollout policy to compute good estimates, and hence
identify good actions. Thus, many practitioners who use POMCP are required to
create strong, domain-specific heuristics.
<br />In this paper, we model POMDPs as stochastic contingent planning problems.
This allows us to leverage domain-independent heuristics that were developed in
the planning community. We suggest two heuristics, the first is based on the
well-known h_add heuristic from classical planning, and the second is computed
in belief space, taking the value of information into account.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02346" title="Abstract">arXiv:2310.02346</a> [<a href="/pdf/2310.02346" title="Download PDF">pdf</a>, <a href="/format/2310.02346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Heuristic Search Algorithms in Pathfinding: A Comprehensive  Study on Performance Metrics and Domain Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kherrour%2C+A">Aya Kherrour</a> (University of Trento), 
<a href="/search/cs?searchtype=author&query=Robol%2C+M">Marco Robol</a> (University of Trento), 
<a href="/search/cs?searchtype=author&query=Roveri%2C+M">Marco Roveri</a> (University of Trento), 
<a href="/search/cs?searchtype=author&query=Giorgini%2C+P">Paolo Giorgini</a> (University of Trento)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings AREA 2023, <a href="/abs/2310.00333">arXiv:2310.00333</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 391, 2023, pp. 102-112
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">The paper presents a comprehensive performance evaluation of some heuristic
search algorithms in the context of autonomous systems and robotics. The
objective of the study is to evaluate and compare the performance of different
search algorithms in different problem settings on the pathfinding domain.
Experiments give us insight into the behavior of the evaluated heuristic search
algorithms, over the variation of different parameters: domain size, obstacle
density, and distance between the start and the goal states. Results are then
used to design a selection algorithm that, on the basis of problem
characteristics, suggests the best search algorithm to use.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02347" title="Abstract">arXiv:2310.02347</a> [<a href="/pdf/2310.02347" title="Download PDF">pdf</a>, <a href="/format/2310.02347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strong Mixed-Integer Formulations for Transmission Expansion Planning  with FACTS Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+K">Kevin Wu</a>, 
<a href="/search/eess?searchtype=author&query=Tanneau%2C+M">Mathieu Tanneau</a>, 
<a href="/search/eess?searchtype=author&query=Van+Hentenryck%2C+P">Pascal Van Hentenryck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Transmission Network Expansion Planning (TNEP) problems find the most
economical way of expanding a given grid given long-term growth in generation
capacity and demand patterns. The recent development of Flexible AC
Transmission System (FACTS) devices, which can dynamically re-route power flows
by adjusting individual branches' impedance, call for their integration into
TNEP problems. However, the resulting TNEP+FACTS formulations are significantly
harder to solve than traditional TNEP instances, due to the nonlinearity of
FACTS behavior. This paper proposes a new mixed-integer formulation for
TNEP+FACTS, which directly represents the change in power flow induced by
individual FACTS devices. The proposed formulation uses an extended formulation
and facet-defining constraints, which are stronger than big-M constraints
typically used in the literature. The paper conducts numerical experiments on a
synthetic model of the Texas system with high renewable penetration. The
results demonstrate the computational superiority of the proposed approach,
which achieves a 4x speedup over state-of-the-art formulations, and highlight
the potential of FACTS devices to mitigate congestion.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02350" title="Abstract">arXiv:2310.02350</a> [<a href="/pdf/2310.02350" title="Download PDF">pdf</a>, <a href="/format/2310.02350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neuromimetic Dynamic Networks with Hebbian Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sun%2C+Z">Zexin Sun</a>, 
<a href="/search/eess?searchtype=author&query=Baillieul%2C+J">John Baillieul</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Leveraging recent advances in neuroscience and control theory, this paper
presents a neuromimetic network model with dynamic symmetric connections
governed by Hebbian learning rules. Formal analysis grounded in graph theory
and classical control establishes that this biologically plausible model
exhibits boundedness, stability, and structural controllability given a
generalized sym-cactus structure with multiple control nodes. We prove the
necessity of this topology when there are distributed control inputs.
Simulations using a 14-node generalized sym-cactus network with two input types
validate the model's effectiveness in capturing key neural dynamics.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02353" title="Abstract">arXiv:2310.02353</a> [<a href="/pdf/2310.02353" title="Download PDF">pdf</a>, <a href="/format/2310.02353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Proactive Multi-Task Assignment with Resource Availability  Anticipation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nedelmann%2C+D+C">D&#xe9;borah Conforto Nedelmann</a>, 
<a href="/search/cs?searchtype=author&query=Lacan%2C+J">J&#xe9;r&#xf4;me Lacan</a>, 
<a href="/search/cs?searchtype=author&query=Chanel%2C+C">Caroline Chanel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings AREA 2023, <a href="/abs/2310.00333">arXiv:2310.00333</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 391, 2023, pp. 3-17
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Performance (cs.PF)

</div>
<p class="mathjax">With the emergence of services and online applications as taxi dispatching,
crowdsourcing, package or food delivery, industrials and researchers are paying
attention to the online multi-task assignment optimization field to quickly and
efficiently met demands. In this context, this paper is interested in the
multi-task assignment problem where multiple requests (e.g. tasks) arrive over
time and must be dynamically matched to (mobile) agents. This optimization
problem is known to be NP-hard. In order to treat this problem with a proactive
mindset, we propose to use a receding-horizon approach to determine which
resources (e.g. taxis, mobile agents, drones, robots) would be available within
this (possibly dynamic) receding-horizon to meet the current set of requests
(i.e. tasks) as good as possible. Contrarily to several works in this domain,
we have chosen to make no assumption concerning future locations of requests.
To achieve fast optimized online solutions in terms of costs and amount of
allocated tasks, we have designed a genetic algorithm based on a fitness
function integrating the traveled distance and the age of the requests. We
compared our proactive multi-task assignment with resource availability
anticipation approach with a classical reactive approach. The results obtained
in two benchmark problems, one synthetic and another based on real data, show
that our resource availability anticipation method can achieve better results
in terms of costs (e.g. traveled distance) and amount of allocated tasks than
reactive approaches while decreasing resources idle time.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02355" title="Abstract">arXiv:2310.02355</a> [<a href="/pdf/2310.02355" title="Download PDF">pdf</a>, <a href="/ps/2310.02355" title="Download PostScript">ps</a>, <a href="/format/2310.02355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reasoning about Intuitionistic Computation Tree Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Catta%2C+D">Davide Catta</a>, 
<a href="/search/cs?searchtype=author&query=Malvone%2C+V">Vadim Malvone</a>, 
<a href="/search/cs?searchtype=author&query=Murano%2C+A">Aniello Murano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings AREA 2023, <a href="/abs/2310.00333">arXiv:2310.00333</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 391, 2023, pp. 42-48
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we define an intuitionistic version of Computation Tree Logic.
After explaining the semantic features of intuitionistic logic, we examine how
these characteristics can be interesting for formal verification purposes.
Subsequently, we define the syntax and semantics of our intuitionistic version
of CTL and study some simple properties of the so obtained logic. We conclude
by demonstrating that some fixed-point axioms of CTL are not valid in the
intuitionistic version of CTL we have defined.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02356" title="Abstract">arXiv:2310.02356</a> [<a href="/pdf/2310.02356" title="Download PDF">pdf</a>, <a href="/format/2310.02356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ORTAC+ : A User Friendly Domain Specific Language for Multi-Agent  Mission Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonhomme%2C+C">Caroline Bonhomme</a> (Safran Electronics and Defense, ONERA), 
<a href="/search/cs?searchtype=author&query=Dufour%2C+J">Jean-Louis Dufour</a> (Safran Electronics and Defense)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings AREA 2023, <a href="/abs/2310.00333">arXiv:2310.00333</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 391, 2023, pp. 127-133
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">A tactical military unit is a complex system composed of many agents such as
infantry, robots, or drones. Given a mission, an automated planner can find an
optimal plan. Therefore, the mission itself must be modeled. The problem is
that languages like PDDL are too low-level to be usable by the end-user: an
officer in the field. We present ORTAC+, a language and a planning tool
designed for this end-user. Its main objective is to allow a natural modeling
of the mission, to minimize the risk of bad modeling, and thus obtain reliable
plans. The language offers high-level constructs specifically designed to
describe tactical missions, but at the same time has clear semantics allowing a
translation to PDDL, to take advantage of state-of-the-art planners.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02357" title="Abstract">arXiv:2310.02357</a> [<a href="/pdf/2310.02357" title="Download PDF">pdf</a>, <a href="/ps/2310.02357" title="Download PostScript">ps</a>, <a href="/format/2310.02357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the definition of toxicity in NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berezin%2C+S">Sergey Berezin</a>, 
<a href="/search/cs?searchtype=author&query=Farahbakhsh%2C+R">Reza Farahbakhsh</a>, 
<a href="/search/cs?searchtype=author&query=Crespi%2C+N">Noel Crespi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The fundamental problem in toxicity detection task lies in the fact that the
toxicity is ill-defined. Jigsaw, a unit within Google and one of the leaders in
the field, uses a definition of toxicity given by Dixon et al. - 'rude,
disrespectful, or unreasonable language that is likely to make someone leave a
discussion'. One can instantly see the issue with this definition, as it gives
no quantitative measure of the toxicity and operates with highly subjective
cultural terms. Despite all vagueness and flaws, this definition is de-facto
widely used by many researchers. In this work we suggest quantative
stress-based defenition for the toxicity that overcomes existing shortcomings.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02360" title="Abstract">arXiv:2310.02360</a> [<a href="/pdf/2310.02360" title="Download PDF">pdf</a>, <a href="/format/2310.02360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prioritized Soft Q-Decomposition for Lexicographic Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rietz%2C+F">Finn Rietz</a>, 
<a href="/search/cs?searchtype=author&query=Heinrich%2C+S">Stefan Heinrich</a>, 
<a href="/search/cs?searchtype=author&query=Schaffernicht%2C+E">Erik Schaffernicht</a>, 
<a href="/search/cs?searchtype=author&query=Stork%2C+J+A">Johannes Andreas Stork</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Reinforcement learning (RL) for complex tasks remains a challenge, primarily
due to the difficulties of engineering scalar reward functions and the inherent
inefficiency of training models from scratch. Instead, it would be better to
specify complex tasks in terms of elementary subtasks and to reuse subtask
solutions whenever possible. In this work, we address continuous space
lexicographic multi-objective RL problems, consisting of prioritized subtasks,
which are notoriously difficult to solve. We show that these can be scalarized
with a subtask transformation and then solved incrementally using value
decomposition. Exploiting this insight, we propose prioritized soft
Q-decomposition (PSQD), a novel algorithm for learning and adapting subtask
solutions under lexicographic priorities in continuous state-action spaces.
PSQD offers the ability to reuse previously learned subtask solutions in a
zero-shot composition, followed by an adaptation step. Its ability to use
retained subtask training data for offline learning eliminates the need for new
environment interaction during adaptation. We demonstrate the efficacy of our
approach by presenting successful learning, reuse, and adaptation results for
both low- and high-dimensional simulated robot control tasks, as well as
offline learning results. In contrast to baseline approaches, PSQD does not
trade off between conflicting subtasks or priority constraints and satisfies
subtask priorities during learning. PSQD provides an intuitive framework for
tackling complex RL problems, offering insights into the inner workings of the
subtask composition.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02361" title="Abstract">arXiv:2310.02361</a> [<a href="/pdf/2310.02361" title="Download PDF">pdf</a>, <a href="/format/2310.02361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event-Enhanced Multi-Modal Spiking Neural Network for Dynamic Obstacle  Avoidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+B">Bo Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yunduo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+H">Haiyang Mei</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Ziqi Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xin Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the 31st ACM International Conference on Multimedia (ACM MM 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Autonomous obstacle avoidance is of vital importance for an intelligent agent
such as a mobile robot to navigate in its environment. Existing
state-of-the-art methods train a spiking neural network (SNN) with deep
reinforcement learning (DRL) to achieve energy-efficient and fast inference
speed in complex/unknown scenes. These methods typically assume that the
environment is static while the obstacles in real-world scenes are often
dynamic. The movement of obstacles increases the complexity of the environment
and poses a great challenge to the existing methods. In this work, we approach
robust dynamic obstacle avoidance twofold. First, we introduce the neuromorphic
vision sensor (i.e., event camera) to provide motion cues complementary to the
traditional Laser depth data for handling dynamic obstacles. Second, we develop
an DRL-based event-enhanced multimodal spiking actor network (EEM-SAN) that
extracts information from motion events data via unsupervised representation
learning and fuses Laser and event camera data with learnable thresholding.
Experiments demonstrate that our EEM-SAN outperforms state-of-the-art obstacle
avoidance methods by a significant margin, especially for dynamic obstacle
avoidance.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02366" title="Abstract">arXiv:2310.02366</a> [<a href="/pdf/2310.02366" title="Download PDF">pdf</a>, <a href="/ps/2310.02366" title="Download PostScript">ps</a>, <a href="/format/2310.02366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic force inference via density estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chard%C3%A8s%2C+V">Victor Chard&#xe8;s</a>, 
<a href="/search/cs?searchtype=author&query=Maddu%2C+S">Suryanarayana Maddu</a>, 
<a href="/search/cs?searchtype=author&query=Shelley%2C+M+J">Michael J. Shelley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biological Physics (physics.bio-ph); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Inferring dynamical models from low-resolution temporal data continues to be
a significant challenge in biophysics, especially within transcriptomics, where
separating molecular programs from noise remains an important open problem. We
explore a common scenario in which we have access to an adequate amount of
cross-sectional samples at a few time-points, and assume that our samples are
generated from a latent diffusion process. We propose an approach that relies
on the probability flow associated with an underlying diffusion process to
infer an autonomous, nonlinear force field interpolating between the
distributions. Given a prior on the noise model, we employ score-matching to
differentiate the force field from the intrinsic noise. Using relevant
biophysical examples, we demonstrate that our approach can extract
non-conservative forces from non-stationary data, that it learns equilibrium
dynamics when applied to steady-state data, and that it can do so with both
additive and multiplicative noise models.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02367" title="Abstract">arXiv:2310.02367</a> [<a href="/pdf/2310.02367" title="Download PDF">pdf</a>, <a href="/format/2310.02367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Recurrent Units for Sequential Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+Z">Zhenrui Yue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yueqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhankui He</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+H">Huimin Zeng</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">State-of-the-art sequential recommendation relies heavily on
self-attention-based recommender models. Yet such models are computationally
expensive and often too slow for real-time recommendation. Furthermore, the
self-attention operation is performed at a sequence-level, thereby making
low-cost incremental inference challenging. Inspired by recent advances in
efficient language modeling, we propose linear recurrent units for sequential
recommendation (LRURec). Similar to recurrent neural networks, LRURec offers
rapid inference and can achieve incremental inference on sequential inputs. By
decomposing the linear recurrence operation and designing recursive
parallelization in our framework, LRURec provides the additional benefits of
reduced model size and parallelizable training. Moreover, we optimize the
architecture of LRURec by implementing a series of modifications to address the
lack of non-linearity and improve training dynamics. To validate the
effectiveness of our proposed LRURec, we conduct extensive experiments on
multiple real-world datasets and compare its performance against
state-of-the-art sequential recommenders. Experimental results demonstrate the
effectiveness of LRURec, which consistently outperforms baselines by a
significant margin. Results also highlight the efficiency of LRURec with our
parallelized training paradigm and fast inference on long sequences, showing
its potential to further enhance user experience in sequential recommendation.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02368" title="Abstract">arXiv:2310.02368</a> [<a href="/pdf/2310.02368" title="Download PDF">pdf</a>, <a href="/format/2310.02368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning from Automatic Feedback for High-Quality Unit  Test Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Steenhoek%2C+B">Benjamin Steenhoek</a>, 
<a href="/search/cs?searchtype=author&query=Tufano%2C+M">Michele Tufano</a>, 
<a href="/search/cs?searchtype=author&query=Sundaresan%2C+N">Neel Sundaresan</a>, 
<a href="/search/cs?searchtype=author&query=Svyatkovskiy%2C+A">Alexey Svyatkovskiy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Software testing is a crucial aspect of software development, and the
creation of high-quality tests that adhere to best practices is essential for
effective maintenance. Recently, Large Language Models (LLMs) have gained
popularity for code generation, including the automated creation of test cases.
However, these LLMs are often trained on vast amounts of publicly available
code, which may include test cases that do not adhere to best practices and may
even contain test smells (anti-patterns). To address this issue, we propose a
novel technique called Reinforcement Learning from Static Quality Metrics
(RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show
that LLMs can generate undesirable test smells. Thus, we train specific reward
models for each static quality metric, then utilize Proximal Policy
Optimization (PPO) to train models for optimizing a single quality metric at a
time. Furthermore, we amalgamate these rewards into a unified reward model
aimed at capturing different best practices and quality aspects of tests. By
comparing RL-trained models with those trained using supervised learning, we
provide insights into how reliably utilize RL to improve test generation
quality and into the effects of various training strategies. Our experimental
results demonstrate that the RL-optimized model consistently generated
high-quality test cases compared to the base LLM, improving the model by up to
21%, and successfully generates nearly 100% syntactically correct code. RLSQM
also outperformed GPT-4 on four out of seven metrics. This represents a
significant step towards enhancing the overall efficiency and reliability of
software testing through Reinforcement Learning and static quality metrics. Our
data are available at this link: https://figshare.com/s/ded476c8d4c221222849.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02372" title="Abstract">arXiv:2310.02372</a> [<a href="/pdf/2310.02372" title="Download PDF">pdf</a>, <a href="/format/2310.02372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProtoNER: Few shot Incremental Learning for Named Entity Recognition  using Prototypical Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Ritesh Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+S">Saurabh Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+A">Ashish Verma</a>, 
<a href="/search/cs?searchtype=author&query=Isahagian%2C+V">Vatche Isahagian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Key value pair (KVP) extraction or Named Entity Recognition(NER) from
visually rich documents has been an active area of research in document
understanding and data extraction domain. Several transformer based models such
as LayoutLMv2, LayoutLMv3, and LiLT have emerged achieving state of the art
results. However, addition of even a single new class to the existing model
requires (a) re-annotation of entire training dataset to include this new class
and (b) retraining the model again. Both of these issues really slow down the
deployment of updated model. \\ We present \textbf{ProtoNER}: Prototypical
Network based end-to-end KVP extraction model that allows addition of new
classes to an existing model while requiring minimal number of newly annotated
training samples. The key contributions of our model are: (1) No dependency on
dataset used for initial training of the model, which alleviates the need to
retain original training dataset for longer duration as well as data
re-annotation which is very time consuming task, (2) No intermediate synthetic
data generation which tends to add noise and results in model's performance
degradation, and (3) Hybrid loss function which allows model to retain
knowledge about older classes as well as learn about newly added classes.\\
Experimental results show that ProtoNER finetuned with just 30 samples is able
to achieve similar results for the newly added classes as that of regular model
finetuned with 2600 samples.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02373" title="Abstract">arXiv:2310.02373</a> [<a href="/pdf/2310.02373" title="Download PDF">pdf</a>, <a href="/format/2310.02373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secure and Effective Data Appraisal for Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+X">Xu Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Changhong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+F+X">Felix Xiaozhu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Y">Yangfeng Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Essential for an unfettered data market is the ability to discreetly select
and evaluate training data before finalizing a transaction between the data
owner and model owner. To safeguard the privacy of both data and model, this
process involves scrutinizing the target model through Multi-Party Computation
(MPC). While prior research has posited that the MPC-based evaluation of
Transformer models is excessively resource-intensive, this paper introduces an
innovative approach that renders data selection practical. The contributions of
this study encompass three pivotal elements: (1) a groundbreaking pipeline for
confidential data selection using MPC, (2) replicating intricate
high-dimensional operations with simplified low-dimensional MLPs trained on a
limited subset of pertinent data, and (3) implementing MPC in a concurrent,
multi-phase manner. The proposed method is assessed across an array of
Transformer models and NLP/CV benchmarks. In comparison to the direct MPC-based
evaluation of the target model, our approach substantially reduces the time
required, from thousands of hours to mere tens of hours, with only a nominal
0.20% dip in accuracy when training with the selected data.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02374" title="Abstract">arXiv:2310.02374</a> [<a href="/pdf/2310.02374" title="Download PDF">pdf</a>, <a href="/format/2310.02374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conversational Health Agents: A Personalized LLM-Powered Agent Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abbasian%2C+M">Mahyar Abbasian</a>, 
<a href="/search/cs?searchtype=author&query=Azimi%2C+I">Iman Azimi</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+A+M">Amir M. Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Ramesh Jain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 5 figures, journal paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Conversational Health Agents (CHAs) are interactive systems designed to
enhance personal healthcare services by engaging in empathetic conversations
and processing multimodal data. While current CHAs, especially those utilizing
Large Language Models (LLMs), primarily focus on conversation, they often lack
comprehensive agent capabilities. This includes the ability to access personal
user health data from wearables, 24/7 data collection sources, and electronic
health records, as well as integrating the latest published health insights and
connecting with established multimodal data analysis tools. We are developing a
framework to empower CHAs by equipping them with critical thinking, knowledge
acquisition, and problem-solving abilities. Our CHA platform, powered by LLMs,
seamlessly integrates healthcare tools, enables multilingual and multimodal
conversations, and interfaces with a variety of user data analysis tools. We
illustrate its proficiency in handling complex healthcare tasks, such as stress
level estimation, showcasing the agent's cognitive and operational
capabilities.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02379" title="Abstract">arXiv:2310.02379</a> [<a href="/pdf/2310.02379" title="Download PDF">pdf</a>, <a href="/format/2310.02379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Industry 4.0 and Beyond: The Role of 5G, WiFi 7, and TSN in Enabling  Smart Manufacturing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=John%2C+J">Jobish John</a>, 
<a href="/search/cs?searchtype=author&query=Noor-A-Rahim%2C+M">Md. Noor-A-Rahim</a>, 
<a href="/search/cs?searchtype=author&query=Vijayan%2C+A">Aswathi Vijayan</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>, 
<a href="/search/cs?searchtype=author&query=Pesch%2C+D">Dirk Pesch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">This paper explores the role that 5G, WiFi-7, and Time-Sensitive Networking
(TSN) can play in driving smart manufacturing as a fundamental part of the
Industry 4.0 vision. The paper provides an in-depth analysis of each
technology's application in industrial communications, with a focus on TSN and
its key elements that enable reliable and secure communication in industrial
networks. In addition, the paper includes a comparative study of these
technologies, analyzing them based on a number of industrial use-cases,
supported secondary applications, industry adoption, and current market trends.
The paper concludes by highlighting the challenges and future directions for
the adoption of these technologies in industrial networks and emphasizes their
importance in realizing the Industry 4.0 vision within the context of smart
manufacturing.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02380" title="Abstract">arXiv:2310.02380</a> [<a href="/pdf/2310.02380" title="Download PDF">pdf</a>, <a href="/format/2310.02380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-blocking Dynamic Unbounded Graphs with Wait-Free Snapshot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhardwaj%2C+G">Gaurav Bhardwaj</a>, 
<a href="/search/cs?searchtype=author&query=Peri%2C+S">Sathya Peri</a>, 
<a href="/search/cs?searchtype=author&query=Shetty%2C+P">Pratik Shetty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Graphs are arguably one of the most fundamental data-structure used in many
domains such as block-chain, networks etc. Theoretically and practically,
improving Graph performance is one of the most studied and omnipresent research
problems. In this paper, we have implemented a dynamic unbounded concurrent
graph which can perform the add, delete or lookup operations on vertices and
edges concurrently. All these methods are lock-free and linearizable. On top of
this, we have also implemented the wait-free graph snapshot algorithm. To the
best of knowledge this is first wait-free implementation of snapshot on
concurrent graphs. We have used the snapshot of the algorithm to calculate the
diameter and between centrality. We have compared our implementation with its
counterparts and outperformed them by a good margin. This illustrates the
efficiency of our snapshot method which is a generic method and can be used to
perform other useful graph analytics operations.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02382" title="Abstract">arXiv:2310.02382</a> [<a href="/pdf/2310.02382" title="Download PDF">pdf</a>, <a href="/format/2310.02382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Speech Recognition with N-Skipgram and Positional Unigram  Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hasegawa-Johnson%2C+M">Mark Hasegawa-Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+C+D">Chang D. Yoo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Training unsupervised speech recognition systems presents challenges due to
GAN-associated instability, misalignment between speech and text, and
significant memory demands. To tackle these challenges, we introduce a novel
ASR system, ESPUM. This system harnesses the power of lower-order N-skipgrams
(up to N=3) combined with positional unigram statistics gathered from a small
batch of samples. Evaluated on the TIMIT benchmark, our model showcases
competitive performance in ASR and phoneme segmentation tasks. Access our
publicly available code at https://github.com/lwang114/GraphUnsupASR.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02383" title="Abstract">arXiv:2310.02383</a> [<a href="/pdf/2310.02383" title="Download PDF">pdf</a>, <a href="/format/2310.02383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Multi-Path Web Story Creation from a Structural Article
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nkemelu%2C+D">Daniel Nkemelu</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+P">Peggy Chi</a>, 
<a href="/search/cs?searchtype=author&query=Chin%2C+D+C">Daniel Castro Chin</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+K">Krishna Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Essa%2C+I">Irfan Essa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Web articles such as Wikipedia serve as one of the major sources of knowledge
dissemination and online learning. However, their in-depth information--often
in a dense text format--may not be suitable for mobile browsing, even in a
responsive UI. We propose an automatic approach that converts a structural
article of any length into a set of interactive Web Stories that are ideal for
mobile experiences. We focused on Wikipedia articles and developed Wiki2Story,
a pipeline based on language and layout models, to demonstrate the concept.
Wiki2Story dynamically slices an article and plans one to multiple Story paths
according to the document hierarchy. For each slice, it generates a multi-page
summary Story composed of text and image pairs in visually-appealing layouts.
We derived design principles from an analysis of manually-created Story
practices. We executed our pipeline on 500 Wikipedia documents and conducted
user studies to review selected outputs. Results showed that Wiki2Story
effectively captured and presented salient content from the original articles
and sparked interest in viewers.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02386" title="Abstract">arXiv:2310.02386</a> [<a href="/pdf/2310.02386" title="Download PDF">pdf</a>, <a href="/format/2310.02386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ScaleNet: An Unsupervised Representation Learning Method for Limited  Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Huili Huang</a>, 
<a href="/search/cs?searchtype=author&query=Roozbahani%2C+M+M">M. Mahdi Roozbahani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by DAGM GCPR 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Although large-scale labeled data are essential for deep convolutional neural
networks (ConvNets) to learn high-level semantic visual representations, it is
time-consuming and impractical to collect and annotate large-scale datasets. A
simple and efficient unsupervised representation learning method named ScaleNet
based on multi-scale images is proposed in this study to enhance the
performance of ConvNets when limited information is available. The input images
are first resized to a smaller size and fed to the ConvNet to recognize the
rotation degree. Next, the ConvNet learns the rotation-prediction task for the
original size images based on the parameters transferred from the previous
model. The CIFAR-10 and ImageNet datasets are examined on different
architectures such as AlexNet and ResNet50 in this study. The current study
demonstrates that specific image features, such as Harris corner information,
play a critical role in the efficiency of the rotation-prediction task. The
ScaleNet supersedes the RotNet by ~7% in the limited CIFAR-10 dataset. The
transferred parameters from a ScaleNet model with limited data improve the
ImageNet Classification task by about 6% compared to the RotNet model. This
study shows the capability of the ScaleNet method to improve other cutting-edge
models such as SimCLR by learning effective features for classification tasks.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02387" title="Abstract">arXiv:2310.02387</a> [<a href="/pdf/2310.02387" title="Download PDF">pdf</a>, <a href="/format/2310.02387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exponential Lower Bounds for Fictitious Play in Potential Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panageas%2C+I">Ioannis Panageas</a>, 
<a href="/search/cs?searchtype=author&query=Patris%2C+N">Nikolas Patris</a>, 
<a href="/search/cs?searchtype=author&query=Skoulakis%2C+S">Stratis Skoulakis</a>, 
<a href="/search/cs?searchtype=author&query=Cevher%2C+V">Volkan Cevher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to appear in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Fictitious Play (FP) is a simple and natural dynamic for repeated play with
many applications in game theory and multi-agent reinforcement learning. It was
introduced by Brown (1949,1951) and its convergence properties for two-player
zero-sum games was established later by Robinson (1951). Potential games
Monderer and Shapley (1996b) is another class of games which exhibit the FP
property (Monderer and Shapley (1996a)), i.e., FP dynamics converges to a Nash
equilibrium if all agents follows it. Nevertheless, except for two-player
zero-sum games and for specific instances of payoff matrices (Abernethy et al.
(2021)) or for adversarial tie-breaking rules (Daskalakis and Pan (2014)), the
convergence rate of FP is unknown. In this work, we focus on the rate of
convergence of FP when applied to potential games and more specifically
identical payoff games. We prove that FP can take exponential time (in the
number of strategies) to reach a Nash equilibrium, even if the game is
restricted to two agents and for arbitrary tie-breaking rules. To prove this,
we recursively construct a two-player coordination game with a unique Nash
equilibrium. Moreover, every approximate Nash equilibrium in the constructed
game must be close to the pure Nash equilibrium in $\ell_1$-distance.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02388" title="Abstract">arXiv:2310.02388</a> [<a href="/pdf/2310.02388" title="Download PDF">pdf</a>, <a href="/format/2310.02388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing a Sparse Approximate Inverse on Quantum Annealing Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Suresh%2C+S">Sanjay Suresh</a>, 
<a href="/search/math?searchtype=author&query=Suresh%2C+K">Krishnan Suresh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Many engineering problems involve solving large linear systems of equations.
Conjugate gradient (CG) is one of the most popular iterative methods for
solving such systems. However, CG typically requires a good preconditioner to
speed up convergence. One such preconditioner is the sparse approximate inverse
(SPAI).
<br />In this paper, we explore the computation of an SPAI on quantum annealing
machines by solving a series of quadratic unconstrained binary optimization
(QUBO) problems. Numerical experiments are conducted using both
well-conditioned and poorly-conditioned linear systems arising from a 2D finite
difference formulation of the Poisson problem.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02390" title="Abstract">arXiv:2310.02390</a> [<a href="/pdf/2310.02390" title="Download PDF">pdf</a>, <a href="/ps/2310.02390" title="Download PostScript">ps</a>, <a href="/format/2310.02390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shared Sequencing and Latency Competition as a Noisy Contest
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mamageishvili%2C+A">Akaki Mamageishvili</a>, 
<a href="/search/cs?searchtype=author&query=Schlegel%2C+J+C">Jan Christoph Schlegel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study shared sequencing for different chains from an economic angle. We
introduce a minimal non-trivial model that captures cross-domain arbitrageurs'
behavior and compare the performance of shared sequencing to that of separate
sequencing. While shared sequencing dominates separate sequencing trivially in
the sense that it makes it more likely that cross-chain arbitrage opportunities
are realized, the investment and revenue comparison is more subtle: In the
simple latency competition induced by First Come First Serve ordering, shared
sequencing creates more wasteful latency competition compared to separate
sequencing. For bidding-based sequencing, the most surprising insight is that
the revenue of shared sequencing is not always higher than that of separate
sequencing and depends on the transaction ordering rule applied and the
arbitrage value potentially realized.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02391" title="Abstract">arXiv:2310.02391</a> [<a href="/pdf/2310.02391" title="Download PDF">pdf</a>, <a href="/format/2310.02391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SE(3)-Stochastic Flow Matching for Protein Backbone Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bose%2C+A+J">Avishek Joey Bose</a>, 
<a href="/search/cs?searchtype=author&query=Akhound-Sadegh%2C+T">Tara Akhound-Sadegh</a>, 
<a href="/search/cs?searchtype=author&query=Fatras%2C+K">Kilian Fatras</a>, 
<a href="/search/cs?searchtype=author&query=Huguet%2C+G">Guillaume Huguet</a>, 
<a href="/search/cs?searchtype=author&query=Rector-Brooks%2C+J">Jarrid Rector-Brooks</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Cheng-Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Nica%2C+A+C">Andrei Cristian Nica</a>, 
<a href="/search/cs?searchtype=author&query=Korablyov%2C+M">Maksym Korablyov</a>, 
<a href="/search/cs?searchtype=author&query=Bronstein%2C+M">Michael Bronstein</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+A">Alexander Tong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The computational design of novel protein structures has the potential to
impact numerous scientific disciplines greatly. Toward this goal, we introduce
$\text{FoldFlow}$ a series of novel generative models of increasing modeling
power based on the flow-matching paradigm over $3\text{D}$ rigid motions --
i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein
backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free
approach to learning deterministic continuous-time dynamics and matching
invariant target distributions on $\text{SE(3)}$. We next accelerate training
by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$,
leading to the construction of both more simple and stable flows. Finally, we
design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free
training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our
family of $\text{FoldFlow}$ generative models offer several key advantages over
previous approaches to the generative modeling of proteins: they are more
stable and faster to train than diffusion-based approaches, and our models
enjoy the ability to map any invariant source distribution to any invariant
target distribution over $\text{SE(3)}$. Empirically, we validate our FoldFlow
models on protein backbone generation of up to $300$ amino acids leading to
high-quality designable, diverse, and novel samples.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02392" title="Abstract">arXiv:2310.02392</a> [<a href="/pdf/2310.02392" title="Download PDF">pdf</a>, <a href="/format/2310.02392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A 3D Mixed Reality Interface for Human-Robot Teaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Boyang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Pollefeys%2C+M">Marc Pollefeys</a>, 
<a href="/search/cs?searchtype=author&query=Blum%2C+H">Hermann Blum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper presents a mixed-reality human-robot teaming system. It allows
human operators to see in real-time where robots are located, even if they are
not in line of sight. The operator can also visualize the map that the robots
create of their environment and can easily send robots to new goal positions.
The system mainly consists of a mapping and a control module. The mapping
module is a real-time multi-agent visual SLAM system that co-localizes all
robots and mixed-reality devices to a common reference frame. Visualizations in
the mixed-reality device then allow operators to see a virtual life-sized
representation of the cumulative 3D map overlaid onto the real environment. As
such, the operator can effectively "see through" walls into other rooms. To
control robots and send them to new locations, we propose a drag-and-drop
interface. An operator can grab any robot hologram in a 3D mini map and drag it
to a new desired goal pose. We validate the proposed system through a user
study and real-world deployments. We make the mixed-reality application
publicly available at https://github.com/cvg/HoloLens_ros.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02393" title="Abstract">arXiv:2310.02393</a> [<a href="/pdf/2310.02393" title="Download PDF">pdf</a>, <a href="/ps/2310.02393" title="Download PostScript">ps</a>, <a href="/format/2310.02393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symbolic Automata: $&#x3c9;$-Regularity Modulo Theories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Veanes%2C+M">Margus Veanes</a>, 
<a href="/search/cs?searchtype=author&query=Ball%2C+T">Thomas Ball</a>, 
<a href="/search/cs?searchtype=author&query=Ebner%2C+G">Gabriel Ebner</a>, 
<a href="/search/cs?searchtype=author&query=Saarikivi%2C+O">Olli Saarikivi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Symbolic automata are finite state automata that support potentially infinite
alphabets, such as the set of rational numbers, generally applied to regular
expressions/languages over finite words. In symbolic automata (or automata
modulo theories), an alphabet is represented by an effective Boolean algebra,
supported by a decision procedure for satisfiability. Regular languages over
infinite words (so called $\omega$-regular languages) have a rich history
paralleling that of regular languages over finite words, with well known
applications to model checking via B\"uchi automata and temporal logics.
<br />We generalize symbolic automata to support $\omega$-regular languages via
symbolic transition terms and symbolic derivatives, bringing together a variety
of classic automata and logics in a unified framework that provides all the
necessary ingredients to support symbolic model checking modulo $A$, $NBW_A$.
In particular, we define: (1) alternating B\"uchi automata modulo $A$, $ABW_A$
as well (non-alternating) non-deterministic B\"uchi automata modulo $A$,
$NBW_A$; (2) an alternation elimination algorithm that incrementally constructs
an $NBW_A$ from an $ABW_A$, and can also be used for constructing the product
of two $NBW_A$'s; (3) a definition of linear temporal logic (LTL) modulo $A$
that generalizes Vardi's construction of alternating B\"uchi automata from LTL,
using (2) to go from LTL modulo $A$ to $NBW_A$ via $ABW_A$.
<br />Finally, we present a combination of LTL modulo $A$ with extended regular
expressions modulo $A$ that generalizes the Property Specification Language
(PSL). Our combination allows regex complement, that is not supported in PSL
but can be supported naturally by using symbolic transition terms.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02394" title="Abstract">arXiv:2310.02394</a> [<a href="/pdf/2310.02394" title="Download PDF">pdf</a>, <a href="/format/2310.02394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating systemic importance with missing data in input-output graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geneson%2C+J">Jesse Geneson</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+A">Alvin Moon</a>, 
<a href="/search/cs?searchtype=author&query=Robles%2C+N">Nicolas Robles</a>, 
<a href="/search/cs?searchtype=author&query=Strong%2C+A">Aaron Strong</a>, 
<a href="/search/cs?searchtype=author&query=Welburn%2C+J">Jonathan Welburn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">In the context of the Cobb-Douglas productivity model we consider the $N
\times N$ input-output linkage matrix $W$ for a network of $N$ firms $f_1, f_2,
\cdots, f_N$. The associated influence vector $v_w$ of $W$ is defined in terms
of the Leontief inverse $L_W$ of $W$ as $v_W = \frac{\alpha}{N} L_W
\vec{\mathbf{1}}$ where $L_W = (I - (1-\alpha) W')^{-1}$, $W'$ denotes the
transpose of $W$ and $I$ is the identity matrix. Here $\vec{\mathbf{1}}$ is the
$N \times 1$ vector whose entries are all one. The influence vector is a metric
of the importance for the firms in the production network. Under the realistic
assumption that the data to compute the influence vector is incomplete, we
prove bounds on the worst-case error for the influence vector that are sharp up
to a constant factor. We also consider the situation where the missing data is
binomially distributed and contextualize the bound on the influence vector
accordingly. We also investigate how far off the influence vector can be when
we only have data on nodes and connections that are within distance $k$ of some
source node. A comparison of our results is juxtaposed against PageRank
analogues. We close with a discussion on a possible extension beyond
Cobb-Douglas to the Constant Elasticity of Substitution model, as well as the
possibility of considering other probability distributions for missing data.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02395" title="Abstract">arXiv:2310.02395</a> [<a href="/pdf/2310.02395" title="Download PDF">pdf</a>, <a href="/format/2310.02395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Semantic Conflicts with Unit Tests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Da+Silva%2C+L">L&#xe9;uson Da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Borba%2C+P">Paulo Borba</a>, 
<a href="/search/cs?searchtype=author&query=Maciel%2C+T">Toni Maciel</a>, 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+W">Wardah Mahmood</a>, 
<a href="/search/cs?searchtype=author&query=Berger%2C+T">Thorsten Berger</a>, 
<a href="/search/cs?searchtype=author&query=Moisakis%2C+J">Jo&#xe3;o Moisakis</a>, 
<a href="/search/cs?searchtype=author&query=Gomes%2C+A">Aldiberg Gomes</a>, 
<a href="/search/cs?searchtype=author&query=Leite%2C+V">Vin&#xed;cius Leite</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 53 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Branching and merging are common practices in collaborative software
development, increasing developer's productivity. Despite such benefits,
developers need to merge software and resolve merge conflicts. While modern
merge techniques can resolve textual conflicts automatically, they fail when
the conflict arises at the semantic level. Although semantic merge tools have
been proposed, they are usually based on heavyweight static analyses or need
explicit specifications of program behavior. In this work, we take a different
route and propose SAM (SemAntic Merge), a semantic merge tool based on the
automated generation of unit tests that are used as partial specifications. To
evaluate SAM's feasibility for detecting conflicts, we perform an empirical
study analyzing more than 80 pairs of changes integrated into common class
elements from 51 merge scenarios. Furthermore, we also assess how the four
unit-test generation tools used by SAM contribute to conflict identification.
We propose and assess the adoption of Testability Transformations and
Serialization. Our results show that SAM best performs when combining only the
tests generated by Differential EvoSuite and EvoSuite and using the proposed
Testability Transformations (nine detected conflicts out of 28). These results
reinforce previous findings about the potential of using test-case generation
to detect test conflicts.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02396" title="Abstract">arXiv:2310.02396</a> [<a href="/pdf/2310.02396" title="Download PDF">pdf</a>, <a href="/format/2310.02396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit regularization of multi-task learning and finetuning in  overparameterized neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lindsey%2C+J+W">Jack W. Lindsey</a>, 
<a href="/search/cs?searchtype=author&query=Lippl%2C+S">Samuel Lippl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">It is common in deep learning to train networks on auxiliary tasks with the
expectation that the learning will transfer, at least partially, to another
task of interest. In this work, we investigate the inductive biases that result
from learning auxiliary tasks, either simultaneously (multi-task learning, MTL)
or sequentially (pretraining and subsequent finetuning, PT+FT). In the
simplified setting of two-layer diagonal linear networks trained with gradient
descent, we identify implicit regularization penalties associated with MTL and
PT+FT, both of which incentivize feature sharing between tasks and sparsity in
learned task-specific features. Notably, our results imply that during
finetuning, networks operate in a hybrid of the kernel (or "lazy") regime and
the feature learning ("rich") regime identified in prior work. Moreover, PT+FT
can exhibit a novel "nested feature learning" behavior not captured by either
regime, which biases it to extract a sparse subset of the features learned
during pretraining. In ReLU networks, we reproduce all of these qualitative
behaviors. We also observe that PT+FT (but not MTL) is biased to learn features
that are correlated with (but distinct from) those needed for the auxiliary
task, while MTL is biased toward using identical features for both tasks. As a
result, we find that in realistic settings, MTL generalizes better when
comparatively little data is available for the task of interest, while PT+FT
outperforms it with more data available. We show that our findings hold
qualitatively for a deep architecture trained on image classification tasks.
Our characterization of the nested feature learning regime also motivates a
modification to PT+FT that we find empirically improves performance. Overall,
our results shed light on the impact of auxiliary task learning and suggest
ways to leverage it more effectively.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02398" title="Abstract">arXiv:2310.02398</a> [<a href="/pdf/2310.02398" title="Download PDF">pdf</a>, <a href="/format/2310.02398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing Intraspecies and Interspecies Covariate Shift in Traumatic  Brain Injury EEG of Humans and Mice Using Transfer Euclidean Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vishwanath%2C+M">Manoj Vishwanath</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+S">Steven Cao</a>, 
<a href="/search/cs?searchtype=author&query=Dutt%2C+N">Nikil Dutt</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+A+M">Amir M. Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+M+M">Miranda M. Lim</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+H">Hung Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">While analytics of sleep electroencephalography (EEG) holds certain
advantages over other methods in clinical applications, high variability across
subjects poses a significant challenge when it comes to deploying machine
learning models for classification tasks in the real world. In such instances,
machine learning models that exhibit exceptional performance on a specific
dataset may not necessarily demonstrate similar proficiency when applied to a
distinct dataset for the same task. The scarcity of high-quality biomedical
data further compounds this challenge, making it difficult to evaluate the
model's generality comprehensively. In this paper, we introduce Transfer
Euclidean Alignment - a transfer learning technique to tackle the problem of
the dearth of human biomedical data for training deep learning models. We
tested the robustness of this transfer learning technique on various rule-based
classical machine learning models as well as the EEGNet-based deep learning
model by evaluating on different datasets, including human and mouse data in a
binary classification task of detecting individuals with versus without
traumatic brain injury (TBI). By demonstrating notable improvements with an
average increase of 14.42% for intraspecies datasets and 5.53% for interspecies
datasets, our findings underscore the importance of the use of transfer
learning to improve the performance of machine learning and deep learning
models when using diverse datasets for training.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02399" title="Abstract">arXiv:2310.02399</a> [<a href="/pdf/2310.02399" title="Download PDF">pdf</a>, <a href="/format/2310.02399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can 5G NR Sidelink communications support wireless augmented reality?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+A">Ashutosh Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Ping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Q">Qi Qu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Zhu Ji</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+Y+S">Yee Sin Chan</a>, 
<a href="/search/cs?searchtype=author&query=Panwar%2C+S+S">Shivendra S. Panwar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures, accepted for publication in 2023 IEEE Global Communications Conference: Mobile and Wireless Networks (Globecom 2023 MWN), Kuala Lumpur, Malaysia, Dec. 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Smart glasses that support augmented reality (AR) have the potential to
become the consumer's primary medium of connecting to the future internet. For
the best quality of user experience, AR glasses must have a small form factor
and long battery life, while satisfying the data rate and latency requirements
of AR applications. To extend the AR glasses' battery life, the computation and
processing involved in AR may be offloaded to a companion device, such as a
smartphone, through a wireless connection. Sidelink (SL), i.e., the D2D
communication interface of 5G NR, is a potential candidate for this wireless
link. In this paper, we use system-level simulations to analyze the feasibility
of NR SL for supporting AR. Our simulator incorporates the PHY layer structure
and MAC layer resource scheduling of 3GPP SL, standard 3GPP channel models, and
MCS configurations. Our results suggest that the current SL standard
specifications are insufficient for high-end AR use cases with heavy
interaction but can support simpler previews and file transfers. We further
propose two enhancements to SL resource allocation, which have the potential to
offer significant performance improvements for AR applications.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02401" title="Abstract">arXiv:2310.02401</a> [<a href="/pdf/2310.02401" title="Download PDF">pdf</a>, <a href="/format/2310.02401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yingqian Cui</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yuping Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Han Xu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengfei He</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+Y">Yue Xing</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wenqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Text-to-image generative models based on latent diffusion models (LDM) have
demonstrated their outstanding ability in generating high-quality and
high-resolution images according to language prompt. Based on these powerful
latent diffusion models, various fine-tuning methods have been proposed to
achieve the personalization of text-to-image diffusion models such as artistic
style adaptation and human face transfer. However, the unauthorized usage of
data for model personalization has emerged as a prevalent concern in relation
to copyright violations. For example, a malicious user may use the fine-tuning
technique to generate images which mimic the style of a painter without his/her
permission. In light of this concern, we have proposed FT-Shield, a
watermarking approach specifically designed for the fine-tuning of
text-to-image diffusion models to aid in detecting instances of infringement.
We develop a novel algorithm for the generation of the watermark to ensure that
the watermark on the training images can be quickly and accurately transferred
to the generated images of text-to-image diffusion models. A watermark will be
detected on an image by a binary watermark detector if the image is generated
by a model that has been fine-tuned using the protected watermarked images.
Comprehensive experiments were conducted to validate the effectiveness of
FT-Shield.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02402" title="Abstract">arXiv:2310.02402</a> [<a href="/pdf/2310.02402" title="Download PDF">pdf</a>, <a href="/format/2310.02402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Parallel Complexity of Multilevel Monte Carlo in Stocahstic  Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ishikawa%2C+K">Kei Ishikawa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In the stochastic gradient descent (SGD) for sequential simulations such as
the neural stochastic differential equations, the Multilevel Monte Carlo (MLMC)
method is known to offer better theoretical computational complexity compared
to the naive Monte Carlo approach. However, in practice, MLMC scales poorly on
massively parallel computing platforms such as modern GPUs, because of its
large parallel complexity which is equivalent to that of the naive Monte Carlo
method. To cope with this issue, we propose the delayed MLMC gradient estimator
that drastically reduces the parallel complexity of MLMC by recycling
previously computed gradient components from earlier steps of SGD. The proposed
estimator provably reduces the average parallel complexity per iteration at the
cost of a slightly worse per-iteration convergence rate. In our numerical
experiments, we use an example of deep hedging to demonstrate the superior
parallel complexity of our method compared to the standard MLMC in SGD.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02405" title="Abstract">arXiv:2310.02405</a> [<a href="/pdf/2310.02405" title="Download PDF">pdf</a>, <a href="/format/2310.02405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PCGPT: Procedural Content Generation via Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohaghegh%2C+S">Sajad Mohaghegh</a>, 
<a href="/search/cs?searchtype=author&query=Dehnavi%2C+M+A+R">Mohammad Amin Ramezan Dehnavi</a>, 
<a href="/search/cs?searchtype=author&query=Abdollahinejad%2C+G">Golnoosh Abdollahinejad</a>, 
<a href="/search/cs?searchtype=author&query=Hashemi%2C+M">Matin Hashemi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The paper presents the PCGPT framework, an innovative approach to procedural
content generation (PCG) using offline reinforcement learning and transformer
networks. PCGPT utilizes an autoregressive model based on transformers to
generate game levels iteratively, addressing the challenges of traditional PCG
methods such as repetitive, predictable, or inconsistent content. The framework
models trajectories of actions, states, and rewards, leveraging the
transformer's self-attention mechanism to capture temporal dependencies and
causal relationships. The approach is evaluated in the Sokoban puzzle game,
where the model predicts items that are needed with their corresponding
locations. Experimental results on the game Sokoban demonstrate that PCGPT
generates more complex and diverse game content. Interestingly, it achieves
these results in significantly fewer steps compared to existing methods,
showcasing its potential for enhancing game design and online content
generation. Our model represents a new PCG paradigm which outperforms previous
methods.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02407" title="Abstract">arXiv:2310.02407</a> [<a href="/pdf/2310.02407" title="Download PDF">pdf</a>, <a href="/format/2310.02407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Bug Generation in the era of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ibrahimzada%2C+A+R">Ali Reza Ibrahimzada</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Rong%2C+R">Ryan Rong</a>, 
<a href="/search/cs?searchtype=author&query=Jabbarvand%2C+R">Reyhaneh Jabbarvand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Bugs are essential in software engineering; many research studies in the past
decades have been proposed to detect, localize, and repair bugs in software
systems. Effectiveness evaluation of such techniques requires complex bugs,
i.e., those that are hard to detect through testing and hard to repair through
debugging. From the classic software engineering point of view, a
hard-to-repair bug differs from the correct code in multiple locations, making
it hard to localize and repair. Hard-to-detect bugs, on the other hand,
manifest themselves under specific test inputs and reachability conditions.
These two objectives, i.e., generating hard-to-detect and hard-to-repair bugs,
are mostly aligned; a bug generation technique can change multiple statements
to be covered only under a specific set of inputs. However, these two
objectives are conflicting for learning-based techniques: A bug should have a
similar code representation to the correct code in the training data to
challenge a bug prediction model to distinguish them. The hard-to-repair bug
definition remains the same but with a caveat: the more a bug differs from the
original code (at multiple locations), the more distant their representations
are and easier to be detected. We propose BugFarm, to transform arbitrary code
into multiple complex bugs. BugFarm leverages LLMs to mutate code in multiple
locations (hard-to-repair). To ensure that multiple modifications do not
notably change the code representation, BugFarm analyzes the attention of the
underlying model and instructs LLMs to only change the least attended locations
(hard-to-detect). Our comprehensive evaluation of 320k+ bugs from over 2.5M
mutants generated by BugFarm and two alternative approaches demonstrates our
superiority in generating bugs that are hard to detect by learning-based bug
prediction approaches and hard to repair by SOTA learning-based program repair
technique.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02408" title="Abstract">arXiv:2310.02408</a> [<a href="/pdf/2310.02408" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MindTheDApp: A Toolchain for Complex Network-Driven Structural Analysis  of Ethereum-based Decentralised Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ibba%2C+G">Giacomo Ibba</a>, 
<a href="/search/cs?searchtype=author&query=Aufiero%2C+S">Sabrina Aufiero</a>, 
<a href="/search/cs?searchtype=author&query=Bartolucci%2C+S">Silvia Bartolucci</a>, 
<a href="/search/cs?searchtype=author&query=Neykova%2C+R">Rumyana Neykova</a>, 
<a href="/search/cs?searchtype=author&query=Ortu%2C+M">Marco Ortu</a>, 
<a href="/search/cs?searchtype=author&query=Tonelli%2C+R">Roberto Tonelli</a>, 
<a href="/search/cs?searchtype=author&query=Destefanis%2C+G">Giuseppe Destefanis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">This paper presents MindTheDApp, a toolchain designed specifically for the
structural analysis of Ethereum-based Decentralized Applications (DApps), with
a distinct focus on a complex network-driven approach. Unlike existing tools,
our toolchain combines the power of ANTLR4 and Abstract Syntax Tree (AST)
traversal techniques to transform the architecture and interactions within
smart contracts into a specialized bipartite graph. This enables advanced
network analytics to highlight operational efficiencies within the DApp's
architecture.
<br />The bipartite graph generated by the proposed tool comprises two sets of
nodes: one representing smart contracts, interfaces, and libraries, and the
other including functions, events, and modifiers. Edges in the graph connect
functions to smart contracts they interact with, offering a granular view of
interdependencies and execution flow within the DApp. This network-centric
approach allows researchers and practitioners to apply complex network theory
in understanding the robustness, adaptability, and intricacies of decentralized
systems.
<br />Our work contributes to the enhancement of security in smart contracts by
allowing the visualisation of the network, and it provides a deep understanding
of the architecture and operational logic within DApps. Given the growing
importance of smart contracts in the blockchain ecosystem and the emerging
application of complex network theory in technology, our toolchain offers a
timely contribution to both academic research and practical applications in the
field of blockchain technology.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02409" title="Abstract">arXiv:2310.02409</a> [<a href="/pdf/2310.02409" title="Download PDF">pdf</a>, <a href="/format/2310.02409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+G">Guanghui Qin</a>, 
<a href="/search/cs?searchtype=author&query=Rosset%2C+C">Corby Rosset</a>, 
<a href="/search/cs?searchtype=author&query=Chau%2C+E+C">Ethan C. Chau</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+N">Nikhil Rao</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. 15 pages and 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Standard Transformer-based language models (LMs) scale poorly to long
contexts. We propose a solution based on dynamic contextual compression, which
extends the Nugget approach of Qin &amp; Van Durme (2023) from BERT-like frameworks
to decoder-only LMs. Our method models history as compressed "nuggets" which
are trained to allow for reconstruction, and it can be initialized with
off-the-shelf models such as LLaMA. We demonstrate through experiments in
language modeling, question answering, and summarization that Nugget2D retains
capabilities in these tasks, while drastically reducing the overhead during
decoding in terms of time and space. For example, in the experiments of
autoencoding, Nugget2D can shrink context at a 20x compression ratio with a
BLEU score of 98% for reconstruction, achieving nearly lossless encoding.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02410" title="Abstract">arXiv:2310.02410</a> [<a href="/pdf/2310.02410" title="Download PDF">pdf</a>, <a href="/format/2310.02410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit  Quantization and Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y+J">Young Jin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Fahim%2C+R">Raffy Fahim</a>, 
<a href="/search/cs?searchtype=author&query=Awadalla%2C+H+H">Hany Hassan Awadalla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Mixture of Experts (MoE) models could achieve state-of-the-art quality
on various language tasks, including machine translation task, thanks to the
efficient model scaling capability with expert parallelism. However, it has
brought a fundamental issue of larger memory consumption and increased memory
bandwidth bottleneck at deployment time. In this paper, we propose Mixture of
Quantized Experts (MoQE) which is a simple weight-only quantization method
applying ultra low-bit down to 2-bit quantizations only to expert weights for
mitigating the increased memory and latency issues of MoE models. We show that
low-bit quantization together with the MoE architecture delivers a reliable
model performance while reducing the memory size significantly even without any
additional training in most cases. In particular, expert layers in MoE models
are much more robust to the quantization than conventional feedforward networks
(FFN) layers. In our comprehensive analysis, we show that MoE models with 2-bit
expert weights can deliver better model performance than the dense model
trained on the same dataset. As a result of low-bit quantization, we show the
model size can be reduced by 79.6% of the original half precision floating
point (fp16) MoE model. Combined with an optimized GPU runtime implementation,
it also achieves 1.24X speed-up on A100 GPUs.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02416" title="Abstract">arXiv:2310.02416</a> [<a href="/pdf/2310.02416" title="Download PDF">pdf</a>, <a href="/format/2310.02416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bag of Tricks for Fully Test-Time Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mounsaveng%2C+S">Saypraseuth Mounsaveng</a>, 
<a href="/search/cs?searchtype=author&query=Chiaroni%2C+F">Florent Chiaroni</a>, 
<a href="/search/cs?searchtype=author&query=Boudiaf%2C+M">Malik Boudiaf</a>, 
<a href="/search/cs?searchtype=author&query=Pedersoli%2C+M">Marco Pedersoli</a>, 
<a href="/search/cs?searchtype=author&query=Ayed%2C+I+B">Ismail Ben Ayed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Fully Test-Time Adaptation (TTA), which aims at adapting models to data
drifts, has recently attracted wide interest. Numerous tricks and techniques
have been proposed to ensure robust learning on arbitrary streams of unlabeled
data. However, assessing the true impact of each individual technique and
obtaining a fair comparison still constitutes a significant challenge. To help
consolidate the community's knowledge, we present a categorization of selected
orthogonal TTA techniques, including small batch normalization, stream
rebalancing, reliable sample selection, and network confidence calibration. We
meticulously dissect the effect of each approach on different scenarios of
interest. Through our analysis, we shed light on trade-offs induced by those
techniques between accuracy, the computational power required, and model
complexity. We also uncover the synergy that arises when combining techniques
and are able to establish new state-of-the-art results.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02417" title="Abstract">arXiv:2310.02417</a> [<a href="/pdf/2310.02417" title="Download PDF">pdf</a>, <a href="/format/2310.02417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jailbreaker in Jail: Moving Target Defense for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bocheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Paliwal%2C+A">Advait Paliwal</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Q">Qiben Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MTD Workshop in CCS'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Large language models (LLMs), known for their capability in understanding and
following instructions, are vulnerable to adversarial attacks. Researchers have
found that current commercial LLMs either fail to be "harmless" by presenting
unethical answers, or fail to be "helpful" by refusing to offer meaningful
answers when faced with adversarial queries. To strike a balance between being
helpful and harmless, we design a moving target defense (MTD) enhanced LLM
system. The system aims to deliver non-toxic answers that align with outputs
from multiple model candidates, making them more robust against adversarial
attacks. We design a query and output analysis model to filter out unsafe or
non-responsive answers. %to achieve the two objectives of randomly selecting
outputs from different LLMs. We evaluate over 8 most recent chatbot models with
state-of-the-art adversarial queries. Our MTD-enhanced LLM system reduces the
attack success rate from 37.5\% to 0\%. Meanwhile, it decreases the response
refusal rate from 50\% to 0\%.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02420" title="Abstract">arXiv:2310.02420</a> [<a href="/pdf/2310.02420" title="Download PDF">pdf</a>, <a href="/format/2310.02420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedL2P: Federated Learning to Personalize
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+R">Royson Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minyoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Da Li</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xinchi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Hospedales%2C+T">Timothy Hospedales</a>, 
<a href="/search/cs?searchtype=author&query=Husz%C3%A1r%2C+F">Ferenc Husz&#xe1;r</a>, 
<a href="/search/cs?searchtype=author&query=Lane%2C+N+D">Nicholas D. Lane</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated learning (FL) research has made progress in developing algorithms
for distributed learning of global models, as well as algorithms for local
personalization of those common models to the specifics of each client's local
data distribution. However, different FL problems may require different
personalization strategies, and it may not even be possible to define an
effective one-size-fits-all personalization strategy for all clients: depending
on how similar each client's optimal predictor is to that of the global model,
different personalization strategies may be preferred. In this paper, we
consider the federated meta-learning problem of learning personalization
strategies. Specifically, we consider meta-nets that induce the batch-norm and
learning rate parameters for each client given local data statistics. By
learning these meta-nets through FL, we allow the whole FL network to
collaborate in learning a customized personalization strategy for each client.
Empirical results show that this framework improves on a range of standard
hand-crafted personalization baselines in both label and feature shift
situations.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02421" title="Abstract">arXiv:2310.02421</a> [<a href="/pdf/2310.02421" title="Download PDF">pdf</a>, <a href="/ps/2310.02421" title="Download PostScript">ps</a>, <a href="/format/2310.02421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can a student Large Language Model perform as well as it&#x27;s teacher?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gholami%2C+S">Sia Gholami</a>, 
<a href="/search/cs?searchtype=author&query=Omar%2C+M">Marwan Omar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">The burgeoning complexity of contemporary deep learning models, while
achieving unparalleled accuracy, has inadvertently introduced deployment
challenges in resource-constrained environments. Knowledge distillation, a
technique aiming to transfer knowledge from a high-capacity "teacher" model to
a streamlined "student" model, emerges as a promising solution to this dilemma.
This paper provides a comprehensive overview of the knowledge distillation
paradigm, emphasizing its foundational principles such as the utility of soft
labels and the significance of temperature scaling. Through meticulous
examination, we elucidate the critical determinants of successful distillation,
including the architecture of the student model, the caliber of the teacher,
and the delicate balance of hyperparameters. While acknowledging its profound
advantages, we also delve into the complexities and challenges inherent in the
process. Our exploration underscores knowledge distillation's potential as a
pivotal technique in optimizing the trade-off between model performance and
deployment efficiency.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02422" title="Abstract">arXiv:2310.02422</a> [<a href="/pdf/2310.02422" title="Download PDF">pdf</a>, <a href="/format/2310.02422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OneAdapt: Fast Adaptation for Deep Learning Applications via  Backpropagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+K">Kuntai Du</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuhan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yitian Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qizheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haodong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuyang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ananthanarayanan%2C+G">Ganesh Ananthanarayanan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junchen Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SoCC' 23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Multimedia (cs.MM); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Deep learning inference on streaming media data, such as object detection in
video or LiDAR feeds and text extraction from audio waves, is now ubiquitous.
To achieve high inference accuracy, these applications typically require
significant network bandwidth to gather high-fidelity data and extensive GPU
resources to run deep neural networks (DNNs). While the high demand for network
bandwidth and GPU resources could be substantially reduced by optimally
adapting the configuration knobs, such as video resolution and frame rate,
current adaptation techniques fail to meet three requirements simultaneously:
adapt configurations (i) with minimum extra GPU or bandwidth overhead; (ii) to
reach near-optimal decisions based on how the data affects the final DNN's
accuracy, and (iii) do so for a range of configuration knobs. This paper
presents OneAdapt, which meets these requirements by leveraging a
gradient-ascent strategy to adapt configuration knobs. The key idea is to
embrace DNNs' differentiability to quickly estimate the accuracy's gradient to
each configuration knob, called AccGrad. Specifically, OneAdapt estimates
AccGrad by multiplying two gradients: InputGrad (i.e. how each configuration
knob affects the input to the DNN) and DNNGrad (i.e. how the DNN input affects
the DNN inference output). We evaluate OneAdapt across five types of
configurations, four analytic tasks, and five types of input data. Compared to
state-of-the-art adaptation schemes, OneAdapt cuts bandwidth usage and GPU
usage by 15-59% while maintaining comparable accuracy or improves accuracy by
1-5% while using equal or fewer resources.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02423" title="Abstract">arXiv:2310.02423</a> [<a href="/pdf/2310.02423" title="Download PDF">pdf</a>, <a href="/format/2310.02423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Delta-AI: Local objectives for amortized inference in sparse graphical  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Falet%2C+J">Jean-Pierre Falet</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H+B">Hae Beom Lee</a>, 
<a href="/search/cs?searchtype=author&query=Malkin%2C+N">Nikolay Malkin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Secrieru%2C+D">Dragos Secrieru</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dinghuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lajoie%2C+G">Guillaume Lajoie</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, code: <a href="https://github.com/GFNOrg/Delta-AI/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We present a new algorithm for amortized inference in sparse probabilistic
graphical models (PGMs), which we call $\Delta$-amortized inference
($\Delta$-AI). Our approach is based on the observation that when the sampling
of variables in a PGM is seen as a sequence of actions taken by an agent,
sparsity of the PGM enables local credit assignment in the agent's policy
learning objective. This yields a local constraint that can be turned into a
local loss in the style of generative flow networks (GFlowNets) that enables
off-policy training but avoids the need to instantiate all the random variables
for each parameter update, thus speeding up training considerably. The
$\Delta$-AI objective matches the conditional distribution of a variable given
its Markov blanket in a tractable learned sampler, which has the structure of a
Bayesian network, with the same conditional distribution under the target PGM.
As such, the trained sampler recovers marginals and conditional distributions
of interest and enables inference of partial subsets of variables. We
illustrate $\Delta$-AI's effectiveness for sampling from synthetic PGMs and
training latent variable models with sparse factor structure.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02424" title="Abstract">arXiv:2310.02424</a> [<a href="/pdf/2310.02424" title="Download PDF">pdf</a>, <a href="/format/2310.02424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AXNav: Replaying Accessibility Tests from Natural Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taeb%2C+M">Maryam Taeb</a>, 
<a href="/search/cs?searchtype=author&query=Swearngin%2C+A">Amanda Swearngin</a>, 
<a href="/search/cs?searchtype=author&query=School%2C+E">Eldon School</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Ruijia Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yue Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Nichols%2C+J">Jeffrey Nichols</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Developers and quality assurance testers often rely on manual testing to test
accessibility features throughout the product lifecycle. Unfortunately, manual
testing can be tedious, often has an overwhelming scope, and can be difficult
to schedule amongst other development milestones. Recently, Large Language
Models (LLMs) have been used for a variety of tasks including automation of
UIs, however to our knowledge no one has yet explored their use in controlling
assistive technologies for the purposes of supporting accessibility testing. In
this paper, we explore the requirements of a natural language based
accessibility testing workflow, starting with a formative study. From this we
build a system that takes as input a manual accessibility test (e.g., ``Search
for a show in VoiceOver'') and uses an LLM combined with pixel-based UI
Understanding models to execute the test and produce a chaptered, navigable
video. In each video, to help QA testers we apply heuristics to detect and flag
accessibility issues (e.g., Text size not increasing with Large Text enabled,
VoiceOver navigation loops). We evaluate this system through a 10 participant
user study with accessibility QA professionals who indicated that the tool
would be very useful in their current work and performed tests similarly to how
they would manually test the features. The study also reveals insights for
future work on using LLMs for accessibility testing.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02426" title="Abstract">arXiv:2310.02426</a> [<a href="/pdf/2310.02426" title="Download PDF">pdf</a>, <a href="/format/2310.02426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EditVal: Benchmarking Diffusion Based Text-Guided Image Editing Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Basu%2C+S">Samyadeep Basu</a>, 
<a href="/search/cs?searchtype=author&query=Saberi%2C+M">Mehrdad Saberi</a>, 
<a href="/search/cs?searchtype=author&query=Bhardwaj%2C+S">Shweta Bhardwaj</a>, 
<a href="/search/cs?searchtype=author&query=Chegini%2C+A+M">Atoosa Malemir Chegini</a>, 
<a href="/search/cs?searchtype=author&query=Massiceti%2C+D">Daniela Massiceti</a>, 
<a href="/search/cs?searchtype=author&query=Sanjabi%2C+M">Maziar Sanjabi</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S+X">Shell Xu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">A plethora of text-guided image editing methods have recently been developed
by leveraging the impressive capabilities of large-scale diffusion-based
generative models such as Imagen and Stable Diffusion. A standardized
evaluation protocol, however, does not exist to compare methods across
different types of fine-grained edits. To address this gap, we introduce
EditVal, a standardized benchmark for quantitatively evaluating text-guided
image editing methods. EditVal consists of a curated dataset of images, a set
of editable attributes for each image drawn from 13 possible edit types, and an
automated evaluation pipeline that uses pre-trained vision-language models to
assess the fidelity of generated images for each edit type. We use EditVal to
benchmark 8 cutting-edge diffusion-based editing methods including SINE, Imagic
and Instruct-Pix2Pix. We complement this with a large-scale human study where
we show that EditVall's automated evaluation pipeline is strongly correlated
with human-preferences for the edit types we considered. From both the human
study and automated evaluation, we find that: (i) Instruct-Pix2Pix, Null-Text
and SINE are the top-performing methods averaged across different edit types,
however {\it only} Instruct-Pix2Pix and Null-Text are able to preserve original
image properties; (ii) Most of the editing methods fail at edits involving
spatial operations (e.g., changing the position of an object). (iii) There is
no `winner' method which ranks the best individually across a range of
different edit types. We hope that our benchmark can pave the way to developing
more reliable text-guided image editing tools in the future. We will publicly
release EditVal, and all associated code and human-study templates to support
these research directions in https://deep-ml-research.github.io/editval/.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02428" title="Abstract">arXiv:2310.02428</a> [<a href="/pdf/2310.02428" title="Download PDF">pdf</a>, <a href="/format/2310.02428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields  for Atomistic Simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bihani%2C+V">Vaibhav Bihani</a>, 
<a href="/search/cs?searchtype=author&query=Pratiush%2C+U">Utkarsh Pratiush</a>, 
<a href="/search/cs?searchtype=author&query=Mannan%2C+S">Sajid Mannan</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+T">Tao Du</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhimin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Miret%2C+S">Santiago Miret</a>, 
<a href="/search/cs?searchtype=author&query=Micoulaut%2C+M">Matthieu Micoulaut</a>, 
<a href="/search/cs?searchtype=author&query=Smedskjaer%2C+M+M">Morten M Smedskjaer</a>, 
<a href="/search/cs?searchtype=author&query=Ranu%2C+S">Sayan Ranu</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+N+M+A">N M Anoop Krishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Materials Science (cond-mat.mtrl-sci)

</div>
<p class="mathjax">Equivariant graph neural networks force fields (EGraFFs) have shown great
promise in modelling complex interactions in atomic systems by exploiting the
graphs' inherent symmetries. Recent works have led to a surge in the
development of novel architectures that incorporate equivariance-based
inductive biases alongside architectural innovations like graph transformers
and message passing to model atomic interactions. However, thorough evaluations
of these deploying EGraFFs for the downstream task of real-world atomistic
simulations, is lacking. To this end, here we perform a systematic benchmarking
of 6 EGraFF algorithms (NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet),
with the aim of understanding their capabilities and limitations for realistic
atomistic simulations. In addition to our thorough evaluation and analysis on
eight existing datasets based on the benchmarking literature, we release two
new benchmark datasets, propose four new metrics, and three new challenging
tasks. The new datasets and tasks evaluate the performance of EGraFF to
out-of-distribution data, in terms of different crystal structures,
temperatures, and new molecules. Interestingly, evaluation of the EGraFF models
based on dynamic simulations reveals that having a lower error on energy or
force does not guarantee stable or reliable simulation or faithful replication
of the atomic structures. Moreover, we find that no model clearly outperforms
other models on all datasets and tasks. Importantly, we show that the
performance of all the models on out-of-distribution datasets is unreliable,
pointing to the need for the development of a foundation model for force fields
that can be used in real-world simulations. In summary, this work establishes a
rigorous framework for evaluating machine learning force fields in the context
of atomic simulations and points to open research challenges within this
domain.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02430" title="Abstract">arXiv:2310.02430</a> [<a href="/pdf/2310.02430" title="Download PDF">pdf</a>, <a href="/format/2310.02430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Episodic Memory Theory for the Mechanistic Interpretation of Recurrent  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karuvally%2C+A">Arjun Karuvally</a>, 
<a href="/search/cs?searchtype=author&query=Delmastro%2C+P">Peter Delmastro</a>, 
<a href="/search/cs?searchtype=author&query=Siegelmann%2C+H+T">Hava T. Siegelmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Understanding the intricate operations of Recurrent Neural Networks (RNNs)
mechanistically is pivotal for advancing their capabilities and applications.
In this pursuit, we propose the Episodic Memory Theory (EMT), illustrating that
RNNs can be conceptualized as discrete-time analogs of the recently proposed
General Sequential Episodic Memory Model. To substantiate EMT, we introduce a
novel set of algorithmic tasks tailored to probe the variable binding behavior
in RNNs. Utilizing the EMT, we formulate a mathematically rigorous circuit that
facilitates variable binding in these tasks. Our empirical investigations
reveal that trained RNNs consistently converge to the variable binding circuit,
thus indicating universality in the dynamics of RNNs. Building on these
findings, we devise an algorithm to define a privileged basis, which reveals
hidden neurons instrumental in the temporal storage and composition of
variables, a mechanism vital for the successful generalization in these tasks.
We show that the privileged basis enhances the interpretability of the learned
parameters and hidden states of RNNs. Our work represents a step toward
demystifying the internal mechanisms of RNNs and, for computational
neuroscience, serves to bridge the gap between artificial neural networks and
neural memory models.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02431" title="Abstract">arXiv:2310.02431</a> [<a href="/pdf/2310.02431" title="Download PDF">pdf</a>, <a href="/format/2310.02431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models Provide Security &amp; Privacy Advice? Measuring  the Ability of LLMs to Refute Misconceptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yufan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Arunasalam%2C+A">Arjun Arunasalam</a>, 
<a href="/search/cs?searchtype=author&query=Celik%2C+Z+B">Z. Berkay Celik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Annual Computer Security Applications Conference (ACSAC), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Users seek security &amp; privacy (S&amp;P) advice from online resources, including
trusted websites and content-sharing platforms. These resources help users
understand S&amp;P technologies and tools and suggest actionable strategies. Large
Language Models (LLMs) have recently emerged as trusted information sources.
However, their accuracy and correctness have been called into question. Prior
research has outlined the shortcomings of LLMs in answering multiple-choice
questions and user ability to inadvertently circumvent model restrictions
(e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable
S&amp;P advice is not well-explored. In this paper, we measure their ability to
refute popular S&amp;P misconceptions that the general public holds. We first study
recent academic literature to curate a dataset of over a hundred S&amp;P-related
misconceptions across six different topics. We then query two popular LLMs
(Bard and ChatGPT) and develop a labeling guide to evaluate their responses to
these misconceptions. To comprehensively evaluate their responses, we further
apply three strategies: query each misconception multiple times, generate and
query their paraphrases, and solicit source URLs of the responses. Both models
demonstrate, on average, a 21.3% non-negligible error rate, incorrectly
supporting popular S&amp;P misconceptions. The error rate increases to 32.6% when
we repeatedly query LLMs with the same or paraphrased misconceptions. We also
expose that models may partially support a misconception or remain
noncommittal, refusing a firm stance on misconceptions. Our exploration of
information sources for responses revealed that LLMs are susceptible to
providing invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point to
unrelated sources (44.2% returned by Bard and 18.3% by ChatGPT).
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02432" title="Abstract">arXiv:2310.02432</a> [<a href="/pdf/2310.02432" title="Download PDF">pdf</a>, <a href="/format/2310.02432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Dark Patterns: A Concept-Based Framework for Software Design  Ethics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Caragay%2C+E">Evan Caragay</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+K">Katherine Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zong%2C+J">Jonathan Zong</a>, 
<a href="/search/cs?searchtype=author&query=Jackson%2C+D">Daniel Jackson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">We present a framework of dark patterns grounded in user expectations. In
contrast to prior approaches that treat design techniques as inherently either
good or bad, we analyze mismatched user expectations for application behavior
using concepts -- reusable units of functionality that users encounter across
applications. We define a design as dark when its concepts violate users'
expectations, and benefit the provider of the application at the user's
expense. Though user expectations can differ, leading to subjective perceptions
of the ethics of an interface, users tend to develop common expectations as
they encounter the same concepts repeatedly across multiple applications. This
reuse results in users having shared expectations of concept functionality,
which we can record as standard concepts. Through case studies, we illustrate
how concept analysis helps designers identify, compare, and resolve dark
patterns. We suggest a shift away from dark pattern taxonomies toward more
systematic, actionable accounts of interface design ethics
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02435" title="Abstract">arXiv:2310.02435</a> [<a href="/pdf/2310.02435" title="Download PDF">pdf</a>, <a href="/format/2310.02435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Agent Reinforcement Learning Based on Representational  Communication for Large-Scale Traffic Signal Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bokade%2C+R">Rohit Bokade</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiaoning Jin</a>, 
<a href="/search/cs?searchtype=author&query=Amato%2C+C">Christopher Amato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Access (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Traffic signal control (TSC) is a challenging problem within intelligent
transportation systems and has been tackled using multi-agent reinforcement
learning (MARL). While centralized approaches are often infeasible for
large-scale TSC problems, decentralized approaches provide scalability but
introduce new challenges, such as partial observability. Communication plays a
critical role in decentralized MARL, as agents must learn to exchange
information using messages to better understand the system and achieve
effective coordination. Deep MARL has been used to enable inter-agent
communication by learning communication protocols in a differentiable manner.
However, many deep MARL communication frameworks proposed for TSC allow agents
to communicate with all other agents at all times, which can add to the
existing noise in the system and degrade overall performance. In this study, we
propose a communication-based MARL framework for large-scale TSC. Our framework
allows each agent to learn a communication policy that dictates "which" part of
the message is sent "to whom". In essence, our framework enables agents to
selectively choose the recipients of their messages and exchange variable
length messages with them. This results in a decentralized and flexible
communication mechanism in which agents can effectively use the communication
channel only when necessary. We designed two networks, a synthetic $4 \times 4$
grid network and a real-world network based on the Pasubio neighborhood in
Bologna. Our framework achieved the lowest network congestion compared to
related methods, with agents utilizing $\sim 47-65 \%$ of the communication
channel. Ablation studies further demonstrated the effectiveness of the
communication policies learned within our framework.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02437" title="Abstract">arXiv:2310.02437</a> [<a href="/pdf/2310.02437" title="Download PDF">pdf</a>, <a href="/format/2310.02437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EvDNeRF: Reconstructing Event Data with Dynamic Neural Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+A">Anish Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Madaan%2C+R">Ratnesh Madaan</a>, 
<a href="/search/cs?searchtype=author&query=Cladera%2C+F">Fernando Cladera</a>, 
<a href="/search/cs?searchtype=author&query=Vemprala%2C+S">Sai Vemprala</a>, 
<a href="/search/cs?searchtype=author&query=Bonatti%2C+R">Rogerio Bonatti</a>, 
<a href="/search/cs?searchtype=author&query=Daniilidis%2C+K">Kostas Daniilidis</a>, 
<a href="/search/cs?searchtype=author&query=Kapoor%2C+A">Ashish Kapoor</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V">Vijay Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Matni%2C+N">Nikolai Matni</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+J+K">Jayesh K. Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 20 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present EvDNeRF, a pipeline for generating event data and training an
event-based dynamic NeRF, for the purpose of faithfully reconstructing
eventstreams on scenes with rigid and non-rigid deformations that may be too
fast to capture with a standard camera. Event cameras register asynchronous
per-pixel brightness changes at MHz rates with high dynamic range, making them
ideal for observing fast motion with almost no motion blur. Neural radiance
fields (NeRFs) offer visual-quality geometric-based learnable rendering, but
prior work with events has only considered reconstruction of static scenes. Our
EvDNeRF can predict eventstreams of dynamic scenes from a static or moving
viewpoint between any desired timestamps, thereby allowing it to be used as an
event-based simulator for a given scene. We show that by training on varied
batch sizes of events, we can improve test-time predictions of events at fine
time resolutions, outperforming baselines that pair standard dynamic NeRFs with
event simulators. We release our simulated and real datasets, as well as code
for both event-based data generation and the training of event-based dynamic
NeRF models (https://github.com/anish-bhattacharya/EvDNeRF).
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02439" title="Abstract">arXiv:2310.02439</a> [<a href="/pdf/2310.02439" title="Download PDF">pdf</a>, <a href="/format/2310.02439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of  Large Language Models with Misconceptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Naiming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sonkar%2C+S">Shashank Sonkar</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Woodhead%2C+S">Simon Woodhead</a>, 
<a href="/search/cs?searchtype=author&query=Baraniuk%2C+R+G">Richard G. Baraniuk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We propose novel evaluations for mathematical reasoning capabilities of Large
Language Models (LLMs) based on mathematical misconceptions. Our primary
approach is to simulate LLMs as a novice learner and an expert tutor, aiming to
identify the incorrect answer to math question resulted from a specific
misconception and to recognize the misconception(s) behind an incorrect answer,
respectively. Contrary to traditional LLMs-based mathematical evaluations that
focus on answering math questions correctly, our approach takes inspirations
from principles in educational learning sciences. We explicitly ask LLMs to
mimic a novice learner by answering questions in a specific incorrect manner
based on incomplete knowledge; and to mimic an expert tutor by identifying
misconception(s) corresponding to an incorrect answer to a question. Using
simple grade-school math problems, our experiments reveal that, while LLMs can
easily answer these questions correctly, they struggle to identify 1) the
incorrect answer corresponding to specific incomplete knowledge
(misconceptions); 2) the misconceptions that explain particular incorrect
answers. Our study indicates new opportunities for enhancing LLMs' math
reasoning capabilities, especially on developing robust student simulation and
expert tutoring models in the educational applications such as intelligent
tutoring systems.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02440" title="Abstract">arXiv:2310.02440</a> [<a href="/pdf/2310.02440" title="Download PDF">pdf</a>, <a href="/format/2310.02440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Diverse Skills for Local Navigation under Multi-constraint  Optimality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Jin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Vlastelica%2C+M">Marin Vlastelica</a>, 
<a href="/search/cs?searchtype=author&query=Kolev%2C+P">Pavel Kolev</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Martius%2C+G">Georg Martius</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, in submission to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite many successful applications of data-driven control in robotics,
extracting meaningful diverse behaviors remains a challenge. Typically, task
performance needs to be compromised in order to achieve diversity. In many
scenarios, task requirements are specified as a multitude of reward terms, each
requiring a different trade-off. In this work, we take a constrained
optimization viewpoint on the quality-diversity trade-off and show that we can
obtain diverse policies while imposing constraints on their value functions
which are defined through distinct rewards. In line with previous work, further
control of the diversity level can be achieved through an attract-repel reward
term motivated by the Van der Waals force. We demonstrate the effectiveness of
our method on a local navigation task where a quadruped robot needs to reach
the target within a finite horizon. Finally, our trained policies transfer well
to the real 12-DoF quadruped robot, Solo12, and exhibit diverse agile behaviors
with successful obstacle traversal.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02442" title="Abstract">arXiv:2310.02442</a> [<a href="/pdf/2310.02442" title="Download PDF">pdf</a>, <a href="/format/2310.02442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenCO: Generating Diverse Solutions to Design Problems with  Combinatorial Nature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferber%2C+A">Aaron Ferber</a>, 
<a href="/search/cs?searchtype=author&query=Zharmagambetov%2C+A">Arman Zharmagambetov</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Taoan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Dilkina%2C+B">Bistra Dilkina</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuandong Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Generating diverse objects (e.g., images) using generative models (such as
GAN or VAE) has achieved impressive results in the recent years, to help solve
many design problems that are traditionally done by humans. Going beyond image
generation, we aim to find solutions to more general design problems, in which
both the diversity of the design and conformity of constraints are important.
Such a setting has applications in computer graphics, animation, industrial
design, material science, etc, in which we may want the output of the generator
to follow discrete/combinatorial constraints and penalize any deviation, which
is non-trivial with existing generative models and optimization solvers. To
address this, we propose GenCO, a novel framework that conducts end-to-end
training of deep generative models integrated with embedded combinatorial
solvers, aiming to uncover high-quality solutions aligned with nonlinear
objectives. While structurally akin to conventional generative models, GenCO
diverges in its role - it focuses on generating instances of combinatorial
optimization problems rather than final objects (e.g., images). This shift
allows finer control over the generated outputs, enabling assessments of their
feasibility and introducing an additional combinatorial loss component. We
demonstrate the effectiveness of our approach on a variety of generative tasks
characterized by combinatorial intricacies, including game level generation and
map creation for path planning, consistently demonstrating its capability to
yield diverse, high-quality solutions that reliably adhere to user-specified
combinatorial properties.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02446" title="Abstract">arXiv:2310.02446</a> [<a href="/pdf/2310.02446" title="Download PDF">pdf</a>, <a href="/format/2310.02446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Resource Languages Jailbreak GPT-4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yong%2C+Z">Zheng-Xin Yong</a>, 
<a href="/search/cs?searchtype=author&query=Menghini%2C+C">Cristina Menghini</a>, 
<a href="/search/cs?searchtype=author&query=Bach%2C+S+H">Stephen H. Bach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">AI safety training and red-teaming of large language models (LLMs) are
measures to mitigate the generation of unsafe content. Our work exposes the
inherent cross-lingual vulnerability of these safety mechanisms, resulting from
the linguistic inequality of safety training data, by successfully
circumventing GPT-4's safeguard through translating unsafe English inputs into
low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe
translated inputs and provides actionable items that can get the users towards
their harmful goals 79% of the time, which is on par with or even surpassing
state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have
significantly lower attack success rate, which suggests that the cross-lingual
vulnerability mainly applies to low-resource languages. Previously, limited
training on low-resource languages primarily affects speakers of those
languages, causing technological disparities. However, our work highlights a
crucial shift: this deficiency now poses a risk to all LLMs users. Publicly
available translation APIs enable anyone to exploit LLMs' safety
vulnerabilities. Therefore, our work calls for a more holistic red-teaming
efforts to develop robust multilingual safeguards with wide language coverage.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02447" title="Abstract">arXiv:2310.02447</a> [<a href="/pdf/2310.02447" title="Download PDF">pdf</a>, <a href="/format/2310.02447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine learning assist nyc subway navigation safer and faster
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+W">Wencheng Bao</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shi Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Mainstream navigation software, like Google and Apple Maps, often lacks the
ability to provide routes prioritizing safety. However, safety remains a
paramount concern for many. Our aim is to strike a balance between safety and
efficiency. To achieve this, we're devising an Integer Programming model that
takes into account both the shortest path and the safest route. We will harness
machine learning to derive safety coefficients, employing methodologies such as
generalized linear models, linear regression, and recurrent neural networks.
Our evaluation will be based on the Root Mean Square Error (RMSE) across
various subway stations, helping us identify the most accurate model for safety
coefficient estimation. Furthermore, we'll conduct a comprehensive review of
different shortest-path algorithms, assessing them based on time complexity and
real-world data to determine their appropriateness in merging both safety and
time efficiency.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02448" title="Abstract">arXiv:2310.02448</a> [<a href="/pdf/2310.02448" title="Download PDF">pdf</a>, <a href="/format/2310.02448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feather: An Elegant Solution to Effective DNN Sparsification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Georgoulakis%2C+A+G">Athanasios Glentis Georgoulakis</a>, 
<a href="/search/cs?searchtype=author&query=Retsinas%2C+G">George Retsinas</a>, 
<a href="/search/cs?searchtype=author&query=Maragos%2C+P">Petros Maragos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Neural Network pruning is an increasingly popular way for producing compact
and efficient models, suitable for resource-limited environments, while
preserving high performance. While the pruning can be performed using a
multi-cycle training and fine-tuning process, the recent trend is to encompass
the sparsification process during the standard course of training. To this end,
we introduce Feather, an efficient sparse training module utilizing the
powerful Straight-Through Estimator as its core, coupled with a new
thresholding operator and a gradient scaling technique, enabling robust,
out-of-the-box sparsification performance. Feather's effectiveness and
adaptability is demonstrated using various architectures on the CIFAR dataset,
while on ImageNet it achieves state-of-the-art Top-1 validation accuracy using
the ResNet-50 architecture, surpassing existing methods, including more complex
and computationally heavy ones, by a considerable margin. Code is publicly
available at https://github.com/athglentis/feather .
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02449" title="Abstract">arXiv:2310.02449</a> [<a href="/pdf/2310.02449" title="Download PDF">pdf</a>, <a href="/format/2310.02449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of geography on the importance of parameters in infectious  disease models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+A">Arindam Saha</a>, 
<a href="/search/cs?searchtype=author&query=Ghorbani%2C+M">Maziar Ghorbani</a>, 
<a href="/search/cs?searchtype=author&query=Suleimenova%2C+D">Diana Suleimenova</a>, 
<a href="/search/cs?searchtype=author&query=Anagnostou%2C+A">Anastasia Anagnostou</a>, 
<a href="/search/cs?searchtype=author&query=Groen%2C+D">Derek Groen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Agent-based models are widely used to predict infectious disease spread. For
these predictions, one needs to understand how each input parameter affects the
result. Here, some parameters may affect the sensitivities of others, requiring
the analysis of higher order coefficients through e.g. Sobol sensitivity
analysis. The geographical structures of real-world regions are distinct in
that they are difficult to reduce to single parameter values, making a unified
sensitivity analysis intractable. Yet analyzing the importance of geographical
structure on the sensitivity of other input parameters is important because a
strong effect would justify the use of models with real-world geographical
representations, as opposed to stylized ones.
<br />Here we perform a grouped Sobol's sensitivity analysis on COVID-19 spread
simulations across a set of three diverse real-world geographical
representations. We study the differences in both results and the sensitivity
of non-geographical parameters across these geographies. By comparing Sobol
indices of parameters across geographies, we find evidence that infection rate
could have more sensitivity in regions where the population is segregated,
while parameters like recovery period of mild cases are more sensitive in
regions with mixed populations. We also show how geographical structure affects
parameter sensitivity changes over time.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02451" title="Abstract">arXiv:2310.02451</a> [<a href="/pdf/2310.02451" title="Download PDF">pdf</a>, <a href="/format/2310.02451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Backdoor Adjustment of Confounding by Provenance for Robust Text  Classification of Multi-institutional Clinical Notes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xiruo Ding</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Z">Zhecheng Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Yeti%C5%9Fgen%2C+M">Meliha Yeti&#x15f;gen</a>, 
<a href="/search/cs?searchtype=author&query=Pakhomov%2C+S">Serguei Pakhomov</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+T">Trevor Cohen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in AMIA 2023 Annual Symposium
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Natural Language Processing (NLP) methods have been broadly applied to
clinical tasks. Machine learning and deep learning approaches have been used to
improve the performance of clinical NLP. However, these approaches require
sufficiently large datasets for training, and trained models have been shown to
transfer poorly across sites. These issues have led to the promotion of data
collection and integration across different institutions for accurate and
portable models. However, this can introduce a form of bias called confounding
by provenance. When source-specific data distributions differ at deployment,
this may harm model performance. To address this issue, we evaluate the utility
of backdoor adjustment for text classification in a multi-site dataset of
clinical notes annotated for mentions of substance abuse. Using an evaluation
framework devised to measure robustness to distributional shifts, we assess the
utility of backdoor adjustment. Our results indicate that backdoor adjustment
can effectively mitigate for confounding shift.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02453" title="Abstract">arXiv:2310.02453</a> [<a href="/pdf/2310.02453" title="Download PDF">pdf</a>, <a href="/format/2310.02453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual-stage Flows-based Generative Modeling for Traceable Urban Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xuanming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wei Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dongjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pengyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yanjie Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Urban planning, which aims to design feasible land-use configurations for
target areas, has become increasingly essential due to the high-speed
urbanization process in the modern era. However, the traditional urban planning
conducted by human designers can be a complex and onerous task. Thanks to the
advancement of deep learning algorithms, researchers have started to develop
automated planning techniques. While these models have exhibited promising
results, they still grapple with a couple of unresolved limitations: 1)
Ignoring the relationship between urban functional zones and configurations and
failing to capture the relationship among different functional zones. 2) Less
interpretable and stable generation process. To overcome these limitations, we
propose a novel generative framework based on normalizing flows, namely
Dual-stage Urban Flows (DSUF) framework. Specifically, the first stage is to
utilize zone-level urban planning flows to generate urban functional zones
based on given surrounding contexts and human guidance. Then we employ an
Information Fusion Module to capture the relationship among functional zones
and fuse the information of different aspects. The second stage is to use
configuration-level urban planning flows to obtain land-use configurations
derived from fused information. We design several experiments to indicate that
our framework can outperform compared to other generative models for the urban
planning task.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02456" title="Abstract">arXiv:2310.02456</a> [<a href="/pdf/2310.02456" title="Download PDF">pdf</a>, <a href="/format/2310.02456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Optimal Advantage from Preferences and Mistaking it for Reward
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Knox%2C+W+B">W. Bradley Knox</a>, 
<a href="/search/cs?searchtype=author&query=Hatgis-Kessell%2C+S">Stephane Hatgis-Kessell</a>, 
<a href="/search/cs?searchtype=author&query=Adalgeirsson%2C+S+O">Sigurdur Orn Adalgeirsson</a>, 
<a href="/search/cs?searchtype=author&query=Booth%2C+S">Serena Booth</a>, 
<a href="/search/cs?searchtype=author&query=Dragan%2C+A">Anca Dragan</a>, 
<a href="/search/cs?searchtype=author&query=Stone%2C+P">Peter Stone</a>, 
<a href="/search/cs?searchtype=author&query=Niekum%2C+S">Scott Niekum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages (16 pages with references and appendix), 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We consider algorithms for learning reward functions from human preferences
over pairs of trajectory segments, as used in reinforcement learning from human
feedback (RLHF). Most recent work assumes that human preferences are generated
based only upon the reward accrued within those segments, or their partial
return. Recent work casts doubt on the validity of this assumption, proposing
an alternative preference model based upon regret. We investigate the
consequences of assuming preferences are based upon partial return when they
actually arise from regret. We argue that the learned function is an
approximation of the optimal advantage function, $\hat{A^*_r}$, not a reward
function. We find that if a specific pitfall is addressed, this incorrect
assumption is not particularly harmful, resulting in a highly shaped reward
function. Nonetheless, this incorrect usage of $\hat{A^*_r}$ is less desirable
than the appropriate and simpler approach of greedy maximization of
$\hat{A^*_r}$. From the perspective of the regret preference model, we also
provide a clearer interpretation of fine tuning contemporary large language
models with RLHF. This paper overall provides insight regarding why learning
under the partial return preference model tends to work so well in practice,
despite it conforming poorly to how humans give preferences.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02457" title="Abstract">arXiv:2310.02457</a> [<a href="/pdf/2310.02457" title="Download PDF">pdf</a>, <a href="/format/2310.02457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Empty Signifier Problem: Towards Clearer Paradigms for  Operationalising &quot;Alignment&quot; in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kirk%2C+H+R">Hannah Rose Kirk</a>, 
<a href="/search/cs?searchtype=author&query=Vidgen%2C+B">Bertie Vidgen</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%B6ttger%2C+P">Paul R&#xf6;ttger</a>, 
<a href="/search/cs?searchtype=author&query=Hale%2C+S+A">Scott A. Hale</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">In this paper, we address the concept of "alignment" in large language models
(LLMs) through the lens of post-structuralist socio-political theory,
specifically examining its parallels to empty signifiers. To establish a shared
vocabulary around how abstract concepts of alignment are operationalised in
empirical datasets, we propose a framework that demarcates: 1) which dimensions
of model behaviour are considered important, then 2) how meanings and
definitions are ascribed to these dimensions, and by whom. We situate existing
empirical literature and provide guidance on deciding which paradigm to follow.
Through this framework, we aim to foster a culture of transparency and critical
evaluation, aiding the community in navigating the complexities of aligning
LLMs with human populations.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02459" title="Abstract">arXiv:2310.02459</a> [<a href="/pdf/2310.02459" title="Download PDF">pdf</a>, <a href="/format/2310.02459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributionally Safe Reinforcement Learning under Model Uncertainty: A  Single-Level Approach by Differentiable Convex Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chriat%2C+A+E">Alaa Eddine Chriat</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chuangchuang Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO); Systems and Control (eess.SY)

</div>
<p class="mathjax">Safety assurance is uncompromisable for safety-critical environments with the
presence of drastic model uncertainties (e.g., distributional shift),
especially with humans in the loop. However, incorporating uncertainty in safe
learning will naturally lead to a bi-level problem, where at the lower level
the (worst-case) safety constraint is evaluated within the uncertainty
ambiguity set. In this paper, we present a tractable distributionally safe
reinforcement learning framework to enforce safety under a distributional shift
measured by a Wasserstein metric. To improve the tractability, we first use
duality theory to transform the lower-level optimization from
infinite-dimensional probability space where distributional shift is measured,
to a finite-dimensional parametric space. Moreover, by differentiable convex
programming, the bi-level safe learning problem is further reduced to a
single-level one with two sequential computationally efficient modules: a
convex quadratic program to guarantee safety followed by a projected gradient
ascent to simultaneously find the worst-case uncertainty. This end-to-end
differentiable framework with safety constraints, to the best of our knowledge,
is the first tractable single-level solution to address distributional safety.
We test our approach on first and second-order systems with varying
complexities and compare our results with the uncertainty-agnostic policies,
where our approach demonstrates a significant improvement on safety guarantees.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02462" title="Abstract">arXiv:2310.02462</a> [<a href="/pdf/2310.02462" title="Download PDF">pdf</a>, <a href="/format/2310.02462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Inference of Human Intent by Combining Plan Recognition and  Language Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Idrees%2C+I">Ifrah Idrees</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+T">Tian Yun</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+N">Naveen Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yunxin Deng</a>, 
<a href="/search/cs?searchtype=author&query=Gopalan%2C+N">Nakul Gopalan</a>, 
<a href="/search/cs?searchtype=author&query=Konidaris%2C+G">George Konidaris</a>, 
<a href="/search/cs?searchtype=author&query=Tellex%2C+S">Stefanie Tellex</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in IROS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Conversational assistive robots can aid people, especially those with
cognitive impairments, to accomplish various tasks such as cooking meals,
performing exercises, or operating machines. However, to interact with people
effectively, robots must recognize human plans and goals from noisy
observations of human actions, even when the user acts sub-optimally. Previous
works on Plan and Goal Recognition (PGR) as planning have used hierarchical
task networks (HTN) to model the actor/human. However, these techniques are
insufficient as they do not have user engagement via natural modes of
interaction such as language. Moreover, they have no mechanisms to let users,
especially those with cognitive impairments, know of a deviation from their
original plan or about any sub-optimal actions taken towards their goal. We
propose a novel framework for plan and goal recognition in partially observable
domains -- Dialogue for Goal Recognition (D4GR) enabling a robot to rectify its
belief in human progress by asking clarification questions about noisy sensor
data and sub-optimal human actions. We evaluate the performance of D4GR over
two simulated domains -- kitchen and blocks domain. With language feedback and
the world state information in a hierarchical task model, we show that D4GR
framework for the highest sensor noise performs 1% better than HTN in goal
accuracy in both domains. For plan accuracy, D4GR outperforms by 4% in the
kitchen domain and 2% in the blocks domain in comparison to HTN. The ALWAYS-ASK
oracle outperforms our policy by 3% in goal recognition and 7%in plan
recognition. D4GR does so by asking 68% fewer questions than an oracle
baseline. We also demonstrate a real-world robot scenario in the kitchen
domain, validating the improved plan and goal recognition of D4GR in a
realistic setting.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02466" title="Abstract">arXiv:2310.02466</a> [<a href="/pdf/2310.02466" title="Download PDF">pdf</a>, <a href="/format/2310.02466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameterized Model-checking of Discrete-Timed Networks and  Symmetric-Broadcast Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aminof%2C+B">Benjamin Aminof</a>, 
<a href="/search/cs?searchtype=author&query=Rubin%2C+S">Sasha Rubin</a>, 
<a href="/search/cs?searchtype=author&query=Spegni%2C+F">Francesco Spegni</a>, 
<a href="/search/cs?searchtype=author&query=Zuleger%2C+F">Florian Zuleger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We study the complexity of the model-checking problem for discrete-timed
systems with arbitrarily many anonymous and identical contributors, with and
without a distinguished "controller" process, communicating via synchronous
rendezvous. Our work extends the seminal work on untimed systems by German and
Sistla adding discrete-time clocks, thus allowing one to model more realistic
protocols. For the case without a controller, we show that the systems can be
efficiently simulated -- and vice versa -- by systems of untimed processes
communicating via rendezvous and symmetric broadcast, which we call
"RB-systems". Symmetric broadcast is a novel communication primitive that, like
ordinary asymmetric broadcast, allows all processes to synchronize without
distinction between sender/receiver processes. We show that the complexity of
the parameterized model-checking problem for safety specifications is
pspace-complete, and for liveness specifications it is decidable in exptime.
The latter result required automata theory, rational linear programming, and
geometric reasoning for solving certain reachability questions in a new variant
of vector addition systems called "vector rendezvous systems". We believe such
proof techniques are of independent interest and will be useful in solving
related problems. For the case with a controller, we show that the
parameterized model-checking problems for RB-systems and systems with
asymmetric broadcast are inter-reducible. This implies that for discrete
timed-networks with a controller the parameterized model-checking problem is
undecidable for liveness specifications. Our work exploits the intimate
connection between discrete-timed systems and systems of processes
communicating via broadcast. This allows us to prove decidability results for
liveness properties of parameterized timed-systems, as well as extend work from
untimed systems to timed systems.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02469" title="Abstract">arXiv:2310.02469</a> [<a href="/pdf/2310.02469" title="Download PDF">pdf</a>, <a href="/format/2310.02469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Can Be Good Privacy Protection Learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yijia Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yiqiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yushi Bai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xianjun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenchao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xujiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanchi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 4 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The proliferation of Large Language Models (LLMs) has driven considerable
interest in fine-tuning them with domain-specific data to create specialized
language models. Nevertheless, such domain-specific fine-tuning data often
contains sensitive personally identifiable information (PII). Direct
fine-tuning LLMs on this data without privacy protection poses a risk of
leakage. To address this challenge, we introduce Privacy Protection Language
Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects
domain-specific knowledge while safeguarding data privacy. Our work offers a
theoretical analysis for model design and delves into various techniques such
as corpus curation, penalty-based unlikelihood in training loss, and
instruction-based tuning, etc. Extensive experiments across diverse datasets
and scenarios demonstrate the effectiveness of our approaches. In particular,
instruction tuning with both positive and negative examples, stands out as a
promising method, effectively protecting private data while enhancing the
model's knowledge. Our work underscores the potential for Large Language Models
as robust privacy protection learners.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02470" title="Abstract">arXiv:2310.02470</a> [<a href="/pdf/2310.02470" title="Download PDF">pdf</a>, <a href="/format/2310.02470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SGORP: A Subgradient-based Method for d-Dimensional Rectilinear  Partitioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balin%2C+M+F">Muhammed Fatih Balin</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+X">Xiaojing An</a>, 
<a href="/search/cs?searchtype=author&query=Ya%C5%9Far%2C+A">Abdurrahman Ya&#x15f;ar</a>, 
<a href="/search/cs?searchtype=author&query=%C3%87ataly%C3%BCrek%2C+%C3%9C+V">&#xdc;mit V. &#xc7;ataly&#xfc;rek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Partitioning for load balancing is a crucial first step to parallelize any
type of computation. In this work, we propose SGORP, a new spatial partitioning
method based on Subgradient Optimization, to solve the $d$-dimensional
Rectilinear Partitioning Problem (RPP). Our proposed method allows the use of
customizable objective functions as well as some user-specific constraints,
such as symmetric partitioning on selected dimensions. Extensive experimental
evaluation using over 600 test matrices shows that our algorithm achieves
favorable performance against the state-of-the-art RPP and Symmetric RPP
algorithms. Additionally, we show the effectiveness of our algorithm to do
application-specific load balancing using two applications as motivation:
Triangle Counting and Sparse Matrix Multiplication (SpGEMM), where we model
their load-balancing problems as $3$-dimensional RPPs.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02473" title="Abstract">arXiv:2310.02473</a> [<a href="/pdf/2310.02473" title="Download PDF">pdf</a>, <a href="/format/2310.02473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting-based Efficient Temporal Domain Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+S">Sepidehsadat Hosseini</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+M">Mengyao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Hajimirsadegh%2C+H">Hossein Hajimirsadegh</a>, 
<a href="/search/cs?searchtype=author&query=Tung%2C+F">Frederick Tung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Machine learning traditionally assumes that training and testing data are
distributed independently and identically. However, in many real-world
settings, the data distribution can shift over time, leading to poor
generalization of trained models in future time periods. Our paper presents a
novel prompting-based approach to temporal domain generalization that is
parameter-efficient, time-efficient, and does not require access to the target
domain data (i.e., unseen future time periods) during training. Our method
adapts a target pre-trained model to temporal drift by learning global prompts,
domain-specific prompts, and drift-aware prompts that capture underlying
temporal dynamics. It is compatible across diverse tasks, such as
classification, regression, and time series forecasting, and sets a new
state-of-the-art benchmark in temporal domain generalization. The code
repository will be publicly shared.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02476" title="Abstract">arXiv:2310.02476</a> [<a href="/pdf/2310.02476" title="Download PDF">pdf</a>, <a href="/ps/2310.02476" title="Download PostScript">ps</a>, <a href="/format/2310.02476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ML4EJ: Decoding the Role of Urban Features in Shaping Environmental  Injustice Using Interpretable Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ho%2C+Y">Yu-Hsuan Ho</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhewei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Cheng-Chun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Mostafavi%2C+A">Ali Mostafavi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Understanding the key factors shaping environmental hazard exposures and
their associated environmental injustice issues is vital for formulating
equitable policy measures. Traditional perspectives on environmental injustice
have primarily focused on the socioeconomic dimensions, often overlooking the
influence of heterogeneous urban characteristics. This limited view may
obstruct a comprehensive understanding of the complex nature of environmental
justice and its relationship with urban design features. To address this gap,
this study creates an interpretable machine learning model to examine the
effects of various urban features and their non-linear interactions to the
exposure disparities of three primary hazards: air pollution, urban heat, and
flooding. The analysis trains and tests models with data from six metropolitan
counties in the United States using Random Forest and XGBoost. The performance
is used to measure the extent to which variations of urban features shape
disparities in environmental hazard levels. In addition, the analysis of
feature importance reveals features related to social-demographic
characteristics as the most prominent urban features that shape hazard extent.
Features related to infrastructure distribution and land cover are relatively
important for urban heat and air pollution exposure respectively. Moreover, we
evaluate the models' transferability across different regions and hazards. The
results highlight limited transferability, underscoring the intricate
differences among hazards and regions and the way in which urban features shape
hazard exposures. The insights gleaned from this study offer fresh perspectives
on the relationship among urban features and their interplay with environmental
hazard exposure disparities, informing the development of more integrated urban
design policies to enhance social equity and environmental injustice issues.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02477" title="Abstract">arXiv:2310.02477</a> [<a href="/pdf/2310.02477" title="Download PDF">pdf</a>, <a href="/format/2310.02477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-Like Autonomous Driving on Dense Traffic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yildirim%2C+M">Mustafa Yildirim</a>, 
<a href="/search/cs?searchtype=author&query=Fallah%2C+S">Saber Fallah</a>, 
<a href="/search/cs?searchtype=author&query=Tamaddoni-Nezhad%2C+A">Alireza Tamaddoni-Nezhad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper proposes a imitation learning model for autonomous driving on
highway traffic by mimicking human drivers' driving behaviours. The study
utilizes the HighD traffic dataset, which is complex, high-dimensional, and
diverse in vehicle variations. Imitation learning is an alternative solution to
autonomous highway driving that reduces the sample complexity of learning a
challenging task compared to reinforcement learning. However, imitation
learning has limitations such as vulnerability to compounding errors in unseen
states, poor generalization, and inability to predict outlier driver profiles.
To address these issues, the paper proposes mixture density network behaviour
cloning model to manage complex and non-linear relationships between inputs and
outputs and make more informed decisions about the vehicle's actions.
Additional improvement is using collision penalty based on the GAIL model. The
paper includes a simulated driving test to demonstrate the effectiveness of the
proposed method based on real traffic scenarios and provides conclusions on its
potential impact on autonomous driving.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02480" title="Abstract">arXiv:2310.02480</a> [<a href="/pdf/2310.02480" title="Download PDF">pdf</a>, <a href="/format/2310.02480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Splitting the Difference on Adversarial Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levi%2C+M">Matan Levi</a>, 
<a href="/search/cs?searchtype=author&query=Kontorovich%2C+A">Aryeh Kontorovich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The existence of adversarial examples points to a basic weakness of deep
neural networks. One of the most effective defenses against such examples,
adversarial training, entails training models with some degree of robustness,
usually at the expense of a degraded natural accuracy. Most adversarial
training methods aim to learn a model that finds, for each class, a common
decision boundary encompassing both the clean and perturbed examples. In this
work, we take a fundamentally different approach by treating the perturbed
examples of each class as a separate class to be learned, effectively splitting
each class into two classes: "clean" and "adversarial." This split doubles the
number of classes to be learned, but at the same time considerably simplifies
the decision boundaries. We provide a theoretical plausibility argument that
sheds some light on the conditions under which our approach can be expected to
be beneficial. Likewise, we empirically demonstrate that our method learns
robust models while attaining optimal or near-optimal natural accuracy, e.g.,
on CIFAR-10 we obtain near-optimal natural accuracy of $95.01\%$ alongside
significant robustness across multiple tasks. The ability to achieve such
near-optimal natural accuracy, while maintaining a significant level of
robustness, makes our method applicable to real-world applications where
natural accuracy is at a premium. As a whole, our main contribution is a
general method that confers a significant level of robustness upon classifiers
with only minor or negligible degradation of their natural accuracy.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02485" title="Abstract">arXiv:2310.02485</a> [<a href="/pdf/2310.02485" title="Download PDF">pdf</a>, <a href="/format/2310.02485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computationally Efficient Chance Constrained Covariance Control with  Output Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pilipovsky%2C+J">Joshua Pilipovsky</a>, 
<a href="/search/eess?searchtype=author&query=Tsiotras%2C+P">Panagiotis Tsiotras</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper studies the problem of developing computationally efficient
solutions for steering the distribution of the state of a stochastic, linear
dynamical system between two boundary Gaussian distributions in the presence of
chance-constraints on the state and control input. It is assumed that the state
is only partially available through a measurement model corrupted with noise.
The filtered state is reconstructed with a Kalman filter, the chance
constraints are reformulated as difference of convex (DC) constraints, and the
resulting covariance control problem is reformulated as a DC program, which is
solved using successive convexification. The efficiency of the proposed method
is illustrated on a double integrator example with varying time horizons, and
is compared to other state-of-the-art chance constrained covariance control
methods.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02489" title="Abstract">arXiv:2310.02489</a> [<a href="/pdf/2310.02489" title="Download PDF">pdf</a>, <a href="/format/2310.02489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ResidualTransformer: Residual Low-rank Learning with Weight-sharing for  Transformer Layers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyu Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE ICASSP 2024. 5 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Memory constraint of always-on devices is one of the major concerns when
deploying speech processing models on these devices. While larger models
trained with sufficiently large amount of data generally perform better, making
them fit in the device memory is a demanding challenge. In this paper, we aim
to reduce model size by reparameterizing model weights across Transformer
encoder layers and assuming a special weight composition and structure. More
specifically, inspired by ResNet and the more recent LoRA work, we propose an
approach named ResidualTransformer, where each weight matrix in a Transformer
layer comprises 1) a shared full-rank component with its adjacent layers, and
2) a unique low-rank component to itself. The low-rank matrices only account
for a small amount of model size increase. In addition, we add diagonal weight
matrices to improve modeling capacity of the low-rank matrices. Experiments of
our 10k-hour speech recognition and speech translation tasks show that the
Transformer encoder size can be reduced by ~3X with very slight performance
degradation.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02491" title="Abstract">arXiv:2310.02491</a> [<a href="/pdf/2310.02491" title="Download PDF">pdf</a>, <a href="/format/2310.02491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DON-LSTM: Multi-Resolution Learning with DeepONets and Long Short-Term  Memory Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Micha%C5%82owska%2C+K">Katarzyna Micha&#x142;owska</a>, 
<a href="/search/cs?searchtype=author&query=Goswami%2C+S">Somdatta Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Karniadakis%2C+G+E">George Em Karniadakis</a>, 
<a href="/search/cs?searchtype=author&query=Riemer-S%C3%B8rensen%2C+S">Signe Riemer-S&#xf8;rensen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Deep operator networks (DeepONets, DONs) offer a distinct advantage over
traditional neural networks in their ability to be trained on multi-resolution
data. This property becomes especially relevant in real-world scenarios where
high-resolution measurements are difficult to obtain, while low-resolution data
is more readily available. Nevertheless, DeepONets alone often struggle to
capture and maintain dependencies over long sequences compared to other
state-of-the-art algorithms. We propose a novel architecture, named DON-LSTM,
which extends the DeepONet with a long short-term memory network (LSTM).
Combining these two architectures, we equip the network with explicit
mechanisms to leverage multi-resolution data, as well as capture temporal
dependencies in long sequences. We test our method on long-time-evolution
modeling of multiple non-linear systems and show that the proposed
multi-resolution DON-LSTM achieves significantly lower generalization error and
requires fewer high-resolution samples compared to its vanilla counterparts.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02492" title="Abstract">arXiv:2310.02492</a> [<a href="/pdf/2310.02492" title="Download PDF">pdf</a>, <a href="/format/2310.02492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eye Fairness: A Large-Scale 3D Imaging Dataset for Equitable Eye  Diseases Screening and Fair Identity Scaling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+M">Min Shi</a>, 
<a href="/search/cs?searchtype=author&query=Elze%2C+T">Tobias Elze</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengyu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Fairness or equity in machine learning is profoundly important for societal
well-being, but limited public datasets hinder its progress, especially in the
area of medicine. It is undeniable that fairness in medicine is one of the most
important areas for fairness learning's applications. Currently, no large-scale
public medical datasets with 3D imaging data for fairness learning are
available, while 3D imaging data in modern clinics are standard tests for
disease diagnosis. In addition, existing medical fairness datasets are actually
repurposed datasets, and therefore they typically have limited demographic
identity attributes with at most three identity attributes of age, gender, and
race for fairness modeling. To address this gap, we introduce our Eye Fairness
dataset with 30,000 subjects (Harvard-EF) covering three major eye diseases
including age-related macular degeneration, diabetic retinopathy, and glaucoma
affecting 380 million patients globally. Our Harvard-EF dataset includes both
2D fundus photos and 3D optical coherence tomography scans with six demographic
identity attributes including age, gender, race, ethnicity, preferred language,
and marital status. We also propose a fair identity scaling (FIS) approach
combining group and individual scaling together to improve model fairness. Our
FIS approach is compared with various state-of-the-art fairness learning
methods with superior performance in the racial, gender, and ethnicity fairness
tasks with 2D and 3D imaging data, which demonstrate the utilities of our
Harvard-EF dataset for fairness learning. To facilitate fairness comparisons
between different models, we propose performance-scaled disparity measures,
which can be used to compare model fairness accounting for overall performance
levels. The dataset and code are publicly accessible via
\url{https://ophai.hms.harvard.edu/datasets/harvard-ef30k}.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02494" title="Abstract">arXiv:2310.02494</a> [<a href="/pdf/2310.02494" title="Download PDF">pdf</a>, <a href="/format/2310.02494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Financial Consequences of Simplified Battery Sizing Models  without Considering Operational Details
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dinh%2C+N+T">Nam Trong Dinh</a>, 
<a href="/search/eess?searchtype=author&query=Karimi-Arpanahi%2C+S">Sahand Karimi-Arpanahi</a>, 
<a href="/search/eess?searchtype=author&query=Pourmousavi%2C+S+A">S. Ali Pourmousavi</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+M">Mingyu Guo</a>, 
<a href="/search/eess?searchtype=author&query=Lemos-Vinasco%2C+J">Julian Lemos-Vinasco</a>, 
<a href="/search/eess?searchtype=author&query=Liisberg%2C+J+A+R">Jon A. R. Liisberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript has been submitted to PSCC 2024 for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Optimal battery sizing studies tend to overly simplify the practical aspects
of battery operation within the battery sizing framework. Such assumptions may
lead to a suboptimal battery capacity, resulting in significant financial
losses for a battery project that could last more than a decade. In this paper,
we compare the most common existing sizing methods in the literature with a
battery sizing model that incorporates the practical operation of a battery,
that is, receding horizon operation. Consequently, we quantify the financial
losses caused by the suboptimal capacities obtained by these models for a
realistic case study related to community battery storage (CBS). We develop the
case study by constructing a mathematical framework for the CBS and local end
users. Our results show that existing sizing methods can lead to financial
losses of up to 22%.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02497" title="Abstract">arXiv:2310.02497</a> [<a href="/pdf/2310.02497" title="Download PDF">pdf</a>, <a href="/format/2310.02497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards an Interpretable Representation of Speaker Identity via  Perceptual Voice Qualities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Netzorg%2C+R">Robin Netzorg</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bohan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Guzman%2C+A">Andrea Guzman</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Peter Wu</a>, 
<a href="/search/cs?searchtype=author&query=McNulty%2C+L">Luna McNulty</a>, 
<a href="/search/cs?searchtype=author&query=Anumanchipalli%2C+G">Gopala Anumanchipalli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Unlike other data modalities such as text and vision, speech does not lend
itself to easy interpretation. While lay people can understand how to describe
an image or sentence via perception, non-expert descriptions of speech often
end at high-level demographic information, such as gender or age. In this
paper, we propose a possible interpretable representation of speaker identity
based on perceptual voice qualities (PQs). By adding gendered PQs to the
pathology-focused Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V)
protocol, our PQ-based approach provides a perceptual latent space of the
character of adult voices that is an intermediary of abstraction between
high-level demographics and low-level acoustic, physical, or learned
representations. Contrary to prior belief, we demonstrate that these PQs are
hearable by ensembles of non-experts, and further demonstrate that the
information encoded in a PQ-based representation is predictable by various
speech representations.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02505" title="Abstract">arXiv:2310.02505</a> [<a href="/pdf/2310.02505" title="Download PDF">pdf</a>, <a href="/format/2310.02505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Reach Goals via Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+V">Vineet Jain</a>, 
<a href="/search/cs?searchtype=author&query=Ravanbakhsh%2C+S">Siamak Ravanbakhsh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Diffusion models are a powerful class of generative models capable of mapping
random noise in high-dimensional spaces to a target manifold through iterative
denoising. In this work, we present a novel perspective on goal-conditioned
reinforcement learning by framing it within the context of diffusion modeling.
Analogous to the diffusion process, where Gaussian noise is used to create
random trajectories that walk away from the data manifold, we construct
trajectories that move away from potential goal states. We then learn a
goal-conditioned policy analogous to the score function. This approach, which
we call Merlin, can reach predefined or novel goals from an arbitrary initial
state without learning a separate value function. We consider three choices for
the noise model to replace Gaussian noise in diffusion - reverse play from the
buffer, reverse dynamics model, and a novel non-parametric approach. We
theoretically justify our approach and validate it on offline goal-reaching
tasks. Empirical results are competitive with state-of-the-art methods, which
suggests this perspective on diffusion for RL is a simple, scalable, and
effective direction for sequential decision-making.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02506" title="Abstract">arXiv:2310.02506</a> [<a href="/pdf/2310.02506" title="Download PDF">pdf</a>, <a href="/format/2310.02506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proactive Human-Robot Interaction using Visuo-Lingual Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mathur%2C+P">Pranay Mathur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IROS'23 Workshop: Geriatronics: AI and Robotics for Health &amp; Well-Being in Older Age and Workshop: Assistive Robotics for Citizens
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Humans possess the innate ability to extract latent visuo-lingual cues to
infer context through human interaction. During collaboration, this enables
proactive prediction of the underlying intention of a series of tasks. In
contrast, robotic agents collaborating with humans naively follow elementary
instructions to complete tasks or use specific hand-crafted triggers to
initiate proactive collaboration when working towards the completion of a goal.
Endowing such robots with the ability to reason about the end goal and
proactively suggest intermediate tasks will engender a much more intuitive
method for human-robot collaboration. To this end, we propose a learning-based
method that uses visual cues from the scene, lingual commands from a user and
knowledge of prior object-object interaction to identify and proactively
predict the underlying goal the user intends to achieve. Specifically, we
propose ViLing-MMT, a vision-language multimodal transformer-based architecture
that captures inter and intra-modal dependencies to provide accurate scene
descriptions and proactively suggest tasks where applicable. We evaluate our
proposed model in simulation and real-world scenarios.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02508" title="Abstract">arXiv:2310.02508</a> [<a href="/pdf/2310.02508" title="Download PDF">pdf</a>, <a href="/format/2310.02508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ophiuchus: Scalable Modeling of Protein Structures through Hierarchical  Coarse-graining SO(3)-Equivariant Autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Costa%2C+A+d+S">Allan dos Santos Costa</a>, 
<a href="/search/cs?searchtype=author&query=Mitnikov%2C+I">Ilan Mitnikov</a>, 
<a href="/search/cs?searchtype=author&query=Geiger%2C+M">Mario Geiger</a>, 
<a href="/search/cs?searchtype=author&query=Ponnapati%2C+M">Manvitha Ponnapati</a>, 
<a href="/search/cs?searchtype=author&query=Smidt%2C+T">Tess Smidt</a>, 
<a href="/search/cs?searchtype=author&query=Jacobson%2C+J">Joseph Jacobson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Three-dimensional native states of natural proteins display recurring and
hierarchical patterns. Yet, traditional graph-based modeling of protein
structures is often limited to operate within a single fine-grained resolution,
and lacks hourglass neural architectures to learn those high-level building
blocks. We narrow this gap by introducing Ophiuchus, an SO(3)-equivariant
coarse-graining model that efficiently operates on all heavy atoms of standard
protein residues, while respecting their relevant symmetries. Our model departs
from current approaches that employ graph modeling, instead focusing on local
convolutional coarsening to model sequence-motif interactions in log-linear
length complexity. We train Ophiuchus on contiguous fragments of PDB monomers,
investigating its reconstruction capabilities across different compression
rates. We examine the learned latent space and demonstrate its prompt usage in
conformational interpolation, comparing interpolated trajectories to structure
snapshots from the PDBFlex dataset. Finally, we leverage denoising diffusion
probabilistic models (DDPM) to efficiently sample readily-decodable latent
embeddings of diverse miniproteins. Our experiments demonstrate Ophiuchus to be
a scalable basis for efficient protein modeling and generation.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02513" title="Abstract">arXiv:2310.02513</a> [<a href="/pdf/2310.02513" title="Download PDF">pdf</a>, <a href="/format/2310.02513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Recipe for Improved Certifiable Robustness: Capacity and Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+K">Kai Hu</a>, 
<a href="/search/cs?searchtype=author&query=Leino%2C+K">Klas Leino</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fredrikson%2C+M">Matt Fredrikson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A key challenge, supported both theoretically and empirically, is that
robustness demands greater network capacity and more data than standard
training. However, effectively adding capacity under stringent Lipschitz
constraints has proven more difficult than it may seem, evident by the fact
that state-of-the-art approach tend more towards \emph{underfitting} than
overfitting. Moreover, we posit that a lack of careful exploration of the
design space for Lipshitz-based approaches has left potential performance gains
on the table. In this work, we provide a more comprehensive evaluation to
better uncover the potential of Lipschitz-based certification methods. Using a
combination of novel techniques, design optimizations, and synthesis of prior
work, we are able to significantly improve the state-of-the-art \emph{verified
robust accuracy} (VRA) for deterministic certification on a variety of
benchmark datasets, and over a range of perturbation sizes. Of particular note,
we discover that the addition of large "Cholesky-orthogonalized residual dense"
layers to the end of existing state-of-the-art Lipschitz-controlled ResNet
architectures is especially effective for increasing network capacity and
performance. Combined with filtered generative data augmentation, our final
results further the state of the art deterministic VRA by up to 8.5 percentage
points. Code is available at \url{https://github.com/hukkai/liresnet}.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02515" title="Abstract">arXiv:2310.02515</a> [<a href="/pdf/2310.02515" title="Download PDF">pdf</a>, <a href="/format/2310.02515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Community Archetypes: An Empirical Framework for Guiding Research  Methodologies to Reflect User Experiences of Sense of Virtual Community
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prinster%2C+G+H">Gale H. Prinster</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+C+E">C. Estelle Smith</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Chenhao Tan</a>, 
<a href="/search/cs?searchtype=author&query=Keegan%2C+B+C">Brian C. Keegan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Humans need a sense of community (SOC), and social media platforms afford
opportunities to address this need by providing users with a sense of virtual
community (SOVC). This paper explores SOVC on Reddit and is motivated by two
goals: (1) providing researchers with an excellent resource for methodological
decisions in studies of Reddit communities; and (2) creating the foundation for
a new class of research methods and community support tools that reflect users'
experiences of SOVC. To ensure that methods are respectfully and ethically
designed in service and accountability to impacted communities, our work takes
a qualitative, community-centered approach by engaging with two key stakeholder
groups. First, we interviewed 21 researchers to understand how they study
"community" on Reddit. Second, we surveyed 12 subreddits to gain insight into
user experiences of SOVC. Results show that some research methods can broadly
reflect users' SOVC regardless of the topic or type of subreddit. However, user
responses also evidenced the existence of five distinct Community Archetypes:
Topical Q&amp;A, Learning &amp; Perspective Broadening, Social Support, Content
Generation, and Affiliation with an Entity. We offer the Community Archetypes
framework to support future work in designing methods that align more closely
with user experiences of SOVC and to create community support tools that can
meaningfully nourish the human need for SOC/SOVC in our modern world.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02518" title="Abstract">arXiv:2310.02518</a> [<a href="/pdf/2310.02518" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shaping the Epochal Individuality and Generality: The Temporal Dynamics  of Uncertainty and Prediction Error in Musical Improvisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daikoku%2C+T">Tatsuya Daikoku</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Information Retrieval (cs.IR); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Musical improvisation, much like spontaneous speech, reveals intricate facets
of the improviser's state of mind and emotional character. However, the
specific musical components that reveal such individuality remain largely
unexplored. Within the framework of brain's statistical learning and predictive
processing, this study examined the temporal dynamics of uncertainty and
surprise (prediction error) in a piece of musical improvisation. This study
employed the HBSL model to analyze a corpus of 456 Jazz improvisations,
spanning 1905 to 2009, from 78 distinct Jazz musicians. The results indicated
distinctive temporal patterns of surprise and uncertainty, especially in pitch
and pitch-rhythm sequences, revealing era-specific features from the early 20th
to the 21st centuries. Conversely, rhythm sequences exhibited a consistent
degree of uncertainty across eras. Further, the acoustic properties remain
unchanged across different periods. These findings highlight the importance of
how temporal dynamics of surprise and uncertainty in improvisational music
change over periods, profoundly influencing the distinctive methodologies
artists adopt for improvisation in each era. Further, it is suggested that the
development of improvisational music can be attributed to the brain's adaptive
statistical learning mechanisms, which constantly refine internal models to
mirror the cultural and emotional nuances of their respective epochs. This
study unravels the evolutionary trajectory of improvisational music and
highlights the nuanced shifts artists employ to resonate with the cultural and
emotional landscapes of their times.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02519" title="Abstract">arXiv:2310.02519</a> [<a href="/pdf/2310.02519" title="Download PDF">pdf</a>, <a href="/format/2310.02519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameterized Convex Minorant for Objective Function Approximation in  Amortized Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jinrae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Youdan Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Parameterized convex minorant (PCM) method is proposed for the approximation
of the objective function in amortized optimization. In the proposed method,
the objective function approximator is expressed by the sum of a PCM and a
nonnegative gap function, where the objective function approximator is bounded
from below by the PCM convex in the optimization variable. The proposed
objective function approximator is a universal approximator for continuous
functions, and the global minimizer of the PCM attains the global minimum of
the objective function approximator. Therefore, the global minimizer of the
objective function approximator can be obtained by a single convex
optimization. As a realization of the proposed method, extended parameterized
log-sum-exp network is proposed by utilizing a parameterized log-sum-exp
network as the PCM. Numerical simulation is performed for
non-parameterized-convex objective function approximation and for
learning-based nonlinear model predictive control to demonstrate the
performance and characteristics of the proposed method. The simulation results
support that the proposed method can be used to learn objective functions and
to find the global minimizer reliably and quickly by using convex optimization
algorithms.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02520" title="Abstract">arXiv:2310.02520</a> [<a href="/pdf/2310.02520" title="Download PDF">pdf</a>, <a href="/format/2310.02520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data  Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yuan Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+S">Suhan Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaochen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Ziyi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+H">Houping Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Huai%2C+M">Mengdi Huai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Ting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fenglong Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 main pages + 3 appendix pages , submitted to SDM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Health risk prediction is one of the fundamental tasks under predictive
modeling in the medical domain, which aims to forecast the potential health
risks that patients may face in the future using their historical Electronic
Health Records (EHR). Researchers have developed several risk prediction models
to handle the unique challenges of EHR data, such as its sequential nature,
high dimensionality, and inherent noise. These models have yielded impressive
results. Nonetheless, a key issue undermining their effectiveness is data
insufficiency. A variety of data generation and augmentation methods have been
introduced to mitigate this issue by expanding the size of the training data
set through the learning of underlying data distributions. However, the
performance of these methods is often limited due to their task-unrelated
design. To address these shortcomings, this paper introduces a novel,
end-to-end diffusion-based risk prediction model, named MedDiffusion. It
enhances risk prediction performance by creating synthetic patient data during
training to enlarge sample space. Furthermore, MedDiffusion discerns hidden
relationships between patient visits using a step-wise attention mechanism,
enabling the model to automatically retain the most vital information for
generating high-quality data. Experimental evaluation on four real-world
medical datasets demonstrates that MedDiffusion outperforms 14 cutting-edge
baselines in terms of PR-AUC, F1, and Cohen's Kappa. We also conduct ablation
studies and benchmark our model against GAN-based alternatives to further
validate the rationality and adaptability of our model design. Additionally, we
analyze generated data to offer fresh insights into the model's
interpretability.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02521" title="Abstract">arXiv:2310.02521</a> [<a href="/pdf/2310.02521" title="Download PDF">pdf</a>, <a href="/format/2310.02521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Who Audits the Auditors? Recommendations from a field scan of the  algorithmic auditing ecosystem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Costanza-Chock%2C+S">Sasha Costanza-Chock</a>, 
<a href="/search/cs?searchtype=author&query=Harvey%2C+E">Emma Harvey</a>, 
<a href="/search/cs?searchtype=author&query=Raji%2C+I+D">Inioluwa Deborah Raji</a>, 
<a href="/search/cs?searchtype=author&query=Czernuszenko%2C+M">Martha Czernuszenko</a>, 
<a href="/search/cs?searchtype=author&query=Buolamwini%2C+J">Joy Buolamwini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 2 figures. Published in the Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">AI audits are an increasingly popular mechanism for algorithmic
accountability; however, they remain poorly defined. Without a clear
understanding of audit practices, let alone widely used standards or regulatory
guidance, claims that an AI product or system has been audited, whether by
first-, second-, or third-party auditors, are difficult to verify and may
exacerbate, rather than mitigate, bias and harm. To address this knowledge gap,
we provide the first comprehensive field scan of the AI audit ecosystem. We
share a catalog of individuals (N=438) and organizations (N=189) who engage in
algorithmic audits or whose work is directly relevant to algorithmic audits;
conduct an anonymous survey of the group (N=152); and interview industry
leaders (N=10). We identify emerging best practices as well as methods and
tools that are becoming commonplace, and enumerate common barriers to
leveraging algorithmic audits as effective accountability mechanisms. We
outline policy recommendations to improve the quality and impact of these
audits, and highlight proposals with wide support from algorithmic auditors as
well as areas of debate. Our recommendations have implications for lawmakers,
regulators, internal company policymakers, and standards-setting bodies, as
well as for auditors. They are: 1) require the owners and operators of AI
systems to engage in independent algorithmic audits against clearly defined
standards; 2) notify individuals when they are subject to algorithmic
decision-making systems; 3) mandate disclosure of key components of audit
findings for peer review; 4) consider real-world harm in the audit process,
including through standardized harm incident reporting and response mechanisms;
5) directly involve the stakeholders most likely to be harmed by AI systems in
the algorithmic audit process; and 6) formalize evaluation and, potentially,
accreditation of algorithmic auditors.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02522" title="Abstract">arXiv:2310.02522</a> [<a href="/pdf/2310.02522" title="Download PDF">pdf</a>, <a href="/format/2310.02522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SCB-Dataset3: A Benchmark for Detecting Student Classroom Behavior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2304.02488">arXiv:2304.02488</a>, <a href="/abs/2306.03318">arXiv:2306.03318</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The use of deep learning methods to automatically detect students' classroom
behavior is a promising approach for analyzing their class performance and
improving teaching effectiveness. However, the lack of publicly available
datasets on student behavior poses a challenge for researchers in this field.
To address this issue, we propose the Student Classroom Behavior dataset
(SCB-dataset3), which represents real-life scenarios. Our dataset comprises
5686 images with 45578 labels, focusing on six behaviors: hand-raising,
reading, writing, using a phone, bowing the head, and leaning over the table.
We evaluated the dataset using the YOLOv5, YOLOv7, and YOLOv8 algorithms,
achieving a mean average precision (map) of up to 80.3$\%$. We believe that our
dataset can serve as a robust foundation for future research in student
behavior detection and contribute to advancements in this field. Our
SCB-dataset3 is available for download at:
https://github.com/Whiffe/SCB-dataset
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02523" title="Abstract">arXiv:2310.02523</a> [<a href="/pdf/2310.02523" title="Download PDF">pdf</a>, <a href="/format/2310.02523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Spatio-Temporal Attention-Based Method for Detecting Student Classroom  Behaviors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Accurately detecting student behavior from classroom videos is beneficial for
analyzing their classroom status and improving teaching efficiency. However,
low accuracy in student classroom behavior detection is a prevalent issue. To
address this issue, we propose a Spatio-Temporal Attention-Based Method for
Detecting Student Classroom Behaviors (BDSTA). Firstly, the SlowFast network is
used to generate motion and environmental information feature maps from the
video. Then, the spatio-temporal attention module is applied to the feature
maps, including information aggregation, compression and stimulation processes.
Subsequently, attention maps in the time, channel and space dimensions are
obtained, and multi-label behavior classification is performed based on these
attention maps. To solve the long-tail data problem that exists in student
classroom behavior datasets, we use an improved focal loss function to assign
more weight to the tail class data during training. Experimental results are
conducted on a self-made student classroom behavior dataset named STSCB.
Compared with the SlowFast model, the average accuracy of student behavior
classification detection improves by 8.94\% using BDSTA.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02524" title="Abstract">arXiv:2310.02524</a> [<a href="/pdf/2310.02524" title="Download PDF">pdf</a>, <a href="/format/2310.02524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Conditional Stochastic Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xidong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jianhui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhengmian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Aidong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heng Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Conditional stochastic optimization has found applications in a wide range of
machine learning tasks, such as invariant learning, AUPRC maximization, and
meta-learning. As the demand for training models with large-scale distributed
data grows in these applications, there is an increasing need for
communication-efficient distributed optimization algorithms, such as federated
learning algorithms. This paper considers the nonconvex conditional stochastic
optimization in federated learning and proposes the first federated conditional
stochastic optimization algorithm (FCSG) with a conditional stochastic gradient
estimator and a momentum-based algorithm (FCSG-M). To match the lower bound
complexity in the single-machine setting, we design an accelerated algorithm
(Acc-FCSG-M) via the variance reduction to achieve the best sample and
communication complexity. Compared with the existing optimization analysis for
MAML in FL, federated conditional stochastic optimization considers the sample
of tasks. Extensive experimental results on various tasks validate the
efficiency of these algorithms.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02527" title="Abstract">arXiv:2310.02527</a> [<a href="/pdf/2310.02527" title="Download PDF">pdf</a>, <a href="/format/2310.02527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CITING: Large Language Models Create Curriculum for Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Tao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jimeng Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The recent advancement of large language models (LLMs) has been achieved
through a combo of instruction tuning and human alignment. However, building
manually crafted instruction datasets and performing human alignment become the
bottleneck for scaling the development of LLMs. In this paper, we exploit the
idea of leveraging AI models in lieu of humans as the teacher to train student
LLMs. Our method is inspired by how human students refine their writing skills
by following the rubrics and learning from the revisions offered by their
tutors. Specifically, we employ a teacher LLM to create a curriculum for
instruction tuning of the student LLM, namely Curriculum Instruction TunING
(CITING). It encompasses two main steps: (1) the teacher LLM crafts the rubrics
for evaluating the answers corresponding to various types of questions, and (2)
the student LLM learns to follow the rubrics and perform self-correction from
the revision made by the teacher. We further iteratively carry out it to embody
the procedure of CITING. We compare CITING to a series of state-of-the-art
baselines on four datasets. Our method demonstrates strong improvement in terms
of articulate, in-depth, and comprehensive by GPT-4 evaluation. Specifically,
it achieves an average winning rate of 79.4% over SFT, 73.4% over RLHF, 78.1%
over RRHF, and 76.3% over RAFT, respectively.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02528" title="Abstract">arXiv:2310.02528</a> [<a href="/pdf/2310.02528" title="Download PDF">pdf</a>, <a href="/format/2310.02528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Cognition of Visual Question Answering Models and Human  Intelligence: A Comparative Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liben Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Long Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ellison-Chen%2C+T">Tian Ellison-Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhuoyuan Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Visual Question Answering (VQA) is a challenging task that requires
cross-modal understanding and reasoning of visual image and natural language
question. To inspect the association of VQA models to human cognition, we
designed a survey to record human thinking process and analyzed VQA models by
comparing the outputs and attention maps with those of humans. We found that
although the VQA models resemble human cognition in architecture and performs
similarly with human on the recognition-level, they still struggle with
cognitive inferences. The analysis of human thinking procedure serves to direct
future research and introduce more cognitive capacity into modeling features
and architectures.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02529" title="Abstract">arXiv:2310.02529</a> [<a href="/pdf/2310.02529" title="Download PDF">pdf</a>, <a href="/ps/2310.02529" title="Download PostScript">ps</a>, <a href="/format/2310.02529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIDDAG: Where Does Our News Go? Investigating Information Diffusion via  Community-Level Information Pathways
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+M+D">Mingyu Derek Ma</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+A+K">Alexander K. Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+N">Nuan Wen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kung%2C+P">Po-Nien Kung</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+W">Wenna Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+S">Shicheng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Azure Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xuezhe Ma</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> System demo video: info-pathways.github.io
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">We present MIDDAG, an intuitive, interactive system that visualizes the
information propagation paths on social media triggered by COVID-19-related
news articles accompanied by comprehensive insights including user/community
susceptibility level, as well as events and popular opinions raised by the
crowd while propagating the information. Besides discovering information flow
patterns among users, we construct communities among users and develop the
propagation forecasting capability, enabling tracing and understanding of how
information is disseminated at a higher level.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02530" title="Abstract">arXiv:2310.02530</a> [<a href="/pdf/2310.02530" title="Download PDF">pdf</a>, <a href="/format/2310.02530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Vulnerability Patches by Comprehending Code Commits with  Comprehensive Change Contexts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+T">Taotao Qian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zeyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+G">Guangtai Liang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Ding Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tao Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">To help application developers apply vulnerability patches timely, security
researchers maintain vulnerability databases such as National Vulnerability
Database (NVD). By directly monitoring NVD with the name of each used library,
application developers can be aware of vulnerabilities and their patches. Given
that the monitoring results of vulnerability patches are unreliable due to
patch incompleteness of NVD, existing approaches employ deep-learning (DL)
models to identify additional vulnerability patches by determining whether a
code commit fixes a vulnerability. However, these approaches suffer from low
accuracy due to not considering code commits' comprehensive contexts such as
control/data-flow contexts or method-invocation contexts. To improve accuracy,
we design CompVPD, the first approach to identify vulnerability patches by
fine-tuning a large language model (LLM) named StarCoder to comprehend code
commits with comprehensive contexts. Considering that including comprehensive
contexts needs to balance the context size and the training costs of LLM,
CompVPD includes our two novel algorithms to generate comprehensive contexts
within the given window size by removing irrelevant components (i.e., files,
methods, and statements) and adaptively expanding each context. We empirically
compare CompVPD with four state-of-the-art/practice (SOTA) approaches that
identify vulnerability patches. The results show that CompVPD improves the AUC
score by 11% and the F1 score by 30% when compared with the best scores of the
SOTA approaches. Additionally, CompVPD provides high value to security practice
by helping identify 20 vulnerability patches and 18 fixes of high-risk bugs
from 2,500 recent code commits of five highly popular open-source projects.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02532" title="Abstract">arXiv:2310.02532</a> [<a href="/pdf/2310.02532" title="Download PDF">pdf</a>, <a href="/format/2310.02532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ShaSTA-Fuse: Camera-LiDAR Sensor Fusion to Model Shape and  Spatio-Temporal Affinities for 3D Multi-Object Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sadjadpour%2C+T">Tara Sadjadpour</a>, 
<a href="/search/cs?searchtype=author&query=Ambrus%2C+R">Rares Ambrus</a>, 
<a href="/search/cs?searchtype=author&query=Bohg%2C+J">Jeannette Bohg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">3D multi-object tracking (MOT) is essential for an autonomous mobile agent to
safely navigate a scene. In order to maximize the perception capabilities of
the autonomous agent, we aim to develop a 3D MOT framework that fuses camera
and LiDAR sensor information. Building on our prior LiDAR-only work, ShaSTA,
which models shape and spatio-temporal affinities for 3D MOT, we propose a
novel camera-LiDAR fusion approach for learning affinities. At its core, this
work proposes a fusion technique that generates a rich sensory signal
incorporating information about depth and distant objects to enhance affinity
estimation for improved data association, track lifecycle management,
false-positive elimination, false-negative propagation, and track confidence
score refinement. Our main contributions include a novel fusion approach for
combining camera and LiDAR sensory signals to learn affinities, and a
first-of-its-kind multimodal sequential track confidence refinement technique
that fuses 2D and 3D detections. Additionally, we perform an ablative analysis
on each fusion step to demonstrate the added benefits of incorporating the
camera sensor, particular for small, distant objects that tend to suffer from
the depth-sensing limits and sparsity of LiDAR sensors. In sum, our technique
achieves state-of-the-art performance on the nuScenes benchmark amongst
multimodal 3D MOT algorithms using CenterPoint detections.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02533" title="Abstract">arXiv:2310.02533</a> [<a href="/pdf/2310.02533" title="Download PDF">pdf</a>, <a href="/format/2310.02533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying and mitigating the impact of label errors on model disparity  metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adebayo%2C+J">Julius Adebayo</a>, 
<a href="/search/cs?searchtype=author&query=Hall%2C+M">Melissa Hall</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bowen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chern%2C+B">Bobbie Chern</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference paper at ICLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Errors in labels obtained via human annotation adversely affect a model's
performance. Existing approaches propose ways to mitigate the effect of label
error on a model's downstream accuracy, yet little is known about its impact on
a model's disparity metrics. Here we study the effect of label error on a
model's disparity metrics. We empirically characterize how varying levels of
label error, in both training and test data, affect these disparity metrics. We
find that group calibration and other metrics are sensitive to train-time and
test-time label error -- particularly for minority groups. This disparate
effect persists even for models trained with noise-aware algorithms. To
mitigate the impact of training-time label error, we present an approach to
estimate the influence of a training input's label on a model's group disparity
metric. We empirically assess the proposed approach on a variety of datasets
and find significant improvement, compared to alternative approaches, in
identifying training inputs that improve a model's disparity metric. We
complement the approach with an automatic relabel-and-finetune scheme that
produces updated models with, provably, improved group calibration error.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02536" title="Abstract">arXiv:2310.02536</a> [<a href="/pdf/2310.02536" title="Download PDF">pdf</a>, <a href="/format/2310.02536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Key to Deobfuscation is Pattern of Life, not Overcoming Encryption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Henderson%2C+T">Taylor Henderson</a>, 
<a href="/search/cs?searchtype=author&query=Osterweil%2C+E">Eric Osterweil</a>, 
<a href="/search/cs?searchtype=author&query=Dinesh%2C+P+K">Pavan Kumar Dinesh</a>, 
<a href="/search/cs?searchtype=author&query=Simon%2C+R">Robert Simon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Preserving privacy is an undeniable benefit to users online. However, this
benefit (unfortunately) also extends to those who conduct cyber attacks and
other types of malfeasance. In this work, we consider the scenario in which
Privacy Preserving Technologies (PPTs) have been used to obfuscate users who
are communicating online with ill intentions. We present a novel methodology
that is effective at deobfuscating such sources by synthesizing measurements
from key locations along protocol transaction paths. Our approach links online
personas with their origin IP addresses based on a Pattern of Life (PoL)
analysis, and is successful even when different PPTs are used. We show that,
when monitoring in the correct places on the Internet, DNS over HTTPS (DoH) and
DNS over TLS (DoT) can be deobfuscated with up to 100% accuracy, when they are
the only privacy-preserving technologies used. Our evaluation used multiple
simulated monitoring points and communications are sampled from an actual
multiyear-long social network message board to replay actual user behavior. Our
evaluation compared plain old DNS, DoH, DoT, and VPN in order to quantify their
relative privacy-preserving abilities and provide recommendations for where
ideal monitoring vantage points would be in the Internet to achieve the best
performance. To illustrate the utility of our methodology, we created a
proof-of-concept cybersecurity analyst dashboard (with backend processing
infrastructure) that uses a search engine interface to allow analysts to
deobfuscate sources based on observed screen names and by providing packet
captures from subsets of vantage points.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02537" title="Abstract">arXiv:2310.02537</a> [<a href="/pdf/2310.02537" title="Download PDF">pdf</a>, <a href="/format/2310.02537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Context-Aware CEO Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seo%2C+D">Daewon Seo</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+S+H">Sung Hoon Lim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yongjune Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In many sensor network applications, a fusion center often has additional
valuable information, such as context data, which cannot be obtained directly
from the sensors. Motivated by this, we study a generalized CEO problem where a
CEO has access to context information. The main contribution of this work is
twofold. Firstly, we characterize the asymptotically optimal error exponent per
rate as the number of sensors and sum rate grow without bound. The proof
extends the Berger-Tung coding scheme and the converse argument by Berger et
al. (1996) taking into account context information. The resulting expression
includes the minimum Chernoff divergence over context information. Secondly,
assuming that the sizes of the source and context alphabets are respectively
$|\mathcal{X}|$ and $|\mathcal{S}|$, we prove that it is asymptotically optimal
to partition all sensors into at most $\binom{|\mathcal{X}|}{2} |\mathcal{S}|$
groups and have the sensors in each group adopt the same encoding scheme. Our
problem subsumes the original CEO problem by Berger et al. (1996) as a special
case if there is only one letter for context information; in this case, our
result tightens its required number of groups from $\binom{|\mathcal{X}|}{2}+2$
to $\binom{|\mathcal{X}|}{2}$. We also numerically demonstrate the effect of
context information for a simple Gaussian scenario.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02540" title="Abstract">arXiv:2310.02540</a> [<a href="/pdf/2310.02540" title="Download PDF">pdf</a>, <a href="/format/2310.02540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auto-FP: An Experimental Study of Automated Feature Preprocessing for  Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+D">Danrui Qi</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+J">Jinglin Peng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yongjun He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiannan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Classical machine learning models, such as linear models and tree-based
models, are widely used in industry. These models are sensitive to data
distribution, thus feature preprocessing, which transforms features from one
distribution to another, is a crucial step to ensure good model quality.
Manually constructing a feature preprocessing pipeline is challenging because
data scientists need to make difficult decisions about which preprocessors to
select and in which order to compose them. In this paper, we study how to
automate feature preprocessing (Auto-FP) for tabular data. Due to the large
search space, a brute-force solution is prohibitively expensive. To address
this challenge, we interestingly observe that Auto-FP can be modelled as either
a hyperparameter optimization (HPO) or a neural architecture search (NAS)
problem. This observation enables us to extend a variety of HPO and NAS
algorithms to solve the Auto-FP problem. We conduct a comprehensive evaluation
and analysis of 15 algorithms on 45 public ML datasets. Overall,
evolution-based algorithms show the leading average ranking. Surprisingly, the
random search turns out to be a strong baseline. Many surrogate-model-based and
bandit-based search algorithms, which achieve good performance for HPO and NAS,
do not outperform random search for Auto-FP. We analyze the reasons for our
findings and conduct a bottleneck analysis to identify the opportunities to
improve these algorithms. Furthermore, we explore how to extend Auto-FP to
support parameter search and compare two ways to achieve this goal. In the end,
we evaluate Auto-FP in an AutoML context and discuss the limitations of popular
AutoML tools. To the best of our knowledge, this is the first study on
automated feature preprocessing. We hope our work can inspire researchers to
develop new algorithms tailored for Auto-FP.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02541" title="Abstract">arXiv:2310.02541</a> [<a href="/pdf/2310.02541" title="Download PDF">pdf</a>, <a href="/format/2310.02541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhiwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yutong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Frei%2C+S">Spencer Frei</a>, 
<a href="/search/cs?searchtype=author&query=Vardi%2C+G">Gal Vardi</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wei Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Neural networks trained by gradient descent (GD) have exhibited a number of
surprising generalization behaviors. First, they can achieve a perfect fit to
noisy training data and still generalize near-optimally, showing that
overfitting can sometimes be benign. Second, they can undergo a period of
classical, harmful overfitting -- achieving a perfect fit to training data with
near-random performance on test data -- before transitioning ("grokking") to
near-optimal generalization later in training. In this work, we show that both
of these phenomena provably occur in two-layer ReLU networks trained by GD on
XOR cluster data where a constant fraction of the training labels are flipped.
In this setting, we show that after the first step of GD, the network achieves
100% training accuracy, perfectly fitting the noisy labels in the training
data, but achieves near-random test accuracy. At a later training step, the
network achieves near-optimal test accuracy while still fitting the random
labels in the training data, exhibiting a "grokking" phenomenon. This provides
the first theoretical result of benign overfitting in neural network
classification when the data distribution is not linearly separable. Our proofs
rely on analyzing the feature learning process under GD, which reveals that the
network implements a non-generalizable linear classifier after one step and
gradually learns generalizable features in later steps.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02542" title="Abstract">arXiv:2310.02542</a> [<a href="/pdf/2310.02542" title="Download PDF">pdf</a>, <a href="/format/2310.02542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tightly Joining Positioning and Control for Trustworthy Unmanned Aerial  Vehicles Based on Factor Graph Optimization in Urban Transportation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+P">Peiwen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+W">Weisong Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Unmanned aerial vehicles (UAV) showed great potential in improving the
efficiency of parcel delivery applications in the coming smart cities era.
Unfortunately, the trustworthy positioning and control algorithms of the UAV
are significantly challenged in complex urban areas. For example, the
ubiquitous global navigation satellite system (GNSS) positioning can be
degraded by the signal reflections from surrounding high-rising buildings,
leading to significantly increased positioning uncertainty. An additional
challenge is introduced to the control algorithm due to the complex wind
disturbances in urban canyons. Given the fact that the system positioning and
control are highly correlated with each other, for example, the system dynamics
of the control can largely help with the positioning, this paper proposed a
joint positioning and control method (JPCM) based on factor graph optimization
(FGO), which combines sensors' measurements and control intention. In
particular, the positioning measurements are formulated as the factors in the
factor graph model, such as the positioning from the GNSS. The model predictive
control (MPC) is also formulated as the additional factors in the factor graph
model. By solving the factor graph contributed by both the positioning factor
and the MPC-based factors, the complementariness of positioning and control can
be fully explored. To guarantee reliable system dynamic parameters, we validate
the effectiveness of the proposed method using a simulated quadrotor system
which showed significantly improved trajectory following performance. To
benefit the research community, we open-source our code and make it available
at https://github.com/RoboticsPolyu/IPN_MPC.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02543" title="Abstract">arXiv:2310.02543</a> [<a href="/pdf/2310.02543" title="Download PDF">pdf</a>, <a href="/format/2310.02543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provable Tensor Completion with Graph Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaidong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+X">Xiuwu Liao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Shaojie Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Can Yang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+D">Deyu Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graphs, depicting the interrelations between variables, has been widely used
as effective side information for accurate data recovery in various
matrix/tensor recovery related applications. In this paper, we study the tensor
completion problem with graph information. Current research on
graph-regularized tensor completion tends to be task-specific, lacking
generality and systematic approaches. Furthermore, a recovery theory to ensure
performance remains absent. Moreover, these approaches overlook the dynamic
aspects of graphs, treating them as static akin to matrices, even though graphs
could exhibit dynamism in tensor-related scenarios. To confront these
challenges, we introduce a pioneering framework in this paper that
systematically formulates a novel model, theory, and algorithm for solving the
dynamic graph regularized tensor completion problem. For the model, we
establish a rigorous mathematical representation of the dynamic graph, based on
which we derive a new tensor-oriented graph smoothness regularization. By
integrating this regularization into a tensor decomposition model based on
transformed t-SVD, we develop a comprehensive model simultaneously capturing
the low-rank and similarity structure of the tensor. In terms of theory, we
showcase the alignment between the proposed graph smoothness regularization and
a weighted tensor nuclear norm. Subsequently, we establish assurances of
statistical consistency for our model, effectively bridging a gap in the
theoretical examination of the problem involving tensor recovery with graph
information. In terms of the algorithm, we develop a solution of high
effectiveness, accompanied by a guaranteed convergence, to address the
resulting model. To showcase the prowess of our proposed model in contrast to
established ones, we provide in-depth numerical experiments encompassing
synthetic data as well as real-world datasets.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02544" title="Abstract">arXiv:2310.02544</a> [<a href="/pdf/2310.02544" title="Download PDF">pdf</a>, <a href="/format/2310.02544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SlowFormer: Universal Adversarial Patch for Attack on Compute and Energy  Efficiency of Inference Efficient Vision Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Navaneet%2C+K">KL Navaneet</a>, 
<a href="/search/cs?searchtype=author&query=Koohpayegani%2C+S+A">Soroush Abbasi Koohpayegani</a>, 
<a href="/search/cs?searchtype=author&query=Sleiman%2C+E">Essam Sleiman</a>, 
<a href="/search/cs?searchtype=author&query=Pirsiavash%2C+H">Hamed Pirsiavash</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/UCDvision/SlowFormer">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, there has been a lot of progress in reducing the computation of
deep models at inference time. These methods can reduce both the computational
needs and power usage of deep models. Some of these approaches adaptively scale
the compute based on the input instance. We show that such models can be
vulnerable to a universal adversarial patch attack, where the attacker
optimizes for a patch that when pasted on any image, can increase the compute
and power consumption of the model. We run experiments with three different
efficient vision transformer methods showing that in some cases, the attacker
can increase the computation to the maximum possible level by simply pasting a
patch that occupies only 8\% of the image area. We also show that a standard
adversarial training defense method can reduce some of the attack's success. We
believe adaptive efficient methods will be necessary for the future to lower
the power usage of deep models, so we hope our paper encourages the community
to study the robustness of these methods and develop better defense methods for
the proposed attack.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02546" title="Abstract">arXiv:2310.02546</a> [<a href="/pdf/2310.02546" title="Download PDF">pdf</a>, <a href="/format/2310.02546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Design of Protein Sequence and Structure based on Motifs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhenqiao Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yunlong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yufei Song</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wenxian Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Designing novel proteins with desired functions is crucial in biology and
chemistry. However, most existing work focus on protein sequence design,
leaving protein sequence and structure co-design underexplored. In this paper,
we propose GeoPro, a method to design protein backbone structure and sequence
jointly. Our motivation is that protein sequence and its backbone structure
constrain each other, and thus joint design of both can not only avoid
nonfolding and misfolding but also produce more diverse candidates with desired
functions. To this end, GeoPro is powered by an equivariant encoder for
three-dimensional (3D) backbone structure and a protein sequence decoder guided
by 3D geometry. Experimental results on two biologically significant
metalloprotein datasets, including $\beta$-lactamases and myoglobins, show that
our proposed GeoPro outperforms several strong baselines on most metrics.
Remarkably, our method discovers novel $\beta$-lactamases and myoglobins which
are not present in protein data bank (PDB) and UniProt. These proteins exhibit
stable folding and active site environments reminiscent of those of natural
proteins, demonstrating their excellent potential to be biologically
functional.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02548" title="Abstract">arXiv:2310.02548</a> [<a href="/pdf/2310.02548" title="Download PDF">pdf</a>, <a href="/format/2310.02548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact and soft boundary conditions in Physics-Informed Neural Networks  for the Variable Coefficient Poisson equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barschkis%2C+S">Sebastian Barschkis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Boundary conditions (BCs) are a key component in every Physics-Informed
Neural Network (PINN). By defining the solution to partial differential
equations (PDEs) along domain boundaries, BCs constrain the underlying boundary
value problem (BVP) that a PINN tries to approximate. Without them, unique PDE
solutions may not exist and finding approximations with PINNs would be a
challenging, if not impossible task. This study examines how soft loss-based
and exact distance function-based BC imposition approaches differ when applied
in PINNs. The well known variable coefficient Poisson equation serves as the
target PDE for all PINN models trained in this work. Besides comparing BC
imposition approaches, the goal of this work is to also provide resources on
how to implement these PINNs in practice. To this end, Keras models with
Tensorflow backend as well as a Python notebook with code examples and
step-by-step explanations on how to build soft/exact BC PINNs are published
alongside this review.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02549" title="Abstract">arXiv:2310.02549</a> [<a href="/pdf/2310.02549" title="Download PDF">pdf</a>, <a href="/format/2310.02549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heterogeneous Federated Learning Using Knowledge Codistillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lichtarge%2C+J">Jared Lichtarge</a>, 
<a href="/search/cs?searchtype=author&query=Amid%2C+E">Ehsan Amid</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Shankar Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tien-Ju Yang</a>, 
<a href="/search/cs?searchtype=author&query=Anil%2C+R">Rohan Anil</a>, 
<a href="/search/cs?searchtype=author&query=Mathews%2C+R">Rajiv Mathews</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated Averaging, and many federated learning algorithm variants which
build upon it, have a limitation: all clients must share the same model
architecture. This results in unused modeling capacity on many clients, which
limits model performance. To address this issue, we propose a method that
involves training a small model on the entire pool and a larger model on a
subset of clients with higher capacity. The models exchange information
bidirectionally via knowledge distillation, utilizing an unlabeled dataset on a
server without sharing parameters. We present two variants of our method, which
improve upon federated averaging on image classification and language modeling
tasks. We show this technique can be useful even if only out-of-domain or
limited in-domain distillation data is available. Additionally, the
bi-directional knowledge distillation allows for domain transfer between the
models when different pool populations introduce domain shift.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02550" title="Abstract">arXiv:2310.02550</a> [<a href="/pdf/2310.02550" title="Download PDF">pdf</a>, <a href="/ps/2310.02550" title="Download PostScript">ps</a>, <a href="/format/2310.02550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence Analysis and Latency Minimization for Semi-Federated  Learning in Massive IoT Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jianyang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+W">Wanli Ni</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+H">Hui Tian</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+G">Gaofeng Nie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by IEEE Transactions on Green Communications and Networking
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">As the number of sensors becomes massive in Internet of Things (IoT)
networks, the amount of data is humongous. To process data in real-time while
protecting user privacy, federated learning (FL) has been regarded as an
enabling technique to push edge intelligence into IoT networks with massive
devices. However, FL latency increases dramatically due to the increase of the
number of parameters in deep neural network and the limited computation and
communication capabilities of IoT devices. To address this issue, we propose a
semi-federated learning (SemiFL) paradigm in which network pruning and
over-the-air computation are efficiently applied. To be specific, each small
base station collects the raw data from its served sensors and trains its local
pruned model. After that, the global aggregation of local gradients is achieved
through over-the-air computation. We first analyze the performance of the
proposed SemiFL by deriving its convergence upper bound. To reduce latency, a
convergence-constrained SemiFL latency minimization problem is formulated. By
decoupling the original problem into several sub-problems, iterative algorithms
are designed to solve them efficiently. Finally, numerical simulations are
conducted to verify the effectiveness of our proposed scheme in reducing
latency and guaranteeing the identification accuracy.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02554" title="Abstract">arXiv:2310.02554</a> [<a href="/pdf/2310.02554" title="Download PDF">pdf</a>, <a href="/format/2310.02554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhipeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+N">Nanqing Dong</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiahao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Knottenbelt%2C+W">William Knottenbelt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Federated Learning (FL) is a machine learning paradigm, which enables
multiple and decentralized clients to collaboratively train a model under the
orchestration of a central aggregator. Traditional FL solutions rely on the
trust assumption of the centralized aggregator, which forms cohorts of clients
in a fair and honest manner. However, a malicious aggregator, in reality, could
abandon and replace the client's training models, or launch Sybil attacks to
insert fake clients. Such malicious behaviors give the aggregator more power to
control clients in the FL setting and determine the final training results. In
this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to
tackle the issue of a malicious aggregator during the training model
aggregation process. To guarantee the correct aggregation results, the
aggregator needs to provide a proof per round. The proof can demonstrate to the
clients that the aggregator executes the intended behavior faithfully. To
further reduce the verification cost of clients, we employ a blockchain to
handle the proof in a zero-knowledge way, where miners (i.e., the nodes
validating and maintaining the blockchain data) can verify the proof without
knowing the clients' local and aggregated models. The theoretical analysis and
empirical results show that zkFL can achieve better security and privacy than
traditional FL, without modifying the underlying FL network structure or
heavily compromising the training speed.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02556" title="Abstract">arXiv:2310.02556</a> [<a href="/pdf/2310.02556" title="Download PDF">pdf</a>, <a href="/format/2310.02556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NOLA: Networks as Linear Combination of Low Rank Random Basis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koohpayegani%2C+S+A">Soroush Abbasi Koohpayegani</a>, 
<a href="/search/cs?searchtype=author&query=Navaneet%2C+K">KL Navaneet</a>, 
<a href="/search/cs?searchtype=author&query=Nooralinejad%2C+P">Parsa Nooralinejad</a>, 
<a href="/search/cs?searchtype=author&query=Kolouri%2C+S">Soheil Kolouri</a>, 
<a href="/search/cs?searchtype=author&query=Pirsiavash%2C+H">Hamed Pirsiavash</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our code is available here: <a href="https://github.com/UCDvision/NOLA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Large Language Models (LLMs) have recently gained popularity due to their
impressive few-shot performance across various downstream tasks. However,
fine-tuning all parameters and storing a unique model for each downstream task
or domain becomes impractical because of the massive size of checkpoints (e.g.,
350GB in GPT-3). Current literature, such as LoRA, showcases the potential of
low-rank modifications to the original weights of an LLM, enabling efficient
adaptation and storage for task-specific models. These methods can reduce the
number of parameters needed to fine-tune an LLM by several orders of magnitude.
Yet, these methods face two primary limitations: 1) the parameter reduction is
lower-bounded by the rank one decomposition, and 2) the extent of reduction is
heavily influenced by both the model architecture and the chosen rank. For
instance, in larger models, even a rank one decomposition might exceed the
number of parameters truly needed for adaptation. In this paper, we introduce
NOLA, which overcomes the rank one lower bound present in LoRA. It achieves
this by re-parameterizing the low-rank matrices in LoRA using linear
combinations of randomly generated matrices (basis) and optimizing the linear
mixture coefficients only. This approach allows us to decouple the number of
trainable parameters from both the choice of rank and the network architecture.
We present adaptation results using GPT-2 and ViT in natural language and
computer vision tasks. NOLA performs as well as, or better than models with
equivalent parameter counts. Furthermore, we demonstrate that we can halve the
parameters in larger models compared to LoRA with rank one, without sacrificing
performance.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02557" title="Abstract">arXiv:2310.02557</a> [<a href="/pdf/2310.02557" title="Download PDF">pdf</a>, <a href="/format/2310.02557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization in diffusion models arises from geometry-adaptive  harmonic representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kadkhodaie%2C+Z">Zahra Kadkhodaie</a>, 
<a href="/search/cs?searchtype=author&query=Guth%2C+F">Florentin Guth</a>, 
<a href="/search/cs?searchtype=author&query=Simoncelli%2C+E+P">Eero P. Simoncelli</a>, 
<a href="/search/cs?searchtype=author&query=Mallat%2C+S">St&#xe9;phane Mallat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">High-quality samples generated with score-based reverse diffusion algorithms
provide evidence that deep neural networks (DNN) trained for denoising can
learn high-dimensional densities, despite the curse of dimensionality. However,
recent reports of memorization of the training set raise the question of
whether these networks are learning the "true" continuous density of the data.
Here, we show that two denoising DNNs trained on non-overlapping subsets of a
dataset learn nearly the same score function, and thus the same density, with a
surprisingly small number of training images. This strong generalization
demonstrates an alignment of powerful inductive biases in the DNN architecture
and/or training algorithm with properties of the data distribution. We analyze
these, demonstrating that the denoiser performs a shrinkage operation in a
basis adapted to the underlying image. Examination of these bases reveals
oscillating harmonic structures along contours and in homogeneous image
regions. We show that trained denoisers are inductively biased towards these
geometry-adaptive harmonic representations by demonstrating that they arise
even when the network is trained on image classes such as low-dimensional
manifolds, for which the harmonic basis is suboptimal. Additionally, we show
that the denoising performance of the networks is near-optimal when trained on
regular image classes for which the optimal basis is known to be
geometry-adaptive and harmonic.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02559" title="Abstract">arXiv:2310.02559</a> [<a href="/pdf/2310.02559" title="Download PDF">pdf</a>, <a href="/ps/2310.02559" title="Download PostScript">ps</a>, <a href="/format/2310.02559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Federated Learning: Convergence Analysis and Optimization of A  Hybrid Learning Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jingheng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+W">Wanli Ni</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+H">Hui Tian</a>, 
<a href="/search/cs?searchtype=author&query=Gunduz%2C+D">Deniz Gunduz</a>, 
<a href="/search/cs?searchtype=author&query=Quek%2C+T+Q+S">Tony Q. S. Quek</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhu Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by IEEE Transactions on Wireless Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Under the organization of the base station (BS), wireless federated learning
(FL) enables collaborative model training among multiple devices. However, the
BS is merely responsible for aggregating local updates during the training
process, which incurs a waste of the computational resource at the BS. To
tackle this issue, we propose a semi-federated learning (SemiFL) paradigm to
leverage the computing capabilities of both the BS and devices for a hybrid
implementation of centralized learning (CL) and FL. Specifically, each device
sends both local gradients and data samples to the BS for training a shared
global model. To improve communication efficiency over the same time-frequency
resources, we integrate over-the-air computation for aggregation and
non-orthogonal multiple access for transmission by designing a novel
transceiver structure. To gain deep insights, we conduct convergence analysis
by deriving a closed-form optimality gap for SemiFL and extend the result to
two extra cases. In the first case, the BS uses all accumulated data samples to
calculate the CL gradient, while a decreasing learning rate is adopted in the
second case. Our analytical results capture the destructive effect of wireless
communication and show that both FL and CL are special cases of SemiFL. Then,
we formulate a non-convex problem to reduce the optimality gap by jointly
optimizing the transmit power and receive beamformers. Accordingly, we propose
a two-stage algorithm to solve this intractable problem, in which we provide
the closed-form solutions to the beamformers. Extensive simulation results on
two real-world datasets corroborate our theoretical analysis, and show that the
proposed SemiFL outperforms conventional FL and achieves 3.2% accuracy gain on
the MNIST dataset compared to state-of-the-art benchmarks.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02563" title="Abstract">arXiv:2310.02563</a> [<a href="/pdf/2310.02563" title="Download PDF">pdf</a>, <a href="/format/2310.02563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical, Private Assurance of the Value of Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asghar%2C+H+J">Hassan Jameel Asghar</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhigang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhongrui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Kaafar%2C+D">Dali Kaafar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Two parties wish to collaborate on their datasets. However, before they
reveal their datasets to each other, the parties want to have the guarantee
that the collaboration would be fruitful. We look at this problem from the
point of view of machine learning, where one party is promised an improvement
on its prediction model by incorporating data from the other party. The parties
would only wish to collaborate further if the updated model shows an
improvement in accuracy. Before this is ascertained, the two parties would not
want to disclose their models and datasets. In this work, we construct an
interactive protocol for this problem based on the fully homomorphic encryption
scheme over the Torus (TFHE) and label differential privacy, where the
underlying machine learning model is a neural network. Label differential
privacy is used to ensure that computations are not done entirely in the
encrypted domain, which is a significant bottleneck for neural network training
according to the current state-of-the-art FHE implementations. We prove the
security of our scheme in the universal composability framework assuming
honest-but-curious parties, but where one party may not have any expertise in
labelling its initial dataset. Experiments show that we can obtain the output,
i.e., the accuracy of the updated model, with time many orders of magnitude
faster than a protocol using entirely FHE operations.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02565" title="Abstract">arXiv:2310.02565</a> [<a href="/pdf/2310.02565" title="Download PDF">pdf</a>, <a href="/format/2310.02565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Drumming Robot Via Attention Transformer Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+Y">Yang Yi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zonghan Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Robotic technology has been widely used in nowadays society, which has made
great progress in various fields such as agriculture, manufacturing and
entertainment. In this paper, we focus on the topic of drumming robots in
entertainment. To this end, we introduce an improving drumming robot that can
automatically complete music transcription based on the popular vision
transformer network based on the attention mechanism. Equipped with the
attention transformer network, our method can efficiently handle the sequential
audio embedding input and model their global long-range dependencies. Massive
experimental results demonstrate that the improving algorithm can help the
drumming robot promote drum classification performance, which can also help the
robot to enjoy a variety of smart applications and services.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02567" title="Abstract">arXiv:2310.02567</a> [<a href="/pdf/2310.02567" title="Download PDF">pdf</a>, <a href="/format/2310.02567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Automatic VQA Evaluation Using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%C3%B1as%2C+O">Oscar Ma&#xf1;as</a>, 
<a href="/search/cs?searchtype=author&query=Krojer%2C+B">Benno Krojer</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+A">Aishwarya Agrawal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">8 years after the visual question answering (VQA) task was proposed, accuracy
remains the primary metric for automatic evaluation. VQA Accuracy has been
effective so far in the IID evaluation setting. However, our community is
undergoing a shift towards open-ended generative models and OOD evaluation. In
this new paradigm, the existing VQA Accuracy metric is overly stringent and
underestimates the performance of VQA systems. Thus, there is a need to develop
more robust automatic VQA metrics that serve as a proxy for human judgment. In
this work, we propose to leverage the in-context learning capabilities of
instruction-tuned large language models (LLMs) to build a better VQA metric. We
formulate VQA evaluation as an answer-rating task where the LLM is instructed
to score the accuracy of a candidate answer given a set of reference answers.
We demonstrate the proposed metric better correlates with human judgment
compared to existing metrics across several VQA models and benchmarks. We hope
wide adoption of our metric will contribute to better estimating the research
progress on the VQA task.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02568" title="Abstract">arXiv:2310.02568</a> [<a href="/pdf/2310.02568" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stand for Something or Fall for Everything: Predict Misinformation  Spread with Stance-Aware Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zihan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingyi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Rong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+F">Feng Mai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 2023 International Conference on Information Systems (ICIS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Although pervasive spread of misinformation on social media platforms has
become a pressing challenge, existing platform interventions have shown limited
success in curbing its dissemination. In this study, we propose a stance-aware
graph neural network (stance-aware GNN) that leverages users' stances to
proactively predict misinformation spread. As different user stances can form
unique echo chambers, we customize four information passing paths in
stance-aware GNN, while the trainable attention weights provide explainability
by highlighting each structure's importance. Evaluated on a real-world dataset,
stance-aware GNN outperforms benchmarks by 32.65% and exceeds advanced GNNs
without user stance by over 4.69%. Furthermore, the attention weights indicate
that users' opposition stances have a higher impact on their neighbors'
behaviors than supportive ones, which function as social correction to halt
misinformation propagation. Overall, our study provides an effective predictive
model for platforms to combat misinformation, and highlights the impact of user
stances in the misinformation propagation.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02569" title="Abstract">arXiv:2310.02569</a> [<a href="/pdf/2310.02569" title="Download PDF">pdf</a>, <a href="/format/2310.02569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReForm-Eval: Evaluating Large Vision Language Models via Unified  Re-Formulation of Task-Oriented Benchmarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zejun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ye Wang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Mengfei Du</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qingwen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Binhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiwen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chengxing Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zhihao Fan</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingjing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhongyu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 11 figures, 24 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent years have witnessed remarkable progress in the development of large
vision-language models (LVLMs). Benefiting from the strong language backbones
and efficient cross-modal alignment strategies, LVLMs exhibit surprising
capabilities to perceive visual signals and perform visually grounded
reasoning. However, the capabilities of LVLMs have not been comprehensively and
quantitatively evaluate. Most existing multi-modal benchmarks require
task-oriented input-output formats, posing great challenges to automatically
assess the free-form text output of LVLMs. To effectively leverage the
annotations available in existing benchmarks and reduce the manual effort
required for constructing new benchmarks, we propose to re-formulate existing
benchmarks into unified LVLM-compatible formats. Through systematic data
collection and reformulation, we present the ReForm-Eval benchmark, offering
substantial data for evaluating various capabilities of LVLMs. Based on
ReForm-Eval, we conduct extensive experiments, thoroughly analyze the strengths
and weaknesses of existing LVLMs, and identify the underlying factors. Our
benchmark and evaluation framework will be open-sourced as a cornerstone for
advancing the development of LVLMs.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02570" title="Abstract">arXiv:2310.02570</a> [<a href="/pdf/2310.02570" title="Download PDF">pdf</a>, <a href="/format/2310.02570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving severity preservation of healthy-to-pathological voice  conversion with global style tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Halpern%2C+B+M">Bence Mark Halpern</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wen-Chin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Violeta%2C+L+P">Lester Phillip Violeta</a>, 
<a href="/search/cs?searchtype=author&query=van+Son%2C+R+J+J+H">R.J.J.H. van Son</a>, 
<a href="/search/cs?searchtype=author&query=Toda%2C+T">Tomoki Toda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures, 5 tables. Accepted to IEEE Automatic Speech Recognition and Understanding Workshop 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In healthy-to-pathological voice conversion (H2P-VC), healthy speech is
converted into pathological while preserving the identity. The paper improves
on previous two-stage approach to H2P-VC where (1) speech is created first with
the appropriate severity, (2) then the speaker identity of the voice is
converted while preserving the severity of the voice. Specifically, we propose
improvements to (2) by using phonetic posteriorgrams (PPG) and global style
tokens (GST). Furthermore, we present a new dataset that contains parallel
recordings of pathological and healthy speakers with the same identity which
allows more precise evaluation. Listening tests by expert listeners show that
the framework preserves severity of the source sample, while modelling target
speaker's voice. We also show that (a) pathology impacts x-vectors but not all
speaker information is lost, (b) choosing source speakers based on severity
labels alone is insufficient.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02571" title="Abstract">arXiv:2310.02571</a> [<a href="/pdf/2310.02571" title="Download PDF">pdf</a>, <a href="/ps/2310.02571" title="Download PostScript">ps</a>, <a href="/format/2310.02571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The role of local bounds on neighborhoods in the network for scale-free  state synchronization of multi-agent systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Stoorvogel%2C+A+A">Anton A. Stoorvogel</a>, 
<a href="/search/eess?searchtype=author&query=Saberi%2C+A">Ali Saberi</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zhenwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was submitted to IJRNC on Aug. 3, 2023 and obtained the recommendation of Major Revision on Oct. 2, 2023. Now, the authors are revising this paper by following the referees' comments
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper provides necessary and sufficient conditions for the existence of
solutions to the state synchronization problem of homogeneous multi-agent
systems (MAS) via scale-free linear dynamic non-collaborative protocol for both
continuous- and discrete-time. We investigate protocol design with and without
utilizing local bounds on neighborhood. The results show that the availability
of local bounds on neighborhoods plays a key role.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02572" title="Abstract">arXiv:2310.02572</a> [<a href="/pdf/2310.02572" title="Download PDF">pdf</a>, <a href="/format/2310.02572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Knowledge Distillation with Teacher&#x27;s Explanation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+S">Sayantan Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+B">Ben Liang</a>, 
<a href="/search/cs?searchtype=author&query=Tizghadam%2C+A">Ali Tizghadam</a>, 
<a href="/search/cs?searchtype=author&query=Albanese%2C+I">Ilijc Albanese</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Knowledge distillation (KD) improves the performance of a low-complexity
student model with the help of a more powerful teacher. The teacher in KD is a
black-box model, imparting knowledge to the student only through its
predictions. This limits the amount of transferred knowledge. In this work, we
introduce a novel Knowledge Explaining Distillation (KED) framework, which
allows the student to learn not only from the teacher's predictions but also
from the teacher's explanations. We propose a class of superfeature-explaining
teachers that provide explanation over groups of features, along with the
corresponding student model. We also present a method for constructing the
superfeatures. We then extend KED to reduce complexity in convolutional neural
networks, to allow augmentation with hidden-representation distillation
methods, and to work with a limited amount of training data using chimeric
sets. Our experiments over a variety of datasets show that KED students can
substantially outperform KD students of similar complexity.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02573" title="Abstract">arXiv:2310.02573</a> [<a href="/pdf/2310.02573" title="Download PDF">pdf</a>, <a href="/format/2310.02573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Collision Detection for Robots with Variable Stiffness Actuation  by Using MAD-CNN: Modularized-Attention-Dilated Convolutional Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Zhenwei Niu</a>, 
<a href="/search/cs?searchtype=author&query=Saoud%2C+L+S">Lyes Saad Saoud</a>, 
<a href="/search/cs?searchtype=author&query=Hussain%2C+I">Irfan Hussain</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Ensuring safety is paramount in the field of collaborative robotics to
mitigate the risks of human injury and environmental damage. Apart from
collision avoidance, it is crucial for robots to rapidly detect and respond to
unexpected collisions. While several learning-based collision detection methods
have been introduced as alternatives to purely model-based detection
techniques, there is currently a lack of such methods designed for
collaborative robots equipped with variable stiffness actuators. Moreover,
there is potential for further enhancing the network's robustness and improving
the efficiency of data training. In this paper, we propose a new network, the
Modularized Attention-Dilated Convolutional Neural Network (MAD-CNN), for
collision detection in robots equipped with variable stiffness actuators. Our
model incorporates a dual inductive bias mechanism and an attention module to
enhance data efficiency and improve robustness. In particular, MAD-CNN is
trained using only a four-minute collision dataset focusing on the highest
level of joint stiffness. Despite limited training data, MAD-CNN robustly
detects all collisions with minimal detection delay across various stiffness
conditions. Moreover, it exhibits a higher level of collision sensitivity,
which is beneficial for effectively handling false positives, which is a common
issue in learning-based methods. Experimental results demonstrate that the
proposed MAD-CNN model outperforms existing state-of-the-art models in terms of
collision sensitivity and robustness.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02575" title="Abstract">arXiv:2310.02575</a> [<a href="/pdf/2310.02575" title="Download PDF">pdf</a>, <a href="/format/2310.02575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaMerging: Adaptive Model Merging for Multi-Task Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+E">Enneng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+G">Guibing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xingwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Multi-task learning (MTL) aims to empower a model to tackle multiple tasks
simultaneously. A recent development known as task arithmetic has revealed that
several models, each fine-tuned for distinct tasks, can be directly merged into
a single model to execute MTL without necessitating a retraining process using
the initial training data. Nevertheless, this direct addition of models often
leads to a significant deterioration in the overall performance of the merged
model. This decline occurs due to potential conflicts and intricate
correlations among the multiple tasks. Consequently, the challenge emerges of
how to merge pre-trained models more effectively without using their original
training data. This paper introduces an innovative technique called Adaptive
Model Merging (AdaMerging). This approach aims to autonomously learn the
coefficients for model merging, either in a task-wise or layer-wise manner,
without relying on the original training data. Specifically, our AdaMerging
method operates as an automatic, unsupervised task arithmetic scheme. It
leverages entropy minimization on unlabeled test samples from the multi-task
setup as a surrogate objective function to iteratively refine the merging
coefficients of the multiple models. Our experimental findings across eight
tasks demonstrate the efficacy of the AdaMerging scheme we put forth. Compared
to the current state-of-the-art task arithmetic merging scheme, AdaMerging
showcases a remarkable 11\% improvement in performance. Notably, AdaMerging
also exhibits superior generalization capabilities when applied to unseen
downstream tasks. Furthermore, it displays a significantly enhanced robustness
to data distribution shifts that may occur during the testing phase.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02576" title="Abstract">arXiv:2310.02576</a> [<a href="/pdf/2310.02576" title="Download PDF">pdf</a>, <a href="/format/2310.02576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Prototype-Based Neural Network for Image Anomaly Detection and  Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Z">Zhao Kang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hong Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image anomaly detection and localization perform not only image-level anomaly
classification but also locate pixel-level anomaly regions. Recently, it has
received much research attention due to its wide application in various fields.
This paper proposes ProtoAD, a prototype-based neural network for image anomaly
detection and localization. First, the patch features of normal images are
extracted by a deep network pre-trained on nature images. Then, the prototypes
of the normal patch features are learned by non-parametric clustering. Finally,
we construct an image anomaly localization network (ProtoAD) by appending the
feature extraction network with $L2$ feature normalization, a $1\times1$
convolutional layer, a channel max-pooling, and a subtraction operation. We use
the prototypes as the kernels of the $1\times1$ convolutional layer; therefore,
our neural network does not need a training phase and can conduct anomaly
detection and localization in an end-to-end manner. Extensive experiments on
two challenging industrial anomaly detection datasets, MVTec AD and BTAD,
demonstrate that ProtoAD achieves competitive performance compared to the
state-of-the-art methods with a higher inference speed. The source code is
available at: https://github.com/98chao/ProtoAD.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02579" title="Abstract">arXiv:2310.02579</a> [<a href="/pdf/2310.02579" title="Download PDF">pdf</a>, <a href="/format/2310.02579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Stability of Expressive Positional Encodings for Graph Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yinan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">William Lu</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+J">Joshua Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muhan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jegelka%2C+S">Stefanie Jegelka</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pan Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Designing effective positional encodings for graphs is key to building
powerful graph transformers and enhancing message-passing graph neural
networks. Although widespread, using Laplacian eigenvectors as positional
encodings faces two fundamental challenges: (1) \emph{Non-uniqueness}: there
are many different eigendecompositions of the same Laplacian, and (2)
\emph{Instability}: small perturbations to the Laplacian could result in
completely different eigenspaces, leading to unpredictable changes in
positional encoding.
<br />Despite many attempts to address non-uniqueness, most methods overlook
stability, leading to poor generalization on unseen graph structures. We
identify the cause of instability to be a "hard partition" of eigenspaces.
Hence, we introduce Stable and Expressive Positional Encodings (SPE), an
architecture for processing eigenvectors that uses eigenvalues to "softly
partition" eigenspaces. SPE is the first architecture that is (1) provably
stable, and (2) universally expressive for basis invariant functions whilst
respecting all symmetries of eigenvectors. Besides guaranteed stability, we
prove that SPE is at least as expressive as existing methods, and highly
capable of counting graph structures. Finally, we evaluate the effectiveness of
our method on molecular property prediction, and out-of-distribution
generalization tasks, finding improved generalization compared to existing
positional encoding methods.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02582" title="Abstract">arXiv:2310.02582</a> [<a href="/pdf/2310.02582" title="Download PDF">pdf</a>, <a href="/format/2310.02582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Nash Equilibrium Solution for Periodic Double Auctions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Manvi%2C+B">Bharat Manvi</a>, 
<a href="/search/eess?searchtype=author&query=Subramanian%2C+E">Easwar Subramanian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">We consider a periodic double auction (PDA) setting where buyers of the
auction have multiple (but finite) opportunities to procure multiple but fixed
units of a commodity. The goal of each buyer participating in such auctions is
to reduce their cost of procurement by planning their purchase across multiple
rounds of the PDA. Formulating such optimal bidding strategies in a multi-agent
periodic double auction setting is a challenging problem as such strategies
involve planning across current and future auctions. In this work, we consider
one such setup wherein the composite supply curve is known to all buyers.
Specifically, for the complete information setting, we model the PDA as a
Markov game and derive Markov perfect Nash equilibrium (MPNE) solution to
devise an optimal bidding strategy for the case when each buyer is allowed to
make one bid per round of the PDA. Thereafter, the efficacy of the Nash
policies obtained is demonstrated with numerical experiments.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02583" title="Abstract">arXiv:2310.02583</a> [<a href="/pdf/2310.02583" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning-Enabled Precision Position Control and Thermal  Regulation in Advanced Thermal Actuators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mirvakili%2C+S+M">Seyed Mo Mirvakili</a>, 
<a href="/search/cs?searchtype=author&query=Haghighat%2C+E">Ehsan Haghighat</a>, 
<a href="/search/cs?searchtype=author&query=Sim%2C+D">Douglas Sim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With their unique combination of characteristics - an energy density almost
100 times that of human muscle, and a power density of 5.3 kW/kg, similar to a
jet engine's output - Nylon artificial muscles stand out as particularly apt
for robotics applications. However, the necessity of integrating sensors and
controllers poses a limitation to their practical usage. Here we report a
constant power open-loop controller based on machine learning. We show that we
can control the position of a nylon artificial muscle without external sensors.
To this end, we construct a mapping from a desired displacement trajectory to a
required power using an ensemble encoder-style feed-forward neural network. The
neural controller is carefully trained on a physics-based denoised dataset and
can be fine-tuned to accommodate various types of thermal artificial muscles,
irrespective of the presence or absence of hysteresis.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02588" title="Abstract">arXiv:2310.02588</a> [<a href="/pdf/2310.02588" title="Download PDF">pdf</a>, <a href="/format/2310.02588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViT-ReciproCAM: Gradient and Attention-Free Visual Explanations for  Vision Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Byun%2C+S">Seok-Yong Byun</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+W">Wonju Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents a novel approach to address the challenges of
understanding the prediction process and debugging prediction errors in Vision
Transformers (ViT), which have demonstrated superior performance in various
computer vision tasks such as image classification and object detection. While
several visual explainability techniques, such as CAM, Grad-CAM, Score-CAM, and
Recipro-CAM, have been extensively researched for Convolutional Neural Networks
(CNNs), limited research has been conducted on ViT. Current state-of-the-art
solutions for ViT rely on class agnostic Attention-Rollout and Relevance
techniques. In this work, we propose a new gradient-free visual explanation
method for ViT, called ViT-ReciproCAM, which does not require attention matrix
and gradient information. ViT-ReciproCAM utilizes token masking and generated
new layer outputs from the target layer's input to exploit the correlation
between activated tokens and network predictions for target classes. Our
proposed method outperforms the state-of-the-art Relevance method in the
Average Drop-Coherence-Complexity (ADCC) metric by $4.58\%$ to $5.80\%$ and
generates more localized saliency maps. Our experiments demonstrate the
effectiveness of ViT-ReciproCAM and showcase its potential for understanding
and debugging ViT models. Our proposed method provides an efficient and
easy-to-implement alternative for generating visual explanations, without
requiring attention and gradient information, which can be beneficial for
various applications in the field of computer vision.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02590" title="Abstract">arXiv:2310.02590</a> [<a href="/pdf/2310.02590" title="Download PDF">pdf</a>, <a href="/format/2310.02590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Evaluation of Video Streaming Applications with Target Wake  Time in Wi-Fi 6
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rajendran%2C+G">Govind Rajendran</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+R">Rishabh Roy</a>, 
<a href="/search/cs?searchtype=author&query=Hathi%2C+P">Preyas Hathi</a>, 
<a href="/search/cs?searchtype=author&query=Akhtar%2C+N">Nadeem Akhtar</a>, 
<a href="/search/cs?searchtype=author&query=Agnihotri%2C+S">Samar Agnihotri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was part of 15th International Conference on COMmunication Systems &amp; NETworkS (COMSNETS), Bangalore, India, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The Target Wake Time (TWT) feature, introduced in Wi-Fi 6, was primarily
meant as an advanced power save mechanism. However, it has some interesting
applications in scheduling and resource allocation. TWT-based resource
allocation can be used to improve the user experience for certain applications,
e.g., VoIP, IoT, video streaming, etc. In this work, we analyze the packet
arrival pattern for streaming traffic and develop a synthetic video streaming
traffic generator that mimics real-world streaming traffic. We propose a
two-stage approach where we calculate the TWT duty cycle in the first step. In
the subsequent step, we determine the Multiplication Factor(MF), which jointly
dictates the required TWT schedule for the synthetic traffic model. Initial
testing shows that key QoS metrics can be met for sustained performance of
synthetic traffic upon enabling TWT, even in the presence of peak background
congestion in the network.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02591" title="Abstract">arXiv:2310.02591</a> [<a href="/pdf/2310.02591" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Inception Architecture with Residual Connection: Fine-tuned  Inception-ResNet Deep Learning Model for Lung Inflammation Diagnosis from  Chest Radiographs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neshat%2C+M">Mehdi Neshat</a>, 
<a href="/search/cs?searchtype=author&query=Ahmedb%2C+M">Muktar Ahmedb</a>, 
<a href="/search/cs?searchtype=author&query=Askarid%2C+H">Hossein Askarid</a>, 
<a href="/search/cs?searchtype=author&query=Thilakaratnee%2C+M">Menasha Thilakaratnee</a>, 
<a href="/search/cs?searchtype=author&query=Mirjalilia%2C+S">Seyedali Mirjalilia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at International Conference on Machine Learning and Data Engineering (ICMLDE 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Diagnosing lung inflammation, particularly pneumonia, is of paramount
importance for effectively treating and managing the disease. Pneumonia is a
common respiratory infection caused by bacteria, viruses, or fungi and can
indiscriminately affect people of all ages. As highlighted by the World Health
Organization (WHO), this prevalent disease tragically accounts for a
substantial 15% of global mortality in children under five years of age. This
article presents a comparative study of the Inception-ResNet deep learning
model's performance in diagnosing pneumonia from chest radiographs. The study
leverages Mendeleys chest X-ray images dataset, which contains 5856 2D images,
including both Viral and Bacterial Pneumonia X-ray images. The Inception-ResNet
model is compared with seven other state-of-the-art convolutional neural
networks (CNNs), and the experimental results demonstrate the Inception-ResNet
model's superiority in extracting essential features and saving computation
runtime. Furthermore, we examine the impact of transfer learning with
fine-tuning in improving the performance of deep convolutional models. This
study provides valuable insights into using deep learning models for pneumonia
diagnosis and highlights the potential of the Inception-ResNet model in this
field. In classification accuracy, Inception-ResNet-V2 showed superior
performance compared to other models, including ResNet152V2, MobileNet-V3
(Large and Small), EfficientNetV2 (Large and Small), InceptionV3, and
NASNet-Mobile, with substantial margins. It outperformed them by 2.6%, 6.5%,
7.1%, 13%, 16.1%, 3.9%, and 1.6%, respectively, demonstrating its significant
advantage in accurate classification.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02592" title="Abstract">arXiv:2310.02592</a> [<a href="/pdf/2310.02592" title="Download PDF">pdf</a>, <a href="/ps/2310.02592" title="Download PostScript">ps</a>, <a href="/format/2310.02592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Faster Deterministic Approximation Algorithm for TTP-2
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kanaya%2C+Y">Yuga Kanaya</a>, 
<a href="/search/cs?searchtype=author&query=Takazawa%2C+K">Kenjiro Takazawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 42 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">The traveling tournament problem (TTP) is to minimize the total traveling
distance of all teams in a double round-robin tournament. In this paper, we
focus on TTP-2, in which each team plays at most two consecutive home games and
at most two consecutive away games. For the case where the number of teams
$n\equiv2$ (mod 4), Zhao and Xiao (2022) presented a $(1+5/n)$-approximation
algorithm. This is a randomized algorithm running in $O(n^3)$ time, and its
derandomized version runs in $O(n^4)$ time. In this paper, we present a faster
deterministic algorithm running in $O(n^3)$ time, with approximation ratio
$1+9/n$. This ratio improves the previous approximation ratios of the
deterministic algorithms with the same time complexity.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02593" title="Abstract">arXiv:2310.02593</a> [<a href="/pdf/2310.02593" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A ModelOps-based Framework for Intelligent Medical Knowledge Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+H">Hongxin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+P">Peinie Zou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Junfeng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yasha Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qiang Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Extracting medical knowledge from healthcare texts enhances downstream tasks
like medical knowledge graph construction and clinical decision-making.
However, the construction and application of knowledge extraction models lack
automation, reusability and unified management, leading to inefficiencies for
researchers and high barriers for non-AI experts such as doctors, to utilize
knowledge extraction. To address these issues, we propose a ModelOps-based
intelligent medical knowledge extraction framework that offers a low-code
system for model selection, training, evaluation and optimization.
Specifically, the framework includes a dataset abstraction mechanism based on
multi-layer callback functions, a reusable model training, monitoring and
management mechanism. We also propose a model recommendation method based on
dataset similarity, which helps users quickly find potentially suitable models
for a given dataset. Our framework provides convenience for researchers to
develop models and simplifies model access for non-AI experts such as doctors.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02594" title="Abstract">arXiv:2310.02594</a> [<a href="/pdf/2310.02594" title="Download PDF">pdf</a>, <a href="/format/2310.02594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> I$^2$KD-SLU: An Intra-Inter Knowledge Distillation Framework for  Zero-Shot Cross-Lingual Spoken Language Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+T">Tianjun Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenghong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages,2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Spoken language understanding (SLU) typically includes two subtasks: intent
detection and slot filling. Currently, it has achieved great success in
high-resource languages, but it still remains challenging in low-resource
languages due to the scarcity of labeled training data. Hence, there is a
growing interest in zero-shot cross-lingual SLU. Despite of the success of
existing zero-shot cross-lingual SLU models, most of them neglect to achieve
the mutual guidance between intent and slots. To address this issue, we propose
an Intra-Inter Knowledge Distillation framework for zero-shot cross-lingual
Spoken Language Understanding (I$^2$KD-SLU) to model the mutual guidance.
Specifically, we not only apply intra-knowledge distillation between intent
predictions or slot predictions of the same utterance in different languages,
but also apply inter-knowledge distillation between intent predictions and slot
predictions of the same utterance. Our experimental results demonstrate that
our proposed framework significantly improves the performance compared with the
strong baselines and achieves the new state-of-the-art performance on the
MultiATIS++ dataset, obtaining a significant improvement over the previous best
model in overall accuracy.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02596" title="Abstract">arXiv:2310.02596</a> [<a href="/pdf/2310.02596" title="Download PDF">pdf</a>, <a href="/format/2310.02596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent  Text-to-3D
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weiyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Rui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuelin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+P">Ping Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://sweetdreamer3d.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">It is inherently ambiguous to lift 2D results from pre-trained diffusion
models to a 3D world for text-to-3D generation. 2D diffusion models solely
learn view-agnostic priors and thus lack 3D knowledge during the lifting,
leading to the multi-view inconsistency problem. We find that this problem
primarily stems from geometric inconsistency, and avoiding misplaced geometric
structures substantially mitigates the problem in the final outputs. Therefore,
we improve the consistency by aligning the 2D geometric priors in diffusion
models with well-defined 3D shapes during the lifting, addressing the vast
majority of the problem. This is achieved by fine-tuning the 2D diffusion model
to be viewpoint-aware and to produce view-specific coordinate maps of
canonically oriented 3D objects. In our process, only coarse 3D information is
used for aligning. This "coarse" alignment not only resolves the multi-view
inconsistency in geometries but also retains the ability in 2D diffusion models
to generate detailed and diversified high-quality objects unseen in the 3D
datasets. Furthermore, our aligned geometric priors (AGP) are generic and can
be seamlessly integrated into various state-of-the-art pipelines, obtaining
high generalizability in terms of unseen shapes and visual appearance while
greatly alleviating the multi-view inconsistency problem. Our method represents
a new state-of-the-art performance with an 85+% consistency rate by human
evaluation, while many previous methods are around 30%. Our project page is
https://sweetdreamer3d.github.io/
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02598" title="Abstract">arXiv:2310.02598</a> [<a href="/pdf/2310.02598" title="Download PDF">pdf</a>, <a href="/format/2310.02598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Algorithm Cards: Streamlining the development of hybrid  classical-quantum applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stirbu%2C+V">Vlad Stirbu</a>, 
<a href="/search/cs?searchtype=author&query=Haghparast%2C+M">Majid Haghparast</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The emergence of quantum computing proposes a revolutionary paradigm that can
radically transform numerous scientific and industrial application domains. The
ability of quantum computers to scale computations implies better performance
and efficiency for certain algorithmic tasks than current computers provide.
However, to gain benefit from such improvement, quantum computers must be
integrated with existing software systems, a process that is not
straightforward. In this paper, we investigate challenges that emerge when
building larger hybrid classical-quantum computers and introduce the Quantum
Algorithm Card (QAC) concept, an approach that could be employed to facilitate
the decision making process around quantum technology.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02599" title="Abstract">arXiv:2310.02599</a> [<a href="/pdf/2310.02599" title="Download PDF">pdf</a>, <a href="/format/2310.02599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does It Spin? On the Adoption and Use of QUIC&#x27;s Spin Bit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kunze%2C+I">Ike Kunze</a>, 
<a href="/search/cs?searchtype=author&query=Sander%2C+C">Constantin Sander</a>, 
<a href="/search/cs?searchtype=author&query=Wehrle%2C+K">Klaus Wehrle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Encrypted QUIC traffic complicates network management as traditional
transport layer semantics can no longer be used for RTT or packet loss
measurements. Addressing this challenge, QUIC includes an optional, carefully
designed mechanism: the spin bit. While its capabilities have already been
studied in test settings, its real-world usefulness and adoption are unknown.
In this paper, we thus investigate the spin bit's deployment and utility on the
web.
<br />Analyzing our long-term measurements of more than 200M domains, we find that
the spin bit is enabled on ~10% of those with QUIC support and for ~50% / 60%
of the underlying IPv4 / IPv6 hosts. The support is mainly driven by
medium-sized cloud providers while most hyperscalers do not implement it.
Assessing the utility of spin bit RTT measurements, the theoretical issue of
reordering does not significantly manifest in our study and the spin bit
provides accurate estimates for around 30.5% of connections using the
mechanism, but drastically overestimates the RTT for another 51.7%. Overall, we
conclude that the spin bit, even though an optional feature, indeed sees use in
the wild and is able to provide reasonable RTT estimates for a solid share of
QUIC connections, but requires solutions for making its measurements more
robust.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02601" title="Abstract">arXiv:2310.02601</a> [<a href="/pdf/2310.02601" title="Download PDF">pdf</a>, <a href="/format/2310.02601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MagicDrive: Street View Generation with Diverse 3D Geometry Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+R">Ruiyuan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lanqing Hong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+D">Dit-Yan Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qiang Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://flymin.github.io/magicdrive">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements in diffusion models have significantly enhanced the data
synthesis with 2D control. Yet, precise 3D control in street view generation,
crucial for 3D perception tasks, remains elusive. Specifically, utilizing
Bird's-Eye View (BEV) as the primary condition often leads to challenges in
geometry control (e.g., height), affecting the representation of object shapes,
occlusion patterns, and road surface elevations, all of which are essential to
perception data synthesis, especially for 3D object detection tasks. In this
paper, we introduce MagicDrive, a novel street view generation framework
offering diverse 3D geometry controls, including camera poses, road maps, and
3D bounding boxes, together with textual descriptions, achieved through
tailored encoding strategies. Besides, our design incorporates a cross-view
attention module, ensuring consistency across multiple camera views. With
MagicDrive, we achieve high-fidelity street-view synthesis that captures
nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV
segmentation and 3D object detection.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02604" title="Abstract">arXiv:2310.02604</a> [<a href="/pdf/2310.02604" title="Download PDF">pdf</a>, <a href="/format/2310.02604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Last-iterate Convergence in Time-varying Zero-sum Games: Extra  Gradient Succeeds where Optimism Fails
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Hu Fu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qun Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Ping Li</a>, 
<a href="/search/cs?searchtype=author&query=Panageas%2C+I">Ioannis Panageas</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Bo Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, accepted for NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Last-iterate convergence has received extensive study in two player zero-sum
games starting from bilinear, convex-concave up to settings that satisfy the
MVI condition. Typical methods that exhibit last-iterate convergence for the
aforementioned games include extra-gradient (EG) and optimistic gradient
descent ascent (OGDA). However, all the established last-iterate convergence
results hold for the restrictive setting where the underlying repeated game
does not change over time. Recently, a line of research has focused on regret
analysis of OGDA in time-varying games, i.e., games where payoffs evolve with
time; the last-iterate behavior of OGDA and EG in time-varying environments
remains unclear though. In this paper, we study the last-iterate behavior of
various algorithms in two types of unconstrained, time-varying, bilinear
zero-sum games: periodic and convergent perturbed games. These models expand
upon the usual repeated game formulation and incorporate external environmental
factors, such as the seasonal effects on species competition and vanishing
external noise. In periodic games, we prove that EG will converge while OGDA
and momentum method will diverge. This is quite surprising, as to the best of
our knowledge, it is the first result that indicates EG and OGDA have
qualitatively different last-iterate behaviors and do not exhibit similar
behavior. In convergent perturbed games, we prove all these algorithms converge
as long as the game itself stabilizes with a faster rate than $1/t$.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02605" title="Abstract">arXiv:2310.02605</a> [<a href="/pdf/2310.02605" title="Download PDF">pdf</a>, <a href="/format/2310.02605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Agent Reinforcement Learning for Power Grid Topology Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+der+Sar%2C+E">Erica van der Sar</a>, 
<a href="/search/cs?searchtype=author&query=Zocca%2C+A">Alessandro Zocca</a>, 
<a href="/search/cs?searchtype=author&query=Bhulai%2C+S">Sandjai Bhulai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to PSCC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Machine Learning (stat.ML)

</div>
<p class="mathjax">Recent challenges in operating power networks arise from increasing energy
demands and unpredictable renewable sources like wind and solar. While
reinforcement learning (RL) shows promise in managing these networks, through
topological actions like bus and line switching, efficiently handling large
action spaces as networks grow is crucial. This paper presents a hierarchical
multi-agent reinforcement learning (MARL) framework tailored for these
expansive action spaces, leveraging the power grid's inherent hierarchical
nature. Experimental results indicate the MARL framework's competitive
performance with single-agent RL methods. We also compare different RL
algorithms for lower-level agents alongside different policies for higher-order
agents.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02606" title="Abstract">arXiv:2310.02606</a> [<a href="/pdf/2310.02606" title="Download PDF">pdf</a>, <a href="/format/2310.02606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning adjacency matrix for dynamic graph neural network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+O">Osama Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Jalil%2C+O+A">Omer Abdul Jalil</a>, 
<a href="/search/cs?searchtype=author&query=Nazir%2C+U">Usman Nazir</a>, 
<a href="/search/cs?searchtype=author&query=Taj%2C+M">Murtaza Taj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">In recent work, [1] introduced the concept of using a Block Adjacency Matrix
(BA) for the representation of spatio-temporal data. While their method
successfully concatenated adjacency matrices to encapsulate spatio-temporal
relationships in a single graph, it formed a disconnected graph. This
limitation hampered the ability of Graph Convolutional Networks (GCNs) to
perform message passing across nodes belonging to different time steps, as no
temporal links were present. To overcome this challenge, we introduce an
encoder block specifically designed to learn these missing temporal links. The
encoder block processes the BA and predicts connections between previously
unconnected subgraphs, resulting in a Spatio-Temporal Block Adjacency Matrix
(STBAM). This enriched matrix is then fed into a Graph Neural Network (GNN) to
capture the complex spatio-temporal topology of the network. Our evaluations on
benchmark datasets, surgVisDom and C2D2, demonstrate that our method, with
slightly higher complexity, achieves superior results compared to
state-of-the-art results. Our approach's computational overhead remains
significantly lower than conventional non-graph-based methodologies for
spatio-temporal data.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02609" title="Abstract">arXiv:2310.02609</a> [<a href="/pdf/2310.02609" title="Download PDF">pdf</a>, <a href="/format/2310.02609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RLTrace: Synthesizing High-Quality System Call Traces for OS Fuzz  Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huaijin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+W">Weixi Gu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Information Security Conference 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Securing operating system (OS) kernel is one central challenge in today's
cyber security landscape. The cutting-edge testing technique of OS kernel is
software fuzz testing. By mutating the program inputs with random variations
for iterations, fuzz testing aims to trigger program crashes and hangs caused
by potential bugs that can be abused by the inputs. To achieve high OS code
coverage, the de facto OS fuzzer typically composes system call traces as the
input seed to mutate and to interact with OS kernels. Hence, quality and
diversity of the employed system call traces become the prominent factor to
decide the effectiveness of OS fuzzing. However, these system call traces to
date are generated with hand-coded rules, or by analyzing system call logs of
OS utility programs. Our observation shows that such system call traces can
only subsume common usage scenarios of OS system calls, and likely omit hidden
bugs.
<br />In this research, we propose a deep reinforcement learning-based solution,
called RLTrace, to synthesize diverse and comprehensive system call traces as
the seed to fuzz OS kernels. During model training, the deep learning model
interacts with OS kernels and infers optimal system call traces w.r.t. our
learning goal -- maximizing kernel code coverage. Our evaluation shows that
RLTrace outperforms other seed generators by producing more comprehensive
system call traces, subsuming system call corner usage cases and subtle
dependencies. By feeding the de facto OS fuzzer, SYZKALLER, with system call
traces synthesized by RLTrace, we show that SYZKALLER can achieve higher code
coverage for testing Linux kernels. Furthermore, RLTrace found one
vulnerability in the Linux kernel (version 5.5-rc6), which is publicly unknown
to the best of our knowledge by the time of writing.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02611" title="Abstract">arXiv:2310.02611</a> [<a href="/pdf/2310.02611" title="Download PDF">pdf</a>, <a href="/format/2310.02611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing and Improving OT-based Adversarial Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaemoo Choi</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaewoong Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+M">Myungjoo Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Optimal Transport (OT) problem aims to find a transport plan that bridges two
distributions while minimizing a given cost function. OT theory has been widely
utilized in generative modeling. In the beginning, OT distance has been used as
a measure for assessing the distance between data and generated distributions.
Recently, OT transport map between data and prior distributions has been
utilized as a generative model. These OT-based generative models share a
similar adversarial training objective. In this paper, we begin by unifying
these OT-based adversarial methods within a single framework. Then, we
elucidate the role of each component in training dynamics through a
comprehensive analysis of this unified framework. Moreover, we suggest a simple
but novel method that improves the previously best-performing OT-based model.
Intuitively, our approach conducts a gradual refinement of the generated
distribution, progressively aligning it with the data distribution. Our
approach achieves a FID score of 2.51 on CIFAR-10, outperforming unified
OT-based adversarial approaches.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02612" title="Abstract">arXiv:2310.02612</a> [<a href="/pdf/2310.02612" title="Download PDF">pdf</a>, <a href="/format/2310.02612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Top-k contrast order-preserving pattern mining for time series  classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Youxi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Y">Yufei Meng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yan Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xingquan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Fournier-Viger%2C+P">Philippe Fournier-Viger</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xindong Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Recently, order-preserving pattern (OPP) mining, a new sequential pattern
mining method, has been proposed to mine frequent relative orders in a time
series. Although frequent relative orders can be used as features to classify a
time series, the mined patterns do not reflect the differences between two
classes of time series well. To effectively discover the differences between
time series, this paper addresses the top-k contrast OPP (COPP) mining and
proposes a COPP-Miner algorithm to discover the top-k contrast patterns as
features for time series classification, avoiding the problem of improper
parameter setting. COPP-Miner is composed of three parts: extreme point
extraction to reduce the length of the original time series, forward mining,
and reverse mining to discover COPPs. Forward mining contains three steps:
group pattern fusion strategy to generate candidate patterns, the support rate
calculation method to efficiently calculate the support of a pattern, and two
pruning strategies to further prune candidate patterns. Reverse mining uses one
pruning strategy to prune candidate patterns and consists of applying the same
process as forward mining. Experimental results validate the efficiency of the
proposed algorithm and show that top-k COPPs can be used as features to obtain
a better classification performance.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02614" title="Abstract">arXiv:2310.02614</a> [<a href="/pdf/2310.02614" title="Download PDF">pdf</a>, <a href="/ps/2310.02614" title="Download PostScript">ps</a>, <a href="/format/2310.02614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Quantified Observability Analysis in Multiagent Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mu%2C+C">Chunyan Mu</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+J">Jun Pang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">In multiagent systems (MASs), agents' observation upon system behaviours may
improve the overall team performance, but may also leak sensitive information
to an observer. A quantified observability analysis can thus be useful to
assist decision-making in MASs by operators seeking to optimise the
relationship between performance effectiveness and information exposure through
observations in practice. This paper presents a novel approach to
quantitatively analysing the observability properties in MASs. The concept of
opacity is applied to formally express the characterisation of observability in
MASs modelled as partially observable multiagent systems. We propose a temporal
logic oPATL to reason about agents' observability with quantitative goals,
which capture the probability of information transparency of system behaviours
to an observer, and develop verification techniques for quantitatively
analysing such properties. We implement the approach as an extension of the
PRISM model checker, and illustrate its applicability via several examples.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02619" title="Abstract">arXiv:2310.02619</a> [<a href="/pdf/2310.02619" title="Download PDF">pdf</a>, <a href="/format/2310.02619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Modeling of Regular and Irregular Time Series Data via  Koopman VAEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naiman%2C+I">Ilan Naiman</a>, 
<a href="/search/cs?searchtype=author&query=Erichson%2C+N+B">N. Benjamin Erichson</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+P">Pu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Mahoney%2C+M+W">Michael W. Mahoney</a>, 
<a href="/search/cs?searchtype=author&query=Azencot%2C+O">Omri Azencot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Generating realistic time series data is important for many engineering and
scientific applications. Existing work tackles this problem using generative
adversarial networks (GANs). However, GANs are often unstable during training,
and they can suffer from mode collapse. While variational autoencoders (VAEs)
are known to be more robust to these issues, they are (surprisingly) less often
considered for time series generation. In this work, we introduce Koopman VAE
(KVAE), a new generative framework that is based on a novel design for the
model prior, and that can be optimized for either regular and irregular
training data. Inspired by Koopman theory, we represent the latent conditional
prior dynamics using a linear map. Our approach enhances generative modeling
with two desired features: (i) incorporating domain knowledge can be achieved
by leverageing spectral tools that prescribe constraints on the eigenvalues of
the linear map; and (ii) studying the qualitative behavior and stablity of the
system can be performed using tools from dynamical systems theory. Our results
show that KVAE outperforms state-of-the-art GAN and VAE methods across several
challenging synthetic and real-world time series generation benchmarks. Whether
trained on regular or irregular data, KVAE generates time series that improve
both discriminative and predictive metrics. We also present visual evidence
suggesting that KVAE learns probability density functions that better
approximate empirical ground truth distributions.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02620" title="Abstract">arXiv:2310.02620</a> [<a href="/pdf/2310.02620" title="Download PDF">pdf</a>, <a href="/format/2310.02620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A priori error analysis of multirate time-stepping schemes for two-phase  flow problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Soszynska%2C+M">Martyn&#x105; Soszynska</a>, 
<a href="/search/math?searchtype=author&query=Richter%2C+T">Thomas Richter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present a priori error estimates for a multirate time-stepping scheme for
coupled differential equations. The discretization is based on Galerkin methods
in time using two different time meshes for two parts of the problem. We aim at
surface coupled multiphysics problems like two-phase flows. Special focus is on
the handling of the interface coupling to guarantee a coercive formulation as
key to optimal order error estimates.
<br />In a sequence of increasing complexity, we begin with the coupling of two
ordinary differential equations, coupled heat conduction equation, and finally
a coupled Stokes problem. For this we show optimal multi-rate estimates in
velocity and a suboptimal result in pressure.
<br />The a priori estimates prove that the multirate method decouples the two
subproblems exactly. This is the basis for adaptive methods which can choose
optimal lattices for the respective subproblems.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02622" title="Abstract">arXiv:2310.02622</a> [<a href="/pdf/2310.02622" title="Download PDF">pdf</a>, <a href="/ps/2310.02622" title="Download PostScript">ps</a>, <a href="/format/2310.02622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral vs Energy Efficiency in 6G: Impact of the Receiver Front-End
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lozano%2C+A">Angel Lozano</a>, 
<a href="/search/cs?searchtype=author&query=Rangan%2C+S">Sundeep Rangan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This article puts the spotlight on the receiver front-end (RFE), an integral
part of any wireless device that information theory typically idealizes into a
mere addition of noise. While this idealization was sound in the past, as
operating frequencies, bandwidths, and antenna counts rise, a soaring amount of
power is required for the RFE to behave accordingly. Containing this surge in
power expenditure exposes a harsher behavior on the part of the RFE (more
noise, nonlinearities, and coarse quantization), setting up a tradeoff between
the spectral efficiency under such nonidealities and the efficiency in the use
of energy by the RFE. With the urge for radically better power consumptions and
energy efficiencies in 6G, this emerges as an issue on which information theory
can cast light at a fundamental level. More broadly, this article advocates the
interest of having information theory embrace the device power consumption in
its analyses. In turn, this calls for new models and abstractions such as the
ones herein put together for the RFE, and for a more holistic perspective.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02623" title="Abstract">arXiv:2310.02623</a> [<a href="/pdf/2310.02623" title="Download PDF">pdf</a>, <a href="/format/2310.02623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability Analysis of Hypersampled Model Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gautam%2C+Y">Yaashia Gautam</a>, 
<a href="/search/eess?searchtype=author&query=Nicotra%2C+M+M">Marco M. Nicotra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to American Control Conference,2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper introduces a new framework for analyzing the stability of
discrete-time model predictive controllers acting on continuous-time systems.
The proposed framework introduces the distinction between discretization time
(used to generate the optimal control problem) and sampling time (used to
implement the controller). The paper not only shows that these two time
constants are independent, but also motivates the benefits of selecting a
sampling time that is smaller than the discretization time. The resulting
approach, hereafter referred to as Hypersampled Model Predictive Control,
overcomes the traditional trade-off between performance and computational
complexity that arises when selecting the sampling time of traditional
discrete-time model predictive controllers.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02625" title="Abstract">arXiv:2310.02625</a> [<a href="/pdf/2310.02625" title="Download PDF">pdf</a>, <a href="/format/2310.02625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Spatio-Temporal Voxels Based Trajectory Planning for Autonomous  Driving in Highway Traffic Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jian%2C+Z">Zhiqiang Jian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Songyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lingfeng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wei Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nanning Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE ITSC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Trajectory planning is crucial for the safe driving of autonomous vehicles in
highway traffic flow. Currently, some advanced trajectory planning methods
utilize spatio-temporal voxels to construct feasible regions and then convert
trajectory planning into optimization problem solving based on the feasible
regions. However, these feasible region construction methods cannot adapt to
the changes in dynamic environments, making them difficult to apply in complex
traffic flow. In this paper, we propose a trajectory planning method based on
adaptive spatio-temporal voxels which improves the construction of feasible
regions and trajectory optimization while maintaining the quadratic programming
form. The method can adjust feasible regions and trajectory planning according
to real-time traffic flow and environmental changes, realizing vehicles to
drive safely in complex traffic flow. The proposed method has been tested in
both open-loop and closed-loop environments, and the test results show that our
method outperforms the current planning methods.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02629" title="Abstract">arXiv:2310.02629</a> [<a href="/pdf/2310.02629" title="Download PDF">pdf</a>, <a href="/format/2310.02629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BA-MoE: Boundary-Aware Mixture-of-Experts Adapter for Code-Switching  Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Peikun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+Y">Yuhao Lian</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+H">Hongfei Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xucheng Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Naijun Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lei Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages,3figures,Accept by ASRU2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Mixture-of-experts based models, which use language experts to extract
language-specific representations effectively, have been well applied in
code-switching automatic speech recognition. However, there is still
substantial space to improve as similar pronunciation across languages may
result in ineffective multi-language modeling and inaccurate language boundary
estimation. To eliminate these drawbacks, we propose a cross-layer language
adapter and a boundary-aware training method, namely Boundary-Aware
Mixture-of-Experts (BA-MoE). Specifically, we introduce language-specific
adapters to separate language-specific representations and a unified gating
layer to fuse representations within each encoder layer. Second, we compute
language adaptation loss of the mean output of each language-specific adapter
to improve the adapter module's language-specific representation learning.
Besides, we utilize a boundary-aware predictor to learn boundary
representations for dealing with language boundary confusion. Our approach
achieves significant performance improvement, reducing the mixture error rate
by 16.55\% compared to the baseline on the ASRU 2019 Mandarin-English
code-switching challenge dataset.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02633" title="Abstract">arXiv:2310.02633</a> [<a href="/pdf/2310.02633" title="Download PDF">pdf</a>, <a href="/format/2310.02633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-rules mining algorithm for combinatorially exploded decision trees  with modified Aitchison-Aitken function-based Bayesian optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Omae%2C+Y">Yuto Omae</a>, 
<a href="/search/cs?searchtype=author&query=Mori%2C+M">Masaya Mori</a>, 
<a href="/search/cs?searchtype=author&query=Kakimoto%2C+Y">Yohei Kakimoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Decision trees offer the benefit of easy interpretation because they allow
the classification of input data based on if--then rules. However, as decision
trees are constructed by an algorithm that achieves clear classification with
minimum necessary rules, the trees possess the drawback of extracting only
minimum rules, even when various latent rules exist in data. Approaches that
construct multiple trees using randomly selected feature subsets do exist.
However, the number of trees that can be constructed remains at the same scale
because the number of feature subsets is a combinatorial explosion.
Additionally, when multiple trees are constructed, numerous rules are
generated, of which several are untrustworthy and/or highly similar. Therefore,
we propose "MAABO-MT" and "GS-MRM" algorithms that strategically construct
trees with high estimation performance among all possible trees with small
computational complexity and extract only reliable and non-similar rules,
respectively. Experiments are conducted using several open datasets to analyze
the effectiveness of the proposed method. The results confirm that MAABO-MT can
discover reliable rules at a lower computational cost than other methods that
rely on randomness. Furthermore, the proposed method is confirmed to provide
deeper insights than single decision trees commonly used in previous studies.
Therefore, MAABO-MT and GS-MRM can efficiently extract rules from
combinatorially exploded decision trees.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02635" title="Abstract">arXiv:2310.02635</a> [<a href="/pdf/2310.02635" title="Download PDF">pdf</a>, <a href="/format/2310.02635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundation Reinforcement Learning: towards Embodied Generalist Agents  with Foundation Prior Assistance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Weirui Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunsheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengchen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shengjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+X">Xianfan Gu</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, people have shown that large-scale pre-training from internet-scale
data is the key to building generalist models, as witnessed in NLP. To build
embodied generalist agents, we and many other researchers hypothesize that such
foundation prior is also an indispensable component. However, it is unclear
what is the proper concrete form to represent those embodied foundation priors
and how they should be used in the downstream task. In this paper, we propose
an intuitive and effective set of embodied priors that consist of foundation
policy, value, and success reward. The proposed priors are based on the
goal-conditioned MDP. To verify their effectiveness, we instantiate an
actor-critic method assisted by the priors, called Foundation Actor-Critic
(FAC). We name our framework as Foundation Reinforcement Learning (FRL), since
it completely relies on embodied foundation priors to explore, learn and
reinforce. The benefits of FRL are threefold. (1) Sample efficient. With
foundation priors, FAC learns significantly faster than traditional RL. Our
evaluation on the Meta-World has proved that FAC can achieve 100% success rates
for 7/8 tasks under less than 200k frames, which outperforms the baseline
method with careful manual-designed rewards under 1M frames. (2) Robust to
noisy priors. Our method tolerates the unavoidable noise in embodied foundation
models. We show that FAC works well even under heavy noise or quantization
errors. (3) Minimal human intervention: FAC completely learns from the
foundation priors, without the need of human-specified dense reward, or
providing teleoperated demos. Thus, FAC can be easily scaled up. We believe our
FRL framework could enable the future robot to autonomously explore and learn
without human intervention in the physical world. In summary, our proposed FRL
is a novel and powerful learning paradigm, towards achieving embodied
generalist agents.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02637" title="Abstract">arXiv:2310.02637</a> [<a href="/pdf/2310.02637" title="Download PDF">pdf</a>, <a href="/format/2310.02637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometric Assignment and Geometric Bottleneck
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cabello%2C+S">Sergio Cabello</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Siu-Wing Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Cheong%2C+O">Otfried Cheong</a>, 
<a href="/search/cs?searchtype=author&query=Knauer%2C+C">Christian Knauer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">Let $P$ be a set of at most $n$ points and let $R$ be a set of at most $n$
geometric ranges, such as for example disks or rectangles, where each $p \in P$
has an associated supply $s_{p} &gt; 0$, and each $r \in R$ has an associated
demand $d_{r} &gt; 0$. An assignment is a set $\mathcal{A}$ of ordered triples
$(p,r,a_{pr}) \in P \times R \times \mathbb{R}_{&gt;0}$ such that $p \in r$. We
show how to compute a maximum assignment that satisfies the constraints given
by the supplies and demands.
<br />Using our techniques, we can also solve minimum bottleneck problems, such as
computing a perfect matching between a set of $n$ red points~$P$ and $n$ blue
points $Q$ that minimizes the length of the longest edge. For the
$L_\infty$-metric, we can do this in time $O(n^{1+\varepsilon})$ in any fixed
dimension, for the $L_2$-metric in the plane in time $O(n^{4/3 +
\varepsilon})$, for any $\varepsilon &gt; 0$.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02638" title="Abstract">arXiv:2310.02638</a> [<a href="/pdf/2310.02638" title="Download PDF">pdf</a>, <a href="/format/2310.02638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> P2CADNet: An End-to-End Reconstruction Network for Parametric 3D CAD  Model from Point Clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zong%2C+Z">Zhihao Zong</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+F">Fazhi He</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+R">Rubin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuxin Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Computer Aided Design (CAD), especially the feature-based parametric CAD,
plays an important role in modern industry and society. However, the
reconstruction of featured CAD model is more challenging than the
reconstruction of other CAD models. To this end, this paper proposes an
end-to-end network to reconstruct featured CAD model from point cloud
(P2CADNet). Initially, the proposed P2CADNet architecture combines a point
cloud feature extractor, a CAD sequence reconstructor and a parameter
optimizer. Subsequently, in order to reconstruct the featured CAD model in an
autoregressive way, the CAD sequence reconstructor applies two transformer
decoders, one with target mask and the other without mask. Finally, for
predicting parameters more precisely, we design a parameter optimizer with
cross-attention mechanism to further refine the CAD feature parameters. We
evaluate P2CADNet on the public dataset, and the experimental results show that
P2CADNet has excellent reconstruction quality and accuracy. To our best
knowledge, P2CADNet is the first end-to-end network to reconstruct featured CAD
model from point cloud, and can be regarded as baseline for future works.
Therefore, we open the source code at https://github.com/Blice0415/P2CADNet.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02641" title="Abstract">arXiv:2310.02641</a> [<a href="/pdf/2310.02641" title="Download PDF">pdf</a>, <a href="/format/2310.02641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deformation-Invariant Neural Network and Its Applications in Distorted  Image Restoration and Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qiguang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lui%2C+L+M">Lok Ming Lui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Images degraded by geometric distortions pose a significant challenge to
imaging and computer vision tasks such as object recognition. Deep
learning-based imaging models usually fail to give accurate performance for
geometrically distorted images. In this paper, we propose the
deformation-invariant neural network (DINN), a framework to address the problem
of imaging tasks for geometrically distorted images. The DINN outputs
consistent latent features for images that are geometrically distorted but
represent the same underlying object or scene. The idea of DINN is to
incorporate a simple component, called the quasiconformal transformer network
(QCTN), into other existing deep networks for imaging tasks. The QCTN is a deep
neural network that outputs a quasiconformal map, which can be used to
transform a geometrically distorted image into an improved version that is
closer to the distribution of natural or good images. It first outputs a
Beltrami coefficient, which measures the quasiconformality of the output
deformation map. By controlling the Beltrami coefficient, the local geometric
distortion under the quasiconformal mapping can be controlled. The QCTN is
lightweight and simple, which can be readily integrated into other existing
deep neural networks to enhance their performance. Leveraging our framework, we
have developed an image classification network that achieves accurate
classification of distorted images. Our proposed framework has been applied to
restore geometrically distorted images by atmospheric turbulence and water
turbulence. DINN outperforms existing GAN-based restoration methods under these
scenarios, demonstrating the effectiveness of the proposed framework.
Additionally, we apply our proposed framework to the 1-1 verification of human
face images under atmospheric turbulence and achieve satisfactory performance,
further demonstrating the efficacy of our approach.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02642" title="Abstract">arXiv:2310.02642</a> [<a href="/pdf/2310.02642" title="Download PDF">pdf</a>, <a href="/format/2310.02642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GET: Group Event Transformer for Event-Based Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yansong Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yueyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zhiwei Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaoyan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Feng Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted by ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Event cameras are a type of novel neuromorphic sen-sor that has been gaining
increasing attention. Existing event-based backbones mainly rely on image-based
designs to extract spatial information within the image transformed from
events, overlooking important event properties like time and polarity. To
address this issue, we propose a novel Group-based vision Transformer backbone
for Event-based vision, called Group Event Transformer (GET), which de-couples
temporal-polarity information from spatial infor-mation throughout the feature
extraction process. Specifi-cally, we first propose a new event representation
for GET, named Group Token, which groups asynchronous events based on their
timestamps and polarities. Then, GET ap-plies the Event Dual Self-Attention
block, and Group Token Aggregation module to facilitate effective feature
commu-nication and integration in both the spatial and temporal-polarity
domains. After that, GET can be integrated with different downstream tasks by
connecting it with vari-ous heads. We evaluate our method on four event-based
classification datasets (Cifar10-DVS, N-MNIST, N-CARS, and DVS128Gesture) and
two event-based object detection datasets (1Mpx and Gen1), and the results
demonstrate that GET outperforms other state-of-the-art methods. The code is
available at https://github.com/Peterande/GET-Group-Event-Transformer.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02643" title="Abstract">arXiv:2310.02643</a> [<a href="/pdf/2310.02643" title="Download PDF">pdf</a>, <a href="/ps/2310.02643" title="Download PostScript">ps</a>, <a href="/format/2310.02643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Algorithms for Spectral Hypergraph Sparsification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soma%2C+T">Tasuku Soma</a>, 
<a href="/search/cs?searchtype=author&query=Tung%2C+K+C">Kam Chuen Tung</a>, 
<a href="/search/cs?searchtype=author&query=Yoshida%2C+Y">Yuichi Yoshida</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We provide the first online algorithm for spectral hypergraph sparsification.
In the online setting, hyperedges with positive weights are arriving in a
stream, and upon the arrival of each hyperedge, we must irrevocably decide
whether or not to include it in the sparsifier. Our algorithm produces an
$(\epsilon, \delta)$-spectral sparsifier with multiplicative error $\epsilon$
and additive error $\delta$ that has $O(\epsilon^{-2} n (\log n)^2 \log(1 +
\epsilon W/\delta n))$ hyperedges with high probability, where $\epsilon,
\delta \in (0,1)$, $n$ is the number of nodes, and $W$ is the sum of edge
weights. The space complexity of our algorithm is $O(n^2)$, while previous
algorithms require the space complexity of $\Omega(m)$, where $m$ is the number
of hyperedges. This provides an exponential improvement in the space complexity
since $m$ can be exponential in $n$.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02648" title="Abstract">arXiv:2310.02648</a> [<a href="/pdf/2310.02648" title="Download PDF">pdf</a>, <a href="/format/2310.02648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long-Term Dynamic Window Approach for Kinodynamic Local Planning in  Static and Crowd Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jian%2C+Z">Zhiqiang Jian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Songyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lingfeng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wei Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nanning Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE RA-L
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Local planning for a differential wheeled robot is designed to generate
kinodynamic feasible actions that guide the robot to a goal position along the
navigation path while avoiding obstacles. Reactive, predictive, and
learning-based methods are widely used in local planning. However, few of them
can fit static and crowd environments while satisfying kinodynamic constraints
simultaneously. To solve this problem, we propose a novel local planning
method. The method applies a long-term dynamic window approach to generate an
initial trajectory and then optimizes it with graph optimization. The method
can plan actions under the robot's kinodynamic constraints in real time while
allowing the generated actions to be safer and more jitterless. Experimental
results show that the proposed method adapts well to crowd and static
environments and outperforms most SOTA approaches.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02650" title="Abstract">arXiv:2310.02650</a> [<a href="/pdf/2310.02650" title="Download PDF">pdf</a>, <a href="/format/2310.02650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Visual Localization for Multi-Agent Collaboration: A Data-Driven  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanlon%2C+M">Matthew Hanlon</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Boyang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Pollefeys%2C+M">Marc Pollefeys</a>, 
<a href="/search/cs?searchtype=author&query=Blum%2C+H">Hermann Blum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Rather than having each newly deployed robot create its own map of its
surroundings, the growing availability of SLAM-enabled devices provides the
option of simply localizing in a map of another robot or device. In cases such
as multi-robot or human-robot collaboration, localizing all agents in the same
map is even necessary. However, localizing e.g. a ground robot in the map of a
drone or head-mounted MR headset presents unique challenges due to viewpoint
changes. This work investigates how active visual localization can be used to
overcome such challenges of viewpoint changes. Specifically, we focus on the
problem of selecting the optimal viewpoint at a given location. We compare
existing approaches in the literature with additional proposed baselines and
propose a novel data-driven approach. The result demonstrates the superior
performance of the data-driven approach when compared to existing methods, both
in controlled simulation experiments and real-world deployment.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02651" title="Abstract">arXiv:2310.02651</a> [<a href="/pdf/2310.02651" title="Download PDF">pdf</a>, <a href="/format/2310.02651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hire When You Need to: Gradual Participant Recruitment for Auction-based  Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xavier Tan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Han Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 Pages, 3 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">The success of federated Learning (FL) depends on the quantity and quality of
the data owners (DOs) as well as their motivation to join FL model training.
Reputation-based FL participant selection methods have been proposed. However,
they still face the challenges of the cold start problem and potential
selection bias towards highly reputable DOs. Such a bias can result in lower
reputation DOs being prematurely excluded from future FL training rounds,
thereby reducing the diversity of training data and the generalizability of the
resulting models. To address these challenges, we propose the Gradual
Participant Selection scheme for Auction-based Federated Learning (GPS-AFL).
Unlike existing AFL incentive mechanisms which generally assume that all DOs
required for an FL task must be selected in one go, GPS-AFL gradually selects
the required DOs over multiple rounds of training as more information is
revealed through repeated interactions. It is designed to strike a balance
between cost saving and performance enhancement, while mitigating the drawbacks
of selection bias in reputation-based FL. Extensive experiments based on
real-world datasets demonstrate the significant advantages of GPS-AFL, which
reduces costs by 33.65% and improved total utility by 2.91%, on average
compared to the best-performing state-of-the-art approach.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02654" title="Abstract">arXiv:2310.02654</a> [<a href="/pdf/2310.02654" title="Download PDF">pdf</a>, <a href="/format/2310.02654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Study of Quantisation-aware Training on Time Series Transformer Models  for Resource-constrained FPGAs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+T">Tianheng Ling</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chao Qian</a>, 
<a href="/search/cs?searchtype=author&query=Einhaus%2C+L">Lukas Einhaus</a>, 
<a href="/search/cs?searchtype=author&query=Schiele%2C+G">Gregor Schiele</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR)

</div>
<p class="mathjax">This study explores the quantisation-aware training (QAT) on time series
Transformer models. We propose a novel adaptive quantisation scheme that
dynamically selects between symmetric and asymmetric schemes during the QAT
phase. Our approach demonstrates that matching the quantisation scheme to the
real data distribution can reduce computational overhead while maintaining
acceptable precision. Moreover, our approach is robust when applied to
real-world data and mixed-precision quantisation, where most objects are
quantised to 4 bits. Our findings inform model quantisation and deployment
decisions while providing a foundation for advancing quantisation techniques.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02655" title="Abstract">arXiv:2310.02655</a> [<a href="/pdf/2310.02655" title="Download PDF">pdf</a>, <a href="/format/2310.02655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AGIR: Automating Cyber Threat Intelligence Reporting with Natural  Language Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perrina%2C+F">Filippo Perrina</a>, 
<a href="/search/cs?searchtype=author&query=Marchiori%2C+F">Francesco Marchiori</a>, 
<a href="/search/cs?searchtype=author&query=Conti%2C+M">Mauro Conti</a>, 
<a href="/search/cs?searchtype=author&query=Verde%2C+N+V">Nino Vincenzo Verde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Cyber Threat Intelligence (CTI) reporting is pivotal in contemporary risk
management strategies. As the volume of CTI reports continues to surge, the
demand for automated tools to streamline report generation becomes increasingly
apparent. While Natural Language Processing techniques have shown potential in
handling text data, they often struggle to address the complexity of diverse
data sources and their intricate interrelationships. Moreover, established
paradigms like STIX have emerged as de facto standards within the CTI
community, emphasizing the formal categorization of entities and relations to
facilitate consistent data sharing. In this paper, we introduce AGIR (Automatic
Generation of Intelligence Reports), a transformative Natural Language
Generation tool specifically designed to address the pressing challenges in the
realm of CTI reporting. AGIR's primary objective is to empower security
analysts by automating the labor-intensive task of generating comprehensive
intelligence reports from formal representations of entity graphs. AGIR
utilizes a two-stage pipeline by combining the advantages of template-based
approaches and the capabilities of Large Language Models such as ChatGPT. We
evaluate AGIR's report generation capabilities both quantitatively and
qualitatively. The generated reports accurately convey information expressed
through formal language, achieving a high recall value (0.99) without
introducing hallucination. Furthermore, we compare the fluency and utility of
the reports with state-of-the-art approaches, showing how AGIR achieves higher
scores in terms of Syntactic Log-Odds Ratio (SLOR) and through questionnaires.
By using our tool, we estimate that the report writing time is reduced by more
than 40%, therefore streamlining the CTI production of any organization and
contributing to the automation of several CTI tasks.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02656" title="Abstract">arXiv:2310.02656</a> [<a href="/pdf/2310.02656" title="Download PDF">pdf</a>, <a href="/format/2310.02656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blend: A Unified Data Discovery System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esmailoghli%2C+M">Mahdi Esmailoghli</a>, 
<a href="/search/cs?searchtype=author&query=Schnell%2C+C">Christoph Schnell</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+R+J">Ren&#xe9;e J. Miller</a>, 
<a href="/search/cs?searchtype=author&query=Abedjan%2C+Z">Ziawasch Abedjan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Data discovery is an iterative and incremental process that necessitates the
execution of multiple data discovery queries to identify the desired tables
from large and diverse data lakes. Current methodologies concentrate on single
discovery tasks such as join, correlation, or union discovery. However, in
practice, a series of these approaches and their corresponding index structures
are necessary to enable the user to discover the desired tables. This paper
presents BLEND, a comprehensive data discovery system that empowers users to
develop ad-hoc discovery tasks without the need to develop new algorithms or
build a new index structure. To achieve this goal, we introduce a general index
structure capable of addressing multiple discovery queries. We develop a set of
lower-level operators that serve as the fundamental building blocks for more
complex and sophisticated user tasks. These operators are highly efficient and
enable end-to-end efficiency. To enhance the execution of the discovery
pipeline, we rewrite the search queries into optimized SQL statements to push
the data operators down to the database. We demonstrate that our holistic
system is able to achieve comparable effectiveness and runtime efficiency to
the individual state-of-the-art approaches specifically designed for a single
task.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02658" title="Abstract">arXiv:2310.02658</a> [<a href="/pdf/2310.02658" title="Download PDF">pdf</a>, <a href="/format/2310.02658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Multi-Configuration Problems: A Performance Analysis with Choco  Solver
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ritz%2C+B">Benjamin Ritz</a>, 
<a href="/search/cs?searchtype=author&query=Felfernig%2C+A">Alexander Felfernig</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+V">Viet-Man Le</a>, 
<a href="/search/cs?searchtype=author&query=Lubos%2C+S">Sebastian Lubos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint of the paper to be presented at ConfWS'23: 25th International Workshop on Configuration, September 6-7, 2023, M\'alaga, Spain. The paper will be published in the workshop proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In many scenarios, configurators support the configuration of a solution that
satisfies the preferences of a single user. The concept of
\emph{multi-configuration} is based on the idea of configuring a set of
configurations. Such a functionality is relevant in scenarios such as the
configuration of personalized exams, the configuration of project teams, and
the configuration of different trips for individual members of a tourist group
(e.g., when visiting a specific city). In this paper, we exemplify the
application of multi-configuration for generating individualized exams. We also
provide a constraint solver performance analysis which helps to gain some
insights into corresponding performance issues.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02663" title="Abstract">arXiv:2310.02663</a> [<a href="/pdf/2310.02663" title="Download PDF">pdf</a>, <a href="/format/2310.02663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MedPrompt: Cross-Modal Prompting for Multi-Task Medical Image  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuhang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pun%2C+C">Chi-Man Pun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuqiang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Cross-modal medical image translation is an essential task for synthesizing
missing modality data for clinical diagnosis. However, current learning-based
techniques have limitations in capturing cross-modal and global features,
restricting their suitability to specific pairs of modalities. This lack of
versatility undermines their practical usefulness, particularly considering
that the missing modality may vary for different cases. In this study, we
present MedPrompt, a multi-task framework that efficiently translates different
modalities. Specifically, we propose the Self-adaptive Prompt Block, which
dynamically guides the translation network towards distinct modalities. Within
this framework, we introduce the Prompt Extraction Block and the Prompt Fusion
Block to efficiently encode the cross-modal prompt. To enhance the extraction
of global features across diverse modalities, we incorporate the Transformer
model. Extensive experimental results involving five datasets and four pairs of
modalities demonstrate that our proposed model achieves state-of-the-art visual
quality and exhibits excellent generalization capability.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02664" title="Abstract">arXiv:2310.02664</a> [<a href="/pdf/2310.02664" title="Download PDF">pdf</a>, <a href="/format/2310.02664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Memorization in Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+X">Xiangming Gu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chao Du</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+T">Tianyu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chongxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Min Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ye Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Due to their capacity to generate novel and high-quality samples, diffusion
models have attracted significant research interest in recent years. Notably,
the typical training objective of diffusion models, i.e., denoising score
matching, has a closed-form optimal solution that can only generate training
data replicating samples. This indicates that a memorization behavior is
theoretically expected, which contradicts the common generalization ability of
state-of-the-art diffusion models, and thus calls for a deeper understanding.
Looking into this, we first observe that memorization behaviors tend to occur
on smaller-sized datasets, which motivates our definition of effective model
memorization (EMM), a metric measuring the maximum size of training data at
which a learned diffusion model approximates its theoretical optimum. Then, we
quantify the impact of the influential factors on these memorization behaviors
in terms of EMM, focusing primarily on data distribution, model configuration,
and training procedure. Besides comprehensive empirical results identifying the
influential factors, we surprisingly find that conditioning training data on
uninformative random labels can significantly trigger the memorization in
diffusion models. Our study holds practical significance for diffusion model
users and offers clues to theoretical research in deep generative models. Code
is available at https://github.com/sail-sg/DiffMemorize.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02670" title="Abstract">arXiv:2310.02670</a> [<a href="/pdf/2310.02670" title="Download PDF">pdf</a>, <a href="/format/2310.02670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Searching 2D-Strings for Matching Frames
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boneh%2C+I">Itai Boneh</a>, 
<a href="/search/cs?searchtype=author&query=Fried%2C+D">Dvir Fried</a>, 
<a href="/search/cs?searchtype=author&query=Golan%2C+S">Shay Golan</a>, 
<a href="/search/cs?searchtype=author&query=Kraus%2C+M">Matan Kraus</a>, 
<a href="/search/cs?searchtype=author&query=Miclaus%2C+A">Adrian Miclaus</a>, 
<a href="/search/cs?searchtype=author&query=Shur%2C+A">Arseny Shur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We introduce the natural notion of a matching frame in a $2$-dimensional
string. A matching frame in a $2$-dimensional $n\times m$ string $M$, is a
rectangle such that the strings written on the horizontal sides of the
rectangle are identical, and so are the strings written on the vertical sides
of the rectangle. Formally, a matching frame in $M$ is a tuple $(u,d,\ell,r)$
such that $M[u][\ell ..r] = M[d][\ell ..r]$ and $M[u..d][\ell] = M[u..d][r]$.
<br />In this paper, we present an algorithm for finding the maximum perimeter
matching frame in a matrix $M$ in $\tilde{O}(n^{2.5})$ time (assuming $n \ge
m)$. Additionally, for every constant $\epsilon&gt; 0$ we present a near-linear
$(1-\epsilon)$-approximation algorithm for the maximum perimeter of a matching
frame.
<br />In the development of the aforementioned algorithms, we introduce inventive
technical elements and uncover distinctive structural properties that we
believe will captivate the curiosity of the community.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02674" title="Abstract">arXiv:2310.02674</a> [<a href="/pdf/2310.02674" title="Download PDF">pdf</a>, <a href="/format/2310.02674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Land-cover change detection using paired OpenStreetMap data and optical  high-resolution imagery via object-guided Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongruixuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+C">Cuiling Lan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jian Song</a>, 
<a href="/search/cs?searchtype=author&query=Broni-Bediako%2C+C">Clifford Broni-Bediako</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+J">Junshi Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yokoya%2C+N">Naoto Yokoya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Multimedia (cs.MM)

</div>
<p class="mathjax">Optical high-resolution imagery and OpenStreetMap (OSM) data are two
important data sources for land-cover change detection. Previous studies in
these two data sources focus on utilizing the information in OSM data to aid
the change detection on multi-temporal optical high-resolution images. This
paper pioneers the direct detection of land-cover changes utilizing paired OSM
data and optical imagery, thereby broadening the horizons of change detection
tasks to encompass more dynamic earth observations. To this end, we propose an
object-guided Transformer (ObjFormer) architecture by naturally combining the
prevalent object-based image analysis (OBIA) technique with the advanced vision
Transformer architecture. The introduction of OBIA can significantly reduce the
computational overhead and memory burden in the self-attention module.
Specifically, the proposed ObjFormer has a hierarchical pseudo-siamese encoder
consisting of object-guided self-attention modules that extract representative
features of different levels from OSM data and optical images; a decoder
consisting of object-guided cross-attention modules can progressively recover
the land-cover changes from the extracted heterogeneous features. In addition
to the basic supervised binary change detection task, this paper raises a new
semi-supervised semantic change detection task that does not require any
manually annotated land-cover labels of optical images to train semantic change
detectors. Two lightweight semantic decoders are added to ObjFormer to
accomplish this task efficiently. A converse cross-entropy loss is designed to
fully utilize the negative samples, thereby contributing to the great
performance improvement in this task. The first large-scale benchmark dataset
containing 1,287 map-image pairs (1024$\times$ 1024 pixels for each sample)
covering 40 regions on six continents ...(see the manuscript for the full
abstract)
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02676" title="Abstract">arXiv:2310.02676</a> [<a href="/pdf/2310.02676" title="Download PDF">pdf</a>, <a href="/format/2310.02676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PostRainBench: A comprehensive benchmark and a new model for  precipitation forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yujin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiaming Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xiang Pan</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Z">Zeying Gong</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Junwei Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures. arXiv admin note: text overlap with <a href="/abs/2105.05537">arXiv:2105.05537</a>, <a href="/abs/2206.15241">arXiv:2206.15241</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Accurate precipitation forecasting is a vital challenge of both scientific
and societal importance. Data-driven approaches have emerged as a widely used
solution for addressing this challenge. However, solely relying on data-driven
approaches has limitations in modeling the underlying physics, making accurate
predictions difficult. Coupling AI-based post-processing techniques with
traditional Numerical Weather Prediction (NWP) methods offers a more effective
solution for improving forecasting accuracy. Despite previous post-processing
efforts, accurately predicting heavy rainfall remains challenging due to the
imbalanced precipitation data across locations and complex relationships
between multiple meteorological variables. To address these limitations, we
introduce the PostRainBench, a comprehensive multi-variable NWP post-processing
benchmark consisting of three datasets for NWP post-processing-based
precipitation forecasting. We propose CAMT, a simple yet effective Channel
Attention Enhanced Multi-task Learning framework with a specially designed
weighted loss function. Its flexible design allows for easy plug-and-play
integration with various backbones. Extensive experimental results on the
proposed benchmark show that our method outperforms state-of-the-art methods by
6.3%, 4.7%, and 26.8% in rain CSI on the three datasets respectively. Most
notably, our model is the first deep learning-based method to outperform
traditional Numerical Weather Prediction (NWP) approaches in extreme
precipitation conditions. It shows improvements of 15.6%, 17.4%, and 31.8% over
NWP predictions in heavy rain CSI on respective datasets. These results
highlight the potential impact of our model in reducing the severe consequences
of extreme weather events.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02678" title="Abstract">arXiv:2310.02678</a> [<a href="/pdf/2310.02678" title="Download PDF">pdf</a>, <a href="/format/2310.02678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Gimbal: A 3 Degrees of Freedom Open Source Sensing and Testing  Platform for Nano and Micro UAVs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sharma%2C+S">Suryansh Sharma</a>, 
<a href="/search/eess?searchtype=author&query=Dijkstra%2C+T">Tristan Dijkstra</a>, 
<a href="/search/eess?searchtype=author&query=Prasad%2C+R+V">R. Venkatesha Prasad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Link to open source repository: <a href="https://doi.org/10.5281/zenodo.8052218">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> in IEEE Sensors Letters, vol. 7, no. 9, pp. 1-4, Sept. 2023, Art
  no. 2502704
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Testing the aerodynamics of micro- and nano-UAVs without actually flying is
highly challenging. To address this issue, we introduce Open Gimbal, a
specially designed 3 Degrees of Freedom platform that caters to the unique
requirements of micro- and nano-UAVs. This platform allows for unrestricted and
free rotational motion, enabling comprehensive experimentation and evaluation
of these UAVs. Our approach focuses on simplicity and accessibility. We
developed an open-source, 3D printable electro-mechanical design that has
minimal size and low complexity. This design facilitates easy replication and
customization, making it widely accessible to researchers and developers.
Addressing the challenges of sensing flight dynamics at a small scale, we have
devised an integrated wireless batteryless sensor subsystem. Our innovative
solution eliminates the need for complex wiring and instead uses wireless power
transfer for sensor data reception. To validate the effectiveness of open
gimbal, we thoroughly evaluate and test its communication link and sensing
performance using a typical nano-quadrotor. Through comprehensive testing, we
verify the reliability and accuracy of open gimbal in real-world scenarios.
These advancements provide valuable tools and insights for researchers and
developers working with mUAVs and nUAVs, contributing to the progress of this
rapidly evolving field.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02679" title="Abstract">arXiv:2310.02679</a> [<a href="/pdf/2310.02679" title="Download PDF">pdf</a>, <a href="/format/2310.02679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Generative Flow Samplers: Improving learning signals through  partial trajectory optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dinghuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R+T+Q">Ricky Tian Qi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Cheng-Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Courville%2C+A">Aaron Courville</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation (stat.CO); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">We tackle the problem of sampling from intractable high-dimensional density
functions, a fundamental task that often appears in machine learning and
statistics. We extend recent sampling-based approaches that leverage controlled
stochastic processes to model approximate samples from these target densities.
The main drawback of these approaches is that the training objective requires
full trajectories to compute, resulting in sluggish credit assignment issues
due to use of entire trajectories and a learning signal present only at the
terminal time. In this work, we present Diffusion Generative Flow Samplers
(DGFS), a sampling-based framework where the learning process can be tractably
broken down into short partial trajectory segments, via parameterizing an
additional "flow function". Our method takes inspiration from the theory
developed for generative flow networks (GFlowNets), allowing us to make use of
intermediate learning signals and benefit from off-policy exploration
capabilities. Through a variety of challenging experiments, we demonstrate that
DGFS results in more accurate estimates of the normalization constant than
closely-related prior methods.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02687" title="Abstract">arXiv:2310.02687</a> [<a href="/pdf/2310.02687" title="Download PDF">pdf</a>, <a href="/format/2310.02687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Moyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lingzhe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+B">Bangyan Liao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peidong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Neural Radiance Fields (NeRF) has received much attention recently due to its
impressive capability to represent 3D scene and synthesize novel view images.
Existing works usually assume that the input images are captured by a global
shutter camera. Thus, rolling shutter (RS) images cannot be trivially applied
to an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter
effect would also affect the accuracy of the camera pose estimation (e.g. via
COLMAP), which further prevents the success of NeRF algorithm with RS images.
In this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance
Fields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and
recover accurate camera motion trajectory simultaneously under the framework of
NeRF, by modeling the physical image formation process of a RS camera.
Experimental results demonstrate that USB-NeRF achieves better performance
compared to prior works, in terms of RS effect removal, novel view image
synthesis as well as camera motion estimation. Furthermore, our algorithm can
also be used to recover high-fidelity high frame-rate global shutter video from
a sequence of RS images.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02691" title="Abstract">arXiv:2310.02691</a> [<a href="/pdf/2310.02691" title="Download PDF">pdf</a>, <a href="/format/2310.02691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Ocean Subgrid-Scale Parameterizations Using Fourier Neural  Operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mangeleer%2C+V">Victor Mangeleer</a>, 
<a href="/search/cs?searchtype=author&query=Louppe%2C+G">Gilles Louppe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">In climate simulations, small-scale processes shape ocean dynamics but remain
computationally expensive to resolve directly. For this reason, their
contributions are commonly approximated using empirical parameterizations,
which lead to significant errors in long-term projections. In this work, we
develop parameterizations based on Fourier Neural Operators, showcasing their
accuracy and generalizability in comparison to other approaches. Finally, we
discuss the potential and limitations of neural networks operating in the
frequency domain, paving the way for future investigation.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02692" title="Abstract">arXiv:2310.02692</a> [<a href="/pdf/2310.02692" title="Download PDF">pdf</a>, <a href="/format/2310.02692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging the Domain Gap by Clustering-based Image-Text Graph Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+N">Nokyung Park</a>, 
<a href="/search/cs?searchtype=author&query=Chae%2C+D">Daewon Chae</a>, 
<a href="/search/cs?searchtype=author&query=Shim%2C+J">Jeongyong Shim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sangpil Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+E">Eun-Sol Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jinkyu Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Learning domain-invariant representations is important to train a model that
can generalize well to unseen target task domains. Text descriptions inherently
contain semantic structures of concepts and such auxiliary semantic cues can be
used as effective pivot embedding for domain generalization problems. Here, we
use multimodal graph representations, fusing images and text, to get
domain-invariant pivot embeddings by considering the inherent semantic
structure between local images and text descriptors. Specifically, we aim to
learn domain-invariant features by (i) representing the image and text
descriptions with graphs, and by (ii) clustering and matching the graph-based
image node features into textual graphs simultaneously. We experiment with
large-scale public datasets, such as CUB-DG and DomainBed, and our model
achieves matched or better state-of-the-art performance on these datasets. Our
code will be publicly available upon publication.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02693" title="Abstract">arXiv:2310.02693</a> [<a href="/pdf/2310.02693" title="Download PDF">pdf</a>, <a href="/format/2310.02693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resilient Clock Synchronization Architecture for Industrial  Time-Sensitive Networking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sun%2C+Y">Yafei Sun</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Q">Qimin Xu</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+C">Cailian Chen</a>, 
<a href="/search/eess?searchtype=author&query=Guan%2C+X">Xinping Guan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Transactions on Cybernetics on 2023-Sep-28. 11 pages incl. references, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Time-Sensitive Networking (TSN) is a promising industrial Internet of Things
technology. Clock synchronization provides unified time reference, which is
critical to the deterministic communication of TSN. However, changes in
internal network status and external work environments of devices both degrade
practical synchronization performance. This paper proposes a
temperature-resilient architecture considering delay asymmetry (TACD) to
enhance the timing accuracy under the impacts of internal delay and external
thermal changes. In TACD, an anti-delay-asymmetry method is developed, which
employs a partial variational Bayesian algorithm to promote adaptability to
non-stationary delay variation. An optimized skew estimator is further
proposed, fusing the temperature skew model for ambiance perception with the
traditional linear clock model to compensate for nonlinear error caused by
temperature changes. Theoretical derivation of skew estimation lower bound
proves the promotion of optimal accuracy after the fusion of clock models.
Evaluations based on measured delay data demonstrate accuracy advantages
regardless of internal or external influences.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02696" title="Abstract">arXiv:2310.02696</a> [<a href="/pdf/2310.02696" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Curve Trajectory Model for Human Preferred Path Planning of Automated  Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Igneczi%2C+G">Gergo Igneczi</a>, 
<a href="/search/cs?searchtype=author&query=Horvath%2C+E">Erno Horvath</a>, 
<a href="/search/cs?searchtype=author&query=Toth%2C+R">Roland Toth</a>, 
<a href="/search/cs?searchtype=author&query=Nyilas%2C+K">Krisztian Nyilas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Automated driving systems are often used for lane keeping tasks. By these
systems, a local path is planned ahead of the vehicle. However, these paths are
often found unnatural by human drivers. We propose a linear driver model, which
can calculate node points that reflect the preferences of human drivers and
based on these node points a human driver preferred motion path can be designed
for autonomous driving. The model input is the road curvature. We apply this
model to a self-developed Euler-curve-based curve fitting algorithm. Through a
case study, we show that the model based planned path can reproduce the average
behavior of human curve path selection. We analyze the performance of the
proposed model through statistical analysis that shows the validity of the
captured relations.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02698" title="Abstract">arXiv:2310.02698</a> [<a href="/pdf/2310.02698" title="Download PDF">pdf</a>, <a href="/format/2310.02698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Federated Optimization by Reducing Variance of Adaptive  Unbiased Client Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+D">Dun Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zenglin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yu Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaoying Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated Learning (FL) systems usually sample a fraction of clients to
conduct a training process. Notably, the variance of global estimates for
updating the global model built on information from sampled clients is highly
related to federated optimization quality. This paper explores a line of "free"
adaptive client sampling techniques in federated optimization, where the server
builds promising sampling probability and reliable global estimates without
requiring additional local communication and computation. We capture a minor
variant in the sampling procedure and improve the global estimation
accordingly. Based on that, we propose a novel sampler called K-Vib, which
solves an online convex optimization respecting client sampling in federated
optimization. It achieves improved a linear speed up on regret bound
$\tilde{\mathcal{O}}\big(N^{\frac{1}{3}}T^{\frac{2}{3}}/K^{\frac{4}{3}}\big)$
with communication budget $K$. As a result, it significantly improves the
performance of federated optimization. Theoretical improvements and intensive
experiments on classic federated tasks demonstrate our findings.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02700" title="Abstract">arXiv:2310.02700</a> [<a href="/pdf/2310.02700" title="Download PDF">pdf</a>, <a href="/format/2310.02700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Tracking for a 3D Diffusion Equation: Controlling Seismicity Rate  in Geothermal Reservoirs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gutierrez-Oribio%2C+D">Diego Gutierrez-Oribio</a>, 
<a href="/search/eess?searchtype=author&query=Stefanou%2C+I">Ioannis Stefanou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Deep Geothermal Energy has significant potential to meet the large-scale
needs of the energy sector. However, the injection of fluids into the earth's
crust, upon which it relies, can lead to the formation of new seismogenic
faults or the reactivation of existing ones, thereby causing earthquakes. To
date, no effective method exists for mitigating these human-induced
earthquakes. In this study, we propose a novel approach based on control theory
to address this issue. First, we model induced seismicity resulting from fluid
injections in a geothermal reservoir using a diffusion equation in three
dimensions. Then, we design a robust tracking control approach to force the
seismicity rate to follow the desired references. In this way, the induced
seismicity is minimized while ensuring fluid circulation for the needs of
energy production. The designed control guarantees the stabilization of the
error variable even in the presence of system uncertainties and unknown
dynamics. Finally, we present simulations of a geothermal reservoir under
different scenarios of intermittent energy demand to show the reliability and
performance of the control approach, opening new perspectives for field
experiments based on real-time regulators for the first time.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02702" title="Abstract">arXiv:2310.02702</a> [<a href="/pdf/2310.02702" title="Download PDF">pdf</a>, <a href="/format/2310.02702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tackling Hybrid Heterogeneity on Federated Optimization via Gradient  Diversity Maximization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+D">Dun Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zenglin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yu Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaoying Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated learning refers to a distributed machine learning paradigm in which
data samples are decentralized and distributed among multiple clients. These
samples may exhibit statistical heterogeneity, which refers to data
distributions are not independent and identical across clients. Additionally,
system heterogeneity, or variations in the computational power of the clients,
introduces biases into federated learning. The combined effects of statistical
and system heterogeneity can significantly reduce the efficiency of federated
optimization. However, the impact of hybrid heterogeneity is not rigorously
discussed. This paper explores how hybrid heterogeneity affects federated
optimization by investigating server-side optimization. The theoretical results
indicate that adaptively maximizing gradient diversity in server update
direction can help mitigate the potential negative consequences of hybrid
heterogeneity. To this end, we introduce a novel server-side gradient-based
optimizer \textsc{FedAWARE} with theoretical guarantees provided. Intensive
experiments in heterogeneous federated settings demonstrate that our proposed
optimizer can significantly enhance the performance of federated learning
across varying degrees of hybrid heterogeneity.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02703" title="Abstract">arXiv:2310.02703</a> [<a href="/pdf/2310.02703" title="Download PDF">pdf</a>, <a href="/format/2310.02703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-fidelity No-U-Turn Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ravi%2C+K">Kislaya Ravi</a>, 
<a href="/search/math?searchtype=author&query=Neckel%2C+T">Tobias Neckel</a>, 
<a href="/search/math?searchtype=author&query=Bungartz%2C+H">Hans-Joachim Bungartz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work was presented in MCQMC conference, 2022. It will be published in the proceedings of the conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Statistics Theory (math.ST)

</div>
<p class="mathjax">Markov Chain Monte Carlo (MCMC) methods often take many iterations to
converge for highly correlated or high-dimensional target density functions.
Methods such as Hamiltonian Monte Carlo (HMC) or No-U-Turn Sampling (NUTS) use
the first-order derivative of the density function to tackle the aforementioned
issues. However, the calculation of the derivative represents a bottleneck for
computationally expensive models. We propose to first build a multi-fidelity
Gaussian Process (GP) surrogate. The building block of the multi-fidelity
surrogate is a hierarchy of models of decreasing approximation error and
increasing computational cost. Then the generated multi-fidelity surrogate is
used to approximate the derivative. The majority of the computation is assigned
to the cheap models thereby reducing the overall computational cost. The
derivative of the multi-fidelity method is used to explore the target density
function and generate proposals. We select or reject the proposals using the
Metropolis Hasting criterion using the highest fidelity model which ensures
that the proposed method is ergodic with respect to the highest fidelity
density function. We apply the proposed method to three test cases including
some well-known benchmarks to compare it with existing methods and show that
multi-fidelity No-U-turn sampling outperforms other methods.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02704" title="Abstract">arXiv:2310.02704</a> [<a href="/pdf/2310.02704" title="Download PDF">pdf</a>, <a href="/ps/2310.02704" title="Download PostScript">ps</a>, <a href="/format/2310.02704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending Isabelle/HOL&#x27;s Code Generator with support for the Go  programming language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=St%C3%BCbinger%2C+T">Terru St&#xfc;binger</a>, 
<a href="/search/cs?searchtype=author&query=Hupel%2C+L">Lars Hupel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, of which 16 pages are main content
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">The Isabelle proof assistant includes a small functional language, which
allows users to write and reason about programs. So far, these programs could
be extracted into a number of functional languages: Standard ML, OCaml, Scala,
and Haskell. This work adds support for Go as a fifth target language for the
Code Generator. Unlike the previous targets, Go is not a functional language
and encourages code in an imperative style, thus many of the features of
Isabelle's language (particularly data types, pattern matching, and type
classes) have to be emulated using imperative language constructs in Go. The
developed Code Generation is provided as an add-on library that can be simply
imported into existing theories.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02710" title="Abstract">arXiv:2310.02710</a> [<a href="/pdf/2310.02710" title="Download PDF">pdf</a>, <a href="/format/2310.02710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Search GFlowNets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+T">Taeyoung Yun</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+E">Emmanuel Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dinghuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+S">Sungsoo Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jinkyoo Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Generative Flow Networks (GFlowNets) are amortized sampling methods that
learn a distribution over discrete objects proportional to their rewards.
GFlowNets exhibit a remarkable ability to generate diverse samples, yet
occasionally struggle to consistently produce samples with high rewards due to
over-exploration on wide sample space. This paper proposes to train GFlowNets
with local search which focuses on exploiting high rewarded sample space to
resolve this issue. Our main idea is to explore the local neighborhood via
destruction and reconstruction guided by backward and forward policies,
respectively. This allows biasing the samples toward high-reward solutions,
which is not possible for a typical GFlowNet solution generation scheme which
uses the forward policy to generate the solution from scratch. Extensive
experiments demonstrate a remarkable performance improvement in several
biochemical tasks. Source code is available:
\url{https://github.com/dbsxodud-11/ls_gfn}.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02712" title="Abstract">arXiv:2310.02712</a> [<a href="/pdf/2310.02712" title="Download PDF">pdf</a>, <a href="/format/2310.02712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space  NeRF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jangho Park</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+G">Gihyun Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J+C">Jong Chul Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Recently, there has been a significant advancement in text-to-image diffusion
models, leading to groundbreaking performance in 2D image generation. These
advancements have been extended to 3D models, enabling the generation of novel
3D objects from textual descriptions. This has evolved into NeRF editing
methods, which allow the manipulation of existing 3D objects through textual
conditioning. However, existing NeRF editing techniques have faced limitations
in their performance due to slow training speeds and the use of loss functions
that do not adequately consider editing. To address this, here we present a
novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding
real-world scenes into the latent space of the latent diffusion model (LDM)
through a unique refinement layer. This approach enables us to obtain a NeRF
backbone that is not only faster but also more amenable to editing compared to
traditional image space NeRF editing. Furthermore, we propose an improved loss
function tailored for editing by migrating the delta denoising score (DDS)
distillation loss, originally used in 2D image editing to the three-dimensional
domain. This novel loss function surpasses the well-known score distillation
sampling (SDS) loss in terms of suitability for editing purposes. Our
experimental results demonstrate that ED-NeRF achieves faster editing speed
while producing improved output quality compared to state-of-the-art 3D editing
models.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02713" title="Abstract">arXiv:2310.02713</a> [<a href="/pdf/2310.02713" title="Download PDF">pdf</a>, <a href="/format/2310.02713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis  in Brain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+G">Gyutaek Oh</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+B">Baekgyu Choi</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+I">Inkyung Jung</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J+C">Jong Chul Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Genomics (q-bio.GN); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Single-cell RNA sequencing (scRNA-seq) has made significant strides in
unraveling the intricate cellular diversity within complex tissues. This is
particularly critical in the brain, presenting a greater diversity of cell
types than other tissue types, to gain a deeper understanding of brain function
within various cellular contexts. However, analyzing scRNA-seq data remains a
challenge due to inherent measurement noise stemming from dropout events and
the limited utilization of extensive gene expression information. In this work,
we introduce scHyena, a foundation model designed to address these challenges
and enhance the accuracy of scRNA-seq analysis in the brain. Specifically,
inspired by the recent Hyena operator, we design a novel Transformer
architecture called singe-cell Hyena (scHyena) that is equipped with a linear
adaptor layer, the positional encoding via gene-embedding, and a
{bidirectional} Hyena operator. This enables us to process full-length
scRNA-seq data without losing any information from the raw data. In particular,
our model learns generalizable features of cells and genes through pre-training
scHyena using the full length of scRNA-seq data. We demonstrate the superior
performance of scHyena compared to other benchmark methods in downstream tasks,
including cell type classification and scRNA-seq imputation.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02714" title="Abstract">arXiv:2310.02714</a> [<a href="/pdf/2310.02714" title="Download PDF">pdf</a>, <a href="/format/2310.02714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GETAvatar: Generative Textured Meshes for Animatable Human Avatars
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuanmeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianfeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chacko%2C+R">Rohan Chacko</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongyi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+G">Guoxian Song</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiashi Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV2023. Project Page: <a href="https://getavatar.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We study the problem of 3D-aware full-body human generation, aiming at
creating animatable human avatars with high-quality textures and geometries.
Generally, two challenges remain in this field: i) existing methods struggle to
generate geometries with rich realistic details such as the wrinkles of
garments; ii) they typically utilize volumetric radiance fields and neural
renderers in the synthesis process, making high-resolution rendering
non-trivial. To overcome these problems, we propose GETAvatar, a Generative
model that directly generates Explicit Textured 3D meshes for animatable human
Avatar, with photo-realistic appearance and fine geometric details.
Specifically, we first design an articulated 3D human representation with
explicit surface modeling, and enrich the generated humans with realistic
surface details by learning from the 2D normal maps of 3D scan data. Second,
with the explicit mesh representation, we can use a rasterization-based
renderer to perform surface rendering, allowing us to achieve high-resolution
image generation efficiently. Extensive experiments demonstrate that GETAvatar
achieves state-of-the-art performance on 3D-aware human generation both in
appearance and geometry quality. Notably, GETAvatar can generate images at
512x512 resolution with 17FPS and 1024x1024 resolution with 14FPS, improving
upon previous methods by 2x. Our code and models will be available.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02717" title="Abstract">arXiv:2310.02717</a> [<a href="/pdf/2310.02717" title="Download PDF">pdf</a>, <a href="/format/2310.02717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Clustering of Bandits with Misspecified User Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiyong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jize Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xutong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuai Li</a>, 
<a href="/search/cs?searchtype=author&query=Lui%2C+J+C+S">John C.S. Lui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The contextual linear bandit is an important online learning problem where
given arm features, a learning agent selects an arm at each round to maximize
the cumulative rewards in the long run. A line of works, called the clustering
of bandits (CB), utilize the collaborative effect over user preferences and
have shown significant improvements over classic linear bandit algorithms.
However, existing CB algorithms require well-specified linear user models and
can fail when this critical assumption does not hold. Whether robust CB
algorithms can be designed for more practical scenarios with misspecified user
models remains an open problem. In this paper, we are the first to present the
important problem of clustering of bandits with misspecified user models
(CBMUM), where the expected rewards in user models can be perturbed away from
perfect linear models. We devise two robust CB algorithms, RCLUMB and RSCLUMB
(representing the learned clustering structure with dynamic graph and sets,
respectively), that can accommodate the inaccurate user preference estimations
and erroneous clustering caused by model misspecifications. We prove regret
upper bounds of $O(\epsilon_*T\sqrt{md\log T} + d\sqrt{mT}\log T)$ for our
algorithms under milder assumptions than previous CB works (notably, we move
past a restrictive technical assumption on the distribution of the arms), which
match the lower bound asymptotically in $T$ up to logarithmic factors, and also
match the state-of-the-art results in several degenerate cases. The techniques
in proving the regret caused by misclustering users are quite general and may
be of independent interest. Experiments on both synthetic and real-world data
show our outperformance over previous algorithms.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02718" title="Abstract">arXiv:2310.02718</a> [<a href="/pdf/2310.02718" title="Download PDF">pdf</a>, <a href="/format/2310.02718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Pan-Sharpening via Generalized Inverse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yutong Bai</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xinyang Han</a>, 
<a href="/search/cs?searchtype=author&query=Yuille%2C+A">Alan Yuille</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Pan-sharpening algorithm utilizes panchromatic image and multispectral image
to obtain a high spatial and high spectral image. However, the optimizations of
the algorithms are designed with different standards. We adopt the simple
matrix equation to describe the Pan-sharpening problem. The solution existence
condition and the acquirement of spectral and spatial resolution are discussed.
A down-sampling enhancement method was introduced for better acquiring the
spatial and spectral down-sample matrices. By the generalized inverse theory,
we derived two forms of general inverse matrix formulations that can correspond
to the two prominent classes of Pan-sharpening methods, that is, component
substitution and multi-resolution analysis methods. Specifically, the Gram
Schmidt Adaptive(GSA) was proved to follow the general inverse matrix
formulation of component substitution. A model prior to the general inverse
matrix of the spectral function was rendered. The theoretical errors are
analyzed. Synthetic experiments and real data experiments are implemented. The
proposed methods are better and sharper than other methods qualitatively in
both synthetic and real experiments. The down-sample enhancement effect is
shown of better results both quantitatively and qualitatively in real
experiments. The generalized inverse matrix theory help us better understand
the Pan-sharpening.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02719" title="Abstract">arXiv:2310.02719</a> [<a href="/pdf/2310.02719" title="Download PDF">pdf</a>, <a href="/format/2310.02719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Condition numbers in multiview geometry, instability in relative pose  estimation, and RANSAC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+H">Hongyi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Kileel%2C+J">Joe Kileel</a>, 
<a href="/search/cs?searchtype=author&query=Kimia%2C+B">Benjamin Kimia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this paper we introduce a general framework for analyzing the numerical
conditioning of minimal problems in multiple view geometry, using tools from
computational algebra and Riemannian geometry. Special motivation comes from
the fact that relative pose estimation, based on standard 5-point or 7-point
Random Sample Consensus (RANSAC) algorithms, can fail even when no outliers are
present and there is enough data to support a hypothesis. We argue that these
cases arise due to the intrinsic instability of the 5- and 7-point minimal
problems. We apply our framework to characterize the instabilities, both in
terms of the world scenes that lead to infinite condition number, and directly
in terms of ill-conditioned image data. The approach produces computational
tests for assessing the condition number before solving the minimal problem.
Lastly synthetic and real data experiments suggest that RANSAC serves not only
to remove outliers, but also to select for well-conditioned image data, as
predicted by our theory.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02720" title="Abstract">arXiv:2310.02720</a> [<a href="/pdf/2310.02720" title="Download PDF">pdf</a>, <a href="/format/2310.02720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised  Learning with Masked Unit Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiatong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Inaguma%2C+H">Hirofumi Inaguma</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xutai Ma</a>, 
<a href="/search/cs?searchtype=author&query=Kulikov%2C+I">Ilia Kulikov</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+A">Anna Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 4 figures, 21 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Existing Self-Supervised Learning (SSL) models for speech typically process
speech signals at a fixed resolution of 20 milliseconds. This approach
overlooks the varying informational content present at different resolutions in
speech signals. In contrast, this paper aims to incorporate multi-resolution
information into speech self-supervised representation learning. We introduce a
SSL model that leverages a hierarchical Transformer architecture, complemented
by HuBERT-style masked prediction objectives, to process speech at multiple
resolutions. Experimental results indicate that the proposed model not only
achieves more efficient inference but also exhibits superior or comparable
performance to the original HuBERT model over various tasks. Specifically,
significant performance improvements over the original HuBERT have been
observed in fine-tuning experiments on the LibriSpeech speech recognition
benchmark as well as in evaluations using the Speech Universal PERformance
Benchmark (SUPERB) and Multilingual SUPERB (ML-SUPERB).
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02721" title="Abstract">arXiv:2310.02721</a> [<a href="/pdf/2310.02721" title="Download PDF">pdf</a>, <a href="/format/2310.02721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Temporal Graph Networks Using Module Decoupling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feldman%2C+O">Or Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Baskin%2C+C">Chaim Baskin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Modern approaches for learning on dynamic graphs have adopted the use of
batches instead of applying updates one by one. The use of batches allows these
techniques to become helpful in streaming scenarios where updates to graphs are
received at extreme speeds. Using batches, however, forces the models to update
infrequently, which results in the degradation of their performance. In this
work, we suggest a decoupling strategy that enables the models to update
frequently while using batches. By decoupling the core modules of temporal
graph networks and implementing them using a minimal number of learnable
parameters, we have developed the Lightweight Decoupled Temporal Graph Network
(LDTGN), an exceptionally efficient model for learning on dynamic graphs. LDTG
was validated on various dynamic graph benchmarks, providing comparable or
state-of-the-art results with significantly higher throughput than previous
art. Notably, our method outperforms previous approaches by more than 20\% on
benchmarks that require rapid model update rates, such as USLegis or UNTrade.
The code to reproduce our experiments is available at
\href{https://orfeld415.github.io/module-decoupling}{this http url}.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02724" title="Abstract">arXiv:2310.02724</a> [<a href="/pdf/2310.02724" title="Download PDF">pdf</a>, <a href="/format/2310.02724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-End Training of a Neural HMM with Label and Transition  Probabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mann%2C+D">Daniel Mann</a>, 
<a href="/search/cs?searchtype=author&query=Raissi%2C+T">Tina Raissi</a>, 
<a href="/search/cs?searchtype=author&query=Michel%2C+W">Wilfried Michel</a>, 
<a href="/search/cs?searchtype=author&query=Schl%C3%BCter%2C+R">Ralf Schl&#xfc;ter</a>, 
<a href="/search/cs?searchtype=author&query=Ney%2C+H">Hermann Ney</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for Presentation at ASRU2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We investigate a novel modeling approach for end-to-end neural network
training using hidden Markov models (HMM) where the transition probabilities
between hidden states are modeled and learned explicitly. Most contemporary
sequence-to-sequence models allow for from-scratch training by summing over all
possible label segmentations in a given topology. In our approach there are
explicit, learnable probabilities for transitions between segments as opposed
to a blank label that implicitly encodes duration statistics. We implement a
GPU-based forward-backward algorithm that enables the simultaneous training of
label and transition probabilities. We investigate recognition results and
additionally Viterbi alignments of our models. We find that while the
transition model training does not improve recognition performance, it has a
positive impact on the alignment quality. The generated alignments are shown to
be viable targets in state-of-the-art Viterbi trainings.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02726" title="Abstract">arXiv:2310.02726</a> [<a href="/pdf/2310.02726" title="Download PDF">pdf</a>, <a href="/format/2310.02726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Collaborative Transportation for Under-Capacitated Vehicle  Routing Problems using Aerial Drone Swarms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sreedhara%2C+A+K">Akash Kopparam Sreedhara</a>, 
<a href="/search/cs?searchtype=author&query=Padala%2C+D">Deepesh Padala</a>, 
<a href="/search/cs?searchtype=author&query=Mahesh%2C+S">Shashank Mahesh</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+K">Kai Cui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mengguang Li</a>, 
<a href="/search/cs?searchtype=author&query=Koeppl%2C+H">Heinz Koeppl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Swarms of aerial drones have recently been considered for last-mile
deliveries in urban logistics or automated construction. At the same time,
collaborative transportation of payloads by multiple drones is another
important area of recent research. However, efficient coordination algorithms
for collaborative transportation of many payloads by many drones remain to be
considered. In this work, we formulate the collaborative transportation of
payloads by a swarm of drones as a novel, under-capacitated generalization of
vehicle routing problems (VRP), which may also be of separate interest. In
contrast to standard VRP and capacitated VRP, we must additionally consider
waiting times for payloads lifted cooperatively by multiple drones, and the
corresponding coordination. Algorithmically, we provide a solution encoding
that avoids deadlocks and formulate an appropriate alternating minimization
scheme to solve the problem. On the hardware side, we integrate our algorithms
with collision avoidance and drone controllers. The approach and the impact of
the system integration are successfully verified empirically, both on a swarm
of real nano-quadcopters and for large swarms in simulation. Overall, we
provide a framework for collaborative transportation with aerial drone swarms,
that uses only as many drones as necessary for the transportation of any single
payload.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02729" title="Abstract">arXiv:2310.02729</a> [<a href="/pdf/2310.02729" title="Download PDF">pdf</a>, <a href="/format/2310.02729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Quantification and Representation of Aggregate Flexibility in  Electric Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Panda%2C+N+K">Nanda Kishor Panda</a>, 
<a href="/search/eess?searchtype=author&query=Tindemans%2C+S+H">Simon H. Tindemans</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Aggregation is crucial to the effective use of flexibility, especially in the
case of electric vehicles (EVs) because of their limited individual battery
sizes and large aggregate impact. This research proposes a novel method of
quantifying and representing the aggregate flexibility of EV fleets within a
fixed flexibility request window. These windows can be chosen based on relevant
network operator needs, such as evening congestion periods. The proposed
UL-flexibility is independent of the number of assets but scales only with the
number of discrete time steps in the chosen window. The representation involves
$2T$ parameters, with T being the number of time steps in the window.
Feasibility of signals can be checked using $2T$ constraints and optimization
using $2(2^T-1)$ constraints, both exactly capturing the flexibility region.
Using a request window eliminates uncertainty related to EV arrival and
departure times outside the window. We present the necessary theoretical
framework for our proposed methods and outline steps for transitioning between
representations. Additionally, we compare the computational efficiency of the
proposed method with the common direct aggregation method.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02731" title="Abstract">arXiv:2310.02731</a> [<a href="/pdf/2310.02731" title="Download PDF">pdf</a>, <a href="/format/2310.02731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> State Feedback Control Design for Input-output Decoupling of Boolean  Control Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yiliang Li</a>, 
<a href="/search/eess?searchtype=author&query=Lyu%2C+H">Hongli Lyu</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+J">Jun-e Feng</a>, 
<a href="/search/eess?searchtype=author&query=Tayebi%2C+A">Abdelhamid Tayebi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">A state feedback control strategy is proposed for input-output (IO)
decoupling of a class of fully output controllable Boolean control networks
(BCNs). Some necessary and sufficient conditions for BCN IO-decoupling are
presented. As an instrumental tool in our design, we introduce a canonical form
for IO-decoupled BCNs along with some conditions guaranteeing its existence.
Finally, two numerical examples are provided to illustrate the effectiveness of
the proposed approach.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02735" title="Abstract">arXiv:2310.02735</a> [<a href="/pdf/2310.02735" title="Download PDF">pdf</a>, <a href="/format/2310.02735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extracting Rules from Event Data for Study Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rafiei%2C+M">Majid Rafiei</a>, 
<a href="/search/cs?searchtype=author&query=Bayrak%2C+D">Duygu Bayrak</a>, 
<a href="/search/cs?searchtype=author&query=Pourbafrani%2C+M">Mahsa Pourbafrani</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+G">Gyunam Park</a>, 
<a href="/search/cs?searchtype=author&query=Helal%2C+H">Hayyan Helal</a>, 
<a href="/search/cs?searchtype=author&query=Lakemeyer%2C+G">Gerhard Lakemeyer</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Aalst%2C+W+M+P">Wil M.P. van der Aalst</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this study, we examine how event data from campus management systems can
be used to analyze the study paths of higher education students. The main goal
is to offer valuable guidance for their study planning. We employ process and
data mining techniques to explore the impact of sequences of taken courses on
academic success. Through the use of decision tree models, we generate
data-driven recommendations in the form of rules for study planning and compare
them to the recommended study plan. The evaluation focuses on RWTH Aachen
University computer science bachelor program students and demonstrates that the
proposed course sequence features effectively explain academic performance
measures. Furthermore, the findings suggest avenues for developing more
adaptable study plans.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02739" title="Abstract">arXiv:2310.02739</a> [<a href="/pdf/2310.02739" title="Download PDF">pdf</a>, <a href="/format/2310.02739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> uTalk: Bridging the Gap Between Humans and AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azzuni%2C+H">Hussam Azzuni</a>, 
<a href="/search/cs?searchtype=author&query=Jamal%2C+S">Sharim Jamal</a>, 
<a href="/search/cs?searchtype=author&query=Elsaddik%2C+A">Abdulmotaleb Elsaddik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have revolutionized various industries by
harnessing their power to improve productivity and facilitate learning across
different fields. One intriguing application involves combining LLMs with
visual models to create a novel approach to Human-Computer Interaction. The
core idea behind this system is to develop an interactive platform that allows
the general public to leverage the capabilities of ChatGPT in their daily
lives. This is achieved by integrating several technologies such as Whisper,
ChatGPT, Microsoft Speech Services, and the state-of-the-art (SOTA) talking
head system, SadTalker, resulting in uTalk, an intelligent AI system. Users
will be able to converse with this portrait, receiving answers to whatever
questions they have in mind. Additionally, they could use uTalk for content
generation by providing an input and their image. This system is hosted on
Streamlit, where the user will initially be requested to provide an image to
serve as their AI assistant. Then, users could choose whether to have a
conversation or generate content based on their preferences. Either way, it
starts by providing an input, where a set of operations will be done, and the
avatar will provide a precise response. The paper discusses how SadTalker is
optimized to improve its running time by 27.72% based on 25FPS generated
videos. In addition, the system's initial performance, uTalk, improved further
by 9.8% after SadTalker was integrated and parallelized with Streamlit.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02742" title="Abstract">arXiv:2310.02742</a> [<a href="/pdf/2310.02742" title="Download PDF">pdf</a>, <a href="/format/2310.02742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative Analysis of Imbalanced Malware Byteplot Image Classification  using Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%2C+J">Jayasudha M</a>, 
<a href="/search/cs?searchtype=author&query=Shaik%2C+A">Ayesha Shaik</a>, 
<a href="/search/cs?searchtype=author&query=Pendharkar%2C+G">Gaurav Pendharkar</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Soham Kumar</a>, 
<a href="/search/cs?searchtype=author&query=B%2C+M+K">Muhesh Kumar B</a>, 
<a href="/search/cs?searchtype=author&query=Balaji%2C+S">Sudharshanan Balaji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at PEIS2023 and will be published in Lecture Notes in Electrical Engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Cybersecurity is a major concern due to the increasing reliance on technology
and interconnected systems. Malware detectors help mitigate cyber-attacks by
comparing malware signatures. Machine learning can improve these detectors by
automating feature extraction, identifying patterns, and enhancing dynamic
analysis. In this paper, the performance of six multiclass classification
models is compared on the Malimg dataset, Blended dataset, and Malevis dataset
to gain insights into the effect of class imbalance on model performance and
convergence. It is observed that the more the class imbalance less the number
of epochs required for convergence and a high variance across the performance
of different models. Moreover, it is also observed that for malware detectors
ResNet50, EfficientNetB0, and DenseNet169 can handle imbalanced and balanced
data well. A maximum precision of 97% is obtained for the imbalanced dataset, a
maximum precision of 95% is obtained on the intermediate imbalance dataset, and
a maximum precision of 95% is obtained for the perfectly balanced dataset.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02743" title="Abstract">arXiv:2310.02743</a> [<a href="/pdf/2310.02743" title="Download PDF">pdf</a>, <a href="/format/2310.02743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reward Model Ensembles Help Mitigate Overoptimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Coste%2C+T">Thomas Coste</a>, 
<a href="/search/cs?searchtype=author&query=Anwar%2C+U">Usman Anwar</a>, 
<a href="/search/cs?searchtype=author&query=Kirk%2C+R">Robert Kirk</a>, 
<a href="/search/cs?searchtype=author&query=Krueger%2C+D">David Krueger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 12 figures (excluding appendix). Submitted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Reinforcement learning from human feedback (RLHF) is a standard approach for
fine-tuning large language models to follow instructions. As part of this
process, learned reward models are used to approximately model human
preferences. However, as imperfect representations of the "true" reward, these
learned reward models are susceptible to \textit{overoptimization}. Gao et al.
(2023) studied this phenomenon in a synthetic human feedback setup with a
significantly larger "gold" reward model acting as the true reward (instead of
humans) and showed that overoptimization remains a persistent problem
regardless of the size of the proxy reward model and training data used. Using
a similar setup, we conduct a systematic study to evaluate the efficacy of
using ensemble-based conservative optimization objectives, specifically
worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for
mitigating reward model overoptimization when using two optimization methods:
(a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We
additionally extend the setup of Gao et al. (2023) to include 25% label noise
to better mirror real-world conditions. Both with and without label noise, we
find that conservative optimization practically eliminates overoptimization and
improves performance by up to 70% for BoN sampling. For PPO, ensemble-based
conservative optimization always reduces overoptimization and outperforms
single reward model optimization. Moreover, combining it with a small KL
penalty successfully prevents overoptimization at no performance cost. Overall,
our results demonstrate that ensemble-based conservative optimization can
effectively counter overoptimization.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02744" title="Abstract">arXiv:2310.02744</a> [<a href="/pdf/2310.02744" title="Download PDF">pdf</a>, <a href="/format/2310.02744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SALSA: Semantically-Aware Latent Space Autoencoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kirchoff%2C+K+E">Kathryn E. Kirchoff</a>, 
<a href="/search/cs?searchtype=author&query=Maxfield%2C+T">Travis Maxfield</a>, 
<a href="/search/cs?searchtype=author&query=Tropsha%2C+A">Alexander Tropsha</a>, 
<a href="/search/cs?searchtype=author&query=Gomez%2C+S+M">Shawn M. Gomez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In deep learning for drug discovery, chemical data are often represented as
simplified molecular-input line-entry system (SMILES) sequences which allow for
straightforward implementation of natural language processing methodologies,
one being the sequence-to-sequence autoencoder. However, we observe that
training an autoencoder solely on SMILES is insufficient to learn molecular
representations that are semantically meaningful, where semantics are defined
by the structural (graph-to-graph) similarities between molecules. We
demonstrate by example that autoencoders may map structurally similar molecules
to distant codes, resulting in an incoherent latent space that does not respect
the structural similarities between molecules. To address this shortcoming we
propose Semantically-Aware Latent Space Autoencoder (SALSA), a
transformer-autoencoder modified with a contrastive task, tailored specifically
to learn graph-to-graph similarity between molecules. Formally, the contrastive
objective is to map structurally similar molecules (separated by a single graph
edit) to nearby codes in the latent space. To accomplish this, we generate a
novel dataset comprised of sets of structurally similar molecules and opt for a
supervised contrastive loss that is able to incorporate full sets of positive
samples. We compare SALSA to its ablated counterparts, and show empirically
that the composed training objective (reconstruction and contrastive task)
leads to a higher quality latent space that is more 1) structurally-aware, 2)
semantically continuous, and 3) property-aware.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02751" title="Abstract">arXiv:2310.02751</a> [<a href="/pdf/2310.02751" title="Download PDF">pdf</a>, <a href="/format/2310.02751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SHOT: Suppressing the Hessian along the Optimization Trajectory for  Gradient-Based Meta-Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">JunHoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+J">Jayeon Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Kwak%2C+N">Nojun Kwak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this paper, we hypothesize that gradient-based meta-learning (GBML)
implicitly suppresses the Hessian along the optimization trajectory in the
inner loop. Based on this hypothesis, we introduce an algorithm called SHOT
(Suppressing the Hessian along the Optimization Trajectory) that minimizes the
distance between the parameters of the target and reference models to suppress
the Hessian in the inner loop. Despite dealing with high-order terms, SHOT does
not increase the computational complexity of the baseline model much. It is
agnostic to both the algorithm and architecture used in GBML, making it highly
versatile and applicable to any GBML baseline. To validate the effectiveness of
SHOT, we conduct empirical tests on standard few-shot learning tasks and
qualitatively analyze its dynamics. We confirm our hypothesis empirically and
demonstrate that SHOT outperforms the corresponding baseline. Code is available
at: https://github.com/JunHoo-Lee/SHOT
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02752" title="Abstract">arXiv:2310.02752</a> [<a href="/pdf/2310.02752" title="Download PDF">pdf</a>, <a href="/ps/2310.02752" title="Download PostScript">ps</a>, <a href="/format/2310.02752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Feature Selection: A Comparison of Multi-Objective Genetic  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brookhouse%2C+J">James Brookhouse</a>, 
<a href="/search/cs?searchtype=author&query=Freitas%2C+A">Alex Freitas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 1 figure, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Machine learning classifiers are widely used to make decisions with a major
impact on people's lives (e.g. accepting or denying a loan, hiring decisions,
etc). In such applications,the learned classifiers need to be both accurate and
fair with respect to different groups of people, with different values of
variables such as sex and race. This paper focuses on fair feature selection
for classification, i.e. methods that select a feature subset aimed at
maximising both the accuracy and the fairness of the predictions made by a
classifier. More specifically, we compare two recently proposed Genetic
Algorithms (GAs) for fair feature selection that are based on two different
multi-objective optimisation approaches: (a) a Pareto dominance-based GA; and
(b) a lexicographic optimisation-based GA, where maximising accuracy has higher
priority than maximising fairness. Both GAs use the same measures of accuracy
and fairness, allowing for a controlled comparison. As far as we know, this is
the first comparison between the Pareto and lexicographic approaches for fair
classification. The results show that, overall, the lexicographic GA
outperformed the Pareto GA with respect to accuracy without degradation of the
fairness of the learned classifiers. This is an important result because at
present nearly all GAs for fair classification are based on the Pareto
approach, so these results suggest a promising new direction for research in
this area.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02753" title="Abstract">arXiv:2310.02753</a> [<a href="/pdf/2310.02753" title="Download PDF">pdf</a>, <a href="/format/2310.02753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MUNCH: Modelling Unique &#x27;N Controllable Heads
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deb%2C+D">Debayan Deb</a>, 
<a href="/search/cs?searchtype=author&query=Tripathi%2C+S">Suvidha Tripathi</a>, 
<a href="/search/cs?searchtype=author&query=Puri%2C+P">Pranit Puri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">The automated generation of 3D human heads has been an intriguing and
challenging task for computer vision researchers. Prevailing methods synthesize
realistic avatars but with limited control over the diversity and quality of
rendered outputs and suffer from limited correlation between shape and texture
of the character. We propose a method that offers quality, diversity, control,
and realism along with explainable network design, all desirable features to
game-design artists in the domain. First, our proposed Geometry Generator
identifies disentangled latent directions and generate novel and diverse
samples. A Render Map Generator then learns to synthesize multiply high-fidelty
physically-based render maps including Albedo, Glossiness, Specular, and
Normals. For artists preferring fine-grained control over the output, we
introduce a novel Color Transformer Model that allows semantic color control
over generated maps. We also introduce quantifiable metrics called Uniqueness
and Novelty and a combined metric to test the overall performance of our model.
Demo for both shapes and textures can be found:
https://munch-seven.vercel.app/. We will release our model along with the
synthetic dataset.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02754" title="Abstract">arXiv:2310.02754</a> [<a href="/pdf/2310.02754" title="Download PDF">pdf</a>, <a href="/ps/2310.02754" title="Download PostScript">ps</a>, <a href="/format/2310.02754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LC-Score: Reference-less estimation of Text Comprehension Difficulty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tardy%2C+P">Paul Tardy</a>, 
<a href="/search/cs?searchtype=author&query=Roze%2C+C">Charlotte Roze</a>, 
<a href="/search/cs?searchtype=author&query=Poupet%2C+P">Paul Poupet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Being able to read and understand written text is critical in a digital era.
However, studies shows that a large fraction of the population experiences
comprehension issues. In this context, further initiatives in accessibility are
required to improve the audience text comprehension. However, writers are
hardly assisted nor encouraged to produce easy-to-understand content. Moreover,
Automatic Text Simplification (ATS) model development suffers from the lack of
metric to accurately estimate comprehension difficulty We present
\textsc{LC-Score}, a simple approach for training text comprehension metric for
any French text without reference \ie predicting how easy to understand a given
text is on a $[0, 100]$ scale. Our objective with this scale is to
quantitatively capture the extend to which a text suits to the \textit{Langage
Clair} (LC, \textit{Clear Language}) guidelines, a French initiative closely
related to English Plain Language. We explore two approaches: (i) using
linguistically motivated indicators used to train statistical models, and (ii)
neural learning directly from text leveraging pre-trained language models. We
introduce a simple proxy task for comprehension difficulty training as a
classification task. To evaluate our models, we run two distinct human
annotation experiments, and find that both approaches (indicator based and
neural) outperforms commonly used readability and comprehension metrics such as
FKGL and SAMSA.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02756" title="Abstract">arXiv:2310.02756</a> [<a href="/pdf/2310.02756" title="Download PDF">pdf</a>, <a href="/format/2310.02756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling of Annual and Daily Electricity Demand of Retrofitted Heat  Pumps based on Gas Smart Meter Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bayer%2C+D+R">Daniel R. Bayer</a>, 
<a href="/search/cs?searchtype=author&query=Pruckner%2C+M">Marco Pruckner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Currently, gas furnaces are common heating systems in Europe. Due to the
efforts for decarbonizing the complete energy sector, heat pumps should
continuously replace existing gas furnaces. At the same time, the
electrification of the heating sector represents a significant challenge for
the power grids and their operators. Thus, new approaches are required to
estimate the additional electricity demand to operate heat pumps. The
electricity required by a heat pump to produce a given amount of heat depends
on the Seasonal Performance Factor (SPF), which is hard to model in theory due
to many influencing factors and hard to measure in reality as the heat produced
by a heat pump is usually not measured. Therefore, we show in this paper that
collected smart meter data forms an excellent data basis on building level for
modeling heat demand and the SPF. We present a novel methodology to estimate
the mean SPF based on an unpaired dataset of heat pump electricity and gas
consumption data taken from buildings within the same city by comparing the
distributions using the Jensen-Shannon Divergence (JSD). Based on a real-world
dataset, we evaluate this novel method by predicting the electricity demand
required if all gas furnaces in a city were replaced by heat pumps and briefly
highlight possible use cases.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02759" title="Abstract">arXiv:2310.02759</a> [<a href="/pdf/2310.02759" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative Study and Framework for Automated Summariser Evaluation:  LangChain and Hybrid Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%2C+B+L">Bagiya Lakshmi S</a>, 
<a href="/search/cs?searchtype=author&query=R%2C+S+V">Sanjjushri Varshini R</a>, 
<a href="/search/cs?searchtype=author&query=Mahadevan%2C+R">Rohith Mahadevan</a>, 
<a href="/search/cs?searchtype=author&query=Raman%2C+R+C">Raja CSP Raman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Automated Essay Score (AES) is proven to be one of the cutting-edge
technologies. Scoring techniques are used for various purposes. Reliable scores
are calculated based on influential variables. Such variables can be computed
by different methods based on the domain. The research is concentrated on the
user's understanding of a given topic. The analysis is based on a scoring index
by using Large Language Models. The user can then compare and contrast the
understanding of a topic that they recently learned. The results are then
contributed towards learning analytics and progression is made for enhancing
the learning ability. In this research, the focus is on summarizing a PDF
document and gauging a user's understanding of its content. The process
involves utilizing a Langchain tool to summarize the PDF and extract the
essential information. By employing this technique, the research aims to
determine how well the user comprehends the summarized content.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02766" title="Abstract">arXiv:2310.02766</a> [<a href="/pdf/2310.02766" title="Download PDF">pdf</a>, <a href="/format/2310.02766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Likelihood-Based Methods Improve Parameter Estimation in Opinion  Dynamics Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lenti%2C+J">Jacopo Lenti</a>, 
<a href="/search/cs?searchtype=author&query=De+Francisci+Morales%2C+G">Gianmarco De Francisci Morales</a>, 
<a href="/search/cs?searchtype=author&query=Monti%2C+C">Corrado Monti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">We show that a maximum likelihood approach for parameter estimation in
agent-based models (ABMs) of opinion dynamics outperforms the typical
simulation-based approach. Simulation-based approaches simulate the model
repeatedly in search of a set of parameters that generates data similar enough
to the observed one. In contrast, likelihood-based approaches derive a
likelihood function that connects the unknown parameters to the observed data
in a statistically principled way. We compare these two approaches on the
well-known bounded-confidence model of opinion dynamics. We do so on three
realistic scenarios of increasing complexity depending on data availability:
(i) fully observed opinions and interactions, (ii) partially observed
interactions, (iii) observed interactions with noisy proxies of the opinions.
We highlight how identifying observed and latent variables is fundamental for
connecting the model to the data. To realize the likelihood-based approach, we
first cast the model into a probabilistic generative guise that supports a
proper data likelihood. Then, we describe the three scenarios via probabilistic
graphical models and show the nuances that go into translating the model.
Finally, we implement the resulting probabilistic models in an automatic
differentiation framework (PyTorch). This step enables easy and efficient
maximum likelihood estimation via gradient descent. Our experimental results
show that the maximum likelihood estimates are up to 4x more accurate and
require up to 200x less computational time.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02767" title="Abstract">arXiv:2310.02767</a> [<a href="/pdf/2310.02767" title="Download PDF">pdf</a>, <a href="/ps/2310.02767" title="Download PostScript">ps</a>, <a href="/format/2310.02767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernel-based function learning in dynamic and non stationary  environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giaretta%2C+A">Alberto Giaretta</a>, 
<a href="/search/cs?searchtype=author&query=Bisiacco%2C+M">Mauro Bisiacco</a>, 
<a href="/search/cs?searchtype=author&query=Pillonetto%2C+G">Gianluigi Pillonetto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">One central theme in machine learning is function estimation from sparse and
noisy data. An example is supervised learning where the elements of the
training set are couples, each containing an input location and an output
response. In the last decades, a substantial amount of work has been devoted to
design estimators for the unknown function and to study their convergence to
the optimal predictor, also characterizing the learning rate. These results
typically rely on stationary assumptions where input locations are drawn from a
probability distribution that does not change in time. In this work, we
consider kernel-based ridge regression and derive convergence conditions under
non stationary distributions, addressing also cases where stochastic adaption
may happen infinitely often. This includes the important
exploration-exploitation problems where e.g. a set of agents/robots has to
monitor an environment to reconstruct a sensorial field and their movements
rules are continuously updated on the basis of the acquired knowledge on the
field and/or the surrounding environment.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02772" title="Abstract">arXiv:2310.02772</a> [<a href="/pdf/2310.02772" title="Download PDF">pdf</a>, <a href="/format/2310.02772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spike Accumulation Forwarding for Effective Training of Spiking Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saiin%2C+R">Ryuji Saiin</a>, 
<a href="/search/cs?searchtype=author&query=Shirakawa%2C+T">Tomoya Shirakawa</a>, 
<a href="/search/cs?searchtype=author&query=Yoshihara%2C+S">Sota Yoshihara</a>, 
<a href="/search/cs?searchtype=author&query=Sawada%2C+Y">Yoshihide Sawada</a>, 
<a href="/search/cs?searchtype=author&query=Kusumoto%2C+H">Hiroyuki Kusumoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 12 figures, Appendix:8 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this article, we propose a new paradigm for training spiking neural
networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are
energy-efficient but difficult to train. Consequently, many researchers have
proposed various methods to solve this problem, among which online training
through time (OTTT) is a method that allows inferring at each time step while
suppressing the memory cost. However, to compute efficiently on GPUs, OTTT
requires operations with spike trains and weighted summation of spike trains
during forwarding. In addition, OTTT has shown a relationship with the Spike
Representation, an alternative training method, though theoretical agreement
with Spike Representation has yet to be proven. Our proposed method can solve
these problems; namely, SAF can halve the number of operations during the
forward process, and it can be theoretically proven that SAF is consistent with
the Spike Representation and OTTT, respectively. Furthermore, we confirmed the
above contents through experiments and showed that it is possible to reduce
memory and training time while maintaining accuracy.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02774" title="Abstract">arXiv:2310.02774</a> [<a href="/pdf/2310.02774" title="Download PDF">pdf</a>, <a href="/format/2310.02774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks and Time Series as Directed Graphs for Quality  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Simonetti%2C+A">Angelica Simonetti</a>, 
<a href="/search/cs?searchtype=author&query=Zanchetta%2C+F">Ferdinando Zanchetta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, Comments Welcome!
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Neural Networks (GNNs) are becoming central in the study of time
series, coupled with existing algorithms as Temporal Convolutional Networks and
Recurrent Neural Networks. In this paper, we see time series themselves as
directed graphs, so that their topology encodes time dependencies and we start
to explore the effectiveness of GNNs architectures on them. We develop two
distinct Geometric Deep Learning models, a supervised classifier and an
autoencoder-like model for signal reconstruction. We apply these models on a
quality recognition problem.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02775" title="Abstract">arXiv:2310.02775</a> [<a href="/pdf/2310.02775" title="Download PDF">pdf</a>, <a href="/ps/2310.02775" title="Download PostScript">ps</a>, <a href="/format/2310.02775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High order numerical methods based on quadratic spline collocation  method and averaged L1 scheme for the variable-order time fractional  mobile/immobile diffusion equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ye%2C+X">Xiao Ye</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+J">Jun Liu</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+B">Bingyin Zhang</a>, 
<a href="/search/math?searchtype=author&query=Fu%2C+H">Hongfei Fu</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+Y">Yue Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we consider the variable-order time fractional mobile/immobile
diffusion (TF-MID) equation in two-dimensional spatial domain, where the
fractional order $\alpha(t)$ satisfies $0&lt;\alpha_{*}\leq \alpha(t)\leq
\alpha^{*}&lt;1$. We combine the quadratic spline collocation (QSC) method and the
$L1^+$ formula to propose a QSC-$L1^+$ scheme. It can be proved that, the
QSC-$L1^+$ scheme is unconditionally stable and convergent with
$\mathcal{O}(\tau^{\min{\{3-\alpha^*-\alpha(0),2\}}} + \Delta x^{2}+\Delta
y^{2})$, where $\tau$, $\Delta x$ and $\Delta y$ are the temporal and spatial
step sizes, respectively. With some proper assumptions on $\alpha(t)$, the
QSC-$L1^+$ scheme has second temporal convergence order even on the uniform
mesh, without any restrictions on the solution of the equation. We further
construct a novel alternating direction implicit (ADI) framework to develop an
ADI-QSC-$L1^+$ scheme, which has the same unconditionally stability and
convergence orders. In addition, a fast implementation for the ADI-QSC-$L1^+$
scheme based on the exponential-sum-approximation (ESA) technique is proposed.
Moreover, we also introduce the optimal QSC method to improve the spatial
convergence to fourth-order. Numerical experiments are attached to support the
theoretical analysis, and to demonstrate the effectiveness of the proposed
schemes.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02776" title="Abstract">arXiv:2310.02776</a> [<a href="/pdf/2310.02776" title="Download PDF">pdf</a>, <a href="/format/2310.02776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Shuffle: An Efficient Channel Mixture Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+K">Kaijun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhuowen Yin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yushu Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+K">Kailing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiangmin Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The redundancy of Convolutional neural networks not only depends on weights
but also depends on inputs. Shuffling is an efficient operation for mixing
channel information but the shuffle order is usually pre-defined. To reduce the
data-dependent redundancy, we devise a dynamic shuffle module to generate
data-dependent permutation matrices for shuffling. Since the dimension of
permutation matrix is proportional to the square of the number of input
channels, to make the generation process efficiently, we divide the channels
into groups and generate two shared small permutation matrices for each group,
and utilize Kronecker product and cross group shuffle to obtain the final
permutation matrices. To make the generation process learnable, based on
theoretical analysis, softmax, orthogonal regularization, and binarization are
employed to asymptotically approximate the permutation matrix. Dynamic shuffle
adaptively mixes channel information with negligible extra computation and
memory occupancy. Experiment results on image classification benchmark datasets
CIFAR-10, CIFAR-100, Tiny ImageNet and ImageNet have shown that our method
significantly increases ShuffleNets' performance. Adding dynamic generated
matrix with learnable static matrix, we further propose static-dynamic-shuffle
and show that it can serve as a lightweight replacement of ordinary pointwise
convolution.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02777" title="Abstract">arXiv:2310.02777</a> [<a href="/pdf/2310.02777" title="Download PDF">pdf</a>, <a href="/format/2310.02777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Role of Linguistic Priors in Measuring Compositional Generalization  of Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chenwei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L+E">Li Erran Li</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>, 
<a href="/search/cs?searchtype=author&query=Haffner%2C+P">Patrick Haffner</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+R">Rong Ge</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zaiwei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Compositionality is a common property in many modalities including natural
languages and images, but the compositional generalization of multi-modal
models is not well-understood. In this paper, we identify two sources of
visual-linguistic compositionality: linguistic priors and the interplay between
images and texts. We show that current attempts to improve compositional
generalization rely on linguistic priors rather than on information in the
image. We also propose a new metric for compositionality without such
linguistic priors.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02778" title="Abstract">arXiv:2310.02778</a> [<a href="/pdf/2310.02778" title="Download PDF">pdf</a>, <a href="/format/2310.02778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A UMLS-Augmented Framework for Improving Factuality in Large Language  Models within Healthcare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Marrese-Taylor%2C+E">Edison Marrese-Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+Y">Yuhe Ke</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Lechao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qingyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+I">Irene Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated powerful text generation
capabilities, bringing unprecedented innovation to the healthcare field. While
LLMs hold immense promise for applications in healthcare, applying them to real
clinical scenarios presents significant challenges, as these models may
generate content that deviates from established medical facts and even exhibit
potential biases. In our research, we develop an augmented LLM framework based
on the Unified Medical Language System (UMLS), aiming to better serve the
healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our
benchmark models, and conduct automatic evaluations using the ROUGE Score and
BERTScore on 104 questions from the LiveQA test set. Additionally, we establish
criteria for physician-evaluation based on four dimensions: Factuality,
Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician
evaluation with 20 questions on the LiveQA test set. Multiple resident
physicians conducted blind reviews to evaluate the generated content, and the
results indicate that this framework effectively enhances the factuality,
completeness, and relevance of generated content. Our research demonstrates the
effectiveness of using UMLS-augmented LLMs and highlights the potential
application value of LLMs in in medical question-answering.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02779" title="Abstract">arXiv:2310.02779</a> [<a href="/pdf/2310.02779" title="Download PDF">pdf</a>, <a href="/format/2310.02779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expected flow networks in stochastic environments and two-player  zero-sum games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiralerspong%2C+M">Marco Jiralerspong</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Bilun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Vucetic%2C+D">Danilo Vucetic</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Gidel%2C+G">Gauthier Gidel</a>, 
<a href="/search/cs?searchtype=author&query=Malkin%2C+N">Nikolay Malkin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Generative flow networks (GFlowNets) are sequential sampling models trained
to match a given distribution. GFlowNets have been successfully applied to
various structured object generation tasks, sampling a diverse set of
high-reward objects quickly. We propose expected flow networks (EFlowNets),
which extend GFlowNets to stochastic environments. We show that EFlowNets
outperform other GFlowNet formulations in stochastic tasks such as protein
design. We then extend the concept of EFlowNets to adversarial environments,
proposing adversarial flow networks (AFlowNets) for two-player zero-sum games.
We show that AFlowNets learn to find above 80% of optimal moves in Connect-4
via self-play and outperform AlphaZero in tournaments.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02782" title="Abstract">arXiv:2310.02782</a> [<a href="/pdf/2310.02782" title="Download PDF">pdf</a>, <a href="/format/2310.02782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering General Reinforcement Learning Algorithms with Adversarial  Environment Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jackson%2C+M+T">Matthew Thomas Jackson</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Minqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Parker-Holder%2C+J">Jack Parker-Holder</a>, 
<a href="/search/cs?searchtype=author&query=Vuorio%2C+R">Risto Vuorio</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chris Lu</a>, 
<a href="/search/cs?searchtype=author&query=Farquhar%2C+G">Gregory Farquhar</a>, 
<a href="/search/cs?searchtype=author&query=Whiteson%2C+S">Shimon Whiteson</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+J+N">Jakob Nicolaus Foerster</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The past decade has seen vast progress in deep reinforcement learning (RL) on
the back of algorithms manually designed by human researchers. Recently, it has
been shown that it is possible to meta-learn update rules, with the hope of
discovering algorithms that can perform well on a wide range of RL tasks.
Despite impressive initial results from algorithms such as Learned Policy
Gradient (LPG), there remains a generalization gap when these algorithms are
applied to unseen environments. In this work, we examine how characteristics of
the meta-training distribution impact the generalization performance of these
algorithms. Motivated by this analysis and building on ideas from Unsupervised
Environment Design (UED), we propose a novel approach for automatically
generating curricula to maximize the regret of a meta-learned optimizer, in
addition to a novel approximation of regret, which we name algorithmic regret
(AR). The result is our method, General RL Optimizers Obtained Via Environment
Design (GROOVE). In a series of experiments, we show that GROOVE achieves
superior generalization to LPG, and evaluate AR against baseline metrics from
UED, identifying it as a critical component of environment design in this
setting. We believe this approach is a step towards the discovery of truly
general RL algorithms, capable of solving a wide range of real-world
environments.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02784" title="Abstract">arXiv:2310.02784</a> [<a href="/pdf/2310.02784" title="Download PDF">pdf</a>, <a href="/format/2310.02784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAD Max Beyond Single-Node: Enabling Large Machine Learning Model  Acceleration on Distributed Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsia%2C+S">Samuel Hsia</a>, 
<a href="/search/cs?searchtype=author&query=Golden%2C+A">Alicia Golden</a>, 
<a href="/search/cs?searchtype=author&query=Acun-Uyan%2C+B">Bilge Acun-Uyan</a>, 
<a href="/search/cs?searchtype=author&query=Ardalani%2C+N">Newsha Ardalani</a>, 
<a href="/search/cs?searchtype=author&query=DeVito%2C+Z">Zachary DeVito</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Gu-Yeon Wei</a>, 
<a href="/search/cs?searchtype=author&query=Brooks%2C+D">David Brooks</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Carole-Jean Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Hardware Architecture (cs.AR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Training and deploying large machine learning (ML) models is time-consuming
and requires significant distributed computing infrastructures. Based on
real-world large model training on datacenter-scale infrastructures, we show
14~32% of all GPU hours are spent on communication with no overlapping
computation. To minimize the outstanding communication latency, in this work,
we develop an agile performance modeling framework to guide parallelization and
hardware-software co-design strategies. Using the suite of real-world large ML
models on state-of-the-art GPU training hardware, we demonstrate 2.24x and
5.27x throughput improvement potential for pre-training and inference
scenarios, respectively.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02790" title="Abstract">arXiv:2310.02790</a> [<a href="/pdf/2310.02790" title="Download PDF">pdf</a>, <a href="/format/2310.02790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low Resource Summarization using Pre-trained Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Munaf%2C+M">Mubashir Munaf</a>, 
<a href="/search/cs?searchtype=author&query=Afzal%2C+H">Hammad Afzal</a>, 
<a href="/search/cs?searchtype=author&query=Iltaf%2C+N">Naima Iltaf</a>, 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+K">Khawir Mahmood</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the advent of Deep Learning based Artificial Neural Networks models,
Natural Language Processing (NLP) has witnessed significant improvements in
textual data processing in terms of its efficiency and accuracy. However, the
research is mostly restricted to high-resource languages such as English and
low-resource languages still suffer from a lack of available resources in terms
of training datasets as well as models with even baseline evaluation results.
Considering the limited availability of resources for low-resource languages,
we propose a methodology for adapting self-attentive transformer-based
architecture models (mBERT, mT5) for low-resource summarization, supplemented
by the construction of a new baseline dataset (76.5k article, summary pairs) in
a low-resource language Urdu. Choosing news (a publicly available source) as
the application domain has the potential to make the proposed methodology
useful for reproducing in other languages with limited resources. Our adapted
summarization model \textit{urT5} with up to 44.78\% reduction in size as
compared to \textit{mT5} can capture contextual information of low resource
language effectively with evaluation score (up to 46.35 ROUGE-1, 77 BERTScore)
at par with state-of-the-art models in high resource language English
\textit{(PEGASUS: 47.21, BART: 45.14 on XSUM Dataset)}. The proposed method
provided a baseline approach towards extractive as well as abstractive
summarization with competitive evaluation results in a limited resource setup.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02791" title="Abstract">arXiv:2310.02791</a> [<a href="/pdf/2310.02791" title="Download PDF">pdf</a>, <a href="/format/2310.02791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> R-LGP: A Reachability-guided Logic-geometric Programming Framework for  Optimal Task and Motion Planning on Mobile Manipulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ly%2C+K+T">Kim Tien Ly</a>, 
<a href="/search/cs?searchtype=author&query=Semenov%2C+V">Valeriy Semenov</a>, 
<a href="/search/cs?searchtype=author&query=Risiglione%2C+M">Mattia Risiglione</a>, 
<a href="/search/cs?searchtype=author&query=Merkt%2C+W">Wolfgang Merkt</a>, 
<a href="/search/cs?searchtype=author&query=Havoutis%2C+I">Ioannis Havoutis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper presents an optimization-based solution to task and motion
planning (TAMP) on mobile manipulators. Logic-geometric programming (LGP) has
shown promising capabilities for optimally dealing with hybrid TAMP problems
that involve abstract and geometric constraints. However, LGP does not scale
well to high-dimensional systems (e.g. mobile manipulators) and can suffer from
obstacle avoidance issues. In this work, we extend LGP with a sampling-based
reachability graph to enable solving optimal TAMP on high-DoF mobile
manipulators. The proposed reachability graph can incorporate environmental
information (obstacles) to provide the planner with sufficient geometric
constraints. This reachability-aware heuristic efficiently prunes infeasible
sequences of actions in the continuous domain, hence, it reduces replanning by
securing feasibility at the final full trajectory optimization. Our framework
proves to be time-efficient in computing optimal and collision-free solutions,
while outperforming the current state of the art on metrics of success rate,
planning time, path length and number of steps. We validate our framework on
the physical Toyota HSR robot and report comparisons on a series of mobile
manipulation tasks of increasing difficulty.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02794" title="Abstract">arXiv:2310.02794</a> [<a href="/pdf/2310.02794" title="Download PDF">pdf</a>, <a href="/ps/2310.02794" title="Download PostScript">ps</a>, <a href="/format/2310.02794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability Improvements for Fast Matrix Multiplication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Vermeylen%2C+C">Charlotte Vermeylen</a>, 
<a href="/search/math?searchtype=author&query=Van+Barel%2C+M">Marc Van Barel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We implement an Augmented Lagrangian method to minimize a constrained
least-squares cost function designed to find polyadic decompositions of the
matrix multiplication tensor. We use this method to obtain new discrete
decompositions and parameter families of decompositions. Using these
parametrizations, faster and more stable matrix multiplication algorithms can
be discovered.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02799" title="Abstract">arXiv:2310.02799</a> [<a href="/pdf/2310.02799" title="Download PDF">pdf</a>, <a href="/format/2310.02799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dissecting Smart Contract Languages: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soud%2C+M">Majd Soud</a>, 
<a href="/search/cs?searchtype=author&query=Hj%C3%A1lmt%C3%BDsson%2C+G">G&#xed;sli Hj&#xe1;lmt&#xfd;sson</a>, 
<a href="/search/cs?searchtype=author&query=Hamdaqa%2C+M">Mohammad Hamdaqa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Blockchain is a distributed ledger technology that gained popularity for
enabling the transformation of cryptocurrency among peers without mediation by
a centralized third-party authority. Smart contracts expand the applications of
blockchain technology and have played a role in its widespread adoption. Smart
contracts are immutable digital programs that are deployed on blockchains to
codify agreements between parties. Existing smart contract implementations have
faced challenges, including security vulnerabilities, leading to significant
losses and concerns. This has stimulated a wave of attempts to improve Smart
Contract Languages (SCLs) to overcome implementation challenges and ensure code
quality, producing many languages with diverse features. Scholars have made
some attempts to classify SCLs and clarify the process of selecting an SCL, but
to the best of our knowledge, no comprehensive survey of existing SCLs has been
published. Our work surpasses earlier efforts by evaluating a significantly
larger set of SCLs, in greater depth, to ease the process of SCL selection for
blockchain research and implementation. In this paper, we (1) propose a robust
framework for comparing existing SCLs, (2) analyze and discuss 36 SCLs,
addressing issues beyond those used to construct the comparison framework, and
(3) define new parameters for future research and development of SCLs. The
survey provides a guide for those who intend to select or use an SCL to
implement smart contracts, develop new SCLs, or add new extensions to the
existing SCLs.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02800" title="Abstract">arXiv:2310.02800</a> [<a href="/pdf/2310.02800" title="Download PDF">pdf</a>, <a href="/format/2310.02800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Everest: GPU-Accelerated System For Mining Temporal Motifs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yichao Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Haojie Ye</a>, 
<a href="/search/cs?searchtype=author&query=Kaza%2C+S+V+W">Sanketh Vedula Wynn Kaza</a>, 
<a href="/search/cs?searchtype=author&query=Talati%2C+N">Nishil Talati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Temporal motif mining is the task of finding the occurrences of subgraph
patterns within a large input temporal graph that obey the specified structural
and temporal constraints. Despite its utility in several critical application
domains that demand high performance (e.g., detecting fraud in financial
transaction graphs), the performance of existing software is limited on
commercial hardware platforms, in that it runs for tens of hours. This paper
presents Everest - a system that efficiently maps the workload of mining
(supports both enumeration and counting) temporal motifs to the highly parallel
GPU architecture. In particular, using an input temporal graph and a more
expressive user-defined temporal motif query definition compared to prior
works, Everest generates an execution plan and runtime primitives that optimize
the workload execution by exploiting the high compute throughput of a GPU.
Everest generates motif-specific mining code to reduce long-latency memory
accesses and frequent thread divergence operations. Everest incorporates novel
low-cost runtime mechanisms to enable load balancing to improve GPU hardware
utilization. To support large graphs that do not fit on GPU memory, Everest
also supports multi-GPU execution by intelligently partitioning the edge list
that prevents inter-GPU communication. Everest hides the implementation
complexity of presented optimizations away from the targeted system user for
better usability. Our evaluation shows that, using proposed optimizations,
Everest improves the performance of a baseline GPU implementation by 19x, on
average.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02804" title="Abstract">arXiv:2310.02804</a> [<a href="/pdf/2310.02804" title="Download PDF">pdf</a>, <a href="/format/2310.02804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DOMINO: A Dual-System for Multi-step Visual Language Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peifang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Golovneva%2C+O">Olga Golovneva</a>, 
<a href="/search/cs?searchtype=author&query=Aghajanyan%2C+A">Armen Aghajanyan</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Celikyilmaz%2C+A">Asli Celikyilmaz</a>, 
<a href="/search/cs?searchtype=author&query=Fazel-Zarandi%2C+M">Maryam Fazel-Zarandi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Visual language reasoning requires a system to extract text or numbers from
information-dense images like charts or plots and perform logical or arithmetic
reasoning to arrive at an answer. To tackle this task, existing work relies on
either (1) an end-to-end vision-language model trained on a large amount of
data, or (2) a two-stage pipeline where a captioning model converts the image
into text that is further read by another large language model to deduce the
answer. However, the former approach forces the model to answer a complex
question with one single step, and the latter approach is prone to inaccurate
or distracting information in the converted text that can confuse the language
model. In this work, we propose a dual-system for multi-step multimodal
reasoning, which consists of a "System-1" step for visual information
extraction and a "System-2" step for deliberate reasoning. Given an input,
System-2 breaks down the question into atomic sub-steps, each guiding System-1
to extract the information required for reasoning from the image. Experiments
on chart and plot datasets show that our method with a pre-trained System-2
module performs competitively compared to prior work on in- and
out-of-distribution data. By fine-tuning the System-2 module (LLaMA-2 70B) on
only a small amount of data on multi-step reasoning, the accuracy of our method
is further improved and surpasses the best fully-supervised end-to-end approach
by 5.7% and a pipeline approach with FlanPaLM (540B) by 7.5% on a challenging
dataset with human-authored questions.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02806" title="Abstract">arXiv:2310.02806</a> [<a href="/pdf/2310.02806" title="Download PDF">pdf</a>, <a href="/format/2310.02806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Data-facilitated Numerical Method for Richards Equation to Model Water  Flow Dynamics in Soil
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Song%2C+Z">Zeyuan Song</a>, 
<a href="/search/math?searchtype=author&query=Jiang%2C+Z">Zheyu Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 11 figures, submitted to Water Resources Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Root-zone soil moisture monitoring is essential for precision agriculture,
smart irrigation, and drought prevention. Modeling the spatiotemporal water
flow dynamics in soil is typically achieved by solving a hydrological model,
such as the Richards equation which is a highly nonlinear partial differential
equation (PDE). In this paper, we present a novel data-facilitated numerical
method for solving the mixed-form Richards equation. This numerical method,
which we call the D-GRW (Data-facilitated global Random Walk) method,
synergistically integrates adaptive linearization scheme, neural networks, and
global random walk in a finite volume discretization framework to produce
accurate numerical solutions of the Richards equation with guaranteed
convergence under reasonable assumptions. Through three illustrative examples,
we demonstrate and discuss the superior accuracy and mass conservation
performance of our D-GRW method and compare it with benchmark numerical methods
and commercial solver.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02807" title="Abstract">arXiv:2310.02807</a> [<a href="/pdf/2310.02807" title="Download PDF">pdf</a>, <a href="/format/2310.02807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Deep Instance Generative Framework for MILP Solvers Under Limited Data  Availability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+Z">Zijie Geng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xijun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongdong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Feng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In the past few years, there has been an explosive surge in the use of
machine learning (ML) techniques to address combinatorial optimization (CO)
problems, especially mixed-integer linear programs (MILPs). Despite the
achievements, the limited availability of real-world instances often leads to
sub-optimal decisions and biased solver assessments, which motivates a suite of
synthetic MILP instance generation techniques. However, existing methods either
rely heavily on expert-designed formulations or struggle to capture the rich
features of real-world instances. To tackle this problem, we propose G2MILP,
which to the best of our knowledge is the first deep generative framework for
MILP instances. Specifically, G2MILP represents MILP instances as bipartite
graphs, and applies a masked variational autoencoder to iteratively corrupt and
replace parts of the original graphs to generate new ones. The appealing
feature of G2MILP is that it can learn to generate novel and realistic MILP
instances without prior expert-designed formulations, while preserving the
structures and computational hardness of real-world datasets, simultaneously.
Thus the generated instances can facilitate downstream tasks for enhancing MILP
solvers under limited data availability. We design a suite of benchmarks to
evaluate the quality of the generated MILP instances. Experiments demonstrate
that our method can produce instances that closely resemble real-world datasets
in terms of both structures and computational hardness.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02812" title="Abstract">arXiv:2310.02812</a> [<a href="/pdf/2310.02812" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-Series Classification in Smart Manufacturing Systems: An  Experimental Evaluation of State-of-the-Art Machine Learning Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farahani%2C+M+A">Mojtaba A. Farahani</a>, 
<a href="/search/cs?searchtype=author&query=McCormick%2C+M+R">M. R. McCormick</a>, 
<a href="/search/cs?searchtype=author&query=Harik%2C+R">Ramy Harik</a>, 
<a href="/search/cs?searchtype=author&query=Wuest%2C+T">Thorsten Wuest</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the Journal of Manufacturing systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Manufacturing is gathering extensive amounts of diverse data, thanks to the
growing number of sensors and rapid advances in sensing technologies. Among the
various data types available in SMS settings, time-series data plays a pivotal
role. Hence, TSC emerges is crucial in this domain. The objective of this study
is to fill this gap by providing a rigorous experimental evaluation of the SoTA
ML and DL algorithms for TSC tasks in manufacturing and industrial settings. We
first explored and compiled a comprehensive list of more than 92 SoTA
algorithms from both TSC and manufacturing literature. Following, we selected
the 36 most representative algorithms from this list. To evaluate their
performance across various manufacturing classification tasks, we curated a set
of 22 manufacturing datasets, representative of different characteristics that
cover diverse manufacturing problems. Subsequently, we implemented and
evaluated the algorithms on the manufacturing benchmark datasets, and analyzed
the results for each dataset. Based on the results, ResNet, DrCIF,
InceptionTime, and ARSENAL are the top-performing algorithms, boasting an
average accuracy of over 96.6% across all 22 manufacturing TSC datasets. These
findings underscore the robustness, efficiency, scalability, and effectiveness
of convolutional kernels in capturing temporal features in time-series data, as
three out of the top four performing algorithms leverage these kernels for
feature extraction. Additionally, LSTM, BiLSTM, and TS-LSTM algorithms deserve
recognition for their effectiveness in capturing features within time-series
data using RNN-based structures.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02815" title="Abstract">arXiv:2310.02815</a> [<a href="/pdf/2310.02815" title="Download PDF">pdf</a>, <a href="/format/2310.02815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoBEV: Elevating Roadside 3D Object Detection with Depth and Height  Complementarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Hao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+C">Chengshan Pang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kailun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+H">Huajian Ni</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yining Lin</a>, 
<a href="/search/cs?searchtype=author&query=Stiefelhagen%2C+R">Rainer Stiefelhagen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaiwei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The source code will be made publicly available at <a href="https://github.com/MasterHow/CoBEV">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Roadside camera-driven 3D object detection is a crucial task in intelligent
transportation systems, which extends the perception range beyond the
limitations of vision-centric vehicles and enhances road safety. While previous
studies have limitations in using only depth or height information, we find
both depth and height matter and they are in fact complementary. The depth
feature encompasses precise geometric cues, whereas the height feature is
primarily focused on distinguishing between various categories of height
intervals, essentially providing semantic context. This insight motivates the
development of Complementary-BEV (CoBEV), a novel end-to-end monocular 3D
object detection framework that integrates depth and height to construct robust
BEV representations. In essence, CoBEV estimates each pixel's depth and height
distribution and lifts the camera features into 3D space for lateral fusion
using the newly proposed two-stage complementary feature selection (CFS)
module. A BEV feature distillation framework is also seamlessly integrated to
further enhance the detection accuracy from the prior knowledge of the
fusion-modal CoBEV teacher. We conduct extensive experiments on the public 3D
detection benchmarks of roadside camera-based DAIR-V2X-I and Rope3D, as well as
the private Supremind-Road dataset, demonstrating that CoBEV not only achieves
the accuracy of the new state-of-the-art, but also significantly advances the
robustness of previous methods in challenging long-distance scenarios and noisy
camera disturbance, and enhances generalization by a large margin in
heterologous settings with drastic changes in scene and camera parameters. For
the first time, the vehicle AP score of a camera model reaches 80% on
DAIR-V2X-I in terms of easy mode. The source code will be made publicly
available at https://github.com/MasterHow/CoBEV.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02817" title="Abstract">arXiv:2310.02817</a> [<a href="/pdf/2310.02817" title="Download PDF">pdf</a>, <a href="/format/2310.02817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explicit Runge Kutta Methods that Alleviate Order Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Biswas%2C+A">Abhijit Biswas</a>, 
<a href="/search/math?searchtype=author&query=Ketcheson%2C+D+I">David I. Ketcheson</a>, 
<a href="/search/math?searchtype=author&query=Roberts%2C+S">Steven Roberts</a>, 
<a href="/search/math?searchtype=author&query=Seibold%2C+B">Benjamin Seibold</a>, 
<a href="/search/math?searchtype=author&query=Shirokoff%2C+D">David Shirokoff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Explicit Runge--Kutta (\rk{}) methods are susceptible to a reduction in the
observed order of convergence when applied to initial-boundary value problem
with time-dependent boundary conditions. We study conditions on \erk{} methods
that guarantee high-order convergence for linear problems; we refer to these
conditions as weak stage order conditions. We prove a general relationship
between the method's order, weak stage order, and number of stages. We derive
\erk{} methods with high weak stage order and demonstrate, through numerical
tests, that they avoid the order reduction phenomenon up to any order for
linear problems and up to order three for nonlinear problems.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02821" title="Abstract">arXiv:2310.02821</a> [<a href="/pdf/2310.02821" title="Download PDF">pdf</a>, <a href="/format/2310.02821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Vision Anomaly Detection with the Guidance of Language  Modality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+K">Kaihang Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yueting Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Siliang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent years have seen a surge of interest in anomaly detection for tackling
industrial defect detection, event detection, etc. However, existing
unsupervised anomaly detectors, particularly those for the vision modality,
face significant challenges due to redundant information and sparse latent
space. Conversely, the language modality performs well due to its relatively
single data. This paper tackles the aforementioned challenges for vision
modality from a multimodal point of view. Specifically, we propose Cross-modal
Guidance (CMG), which consists of Cross-modal Entropy Reduction (CMER) and
Cross-modal Linear Embedding (CMLE), to tackle the redundant information issue
and sparse space issue, respectively. CMER masks parts of the raw image and
computes the matching score with the text. Then, CMER discards irrelevant
pixels to make the detector focus on critical contents. To learn a more compact
latent space for the vision anomaly detector, CMLE learns a correlation
structure matrix from the language modality, and then the latent space of
vision modality will be learned with the guidance of the matrix. Thereafter,
the vision latent space will get semantically similar images closer. Extensive
experiments demonstrate the effectiveness of the proposed methods.
Particularly, CMG outperforms the baseline that only uses images by 16.81%.
Ablation experiments further confirm the synergy among the proposed methods, as
each component depends on the other to achieve optimal performance.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02823" title="Abstract">arXiv:2310.02823</a> [<a href="/pdf/2310.02823" title="Download PDF">pdf</a>, <a href="/format/2310.02823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Scale Logits for Temperature-Conditional GFlowNets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">Joohwan Ko</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dinghuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Ling Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+T">Taeyoung Yun</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Woochang Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jinkyoo Park</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">GFlowNets are probabilistic models that learn a stochastic policy that
sequentially generates compositional structures, such as molecular graphs. They
are trained with the objective of sampling such objects with probability
proportional to the object's reward. Among GFlowNets, the
temperature-conditional GFlowNets represent a family of policies indexed by
temperature, and each is associated with the correspondingly tempered reward
function. The major benefit of temperature-conditional GFlowNets is the
controllability of GFlowNets' exploration and exploitation through adjusting
temperature. We propose Learning to Scale Logits for temperature-conditional
GFlowNets (LSL-GFN), a novel architectural design that greatly accelerates the
training of temperature-conditional GFlowNets. It is based on the idea that
previously proposed temperature-conditioning approaches introduced numerical
challenges in the training of the deep network because different temperatures
may give rise to very different gradient profiles and ideal scales of the
policy's logits. We find that the challenge is greatly reduced if a learned
function of the temperature is used to scale the policy's logits directly. We
empirically show that our strategy dramatically improves the performances of
GFlowNets, outperforming other baselines, including reinforcement learning and
sampling methods, in terms of discovering diverse modes in multiple biochemical
tasks.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02832" title="Abstract">arXiv:2310.02832</a> [<a href="/pdf/2310.02832" title="Download PDF">pdf</a>, <a href="/format/2310.02832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Out-of-Distribution Detection by Leveraging Between-Layer Transformation  Smoothness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeleni%C4%87%2C+F">Fran Jeleni&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Juki%C4%87%2C+J">Josip Juki&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Tutek%2C+M">Martin Tutek</a>, 
<a href="/search/cs?searchtype=author&query=Puljiz%2C+M">Mate Puljiz</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0najder%2C+J">Jan &#x160;najder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Effective OOD detection is crucial for reliable machine learning models, yet
most current methods are limited in practical use due to requirements like
access to training data or intervention in training. We present a novel method
for detecting OOD data in deep neural networks based on transformation
smoothness between intermediate layers of a network (BLOOD), which is
applicable to pre-trained models without access to training data. BLOOD
utilizes the tendency of between-layer representation transformations of
in-distribution (ID) data to be smoother than the corresponding transformations
of OOD data, a property that we also demonstrate empirically for Transformer
networks. We evaluate BLOOD on several text classification tasks with
Transformer networks and demonstrate that it outperforms methods with
comparable resource requirements. Our analysis also suggests that when learning
simpler tasks, OOD data transformations maintain their original sharpness,
whereas sharpness increases with more complex tasks.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02835" title="Abstract">arXiv:2310.02835</a> [<a href="/pdf/2310.02835" title="Download PDF">pdf</a>, <a href="/format/2310.02835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Delving into CLIP latent space for Video Anomaly Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zanella%2C+L">Luca Zanella</a>, 
<a href="/search/cs?searchtype=author&query=Liberatori%2C+B">Benedetta Liberatori</a>, 
<a href="/search/cs?searchtype=author&query=Menapace%2C+W">Willi Menapace</a>, 
<a href="/search/cs?searchtype=author&query=Poiesi%2C+F">Fabio Poiesi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ricci%2C+E">Elisa Ricci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to Computer Vision and Image Understanding, project website and code are available at <a href="https://luca-zanella-dvl.github.io/AnomalyCLIP/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We tackle the complex problem of detecting and recognising anomalies in
surveillance videos at the frame level, utilising only video-level supervision.
We introduce the novel method AnomalyCLIP, the first to combine Large Language
and Vision (LLV) models, such as CLIP, with multiple instance learning for
joint video anomaly detection and classification. Our approach specifically
involves manipulating the latent CLIP feature space to identify the normal
event subspace, which in turn allows us to effectively learn text-driven
directions for abnormal events. When anomalous frames are projected onto these
directions, they exhibit a large feature magnitude if they belong to a
particular class. We also introduce a computationally efficient Transformer
architecture to model short- and long-term temporal dependencies between
frames, ultimately producing the final anomaly score and class prediction
probabilities. We compare AnomalyCLIP against state-of-the-art methods
considering three major anomaly detection benchmarks, i.e. ShanghaiTech,
UCF-Crime, and XD-Violence, and empirically show that it outperforms baselines
in recognising video anomalies.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02840" title="Abstract">arXiv:2310.02840</a> [<a href="/pdf/2310.02840" title="Download PDF">pdf</a>, <a href="/ps/2310.02840" title="Download PostScript">ps</a>, <a href="/format/2310.02840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mosaic benchmark networks: Modular link streams for testing dynamic  community detection algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asgari%2C+Y">Yasaman Asgari</a>, 
<a href="/search/cs?searchtype=author&query=Cazabet%2C+R">Remy Cazabet</a>, 
<a href="/search/cs?searchtype=author&query=Borgnat%2C+P">Pierre Borgnat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Community structure is a critical feature of real networks, providing
insights into nodes' internal organization. Nowadays, with the availability of
highly detailed temporal networks such as link streams, studying community
structures becomes more complex due to increased data precision and time
sensitivity. Despite numerous algorithms developed in the past decade for
dynamic community discovery, assessing their performance on link streams
remains a challenge. Synthetic benchmark graphs are a well-accepted approach
for evaluating static community detection algorithms. Additionally, there have
been some proposals for slowly evolving communities in low-resolution temporal
networks like snapshots. Nevertheless, this approach is not yet suitable for
link streams. To bridge this gap, we introduce a novel framework that generates
synthetic modular link streams with predefined communities. Subsequently, we
evaluate established dynamic community detection methods to uncover limitations
that may not be evident in snapshots with slowly evolving communities. While no
method emerges as a clear winner, we observe notable differences among them.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02842" title="Abstract">arXiv:2310.02842</a> [<a href="/pdf/2310.02842" title="Download PDF">pdf</a>, <a href="/format/2310.02842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task  Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dun%2C+C">Chen Dun</a>, 
<a href="/search/cs?searchtype=author&query=Del+Carmen+Hipolito+Garcia%2C+M">Mirian Del Carmen Hipolito Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Guoqing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Awadallah%2C+A+H">Ahmed Hassan Awadallah</a>, 
<a href="/search/cs?searchtype=author&query=Kyrillidis%2C+A">Anastasios Kyrillidis</a>, 
<a href="/search/cs?searchtype=author&query=Sim%2C+R">Robert Sim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have the ability to solve a variety of tasks,
such as text summarization and mathematical questions, just out of the box, but
they are often trained with a single task in mind. Due to high computational
costs, the current trend is to use prompt instruction tuning to better adjust
monolithic, pretrained LLMs for new -- but often individual -- downstream
tasks. Thus, how one would expand prompt tuning to handle -- concomitantly --
heterogeneous tasks and data distributions is a widely open question. To
address this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs,
associated with smart gating functionality: the latter -- whose design is one
of the contributions of this paper -- can identify relevant skills embedded in
different groups of prompts and dynamically assign combined experts (i.e.,
collection of prompts), based on the target task. Additionally, MoPs are
empirically agnostic to any model compression technique applied -- for
efficiency reasons -- as well as instruction data source and task composition.
In practice, MoPs can simultaneously mitigate prompt training "interference" in
multi-task, multi-source scenarios (e.g., task and data heterogeneity across
sources), as well as possible implications from model approximations. As a
highlight, MoPs manage to decrease final perplexity from $\sim20\%$ up to
$\sim70\%$, as compared to baselines, in the federated scenario, and from $\sim
3\%$ up to $\sim30\%$ in the centralized scenario.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02843" title="Abstract">arXiv:2310.02843</a> [<a href="/pdf/2310.02843" title="Download PDF">pdf</a>, <a href="/ps/2310.02843" title="Download PostScript">ps</a>, <a href="/format/2310.02843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incorporating Target Vehicle Trajectories Predicted by Deep Learning  Into Model Predictive Controlled Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dang%2C+N">Ni Dang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zengjie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jizheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Leibold%2C+M">Marion Leibold</a>, 
<a href="/search/cs?searchtype=author&query=Buss%2C+M">Martin Buss</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Model Predictive Control (MPC) has been widely applied to the motion planning
of autonomous vehicles. An MPC-controlled vehicle is required to predict its
own trajectories in a finite prediction horizon according to its model. Beyond
this, the vehicle should also incorporate the prediction of the trajectory of
its nearby vehicles, or target vehicles (TVs) into its decision-making. The
conventional trajectory prediction methods, such as the constant-speed-based
ones, are too trivial to accurately capture the potential collision risks. In
this report, we propose a novel MPC-based motion planning method for an
autonomous vehicle with a set of risk-aware constraints. These constraints
incorporate the predicted trajectory of a TV learned using a
deep-learning-based method. A recurrent neural network (RNN) is used to predict
the TV's future trajectory based on its historical data. Then, the predicted TV
trajectory is incorporated into the optimization of the MPC of the ego vehicle
to generate collision-free motion. Simulation studies are conducted to showcase
the prediction accuracy of the RNN model and the collision-free trajectories
generated by the MPC.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02845" title="Abstract">arXiv:2310.02845</a> [<a href="/pdf/2310.02845" title="Download PDF">pdf</a>, <a href="/ps/2310.02845" title="Download PostScript">ps</a>, <a href="/format/2310.02845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Note on a Translation from First-Order Logic into the Calculus of  Relations Preserving Validity and Finite Validity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakamura%2C+Y">Yoshiki Nakamura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">In this note, we give a linear-size translation from formulas of first-order
logic into equations of the calculus of relations preserving validity and
finite validity. Our translation also gives a linear-size conservative
reduction from formulas of first-order logic into formulas of the
three-variable fragment of first-order logic.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02847" title="Abstract">arXiv:2310.02847</a> [<a href="/pdf/2310.02847" title="Download PDF">pdf</a>, <a href="/format/2310.02847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Length of Strongly Monotone Descending Chains over $\mathbb{N}^d$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmitz%2C+S">Sylvain Schmitz</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+L">Lia Sch&#xfc;tze</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">A recent breakthrough by K\"unnemann, Mazowiecki, Sch\"utze, Sinclair-Banks,
and Wegrzycki (ICALP, 2023) bounds the running time for the coverability
problem in $d$-dimensional vector addition systems under unary encoding to
$n^{2^{O(d)}}$, improving on Rackoff's $n^{2^{O(d\lg d)}}$ upper bound (Theor.
Comput. Sci., 1978), and provides conditional matching lower bounds.
<br />In this paper, we revisit Lazi\'c and Schmitz' "ideal view" of the backward
coverability algorithm (Inform. Comput., 2021) in the light of this
breakthrough. We show that the controlled strongly monotone descending chains
of downwards-closed sets over $\mathbb{N}^d$ that arise from the dual backward
coverability algorithm of Lazi\'c and Schmitz on $d$-dimensional unary vector
addition systems also enjoy this tight $n^{2^{O(d)}}$ upper bound on their
length, and that this also translates into the same bound on the running time
of the backward coverability algorithm.
<br />Furthermore, our analysis takes place in a more general setting than that of
Lazi\'c and Schmitz, which allows to show the same results and improve on the
2EXPSPACE upper bound derived by Benedikt, Duff, Sharad, and Worrell (LICS,
2017) for the coverability problem in invertible affine nets.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02848" title="Abstract">arXiv:2310.02848</a> [<a href="/pdf/2310.02848" title="Download PDF">pdf</a>, <a href="/format/2310.02848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Magicremover: Tuning-free Text-guided Image inpainting with Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Siyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Liqian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">JingJing Fu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">You He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image inpainting aims to fill in the missing pixels with visually coherent
and semantically plausible content. Despite the great progress brought from
deep generative models, this task still suffers from i. the difficulties in
large-scale realistic data collection and costly model training; and ii. the
intrinsic limitations in the traditionally user-defined binary masks on objects
with unclear boundaries or transparent texture. In this paper, we propose
MagicRemover, a tuning-free method that leverages the powerful diffusion models
for text-guided image inpainting. We introduce an attention guidance strategy
to constrain the sampling process of diffusion models, enabling the erasing of
instructed areas and the restoration of occluded content. We further propose a
classifier optimization algorithm to facilitate the denoising stability within
less sampling steps. Extensive comparisons are conducted among our MagicRemover
and state-of-the-art methods including quantitative evaluation and user study,
demonstrating the significant improvement of MagicRemover on high-quality image
inpainting. We will release our code at https://github.com/exisas/Magicremover.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02854" title="Abstract">arXiv:2310.02854</a> [<a href="/pdf/2310.02854" title="Download PDF">pdf</a>, <a href="/format/2310.02854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Domain Causal Representation Learning via Weak Distributional  Invariances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+K">Kartik Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Mansouri%2C+A">Amin Mansouri</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yixin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Causal representation learning has emerged as the center of action in causal
machine learning research. In particular, multi-domain datasets present a
natural opportunity for showcasing the advantages of causal representation
learning over standard unsupervised representation learning. While recent works
have taken crucial steps towards learning causal representations, they often
lack applicability to multi-domain datasets due to over-simplifying assumptions
about the data; e.g. each domain comes from a different single-node perfect
intervention. In this work, we relax these assumptions and capitalize on the
following observation: there often exists a subset of latents whose certain
distributional properties (e.g., support, variance) remain stable across
domains; this property holds when, for example, each domain comes from a
multi-node imperfect intervention. Leveraging this observation, we show that
autoencoders that incorporate such invariances can provably identify the stable
set of latents from the rest across different settings.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02859" title="Abstract">arXiv:2310.02859</a> [<a href="/pdf/2310.02859" title="Download PDF">pdf</a>, <a href="/format/2310.02859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Sampling in Unbounded Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaglan%2C+K">Kshitijaa Jaglan</a> (1), 
<a href="/search/cs?searchtype=author&query=Chaitanya%2C+M">Meher Chaitanya</a> (2), 
<a href="/search/cs?searchtype=author&query=Sharma%2C+T">Triansh Sharma</a> (1), 
<a href="/search/cs?searchtype=author&query=Singam%2C+A">Abhijeeth Singam</a> (1), 
<a href="/search/cs?searchtype=author&query=Goyal%2C+N">Nidhi Goyal</a> (3), 
<a href="/search/cs?searchtype=author&query=Kumaraguru%2C+P">Ponnurangam Kumaraguru</a> (1), 
<a href="/search/cs?searchtype=author&query=Brandes%2C+U">Ulrik Brandes</a> (2) ((1) IIIT Hyderabad, (2) Social Networks Lab, ETH Z&#xfc;rich, (3) IIIT Delhi)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">The default approach to deal with the enormous size and limited accessibility
of many Web and social media networks is to sample one or more subnetworks from
a conceptually unbounded unknown network. Clearly, the extracted subnetworks
will crucially depend on the sampling scheme. Motivated by studies of homophily
and opinion formation, we propose a variant of snowball sampling designed to
prioritize inclusion of entire cohesive communities rather than any kind of
representativeness, breadth, or depth of coverage. The method is illustrated on
a concrete example, and experiments on synthetic networks suggest that it
behaves as desired.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02861" title="Abstract">arXiv:2310.02861</a> [<a href="/pdf/2310.02861" title="Download PDF">pdf</a>, <a href="/ps/2310.02861" title="Download PostScript">ps</a>, <a href="/format/2310.02861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiangyu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sibo Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph-level anomaly detection has gained significant attention as it finds
many applications in various domains, such as cancer diagnosis and enzyme
prediction. However, existing methods fail to capture the underlying properties
of graph anomalies, resulting in unexplainable framework design and
unsatisfying performance. In this paper, we take a step back and re-investigate
the spectral differences between anomalous and normal graphs. Our main
observation shows a significant disparity in the accumulated spectral energy
between these two classes. Moreover, we prove that the accumulated spectral
energy of the graph signal can be represented by its Rayleigh Quotient,
indicating that the Rayleigh Quotient is a driving factor behind the anomalous
properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph
Neural Network (RQGNN), the first spectral GNN for graph-level anomaly
detection, providing a new perspective on exploring the inherent spectral
features of anomalous graphs. Specifically, we introduce a novel framework that
consists of two components: the Rayleigh Quotient learning component (RQL) and
Chebyshev Wavelet GNN with RQ-pooling (CWGNN-RQ). RQL explicitly captures the
Rayleigh Quotient of graphs and CWGNN-RQ implicitly explores the spectral space
of graphs. Extensive experiments on 10 real-world datasets show that RQGNN
outperforms the best rival by 6.74% in Macro-F1 score and 1.44% in AUC,
demonstrating the effectiveness of our framework.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02862" title="Abstract">arXiv:2310.02862</a> [<a href="/pdf/2310.02862" title="Download PDF">pdf</a>, <a href="/format/2310.02862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel asymmetrical autoencoder with a sparsifying discrete cosine  Stockwell transform layer for gearbox sensor data compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Daoguang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+H">Hongyi Pan</a>, 
<a href="/search/cs?searchtype=author&query=Karimi%2C+H+R">Hamid Reza Karimi</a>, 
<a href="/search/cs?searchtype=author&query=Ozevin%2C+D">Didem Ozevin</a>, 
<a href="/search/cs?searchtype=author&query=Cetin%2C+A+E">Ahmet Enis Cetin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">The lack of an efficient compression model remains a challenge for the
wireless transmission of gearbox data in non-contact gear fault diagnosis
problems. In this paper, we present a signal-adaptive asymmetrical autoencoder
with a transform domain layer to compress sensor signals. First, a new discrete
cosine Stockwell transform (DCST) layer is introduced to replace linear layers
in a multi-layer autoencoder. A trainable filter is implemented in the DCST
domain by utilizing the multiplication property of the convolution. A trainable
hard-thresholding layer is applied to reduce redundant data in the DCST layer
to make the feature map sparse. In comparison to the linear layer, the DCST
layer reduces the number of trainable parameters and improves the accuracy of
data reconstruction. Second, training the autoencoder with a sparsifying DCST
layer only requires a small number of datasets. The proposed method is superior
to other autoencoder-based methods on the University of Connecticut (UoC) and
Southeast University (SEU) gearbox datasets, as the average quality score is
improved by 2.00% at the lowest and 32.35% at the highest with a limited number
of training samples
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02864" title="Abstract">arXiv:2310.02864</a> [<a href="/pdf/2310.02864" title="Download PDF">pdf</a>, <a href="/format/2310.02864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimation of Models with Limited Data by Leveraging Shared Structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rui%2C+M">Maryann Rui</a>, 
<a href="/search/cs?searchtype=author&query=Horel%2C+T">Thibaut Horel</a>, 
<a href="/search/cs?searchtype=author&query=Dahleh%2C+M">Munther Dahleh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE Conference on Decision and Control (CDC) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST)

</div>
<p class="mathjax">Modern data sets, such as those in healthcare and e-commerce, are often
derived from many individuals or systems but have insufficient data from each
source alone to separately estimate individual, often high-dimensional, model
parameters. If there is shared structure among systems however, it may be
possible to leverage data from other systems to help estimate individual
parameters, which could otherwise be non-identifiable. In this paper, we assume
systems share a latent low-dimensional parameter space and propose a method for
recovering $d$-dimensional parameters for $N$ different linear systems, even
when there are only $T&lt;d$ observations per system. To do so, we develop a
three-step algorithm which estimates the low-dimensional subspace spanned by
the systems' parameters and produces refined parameter estimates within the
subspace. We provide finite sample subspace estimation error guarantees for our
proposed method. Finally, we experimentally validate our method on simulations
with i.i.d. regression data and as well as correlated time series data.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02870" title="Abstract">arXiv:2310.02870</a> [<a href="/pdf/2310.02870" title="Download PDF">pdf</a>, <a href="/format/2310.02870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable and Interpretable Deep Learning for Tabular Data: Introducing  InterpreTabNet with the Novel InterpreStability Metric
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wa%2C+S">Shiyun Wa</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xinai Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Minjuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 7 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As Artificial Intelligence (AI) integrates deeper into diverse sectors, the
quest for powerful models has intensified. While significant strides have been
made in boosting model capabilities and their applicability across domains, a
glaring challenge persists: many of these state-of-the-art models remain as
black boxes. This opacity not only complicates the explanation of model
decisions to end-users but also obstructs insights into intermediate processes
for model designers. To address these challenges, we introduce InterpreTabNet,
a model designed to enhance both classification accuracy and interpretability
by leveraging the TabNet architecture with an improved attentive module. This
design ensures robust gradient propagation and computational stability.
Additionally, we present a novel evaluation metric, InterpreStability, which
quantifies the stability of a model's interpretability. The proposed model and
metric mark a significant stride forward in explainable models' research,
setting a standard for transparency and interpretability in AI model design and
application across diverse sectors. InterpreTabNet surpasses other leading
solutions in tabular data analysis across varied application scenarios, paving
the way for further research into creating deep-learning models that are both
highly accurate and inherently explainable. The introduction of the
InterpreStability metric ensures that the interpretability of future models can
be measured and compared in a consistent and rigorous manner. Collectively,
these contributions have the potential to promote the design principles and
development of next-generation interpretable AI models, widening the adoption
of interpretable AI solutions in critical decision-making environments.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02874" title="Abstract">arXiv:2310.02874</a> [<a href="/pdf/2310.02874" title="Download PDF">pdf</a>, <a href="/format/2310.02874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recent Methodological Advances in Federated Learning for Healthcare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kreuter%2C+D">Daniel Kreuter</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yichen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dittmer%2C+S">S&#xf6;ren Dittmer</a>, 
<a href="/search/cs?searchtype=author&query=Tull%2C+S">Samuel Tull</a>, 
<a href="/search/cs?searchtype=author&query=Shadbahr%2C+T">Tolou Shadbahr</a>, 
<a href="/search/cs?searchtype=author&query=BloodCounts%21+Collaboration">BloodCounts! Collaboration</a>, 
<a href="/search/cs?searchtype=author&query=Preller%2C+J">Jacobus Preller</a>, 
<a href="/search/cs?searchtype=author&query=Rudd%2C+J+H+F">James H.F. Rudd</a>, 
<a href="/search/cs?searchtype=author&query=Aston%2C+J+A+D">John A.D. Aston</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6nlieb%2C+C">Carola-Bibiane Sch&#xf6;nlieb</a>, 
<a href="/search/cs?searchtype=author&query=Gleadall%2C+N">Nicholas Gleadall</a>, 
<a href="/search/cs?searchtype=author&query=Roberts%2C+M">Michael Roberts</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Supplementary table of extracted data at the end of the document
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">For healthcare datasets, it is often not possible to combine data samples
from multiple sites due to ethical, privacy or logistical concerns. Federated
learning allows for the utilisation of powerful machine learning algorithms
without requiring the pooling of data. Healthcare data has many simultaneous
challenges which require new methodologies to address, such as highly-siloed
data, class imbalance, missing data, distribution shifts and non-standardised
variables. Federated learning adds significant methodological complexity to
conventional centralised machine learning, requiring distributed optimisation,
communication between nodes, aggregation of models and redistribution of
models. In this systematic review, we consider all papers on Scopus that were
published between January 2015 and February 2023 and which describe new
federated learning methodologies for addressing challenges with healthcare
data. We performed a detailed review of the 89 papers which fulfilled these
criteria. Significant systemic issues were identified throughout the literature
which compromise the methodologies in many of the papers reviewed. We give
detailed recommendations to help improve the quality of the methodology
development for federated learning in healthcare.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02875" title="Abstract">arXiv:2310.02875</a> [<a href="/pdf/2310.02875" title="Download PDF">pdf</a>, <a href="/format/2310.02875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximating Robot Configuration Spaces with few Convex Sets using  Clique Covers of Visibility Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Werner%2C+P">Peter Werner</a>, 
<a href="/search/cs?searchtype=author&query=Amice%2C+A">Alexandre Amice</a>, 
<a href="/search/cs?searchtype=author&query=Marcucci%2C+T">Tobia Marcucci</a>, 
<a href="/search/cs?searchtype=author&query=Rus%2C+D">Daniela Rus</a>, 
<a href="/search/cs?searchtype=author&query=Tedrake%2C+R">Russ Tedrake</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, under review for possible publication at ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">Many computations in robotics can be dramatically accelerated if the robot
configuration space is described as a collection of simple sets. For example,
recently developed motion planners rely on a convex decomposition of the free
space to design collision-free trajectories using fast convex optimization. In
this work, we present an efficient method for approximately covering complex
configuration spaces with a small number of polytopes. The approach constructs
a visibility graph using sampling and generates a clique cover of this graph to
find clusters of samples that have mutual line of sight. These clusters are
then inflated into large, full-dimensional, polytopes. We evaluate our method
on a variety of robotic systems and show that it consistently covers larger
portions of free configuration space, with fewer polytopes, and in a fraction
of the time compared to previous methods.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02876" title="Abstract">arXiv:2310.02876</a> [<a href="/pdf/2310.02876" title="Download PDF">pdf</a>, <a href="/format/2310.02876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hate Speech Detection in Limited Data Contexts using Synthetic Data  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khullar%2C+A">Aman Khullar</a>, 
<a href="/search/cs?searchtype=author&query=Nkemelu%2C+D">Daniel Nkemelu</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+C+V">Cuong V. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Best%2C+M+L">Michael L. Best</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ACM Journal on Computing and Sustainable Societies
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">A growing body of work has focused on text classification methods for
detecting the increasing amount of hate speech posted online. This progress has
been limited to only a select number of highly-resourced languages causing
detection systems to either under-perform or not exist in limited data
contexts. This is majorly caused by a lack of training data which is expensive
to collect and curate in these settings. In this work, we propose a data
augmentation approach that addresses the problem of lack of data for online
hate speech detection in limited data contexts using synthetic data generation
techniques. Given a handful of hate speech examples in a high-resource language
such as English, we present three methods to synthesize new examples of hate
speech data in a target language that retains the hate sentiment in the
original examples but transfers the hate targets. We apply our approach to
generate training data for hate speech classification tasks in Hindi and
Vietnamese. Our findings show that a model trained on synthetic data performs
comparably to, and in some cases outperforms, a model trained only on the
samples available in the target domain. This method can be adopted to bootstrap
hate speech detection models from scratch in limited data contexts. As the
growth of social media within these contexts continues to outstrip response
efforts, this work furthers our capacities for detection, understanding, and
response to hate speech.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02879" title="Abstract">arXiv:2310.02879</a> [<a href="/pdf/2310.02879" title="Download PDF">pdf</a>, <a href="/format/2310.02879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Mechanism Design with Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balkanski%2C+E">Eric Balkanski</a>, 
<a href="/search/cs?searchtype=author&query=Gkatzelis%2C+V">Vasilis Gkatzelis</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xizhi Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Cherlin Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Aiming to overcome some of the limitations of worst-case analysis, the
recently proposed framework of "algorithms with predictions" allows algorithms
to be augmented with a (possibly erroneous) machine-learned prediction that
they can use as a guide. In this framework, the goal is to obtain improved
guarantees when the prediction is correct, which is called \emph{consistency},
while simultaneously guaranteeing some worst-case bounds even when the
prediction is arbitrarily wrong, which is called \emph{robustness}. The vast
majority of the work on this framework has focused on a refined analysis of
online algorithms augmented with predictions regarding the future input. A
subsequent line of work has also successfully adapted this framework to
mechanism design, where the prediction is regarding the private information of
strategic agents. In this paper, we initiate the study of online mechanism
design with predictions, which combines the challenges of online algorithms
with predictions and mechanism design with predictions.
<br />We consider the well-studied problem of designing a revenue-maximizing
auction to sell a single item to strategic bidders who arrive and depart over
time, each with an unknown, private, value for the item. We study the
learning-augmented version of this problem where the auction designer is given
a prediction regarding the maximum value over all agents. Our main result is a
strategyproof mechanism whose revenue guarantees are $\alpha$-consistent with
respect to the highest value and $(1-\alpha^2)/4$-robust with respect to the
second-highest value, for $\alpha \in [0,1]$. We show that this tradeoff is
optimal within a broad and natural family of auctions, meaning that any
$\alpha$-consistent mechanism in that family has robustness at most
$(1-\alpha^2)/4$. Finally, we extend our mechanism to also achieve expected
revenues proportional to the prediction quality.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02880" title="Abstract">arXiv:2310.02880</a> [<a href="/pdf/2310.02880" title="Download PDF">pdf</a>, <a href="/format/2310.02880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Persistent Memory File Systems: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Breukelen%2C+W">Wiebe van Breukelen</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+A">Animesh Trivedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Operating Systems (cs.OS)</span>

</div>
<p class="mathjax">Persistent Memory (PM) is non-volatile byte-addressable memory that offers
read and write latencies in the order of magnitude smaller than flash storage,
such as SSDs. This survey discusses how file systems address the most prominent
challenges in the implementation of file systems for Persistent Memory. First,
we discuss how the properties of Persistent Memory change file system design.
Second, we discuss work that aims to optimize small file I/O and the associated
meta-data resolution. Third, we address how existing Persistent Memory file
systems achieve (meta) data persistence and consistency.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02881" title="Abstract">arXiv:2310.02881</a> [<a href="/pdf/2310.02881" title="Download PDF">pdf</a>, <a href="/format/2310.02881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Immersive ExaBrick: Visualizing Large AMR Data in the CAVE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wesner%2C+S">Stefan Wesner</a>, 
<a href="/search/cs?searchtype=author&query=Zellmann%2C+S">Stefan Zellmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Rendering large adaptive mesh refinement (AMR) data in real-time in virtual
reality (VR) environments is a complex challenge that demands sophisticated
techniques and tools. The proposed solution harnesses the ExaBrick framework
and integrates it as a plugin in COVISE, a robust visualization system equipped
with the VR-centric OpenCOVER render module. This setup enables direct
navigation and interaction within the rendered volume in a VR environment. The
user interface incorporates rendering options and functions, ensuring a smooth
and interactive experience. We show that high-quality volume rendering of AMR
data in VR environments at interactive rates is possible using GPUs.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02882" title="Abstract">arXiv:2310.02882</a> [<a href="/pdf/2310.02882" title="Download PDF">pdf</a>, <a href="/format/2310.02882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Streaming Euclidean $k$-median and $k$-means with $o(\log n)$ Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cohen-Addad%2C+V">Vincent Cohen-Addad</a>, 
<a href="/search/cs?searchtype=author&query=Woodruff%2C+D+P">David P. Woodruff</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Samson Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at FOCS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We consider the classic Euclidean $k$-median and $k$-means objective on data
streams, where the goal is to provide a $(1+\varepsilon)$-approximation to the
optimal $k$-median or $k$-means solution, while using as little memory as
possible. Over the last 20 years, clustering in data streams has received a
tremendous amount of attention and has been the test-bed for a large variety of
new techniques, including coresets, the merge-and-reduce framework, bicriteria
approximation, sensitivity sampling, and so on. Despite this intense effort to
obtain smaller sketches for these problems, all known techniques require
storing at least $\Omega(\log(n\Delta))$ words of memory, where $n$ is the size
of the input and $\Delta$ is the aspect ratio. A natural question is if one can
beat this logarithmic dependence on $n$ and $\Delta$. In this paper, we break
this barrier by first giving an insertion-only streaming algorithm that
achieves a $(1+\varepsilon)$-approximation to the more general
$(k,z)$-clustering problem, using
$\tilde{\mathcal{O}}\left(\frac{dk}{\varepsilon^2}\right)\cdot(2^{z\log
z})\cdot\min\left(\frac{1}{\varepsilon^z},k\right)\cdot\text{poly}(\log\log(n\Delta))$
words of memory. Our techniques can also be used to achieve two-pass algorithms
for $k$-median and $k$-means clustering on dynamic streams using
$\tilde{\mathcal{O}}\left(\frac{1}{\varepsilon^2}\right)\cdot\text{poly}(d,k,\log\log(n\Delta))$
words of memory.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02885" title="Abstract">arXiv:2310.02885</a> [<a href="/pdf/2310.02885" title="Download PDF">pdf</a>, <a href="/format/2310.02885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Something for (almost) nothing: Improving deep ensemble calibration  using unlabeled data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pitas%2C+K">Konstantinos Pitas</a>, 
<a href="/search/cs?searchtype=author&query=Arbel%2C+J">Julyan Arbel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We present a method to improve the calibration of deep ensembles in the small
training data regime in the presence of unlabeled data. Our approach is
extremely simple to implement: given an unlabeled set, for each unlabeled data
point, we simply fit a different randomly selected label with each ensemble
member. We provide a theoretical analysis based on a PAC-Bayes bound which
guarantees that if we fit such a labeling on unlabeled data, and the true
labels on the training data, we obtain low negative log-likelihood and high
ensemble diversity on testing samples. Empirically, through detailed
experiments, we find that for low to moderately-sized training sets, our
ensembles are more diverse and provide better calibration than standard
ensembles, sometimes significantly.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02887" title="Abstract">arXiv:2310.02887</a> [<a href="/pdf/2310.02887" title="Download PDF">pdf</a>, <a href="/format/2310.02887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Grammatical Compositional Model for Video Action Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhijun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+X">Xu Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiahuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+S">Sheng Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Ying Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Analysis of human actions in videos demands understanding complex human
dynamics, as well as the interaction between actors and context. However, these
interaction relationships usually exhibit large intra-class variations from
diverse human poses or object manipulations, and fine-grained inter-class
differences between similar actions. Thus the performance of existing methods
is severely limited. Motivated by the observation that interactive actions can
be decomposed into actor dynamics and participating objects or humans, we
propose to investigate the composite property of them. In this paper, we
present a novel Grammatical Compositional Model (GCM) for action detection
based on typical And-Or graphs. Our model exploits the intrinsic structures and
latent relationships of actions in a hierarchical manner to harness both the
compositionality of grammar models and the capability of expressing rich
features of DNNs. The proposed model can be readily embodied into a neural
network module for efficient optimization in an end-to-end manner. Extensive
experiments are conducted on the AVA dataset and the Something-Else task to
demonstrate the superiority of our model, meanwhile the interpretability is
enhanced through an inference parsing procedure.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02894" title="Abstract">arXiv:2310.02894</a> [<a href="/pdf/2310.02894" title="Download PDF">pdf</a>, <a href="/format/2310.02894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-centric Behavior Description in Videos: New Benchmark and Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Lingru Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yiqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Manqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Peng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanning Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the domain of video surveillance, describing the behavior of each
individual within the video is becoming increasingly essential, especially in
complex scenarios with multiple individuals present. This is because describing
each individual's behavior provides more detailed situational analysis,
enabling accurate assessment and response to potential risks, ensuring the
safety and harmony of public places. Currently, video-level captioning datasets
cannot provide fine-grained descriptions for each individual's specific
behavior. However, mere descriptions at the video-level fail to provide an
in-depth interpretation of individual behaviors, making it challenging to
accurately determine the specific identity of each individual. To address this
challenge, we construct a human-centric video surveillance captioning dataset,
which provides detailed descriptions of the dynamic behaviors of 7,820
individuals. Specifically, we have labeled several aspects of each person, such
as location, clothing, and interactions with other elements in the scene, and
these people are distributed across 1,012 videos. Based on this dataset, we can
link individuals to their respective behaviors, allowing for further analysis
of each person's behavior in surveillance videos. Besides the dataset, we
propose a novel video captioning approach that can describe individual behavior
in detail on a person-level basis, achieving state-of-the-art results. To
facilitate further research in this field, we intend to release our dataset and
code.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02895" title="Abstract">arXiv:2310.02895</a> [<a href="/pdf/2310.02895" title="Download PDF">pdf</a>, <a href="/format/2310.02895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoLiDE: Concomitant Linear DAG Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saboksayr%2C+S+S">Seyed Saman Saboksayr</a>, 
<a href="/search/cs?searchtype=author&query=Mateos%2C+G">Gonzalo Mateos</a>, 
<a href="/search/cs?searchtype=author&query=Tepper%2C+M">Mariano Tepper</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We deal with the combinatorial problem of learning directed acyclic graph
(DAG) structure from observational data adhering to a linear structural
equation model (SEM). Leveraging advances in differentiable, nonconvex
characterizations of acyclicity, recent efforts have advocated a continuous
constrained optimization paradigm to efficiently explore the space of DAGs.
Most existing methods employ lasso-type score functions to guide this search,
which (i) require expensive penalty parameter retuning when the
$\textit{unknown}$ SEM noise variances change across problem instances; and
(ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we
propose a new convex score function for sparsity-aware learning of linear DAGs,
which incorporates concomitant estimation of scale and thus effectively
decouples the sparsity parameter from the exogenous noise levels.
Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE
($\textbf{Co}$ncomitant $\textbf{Li}$near $\textbf{D}$AG
$\textbf{E}$stimation), a regression-based criterion amenable to efficient
gradient computation and closed-form estimation of noise variances in
heteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods
without incurring added complexity, especially when the DAGs are larger and the
noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced
stability manifested via reduced standard deviations in several domain-specific
metrics, underscoring the robustness of our novel linear DAG estimator.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02897" title="Abstract">arXiv:2310.02897</a> [<a href="/pdf/2310.02897" title="Download PDF">pdf</a>, <a href="/format/2310.02897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recovery of Training Data from Overparameterized Autoencoders: An  Inverse Problem Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abitbul%2C+K">Koren Abitbul</a>, 
<a href="/search/cs?searchtype=author&query=Dar%2C+Y">Yehuda Dar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study the recovery of training data from overparameterized autoencoder
models. Given a degraded training sample, we define the recovery of the
original sample as an inverse problem and formulate it as an optimization task.
In our inverse problem, we use the trained autoencoder to implicitly define a
regularizer for the particular training dataset that we aim to retrieve from.
We develop the intricate optimization task into a practical method that
iteratively applies the trained autoencoder and relatively simple computations
that estimate and address the unknown degradation operator. We evaluate our
method for blind inpainting where the goal is to recover training images from
degradation of many missing pixels in an unknown pattern. We examine various
deep autoencoder architectures, such as fully connected and U-Net (with various
nonlinearities and at diverse train loss values), and show that our method
significantly outperforms previous methods for training data recovery from
autoencoders. Importantly, our method greatly improves the recovery performance
also in settings that were previously considered highly challenging, and even
impractical, for such retrieval.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02898" title="Abstract">arXiv:2310.02898</a> [<a href="/pdf/2310.02898" title="Download PDF">pdf</a>, <a href="/format/2310.02898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Some bidding games converging to their unique pure equilibrium
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heymann%2C+B">Benjamin Heymann</a>, 
<a href="/search/cs?searchtype=author&query=Jofr%C3%A9%2C+A">Alejandro Jofr&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We introduce a class of Bayesian bidding games for which we prove that the
set of pure Nash equilibria is a (non-empty) sublattice and we give a
sufficient condition for uniqueness that is often verified in the context of
markets with inelastic demand. We propose a dynamic that converges to the
extrema of the equilibrium set and derive a scheme to compute the extreme Nash
equilibria.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02901" title="Abstract">arXiv:2310.02901</a> [<a href="/pdf/2310.02901" title="Download PDF">pdf</a>, <a href="/format/2310.02901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computationally Efficient Quadratic Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Noel%2C+M+M">Mathew Mithra Noel</a>, 
<a href="/search/cs?searchtype=author&query=Muthiah-Nakarajan%2C+V">Venkataraman Muthiah-Nakarajan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Higher order artificial neurons whose outputs are computed by applying an
activation function to a higher order multinomial function of the inputs have
been considered in the past, but did not gain acceptance due to the extra
parameters and computational cost. However, higher order neurons have
significantly greater learning capabilities since the decision boundaries of
higher order neurons can be complex surfaces instead of just hyperplanes. The
boundary of a single quadratic neuron can be a general hyper-quadric surface
allowing it to learn many nonlinearly separable datasets. Since quadratic forms
can be represented by symmetric matrices, only $\frac{n(n+1)}{2}$ additional
parameters are needed instead of $n^2$. A quadratic Logistic regression model
is first presented. Solutions to the XOR problem with a single quadratic neuron
are considered. The complete vectorized equations for both forward and backward
propagation in feedforward networks composed of quadratic neurons are derived.
A reduced parameter quadratic neural network model with just $ n $ additional
parameters per neuron that provides a compromise between learning ability and
computational cost is presented. Comparison on benchmark classification
datasets are used to demonstrate that a final layer of quadratic neurons
enables networks to achieve higher accuracy with significantly fewer hidden
layer neurons. In particular this paper shows that any dataset composed of $C$
bounded clusters can be separated with only a single layer of $C$ quadratic
neurons.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02902" title="Abstract">arXiv:2310.02902</a> [<a href="/pdf/2310.02902" title="Download PDF">pdf</a>, <a href="/format/2310.02902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Searching for High-Value Molecules Using Reinforcement Learning and  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghugare%2C+R">Raj Ghugare</a>, 
<a href="/search/cs?searchtype=author&query=Miret%2C+S">Santiago Miret</a>, 
<a href="/search/cs?searchtype=author&query=Hugessen%2C+A">Adriana Hugessen</a>, 
<a href="/search/cs?searchtype=author&query=Phielipp%2C+M">Mariano Phielipp</a>, 
<a href="/search/cs?searchtype=author&query=Berseth%2C+G">Glen Berseth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reinforcement learning (RL) over text representations can be effective for
finding high-value policies that can search over graphs. However, RL requires
careful structuring of the search space and algorithm design to be effective in
this challenge. Through extensive experiments, we explore how different design
choices for text grammar and algorithmic choices for training can affect an RL
policy's ability to generate molecules with desired properties. We arrive at a
new RL-based molecular design algorithm (ChemRLformer) and perform a thorough
analysis using 25 molecule design tasks, including computationally complex
protein docking simulations. From this analysis, we discover unique insights in
this problem space and show that ChemRLformer achieves state-of-the-art
performance while being more straightforward than prior work by demystifying
which design choices are actually helpful for text-based molecule design.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02903" title="Abstract">arXiv:2310.02903</a> [<a href="/pdf/2310.02903" title="Download PDF">pdf</a>, <a href="/format/2310.02903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FroSSL: Frobenius Norm Minimization for Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Skean%2C+O">Oscar Skean</a>, 
<a href="/search/cs?searchtype=author&query=Dhakal%2C+A">Aayush Dhakal</a>, 
<a href="/search/cs?searchtype=author&query=Jacobs%2C+N">Nathan Jacobs</a>, 
<a href="/search/cs?searchtype=author&query=Giraldo%2C+L+G+S">Luis Gonzalo Sanchez Giraldo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Self-supervised learning (SSL) is an increasingly popular paradigm for
representation learning. Recent methods can be classified as
sample-contrastive, dimension-contrastive, or asymmetric network-based, with
each family having its own approach to avoiding informational collapse. While
dimension-contrastive methods converge to similar solutions as
sample-contrastive methods, it can be empirically shown that some methods
require more epochs of training to converge. Motivated by closing this divide,
we present the objective function FroSSL which is both sample- and
dimension-contrastive up to embedding normalization. FroSSL works by minimizing
covariance Frobenius norms for avoiding collapse and minimizing mean-squared
error for augmentation invariance. We show that FroSSL converges more quickly
than a variety of other SSL methods and provide theoretical and empirical
support that this faster convergence is due to how FroSSL affects the
eigenvalues of the embedding covariance matrices. We also show that FroSSL
learns competitive representations on linear probe evaluation when used to
train a ResNet18 on the CIFAR-10, CIFAR-100, STL-10, and ImageNet datasets.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02905" title="Abstract">arXiv:2310.02905</a> [<a href="/pdf/2310.02905" title="Download PDF">pdf</a>, <a href="/format/2310.02905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled  with Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xiaoqiang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhaoxuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Z">Zhongxiang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenyang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+Y">Yao Shu</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-Kiong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Jaillet%2C+P">Patrick Jaillet</a>, 
<a href="/search/cs?searchtype=author&query=Low%2C+B+K+H">Bryan Kian Hsiang Low</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) have shown remarkable instruction-following
capabilities and achieved impressive performances in various applications.
However, the performances of LLMs depend heavily on the instructions given to
them, which are typically manually tuned with substantial human efforts. Recent
work has used the query-efficient Bayesian optimization (BO) algorithm to
automatically optimize the instructions given to black-box LLMs. However, BO
usually falls short when optimizing highly sophisticated (e.g.,
high-dimensional) objective functions, such as the functions mapping an
instruction to the performance of an LLM. This is mainly due to the limited
expressive power of the Gaussian process (GP) model which is used by BO as a
surrogate to model the objective function. Meanwhile, it has been repeatedly
shown that neural networks (NNs), especially pre-trained transformers, possess
strong expressive power and can model highly complex functions. So, we adopt a
neural bandit algorithm which replaces the GP in BO by an NN surrogate to
optimize instructions for black-box LLMs. More importantly, the neural bandit
algorithm allows us to naturally couple the NN surrogate with the hidden
representation learned by a pre-trained transformer (i.e., an open-source LLM),
which significantly boosts its performance. These motivate us to propose our
INSTruction optimization usIng Neural bandits Coupled with Transformers}
(INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use
extensive experiments to show that our INSTINCT consistently outperforms the
existing methods in different tasks, such as in various instruction induction
tasks and the task of improving the zero-shot chain-of-thought instruction.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02906" title="Abstract">arXiv:2310.02906</a> [<a href="/pdf/2310.02906" title="Download PDF">pdf</a>, <a href="/format/2310.02906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with  Visual and Textual Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+S">Shiyi Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaosong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yongyi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuyin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaoting Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuille%2C+A">Alan Yuille</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zongwei Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Image synthesis approaches, e.g., generative adversarial networks, have been
popular as a form of data augmentation in medical image analysis tasks. It is
primarily beneficial to overcome the shortage of publicly accessible data and
associated quality annotations. However, the current techniques often lack
control over the detailed contents in generated images, e.g., the type of
disease patterns, the location of lesions, and attributes of the diagnosis. In
this work, we adapt the latest advance in the generative model, i.e., the
diffusion model, with the added control flow using lesion-specific visual and
textual prompts for generating dermatoscopic images. We further demonstrate the
advantage of our diffusion model-based framework over the classical generation
models in both the image quality and boosting the segmentation performance on
skin lesions. It can achieve a 9% increase in the SSIM image quality measure
and an over 5% increase in Dice coefficients over the prior arts.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02907" title="Abstract">arXiv:2310.02907</a> [<a href="/pdf/2310.02907" title="Download PDF">pdf</a>, <a href="/format/2310.02907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Whole-body MPC for highly redundant legged manipulators: experimental  evaluation with a 37 DoF dual-arm quadruped
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dadiotis%2C+I">Ioannis Dadiotis</a>, 
<a href="/search/cs?searchtype=author&query=Laurenzi%2C+A">Arturo Laurenzi</a>, 
<a href="/search/cs?searchtype=author&query=Tsagarakis%2C+N">Nikos Tsagarakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 2023 IEEE-RAS International Conference on Humanoid Robots (Humanoids 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Recent progress in legged locomotion has rendered quadruped manipulators a
promising solution for performing tasks that require both mobility and
manipulation (loco-manipulation). In the real world, task specifications and/or
environment constraints may require the quadruped manipulator to be equipped
with high redundancy as well as whole-body motion coordination capabilities.
This work presents an experimental evaluation of a whole-body Model Predictive
Control (MPC) framework achieving real-time performance on a dual-arm quadruped
platform consisting of 37 actuated joints. To the best of our knowledge this is
the legged manipulator with the highest number of joints to be controlled with
real-time whole-body MPC so far. The computational efficiency of the MPC while
considering the full robot kinematics and the centroidal dynamics model builds
upon an open-source DDP-variant solver and a state-of-the-art optimal control
problem formulation. Differently from previous works on quadruped manipulators,
the MPC is directly interfaced with the low-level joint impedance controllers
without the need of designing an instantaneous whole-body controller. The
feasibility on the real hardware is showcased using the CENTAURO platform for
the challenging task of picking a heavy object from the ground. Dynamic
stepping (trotting) is also showcased for first time with this robot. The
results highlight the potential of replanning with whole-body information in a
predictive control loop.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02911" title="Abstract">arXiv:2310.02911</a> [<a href="/pdf/2310.02911" title="Download PDF">pdf</a>, <a href="/format/2310.02911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deciphering the Crypto-shopper: Knowledge and Preferences of Consumers  Using Cryptocurrencies for Purchases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silenzi%2C+M">Massimiliano Silenzi</a>, 
<a href="/search/cs?searchtype=author&query=Cabuk%2C+U+C">Umut Can Cabuk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CryptoRefills Whitepaper. 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computational Engineering, Finance, and Science (cs.CE); Emerging Technologies (cs.ET)

</div>
<p class="mathjax">The swiftly maturing sector of cryptocurrencies proffers an array of
challenges and prospects for both enterprises and consumers. This study
explores the knowledge, expertise, and purchasing behaviors of individuals
engaged in shopping using cryptocurrencies to furnish an exhaustive
understanding of this distinctive consumer cohort. By analyzing data from our
survey of 516 participants, our findings illuminate a range of knowledge
levels, encompassing neophytes to connoisseurs, with a significant segment
exhibiting high procurement frequency amidst constrained expertise. Regression
analyses unveil that, although knowledge significantly influences purchase
behaviors, its explanatory capacity remains restricted. Additionally, a K-means
cluster analysis discloses three disparate crypto-shopper profiles, each
possessing unique knowledge and expertise levels. These insights contravene
conventional wisdom regarding the nexus between domain knowledge and adoption,
insinuating that the appeal of cryptocurrencies transcends technical knowledge.
The revelations of this research are instrumental for enterprises aspiring to
address the diverse needs of the crypto-shopper demographic, accentuating the
imperative of personalized strategies and user experiences. This exploration
furthermore lays the groundwork for ensuing research focused on unraveling the
extensive implications of crypto acceptance and its confluence with consumer
conduct.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02913" title="Abstract">arXiv:2310.02913</a> [<a href="/pdf/2310.02913" title="Download PDF">pdf</a>, <a href="/format/2310.02913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ELUQuant: Event-Level Uncertainty Quantification in Deep Inelastic  Scattering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fanelli%2C+C">Cristiano Fanelli</a>, 
<a href="/search/cs?searchtype=author&query=Giroux%2C+J">James Giroux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; High Energy Physics - Experiment (hep-ex); Nuclear Experiment (nucl-ex); Data Analysis, Statistics and Probability (physics.data-an); Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce a physics-informed Bayesian Neural Network (BNN) with flow
approximated posteriors using multiplicative normalizing flows (MNF) for
detailed uncertainty quantification (UQ) at the physics event-level. Our method
is capable of identifying both heteroskedastic aleatoric and epistemic
uncertainties, providing granular physical insights. Applied to Deep Inelastic
Scattering (DIS) events, our model effectively extracts the kinematic variables
$x$, $Q^2$, and $y$, matching the performance of recent deep learning
regression techniques but with the critical enhancement of event-level UQ. This
detailed description of the underlying uncertainty proves invaluable for
decision-making, especially in tasks like event filtering. It also allows for
the reduction of true inaccuracies without directly accessing the ground truth.
A thorough DIS simulation using the H1 detector at HERA indicates possible
applications for the future EIC. Additionally, this paves the way for related
tasks such as data quality monitoring and anomaly detection. Remarkably, our
approach effectively processes large samples at high rates.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02918" title="Abstract">arXiv:2310.02918</a> [<a href="/pdf/2310.02918" title="Download PDF">pdf</a>, <a href="/format/2310.02918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning-Aided Warmstart of Model Predictive Control in Uncertain  Fast-Changing Traffic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bouzidi%2C+M">Mohamed-Khalil Bouzidi</a>, 
<a href="/search/eess?searchtype=author&query=Yao%2C+Y">Yue Yao</a>, 
<a href="/search/eess?searchtype=author&query=Goehring%2C+D">Daniel Goehring</a>, 
<a href="/search/eess?searchtype=author&query=Reichardt%2C+J">Joerg Reichardt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Model Predictive Control lacks the ability to escape local minima in
nonconvex problems. Furthermore, in fast-changing, uncertain environments, the
conventional warmstart, using the optimal trajectory from the last timestep,
often falls short of providing an adequately close initial guess for the
current optimal trajectory. This can potentially result in convergence failures
and safety issues. Therefore, this paper proposes a framework for
learning-aided warmstarts of Model Predictive Control algorithms. Our method
leverages a neural network based multimodal predictor to generate multiple
trajectory proposals for the autonomous vehicle, which are further refined by a
sampling-based technique. This combined approach enables us to identify
multiple distinct local minima and provide an improved initial guess. We
validate our approach with Monte Carlo simulations of traffic scenarios.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02919" title="Abstract">arXiv:2310.02919</a> [<a href="/pdf/2310.02919" title="Download PDF">pdf</a>, <a href="/format/2310.02919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention-based Multi-task Learning for Base Editor Outcome Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mollaysa%2C+A">Amina Mollaysa</a>, 
<a href="/search/cs?searchtype=author&query=Allam%2C+A">Ahmed Allam</a>, 
<a href="/search/cs?searchtype=author&query=Krauthammer%2C+M">Michael Krauthammer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Human genetic diseases often arise from point mutations, emphasizing the
critical need for precise genome editing techniques. Among these, base editing
stands out as it allows targeted alterations at the single nucleotide level.
However, its clinical application is hindered by low editing efficiency and
unintended mutations, necessitating extensive trial-and-error experimentation
in the laboratory. To speed up this process, we present an attention-based
two-stage machine learning model that learns to predict the likelihood of all
possible editing outcomes for a given genomic target sequence. We further
propose a multi-task learning schema to jointly learn multiple base editors
(i.e. variants) at once. Our model's predictions consistently demonstrated a
strong correlation with the actual experimental results on multiple datasets
and base editor variants. These results provide further validation for the
models' capacity to enhance and accelerate the process of refining base editing
designs.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02920" title="Abstract">arXiv:2310.02920</a> [<a href="/pdf/2310.02920" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Ayurvedic Diagnosis using Multinomial Naive Bayes and K-modes  Clustering: An Investigation into Prakriti Types and Dosha Overlapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bidve%2C+P">Pranav Bidve</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Shalini Mishra</a>, 
<a href="/search/cs?searchtype=author&query=J%2C+A">Annapurna J</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The identification of Prakriti types for the human body is a long-lost
medical practice in finding the harmony between the nature of human beings and
their behaviour. There are 3 fundamental Prakriti types of individuals. A
person can belong to any Dosha. In the existing models, researchers have made
use of SVM, KNN, PCA, Decision Tree, and various other algorithms. The output
of these algorithms was quite decent, but it can be enhanced with the help of
Multinomial Naive Bayes and K-modes clustering. Most of the researchers have
confined themselves to 3 basic classes. This might not be accurate in the
real-world scenario, where overlapping might occur. Considering these, we have
classified the Doshas into 7 categories, which includes overlapping of Doshas.
These are namely, VATT-Dosha, PITT-Dosha, KAPH-Dosha, VATT-PITT-Dosha,
PITT-KAPH-Dosha, KAPH-VATT-Dosha, and VATT-PITT-KAPH-Dosha. The data used
contains a balanced set of all individual entries on which preprocessing steps
of machine learning have been performed. Chi-Square test for handling
categorical data is being used for feature selection. For model fitting, the
method used in this approach is K-modes clustering. The empirical results
demonstrate a better result while using the MNB classifier. All key findings of
this work have achieved 0.90 accuracy, 0.81 precision, 0.91 F-score, and 0.90
recall. The discussion suggests a provident analysis of the seven clusters and
predicts their occurrence. The results have been consolidated to improve the
Ayurvedic advancements with machine learning.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02925" title="Abstract">arXiv:2310.02925</a> [<a href="/pdf/2310.02925" title="Download PDF">pdf</a>, <a href="/format/2310.02925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Transport with Adaptive Regularisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Assel%2C+H">Hugues Van Assel</a>, 
<a href="/search/cs?searchtype=author&query=Vayer%2C+T">Titouan Vayer</a>, 
<a href="/search/cs?searchtype=author&query=Flamary%2C+R">Remi Flamary</a>, 
<a href="/search/cs?searchtype=author&query=Courty%2C+N">Nicolas Courty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Regularising the primal formulation of optimal transport (OT) with a strictly
convex term leads to enhanced numerical complexity and a denser transport plan.
Many formulations impose a global constraint on the transport plan, for
instance by relying on entropic regularisation. As it is more expensive to
diffuse mass for outlier points compared to central ones, this typically
results in a significant imbalance in the way mass is spread across the points.
This can be detrimental for some applications where a minimum of smoothing is
required per point. To remedy this, we introduce OT with Adaptive
RegularIsation (OTARI), a new formulation of OT that imposes constraints on the
mass going in or/and out of each point. We then showcase the benefits of this
approach for domain adaptation.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02926" title="Abstract">arXiv:2310.02926</a> [<a href="/pdf/2310.02926" title="Download PDF">pdf</a>, <a href="/format/2310.02926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extensions to the SENSEI In situ Framework for Heterogeneous  Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Loring%2C+B">Burlen Loring</a> (1), 
<a href="/search/cs?searchtype=author&query=Bethel%2C+E+W">E. Wes Bethel</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Weber%2C+G+H">Gunther H. Weber</a> (1), 
<a href="/search/cs?searchtype=author&query=Mahoney%2C+M+W">Michael W. Mahoney</a> (1 and 3 and 4) ((1) Lawrence Berkeley National Lab, (2) San Francisco State University, (3) International Computer Science Institute University of California at Berkeley, (4) University of California at Berkeley)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in: ISAV 2023: In Situ Infrastructures for Enabling Extreme-scale Analysis and Visualization, November 13 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">The proliferation of GPUs and accelerators in recent supercomputing systems,
so called heterogeneous architectures, has led to increased complexity in
execution environments and programming models as well as to deeper memory
hierarchies on these systems. In this work, we discuss challenges that arise in
in situ code coupling on these heterogeneous architectures. In particular, we
present data and execution model extensions to the SENSEI in situ framework
that are targeted at the effective use of systems with heterogeneous
architectures. We then use these new data and execution model extensions to
investigate several in situ placement and execution configurations and to
analyze the impact these choices have on overall performance.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02927" title="Abstract">arXiv:2310.02927</a> [<a href="/pdf/2310.02927" title="Download PDF">pdf</a>, <a href="/format/2310.02927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Network Lifetime Maximization and Relay Selection Design in  Underwater Acoustic Sensor Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+Z">Z. Mohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Soleimanpour-Moghadam%2C+M">M. Soleimanpour-Moghadam</a>, 
<a href="/search/cs?searchtype=author&query=Talebi%2C+S">S. Talebi</a>, 
<a href="/search/cs?searchtype=author&query=Ahmadi%2C+H">H. Ahmadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at IEEE Systems journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The paper proposes a new approach to minimize the number of relays while
maximizing the lifetime of underwater acoustic sensor networks (UASNs). This
involves formulating the relay node placement (RNP) problem as a
multi-objective optimization problem and employing the multi-objective
lexico-graphic method (MOLM) to solve it. To achieve the optimal solution, the
MOLM consists of two steps. First, the problem of lifetime maximization is
tackled to find RNP solutions. This transforms the RNP into a non-convex
optimization problem which is then converted into a convex programming
equivalent. The proposed method has the same computational complexity as
previous relay-node adjustment (RA) and difference convex algorithm (DCA)
methods. The second step introduces a novel relay node selection to reach the
optimal number of relays. Simulation results demonstrate that it has superior
network lifetime and efficiency compared to RA and DCA.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02931" title="Abstract">arXiv:2310.02931</a> [<a href="/pdf/2310.02931" title="Download PDF">pdf</a>, <a href="/format/2310.02931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph data modelling for outcome prediction in oropharyngeal cancer  patients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhasker%2C+N">Nithya Bhasker</a>, 
<a href="/search/cs?searchtype=author&query=Leger%2C+S">Stefan Leger</a>, 
<a href="/search/cs?searchtype=author&query=Zwanenburg%2C+A">Alexander Zwanenburg</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+C+B">Chethan Babu Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Bodenstedt%2C+S">Sebastian Bodenstedt</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%B6ck%2C+S">Steffen L&#xf6;ck</a>, 
<a href="/search/cs?searchtype=author&query=Speidel%2C+S">Stefanie Speidel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Graph neural networks (GNNs) are becoming increasingly popular in the medical
domain for the tasks of disease classification and outcome prediction. Since
patient data is not readily available as a graph, most existing methods either
manually define a patient graph, or learn a latent graph based on pairwise
similarities between the patients. There are also hypergraph neural network
(HGNN)-based methods that were introduced recently to exploit potential higher
order associations between the patients by representing them as a hypergraph.
In this work, we propose a patient hypergraph network (PHGN), which has been
investigated in an inductive learning setup for binary outcome prediction in
oropharyngeal cancer (OPC) patients using computed tomography (CT)-based
radiomic features for the first time. Additionally, the proposed model was
extended to perform time-to-event analyses, and compared with GNN and baseline
linear models.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02932" title="Abstract">arXiv:2310.02932</a> [<a href="/pdf/2310.02932" title="Download PDF">pdf</a>, <a href="/format/2310.02932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Large Language Models on Climate Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bulian%2C+J">Jannis Bulian</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%A4fer%2C+M+S">Mike S. Sch&#xe4;fer</a>, 
<a href="/search/cs?searchtype=author&query=Amini%2C+A">Afra Amini</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+H">Heidi Lam</a>, 
<a href="/search/cs?searchtype=author&query=Ciaramita%2C+M">Massimiliano Ciaramita</a>, 
<a href="/search/cs?searchtype=author&query=Gaiarin%2C+B">Ben Gaiarin</a>, 
<a href="/search/cs?searchtype=author&query=Huebscher%2C+M+C">Michelle Chen Huebscher</a>, 
<a href="/search/cs?searchtype=author&query=Buck%2C+C">Christian Buck</a>, 
<a href="/search/cs?searchtype=author&query=Mede%2C+N">Niels Mede</a>, 
<a href="/search/cs?searchtype=author&query=Leippold%2C+M">Markus Leippold</a>, 
<a href="/search/cs?searchtype=author&query=Strauss%2C+N">Nadine Strauss</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Understanding how climate change affects us and learning about available
solutions are key steps toward empowering individuals and communities to
mitigate and adapt to it. As Large Language Models (LLMs) rise in popularity,
it is necessary to assess their capability in this domain. In this study, we
present a comprehensive evaluation framework, grounded in science communication
principles, to analyze LLM responses to climate change topics. Our framework
emphasizes both the presentational and epistemological adequacy of answers,
offering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our
framework discerns up to 30 distinct issues in model outputs. The task is a
real-world example of a growing number of challenging problems where AI can
complement and lift human performance. We introduce a novel and practical
protocol for scalable oversight that uses AI Assistance and relies on raters
with relevant educational backgrounds. We evaluate several recent LLMs and
conduct a comprehensive analysis of the results, shedding light on both the
potential and the limitations of LLMs in the realm of climate communication.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02942" title="Abstract">arXiv:2310.02942</a> [<a href="/pdf/2310.02942" title="Download PDF">pdf</a>, <a href="/format/2310.02942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Constraint Tightening in Stochastic Model Predictive Control: A  Regression Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Capone%2C+A">Alexandre Capone</a>, 
<a href="/search/eess?searchtype=author&query=Br%C3%BCdigam%2C+T">Tim Br&#xfc;digam</a>, 
<a href="/search/eess?searchtype=author&query=Hirche%2C+S">Sandra Hirche</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Transactions on Automatic Control
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Solving chance-constrained stochastic optimal control problems is a
significant challenge in control. This is because no analytical solutions exist
for up to a handful of special cases. A common and computationally efficient
approach for tackling chance-constrained stochastic optimal control problems
consists of reformulating the chance constraints as hard constraints with a
constraint-tightening parameter. However, in such approaches, the choice of
constraint-tightening parameter remains challenging, and guarantees can mostly
be obtained assuming that the process noise distribution is known a priori.
Moreover, the chance constraints are often not tightly satisfied, leading to
unnecessarily high costs. This work proposes a data-driven approach for
learning the constraint-tightening parameters online during control. To this
end, we reformulate the choice of constraint-tightening parameter for the
closed-loop as a binary regression problem. We then leverage a highly
expressive \gls{gp} model for binary regression to approximate the smallest
constraint-tightening parameters that satisfy the chance constraints. By tuning
the algorithm parameters appropriately, we show that the resulting
constraint-tightening parameters satisfy the chance constraints up to an
arbitrarily small margin with high probability. Our approach yields
constraint-tightening parameters that tightly satisfy the chance constraints in
numerical experiments, resulting in a lower average cost than three other
state-of-the-art approaches.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02943" title="Abstract">arXiv:2310.02943</a> [<a href="/pdf/2310.02943" title="Download PDF">pdf</a>, <a href="/format/2310.02943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LibriSpeech-PC: Benchmark for Evaluation of Punctuation and  Capitalization Capabilities of end-to-end ASR Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meister%2C+A">Aleksandr Meister</a>, 
<a href="/search/cs?searchtype=author&query=Novikov%2C+M">Matvei Novikov</a>, 
<a href="/search/cs?searchtype=author&query=Karpov%2C+N">Nikolay Karpov</a>, 
<a href="/search/cs?searchtype=author&query=Bakhturina%2C+E">Evelina Bakhturina</a>, 
<a href="/search/cs?searchtype=author&query=Lavrukhin%2C+V">Vitaly Lavrukhin</a>, 
<a href="/search/cs?searchtype=author&query=Ginsburg%2C+B">Boris Ginsburg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Traditional automatic speech recognition (ASR) models output lower-cased
words without punctuation marks, which reduces readability and necessitates a
subsequent text processing model to convert ASR transcripts into a proper
format. Simultaneously, the development of end-to-end ASR models capable of
predicting punctuation and capitalization presents several challenges,
primarily due to limited data availability and shortcomings in the existing
evaluation methods, such as inadequate assessment of punctuation prediction. In
this paper, we introduce a LibriSpeech-PC benchmark designed to assess the
punctuation and capitalization prediction capabilities of end-to-end ASR
models. The benchmark includes a LibriSpeech-PC dataset with restored
punctuation and capitalization, a novel evaluation metric called Punctuation
Error Rate (PER) that focuses on punctuation marks, and initial baseline
models. All code, data, and models are publicly available.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02944" title="Abstract">arXiv:2310.02944</a> [<a href="/pdf/2310.02944" title="Download PDF">pdf</a>, <a href="/format/2310.02944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Landmark Color for AUV Docking in Visually Dynamic Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Knutson%2C+C">Corey Knutson</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhipeng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Sattar%2C+J">Junaed Sattar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICRA 2024 for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Autonomous Underwater Vehicles (AUVs) conduct missions underwater without the
need for human intervention. A docking station (DS) can extend mission times of
an AUV by providing a location for the AUV to recharge its batteries and
receive updated mission information. Various methods for locating and tracking
a DS exist, but most rely on expensive acoustic sensors, or are vision-based,
which is significantly affected by water quality. In this \doctype, we present
a vision-based method that utilizes adaptive color LED markers and dynamic
color filtering to maximize landmark visibility in varying water conditions.
Both AUV and DS utilize cameras to determine the water background color in
order to calculate the desired marker color. No communication between AUV and
DS is needed to determine marker color. Experiments conducted in a pool and
lake show our method performs 10 times better than static color thresholding
methods as background color varies. DS detection is possible at a range of 5
meters in clear water with minimal false positives.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02945" title="Abstract">arXiv:2310.02945</a> [<a href="/pdf/2310.02945" title="Download PDF">pdf</a>, <a href="/format/2310.02945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proximal Policy Optimization-Based Reinforcement Learning Approach for  DC-DC Boost Converter Control: A Comparative Evaluation Against Traditional  Control Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Saha%2C+U">Utsab Saha</a>, 
<a href="/search/eess?searchtype=author&query=Shahria%2C+S">Shakib Shahria</a>, 
<a href="/search/eess?searchtype=author&query=Rashid%2C+A+B+M+H">A.B.M Harun-Ur Rashid</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This article presents a proximal policy optimization (PPO) based
reinforcement learning (RL) approach for DC-DC boost converter control, which
is compared to traditional control methods. The performance of the PPO
algorithm is evaluated using MATLAB Simulink co-simulation, and the results
demonstrate that the most efficient approach for achieving short settling time
and stability is to combine the PPO algorithm with reinforcement learning based
control method. The simulation results indicate that the step response
characteristics provided using the control method based on RL with the PPO
algorithm outperform traditional control approaches, which can be used to
improve DC-DC boost converter control. This research also highlights the
inherent capability of the reinforcement learning method to enhance the
performance of boost converter control.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02949" title="Abstract">arXiv:2310.02949</a> [<a href="/pdf/2310.02949" title="Download PDF">pdf</a>, <a href="/format/2310.02949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xianjun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Petzold%2C+L">Linda Petzold</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Warning: This paper contains examples of harmful language, and reader
discretion is recommended. The increasing open release of powerful large
language models (LLMs) has facilitated the development of downstream
applications by reducing the essential cost of data annotation and computation.
To ensure AI safety, extensive safety-alignment measures have been conducted to
armor these models against malicious use (primarily hard prompt attack).
However, beneath the seemingly resilient facade of the armor, there might lurk
a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these
safely aligned LLMs can be easily subverted to generate harmful content.
Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of
data can elicit safely-aligned models to adapt to harmful tasks without
sacrificing model helpfulness. Remarkably, the subverted models retain their
capability to respond appropriately to regular inquiries. Experiments across 8
models released by 5 different organizations (LLaMa-2, Falcon, InternLM,
BaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack.
Besides, the single-turn English-only attack successfully transfers to
multi-turn dialogue and other languages. This study serves as a clarion call
for a collective effort to overhaul and fortify the safety of open-source LLMs
against malicious attackers.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02953" title="Abstract">arXiv:2310.02953</a> [<a href="/pdf/2310.02953" title="Download PDF">pdf</a>, <a href="/format/2310.02953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JsonTuning: Towards Generalizable, Robust, and Controllable Instruction  Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenxuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guizhen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+W">Wai Lam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Instruction tuning has emerged as a crucial process for harnessing the
capabilities of large language models (LLMs) by providing explicit task
instructions, leading to improved performance in various tasks. However,
prevalent text-to-text instruction tuning (TextTuning) methods suffer from
limitations in generalization, robustness, and controllability due to the
ambiguity and lack of explicit structure in tasks. In this paper, we propose
JsonTuning, a novel structure-to-structure approach for instruction tuning. By
leveraging the versatility and structured nature of JSON to represent tasks,
JsonTuning enhances generalization by helping the model understand essential
task elements and their relations, improves robustness by minimizing ambiguity,
and increases controllability by providing explicit control over the output. We
conduct a comprehensive comparative study with diverse language models and
evaluation benchmarks. Experimental results show that JsonTuning outperforms
TextTuning in various applications, showcasing improved performance,
adaptability, robustness, and controllability. By overcoming the limitations of
TextTuning, JsonTuning demonstrates significant potential for more effective
and reliable LLMs capable of handling diverse scenarios.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02954" title="Abstract">arXiv:2310.02954</a> [<a href="/pdf/2310.02954" title="Download PDF">pdf</a>, <a href="/format/2310.02954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for  In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jiong Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zixuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chuanyang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhijiang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yichun Yin</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhicheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qingxing Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiongwei Han</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jing Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengming Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaodan Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent advances in natural language processing, primarily propelled by Large
Language Models (LLMs), have showcased their remarkable capabilities grounded
in in-context learning. A promising avenue for guiding LLMs in intricate
reasoning tasks involves the utilization of intermediate reasoning steps within
the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies
in the effective selection of exemplars for facilitating in-context learning.
In this study, we introduce a framework that leverages Dual Queries and
Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars
for in-context learning. Dual Queries first query LLM to obtain LLM-generated
knowledge such as CoT, then query the retriever to obtain the final exemplars
via both question and the knowledge. Moreover, for the second query, LoRe
employs dimensionality reduction techniques to refine exemplar selection,
ensuring close alignment with the input question's knowledge. Through extensive
experiments, we demonstrate that DQ-LoRe significantly outperforms prior
state-of-the-art methods in the automatic selection of exemplars for GPT-4,
enhancing performance from 92.5\% to 94.2\%. Our comprehensive analysis further
reveals that DQ-LoRe consistently outperforms retrieval-based approaches in
terms of both performance and adaptability, especially in scenarios
characterized by distribution shifts. DQ-LoRe pushes the boundaries of
in-context learning and opens up new avenues for addressing complex reasoning
challenges. We will release the code soon.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02955" title="Abstract">arXiv:2310.02955</a> [<a href="/pdf/2310.02955" title="Download PDF">pdf</a>, <a href="/format/2310.02955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perceptual error optimization for Monte Carlo animation rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kora%C4%87%2C+M">Mi&#x161;a Kora&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Sala%C3%BCn%2C+C">Corentin Sala&#xfc;n</a>, 
<a href="/search/cs?searchtype=author&query=Georgiev%2C+I">Iliyan Georgiev</a>, 
<a href="/search/cs?searchtype=author&query=Grittmann%2C+P">Pascal Grittmann</a>, 
<a href="/search/cs?searchtype=author&query=Slusallek%2C+P">Philipp Slusallek</a>, 
<a href="/search/cs?searchtype=author&query=Myszkowski%2C+K">Karol Myszkowski</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+G">Gurprit Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Independently estimating pixel values in Monte Carlo rendering results in a
perceptually sub-optimal white-noise distribution of error in image space.
Recent works have shown that perceptual fidelity can be improved significantly
by distributing pixel error as blue noise instead. Most such works have focused
on static images, ignoring the temporal perceptual effects of animation
display. We extend prior formulations to simultaneously consider the spatial
and temporal domains, and perform an analysis to motivate a perceptually better
spatio-temporal error distribution. We then propose a practical error
optimization algorithm for spatio-temporal rendering and demonstrate its
effectiveness in various configurations.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02956" title="Abstract">arXiv:2310.02956</a> [<a href="/pdf/2310.02956" title="Download PDF">pdf</a>, <a href="/format/2310.02956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Credit card score prediction using machine learning models: A new  dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arram%2C+A">Anas Arram</a>, 
<a href="/search/cs?searchtype=author&query=Ayob%2C+M">Masri Ayob</a>, 
<a href="/search/cs?searchtype=author&query=Albadr%2C+M+A+A">Musatafa Abbas Abbood Albadr</a>, 
<a href="/search/cs?searchtype=author&query=Sulaiman%2C+A">Alaa Sulaiman</a>, 
<a href="/search/cs?searchtype=author&query=Albashish%2C+D">Dheeb Albashish</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The use of credit cards has recently increased, creating an essential need
for credit card assessment methods to minimize potential risks. This study
investigates the utilization of machine learning (ML) models for credit card
default prediction system. The main goal here is to investigate the
best-performing ML model for new proposed credit card scoring dataset. This new
dataset includes credit card transaction histories and customer profiles, is
proposed and tested using a variety of machine learning algorithms, including
logistic regression, decision trees, random forests, multi layer perceptron
(MLP) neural network, XGBoost, and LightGBM. To prepare the data for machine
learning models, we perform data pre-proccessing, feature extraction, feature
selection, and data balancing techniques. Experimental results demonstrate that
MLP outperforms logistic regression, decision trees, random forests, LightGBM,
and XGBoost in terms of predictive performance in true positive rate, achieving
an impressive area under the curve (AUC) of 86.7% and an accuracy rate of
91.6%, with a recall rate exceeding 80%. These results indicate the superiority
of MLP in predicting the default customers and assessing the potential risks.
Furthermore, they help banks and other financial institutions in predicting
loan defaults at an earlier stage.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02959" title="Abstract">arXiv:2310.02959</a> [<a href="/pdf/2310.02959" title="Download PDF">pdf</a>, <a href="/format/2310.02959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co-Optimizing Cache Partitioning and Multi-Core Task Scheduling: Exploit  Cache Sensitivity or Not?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Binqi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+D">Debayan Roy</a>, 
<a href="/search/cs?searchtype=author&query=Kloda%2C+T">Tomasz Kloda</a>, 
<a href="/search/cs?searchtype=author&query=Bastoni%2C+A">Andrea Bastoni</a>, 
<a href="/search/cs?searchtype=author&query=Pellizzoni%2C+R">Rodolfo Pellizzoni</a>, 
<a href="/search/cs?searchtype=author&query=Caccamo%2C+M">Marco Caccamo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be published in IEEE Real-Time Systems Symposium (RTSS), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Operating Systems (cs.OS)

</div>
<p class="mathjax">Cache partitioning techniques have been successfully adopted to mitigate
interference among concurrently executing real-time tasks on multi-core
processors. Considering that the execution time of a cache-sensitive task
strongly depends on the cache available for it to use, co-optimizing cache
partitioning and task allocation improves the system's schedulability. In this
paper, we propose a hybrid multi-layer design space exploration technique to
solve this multi-resource management problem. We explore the interplay between
cache partitioning and schedulability by systematically interleaving three
optimization layers, viz., (i) in the outer layer, we perform a breadth-first
search combined with proactive pruning for cache partitioning; (ii) in the
middle layer, we exploit a first-fit heuristic for allocating tasks to cores;
and (iii) in the inner layer, we use the well-known recurrence relation for the
schedulability analysis of non-preemptive fixed-priority (NP-FP) tasks in a
uniprocessor setting. Although our focus is on NP-FP scheduling, we evaluate
the flexibility of our framework in supporting different scheduling policies
(NP-EDF, P-EDF) by plugging in appropriate analysis methods in the inner layer.
Experiments show that, compared to the state-of-the-art techniques, the
proposed framework can improve the real-time schedulability of NP-FP task sets
by an average of 15.2% with a maximum improvement of 233.6% (when tasks are
highly cache-sensitive) and a minimum of 1.6% (when cache sensitivity is low).
For such task sets, we found that clustering similar-period (or mutually
compatible) tasks often leads to higher schedulability (on average 7.6%) than
clustering by cache sensitivity. In our evaluation, the framework also achieves
good results for preemptive and dynamic-priority scheduling policies.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02960" title="Abstract">arXiv:2310.02960</a> [<a href="/pdf/2310.02960" title="Download PDF">pdf</a>, <a href="/format/2310.02960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoDA: Collaborative Novel Box Discovery and Cross-modal Alignment for  Open-vocabulary 3D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yihan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dan Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023. Project Page: <a href="https://yangcaoai.github.io/publications/CoDA.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Open-vocabulary 3D Object Detection (OV-3DDet) aims to detect objects from an
arbitrary list of categories within a 3D scene, which remains seldom explored
in the literature. There are primarily two fundamental problems in OV-3DDet,
i.e., localizing and classifying novel objects. This paper aims at addressing
the two problems simultaneously via a unified framework, under the condition of
limited base categories. To localize novel 3D objects, we propose an effective
3D Novel Object Discovery strategy, which utilizes both the 3D box geometry
priors and 2D semantic open-vocabulary priors to generate pseudo box labels of
the novel objects. To classify novel object boxes, we further develop a
cross-modal alignment module based on discovered novel boxes, to align feature
spaces between 3D point cloud and image/text modalities. Specifically, the
alignment process contains a class-agnostic and a class-discriminative
alignment, incorporating not only the base objects with annotations but also
the increasingly discovered novel objects, resulting in an iteratively enhanced
alignment. The novel box discovery and crossmodal alignment are jointly learned
to collaboratively benefit each other. The novel object discovery can directly
impact the cross-modal alignment, while a better feature alignment can, in
turn, boost the localization capability, leading to a unified OV-3DDet
framework, named CoDA, for simultaneous novel object localization and
classification. Extensive experiments on two challenging datasets (i.e.,
SUN-RGBD and ScanNet) demonstrate the effectiveness of our method and also show
a significant mAP improvement upon the best-performing alternative method by
80%. Codes and pre-trained models are released on the project page.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02961" title="Abstract">arXiv:2310.02961</a> [<a href="/pdf/2310.02961" title="Download PDF">pdf</a>, <a href="/format/2310.02961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Potential Factors Leading to Popularity Unfairness in Recommender  Systems: A User-Centered Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mansoury%2C+M">Masoud Mansoury</a>, 
<a href="/search/cs?searchtype=author&query=Duijvestijn%2C+F">Finn Duijvestijn</a>, 
<a href="/search/cs?searchtype=author&query=Mourabet%2C+I">Imane Mourabet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Popularity bias is a well-known issue in recommender systems where few
popular items are over-represented in the input data, while majority of other
less popular items are under-represented. This disparate representation often
leads to bias in exposure given to the items in the recommendation results.
Extensive research examined this bias from item perspective and attempted to
mitigate it by enhancing the recommendation of less popular items. However, a
recent research has revealed the impact of this bias on users. Users with
different degree of tolerance toward popular items are not fairly served by the
recommendation system: users interested in less popular items receive more
popular items in their recommendations, while users interested in popular items
are recommended what they want. This is mainly due to the popularity bias that
popular items are over-recommended. In this paper, we aim at investigating the
factors leading to this user-side unfairness of popularity bias in recommender
systems. In particular, we investigate two factors: 1) the relationship between
this unfairness and users' interest toward items' categories (e.g., movie
genres), 2) the relationship between this unfairness and the diversity of the
popularity group in users' profile (the degree to which the user is interested
in items with different degree of popularity). Experiments on a movie
recommendation dataset using multiple recommendation algorithms show that these
two factors are significantly correlated with the degree of popularity
unfairness in the recommendation results.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02963" title="Abstract">arXiv:2310.02963</a> [<a href="/pdf/2310.02963" title="Download PDF">pdf</a>, <a href="/ps/2310.02963" title="Download PostScript">ps</a>, <a href="/format/2310.02963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SNR-Adaptive Ranging Waveform Design Based on Ziv-Zakai Bound  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yifeng Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, submitted to IEEE SPL
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Location-awareness is essential in various wireless applications. The
capability of performing precise ranging is substantial in achieving
high-accuracy localization. Due to the notorious ambiguity phenomenon, optimal
ranging waveforms should be adaptive to the signal-to-noise ratio (SNR). In
this letter, we propose to use the Ziv-Zakai bound (ZZB) as the ranging
performance metric, as well as an associated waveform design algorithm having
theoretical guarantee of achieving the optimal ZZB at a given SNR. Numerical
results suggest that, in stark contrast to the well-known high-SNR design
philosophy, the detection probability of the ranging signal becomes more
important than the resolution in the low-SNR regime.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02964" title="Abstract">arXiv:2310.02964</a> [<a href="/pdf/2310.02964" title="Download PDF">pdf</a>, <a href="/format/2310.02964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co-modeling the Sequential and Graphical Route for Peptide
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Ge Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jiangbin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Peptides are formed by the dehydration condensation of multiple amino acids.
The primary structure of a peptide can be represented either as an amino acid
sequence or as a molecular graph consisting of atoms and chemical bonds.
Previous studies have indicated that deep learning routes specific to
sequential and graphical peptide forms exhibit comparable performance on
downstream tasks. Despite the fact that these models learn representations of
the same modality of peptides, we find that they explain their predictions
differently. Considering sequential and graphical models as two experts making
inferences from different perspectives, we work on fusing expert knowledge to
enrich the learned representations for improving the discriminative
performance. To achieve this, we propose a peptide co-modeling method, RepCon,
which employs a contrastive learning-based framework to enhance the mutual
information of representations from decoupled sequential and graphical
end-to-end models. It considers representations from the sequential encoder and
the graphical encoder for the same peptide sample as a positive pair and learns
to enhance the consistency of representations between positive sample pairs and
to repel representations between negative pairs. Empirical studies of RepCon
and other co-modeling methods are conducted on open-source discriminative
datasets, including aggregation propensity, retention time, antimicrobial
peptide prediction, and family classification from Peptide Database. Our
results demonstrate the superiority of the co-modeling approach over
independent modeling, as well as the superiority of RepCon over other methods
under the co-modeling framework. In addition, the attribution on RepCon further
corroborates the validity of the approach at the level of model explanation.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02969" title="Abstract">arXiv:2310.02969</a> [<a href="/pdf/2310.02969" title="Download PDF">pdf</a>, <a href="/format/2310.02969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual Conic Proxies for AC Optimal Power Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+G">Guancheng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Tanneau%2C+M">Mathieu Tanneau</a>, 
<a href="/search/cs?searchtype=author&query=Van+Hentenryck%2C+P">Pascal Van Hentenryck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In recent years, there has been significant interest in the development of
machine learning-based optimization proxies for AC Optimal Power Flow (AC-OPF).
Although significant progress has been achieved in predicting high-quality
primal solutions, no existing learning-based approach can provide valid dual
bounds for AC-OPF. This paper addresses this gap by training optimization
proxies for a convex relaxation of AC-OPF. Namely, the paper considers a
second-order cone (SOC) relaxation of ACOPF, and proposes a novel dual
architecture that embeds a fast, differentiable (dual) feasibility recovery,
thus providing valid dual bounds. The paper combines this new architecture with
a self-supervised learning scheme, which alleviates the need for costly
training data generation. Extensive numerical experiments on medium- and
large-scale power grids demonstrate the efficiency and scalability of the
proposed methodology.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02970" title="Abstract">arXiv:2310.02970</a> [<a href="/pdf/2310.02970" title="Download PDF">pdf</a>, <a href="/format/2310.02970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast, Expressive SE$(n)$ Equivariant Networks through Weight-Sharing in  Position-Orientation Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bekkers%2C+E+J">Erik J Bekkers</a>, 
<a href="/search/cs?searchtype=author&query=Vadgama%2C+S">Sharvaree Vadgama</a>, 
<a href="/search/cs?searchtype=author&query=Hesselink%2C+R+D">Rob D Hesselink</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Linden%2C+P+A">Putri A van der Linden</a>, 
<a href="/search/cs?searchtype=author&query=Romero%2C+D+W">David W Romero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our code is publicly available at <a href="https://github.com/ebekkers/ponita">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Group Theory (math.GR)

</div>
<p class="mathjax">Based on the theory of homogeneous spaces we derive \textit{geometrically
optimal edge attributes} to be used within the flexible message passing
framework. We formalize the notion of weight sharing in convolutional networks
as the sharing of message functions over point-pairs that should be treated
equally. We define equivalence classes of point-pairs that are identical up to
a transformation in the group and derive attributes that uniquely identify
these classes. Weight sharing is then obtained by conditioning message
functions on these attributes. As an application of the theory, we develop an
efficient equivariant group convolutional network for processing 3D point
clouds. The theory of homogeneous spaces tells us how to do group convolutions
with feature maps over the homogeneous space of positions $\mathbb{R}^3$,
position and orientations $\mathbb{R}^3 {\times} S^2$, and the group SE$(3)$
itself. Among these, $\mathbb{R}^3 {\times} S^2$ is an optimal choice due to
the ability to represent directional information, which $\mathbb{R}^3$ methods
cannot, and it significantly enhances computational efficiency compared to
indexing features on the full SE$(3)$ group. We empirically support this claim
by reaching state-of-the-art results -- in accuracy and speed -- on three
different benchmarks: interatomic potential energy prediction, trajectory
forecasting in N-body systems, and generating molecules via equivariant
diffusion models.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02973" title="Abstract">arXiv:2310.02973</a> [<a href="/pdf/2310.02973" title="Download PDF">pdf</a>, <a href="/format/2310.02973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniverSLU: Universal Spoken Language Understanding for Diverse  Classification and Sequence Generation Tasks with a Single Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Siddhant Arora</a>, 
<a href="/search/cs?searchtype=author&query=Futami%2C+H">Hayato Futami</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+J">Jee-weon Jung</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yifan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Roshan Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Kashiwagi%2C+Y">Yosuke Kashiwagi</a>, 
<a href="/search/cs?searchtype=author&query=Tsunoo%2C+E">Emiru Tsunoo</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Recent studies have demonstrated promising outcomes by employing large
language models with multi-tasking capabilities. They utilize prompts to guide
the model's behavior and surpass performance of task-specific models. Motivated
by this, we ask: can we build a single model that jointly perform various
spoken language understanding (SLU) tasks? To address this, we utilize
pre-trained automatic speech recognition (ASR) models and employ various task
and dataset specifiers as discrete prompts. We demonstrate efficacy of our
single multi-task learning (MTL) model "UniverSLU" for 12 different speech
classification and sequence generation tasks across 17 datasets and 9
languages. Results show that UniverSLU achieves competitive performance and
even surpasses task-specific models. We also conduct preliminary investigations
into enabling human-interpretable natural phrases instead of task specifiers as
discrete prompts and test the model's generalization capabilities to new
paraphrases.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02975" title="Abstract">arXiv:2310.02975</a> [<a href="/pdf/2310.02975" title="Download PDF">pdf</a>, <a href="/ps/2310.02975" title="Download PostScript">ps</a>, <a href="/format/2310.02975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Genalti%2C+G">Gianmarco Genalti</a>, 
<a href="/search/cs?searchtype=author&query=Marsigli%2C+L">Lupo Marsigli</a>, 
<a href="/search/cs?searchtype=author&query=Gatti%2C+N">Nicola Gatti</a>, 
<a href="/search/cs?searchtype=author&query=Metelli%2C+A+M">Alberto Maria Metelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Heavy-tailed distributions naturally arise in many settings, from finance to
telecommunications. While regret minimization under sub-Gaussian or bounded
support rewards has been widely studied, learning on heavy-tailed distributions
only gained popularity over the last decade. In the stochastic heavy-tailed
bandit problem, an agent learns under the assumption that the distributions
have finite moments of maximum order $1+\epsilon$ which are uniformly bounded
by a constant $u$, for some $\epsilon \in (0,1]$. To the best of our knowledge,
literature only provides algorithms requiring these two quantities as an input.
In this paper, we study the stochastic adaptive heavy-tailed bandit, a
variation of the standard setting where both $\epsilon$ and $u$ are unknown to
the agent. We show that adaptivity comes at a cost, introducing two lower
bounds on the regret of any adaptive algorithm, implying a higher regret w.r.t.
the standard setting. Finally, we introduce a specific distributional
assumption and provide Adaptive Robust UCB, a regret minimization strategy
matching the known lower bound for the heavy-tailed MAB problem.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02977" title="Abstract">arXiv:2310.02977</a> [<a href="/pdf/2310.02977" title="Download PDF">pdf</a>, <a href="/format/2310.02977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuze He</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yushi Bai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Matthieu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yubin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+J">Jenny Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+R">Ran Yi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanzi Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong-Jin Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent methods in text-to-3D leverage powerful pretrained diffusion models to
optimize NeRF. Notably, these methods are able to produce high-quality 3D
scenes without training on 3D data. Due to the open-ended nature of the task,
most studies evaluate their results with subjective case studies and user
experiments, thereby presenting a challenge in quantitatively addressing the
question: How has current progress in Text-to-3D gone so far? In this paper, we
introduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing
diverse text prompts of three increasing complexity levels that are specially
designed for 3D generation. To assess both the subjective quality and the text
alignment, we propose two automatic metrics based on multi-view images produced
by the 3D contents. The quality metric combines multi-view text-image scores
and regional convolution to detect quality and view inconsistency. The
alignment metric uses multi-view captioning and Large Language Model (LLM)
evaluation to measure text-3D consistency. Both metrics closely correlate with
different dimensions of human judgments, providing a paradigm for efficiently
evaluating text-to-3D models. The benchmarking results, shown in Fig. 1, reveal
performance differences among six prevalent text-to-3D methods. Our analysis
further highlights the common struggles for current methods on generating
surroundings and multi-object scenes, as well as the bottleneck of leveraging
2D guidance for 3D generation. Our project page is available at:
https://t3bench.com.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02980" title="Abstract">arXiv:2310.02980</a> [<a href="/pdf/2310.02980" title="Download PDF">pdf</a>, <a href="/format/2310.02980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Never Train from Scratch: Fair Comparison of Long-Sequence Models  Requires Data-Driven Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amos%2C+I">Ido Amos</a>, 
<a href="/search/cs?searchtype=author&query=Berant%2C+J">Jonathan Berant</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Ankit Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Modeling long-range dependencies across sequences is a longstanding goal in
machine learning and has led to architectures, such as state space models, that
dramatically outperform Transformers on long sequences. However, these
impressive empirical gains have been by and large demonstrated on benchmarks
(e.g. Long Range Arena), where models are randomly initialized and trained to
predict a target label from an input sequence. In this work, we show that
random initialization leads to gross overestimation of the differences between
architectures and that pretraining with standard denoising objectives, using
$\textit{only the downstream task data}$, leads to dramatic gains across
multiple architectures and to very small gaps between Transformers and state
space models (SSMs). In stark contrast to prior works, we find vanilla
Transformers to match the performance of S4 on Long Range Arena when properly
pretrained, and we improve the best reported results of SSMs on the PathX-256
task by 20 absolute points. Subsequently, we analyze the utility of
previously-proposed structured parameterizations for SSMs and show they become
mostly redundant in the presence of data-driven initialization obtained through
pretraining. Our work shows that, when evaluating different architectures on
supervised tasks, incorporation of data-driven priors via pretraining is
essential for reliable performance estimation, and can be done efficiently.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02982" title="Abstract">arXiv:2310.02982</a> [<a href="/pdf/2310.02982" title="Download PDF">pdf</a>, <a href="/format/2310.02982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are LLMs Useful in the Poorest Schools? theTeacherAI in Sierra Leone
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+J+H">Jun Ho Choi</a>, 
<a href="/search/cs?searchtype=author&query=Garrod%2C+O">Oliver Garrod</a>, 
<a href="/search/cs?searchtype=author&query=Atherton%2C+P">Paul Atherton</a>, 
<a href="/search/cs?searchtype=author&query=Joyce-Gibbons%2C+A">Andrew Joyce-Gibbons</a>, 
<a href="/search/cs?searchtype=author&query=Mason-Sesay%2C+M">Miriam Mason-Sesay</a>, 
<a href="/search/cs?searchtype=author&query=Bj%C3%B6rkegren%2C+D">Daniel Bj&#xf6;rkegren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Education systems in developing countries have few resources to serve large,
poor populations. How might generative AI integrate into classrooms? This paper
introduces an AI chatbot designed to assist teachers in Sierra Leone with
professional development to improve their instruction. We describe initial
findings from early implementation across 122 schools and 193 teachers, and
analyze its use with qualitative observations and by analyzing queries.
Teachers use the system for lesson planning, classroom management, and subject
matter. A subset of teachers use the system intensively. We draw conclusions
from these findings about how generative AI systems can be integrated into
school systems in low income countries.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02985" title="Abstract">arXiv:2310.02985</a> [<a href="/pdf/2310.02985" title="Download PDF">pdf</a>, <a href="/format/2310.02985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous QoS-compliant Orchestration in the Cloud-Edge Continuum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bisicchia%2C+G">Giuseppe Bisicchia</a>, 
<a href="/search/cs?searchtype=author&query=Forti%2C+S">Stefano Forti</a>, 
<a href="/search/cs?searchtype=author&query=Pimentel%2C+E">Ernesto Pimentel</a>, 
<a href="/search/cs?searchtype=author&query=Brogi%2C+A">Antonio Brogi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The problem of managing multi-service applications on top of Cloud-Edge
networks in a QoS-aware manner has been thoroughly studied in recent years from
a decision-making perspective. However, only a few studies addressed the
problem of actively enforcing such decisions while orchestrating multi-service
applications and considering infrastructure and application variations. In this
article, we propose a next-gen orchestrator prototype based on Docker to
achieve the continuous and QoS-compliant management of multiservice
applications on top of geographically distributed Cloud-Edge resources, in
continuity with CI/CD pipelines and infrastructure monitoring tools. Finally,
we assess our proposal over a geographically distributed testbed across Italy.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02986" title="Abstract">arXiv:2310.02986</a> [<a href="/pdf/2310.02986" title="Download PDF">pdf</a>, <a href="/format/2310.02986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Impact of Disrupted Peer-to-Peer Communications on Fully  Decentralized Learning in Disaster Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Palmieri%2C+L">Luigi Palmieri</a>, 
<a href="/search/cs?searchtype=author&query=Boldrini%2C+C">Chiara Boldrini</a>, 
<a href="/search/cs?searchtype=author&query=Valerio%2C+L">Lorenzo Valerio</a>, 
<a href="/search/cs?searchtype=author&query=Passarella%2C+A">Andrea Passarella</a>, 
<a href="/search/cs?searchtype=author&query=Conti%2C+M">Marco Conti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE ICT-DM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Fully decentralized learning enables the distribution of learning resources
and decision-making capabilities across multiple user devices or nodes, and is
rapidly gaining popularity due to its privacy-preserving and decentralized
nature. Importantly, this crowdsourcing of the learning process allows the
system to continue functioning even if some nodes are affected or disconnected.
In a disaster scenario, communication infrastructure and centralized systems
may be disrupted or completely unavailable, hindering the possibility of
carrying out standard centralized learning tasks in these settings. Thus, fully
decentralized learning can help in this case. However, transitioning from
centralized to peer-to-peer communications introduces a dependency between the
learning process and the topology of the communication graph among nodes. In a
disaster scenario, even peer-to-peer communications are susceptible to abrupt
changes, such as devices running out of battery or getting disconnected from
others due to their position. In this study, we investigate the effects of
various disruptions to peer-to-peer communications on decentralized learning in
a disaster setting. We examine the resilience of a decentralized learning
process when a subset of devices drop from the process abruptly. To this end,
we analyze the difference between losing devices holding data, i.e., potential
knowledge, vs. devices contributing only to the graph connectivity, i.e., with
no data. Our findings on a Barabasi-Albert graph topology, where training data
is distributed across nodes in an IID fashion, indicate that the accuracy of
the learning process is more affected by a loss of connectivity than by a loss
of data. Nevertheless, the network remains relatively robust, and the learning
process can achieve a good level of accuracy.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02987" title="Abstract">arXiv:2310.02987</a> [<a href="/pdf/2310.02987" title="Download PDF">pdf</a>, <a href="/format/2310.02987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xufeng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Alacaoglu%2C+A">Ahmet Alacaoglu</a>, 
<a href="/search/cs?searchtype=author&query=Diakonikolas%2C+J">Jelena Diakonikolas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Machine learning approaches relying on such criteria as adversarial
robustness or multi-agent settings have raised the need for solving
game-theoretic equilibrium problems. Of particular relevance to these
applications are methods targeting finite-sum structure, which generically
arises in empirical variants of learning problems in these contexts. Further,
methods with computable approximation errors are highly desirable, as they
provide verifiable exit criteria. Motivated by these applications, we study
finite-sum monotone inclusion problems, which model broad classes of
equilibrium problems. Our main contributions are variants of the classical
Halpern iteration that employ variance reduction to obtain improved complexity
guarantees in which $n$ component operators in the finite sum are ``on
average'' either cocoercive or Lipschitz continuous and monotone, with
parameter $L$. The resulting oracle complexity of our methods, which provide
guarantees for the last iterate and for a (computable) operator norm residual,
is $\widetilde{\mathcal{O}}( n + \sqrt{n}L\varepsilon^{-1})$, which improves
upon existing methods by a factor up to $\sqrt{n}$. This constitutes the first
variance reduction-type result for general finite-sum monotone inclusions and
for more specific problems such as convex-concave optimization when operator
norm residual is the optimality measure. We further argue that, up to
poly-logarithmic factors, this complexity is unimprovable in the monotone
Lipschitz setting; i.e., the provided result is near-optimal.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02988" title="Abstract">arXiv:2310.02988</a> [<a href="/pdf/2310.02988" title="Download PDF">pdf</a>, <a href="/format/2310.02988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probing Intersectional Biases in Vision-Language Models with  Counterfactual Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Howard%2C+P">Phillip Howard</a>, 
<a href="/search/cs?searchtype=author&query=Madasu%2C+A">Avinash Madasu</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Tiep Le</a>, 
<a href="/search/cs?searchtype=author&query=Moreno%2C+G+L">Gustavo Lujan Moreno</a>, 
<a href="/search/cs?searchtype=author&query=Lal%2C+V">Vasudev Lal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While vision-language models (VLMs) have achieved remarkable performance
improvements recently, there is growing evidence that these models also posses
harmful biases with respect to social attributes such as gender and race. Prior
studies have primarily focused on probing such bias attributes individually
while ignoring biases associated with intersections between social attributes.
This could be due to the difficulty of collecting an exhaustive set of
image-text pairs for various combinations of social attributes from existing
datasets. To address this challenge, we employ text-to-image diffusion models
to produce counterfactual examples for probing intserctional social biases at
scale. Our approach utilizes Stable Diffusion with cross attention control to
produce sets of counterfactual image-text pairs that are highly similar in
their depiction of a subject (e.g., a given occupation) while differing only in
their depiction of intersectional social attributes (e.g., race &amp; gender). We
conduct extensive experiments using our generated dataset which reveal the
intersectional social biases present in state-of-the-art VLMs.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02990" title="Abstract">arXiv:2310.02990</a> [<a href="/pdf/2310.02990" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring API Capabilities with Fieldwire
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Victor%2C+N+O+C">Nwosu Obinnaya Chikezie Victor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 9 Figures, 3 Tables, Table 3 KPI evaluation before and after API
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Fieldwire, a cloud-based construction management software, has become a
pivotal tool in the construction industry. It offers a comprehensive suite of
features encompassing project management, task tracking, document management,
and collaboration. With the rise of Application Programming Interfaces (APIs)
in the software industry, Fieldwire has harnessed this trend to further empower
construction professionals. APIs act as bridges between different software
systems, and in Fieldwire's context, they hold the potential to integrate with
specialized construction tools, eliminating data silos, manual data entry, and
real-time information-sharing issues. This integration promises a streamlined
and efficient construction management process, saving both time and resources.
The research outlined in these abstract focuses on understanding Fieldwire's
API capabilities, exploring integration possibilities with various construction
tools, evaluating the impact of integration on efficiency and error reduction,
establishing best practices, and offering recommendations to construction
professionals. Python programming scripts are employed to visualize the
benefits of API integration. Empirical findings indicate that Fieldwire's API
significantly improves data accuracy, reduces project completion times by an
average of 20%, and garners high user satisfaction. Such results are paramount
in an industry reliant on precise data and efficient communication. This
research underscores the transformative potential of Fieldwire's API and its
relevance in modern construction management. It encourages construction
professionals to embrace API integration for enhanced project outcomes and
serves as an inspiration for software developers to innovate further in
construction technology. As the construction industry evolves, API integration
remains crucial for staying competitive and efficient.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02992" title="Abstract">arXiv:2310.02992</a> [<a href="/pdf/2310.02992" title="Download PDF">pdf</a>, <a href="/format/2310.02992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kosmos-G: Generating Images in Context with Multimodal Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xichen Pan</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+L">Li Dong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaohan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Z">Zhiliang Peng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://aka.ms/Kosmos-G">this https URL</a> Project Page: <a href="https://xichenpan.github.io/kosmosg">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Recent advancements in text-to-image (T2I) and vision-language-to-image
(VL2I) generation have made significant strides. However, the generation from
generalized vision-language inputs, especially involving multiple images,
remains under-explored. This paper presents Kosmos-G, a model that leverages
the advanced perception capabilities of Multimodal Large Language Models
(MLLMs) to tackle the aforementioned challenge. Our approach aligns the output
space of MLLM with CLIP using the textual modality as an anchor and performs
compositional instruction tuning on curated data. Kosmos-G demonstrates a
unique capability of zero-shot multi-entity subject-driven generation. Notably,
the score distillation instruction tuning requires no modifications to the
image decoder. This allows for a seamless substitution of CLIP and effortless
integration with a myriad of U-Net techniques ranging from fine-grained
controls to personalized image decoder variants. We posit Kosmos-G as an
initial attempt towards the goal of "image as a foreign language in image
generation."
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02993" title="Abstract">arXiv:2310.02993</a> [<a href="/pdf/2310.02993" title="Download PDF">pdf</a>, <a href="/format/2310.02993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding coherent node groups in directed graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumpulainen%2C+I">Iiro Kumpulainen</a>, 
<a href="/search/cs?searchtype=author&query=Tatti%2C+N">Nikolaj Tatti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Summarizing a large graph by grouping the nodes into clusters is a standard
technique for studying the given network. Traditionally, the order of the
discovered groups does not matter. However, there are applications where, for
example, given a directed graph, we would like to find coherent groups while
minimizing the backward cross edges. More formally, in this paper, we study a
problem where we are given a directed network and are asked to partition the
graph into a sequence of coherent groups while attempting to conform to the
cross edges. We assume that nodes in the network have features, and we measure
the group coherence by comparing these features. Furthermore, we incorporate
the cross edges by penalizing the forward cross edges and backward cross edges
with different weights. If the weights are set to 0, then the problem is
equivalent to clustering. However, if we penalize the backward edges
significantly more, then the order of discovered groups matters, and we can
view our problem as a generalization of a classic segmentation problem. To
solve the algorithm we consider a common iterative approach where we solve the
groups given the centroids, and then find the centroids given the groups. We
show that - unlike in clustering - the first subproblem is NP-hard. However, we
show that if the underlying graph is a tree we can solve the subproblem with
dynamic programming. In addition, if the number of groups is 2, we can solve
the subproblem with a minimum cut. For the more general case, we propose a
heuristic where we optimize each pair of groups separately while keeping the
remaining groups intact. We also propose a greedy search where nodes are moved
between the groups while optimizing the overall loss. We demonstrate with our
experiments that the algorithms are practical and yield interpretable results.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02994" title="Abstract">arXiv:2310.02994</a> [<a href="/pdf/2310.02994" title="Download PDF">pdf</a>, <a href="/format/2310.02994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiple Physics Pretraining for Physical Surrogate Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McCabe%2C+M">Michael McCabe</a>, 
<a href="/search/cs?searchtype=author&query=Blancard%2C+B+R">Bruno R&#xe9;galdo-Saint Blancard</a>, 
<a href="/search/cs?searchtype=author&query=Parker%2C+L+H">Liam Holden Parker</a>, 
<a href="/search/cs?searchtype=author&query=Ohana%2C+R">Ruben Ohana</a>, 
<a href="/search/cs?searchtype=author&query=Cranmer%2C+M">Miles Cranmer</a>, 
<a href="/search/cs?searchtype=author&query=Bietti%2C+A">Alberto Bietti</a>, 
<a href="/search/cs?searchtype=author&query=Eickenberg%2C+M">Michael Eickenberg</a>, 
<a href="/search/cs?searchtype=author&query=Golkar%2C+S">Siavash Golkar</a>, 
<a href="/search/cs?searchtype=author&query=Krawezik%2C+G">Geraud Krawezik</a>, 
<a href="/search/cs?searchtype=author&query=Lanusse%2C+F">Francois Lanusse</a>, 
<a href="/search/cs?searchtype=author&query=Pettee%2C+M">Mariel Pettee</a>, 
<a href="/search/cs?searchtype=author&query=Tesileanu%2C+T">Tiberiu Tesileanu</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+K">Kyunghyun Cho</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+S">Shirley Ho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce multiple physics pretraining (MPP), an autoregressive
task-agnostic pretraining approach for physical surrogate modeling. MPP
involves training large surrogate models to predict the dynamics of multiple
heterogeneous physical systems simultaneously by learning features that are
broadly useful across diverse physical tasks. In order to learn effectively in
this setting, we introduce a shared embedding and normalization strategy that
projects the fields of multiple systems into a single shared embedding space.
We validate the efficacy of our approach on both pretraining and downstream
tasks over a broad fluid mechanics-oriented benchmark. We show that a single
MPP-pretrained transformer is able to match or outperform task-specific
baselines on all pretraining sub-tasks without the need for finetuning. For
downstream tasks, we demonstrate that finetuning MPP-trained models results in
more accurate predictions across multiple time-steps on new physics compared to
training from scratch or finetuning pretrained video foundation models. We
open-source our code and model weights trained at multiple scales for
reproducibility and community experimentation.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02995" title="Abstract">arXiv:2310.02995</a> [<a href="/pdf/2310.02995" title="Download PDF">pdf</a>, <a href="/format/2310.02995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IBCL: Zero-shot Model Generation for Task Trade-offs in Continual  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+P">Pengyuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Caprio%2C+M">Michele Caprio</a>, 
<a href="/search/cs?searchtype=author&query=Eaton%2C+E">Eric Eaton</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+I">Insup Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2305.14782">arXiv:2305.14782</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Like generic multi-task learning, continual learning has the nature of
multi-objective optimization, and therefore faces a trade-off between the
performance of different tasks. That is, to optimize for the current task
distribution, it may need to compromise performance on some previous tasks.
This means that there exist multiple models that are Pareto-optimal at
different times, each addressing a distinct task performance trade-off.
Researchers have discussed how to train particular models to address specific
trade-off preferences. However, existing algorithms require training overheads
proportional to the number of preferences -- a large burden when there are
multiple, possibly infinitely many, preferences. As a response, we propose
Imprecise Bayesian Continual Learning (IBCL). Upon a new task, IBCL (1) updates
a knowledge base in the form of a convex hull of model parameter distributions
and (2) obtains particular models to address task trade-off preferences with
zero-shot. That is, IBCL does not require any additional training overhead to
generate preference-addressing models from its knowledge base. We show that
models obtained by IBCL have guarantees in identifying the Pareto optimal
parameters. Moreover, experiments on standard image classification and NLP
tasks support this guarantee. Statistically, IBCL improves average per-task
accuracy by at most 23\% and peak per-task accuracy by at most 15\% with
respect to the baseline methods, with steadily near-zero or positive backward
transfer. Most importantly, IBCL significantly reduces the training overhead
from training 1 model per preference to at most 3 models for all preferences.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02996" title="Abstract">arXiv:2310.02996</a> [<a href="/pdf/2310.02996" title="Download PDF">pdf</a>, <a href="/format/2310.02996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Stochastic Aggregative Game for Demand-Side Management in  Microgrids with Shared Battery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yadollahi%2C+S">Shahram Yadollahi</a>, 
<a href="/search/eess?searchtype=author&query=Kebriaei%2C+H">Hamed Kebriaei</a>, 
<a href="/search/eess?searchtype=author&query=Soudjani%2C+S">Sadegh Soudjani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we focus on modeling and analysis of demand-side management in
a microgrid where agents utilize grid energy and a shared battery charged by
renewable energy sources. We model the problem as a generalized stochastic
dynamic aggregative game with chance constraints that capture the effects of
uncertainties in the renewable generation and agents' demands. Computing the
solution of the game is a complex task due to probabilistic and coupling
constraints among the agents through the state of charge of the shared battery.
We investigate the Nash equilibrium of this game under uncertainty considering
both the uniqueness of the solution and the effect of uncertainty on the
solution. Simulation results demonstrate that the presented stochastic method
is superior to deterministic methods.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02997" title="Abstract">arXiv:2310.02997</a> [<a href="/pdf/2310.02997" title="Download PDF">pdf</a>, <a href="/format/2310.02997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Key-Selection for Face-based One-Time Biometrics via Morphing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Osorio-Roig%2C+D">Daile Osorio-Roig</a>, 
<a href="/search/cs?searchtype=author&query=Ghafourian%2C+M">Mahdi Ghafourian</a>, 
<a href="/search/cs?searchtype=author&query=Rathgeb%2C+C">Christian Rathgeb</a>, 
<a href="/search/cs?searchtype=author&query=Vera-Rodriguez%2C+R">Ruben Vera-Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Busch%2C+C">Christoph Busch</a>, 
<a href="/search/cs?searchtype=author&query=Fierrez%2C+J">Julian Fierrez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Nowadays, facial recognition systems are still vulnerable to adversarial
attacks. These attacks vary from simple perturbations of the input image to
modifying the parameters of the recognition model to impersonate an authorised
subject. So-called privacy-enhancing facial recognition systems have been
mostly developed to provide protection of stored biometric reference data, i.e.
templates. In the literature, privacy-enhancing facial recognition approaches
have focused solely on conventional security threats at the template level,
ignoring the growing concern related to adversarial attacks. Up to now, few
works have provided mechanisms to protect face recognition against adversarial
attacks while maintaining high security at the template level. In this paper,
we propose different key selection strategies to improve the security of a
competitive cancelable scheme operating at the signal level. Experimental
results show that certain strategies based on signal-level key selection can
lead to complete blocking of the adversarial attack based on an iterative
optimization for the most secure threshold, while for the most practical
threshold, the attack success chance can be decreased to approximately 5.0%.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02998" title="Abstract">arXiv:2310.02998</a> [<a href="/pdf/2310.02998" title="Download PDF">pdf</a>, <a href="/format/2310.02998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sung%2C+Y">Yi-Lin Sung</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jaehong Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://ecoflap.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Vision-Language Models (LVLMs) can understand the world comprehensively
by integrating rich information from different modalities, achieving remarkable
performance improvements on various multimodal downstream tasks. However,
deploying LVLMs is often problematic due to their massive computational/energy
costs and carbon consumption. Such issues make it infeasible to adopt
conventional iterative global pruning, which is costly due to computing the
Hessian matrix of the entire large model for sparsification. Alternatively,
several studies have recently proposed layer-wise pruning approaches to avoid
the expensive computation of global pruning and efficiently compress model
weights according to their importance within a layer. However, these methods
often suffer from suboptimal model compression due to their lack of a global
perspective. To address this limitation in recent efficient pruning methods for
large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP),
a two-stage coarse-to-fine weight pruning approach for LVLMs. We first
determine the sparsity ratios of different layers or blocks by leveraging the
global importance score, which is efficiently computed based on the
zeroth-order approximation of the global model gradients. Then, the multimodal
model performs local layer-wise unstructured weight pruning based on
globally-informed sparsity ratios. We validate our proposed method across
various multimodal and unimodal models and datasets, demonstrating significant
performance improvements over prevalent pruning techniques in the high-sparsity
regime.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03001" title="Abstract">arXiv:2310.03001</a> [<a href="/pdf/2310.03001" title="Download PDF">pdf</a>, <a href="/format/2310.03001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning characteristic parameters and dynamics of centrifugal pumps  under multi-phase flow using physics-informed neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Castro+Teixeira+Carvalho%2C+F">Felipe de Castro Teixeira Carvalho</a>, 
<a href="/search/cs?searchtype=author&query=Nath%2C+K">Kamaljyoti Nath</a>, 
<a href="/search/cs?searchtype=author&query=Serpa%2C+A+L">Alberto Luiz Serpa</a>, 
<a href="/search/cs?searchtype=author&query=Karniadakis%2C+G+E">George Em Karniadakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Electrical submersible pumps (ESP) are the second most used artificial
lifting equipment in the oil and gas industry due to their high flow rates and
boost pressures. They often have to handle multiphase flows, which usually
contain a mixture of hydrocarbons, water, and/or sediments. Given these
circumstances, emulsions are commonly formed. It is a liquid-liquid flow
composed of two immiscible fluids whose effective viscosity and density differ
from the single phase separately. In this context, accurate modeling of ESP
systems is crucial for optimizing oil production and implementing control
strategies. However, real-time and direct measurement of fluid and system
characteristics is often impractical due to time constraints and economy.
Hence, indirect methods are generally considered to estimate the system
parameters. In this paper, we formulate a machine learning model based on
Physics-Informed Neural Networks (PINNs) to estimate crucial system parameters.
In order to study the efficacy of the proposed PINN model, we conduct
computational studies using not only simulated but also experimental data for
different water-oil ratios. We evaluate the state variable's dynamics and
unknown parameters for various combinations when only intake and discharge
pressure measurements are available. We also study structural and practical
identifiability analyses based on commonly available pressure measurements. The
PINN model could reduce the requirement of expensive field laboratory tests
used to estimate fluid properties.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03002" title="Abstract">arXiv:2310.03002</a> [<a href="/pdf/2310.03002" title="Download PDF">pdf</a>, <a href="/format/2310.03002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No Forking Way: Detecting Cloning Attacks on Intel SGX Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Briongos%2C+S">Samira Briongos</a>, 
<a href="/search/cs?searchtype=author&query=Karame%2C+G">Ghassan Karame</a>, 
<a href="/search/cs?searchtype=author&query=Soriente%2C+C">Claudio Soriente</a>, 
<a href="/search/cs?searchtype=author&query=Wilde%2C+A">Annika Wilde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 26 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Forking attacks against TEEs like Intel SGX can be carried out either by
rolling back the application to a previous state, or by cloning the application
and by partitioning its inputs across the cloned instances. Current solutions
to forking attacks require Trusted Third Parties (TTP) that are hard to find in
real-world deployments. In the absence of a TTP, many TEE applications rely on
monotonic counters to mitigate forking attacks based on rollbacks; however,
they have no protection mechanism against forking attack based on cloning. In
this paper, we analyze 72 SGX applications and show that approximately 20% of
those are vulnerable to forking attacks based on cloning - including those that
rely on monotonic counters. To address this problem, we present CloneBuster,
the first practical clone-detection mechanism for Intel SGX that does not rely
on a TTP and, as such, can be used directly to protect existing applications.
CloneBuster allows enclaves to (self-) detect whether another enclave with the
same binary is running on the same platform. To do so, CloneBuster relies on a
cache-based covert channel for enclaves to signal their presence to (and detect
the presence of) clones on the same machine. We show that CloneBuster is robust
despite a malicious OS, only incurs a marginal impact on the application
performance, and adds approximately 800 LoC to the TCB. When used in
conjunction with monotonic counters, CloneBuster allows applications to benefit
from a comprehensive protection against forking attacks.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03003" title="Abstract">arXiv:2310.03003</a> [<a href="/pdf/2310.03003" title="Download PDF">pdf</a>, <a href="/format/2310.03003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Words to Watts: Benchmarking the Energy Costs of Large Language  Model Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Samsi%2C+S">Siddharth Samsi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=McDonald%2C+J">Joseph McDonald</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Baolin Li</a>, 
<a href="/search/cs?searchtype=author&query=Michaleas%2C+A">Adam Michaleas</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+M">Michael Jones</a>, 
<a href="/search/cs?searchtype=author&query=Bergeron%2C+W">William Bergeron</a>, 
<a href="/search/cs?searchtype=author&query=Kepner%2C+J">Jeremy Kepner</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+D">Devesh Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Gadepally%2C+V">Vijay Gadepally</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Large language models (LLMs) have exploded in popularity due to their new
generative capabilities that go far beyond prior state-of-the-art. These
technologies are increasingly being leveraged in various domains such as law,
finance, and medicine. However, these models carry significant computational
challenges, especially the compute and energy costs required for inference.
Inference energy costs already receive less attention than the energy costs of
training LLMs -- despite how often these large models are called on to conduct
inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see
increasing usage and deployment in various domains, a better understanding of
their resource utilization is crucial for cost-savings, scaling performance,
efficient hardware usage, and optimal inference strategies.
<br />In this paper, we describe experiments conducted to study the computational
and energy utilization of inference with LLMs. We benchmark and conduct a
preliminary analysis of the inference performance and inference energy costs of
different sizes of LLaMA -- a recent state-of-the-art LLM -- developed by Meta
AI on two generations of popular GPUs (NVIDIA V100 \&amp; A100) and two datasets
(Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in
research and practice. We present the results of multi-node, multi-GPU
inference using model sharding across up to 32 GPUs. To our knowledge, our work
is the one of the first to study LLM inference performance from the perspective
of computational and energy resources at this scale.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03004" title="Abstract">arXiv:2310.03004</a> [<a href="/pdf/2310.03004" title="Download PDF">pdf</a>, <a href="/format/2310.03004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soft Convex Quantization: Revisiting Vector Quantization with Convex  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gautam%2C+T">Tanmay Gautam</a>, 
<a href="/search/cs?searchtype=author&query=Pryzant%2C+R">Reid Pryzant</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chenguang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Sojoudi%2C+S">Somayeh Sojoudi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM); Optimization and Control (math.OC)

</div>
<p class="mathjax">Vector Quantization (VQ) is a well-known technique in deep learning for
extracting informative discrete latent representations. VQ-embedded models have
shown impressive results in a range of applications including image and speech
generation. VQ operates as a parametric K-means algorithm that quantizes inputs
using a single codebook vector in the forward pass. While powerful, this
technique faces practical challenges including codebook collapse,
non-differentiability and lossy compression. To mitigate the aforementioned
issues, we propose Soft Convex Quantization (SCQ) as a direct substitute for
VQ. SCQ works like a differentiable convex optimization (DCO) layer: in the
forward pass, we solve for the optimal convex combination of codebook vectors
that quantize the inputs. In the backward pass, we leverage differentiability
through the optimality conditions of the forward solution. We then introduce a
scalable relaxation of the SCQ optimization and demonstrate its efficacy on the
CIFAR-10, GTSRB and LSUN datasets. We train powerful SCQ autoencoder models
that significantly outperform matched VQ-based architectures, observing an
order of magnitude better image reconstruction and codebook usage with
comparable quantization runtime.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03005" title="Abstract">arXiv:2310.03005</a> [<a href="/pdf/2310.03005" title="Download PDF">pdf</a>, <a href="/format/2310.03005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reversing Deep Face Embeddings with Probable Privacy Protection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Osorio-Roig%2C+D">Daile Osorio-Roig</a>, 
<a href="/search/cs?searchtype=author&query=Gerlitz%2C+P+A">Paul A. Gerlitz</a>, 
<a href="/search/cs?searchtype=author&query=Rathgeb%2C+C">Christian Rathgeb</a>, 
<a href="/search/cs?searchtype=author&query=Busch%2C+C">Christoph Busch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Generally, privacy-enhancing face recognition systems are designed to offer
permanent protection of face embeddings. Recently, so-called soft-biometric
privacy-enhancement approaches have been introduced with the aim of canceling
soft-biometric attributes. These methods limit the amount of soft-biometric
information (gender or skin-colour) that can be inferred from face embeddings.
Previous work has underlined the need for research into rigorous evaluations
and standardised evaluation protocols when assessing privacy protection
capabilities. Motivated by this fact, this paper explores to what extent the
non-invertibility requirement can be met by methods that claim to provide
soft-biometric privacy protection. Additionally, a detailed vulnerability
assessment of state-of-the-art face embedding extractors is analysed in terms
of the transformation complexity used for privacy protection. In this context,
a well-known state-of-the-art face image reconstruction approach has been
evaluated on protected face embeddings to break soft biometric privacy
protection. Experimental results show that biometric privacy-enhanced face
embeddings can be reconstructed with an accuracy of up to approximately 98%,
depending on the complexity of the protection algorithm.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03006" title="Abstract">arXiv:2310.03006</a> [<a href="/pdf/2310.03006" title="Download PDF">pdf</a>, <a href="/format/2310.03006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COOLer: Class-Incremental Learning for Appearance-Based Multiple Object  Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhizheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Segu%2C+M">Mattia Segu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fisher Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> GCPR 2023 Oral
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Continual learning allows a model to learn multiple tasks sequentially while
retaining the old knowledge without the training data of the preceding tasks.
This paper extends the scope of continual learning research to
class-incremental learning for \ac{mot}, which is desirable to accommodate the
continuously evolving needs of autonomous systems. Previous solutions for
continual learning of object detectors do not address the data association
stage of appearance-based trackers, leading to catastrophic forgetting of
previous classes' re-identification features. We introduce COOLer, a
COntrastive- and cOntinual-Learning-based tracker, which incrementally learns
to track new categories while preserving past knowledge by training on a
combination of currently available ground truth labels and pseudo-labels
generated by the past tracker. To further exacerbate the disentanglement of
instance representations, we introduce a novel contrastive class-incremental
instance representation learning technique. Finally, we propose a practical
evaluation protocol for continual learning for MOT and conduct experiments on
the \bdd and \shift datasets. Experimental results demonstrate that COOLer
continually learns while effectively addressing catastrophic forgetting of both
tracking and detection. The code is available at
\url{https://github.com/BoSmallEar/COOLer}.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03007" title="Abstract">arXiv:2310.03007</a> [<a href="/pdf/2310.03007" title="Download PDF">pdf</a>, <a href="/format/2310.03007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Domain-Specific Features Disentanglement for Domain  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zenan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haobo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Junbo Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Distributional shift between domains poses great challenges to modern machine
learning algorithms. The domain generalization (DG) signifies a popular line
targeting this issue, where these methods intend to uncover universal patterns
across disparate distributions. Noted, the crucial challenge behind DG is the
existence of irrelevant domain features, and most prior works overlook this
information. Motivated by this, we propose a novel contrastive-based
disentanglement method CDDG, to effectively utilize the disentangled features
to exploit the over-looked domain-specific features, and thus facilitating the
extraction of the desired cross-domain category features for DG tasks.
Specifically, CDDG learns to decouple inherent mutually exclusive features by
leveraging them in the latent space, thus making the learning discriminative.
Extensive experiments conducted on various benchmark datasets demonstrate the
superiority of our method compared to other state-of-the-art approaches.
Furthermore, visualization evaluations confirm the potential of our method in
achieving effective feature disentanglement.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03010" title="Abstract">arXiv:2310.03010</a> [<a href="/pdf/2310.03010" title="Download PDF">pdf</a>, <a href="/format/2310.03010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-dimensional SGD aligns with emerging outlier eigenspaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arous%2C+G+B">Gerard Ben Arous</a>, 
<a href="/search/cs?searchtype=author&query=Gheissari%2C+R">Reza Gheissari</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiaoyang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jagannath%2C+A">Aukosh Jagannath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 52 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Probability (math.PR); Machine Learning (stat.ML)

</div>
<p class="mathjax">We rigorously study the joint evolution of training dynamics via stochastic
gradient descent (SGD) and the spectra of empirical Hessian and gradient
matrices. We prove that in two canonical classification tasks for multi-class
high-dimensional mixtures and either 1 or 2-layer neural networks, the SGD
trajectory rapidly aligns with emerging low-rank outlier eigenspaces of the
Hessian and gradient matrices. Moreover, in multi-layer settings this alignment
occurs per layer, with the final layer's outlier eigenspace evolving over the
course of training, and exhibiting rank deficiency when the SGD converges to
sub-optimal classifiers. This establishes some of the rich predictions that
have arisen from extensive numerical studies in the last decade about the
spectra of Hessian and information matrices over the course of training in
overparametrized networks.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03013" title="Abstract">arXiv:2310.03013</a> [<a href="/pdf/2310.03013" title="Download PDF">pdf</a>, <a href="/format/2310.03013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SemiReward: A General Reward Model for Semi-supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Weiyang Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zedong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint of 22 pages with the source code at \url{<a href="https://github.com/Westlake-AI/SemiReward">this https URL</a>}
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Semi-supervised learning (SSL) has witnessed great progress with various
improvements in the self-training framework with pseudo labeling. The main
challenge is how to distinguish high-quality pseudo labels against the
confirmation bias. However, existing pseudo-label selection strategies are
limited to pre-defined schemes or complex hand-crafted policies specially
designed for classification, failing to achieve high-quality labels, fast
convergence, and task versatility simultaneously. To these ends, we propose a
Semi-supervised Reward framework (SemiReward) that predicts reward scores to
evaluate and filter out high-quality pseudo labels, which is pluggable to
mainstream SSL methods in wide task types and scenarios. To mitigate
confirmation bias, SemiReward is trained online in two stages with a generator
model and subsampling strategy. With classification and regression tasks on 13
standard SSL benchmarks of three modalities, extensive experiments verify that
SemiReward achieves significant performance gains and faster convergence speeds
upon Pseudo Label, FlexMatch, and Free/SoftMatch.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03015" title="Abstract">arXiv:2310.03015</a> [<a href="/pdf/2310.03015" title="Download PDF">pdf</a>, <a href="/format/2310.03015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient-3DiM: Learning a Generalizable Single-image Novel-view  Synthesizer in One Day
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yifan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+J+R">Jen-Hao Rick Chang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Liangchen Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Liangliang Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The task of novel view synthesis aims to generate unseen perspectives of an
object or scene from a limited set of input images. Nevertheless, synthesizing
novel views from a single image still remains a significant challenge in the
realm of computer vision. Previous approaches tackle this problem by adopting
mesh prediction, multi-plain image construction, or more advanced techniques
such as neural radiance fields. Recently, a pre-trained diffusion model that is
specifically designed for 2D image synthesis has demonstrated its capability in
producing photorealistic novel views, if sufficiently optimized on a 3D
finetuning task. Although the fidelity and generalizability are greatly
improved, training such a powerful diffusion model requires a vast volume of
training data and model parameters, resulting in a notoriously long time and
high computational costs. To tackle this issue, we propose Efficient-3DiM, a
simple but effective framework to learn a single-image novel-view synthesizer.
Motivated by our in-depth analysis of the inference process of diffusion
models, we propose several pragmatic strategies to reduce the training overhead
to a manageable scale, including a crafted timestep sampling strategy, a
superior 3D feature extractor, and an enhanced training scheme. When combined,
our framework is able to reduce the total training time from 10 days to less
than 1 day, significantly accelerating the training process under the same
computational platform (one instance with 8 Nvidia A100 GPUs). Comprehensive
experiments are conducted to demonstrate the efficiency and generalizability of
our proposed method.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03016" title="Abstract">arXiv:2310.03016</a> [<a href="/pdf/2310.03016" title="Download PDF">pdf</a>, <a href="/format/2310.03016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding In-Context Learning in Transformers and LLMs by Learning  to Learn Discrete Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattamishra%2C+S">Satwik Bhattamishra</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+A">Arkil Patel</a>, 
<a href="/search/cs?searchtype=author&query=Blunsom%2C+P">Phil Blunsom</a>, 
<a href="/search/cs?searchtype=author&query=Kanade%2C+V">Varun Kanade</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In order to understand the in-context learning phenomenon, recent works have
adopted a stylized experimental framework and demonstrated that Transformers
can learn gradient-based learning algorithms for various classes of real-valued
functions. However, the limitations of Transformers in implementing learning
algorithms, and their ability to learn other forms of algorithms are not well
understood. Additionally, the degree to which these capabilities are confined
to attention-based models is unclear. Furthermore, it remains to be seen
whether the insights derived from these stylized settings can be extrapolated
to pretrained Large Language Models (LLMs). In this work, we take a step
towards answering these questions by demonstrating the following: (a) On a
test-bed with a variety of Boolean function classes, we find that Transformers
can nearly match the optimal learning algorithm for 'simpler' tasks, while
their performance deteriorates on more 'complex' tasks. Additionally, we find
that certain attention-free models perform (almost) identically to Transformers
on a range of tasks. (b) When provided a teaching sequence, i.e. a set of
examples that uniquely identifies a function in a class, we show that
Transformers learn more sample-efficiently. Interestingly, our results show
that Transformers can learn to implement two distinct algorithms to solve a
single task, and can adaptively select the more sample-efficient algorithm
depending on the sequence of in-context examples. (c) Lastly, we show that
extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines
on prediction tasks that are guaranteed to not be in their training set.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03017" title="Abstract">arXiv:2310.03017</a> [<a href="/pdf/2310.03017" title="Download PDF">pdf</a>, <a href="/format/2310.03017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Question Answering for Unified Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuxuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multimodal information extraction (MIE) aims to extract structured
information from unstructured multimedia content. Due to the diversity of tasks
and settings, most current MIE models are task-specific and data-intensive,
which limits their generalization to real-world scenarios with diverse task
requirements and limited labeled data. To address these issues, we propose a
novel multimodal question answering (MQA) framework to unify three MIE tasks by
reformulating them into a unified span extraction and multi-choice QA pipeline.
Extensive experiments on six datasets show that: 1) Our MQA framework
consistently and significantly improves the performances of various
off-the-shelf large multimodal models (LMM) on MIE tasks, compared to vanilla
prompting. 2) In the zero-shot setting, MQA outperforms previous
state-of-the-art baselines by a large margin. In addition, the effectiveness of
our framework can successfully transfer to the few-shot setting, enhancing LMMs
on a scale of 10B parameters to be competitive or outperform much larger
language models such as ChatGPT and GPT-4. Our MQA framework can serve as a
general principle of utilizing LMMs to better solve MIE and potentially other
downstream multimodal tasks.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03020" title="Abstract">arXiv:2310.03020</a> [<a href="/pdf/2310.03020" title="Download PDF">pdf</a>, <a href="/format/2310.03020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistent-1-to-3: Consistent Image to 3D View Synthesis via  Geometry-aware Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jianglong Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kejie Li</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yichun Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Heng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://jianglongye.com/consistent123/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Zero-shot novel view synthesis (NVS) from a single image is an essential
problem in 3D object understanding. While recent approaches that leverage
pre-trained generative models can synthesize high-quality novel views from
in-the-wild inputs, they still struggle to maintain 3D consistency across
different views. In this paper, we present Consistent-1-to-3, which is a
generative framework that significantly mitigate this issue. Specifically, we
decompose the NVS task into two stages: (i) transforming observed regions to a
novel view, and (ii) hallucinating unseen regions. We design a scene
representation transformer and view-conditioned diffusion model for performing
these two stages respectively. Inside the models, to enforce 3D consistency, we
propose to employ epipolor-guided attention to incorporate geometry
constraints, and multi-view attention to better aggregate multi-view
information. Finally, we design a hierarchy generation paradigm to generate
long sequences of consistent views, allowing a full 360 observation of the
provided object image. Qualitative and quantitative evaluation over multiple
datasets demonstrate the effectiveness of the proposed mechanisms against
state-of-the-art approaches. Our project page is at
https://jianglongye.com/consistent123/
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03022" title="Abstract">arXiv:2310.03022</a> [<a href="/pdf/2310.03022" title="Download PDF">pdf</a>, <a href="/format/2310.03022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for  Decision Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jeonghye Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Suyoung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Woojun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sung%2C+Y">Youngchul Sung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The recent success of Transformer in natural language processing has sparked
its use in various domains. In offline reinforcement learning (RL), Decision
Transformer (DT) is emerging as a promising model based on Transformer.
However, we discovered that the attention module of DT is not appropriate to
capture the inherent local dependence pattern in trajectories of RL modeled as
a Markov decision process. To overcome the limitations of DT, we propose a
novel action sequence predictor, named Decision ConvFormer (DC), based on the
architecture of MetaFormer, which is a general structure to process multiple
entities in parallel and understand the interrelationship among the multiple
entities. DC employs local convolution filtering as the token mixer and can
effectively capture the inherent local associations of the RL dataset. In
extensive experiments, DC achieved state-of-the-art performance across various
standard RL benchmarks while requiring fewer resources. Furthermore, we show
that DC better understands the underlying meaning in data and exhibits enhanced
generalization capability.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03023" title="Abstract">arXiv:2310.03023</a> [<a href="/pdf/2310.03023" title="Download PDF">pdf</a>, <a href="/format/2310.03023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-oriented Representation Learning for Robotic Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huo%2C+M">Mingxiao Huo</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Mingyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenfeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+T">Thomas Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinghao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+Y">Yao Mu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lingfeng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wei Zhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Humans inherently possess generalizable visual representations that empower
them to efficiently explore and interact with the environments in manipulation
tasks. We advocate that such a representation automatically arises from
simultaneously learning about multiple simple perceptual skills that are
critical for everyday scenarios (e.g., hand detection, state estimate, etc.)
and is better suited for learning robot manipulation policies compared to
current state-of-the-art visual representations purely based on self-supervised
objectives. We formalize this idea through the lens of human-oriented
multi-task fine-tuning on top of pre-trained visual encoders, where each task
is a perceptual skill tied to human-environment interactions. We introduce Task
Fusion Decoder as a plug-and-play embedding translator that utilizes the
underlying relationships among these perceptual skills to guide the
representation learning towards encoding meaningful structure for what's
important for all perceptual skills, ultimately empowering learning of
downstream robotic manipulation tasks. Extensive experiments across a range of
robotic tasks and embodiments, in both simulations and real-world environments,
show that our Task Fusion Decoder consistently improves the representation of
three state-of-the-art visual encoders including R3M, MVP, and EgoVLP, for
downstream manipulation policy-learning. Project page:
https://sites.google.com/view/human-oriented-robot-learning
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03025" title="Abstract">arXiv:2310.03025</a> [<a href="/pdf/2310.03025" title="Download PDF">pdf</a>, <a href="/format/2310.03025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval meets Long Context Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Peng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ping%2C+W">Wei Ping</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xianchao Wu</a>, 
<a href="/search/cs?searchtype=author&query=McAfee%2C+L">Lawrence McAfee</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+S">Sandeep Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Bakhturina%2C+E">Evelina Bakhturina</a>, 
<a href="/search/cs?searchtype=author&query=Shoeybi%2C+M">Mohammad Shoeybi</a>, 
<a href="/search/cs?searchtype=author&query=Catanzaro%2C+B">Bryan Catanzaro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Extending the context window of large language models (LLMs) is getting
popular recently, while the solution of augmenting LLMs with retrieval has
existed for years. The natural questions are: i) Retrieval-augmentation versus
long context window, which one is better for downstream tasks? ii) Can both
methods be combined to get the best of both worlds? In this work, we answer
these questions by studying both solutions using two state-of-the-art
pretrained LLMs, i.e., a proprietary 43B GPT and LLaMA2-70B. Perhaps
surprisingly, we find that LLM with 4K context window using simple
retrieval-augmentation at generation can achieve comparable performance to
finetuned LLM with 16K context window via positional interpolation on long
context tasks, while taking much less computation. More importantly, we
demonstrate that retrieval can significantly improve the performance of LLMs
regardless of their extended context window sizes. Our best model,
retrieval-augmented LLaMA2-70B with 32K context window, outperforms
GPT-3.5-turbo-16k and Davinci003 in terms of average score on seven long
context tasks including question answering and query-based summarization. It
also outperforms its non-retrieval LLaMA2-70B-32k baseline by a margin, while
being much faster at generation. Our study provides general insights on the
choice of retrieval-augmentation versus long context extension of LLM for
practitioners.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03026" title="Abstract">arXiv:2310.03026</a> [<a href="/pdf/2310.03026" title="Download PDF">pdf</a>, <a href="/format/2310.03026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LanguageMPC: Large Language Models as Decision Makers for Autonomous  Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sha%2C+H">Hao Sha</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+Y">Yao Mu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuxuan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Li Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenfeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+E">Shengbo Eben Li</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wei Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Mingyu Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Existing learning-based autonomous driving (AD) systems face challenges in
comprehending high-level information, generalizing to rare events, and
providing interpretability. To address these problems, this work employs Large
Language Models (LLMs) as a decision-making component for complex AD scenarios
that require human commonsense understanding. We devise cognitive pathways to
enable comprehensive reasoning with LLMs, and develop algorithms for
translating LLM decisions into actionable driving commands. Through this
approach, LLM decisions are seamlessly integrated with low-level controllers by
guided parameter matrix adaptation. Extensive experiments demonstrate that our
proposed method not only consistently surpasses baseline approaches in
single-vehicle tasks, but also helps handle complex driving behaviors even
multi-vehicle coordination, thanks to the commonsense reasoning capabilities of
LLMs. This paper presents an initial step toward leveraging LLMs as effective
decision-makers for intricate AD scenarios in terms of safety, efficiency,
generalizability, and interoperability. We aspire for it to serve as
inspiration for future research in this field. Project page:
https://sites.google.com/view/llm-mpc
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Thu,  5 Oct 23</h3>
<dl>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02075" title="Abstract">arXiv:2310.02075</a> (cross-list from quant-ph) [<a href="/pdf/2310.02075" title="Download PDF">pdf</a>, <a href="/format/2310.02075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Quantum Processes with Quantum Statistical Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wadhwa%2C+C">Chirag Wadhwa</a>, 
<a href="/search/quant-ph?searchtype=author&query=Doosti%2C+M">Mina Doosti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Learning complex quantum processes is a central challenge in many areas of
quantum computing and quantum machine learning, with applications in quantum
benchmarking, cryptanalysis, and variational quantum algorithms. This paper
introduces the first learning framework for studying quantum process learning
within the Quantum Statistical Query (QSQ) model, providing the first formal
definition of statistical queries to quantum processes (QPSQs). The framework
allows us to propose an efficient QPSQ learner for arbitrary quantum processes
accompanied by a provable performance guarantee. We also provide numerical
simulations to demonstrate the efficacy of this algorithm. The practical
relevance of this framework is exemplified through application in
cryptanalysis, highlighting vulnerabilities of Classical-Readout Quantum
Physical Unclonable Functions (CR-QPUFs), addressing an important open question
in the field of quantum hardware security. This work marks a significant step
towards understanding the learnability of quantum processes and shedding light
on their security implications.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02270" title="Abstract">arXiv:2310.02270</a> (cross-list from eess.IV) [<a href="/pdf/2310.02270" title="Download PDF">pdf</a>, <a href="/format/2310.02270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative Evaluation of Transfer Learning for Classification of Brain  Tumor Using MRI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Masum%2C+A+K+M">Abu Kaisar Mohammad Masum</a>, 
<a href="/search/eess?searchtype=author&query=Badhon%2C+N">Nusrat Badhon</a>, 
<a href="/search/eess?searchtype=author&query=Badhon%2C+S+M+S+I">S.M. Saiful Islam Badhon</a>, 
<a href="/search/eess?searchtype=author&query=Ria%2C+N+J">Nushrat Jahan Ria</a>, 
<a href="/search/eess?searchtype=author&query=Abujar%2C+S">Sheikh Abujar</a>, 
<a href="/search/eess?searchtype=author&query=Syed%2C+M+M">Muntaser Mansur Syed</a>, 
<a href="/search/eess?searchtype=author&query=Mahmud%2C+N">Naveed Mahmud</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, To Appear at IEEE ICMLA 2023, FL, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Abnormal growth of cells in the brain and its surrounding tissues is known as
a brain tumor. There are two types, one is benign (non-cancerous) and another
is malignant (cancerous) which may cause death. The radiologists' ability to
diagnose malignancies is greatly aided by magnetic resonance imaging (MRI).
Brain cancer diagnosis has been considerably expedited by the field of
computer-assisted diagnostics, especially in machine learning and deep
learning. In our study, we categorize three different kinds of brain tumors
using four transfer learning techniques. Our models were tested on a benchmark
dataset of $3064$ MRI pictures representing three different forms of brain
cancer. Notably, ResNet-50 outperformed other models with a remarkable accuracy
of $99.06\%$. We stress the significance of a balanced dataset for improving
accuracy without the use of augmentation methods. Additionally, we
experimentally demonstrate our method and compare with other classification
algorithms on the CE-MRI dataset using evaluations like F1-score, AUC,
precision and recall.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02276" title="Abstract">arXiv:2310.02276</a> (cross-list from nlin.PS) [<a href="/pdf/2310.02276" title="Download PDF">pdf</a>, <a href="/ps/2310.02276" title="Download PostScript">ps</a>, <a href="/format/2310.02276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep learning soliton dynamics and complex potentials recognition for 1D  and 2D PT-symmetric saturable nonlinear Schr&#xf6;dinger equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/nlin?searchtype=author&query=Song%2C+J">Jin Song</a>, 
<a href="/search/nlin?searchtype=author&query=Yan%2C+Z">Zhenya Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 20 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Physica D 448 (2023) 133729
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Pattern Formation and Solitons (nlin.PS)</span>; Machine Learning (cs.LG); Optics (physics.optics)

</div>
<p class="mathjax">In this paper, we firstly extend the physics-informed neural networks (PINNs)
to learn data-driven stationary and non-stationary solitons of 1D and 2D
saturable nonlinear Schr\"odinger equations (SNLSEs) with two fundamental
PT-symmetric Scarf-II and periodic potentials in optical fibers. Secondly, the
data-driven inverse problems are studied for PT-symmetric potential functions
discovery rather than just potential parameters in the 1D and 2D SNLSEs.
Particularly, we propose a modified PINNs (mPINNs) scheme to identify directly
the PT potential functions of the 1D and 2D SNLSEs by the solution data. And
the inverse problems about 1D and 2D PT -symmetric potentials depending on
propagation distance z are also investigated using mPINNs method. We also
identify the potential functions by the PINNs applied to the stationary
equation of the SNLSE. Furthermore, two network structures are compared under
different parameter conditions such that the predicted PT potentials can
achieve the similar high accuracy. These results illustrate that the
established deep neural networks can be successfully used in 1D and 2D SNLSEs
with high accuracies. Moreover, some main factors affecting neural networks
performance are discussed in 1D and 2D PT Scarf-II and periodic potentials,
including activation functions, structures of the networks, and sizes of the
training data. In particular, twelve different nonlinear activation functions
are in detail analyzed containing the periodic and non-periodic functions such
that it is concluded that selecting activation functions according to the form
of solution and equation usually can achieve better effect.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02281" title="Abstract">arXiv:2310.02281</a> (cross-list from eess.AS) [<a href="/pdf/2310.02281" title="Download PDF">pdf</a>, <a href="/format/2310.02281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-End Continuous Speech Emotion Recognition in Real-life Customer  Service Call Center Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Feng%2C+Y">Yajing Feng</a> (CNRS-LISN), 
<a href="/search/eess?searchtype=author&query=Devillers%2C+L">Laurence Devillers</a> (CNRS-LISN, SU)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 11th International Conference on Affective Computing and
  Intelligent Interaction Workshops and Demos (ACIIW), Sep 2023, Boston (MA),
  United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">Speech Emotion recognition (SER) in call center conversations has emerged as
a valuable tool for assessing the quality of interactions between clients and
agents. In contrast to controlled laboratory environments, real-life
conversations take place under uncontrolled conditions and are subject to
contextual factors that influence the expression of emotions. In this paper, we
present our approach to constructing a large-scale reallife dataset (CusEmo)
for continuous SER in customer service call center conversations. We adopted
the dimensional emotion annotation approach to capture the subtlety,
complexity, and continuity of emotions in real-life call center conversations,
while annotating contextual information. The study also addresses the
challenges encountered during the application of the End-to-End (E2E) SER
system to the dataset, including determining the appropriate label sampling
rate and input segment length, as well as integrating contextual information
(interlocutor's gender and empathy level) with different weights using
multitask learning. The result shows that incorporating the empathy level
information improved the model's performance.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02323" title="Abstract">arXiv:2310.02323</a> (cross-list from quant-ph) [<a href="/pdf/2310.02323" title="Download PDF">pdf</a>, <a href="/format/2310.02323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximately Equivariant Quantum Neural Network for $p4m$ Group  Symmetries in Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chang%2C+S+Y">Su Yeon Chang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Grossi%2C+M">Michele Grossi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Saux%2C+B+L">Bertrand Le Saux</a>, 
<a href="/search/quant-ph?searchtype=author&query=Vallecorsa%2C+S">Sofia Vallecorsa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, To be published as part of the IEEE Quantum Week 2023 Proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Quantum Neural Networks (QNNs) are suggested as one of the quantum algorithms
which can be efficiently simulated with a low depth on near-term quantum
hardware in the presence of noises. However, their performance highly relies on
choosing the most suitable architecture of Variational Quantum Algorithms
(VQAs), and the problem-agnostic models often suffer issues regarding
trainability and generalization power. As a solution, the most recent works
explore Geometric Quantum Machine Learning (GQML) using QNNs equivariant with
respect to the underlying symmetry of the dataset. GQML adds an inductive bias
to the model by incorporating the prior knowledge on the given dataset and
leads to enhancing the optimization performance while constraining the search
space. This work proposes equivariant Quantum Convolutional Neural Networks
(EquivQCNNs) for image classification under planar $p4m$ symmetry, including
reflectional and $90^\circ$ rotational symmetry. We present the results tested
in different use cases, such as phase detection of the 2D Ising model and
classification of the extended MNIST dataset, and compare them with those
obtained with the non-equivariant model, proving that the equivariance fosters
better generalization of the model.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02340" title="Abstract">arXiv:2310.02340</a> (cross-list from eess.IV) [<a href="/pdf/2310.02340" title="Download PDF">pdf</a>, <a href="/format/2310.02340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Interpretable Deep Disentangled Neural Networks for  Hyperspectral Unmixing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Borsoi%2C+R+A">Ricardo Augusto Borsoi</a>, 
<a href="/search/eess?searchtype=author&query=Erdo%C4%9Fmu%C5%9F%2C+D">Deniz Erdo&#x11f;mu&#x15f;</a>, 
<a href="/search/eess?searchtype=author&query=Imbiriba%2C+T">Tales Imbiriba</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Although considerable effort has been dedicated to improving the solution to
the hyperspectral unmixing problem, non-idealities such as complex radiation
scattering and endmember variability negatively impact the performance of most
existing algorithms and can be very challenging to address. Recently, deep
learning-based frameworks have been explored for hyperspectral umixing due to
their flexibility and powerful representation capabilities. However, such
techniques either do not address the non-idealities of the unmixing problem, or
rely on black-box models which are not interpretable. In this paper, we propose
a new interpretable deep learning method for hyperspectral unmixing that
accounts for nonlinearity and endmember variability. The proposed method
leverages a probabilistic variational deep-learning framework, where
disentanglement learning is employed to properly separate the abundances and
endmembers. The model is learned end-to-end using stochastic backpropagation,
and trained using a self-supervised strategy which leverages benefits from
semi-supervised learning techniques. Furthermore, the model is carefully
designed to provide a high degree of interpretability. This includes modeling
the abundances as a Dirichlet distribution, the endmembers using
low-dimensional deep latent variable representations, and using two-stream
neural networks composed of additive piecewise-linear/nonlinear components.
Experimental results on synthetic and real datasets illustrate the performance
of the proposed method compared to state-of-the-art algorithms.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02351" title="Abstract">arXiv:2310.02351</a> (cross-list from stat.AP) [<a href="/pdf/2310.02351" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Speed Deviation Patterns During Glucose Episodes: A  Quantile Regression Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Joshi%2C+A">Aparna Joshi</a>, 
<a href="/search/stat?searchtype=author&query=Merickel%2C+J">Jennifer Merickel</a>, 
<a href="/search/stat?searchtype=author&query=Desouza%2C+C+V">Cyrus V. Desouza</a>, 
<a href="/search/stat?searchtype=author&query=Rizzo%2C+M">Matthew Rizzo</a>, 
<a href="/search/stat?searchtype=author&query=Gunaratne%2C+P">Pujitha Gunaratne</a>, 
<a href="/search/stat?searchtype=author&query=Sharma%2C+A">Anuj Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, 5 Tables, Accepted and Presented at IEEE ITSC 2023 Conference in Bilbao Spain
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Given the growing prevalence of diabetes, there has been significant interest
in determining how diabetes affects instrumental daily functions, like driving.
Complication of glucose control in diabetes includes hypoglycemic and
hyperglycemic episodes, which may impair cognitive and psychomotor functions
needed for safe driving. The goal of this paper was to determine patterns of
diabetes speed behavior during acute glucose to drivers with diabetes who were
euglycemic or control drivers without diabetes in a naturalistic driving
environment. By employing distribution-based analytic methods which capture
distribution patterns, our study advances prior literature that has focused on
conventional approach of average speed to explore speed deviation patterns.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02375" title="Abstract">arXiv:2310.02375</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.02375" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Physical Origins of Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Ushveridze%2C+A">Alex Ushveridze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Classical Physics (physics.class-ph)

</div>
<p class="mathjax">The quest to comprehend the origins of intelligence raises intriguing
questions about the evolution of learning abilities in natural systems. Why do
living organisms possess an inherent drive to acquire knowledge of the unknown?
Is this motivation solely explicable through natural selection, favoring
systems capable of learning due to their increased chances of survival? Or do
there exist additional, more rapid mechanisms that offer immediate rewards to
systems entering the "learning mode" in the "right ways"? This article explores
the latter possibility and endeavors to unravel the possible nature of these
ways. We propose that learning may have non-biological and non-evolutionary
origin. It turns out that key properties of learning can be observed,
explained, and accurately reproduced within simple physical models that
describe energy accumulation mechanisms in open resonant-type systems with
dissipation.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02381" title="Abstract">arXiv:2310.02381</a> (cross-list from eess.IV) [<a href="/pdf/2310.02381" title="Download PDF">pdf</a>, <a href="/format/2310.02381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Prompt Fine-Tuning of Foundation Models for Enhanced Medical Image  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xiangru Li</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yifei Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The Segment Anything Model (SAM) is a powerful foundation model that
introduced revolutionary advancements in natural image segmentation. However,
its performance remains sub-optimal when delineating the intricate structure of
biomedical images, where multiple organs and tissues intertwine in a single
image. In this study, we introduce a novel fine-tuning framework that leverages
SAM's ability to bundle and process multiple prompts per image and seeks to
improve SAM's performance in medical images. We first curated a medical image
dataset that consists of CT scans of lesions in various organs, each with two
annotations for organs and lesions respectively. Then, we fine-tuned SAM's mask
decoder within our framework by batching both bounding boxes generated from
ground truth masks as reference. The batched prompt strategy we introduced not
only addresses the inherent complexity and ambiguity often found in medical
images but also substantially enhances performance metrics when applied onto a
wide range of segmentation tasks.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02406" title="Abstract">arXiv:2310.02406</a> (cross-list from quant-ph) [<a href="/pdf/2310.02406" title="Download PDF">pdf</a>, <a href="/ps/2310.02406" title="Download PostScript">ps</a>, <a href="/format/2310.02406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One Clean Qubit Suffices for Quantum Communication Advantage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Arunachalam%2C+S">Srinivasan Arunachalam</a>, 
<a href="/search/quant-ph?searchtype=author&query=Girish%2C+U">Uma Girish</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lifshitz%2C+N">Noam Lifshitz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">We study the one-clean-qubit model of quantum communication where one qubit
is in a pure state and all other qubits are maximally mixed. We demonstrate a
partial function that has a quantum protocol of cost $O(\log N)$ in this model,
however, every interactive randomized protocol has cost $\Omega(\sqrt{N})$,
settling a conjecture of Klauck and Lim. In contrast, all prior quantum versus
classical communication separations required at least $\Omega(\log N)$ clean
qubits. The function demonstrating our separation also has an efficient
protocol in the quantum-simultaneous-with-entanglement model of cost $O(\log N
)$. We thus recover the state-of-the-art separations between quantum and
classical communication complexity. Our proof is based on a recent
hypercontractivity inequality introduced by Ellis, Kindler, Lifshitz, and
Minzer, in conjunction with tools from the representation theory of compact Lie
groups.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02429" title="Abstract">arXiv:2310.02429</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2310.02429" title="Download PDF">pdf</a>, <a href="/format/2310.02429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The directional flow generated by peristalsis in perivascular networks  -- theoretical and numerical reduced-order descriptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Gjerde%2C+I+G">Ingeborg G. Gjerde</a>, 
<a href="/search/physics?searchtype=author&query=Rognes%2C+M+E">Marie E. Rognes</a>, 
<a href="/search/physics?searchtype=author&query=Sanchez%2C+A+L">Antonio L. Sanchez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Directional fluid flow in perivascular spaces surrounding cerebral arteries
is hypothesized to play a key role in brain solute transport and clearance.
While various drivers for pulsatile flow, such as cardiac or respiratory
pulsations, are well quantified, the question remains as to which mechanisms
could induce directional flow within physiological regimes. To address this
question, we develop theoretical and numerical reduced-order models to quantify
the directional (net) flow induceable by peristaltic pumping in periarterial
networks. Each periarterial element is modeled as a slender annular space
bounded internally by a circular tube supporting a periodic traveling
(peristaltic) wave. Under the reasonable assumptions of small Reynolds number
flow, small radii, and small-amplitude peristaltic waves, we use lubrication
theory and regular perturbation methods to derive theoretical expressions for
the directional net flow and pressure distribution in the perivascular network.
The reduced model is used to derive closed-form analytical expressions for the
net flow for simple network configurations of interest, including single
elements, two elements in tandem, and a three element bifurcation, with results
compared with numerical predictions. In particular, we provide a computable
theoretical estimate of the net flow induced by peristaltic motion in
perivascular networks as a function of physiological parameters, notably wave
length, frequency, amplitude and perivascular dimensions. Quantifying the
maximal net flow for specific physiological regimes, we find that vasomotion
may induce net pial periarterial flow velocities on the order of a few to tens
of mum/s and that sleep-related changes in vasomotion pulsatility may drive a
threefold flow increase.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02486" title="Abstract">arXiv:2310.02486</a> (cross-list from eess.IV) [<a href="/pdf/2310.02486" title="Download PDF">pdf</a>, <a href="/format/2310.02486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OCU-Net: A Novel U-Net Architecture for Enhanced Oral Cancer  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Albishri%2C+A">Ahmed Albishri</a>, 
<a href="/search/eess?searchtype=author&query=Shah%2C+S+J+H">Syed Jawad Hussain Shah</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+Y">Yugyung Lee</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+R">Rong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Accurate detection of oral cancer is crucial for improving patient outcomes.
However, the field faces two key challenges: the scarcity of deep
learning-based image segmentation research specifically targeting oral cancer
and the lack of annotated data. Our study proposes OCU-Net, a pioneering U-Net
image segmentation architecture exclusively designed to detect oral cancer in
hematoxylin and eosin (H&amp;E) stained image datasets. OCU-Net incorporates
advanced deep learning modules, such as the Channel and Spatial Attention
Fusion (CSAF) module, a novel and innovative feature that emphasizes important
channel and spatial areas in H&amp;E images while exploring contextual information.
In addition, OCU-Net integrates other innovative components such as
Squeeze-and-Excite (SE) attention module, Atrous Spatial Pyramid Pooling (ASPP)
module, residual blocks, and multi-scale fusion. The incorporation of these
modules showed superior performance for oral cancer segmentation for two
datasets used in this research. Furthermore, we effectively utilized the
efficient ImageNet pre-trained MobileNet-V2 model as a backbone of our OCU-Net
to create OCU-Netm, an enhanced version achieving state-of-the-art results.
Comprehensive evaluation demonstrates that OCU-Net and OCU-Netm outperformed
existing segmentation methods, highlighting their precision in identifying
cancer cells in H&amp;E images from OCDC and ORCA datasets.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02562" title="Abstract">arXiv:2310.02562</a> (cross-list from eess.SP) [<a href="/pdf/2310.02562" title="Download PDF">pdf</a>, <a href="/ps/2310.02562" title="Download PostScript">ps</a>, <a href="/format/2310.02562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Functional Reconfigurable Intelligent Surface: System Modeling and  Performance Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+W">Wen Wang</a>, 
<a href="/search/eess?searchtype=author&query=Ni%2C+W">Wanli Ni</a>, 
<a href="/search/eess?searchtype=author&query=Tian%2C+H">Hui Tian</a>, 
<a href="/search/eess?searchtype=author&query=Eldar%2C+Y+C">Yonina C. Eldar</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by IEEE Transactions on Wireless Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In this paper, we propose and study a multi-functional reconfigurable
intelligent surface (MF-RIS) architecture. In contrast to conventional
single-functional RIS (SF-RIS) that only reflects signals, the proposed MF-RIS
simultaneously supports multiple functions with one surface, including
reflection, refraction, amplification, and energy harvesting of wireless
signals. As such, the proposed MF-RIS is capable of significantly enhancing RIS
signal coverage by amplifying the signal reflected/refracted by the RIS with
the energy harvested. We present the signal model of the proposed MF-RIS, and
formulate an optimization problem to maximize the sum-rate of multiple users in
an MF-RIS-aided non-orthogonal multiple access network. We jointly optimize the
transmit beamforming, power allocations as well as the operating modes and
parameters for different elements of the MF-RIS and its deployment location,
via an efficient iterative algorithm. Simulation results are provided which
show significant performance gains of the MF-RIS over SF-RISs with only some of
its functions available. Moreover, we demonstrate that there exists a
fundamental trade-off between sum-rate maximization and harvested energy
maximization. In contrast to SF-RISs which can be deployed near either the
transmitter or receiver, the proposed MF-RIS should be deployed closer to the
transmitter for maximizing its communication throughput with more energy
harvested.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02564" title="Abstract">arXiv:2310.02564</a> (cross-list from eess.SP) [<a href="/pdf/2310.02564" title="Download PDF">pdf</a>, <a href="/ps/2310.02564" title="Download PostScript">ps</a>, <a href="/format/2310.02564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Analysis and Optimization of Reconfigurable Multi-Functional  Surface Assisted Wireless Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+W">Wen Wang</a>, 
<a href="/search/eess?searchtype=author&query=Ni%2C+W">Wanli Ni</a>, 
<a href="/search/eess?searchtype=author&query=Tian%2C+H">Hui Tian</a>, 
<a href="/search/eess?searchtype=author&query=Al-Dhahir%2C+N">Naofal Al-Dhahir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by IEEE Transactions on Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Although reconfigurable intelligent surfaces (RISs) can improve the
performance of wireless networks by smartly reconfiguring the radio
environment, existing passive RISs face two key challenges, i.e., double-fading
attenuation and dependence on grid/battery. To address these challenges, this
paper proposes a new RIS architecture, called multi-functional RIS (MF-RIS).
Different from conventional reflecting-only RIS, the proposed MF-RIS is capable
of supporting multiple functions with one surface, including signal reflection,
amplification, and energy harvesting. As such, our MF-RIS is able to overcome
the double-fading attenuation by harvesting energy from incident signals.
Through theoretical analysis, we derive the achievable capacity of an
MF-RIS-aided communication network. Compared to the capacity achieved by the
existing self-sustainable RIS, we derive the number of reflective elements
required for MF-RIS to outperform self-sustainable RIS. To realize a
self-sustainable communication system, we investigate the use of MF-RIS in
improving the sum-rate of multi-user wireless networks. Specifically, we solve
a non-convex optimization problem by jointly designing the transmit beamforming
and MF-RIS coefficients. As an extension, we investigate a resource allocation
problem in a practical scenario with imperfect channel state information. By
approximating the semi-infinite constraints with the S-procedure and the
general sign-definiteness, we propose a robust beamforming scheme to combat the
inevitable channel estimation errors. Finally, numerical results show that: 1)
compared to the self-sustainable RIS, MF-RIS can strike a better balance
between energy self-sustainability and throughput improvement; and 2) unlike
reflecting-only RIS which can be deployed near the transmitter or receiver,
MF-RIS should be deployed closer to the transmitter for higher spectrum
efficiency.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02581" title="Abstract">arXiv:2310.02581</a> (cross-list from stat.ML) [<a href="/pdf/2310.02581" title="Download PDF">pdf</a>, <a href="/format/2310.02581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Estimation and Inference for Robust Policy Evaluation in  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liu%2C+W">Weidong Liu</a>, 
<a href="/search/stat?searchtype=author&query=Tu%2C+J">Jiyuan Tu</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+Y">Yichen Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+X">Xi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 63 pages, 32 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, reinforcement learning has gained prominence in modern statistics,
with policy evaluation being a key component. Unlike traditional machine
learning literature on this topic, our work places emphasis on statistical
inference for the parameter estimates computed using reinforcement learning
algorithms. While most existing analyses assume random rewards to follow
standard distributions, limiting their applicability, we embrace the concept of
robust statistics in reinforcement learning by simultaneously addressing issues
of outlier contamination and heavy-tailed rewards within a unified framework.
In this paper, we develop an online robust policy evaluation procedure, and
establish the limiting distribution of our estimator, based on its Bahadur
representation. Furthermore, we develop a fully-online procedure to efficiently
conduct statistical inference based on the asymptotic distribution. This paper
bridges the gap between robust statistics and statistical inference in
reinforcement learning, offering a more versatile and reliable approach to
policy evaluation. Finally, we validate the efficacy of our algorithm through
numerical experiments conducted in real-world reinforcement learning
experiments.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02690" title="Abstract">arXiv:2310.02690</a> (cross-list from eess.IV) [<a href="/pdf/2310.02690" title="Download PDF">pdf</a>, <a href="/format/2310.02690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Dimension-Embedding-Aware Modality Fusion Transformer for  Psychiatric Disorder Clasification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+G">Guoxin Wang</a>, 
<a href="/search/eess?searchtype=author&query=Cao%2C+X">Xuyang Cao</a>, 
<a href="/search/eess?searchtype=author&query=An%2C+S">Shan An</a>, 
<a href="/search/eess?searchtype=author&query=Fan%2C+F">Fengmei Fan</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jinsong Wang</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+F">Feng Yu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zhiren Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Deep learning approaches, together with neuroimaging techniques, play an
important role in psychiatric disorders classification. Previous studies on
psychiatric disorders diagnosis mainly focus on using functional connectivity
matrices of resting-state functional magnetic resonance imaging (rs-fMRI) as
input, which still needs to fully utilize the rich temporal information of the
time series of rs-fMRI data. In this work, we proposed a
multi-dimension-embedding-aware modality fusion transformer (MFFormer) for
schizophrenia and bipolar disorder classification using rs-fMRI and T1 weighted
structural MRI (T1w sMRI). Concretely, to fully utilize the temporal
information of rs-fMRI and spatial information of sMRI, we constructed a deep
learning architecture that takes as input 2D time series of rs-fMRI and 3D
volumes T1w. Furthermore, to promote intra-modality attention and information
fusion across different modalities, a fusion transformer module (FTM) is
designed through extensive self-attention of hybrid feature maps of
multi-modality. In addition, a dimension-up and dimension-down strategy is
suggested to properly align feature maps of multi-dimensional from different
modalities. Experimental results on our private and public OpenfMRI datasets
show that our proposed MFFormer performs better than that using a single
modality or multi-modality MRI on schizophrenia and bipolar disorder diagnosis.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02694" title="Abstract">arXiv:2310.02694</a> (cross-list from stat.ML) [<a href="/pdf/2310.02694" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Block Term Decomposition for the Modelling of Higher-Order  Arrays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hinrich%2C+J+L">Jesper L&#xf8;ve Hinrich</a>, 
<a href="/search/stat?searchtype=author&query=M%C3%B8rup%2C+M">Morten M&#xf8;rup</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, preprint of submitted article
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
<p class="mathjax">Tensors are ubiquitous in science and engineering and tensor factorization
approaches have become important tools for the characterization of higher order
structure. Factorizations includes the outer-product rank Canonical Polyadic
Decomposition (CPD) as well as the multi-linear rank Tucker decomposition in
which the Block-Term Decomposition (BTD) is a structured intermediate
interpolating between these two representations. Whereas CPD, Tucker, and BTD
have traditionally relied on maximum-likelihood estimation, Bayesian inference
has been use to form probabilistic CPD and Tucker. We propose, an efficient
variational Bayesian probabilistic BTD, which uses the von-Mises Fisher matrix
distribution to impose orthogonality in the multi-linear Tucker parts forming
the BTD. On synthetic and two real datasets, we highlight the Bayesian
inference procedure and demonstrate using the proposed pBTD on noisy data and
for model order quantification. We find that the probabilistic BTD can quantify
suitable multi-linear structures providing a means for robust inference of
patterns in multi-linear data.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02699" title="Abstract">arXiv:2310.02699</a> (cross-list from eess.AS) [<a href="/pdf/2310.02699" title="Download PDF">pdf</a>, <a href="/format/2310.02699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual Contrastive Spoken Language Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cappellazzo%2C+U">Umberto Cappellazzo</a>, 
<a href="/search/eess?searchtype=author&query=Fini%2C+E">Enrico Fini</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+M">Muqiao Yang</a>, 
<a href="/search/eess?searchtype=author&query=Falavigna%2C+D">Daniele Falavigna</a>, 
<a href="/search/eess?searchtype=author&query=Brutti%2C+A">Alessio Brutti</a>, 
<a href="/search/eess?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recently, neural networks have shown impressive progress across diverse
fields, with speech processing being no exception. However, recent
breakthroughs in this area require extensive offline training using large
datasets and tremendous computing resources. Unfortunately, these models
struggle to retain their previously acquired knowledge when learning new tasks
continually, and retraining from scratch is almost always impractical. In this
paper, we investigate the problem of learning sequence-to-sequence models for
spoken language understanding in a class-incremental learning (CIL) setting and
we propose COCONUT, a CIL method that relies on the combination of experience
replay and contrastive learning. Through a modified version of the standard
supervised contrastive loss applied only to the rehearsal samples, COCONUT
preserves the learned representations by pulling closer samples from the same
class and pushing away the others. Moreover, we leverage a multimodal
contrastive loss that helps the model learn more discriminative representations
of the new data by aligning audio and text features. We also investigate
different contrastive designs to combine the strengths of the contrastive loss
with teacher-student architectures used for distillation. Experiments on two
established SLU datasets reveal the effectiveness of our proposed approach and
significant improvements over the baselines. We also show that COCONUT can be
combined with methods that operate on the decoder side of the model, resulting
in further metrics improvements.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02708" title="Abstract">arXiv:2310.02708</a> (cross-list from eess.SP) [<a href="/pdf/2310.02708" title="Download PDF">pdf</a>, <a href="/format/2310.02708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Diagonal Reconfigurable Intelligent Surfaces with Mutual  Coupling: Modeling and Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Hongyu Li</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+S">Shanpu Shen</a>, 
<a href="/search/eess?searchtype=author&query=Nerini%2C+M">Matteo Nerini</a>, 
<a href="/search/eess?searchtype=author&query=Di+Renzo%2C+M">Marco Di Renzo</a>, 
<a href="/search/eess?searchtype=author&query=Clerckx%2C+B">Bruno Clerckx</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, submit to IEEE journal for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">This work studies the modeling and optimization of beyond diagonal
reconfigurable intelligent surface (BD-RIS) aided wireless communication
systems in the presence of mutual coupling among the RIS elements.
Specifically, we first derive the mutual coupling aware BD-RIS aided
communication model using scattering and impedance parameter analysis. Based on
the obtained communication model, we propose a general BD-RIS optimization
algorithm applicable to different architectures of BD-RIS to maximize the
channel gain. Numerical results validate the effectiveness of the proposed
design and demonstrate that the larger the mutual coupling the larger the gain
offered by BD-RIS over conventional diagonal RIS.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02727" title="Abstract">arXiv:2310.02727</a> (cross-list from stat.ML) [<a href="/pdf/2310.02727" title="Download PDF">pdf</a>, <a href="/ps/2310.02727" title="Download PostScript">ps</a>, <a href="/format/2310.02727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Functional trustworthiness of AI systems by statistically valid testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nessler%2C+B">Bernhard Nessler</a>, 
<a href="/search/stat?searchtype=author&query=Doms%2C+T">Thomas Doms</a>, 
<a href="/search/stat?searchtype=author&query=Hochreiter%2C+S">Sepp Hochreiter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Position paper to the current regulation and standardization effort of AI in Europe
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The authors are concerned about the safety, health, and rights of the
European citizens due to inadequate measures and procedures required by the
current draft of the EU Artificial Intelligence (AI) Act for the conformity
assessment of AI systems. We observe that not only the current draft of the EU
AI Act, but also the accompanying standardization efforts in CEN/CENELEC, have
resorted to the position that real functional guarantees of AI systems
supposedly would be unrealistic and too complex anyways. Yet enacting a
conformity assessment procedure that creates the false illusion of trust in
insufficiently assessed AI systems is at best naive and at worst grossly
negligent. The EU AI Act thus misses the point of ensuring quality by
functional trustworthiness and correctly attributing responsibilities.
<br />The trustworthiness of an AI decision system lies first and foremost in the
correct statistical testing on randomly selected samples and in the precision
of the definition of the application domain, which enables drawing samples in
the first place. We will subsequently call this testable quality functional
trustworthiness. It includes a design, development, and deployment that enables
correct statistical testing of all relevant functions.
<br />We are firmly convinced and advocate that a reliable assessment of the
statistical functional properties of an AI system has to be the indispensable,
mandatory nucleus of the conformity assessment. In this paper, we describe the
three necessary elements to establish a reliable functional trustworthiness,
i.e., (1) the definition of the technical distribution of the application, (2)
the risk-based minimum performance requirements, and (3) the statistically
valid testing based on independent random samples.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02732" title="Abstract">arXiv:2310.02732</a> (cross-list from eess.AS) [<a href="/pdf/2310.02732" title="Download PDF">pdf</a>, <a href="/ps/2310.02732" title="Download PostScript">ps</a>, <a href="/format/2310.02732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discriminative Training of VBx Diarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Klement%2C+D">Dominik Klement</a>, 
<a href="/search/eess?searchtype=author&query=Diez%2C+M">Mireia Diez</a>, 
<a href="/search/eess?searchtype=author&query=Landini%2C+F">Federico Landini</a>, 
<a href="/search/eess?searchtype=author&query=Burget%2C+L">Luk&#xe1;&#x161; Burget</a>, 
<a href="/search/eess?searchtype=author&query=Silnova%2C+A">Anna Silnova</a>, 
<a href="/search/eess?searchtype=author&query=Delcroix%2C+M">Marc Delcroix</a>, 
<a href="/search/eess?searchtype=author&query=Tawara%2C+N">Naohiro Tawara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Bayesian HMM clustering of x-vector sequences (VBx) has become a widely
adopted diarization baseline model in publications and challenges. It uses an
HMM to model speaker turns, a generatively trained probabilistic linear
discriminant analysis (PLDA) for speaker distribution modeling, and Bayesian
inference to estimate the assignment of x-vectors to speakers. This paper
presents a new framework for updating the VBx parameters using discriminative
training, which directly optimizes a predefined loss. We also propose a new
loss that better correlates with the diarization error rate compared to binary
cross-entropy $\unicode{x2013}$ the default choice for diarization end-to-end
systems. Proof-of-concept results across three datasets (AMI, CALLHOME, and
DIHARD II) demonstrate the method's capability of automatically finding
hyperparameters, achieving comparable performance to those found by extensive
grid search, which typically requires additional hyperparameter behavior
knowledge. Moreover, we show that discriminative fine-tuning of PLDA can
further improve the model's performance. We release the source code with this
publication.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02781" title="Abstract">arXiv:2310.02781</a> (cross-list from eess.IV) [<a href="/pdf/2310.02781" title="Download PDF">pdf</a>, <a href="/format/2310.02781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LROC-PANGU-GAN: Closing the Simulation Gap in Learning Crater  Segmentation with Planetary Simulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=La%2C+J">Jaewon La</a>, 
<a href="/search/eess?searchtype=author&query=Phadke%2C+J">Jaime Phadke</a>, 
<a href="/search/eess?searchtype=author&query=Hutton%2C+M">Matt Hutton</a>, 
<a href="/search/eess?searchtype=author&query=Schwinning%2C+M">Marius Schwinning</a>, 
<a href="/search/eess?searchtype=author&query=De+Canio%2C+G">Gabriele De Canio</a>, 
<a href="/search/eess?searchtype=author&query=Renk%2C+F">Florian Renk</a>, 
<a href="/search/eess?searchtype=author&query=Kunze%2C+L">Lars Kunze</a>, 
<a href="/search/eess?searchtype=author&query=Gadd%2C+M">Matthew Gadd</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17th Symposium on Advanced Space Technologies in Robotics and Automation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">It is critical for probes landing on foreign planetary bodies to be able to
robustly identify and avoid hazards - as, for example, steep cliffs or deep
craters can pose significant risks to a probe's landing and operational
success. Recent applications of deep learning to this problem show promising
results. These models are, however, often learned with explicit supervision
over annotated datasets. These human-labelled crater databases, such as from
the Lunar Reconnaissance Orbiter Camera (LROC), may lack in consistency and
quality, undermining model performance - as incomplete and/or inaccurate labels
introduce noise into the supervisory signal, which encourages the model to
learn incorrect associations and results in the model making unreliable
predictions. Physics-based simulators, such as the Planet and Asteroid Natural
Scene Generation Utility, have, in contrast, perfect ground truth, as the
internal state that they use to render scenes is known with exactness. However,
they introduce a serious simulation-to-real domain gap - because of fundamental
differences between the simulated environment and the real-world arising from
modelling assumptions, unaccounted for physical interactions, environmental
variability, etc. Therefore, models trained on their outputs suffer when
deployed in the face of realism they have not encountered in their training
data distributions. In this paper, we therefore introduce a system to close
this "realism" gap while retaining label fidelity. We train a CycleGAN model to
synthesise LROC from Planet and Asteroid Natural Scene Generation Utility
(PANGU) images. We show that these improve the training of a downstream crater
segmentation network, with segmentation performance on a test set of real LROC
images improved as compared to using only simulated PANGU images.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02792" title="Abstract">arXiv:2310.02792</a> (cross-list from eess.IV) [<a href="/pdf/2310.02792" title="Download PDF">pdf</a>, <a href="/format/2310.02792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tracking Anything in Heart All at Once
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shen%2C+C">Chengkang Shen</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+H">Hao Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Y">You Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/eess?searchtype=author&query=Yi%2C+S">Si Yi</a>, 
<a href="/search/eess?searchtype=author&query=Dong%2C+L">Lili Dong</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+W">Weipeng Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Brady%2C+D+J">David J. Brady</a>, 
<a href="/search/eess?searchtype=author&query=Cao%2C+X">Xun Cao</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+Z">Zhan Ma</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+Y">Yi Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Myocardial motion tracking stands as an essential clinical tool in the
prevention and detection of Cardiovascular Diseases (CVDs), the foremost cause
of death globally. However, current techniques suffer incomplete and inaccurate
motion estimation of the myocardium both in spatial and temporal dimensions,
hindering the early identification of myocardial dysfunction. In addressing
these challenges, this paper introduces the Neural Cardiac Motion Field
(NeuralCMF). NeuralCMF leverages the implicit neural representation (INR) to
model the 3D structure and the comprehensive 6D forward/backward motion of the
heart. This approach offers memory-efficient storage and continuous capability
to query the precise shape and motion of the myocardium throughout the cardiac
cycle at any specific point. Notably, NeuralCMF operates without the need for
paired datasets, and its optimization is self-supervised through the physics
knowledge priors both in space and time dimensions, ensuring compatibility with
both 2D and 3D echocardiogram video inputs. Experimental validations across
three representative datasets support the robustness and innovative nature of
the NeuralCMF, marking significant advantages over existing state-of-the-arts
in cardiac imaging and motion tracking.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02810" title="Abstract">arXiv:2310.02810</a> (cross-list from math.CO) [<a href="/pdf/2310.02810" title="Download PDF">pdf</a>, <a href="/ps/2310.02810" title="Download PostScript">ps</a>, <a href="/format/2310.02810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Circular external difference families, graceful labellings and cyclotomy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Paterson%2C+M+B">Maura B. Paterson</a>, 
<a href="/search/math?searchtype=author&query=Stinson%2C+D+R">Douglas R. Stinson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">(Strong) circular external difference families (which we denote as CEDFs and
SCEDFs) can be used to construct nonmalleable threshold schemes. They are a
variation of (strong) external difference families, which have been extensively
studied in recent years. We provide a variety of constructions for CEDFs based
on graceful labellings ($\alpha$-valuations) of lexicographic products $C_n
\boldsymbol{\cdot} K_{\ell}^c$, where $C_n$ denotes a cycle of length $n$. We
do not have any nontrivial examples of SCEDFs. However, we can construct close
approximations (more specifically, certain types of circular algebraic
manipulation detection (AMD) codes) using the theory of cyclotomic numbers in
finite fields.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02829" title="Abstract">arXiv:2310.02829</a> (cross-list from eess.IV) [<a href="/pdf/2310.02829" title="Download PDF">pdf</a>, <a href="/format/2310.02829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> All Sizes Matter: Improving Volumetric Brain Segmentation on Small  Lesions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Erdur%2C+A+C">Ayhan Can Erdur</a>, 
<a href="/search/eess?searchtype=author&query=Scholz%2C+D">Daniel Scholz</a>, 
<a href="/search/eess?searchtype=author&query=Buchner%2C+J+A">Josef A. Buchner</a>, 
<a href="/search/eess?searchtype=author&query=Combs%2C+S+E">Stephanie E. Combs</a>, 
<a href="/search/eess?searchtype=author&query=Rueckert%2C+D">Daniel Rueckert</a>, 
<a href="/search/eess?searchtype=author&query=Peeken%2C+J+C">Jan C. Peeken</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Brain metastases (BMs) are the most frequently occurring brain tumors. The
treatment of patients having multiple BMs with stereo tactic radiosurgery
necessitates accurate localization of the metastases. Neural networks can
assist in this time-consuming and costly task that is typically performed by
human experts. Particularly challenging is the detection of small lesions since
they are often underrepresented in exist ing approaches. Yet, lesion detection
is equally important for all sizes. In this work, we develop an ensemble of
neural networks explicitly fo cused on detecting and segmenting small BMs. To
accomplish this task, we trained several neural networks focusing on individual
aspects of the BM segmentation problem: We use blob loss that specifically
addresses the imbalance of lesion instances in terms of size and texture and
is, therefore, not biased towards larger lesions. In addition, a model using a
subtraction sequence between the T1 and T1 contrast-enhanced sequence focuses
on low-contrast lesions. Furthermore, we train additional models only on small
lesions. Our experiments demonstrate the utility of the ad ditional blob loss
and the subtraction sequence. However, including the specialized small lesion
models in the ensemble deteriorates segmentation results. We also find
domain-knowledge-inspired postprocessing steps to drastically increase our
performance in most experiments. Our approach enables us to submit a
competitive challenge entry to the ASNR-MICCAI BraTS Brain Metastasis Challenge
2023.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02836" title="Abstract">arXiv:2310.02836</a> (cross-list from quant-ph) [<a href="/pdf/2310.02836" title="Download PDF">pdf</a>, <a href="/format/2310.02836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Realistic Neutral Atom Image Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Winklmann%2C+J">Jonas Winklmann</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tsevas%2C+D">Dimitrios Tsevas</a>, 
<a href="/search/quant-ph?searchtype=author&query=Schulz%2C+M">Martin Schulz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Software Engineering (cs.SE); Optics (physics.optics)

</div>
<p class="mathjax">Neutral atom quantum computers require accurate single atom detection for the
preparation and readout of their qubits. This is usually done using
fluorescence imaging. The occupancy of an atom site in these images is often
somewhat ambiguous due to the stochastic nature of the imaging process.
Further, the lack of ground truth makes it difficult to rate the accuracy of
reconstruction algorithms. We introduce a bottom-up simulator that is capable
of generating sample images of neutral atom experiments from a description of
the actual state in the simulated system. Possible use cases include the
creation of exemplary images for demonstration purposes, fast training
iterations for deconvolution algorithms, and generation of labeled data for
machine-learning-based atom detection approaches. The implementation is
available through our GitHub as a C library or wrapped Python package. We show
the modeled effects and implementation of the simulations at different stages
of the imaging process. Not all real-world phenomena can be reproduced
perfectly. The main discrepancies are that the simulator allows for only one
characterization of optical aberrations across the whole image, supports only
discrete atom locations, and does not model all effects of CMOS cameras
perfectly. Nevertheless, our experiments show that the generated images closely
match real-world pictures to the point that they are practically
indistinguishable and can be used as labeled data for training the next
generation of detection algorithms.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02855" title="Abstract">arXiv:2310.02855</a> (cross-list from eess.IV) [<a href="/pdf/2310.02855" title="Download PDF">pdf</a>, <a href="/format/2310.02855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Resolution Fusion for Fully Automatic Cephalometric Landmark  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Guo%2C+D">Dongqian Guo</a>, 
<a href="/search/eess?searchtype=author&query=Han%2C+W">Wencheng Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Cephalometric landmark detection on lateral skull X-ray images plays a
crucial role in the diagnosis of certain dental diseases. Accurate and
effective identification of these landmarks presents a significant challenge.
Based on extensive data observations and quantitative analyses, we discovered
that visual features from different receptive fields affect the detection
accuracy of various landmarks differently. As a result, we employed an image
pyramid structure, integrating multiple resolutions as input to train a series
of models with different receptive fields, aiming to achieve the optimal
feature combination for each landmark. Moreover, we applied several data
augmentation techniques during training to enhance the model's robustness
across various devices and measurement alternatives. We implemented this method
in the Cephalometric Landmark Detection in Lateral X-ray Images 2023 Challenge
and achieved a Mean Radial Error (MRE) of 1.62 mm and a Success Detection Rate
(SDR) 2.0mm of 74.18% in the final testing phase.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02863" title="Abstract">arXiv:2310.02863</a> (cross-list from stat.ML) [<a href="/pdf/2310.02863" title="Download PDF">pdf</a>, <a href="/format/2310.02863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformal Predictions for Longitudinal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Batra%2C+D">Devesh Batra</a>, 
<a href="/search/stat?searchtype=author&query=Mercuri%2C+S">Salvatore Mercuri</a>, 
<a href="/search/stat?searchtype=author&query=Khraishi%2C+R">Raad Khraishi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce Longitudinal Predictive Conformal Inference (LPCI), a novel
distribution-free conformal prediction algorithm for longitudinal data. Current
conformal prediction approaches for time series data predominantly focus on the
univariate setting, and thus lack cross-sectional coverage when applied
individually to each time series in a longitudinal dataset. The current
state-of-the-art for longitudinal data relies on creating infinitely-wide
prediction intervals to guarantee both cross-sectional and asymptotic
longitudinal coverage. The proposed LPCI method addresses this by ensuring that
both longitudinal and cross-sectional coverages are guaranteed without
resorting to infinitely wide intervals. In our approach, we model the residual
data as a quantile fixed-effects regression problem, constructing prediction
intervals with a trained quantile regressor. Our extensive experiments
demonstrate that LPCI achieves valid cross-sectional coverage and outperforms
existing benchmarks in terms of longitudinal coverage rates. Theoretically, we
establish LPCI's asymptotic coverage guarantees for both dimensions, with
finite-width intervals. The robust performance of LPCI in generating reliable
prediction intervals for longitudinal data underscores its potential for broad
applications, including in medicine, finance, and supply chain management.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02869" title="Abstract">arXiv:2310.02869</a> (cross-list from math.OC) [<a href="/pdf/2310.02869" title="Download PDF">pdf</a>, <a href="/ps/2310.02869" title="Download PostScript">ps</a>, <a href="/format/2310.02869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harmonic Control Lyapunov Barrier Functions for Constrained Optimal  Control with Reach-Avoid Specifications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mukherjee%2C+A">Amartya Mukherjee</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+R">Ruikun Zhou</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+J">Jun Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Analysis of PDEs (math.AP)

</div>
<p class="mathjax">This paper introduces harmonic control Lyapunov barrier functions (harmonic
CLBF) that aid in constrained control problems such as reach-avoid problems.
Harmonic CLBFs exploit the maximum principle that harmonic functions satisfy to
encode the properties of control Lyapunov barrier functions (CLBFs). As a
result, they can be initiated at the start of an experiment rather than trained
based on sample trajectories. The control inputs are selected to maximize the
inner product of the system dynamics with the steepest descent direction of the
harmonic CLBF. Numerical results are presented with four different systems
under different reach-avoid environments. Harmonic CLBFs show a significantly
low risk of entering unsafe regions and a high probability of entering the goal
region.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02872" title="Abstract">arXiv:2310.02872</a> (cross-list from hep-ex) [<a href="/pdf/2310.02872" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CMSSW Scaling Limits on Many-Core Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ex?searchtype=author&query=Jones%2C+C">Christopher Jones</a> (1), 
<a href="/search/hep-ex?searchtype=author&query=Gartung%2C+P">Patrick Gartung</a> (1) ((1) Fermilab)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26th Intl Conf Computing High Energy &amp; Nuclear Phys (CHEP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Experiment (hep-ex)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Today the LHC offline computing relies heavily on CPU resources, despite the
interest in compute accelerators, such as GPUs, for the longer term future. The
number of cores per CPU socket has continued to increase steadily, reaching the
levels of 64 cores (128 threads) with recent AMD EPYC processors, and 128 cores
on Ampere Altra Max ARM processors. Over the course of the past decade, the CMS
data processing framework, CMSSW, has been transformed from a single-threaded
framework into a highly concurrent one. The first multithreaded version was
brought into production by the start of the LHC Run 2 in 2015. Since then, the
framework's threading efficiency has gradually been improved by adding more
levels of concurrency and reducing the amount of serial code paths. The latest
addition was support for concurrent Runs. In this work we review the
concurrency model of the CMSSW, and measure its scalability with real CMS
applications, such as simulation and reconstruction, on mode rn many-core
machines. We show metrics such as event processing throughput and application
memory usage with and without the contribution of I/O, as I/O has been the
major scaling limitation for the CMS applications.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02877" title="Abstract">arXiv:2310.02877</a> (cross-list from stat.ML) [<a href="/pdf/2310.02877" title="Download PDF">pdf</a>, <a href="/format/2310.02877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stationarity without mean reversion: Improper Gaussian process  regression and improper kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ambrogioni%2C+L">Luca Ambrogioni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Gaussian processes (GP) regression has gained substantial popularity in
machine learning applications. The behavior of a GP regression depends on the
choice of covariance function. Stationary covariance functions are favorite in
machine learning applications. However, (non-periodic) stationary covariance
functions are always mean reverting and can therefore exhibit pathological
behavior when applied to data that does not relax to a fixed global mean value.
In this paper, we show that it is possible to use improper GP prior with
infinite variance to define processes that are stationary but not mean
reverting. To this aim, we introduce a large class of improper kernels that can
only be defined in this improper regime. Specifically, we introduce the Smooth
Walk kernel, which produces infinitely smooth samples, and a family of improper
Mat\'ern kernels, which can be defined to be $j$-times differentiable for any
integer $j$. The resulting posterior distributions can be computed analytically
and it involves a simple correction of the usual formulas. By analyzing both
synthetic and real data, we demonstrate that these improper kernels solve some
known pathologies of mean reverting GP regression while retaining most of the
favourable properties of ordinary smooth stationary kernels.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02896" title="Abstract">arXiv:2310.02896</a> (cross-list from math.HO) [<a href="/pdf/2310.02896" title="Download PDF">pdf</a>, <a href="/format/2310.02896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Notes on a Path to AI Assistance in Mathematical Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kontorovich%2C+A">Alex Kontorovich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">History and Overview (math.HO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">These informal notes are based on the author's lecture at the National
Academies of Science, Engineering, and Mathematics workshop on "AI to Assist
Mathematical Reasoning" in June 2023. The goal is to think through a path by
which we might arrive at AI that is useful for the research mathematician.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02904" title="Abstract">arXiv:2310.02904</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2310.02904" title="Download PDF">pdf</a>, <a href="/format/2310.02904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spline-based neural network interatomic potentials: blending classical  and machine learning models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Vita%2C+J+A">Joshua A. Vita</a>, 
<a href="/search/cond-mat?searchtype=author&query=Trinkle%2C+D+R">Dallas R. Trinkle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">While machine learning (ML) interatomic potentials (IPs) are able to achieve
accuracies nearing the level of noise inherent in the first-principles data to
which they are trained, it remains to be shown if their increased complexities
are strictly necessary for constructing high-quality IPs. In this work, we
introduce a new MLIP framework which blends the simplicity of spline-based MEAM
(s-MEAM) potentials with the flexibility of a neural network (NN) architecture.
The proposed framework, which we call the spline-based neural network potential
(s-NNP), is a simplified version of the traditional NNP that can be used to
describe complex datasets in a computationally efficient manner. We demonstrate
how this framework can be used to probe the boundary between classical and ML
IPs, highlighting the benefits of key architectural changes. Furthermore, we
show that using spline filters for encoding atomic environments results in a
readily interpreted embedding layer which can be coupled with modifications to
the NN to incorporate expected physical behaviors and improve overall
interpretability. Finally, we test the flexibility of the spline filters,
observing that they can be shared across multiple chemical systems in order to
provide a convenient reference point from which to begin performing
cross-system analyses.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02922" title="Abstract">arXiv:2310.02922</a> (cross-list from quant-ph) [<a href="/pdf/2310.02922" title="Download PDF">pdf</a>, <a href="/format/2310.02922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Public verifiable measurement-only blind quantum computation based on  entanglement witnesses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Liu%2C+W">Wen-Jie Liu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+Z">Zi-Xian Li</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+W">Wen-Bo Li</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yang%2C+Q">Qi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Quantum Information Processing, 2023. 22(3): p. 137
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR); Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Recently, Sato et al. proposed an public verifiable blind quantum computation
(BQC) protocol by inserting a third-party arbiter. However, it is not true
public verifiable in a sense, because the arbiter is determined in advance and
participates in the whole process. In this paper, a public verifiable protocol
for measurement-only BQC is proposed. The fidelity between arbitrary states and
the graph states of 2-colorable graphs is estimated by measuring the
entanglement witnesses of the graph states,so as to verify the correctness of
the prepared graph states. Compared with the previous protocol, our protocol is
public verifiable in the true sense by allowing other random clients to execute
the public verification. It also has greater advantages in the efficiency,
where the number of local measurements is O(n^3*log {n}) and graph states'
copies is O(n^2*log{n}).
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02923" title="Abstract">arXiv:2310.02923</a> (cross-list from quant-ph) [<a href="/pdf/2310.02923" title="Download PDF">pdf</a>, <a href="/format/2310.02923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unitary Operator Construction Solution Based on Pauli Group for  Maximal Dense Coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Liu%2C+W">Wenjie Liu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+J">Junxiu Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yu%2C+W">Wenbin Yu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Liu%2C+Z">Zhihao Liu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+H">Hanwu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Quantum Information Processing, 2020. 19(8): p. 231
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Quantum dense coding plays an important role in quantum cryptography
communication, and how to select a set of appropriate unitary operators to
encode message is the primary work in the design of quantum communication
protocols. Shukla et al. proposed a preliminary method for unitary operator
construction based on Pauli group under multiplication, which is used for dense
coding in quantum dialogue. However, this method lacks feasible steps or
conditions, and cannot construct all the possible unitary operator sets. In
this study, a feasible solution of constructing unitary operator sets for
quantum maximal dense coding is proposed, which aims to use minimum qubits to
maximally encode a class of t-qubit symmetric states. These states have an even
number of superposition items, and there is at least one set of t/2 qubits
whose superposition items are orthogonal to each other. Firstly, we propose the
procedure and the corresponding algorithm for constructing 2^t-order
multiplicative modified generalized Pauli subgroups (multiplicative MGP
subgroups). Then, two conditions for t-qubit symmetric states are given to
select appropriate unitary operator sets from the above subgroups. Finally, we
take 3-qubit GHZ, 4-qubit W, 4-qubit cluster and 5-qubit cluster states as
examples, and demonstrate how to find all unitary operator sets for maximal
dense coding through our construction solution, which shows that our solution
is feasible and convenient.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02924" title="Abstract">arXiv:2310.02924</a> (cross-list from quant-ph) [<a href="/pdf/2310.02924" title="Download PDF">pdf</a>, <a href="/format/2310.02924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum forgery attacks against OTR structures based on Simon&#x27;s  algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Liu%2C+W">Wenjie Liu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+M">Mengting Wang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+Z">Zixian Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Modern Physics Letters A, 2023: p. 2350092
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR); Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Classical forgery attacks against Offset Two-round (OTR) structures require
some harsh conditions, such as some plaintext and ciphertext pairs need to be
known, and the success probability is not too high. To solve these problems, a
quantum forgery attack on OTR structure using Simon's algorithm is proposed.
The attacker intercept the ciphertext-tag pair $(C,T)$ between the sender and
receiver, while Simon's algorithm is used to find the period of the tag
generation function in OTR, then we can successfully forge new ciphertext $C'$
($C'\ne C$) for intercepted tag $T$. For a variant of OTR structure
(Pr{/o}st-OTR-Even-Mansour structure), a universal forgery attack, in which it
is easy to generate the correct tag of any given message if the attacker is
allowed to change a single block in it, is proposed. It first obtains the
secret parameter L using Simon's algorithm, then the secret parameter L is used
to find the keys $k_1$ and $k_2$, so that an attacker can forge the changed
messages. It only needs several plaintext blocks to help obtain the keys to
forge any messages. Performance analysis shows that the query complexity of our
attack is $O(n)$, and its success probability is very close to 1.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02930" title="Abstract">arXiv:2310.02930</a> (cross-list from math.OC) [<a href="/pdf/2310.02930" title="Download PDF">pdf</a>, <a href="/ps/2310.02930" title="Download PostScript">ps</a>, <a href="/format/2310.02930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Small-Disturbance Input-to-State Stability of Perturbed Gradient Flows:  Applications to LQR Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cui%2C+L">Leilei Cui</a>, 
<a href="/search/math?searchtype=author&query=Jiang%2C+Z">Zhong-Ping Jiang</a>, 
<a href="/search/math?searchtype=author&query=Sontag%2C+E+D">Eduardo D. Sontag</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper studies the effect of perturbations on the gradient flow of a
general constrained nonlinear programming problem, where the perturbation may
arise from inaccurate gradient estimation in the setting of data-driven
optimization. Under suitable conditions on the objective function, the
perturbed gradient flow is shown to be small-disturbance input-to-state stable
(ISS), which implies that, in the presence of a small-enough perturbation, the
trajectory of the perturbed gradient flow must eventually enter a small
neighborhood of the optimum. This work was motivated by the question of
robustness of direct methods for the linear quadratic regulator problem, and
specifically the analysis of the effect of perturbations caused by gradient
estimation or round-off errors in policy optimization. Interestingly, we show
small-disturbance ISS for three of the most common optimization algorithms:
standard gradient flow, natural gradient flow, and Newton gradient flow.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02941" title="Abstract">arXiv:2310.02941</a> (cross-list from stat.ML) [<a href="/pdf/2310.02941" title="Download PDF">pdf</a>, <a href="/ps/2310.02941" title="Download PostScript">ps</a>, <a href="/format/2310.02941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hoeffding&#x27;s Inequality for Markov Chains under Generalized  Concentrability Condition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/stat?searchtype=author&query=Gupta%2C+A">Abhishek Gupta</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+Y">Yin Sun</a>, 
<a href="/search/stat?searchtype=author&query=Shroff%2C+N">Ness Shroff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper studies Hoeffding's inequality for Markov chains under the
generalized concentrability condition defined via integral probability metric
(IPM). The generalized concentrability condition establishes a framework that
interpolates and extends the existing hypotheses of Markov chain Hoeffding-type
inequalities. The flexibility of our framework allows Hoeffding's inequality to
be applied beyond the ergodic Markov chains in the traditional sense. We
demonstrate the utility by applying our framework to several non-asymptotic
analyses arising from the field of machine learning, including (i) a
generalization bound for empirical risk minimization with Markovian samples,
(ii) a finite sample guarantee for Ployak-Ruppert averaging of SGD, and (iii) a
new regret bound for rested Markovian bandits with general state space.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02946" title="Abstract">arXiv:2310.02946</a> (cross-list from math-ph) [<a href="/pdf/2310.02946" title="Download PDF">pdf</a>, <a href="/format/2310.02946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Max-Entropy and Free Energy Principles, Belief Diffusions and  their Singularities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math-ph?searchtype=author&query=Peltre%2C+O">Olivier Peltre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Physics (math-ph)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Artificial Intelligence (cs.AI); Information Theory (cs.IT); Algebraic Topology (math.AT)

</div>
<p class="mathjax">A comprehensive picture of three Bethe-Kikuchi variational principles
including their relationship to belief propagation (BP) algorithms on
hypergraphs is given. The structure of BP equations is generalized to define
continuous-time diffusions, solving localized versions of the max-entropy
principle (A), the variational free energy principle (B), and a less usual
equilibrium free energy principle (C), Legendre dual to A. Both critical points
of Bethe-Kikuchi functionals and stationary beliefs are shown to lie at the
non-linear intersection of two constraint surfaces, enforcing energy
conservation and marginal consistency respectively. The hypersurface of
singular beliefs, accross which equilibria become unstable as the constraint
surfaces meet tangentially, is described by polynomial equations in the convex
polytope of consistent beliefs. This polynomial is expressed by a loop series
expansion for graphs of binary variables.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02948" title="Abstract">arXiv:2310.02948</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.02948" title="Download PDF">pdf</a>, <a href="/format/2310.02948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HappyFeat -- An interactive and efficient BCI framework for clinical  applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Desbois%2C+A">Arthur Desbois</a>, 
<a href="/search/q-bio?searchtype=author&query=Venot%2C+T">Tristan Venot</a>, 
<a href="/search/q-bio?searchtype=author&query=De+Vico+Fallani%2C+F">Fabrizio De Vico Fallani</a>, 
<a href="/search/q-bio?searchtype=author&query=Corsi%2C+M">Marie-Constance Corsi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures, 2 tables, "Annex" section
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Brain-Computer Interface (BCI) systems allow users to perform actions by
translating their brain activity into commands. Such systems usually need a
training phase, consisting in training a classification algorithm to
discriminate between mental states using specific features from the recorded
signals. This phase of feature selection and training is crucial for BCI
performance and presents specific constraints to be met in a clinical context,
such as post-stroke rehabilitation.
<br />In this paper, we present HappyFeat, a software making Motor Imagery (MI)
based BCI experiments easier, by gathering all necessary manipulations and
analysis in a single convenient GUI and via automation of experiment or
analysis parameters. The resulting workflow allows for effortlessly selecting
the best features, helping to achieve good BCI performance in time-constrained
environments. Alternative features based on Functional Connectivity can be used
and compared or combined with Power Spectral Density, allowing a
network-oriented approach.
<br />We then give details of HappyFeat's main mechanisms, and a review of its
performances in typical use cases. We also show that it can be used as an
efficient tool for comparing different metrics extracted from the signals, to
train the classification algorithm. To this end, we show a comparison between
the commonly-used Power Spectral Density and network metrics based on
Functional Connectivity.
<br />HappyFeat is available as an open-source project which can be freely
downloaded on GitHub.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02951" title="Abstract">arXiv:2310.02951</a> (cross-list from math.OC) [<a href="/pdf/2310.02951" title="Download PDF">pdf</a>, <a href="/ps/2310.02951" title="Download PostScript">ps</a>, <a href="/format/2310.02951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fisher-Rao gradient flow for entropy-regularised Markov decision  processes in Polish spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kerimkulov%2C+B">Bekzhan Kerimkulov</a>, 
<a href="/search/math?searchtype=author&query=Leahy%2C+J">James-Michael Leahy</a>, 
<a href="/search/math?searchtype=author&query=Siska%2C+D">David Siska</a>, 
<a href="/search/math?searchtype=author&query=Szpruch%2C+L">Lukasz Szpruch</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Y">Yufei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Probability (math.PR)

</div>
<p class="mathjax">We study the global convergence of a Fisher-Rao policy gradient flow for
infinite-horizon entropy-regularised Markov decision processes with Polish
state and action space. The flow is a continuous-time analogue of a policy
mirror descent method. We establish the global well-posedness of the gradient
flow and demonstrate its exponential convergence to the optimal policy.
Moreover, we prove the flow is stable with respect to gradient evaluation,
offering insights into the performance of a natural policy gradient flow with
log-linear policy parameterisation. To overcome challenges stemming from the
lack of the convexity of the objective function and the discontinuity arising
from the entropy regulariser, we leverage the performance difference lemma and
the duality relationship between the gradient and mirror descent flows.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02952" title="Abstract">arXiv:2310.02952</a> (cross-list from math.LO) [<a href="/pdf/2310.02952" title="Download PDF">pdf</a>, <a href="/ps/2310.02952" title="Download PostScript">ps</a>, <a href="/format/2310.02952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Some more theorems on structural entailment relations and  non-deterministic semantics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Caleiro%2C+C">Carlos Caleiro</a>, 
<a href="/search/math?searchtype=author&query=Marcelino%2C+S">S&#xe9;rgio Marcelino</a>, 
<a href="/search/math?searchtype=author&query=Rivieccio%2C+U">Umberto Rivieccio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We extend classical work by Janusz Czelakowski on the closure properties of
the class of matrix models of entailment relations - nowadays more commonly
called multiple-conclusion logics - to the setting of non-deterministic
matrices (Nmatrices), characterizing the Nmatrix models of an arbitrary logic
through a generalization of the standard class operators to the
non-deterministic setting. We highlight the main differences that appear in
this more general setting, in particular: the possibility to obtain Nmatrix
quotients using any compatible equivalence relation (not necessarily a
congruence); the problem of determining when strict homomorphisms preserve the
logic of a given Nmatrix; the fact that the operations of taking images and
preimages cannot be swapped, which determines the exact sequence of operators
that generates, from any complete semantics, the class of all Nmatrix models of
a logic. Many results, on the other hand, generalize smoothly to the
non-deterministic setting: we show for instance that a logic is finitely based
if and only if both the class of its Nmatrix models and its complement are
closed under ultraproducts. We conclude by mentioning possible developments in
adapting the Abstract Algebraic Logic approach to logics induced by Nmatrices
and the associated equational reasoning over non-deterministic algebras.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02971" title="Abstract">arXiv:2310.02971</a> (cross-list from eess.AS) [<a href="/pdf/2310.02971" title="Download PDF">pdf</a>, <a href="/format/2310.02971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+M">Ming-Hsin Chen</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+Y">Yun-Ping Lin</a>, 
<a href="/search/eess?searchtype=author&query=Hsu%2C+J+N">Jing Neng Hsu</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+P+K">Paul Kuo-Ming Huang</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+C">Chien-yu Huang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shang-Wen Li</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Signal Processing (eess.SP)

</div>
<p class="mathjax">Prompting and adapter tuning have emerged as efficient alternatives to
fine-tuning (FT) methods. However, existing studies on speech prompting focused
on classification tasks and failed on more complex sequence generation tasks.
Besides, adapter tuning is primarily applied with a focus on encoder-only
self-supervised models. Our experiments show that prompting on Wav2Seq, a
self-supervised encoder-decoder model, surpasses previous works in sequence
generation tasks. It achieves a remarkable 53% relative improvement in word
error rate for ASR and a 27% in F1 score for slot filling. Additionally,
prompting competes with the FT method in the low-resource scenario. Moreover,
we show the transferability of prompting and adapter tuning on Wav2Seq in
cross-lingual ASR. When limited trainable parameters are involved, prompting
and adapter tuning consistently outperform conventional FT across 7 languages.
Notably, in the low-resource scenario, prompting consistently outperforms
adapter tuning.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02972" title="Abstract">arXiv:2310.02972</a> (cross-list from eess.IV) [<a href="/pdf/2310.02972" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Automatic Segmentation of Gross Target Volume and Organs-at-Risk  for Radiotherapy Planning of Nasopharyngeal Carcinoma
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Astaraki%2C+M">Mehdi Astaraki</a>, 
<a href="/search/eess?searchtype=author&query=Bendazzoli%2C+S">Simone Bendazzoli</a>, 
<a href="/search/eess?searchtype=author&query=Toma-Dasu%2C+I">Iuliana Toma-Dasu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures, 3 tables, MICCAI SegRap challenge contribution
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Target segmentation in CT images of Head&amp;Neck (H&amp;N) region is challenging due
to low contrast between adjacent soft tissue. The SegRap 2023 challenge has
been focused on benchmarking the segmentation algorithms of Nasopharyngeal
Carcinoma (NPC) which would be employed as auto-contouring tools for radiation
treatment planning purposes. We propose a fully-automatic framework and develop
two models for a) segmentation of 45 Organs at Risk (OARs) and b) two Gross
Tumor Volumes (GTVs). To this end, we preprocess the image volumes by
harmonizing the intensity distributions and then automatically cropping the
volumes around the target regions. The preprocessed volumes were employed to
train a standard 3D U-Net model for each task, separately. Our method took
second place for each of the tasks in the validation phase of the challenge.
The proposed framework is available at https://github.com/Astarakee/segrap2023
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02984" title="Abstract">arXiv:2310.02984</a> (cross-list from stat.ML) [<a href="/pdf/2310.02984" title="Download PDF">pdf</a>, <a href="/format/2310.02984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Laws for Associative Memories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cabannes%2C+V">Vivien Cabannes</a>, 
<a href="/search/stat?searchtype=author&query=Dohmatob%2C+E">Elvis Dohmatob</a>, 
<a href="/search/stat?searchtype=author&query=Bietti%2C+A">Alberto Bietti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Learning arguably involves the discovery and memorization of abstract rules.
The aim of this paper is to study associative memory mechanisms. Our model is
based on high-dimensional matrices consisting of outer products of embeddings,
which relates to the inner layers of transformer language models. We derive
precise scaling laws with respect to sample size and parameter size, and
discuss the statistical efficiency of different estimators, including
optimization-based algorithms. We provide extensive numerical experiments to
validate and interpret theoretical results, including fine-grained
visualizations of the stored memory associations.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02989" title="Abstract">arXiv:2310.02989</a> (cross-list from stat.ML) [<a href="/pdf/2310.02989" title="Download PDF">pdf</a>, <a href="/format/2310.02989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> xVal: A Continuous Number Encoding for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Golkar%2C+S">Siavash Golkar</a>, 
<a href="/search/stat?searchtype=author&query=Pettee%2C+M">Mariel Pettee</a>, 
<a href="/search/stat?searchtype=author&query=Eickenberg%2C+M">Michael Eickenberg</a>, 
<a href="/search/stat?searchtype=author&query=Bietti%2C+A">Alberto Bietti</a>, 
<a href="/search/stat?searchtype=author&query=Cranmer%2C+M">Miles Cranmer</a>, 
<a href="/search/stat?searchtype=author&query=Krawezik%2C+G">Geraud Krawezik</a>, 
<a href="/search/stat?searchtype=author&query=Lanusse%2C+F">Francois Lanusse</a>, 
<a href="/search/stat?searchtype=author&query=McCabe%2C+M">Michael McCabe</a>, 
<a href="/search/stat?searchtype=author&query=Ohana%2C+R">Ruben Ohana</a>, 
<a href="/search/stat?searchtype=author&query=Parker%2C+L">Liam Parker</a>, 
<a href="/search/stat?searchtype=author&query=Blancard%2C+B+R">Bruno R&#xe9;galdo-Saint Blancard</a>, 
<a href="/search/stat?searchtype=author&query=Tesileanu%2C+T">Tiberiu Tesileanu</a>, 
<a href="/search/stat?searchtype=author&query=Cho%2C+K">Kyunghyun Cho</a>, 
<a href="/search/stat?searchtype=author&query=Ho%2C+S">Shirley Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages 7 figures. Supplementary: 5 pages 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models have not yet been broadly adapted for the analysis of
scientific datasets due in part to the unique difficulties of tokenizing
numbers. We propose xVal, a numerical encoding scheme that represents any real
number using just a single token. xVal represents a given real number by
scaling a dedicated embedding vector by the number value. Combined with a
modified number-inference approach, this strategy renders the model end-to-end
continuous when considered as a map from the numbers of the input string to
those of the output string. This leads to an inductive bias that is generally
more suitable for applications in scientific domains. We empirically evaluate
our proposal on a number of synthetic and real-world datasets. Compared with
existing number encoding schemes, we find that xVal is more token-efficient and
demonstrates improved generalization.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03018" title="Abstract">arXiv:2310.03018</a> (cross-list from eess.AS) [<a href="/pdf/2310.03018" title="Download PDF">pdf</a>, <a href="/format/2310.03018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero Resource Code-switched Speech Benchmark Using Speech Utterance  Pairs For Multiple Spoken Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+K">Kuan-Po Huang</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+C">Chih-Kai Yang</a>, 
<a href="/search/eess?searchtype=author&query=Fu%2C+Y">Yu-Kuan Fu</a>, 
<a href="/search/eess?searchtype=author&query=Dunbar%2C+E">Ewan Dunbar</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
<p class="mathjax">We introduce a new zero resource code-switched speech benchmark designed to
directly assess the code-switching capabilities of self-supervised speech
encoders. We showcase a baseline system of language modeling on discrete units
to demonstrate how the code-switching abilities of speech encoders can be
assessed in a zero-resource manner. Our experiments encompass a variety of
well-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We
examine the impact of pre-training languages and model size on benchmark
performance. Notably, though our results demonstrate that speech encoders with
multilingual pre-training, exemplified by XLSR, outperform monolingual variants
(Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial
room for improvement in their code-switching linguistic abilities.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03024" title="Abstract">arXiv:2310.03024</a> (cross-list from astro-ph.IM) [<a href="/pdf/2310.03024" title="Download PDF">pdf</a>, <a href="/format/2310.03024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AstroCLIP: Cross-Modal Pre-Training for Astronomical Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Lanusse%2C+F">Francois Lanusse</a>, 
<a href="/search/astro-ph?searchtype=author&query=Parker%2C+L">Liam Parker</a>, 
<a href="/search/astro-ph?searchtype=author&query=Golkar%2C+S">Siavash Golkar</a>, 
<a href="/search/astro-ph?searchtype=author&query=Cranmer%2C+M">Miles Cranmer</a>, 
<a href="/search/astro-ph?searchtype=author&query=Bietti%2C+A">Alberto Bietti</a>, 
<a href="/search/astro-ph?searchtype=author&query=Eickenberg%2C+M">Michael Eickenberg</a>, 
<a href="/search/astro-ph?searchtype=author&query=Krawezik%2C+G">Geraud Krawezik</a>, 
<a href="/search/astro-ph?searchtype=author&query=McCabe%2C+M">Michael McCabe</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ohana%2C+R">Ruben Ohana</a>, 
<a href="/search/astro-ph?searchtype=author&query=Pettee%2C+M">Mariel Pettee</a>, 
<a href="/search/astro-ph?searchtype=author&query=Blancard%2C+B+R">Bruno Regaldo-Saint Blancard</a>, 
<a href="/search/astro-ph?searchtype=author&query=Tesileanu%2C+T">Tiberiu Tesileanu</a>, 
<a href="/search/astro-ph?searchtype=author&query=Cho%2C+K">Kyunghyun Cho</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ho%2C+S">Shirley Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the NeurIPS 2023 AI4Science Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present AstroCLIP, a strategy to facilitate the construction of
astronomical foundation models that bridge the gap between diverse
observational modalities. We demonstrate that a cross-modal contrastive
learning approach between images and optical spectra of galaxies yields highly
informative embeddings of both modalities. In particular, we apply our method
on multi-band images and optical spectra from the Dark Energy Spectroscopic
Instrument (DESI), and show that: (1) these embeddings are well-aligned between
modalities and can be used for accurate cross-modal searches, and (2) these
embeddings encode valuable physical information about the galaxies -- in
particular redshift and stellar mass -- that can be used to achieve competitive
zero- and few- shot predictions without further finetuning. Additionally, in
the process of developing our approach, we also construct a novel,
transformer-based model and pretraining approach for processing galaxy spectra.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Thu,  5 Oct 23</h3>
<dl>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1911.02903" title="Abstract">arXiv:1911.02903</a> (replaced) [<a href="/pdf/1911.02903" title="Download PDF">pdf</a>, <a href="/format/1911.02903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Implicit Regularization of ReLU Neural Networks Characterizes the  Learned Function -- Part I: the 1-D Case of Two Layers with Random First  Layer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heiss%2C+J">Jakob Heiss</a>, 
<a href="/search/cs?searchtype=author&query=Teichmann%2C+J">Josef Teichmann</a>, 
<a href="/search/cs?searchtype=author&query=Wutte%2C+H">Hanna Wutte</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> adding Appendix C for more intuition, fixing typos, improving formulations, (moving end of Section 3.1 into Appendix B)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2001.09576" title="Abstract">arXiv:2001.09576</a> (replaced) [<a href="/pdf/2001.09576" title="Download PDF">pdf</a>, <a href="/format/2001.09576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Naive Exploration is Optimal for Online LQR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Simchowitz%2C+M">Max Simchowitz</a>, 
<a href="/search/cs?searchtype=author&query=Foster%2C+D+J">Dylan J. Foster</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.08181" title="Abstract">arXiv:2101.08181</a> (replaced) [<a href="/pdf/2101.08181" title="Download PDF">pdf</a>, <a href="/ps/2101.08181" title="Download PostScript">ps</a>, <a href="/format/2101.08181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Asynchronous Session Subtyping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bravetti%2C+M">Mario Bravetti</a>, 
<a href="/search/cs?searchtype=author&query=Lange%2C+J">Julien Lange</a>, 
<a href="/search/cs?searchtype=author&query=Zavattaro%2C+G">Gianluigi Zavattaro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.12294" title="Abstract">arXiv:2104.12294</a> (replaced) [<a href="/pdf/2104.12294" title="Download PDF">pdf</a>, <a href="/format/2104.12294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wise-SrNet: A Novel Architecture for Enhancing Image Classification by  Learning Spatial Resolution of Feature Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahimzadeh%2C+M">Mohammad Rahimzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Askari%2C+A">AmirAli Askari</a>, 
<a href="/search/cs?searchtype=author&query=Parvin%2C+S">Soroush Parvin</a>, 
<a href="/search/cs?searchtype=author&query=Safi%2C+E">Elnaz Safi</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+M+R">Mohammad Reza Mohammadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The code is shared at <a href="https://github.com/mr7495/image-classification-spatial">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.05455" title="Abstract">arXiv:2106.05455</a> (replaced) [<a href="/pdf/2106.05455" title="Download PDF">pdf</a>, <a href="/format/2106.05455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AKE-GNN: Effective Graph Learning with Adaptive Knowledge Exchange
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+L">Liang Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zijun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yanqiao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jian Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06082" title="Abstract">arXiv:2106.06082</a> (replaced) [<a href="/pdf/2106.06082" title="Download PDF">pdf</a>, <a href="/format/2106.06082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One Sense per Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hauer%2C+B">Bradley Hauer</a>, 
<a href="/search/cs?searchtype=author&query=Kondrak%2C+G">Grzegorz Kondrak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published at IJCNLP-AACL 2023: The 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.05641" title="Abstract">arXiv:2108.05641</a> (replaced) [<a href="/pdf/2108.05641" title="Download PDF">pdf</a>, <a href="/format/2108.05641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SR-HetGNN:Session-based Recommendation with Heterogeneous Graph Neural  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinpeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haiyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xudong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Senzhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+K">Kaimin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiaqi Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.11298" title="Abstract">arXiv:2108.11298</a> (replaced) [<a href="/pdf/2108.11298" title="Download PDF">pdf</a>, <a href="/format/2108.11298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven system analysis of nonlinear systems using polynomial  approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Martin%2C+T">Tim Martin</a>, 
<a href="/search/eess?searchtype=author&query=Allg%C3%B6wer%2C+F">Frank Allg&#xf6;wer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.10984" title="Abstract">arXiv:2110.10984</a> (replaced) [<a href="/pdf/2110.10984" title="Download PDF">pdf</a>, <a href="/format/2110.10984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The popular assignment problem: when cardinality is more important than  popularity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kavitha%2C+T">Telikepalli Kavitha</a>, 
<a href="/search/cs?searchtype=author&query=Kir%C3%A1ly%2C+T">Tam&#xe1;s Kir&#xe1;ly</a>, 
<a href="/search/cs?searchtype=author&query=Matuschke%2C+J">Jannik Matuschke</a>, 
<a href="/search/cs?searchtype=author&query=Schlotter%2C+I">Ildik&#xf3; Schlotter</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt-Kraepelin%2C+U">Ulrike Schmidt-Kraepelin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preliminary version appeared in Proc. of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2022), SIAM, pp. 103-123, 2022. The paper now contains Subsections 4.1 and 4.2, an addition to the previous version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.14053" title="Abstract">arXiv:2110.14053</a> (replaced) [<a href="/pdf/2110.14053" title="Download PDF">pdf</a>, <a href="/format/2110.14053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+M">Mohit Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Khurshid%2C+S">Sarfraz Khurshid</a>, 
<a href="/search/cs?searchtype=author&query=McMillan%2C+K">Kenneth McMillan</a>, 
<a href="/search/cs?searchtype=author&query=Miikkulainen%2C+R">Risto Miikkulainen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.00510" title="Abstract">arXiv:2112.00510</a> (replaced) [<a href="/pdf/2112.00510" title="Download PDF">pdf</a>, <a href="/format/2112.00510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trimap-guided Feature Mining and Fusion Network for Natural Image  Matting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Weihao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dongdong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhaozhi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaoyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zehuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hongtao Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Computer Vision and Image Understanding
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.10578" title="Abstract">arXiv:2112.10578</a> (replaced) [<a href="/pdf/2112.10578" title="Download PDF">pdf</a>, <a href="/ps/2112.10578" title="Download PostScript">ps</a>, <a href="/format/2112.10578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Robot Self-Localization to Global-Localization: An RSSI Based  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lentzas%2C+A">Athanasios Lentzas</a> (School of Informatics, Aristotle University of Thessaloniki), 
<a href="/search/cs?searchtype=author&query=Vrakas%2C+D">Dimitris Vrakas</a> (School of Informatics, Aristotle University of Thessaloniki)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings AREA 2023, <a href="/abs/2310.00333">arXiv:2310.00333</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 391, 2023, pp. 18-25
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.13259" title="Abstract">arXiv:2201.13259</a> (replaced) [<a href="/pdf/2201.13259" title="Download PDF">pdf</a>, <a href="/format/2201.13259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trajectory balance: Improved credit assignment in GFlowNets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malkin%2C+N">Nikolay Malkin</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+M">Moksh Jain</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+E">Emmanuel Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2022; see footnotes for code; v3 fixes minor errata
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.01069" title="Abstract">arXiv:2202.01069</a> (replaced) [<a href="/pdf/2202.01069" title="Download PDF">pdf</a>, <a href="/format/2202.01069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image-based Navigation in Real-World Environments via Multiple Mid-level  Representations: Fusion Models, Benchmark and Efficient Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rosano%2C+M">Marco Rosano</a>, 
<a href="/search/cs?searchtype=author&query=Furnari%2C+A">Antonino Furnari</a>, 
<a href="/search/cs?searchtype=author&query=Gulino%2C+L">Luigi Gulino</a>, 
<a href="/search/cs?searchtype=author&query=Santoro%2C+C">Corrado Santoro</a>, 
<a href="/search/cs?searchtype=author&query=Farinella%2C+G+M">Giovanni Maria Farinella</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted for submission in Autonomous Robots
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.04060" title="Abstract">arXiv:2202.04060</a> (replaced) [<a href="/pdf/2202.04060" title="Download PDF">pdf</a>, <a href="/ps/2202.04060" title="Download PostScript">ps</a>, <a href="/format/2202.04060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Streaming word problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lohrey%2C+M">Markus Lohrey</a>, 
<a href="/search/math?searchtype=author&query=L%C3%BCck%2C+L">Lukas L&#xfc;ck</a>, 
<a href="/search/math?searchtype=author&query=Xochitemol%2C+J">Julio Xochitemol</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2101.06132">arXiv:2101.06132</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Group Theory (math.GR)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.06544" title="Abstract">arXiv:2202.06544</a> (replaced) [<a href="/pdf/2202.06544" title="Download PDF">pdf</a>, <a href="/ps/2202.06544" title="Download PostScript">ps</a>, <a href="/format/2202.06544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact SOHS decompositions of trigonometric univariate polynomials with  Gaussian coefficients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Magron%2C+V">Victor Magron</a>, 
<a href="/search/cs?searchtype=author&query=Din%2C+M+S+E">Mohab Safey El Din</a>, 
<a href="/search/cs?searchtype=author&query=Schweighofer%2C+M">Markus Schweighofer</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+T+H">Trung Hieu Vu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.03461" title="Abstract">arXiv:2203.03461</a> (replaced) [<a href="/pdf/2203.03461" title="Download PDF">pdf</a>, <a href="/format/2203.03461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Path Weight Sampling: Exact Monte Carlo Computation of the Mutual  Information between Stochastic Trajectories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Reinhardt%2C+M">Manuel Reinhardt</a>, 
<a href="/search/q-bio?searchtype=author&query=Tka%C4%8Dik%2C+G">Ga&#x161;per Tka&#x10d;ik</a>, 
<a href="/search/q-bio?searchtype=author&query=Wolde%2C+P+R+t">Pieter Rein ten Wolde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages (+ 14 pages appendix), 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Molecular Networks (q-bio.MN)</span>; Soft Condensed Matter (cond-mat.soft); Information Theory (cs.IT); Biological Physics (physics.bio-ph)

</div>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.09121" title="Abstract">arXiv:2205.09121</a> (replaced) [<a href="/pdf/2205.09121" title="Download PDF">pdf</a>, <a href="/ps/2205.09121" title="Download PostScript">ps</a>, <a href="/format/2205.09121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the efficiency of Stochastic Quasi-Newton Methods for Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yousefi%2C+M">Mahsa Yousefi</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+A">Angeles Martinez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.10159" title="Abstract">arXiv:2205.10159</a> (replaced) [<a href="/pdf/2205.10159" title="Download PDF">pdf</a>, <a href="/format/2205.10159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Getting a-Round Guarantees: Floating-Point Attacks on Certified  Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+J">Jiankai Jin</a>, 
<a href="/search/cs?searchtype=author&query=Ohrimenko%2C+O">Olga Ohrimenko</a>, 
<a href="/search/cs?searchtype=author&query=Rubinstein%2C+B+I+P">Benjamin I. P. Rubinstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.00645" title="Abstract">arXiv:2206.00645</a> (replaced) [<a href="/pdf/2206.00645" title="Download PDF">pdf</a>, <a href="/format/2206.00645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Floorplan Restoration by Structure Hallucinating Transformer Cascades
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+S">Sepidehsadat Hosseini</a>, 
<a href="/search/cs?searchtype=author&query=Furukawa%2C+Y">Yasutaka Furukawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at BMVC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.03776" title="Abstract">arXiv:2206.03776</a> (replaced) [<a href="/pdf/2206.03776" title="Download PDF">pdf</a>, <a href="/ps/2206.03776" title="Download PostScript">ps</a>, <a href="/format/2206.03776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Throughput Secure Multiparty Computation with an Honest Majority in  Various Network Settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harth-Kitzerow%2C+C">Christopher Harth-Kitzerow</a>, 
<a href="/search/cs?searchtype=author&query=Carcle%2C+G">Georg Carcle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.03851" title="Abstract">arXiv:2206.03851</a> (replaced) [<a href="/pdf/2206.03851" title="Download PDF">pdf</a>, <a href="/format/2206.03851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconsidering Learning Objectives in Unbiased Recommendation with  Unobserved Confounders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Teng Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> KDD2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.07136" title="Abstract">arXiv:2206.07136</a> (replaced) [<a href="/pdf/2206.07136" title="Download PDF">pdf</a>, <a href="/format/2206.07136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Clipping: Differentially Private Deep Learning Made Easier and  Stronger
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bu%2C+Z">Zhiqi Bu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+S">Sheng Zha</a>, 
<a href="/search/cs?searchtype=author&query=Karypis%2C+G">George Karypis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.11492" title="Abstract">arXiv:2206.11492</a> (replaced) [<a href="/pdf/2206.11492" title="Download PDF">pdf</a>, <a href="/format/2206.11492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradual Domain Adaptation via Normalizing Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sagawa%2C+S">Shogo Sagawa</a>, 
<a href="/search/stat?searchtype=author&query=Hino%2C+H">Hideitsu Hino</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.10574" title="Abstract">arXiv:2207.10574</a> (replaced) [<a href="/pdf/2207.10574" title="Download PDF">pdf</a>, <a href="/format/2207.10574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co-Located Human-Human Interaction Analysis using Nonverbal Cues: A  Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beyan%2C+C">Cigdem Beyan</a>, 
<a href="/search/cs?searchtype=author&query=Vinciarelli%2C+A">Alessandro Vinciarelli</a>, 
<a href="/search/cs?searchtype=author&query=Del+Bue%2C+A">Alessio Del Bue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the author's version of the work. It is posted here for your personal use. Not for redistribution. The definitive version was published in ACM Computing Surveys, <a href="https://doi.org/10.1145/3626516">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.05950" title="Abstract">arXiv:2208.05950</a> (replaced) [<a href="/pdf/2208.05950" title="Download PDF">pdf</a>, <a href="/format/2208.05950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactive Code Generation via Test-Driven User-Intent Formalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lahiri%2C+S+K">Shuvendu K. Lahiri</a>, 
<a href="/search/cs?searchtype=author&query=Fakhoury%2C+S">Sarah Fakhoury</a>, 
<a href="/search/cs?searchtype=author&query=Naik%2C+A">Aaditya Naik</a>, 
<a href="/search/cs?searchtype=author&query=Sakkas%2C+G">Georgios Sakkas</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Saikat Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Musuvathi%2C+M">Madanlal Musuvathi</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+P">Piali Choudhury</a>, 
<a href="/search/cs?searchtype=author&query=von+Veh%2C+C">Curtis von Veh</a>, 
<a href="/search/cs?searchtype=author&query=Inala%2C+J+P">Jeevana Priya Inala</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenglong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.10317" title="Abstract">arXiv:2208.10317</a> (replaced) [<a href="/pdf/2208.10317" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Neural Stochastic Differential Equations for Change Point  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ryzhikov%2C+A">Artem Ryzhikov</a>, 
<a href="/search/cs?searchtype=author&query=Hushchyn%2C+M">Mikhail Hushchyn</a>, 
<a href="/search/cs?searchtype=author&query=Derkach%2C+D">Denis Derkach</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> in IEEE Access, vol. 11, pp. 104700-104711, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.12000" title="Abstract">arXiv:2208.12000</a> (replaced) [<a href="/pdf/2208.12000" title="Download PDF">pdf</a>, <a href="/format/2208.12000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven Predictive Tracking Control based on Koopman Operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Ye Wang</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+Y">Yujia Yang</a>, 
<a href="/search/eess?searchtype=author&query=Pu%2C+Y">Ye Pu</a>, 
<a href="/search/eess?searchtype=author&query=Manzie%2C+C">Chris Manzie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.00517" title="Abstract">arXiv:2209.00517</a> (replaced) [<a href="/pdf/2209.00517" title="Download PDF">pdf</a>, <a href="/format/2209.00517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Neural Process Family: Survey, Applications and Perspectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Saurav Jha</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+D">Dong Gong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuesong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Turner%2C+R+E">Richard E. Turner</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Lina Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.01774" title="Abstract">arXiv:2209.01774</a> (replaced) [<a href="/pdf/2209.01774" title="Download PDF">pdf</a>, <a href="/format/2209.01774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ElasticROS: An Elastically Collaborative Robot Operation System for Fog  and Cloud Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Boyi Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.09134" title="Abstract">arXiv:2209.09134</a> (replaced) [<a href="/pdf/2209.09134" title="Download PDF">pdf</a>, <a href="/format/2209.09134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safety Index Synthesis via Sum-of-Squares Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weiye Zhao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tairan He</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+T">Tianhao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Simin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changliu Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.12324" title="Abstract">arXiv:2209.12324</a> (replaced) [<a href="/pdf/2209.12324" title="Download PDF">pdf</a>, <a href="/format/2209.12324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matching Queues with Abandonments in Quantum Switches: Stability and  Throughput Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zubeldia%2C+M">Martin Zubeldia</a>, 
<a href="/search/cs?searchtype=author&query=Jhunjhunwala%2C+P+R">Prakirt R. Jhunjhunwala</a>, 
<a href="/search/cs?searchtype=author&query=Maguluri%2C+S+T">Siva Theja Maguluri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.14827" title="Abstract">arXiv:2209.14827</a> (replaced) [<a href="/pdf/2209.14827" title="Download PDF">pdf</a>, <a href="/format/2209.14827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Convergence of AdaGrad(Norm) on $\R^{d}$: Beyond Convexity,  Non-Asymptotic Rate and Acceleration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zijian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+D">Ta Duy Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Ene%2C+A">Alina Ene</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H+L">Huy L. Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated manuscript from ICLR 2023 with fixed typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01176" title="Abstract">arXiv:2210.01176</a> (replaced) [<a href="/pdf/2210.01176" title="Download PDF">pdf</a>, <a href="/format/2210.01176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PersA-FL: Personalized Asynchronous Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toghani%2C+M+T">Mohammad Taha Toghani</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Soomin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Uribe%2C+C+A">C&#xe9;sar A. Uribe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.01804" title="Abstract">arXiv:2211.01804</a> (replaced) [<a href="/pdf/2211.01804" title="Download PDF">pdf</a>, <a href="/format/2211.01804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wasserstein Steepest Descent Flows of Discrepancies with Riesz Kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hertrich%2C+J">Johannes Hertrich</a>, 
<a href="/search/math?searchtype=author&query=Gr%C3%A4f%2C+M">Manuel Gr&#xe4;f</a>, 
<a href="/search/math?searchtype=author&query=Beinert%2C+R">Robert Beinert</a>, 
<a href="/search/math?searchtype=author&query=Steidl%2C+G">Gabriele Steidl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11896" title="Abstract">arXiv:2211.11896</a> (replaced) [<a href="/pdf/2211.11896" title="Download PDF">pdf</a>, <a href="/format/2211.11896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private Ad Modeling with DP-SGD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Denison%2C+C">Carson Denison</a>, 
<a href="/search/cs?searchtype=author&query=Ghazi%2C+B">Badih Ghazi</a>, 
<a href="/search/cs?searchtype=author&query=Kamath%2C+P">Pritish Kamath</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Ravi Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Manurangsi%2C+P">Pasin Manurangsi</a>, 
<a href="/search/cs?searchtype=author&query=Narra%2C+K+G">Krishna Giri Narra</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+A">Amer Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Varadarajan%2C+A+V">Avinash V Varadarajan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chiyuan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AdKDD 2023, 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13024" title="Abstract">arXiv:2211.13024</a> (replaced) [<a href="/pdf/2211.13024" title="Download PDF">pdf</a>, <a href="/format/2211.13024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparison of Motion Encoding Frameworks on Human Manipulation Actions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jahn%2C+L">Lennart Jahn</a>, 
<a href="/search/cs?searchtype=author&query=W%C3%B6rg%C3%B6tter%2C+F">Florentin W&#xf6;rg&#xf6;tter</a>, 
<a href="/search/cs?searchtype=author&query=Kulvicius%2C+T">Tomas Kulvicius</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13785" title="Abstract">arXiv:2211.13785</a> (replaced) [<a href="/pdf/2211.13785" title="Download PDF">pdf</a>, <a href="/format/2211.13785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PuzzleFusion: Unleashing the Power of Diffusion Models for Spatial  Puzzle Solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+S">Sepidehsadat Hosseini</a>, 
<a href="/search/cs?searchtype=author&query=Shabani%2C+M+A">Mohammad Amin Shabani</a>, 
<a href="/search/cs?searchtype=author&query=Irandoust%2C+S">Saghar Irandoust</a>, 
<a href="/search/cs?searchtype=author&query=Furukawa%2C+Y">Yasutaka Furukawa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13976" title="Abstract">arXiv:2211.13976</a> (replaced) [<a href="/pdf/2211.13976" title="Download PDF">pdf</a>, <a href="/format/2211.13976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expanding Small-Scale Datasets with Guided Imagination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Daquan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiashi Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. Source code: <a href="https://github.com/Vanint/DatasetExpansion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.14118" title="Abstract">arXiv:2211.14118</a> (replaced) [<a href="/pdf/2211.14118" title="Download PDF">pdf</a>, <a href="/format/2211.14118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MS-PS: A Multi-Scale Network for Photometric Stereo With a New  Comprehensive Training Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hardy%2C+C">Cl&#xe9;ment Hardy</a>, 
<a href="/search/cs?searchtype=author&query=Qu%C3%A9au%2C+Y">Yvain Qu&#xe9;au</a>, 
<a href="/search/cs?searchtype=author&query=Tschumperl%C3%A9%2C+D">David Tschumperl&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.03403" title="Abstract">arXiv:2301.03403</a> (replaced) [<a href="/pdf/2301.03403" title="Download PDF">pdf</a>, <a href="/ps/2301.03403" title="Download PostScript">ps</a>, <a href="/format/2301.03403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A comprehensive review of automatic text summarization techniques:  method, data, evaluation and coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cajueiro%2C+D+O">Daniel O. Cajueiro</a>, 
<a href="/search/cs?searchtype=author&query=Nery%2C+A+G">Arthur G. Nery</a>, 
<a href="/search/cs?searchtype=author&query=Tavares%2C+I">Igor Tavares</a>, 
<a href="/search/cs?searchtype=author&query=De+Melo%2C+M+K">Ma&#xed;sa K. De Melo</a>, 
<a href="/search/cs?searchtype=author&query=Reis%2C+S+A+d">Silvia A. dos Reis</a>, 
<a href="/search/cs?searchtype=author&query=Weigang%2C+L">Li Weigang</a>, 
<a href="/search/cs?searchtype=author&query=Celestino%2C+V+R+R">Victor R. R. Celestino</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.05109" title="Abstract">arXiv:2301.05109</a> (replaced) [<a href="/pdf/2301.05109" title="Download PDF">pdf</a>, <a href="/format/2301.05109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining $\mathcal{ELH}$ Concept Descriptions through Counterfactual  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sieger%2C+L+N">Leonie Nora Sieger</a>, 
<a href="/search/cs?searchtype=author&query=Heindorf%2C+S">Stefan Heindorf</a>, 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+Y">Yasir Mahmood</a>, 
<a href="/search/cs?searchtype=author&query=Bl%C3%BCbaum%2C+L">Lukas Bl&#xfc;baum</a>, 
<a href="/search/cs?searchtype=author&query=Ngomo%2C+A+N">Axel-Cyrille Ngonga Ngomo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13310" title="Abstract">arXiv:2301.13310</a> (replaced) [<a href="/pdf/2301.13310" title="Download PDF">pdf</a>, <a href="/format/2301.13310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alternating Updates for Efficient Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baykal%2C+C">Cenk Baykal</a>, 
<a href="/search/cs?searchtype=author&query=Cutler%2C+D">Dylan Cutler</a>, 
<a href="/search/cs?searchtype=author&query=Dikkala%2C+N">Nishanth Dikkala</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+N">Nikhil Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Panigrahy%2C+R">Rina Panigrahy</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01312" title="Abstract">arXiv:2302.01312</a> (replaced) [<a href="/pdf/2302.01312" title="Download PDF">pdf</a>, <a href="/format/2302.01312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Normalizing Flow Ensembles for Rich Aleatoric and Epistemic Uncertainty  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berry%2C+L">Lucas Berry</a>, 
<a href="/search/cs?searchtype=author&query=Meger%2C+D">David Meger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI 2023 (<a href="https://ojs.aaai.org/index.php/AAAI/article/view/25834">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02173" title="Abstract">arXiv:2302.02173</a> (replaced) [<a href="/pdf/2302.02173" title="Download PDF">pdf</a>, <a href="/format/2302.02173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Time Series Analysis with Fourier Transform: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+K">Kun Yi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shoujin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+G">Guodong Long</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Hui He</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Zhendong Niu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04511" title="Abstract">arXiv:2302.04511</a> (replaced) [<a href="/pdf/2302.04511" title="Download PDF">pdf</a>, <a href="/format/2302.04511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Large-Scale Analysis of Persian Tweets Regarding Covid-19 Vaccination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=ShabaniMirzaei%2C+T">Taha ShabaniMirzaei</a>, 
<a href="/search/cs?searchtype=author&query=Chamani%2C+H">Houmaan Chamani</a>, 
<a href="/search/cs?searchtype=author&query=Abaskohi%2C+A">Amirhossein Abaskohi</a>, 
<a href="/search/cs?searchtype=author&query=Zadeh%2C+Z+S+H">Zhivar Sourati Hassan Zadeh</a>, 
<a href="/search/cs?searchtype=author&query=Bahrak%2C+B">Behnam Bahrak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04519" title="Abstract">arXiv:2302.04519</a> (replaced) [<a href="/pdf/2302.04519" title="Download PDF">pdf</a>, <a href="/format/2302.04519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RayNet: A Simulation Platform for Developing Reinforcement  Learning-Driven Network Protocols
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giacomoni%2C+L">Luca Giacomoni</a>, 
<a href="/search/cs?searchtype=author&query=Benny%2C+B">Basil Benny</a>, 
<a href="/search/cs?searchtype=author&query=Parisis%2C+G">George Parisis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04734" title="Abstract">arXiv:2302.04734</a> (replaced) [<a href="/pdf/2302.04734" title="Download PDF">pdf</a>, <a href="/format/2302.04734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pricing cyber-insurance for systems via maturity models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Skeoch%2C+H">Henry Skeoch</a>, 
<a href="/search/econ?searchtype=author&query=Pym%2C+D">David Pym</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 11 figures, 12 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05575" title="Abstract">arXiv:2302.05575</a> (replaced) [<a href="/pdf/2302.05575" title="Download PDF">pdf</a>, <a href="/format/2302.05575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compositional Algorithms on Compositional Data: Deciding Sheaves on  Presheaves
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Althaus%2C+E">Ernst Althaus</a>, 
<a href="/search/cs?searchtype=author&query=Bumpus%2C+B+M">Benjamin Merlin Bumpus</a>, 
<a href="/search/cs?searchtype=author&query=Fairbanks%2C+J">James Fairbanks</a>, 
<a href="/search/cs?searchtype=author&query=Rosiak%2C+D">Daniel Rosiak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revised and simplified notation and improved exposition. The companion code can be found here: <a href="https://github.com/AlgebraicJulia/StructuredDecompositions.jl">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Combinatorics (math.CO); Category Theory (math.CT)

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05624" title="Abstract">arXiv:2302.05624</a> (replaced) [<a href="/pdf/2302.05624" title="Download PDF">pdf</a>, <a href="/format/2302.05624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel approach to generate datasets with XAI ground truth to evaluate  image models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mir%C3%B3-Nicolau%2C+M">Miquel Mir&#xf3;-Nicolau</a>, 
<a href="/search/cs?searchtype=author&query=Jaume-i-Cap%C3%B3%2C+A">Antoni Jaume-i-Cap&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Moy%C3%A0-Alcover%2C+G">Gabriel Moy&#xe0;-Alcover</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07426" title="Abstract">arXiv:2302.07426</a> (replaced) [<a href="/pdf/2302.07426" title="Download PDF">pdf</a>, <a href="/ps/2302.07426" title="Download PostScript">ps</a>, <a href="/format/2302.07426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computational Complexity of Learning Neural Networks: Smoothness and  Degeneracy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daniely%2C+A">Amit Daniely</a>, 
<a href="/search/cs?searchtype=author&query=Srebro%2C+N">Nathan Srebro</a>, 
<a href="/search/cs?searchtype=author&query=Vardi%2C+G">Gal Vardi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Changed the title, and made some other minor modifications. arXiv admin note: text overlap with <a href="/abs/2101.08303">arXiv:2101.08303</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11351" title="Abstract">arXiv:2302.11351</a> (replaced) [<a href="/pdf/2302.11351" title="Download PDF">pdf</a>, <a href="/format/2302.11351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regularised neural networks mimic human insight
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=L%C3%B6we%2C+A+T">Anika T. L&#xf6;we</a>, 
<a href="/search/cs?searchtype=author&query=Touzo%2C+L">L&#xe9;o Touzo</a>, 
<a href="/search/cs?searchtype=author&query=Muhle-Karbe%2C+P+S">Paul S. Muhle-Karbe</a>, 
<a href="/search/cs?searchtype=author&query=Saxe%2C+A+M">Andrew M. Saxe</a>, 
<a href="/search/cs?searchtype=author&query=Summerfield%2C+C">Christopher Summerfield</a>, 
<a href="/search/cs?searchtype=author&query=Schuck%2C+N+W">Nicolas W. Schuck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01590" title="Abstract">arXiv:2303.01590</a> (replaced) [<a href="/pdf/2303.01590" title="Download PDF">pdf</a>, <a href="/format/2303.01590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Technical report: Graph Neural Networks go Grammatical
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piquenot%2C+J">Jason Piquenot</a>, 
<a href="/search/cs?searchtype=author&query=Moscatelli%2C+A">Aldo Moscatelli</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A9rar%2C+M">Maxime B&#xe9;rar</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A9roux%2C+P">Pierre H&#xe9;roux</a>, 
<a href="/search/cs?searchtype=author&query=raveaux%2C+R">Romain raveaux</a>, 
<a href="/search/cs?searchtype=author&query=Ramel%2C+J">Jean-Yves Ramel</a>, 
<a href="/search/cs?searchtype=author&query=Adam%2C+S">S&#xe9;bastien Adam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04995" title="Abstract">arXiv:2303.04995</a> (replaced) [<a href="/pdf/2303.04995" title="Download PDF">pdf</a>, <a href="/format/2303.04995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-Visual Prompting for Efficient 2D Temporal Video Grounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yimeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinghan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+K">Ke Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the CVPR 2023 and code released (<a href="https://github.com/intel/TVP">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06138" title="Abstract">arXiv:2303.06138</a> (replaced) [<a href="/pdf/2303.06138" title="Download PDF">pdf</a>, <a href="/format/2303.06138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Object-Centric Neural Scattering Functions for Free-Viewpoint  Relighting and Scene Composition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong-Xing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Michelle Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fathi%2C+A">Alireza Fathi</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yen-Yu Chang</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+E+R">Eric Ryan Chan</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+R">Ruohan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Funkhouser%2C+T">Thomas Funkhouser</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiajun Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Journal extension of <a href="/abs/2012.08503">arXiv:2012.08503</a> (TMLR 2023). The first two authors contributed equally to this work. Project page: <a href="https://kovenyu.com/osf/">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions on Machine Learning Research (TMLR), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06419" title="Abstract">arXiv:2303.06419</a> (replaced) [<a href="/pdf/2303.06419" title="Download PDF">pdf</a>, <a href="/format/2303.06419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Use Perturbations when Learning from Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heo%2C+J">Juyeon Heo</a>, 
<a href="/search/cs?searchtype=author&query=Piratla%2C+V">Vihari Piratla</a>, 
<a href="/search/cs?searchtype=author&query=Wicker%2C+M">Matthew Wicker</a>, 
<a href="/search/cs?searchtype=author&query=Weller%2C+A">Adrian Weller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08423" title="Abstract">arXiv:2303.08423</a> (replaced) [<a href="/pdf/2303.08423" title="Download PDF">pdf</a>, <a href="/format/2303.08423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication-Efficient Design for Quantized Decentralized Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Li Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weidong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10358" title="Abstract">arXiv:2303.10358</a> (replaced) [<a href="/pdf/2303.10358" title="Download PDF">pdf</a>, <a href="/ps/2303.10358" title="Download PostScript">ps</a>, <a href="/format/2303.10358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Frailty Machine: Beyond proportional hazard assumption in neural  survival regressions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Ruofan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+J">Jiawei Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Mingzhe Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+M">Ming Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tengfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiqiang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11518" title="Abstract">arXiv:2303.11518</a> (replaced) [<a href="/pdf/2303.11518" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Stability of IMEX Upwind gSBP Schemes for 1D Linear  Advection-Diffusion Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ortleb%2C+S">Sigrun Ortleb</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 4 figures, published version with minor modifications
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Communications on Applied Mathematics and Computation, Publication
  Date: 29 August 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11916" title="Abstract">arXiv:2303.11916</a> (replaced) [<a href="/pdf/2303.11916" title="Download PDF">pdf</a>, <a href="/format/2303.11916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+G">Geonmo Gu</a>, 
<a href="/search/cs?searchtype=author&query=Chun%2C+S">Sanghyuk Chun</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Wonjae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jun%2C+H">HeeJae Jun</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yoohoon Kang</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Sangdoo Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contributed equally; 26 pages, 4.1MB
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12959" title="Abstract">arXiv:2303.12959</a> (replaced) [<a href="/pdf/2303.12959" title="Download PDF">pdf</a>, <a href="/format/2303.12959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variantional autoencoder with decremental information bottleneck for  disentanglement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiantao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+S">Shentong Mo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Awais%2C+M">Muhammad Awais</a>, 
<a href="/search/cs?searchtype=author&query=Atito%2C+S">Sara Atito</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingshen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiang Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13278" title="Abstract">arXiv:2303.13278</a> (replaced) [<a href="/pdf/2303.13278" title="Download PDF">pdf</a>, <a href="/format/2303.13278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Anisotropic Gaussian Filters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Keilmann%2C+A">Alex Keilmann</a>, 
<a href="/search/eess?searchtype=author&query=Godehardt%2C+M">Michael Godehardt</a>, 
<a href="/search/eess?searchtype=author&query=Moghiseh%2C+A">Ali Moghiseh</a>, 
<a href="/search/eess?searchtype=author&query=Redenbach%2C+C">Claudia Redenbach</a>, 
<a href="/search/eess?searchtype=author&query=Schladitz%2C+K">Katja Schladitz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17355" title="Abstract">arXiv:2303.17355</a> (replaced) [<a href="/pdf/2303.17355" title="Download PDF">pdf</a>, <a href="/format/2303.17355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regression and Classification Methods for Learning Sound Wave Amplitude  Modulation in Soft Tactile Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%2C+V+R">Vishnu Rajendran S</a>, 
<a href="/search/cs?searchtype=author&query=Mandil%2C+W">Willow Mandil</a>, 
<a href="/search/cs?searchtype=author&query=Parsons%2C+S">Simon Parsons</a>, 
<a href="/search/cs?searchtype=author&query=E%2C+A+G">Amir Ghalamzan E</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17708" title="Abstract">arXiv:2303.17708</a> (replaced) [<a href="/pdf/2303.17708" title="Download PDF">pdf</a>, <a href="/format/2303.17708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Failures and Risks in Deep Learning Model Converters: A Case  Study in the ONNX Ecosystem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jajal%2C+P">Purvish Jajal</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenxin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Tewari%2C+A">Arav Tewari</a>, 
<a href="/search/cs?searchtype=author&query=Woo%2C+J">Joseph Woo</a>, 
<a href="/search/cs?searchtype=author&query=Thiruvathukal%2C+G+K">George K. Thiruvathukal</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+J+C">James C. Davis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17841" title="Abstract">arXiv:2303.17841</a> (replaced) [<a href="/pdf/2303.17841" title="Download PDF">pdf</a>, <a href="/format/2303.17841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Benchmark Generative Probabilistic Model for Weak Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papadopoulos%2C+G">Georgios Papadopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Silavong%2C+F">Fran Silavong</a>, 
<a href="/search/cs?searchtype=author&query=Moran%2C+S">Sean Moran</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Lecture Notes in Computer Science 2023; vol 14174; Springer; p 36
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05727" title="Abstract">arXiv:2304.05727</a> (replaced) [<a href="/pdf/2304.05727" title="Download PDF">pdf</a>, <a href="/format/2304.05727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Linhardt%2C+L">Lorenz Linhardt</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+K">Klaus-Robert M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Montavon%2C+G">Gr&#xe9;goire Montavon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages + supplement
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08319" title="Abstract">arXiv:2304.08319</a> (replaced) [<a href="/pdf/2304.08319" title="Download PDF">pdf</a>, <a href="/ps/2304.08319" title="Download PostScript">ps</a>, <a href="/format/2304.08319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining Computational Performance of Unsupervised Concept Drift  Detection: A Survey and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Werner%2C+E">Elias Werner</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+N">Nishant Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Lieber%2C+M">Matthias Lieber</a>, 
<a href="/search/cs?searchtype=author&query=Torge%2C+S">Sunna Torge</a>, 
<a href="/search/cs?searchtype=author&query=Gumhold%2C+S">Stefan Gumhold</a>, 
<a href="/search/cs?searchtype=author&query=Nagel%2C+W+E">Wolfgang E. Nagel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10880" title="Abstract">arXiv:2304.10880</a> (replaced) [<a href="/pdf/2304.10880" title="Download PDF">pdf</a>, <a href="/format/2304.10880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Med-Tuning: Parameter-Efficient Transfer Learning with Fine-Grained  Feature Enhancement for Medical Volumetric Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jiachen Shen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J">Jianbo Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shanshan Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiangyun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00406" title="Abstract">arXiv:2305.00406</a> (replaced) [<a href="/pdf/2305.00406" title="Download PDF">pdf</a>, <a href="/format/2305.00406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LIMOT: A Tightly-Coupled System for LiDAR-Inertial Odometry and  Multi-Object Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhongyang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Junqiao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kai Huang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+X">Xuebo Tian</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiaye Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+C">Chen Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures. This updated version mainly refines the experiments. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02135" title="Abstract">arXiv:2305.02135</a> (replaced) [<a href="/pdf/2305.02135" title="Download PDF">pdf</a>, <a href="/format/2305.02135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An identification method for oscillators with response-dependent inertia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Harduf%2C+Y">Yuval Harduf</a> (1), 
<a href="/search/eess?searchtype=author&query=Setter%2C+E">Eyal Setter</a> (1), 
<a href="/search/eess?searchtype=author&query=Bucher%2C+I">Izhak Bucher</a> (1) ((1) Technion Israel Institute of Technology, Faculty of mechanical engineering)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 7 figures, revision submitted to Automatica addressing reviewer's comments
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02299" title="Abstract">arXiv:2305.02299</a> (replaced) [<a href="/pdf/2305.02299" title="Download PDF">pdf</a>, <a href="/format/2305.02299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Sparse Training with Structured Sparsity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lasby%2C+M">Mike Lasby</a>, 
<a href="/search/cs?searchtype=author&query=Golubeva%2C+A">Anna Golubeva</a>, 
<a href="/search/cs?searchtype=author&query=Evci%2C+U">Utku Evci</a>, 
<a href="/search/cs?searchtype=author&query=Nica%2C+M">Mihai Nica</a>, 
<a href="/search/cs?searchtype=author&query=Ioannou%2C+Y">Yani Ioannou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03048" title="Abstract">arXiv:2305.03048</a> (replaced) [<a href="/pdf/2305.03048" title="Download PDF">pdf</a>, <a href="/format/2305.03048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalize Segment Anything Model with One Shot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Renrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhengkai Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Ziyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shilin Yan</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Junting Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xianzheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Peng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongsheng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/ZrrSkywalker/Personalize-SAM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08099" title="Abstract">arXiv:2305.08099</a> (replaced) [<a href="/pdf/2305.08099" title="Download PDF">pdf</a>, <a href="/format/2305.08099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised Neural Factor Analysis for Disentangling Utterance-level  Speech Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weiwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Chenhang He</a>, 
<a href="/search/cs?searchtype=author&query=Mak%2C+M">Man-Wai Mak</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Y">Youzhi Tu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11203" title="Abstract">arXiv:2305.11203</a> (replaced) [<a href="/pdf/2305.11203" title="Download PDF">pdf</a>, <a href="/format/2305.11203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PDP: Parameter-free Differentiable Pruning is All You Need
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+M">Minsik Cho</a>, 
<a href="/search/cs?searchtype=author&query=Adya%2C+S">Saurabh Adya</a>, 
<a href="/search/cs?searchtype=author&query=Naik%2C+D">Devang Naik</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11244" title="Abstract">arXiv:2305.11244</a> (replaced) [<a href="/pdf/2305.11244" title="Download PDF">pdf</a>, <a href="/format/2305.11244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Parameter-Efficient Learning Approach to Arabic Dialect Identification  with Pre-Trained General-Purpose Speech Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Radhakrishnan%2C+S">Srijith Radhakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C+H">Chao-Han Huck Yang</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S+A">Sumeer Ahmad Khan</a>, 
<a href="/search/cs?searchtype=author&query=Kiani%2C+N+A">Narsis A. Kiani</a>, 
<a href="/search/cs?searchtype=author&query=Gomez-Cabrero%2C+D">David Gomez-Cabrero</a>, 
<a href="/search/cs?searchtype=author&query=Tegner%2C+J+N">Jesper N. Tegner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Interspeech 2023, 5 pages. Code is available at: <a href="https://github.com/Srijith-rkr/KAUST-Whisper-Adapter">this https URL</a> under MIT license
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11463" title="Abstract">arXiv:2305.11463</a> (replaced) [<a href="/pdf/2305.11463" title="Download PDF">pdf</a>, <a href="/format/2305.11463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Sliced MMD Flows with Riesz Kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hertrich%2C+J">Johannes Hertrich</a>, 
<a href="/search/cs?searchtype=author&query=Wald%2C+C">Christian Wald</a>, 
<a href="/search/cs?searchtype=author&query=Altekr%C3%BCger%2C+F">Fabian Altekr&#xfc;ger</a>, 
<a href="/search/cs?searchtype=author&query=Hagemann%2C+P">Paul Hagemann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Probability (math.PR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11589" title="Abstract">arXiv:2305.11589</a> (replaced) [<a href="/pdf/2305.11589" title="Download PDF">pdf</a>, <a href="/format/2305.11589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-based DRL Autonomous Driving Agent with Sim2Real Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dianzhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Okhrin%2C+O">Ostap Okhrin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11857" title="Abstract">arXiv:2305.11857</a> (replaced) [<a href="/pdf/2305.11857" title="Download PDF">pdf</a>, <a href="/format/2305.11857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing high-dimensional optimal transport by flow neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xu%2C+C">Chen Xu</a>, 
<a href="/search/stat?searchtype=author&query=Cheng%2C+X">Xiuyuan Cheng</a>, 
<a href="/search/stat?searchtype=author&query=Xie%2C+Y">Yao Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12569" title="Abstract">arXiv:2305.12569</a> (replaced) [<a href="/pdf/2305.12569" title="Download PDF">pdf</a>, <a href="/format/2305.12569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Generative Modeling for High-dimensional Marked Temporal  Point Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dong%2C+Z">Zheng Dong</a>, 
<a href="/search/stat?searchtype=author&query=Fan%2C+Z">Zekai Fan</a>, 
<a href="/search/stat?searchtype=author&query=Zhu%2C+S">Shixiang Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13650" title="Abstract">arXiv:2305.13650</a> (replaced) [<a href="/pdf/2305.13650" title="Download PDF">pdf</a>, <a href="/format/2305.13650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Model-Based Optimization for Challenging Fitness Landscapes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghaffari%2C+S">Saba Ghaffari</a>, 
<a href="/search/cs?searchtype=author&query=Saleh%2C+E">Ehsan Saleh</a>, 
<a href="/search/cs?searchtype=author&query=Schwing%2C+A+G">Alexander G. Schwing</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Burke%2C+M+D">Martin D. Burke</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+S">Saurabh Sinha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14585" title="Abstract">arXiv:2305.14585</a> (replaced) [<a href="/pdf/2305.14585" title="Download PDF">pdf</a>, <a href="/format/2305.14585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faithful and Efficient Explanations for Neural Networks via Neural  Tangent Kernel Surrogate Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Engel%2C+A">Andrew Engel</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Frank%2C+N+S">Natalie S. Frank</a>, 
<a href="/search/cs?searchtype=author&query=Dumitriu%2C+I">Ioana Dumitriu</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Sutanay Choudhury</a>, 
<a href="/search/cs?searchtype=author&query=Sarwate%2C+A">Anand Sarwate</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+T">Tony Chiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated 10/4/2023: significant changes for ICLR2023 submission. Github repository will be live soon. 9 pages, 2 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14777" title="Abstract">arXiv:2305.14777</a> (replaced) [<a href="/pdf/2305.14777" title="Download PDF">pdf</a>, <a href="/format/2305.14777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Modeling through the Semi-dual Formulation of Unbalanced  Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaemoo Choi</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaewoong Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+M">Myungjoo Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14779" title="Abstract">arXiv:2305.14779</a> (replaced) [<a href="/pdf/2305.14779" title="Download PDF">pdf</a>, <a href="/format/2305.14779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alt-Text with Context: Improving Accessibility for Images on Twitter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srivatsan%2C+N">Nikita Srivatsan</a>, 
<a href="/search/cs?searchtype=author&query=Samaniego%2C+S">Sofia Samaniego</a>, 
<a href="/search/cs?searchtype=author&query=Florez%2C+O">Omar Florez</a>, 
<a href="/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T">Taylor Berg-Kirkpatrick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15508" title="Abstract">arXiv:2305.15508</a> (replaced) [<a href="/pdf/2305.15508" title="Download PDF">pdf</a>, <a href="/format/2305.15508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to fix a broken confidence estimator: Evaluating post-hoc methods  for selective classification with deep neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cattelan%2C+L+F+P">Lu&#xed;s Felipe P. Cattelan</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+D">Danilo Silva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15555" title="Abstract">arXiv:2305.15555</a> (replaced) [<a href="/pdf/2305.15555" title="Download PDF">pdf</a>, <a href="/format/2305.15555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning with Plasticity Injection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nikishin%2C+E">Evgenii Nikishin</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+J">Junhyuk Oh</a>, 
<a href="/search/cs?searchtype=author&query=Ostrovski%2C+G">Georg Ostrovski</a>, 
<a href="/search/cs?searchtype=author&query=Lyle%2C+C">Clare Lyle</a>, 
<a href="/search/cs?searchtype=author&query=Pascanu%2C+R">Razvan Pascanu</a>, 
<a href="/search/cs?searchtype=author&query=Dabney%2C+W">Will Dabney</a>, 
<a href="/search/cs?searchtype=author&query=Barreto%2C+A">Andr&#xe9; Barreto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15842" title="Abstract">arXiv:2305.15842</a> (replaced) [<a href="/pdf/2305.15842" title="Download PDF">pdf</a>, <a href="/format/2305.15842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-to-Motion Retrieval: Towards Joint Understanding of Human Motion  Data and Natural Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Messina%2C+N">Nicola Messina</a>, 
<a href="/search/cs?searchtype=author&query=Sedmidubsky%2C+J">Jan Sedmidubsky</a>, 
<a href="/search/cs?searchtype=author&query=Falchi%2C+F">Fabrizio Falchi</a>, 
<a href="/search/cs?searchtype=author&query=Rebok%2C+T">Tom&#xe1;&#x161; Rebok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGIR 2023 (best short paper honorable mention)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16066" title="Abstract">arXiv:2305.16066</a> (replaced) [<a href="/pdf/2305.16066" title="Download PDF">pdf</a>, <a href="/format/2305.16066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guided Attention for Next Active Object @ EGO4D STA Challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thakur%2C+S">Sanket Thakur</a>, 
<a href="/search/cs?searchtype=author&query=Beyan%2C+C">Cigdem Beyan</a>, 
<a href="/search/cs?searchtype=author&query=Morerio%2C+P">Pietro Morerio</a>, 
<a href="/search/cs?searchtype=author&query=Murino%2C+V">Vittorio Murino</a>, 
<a href="/search/cs?searchtype=author&query=Del+Bue%2C+A">Alessio Del Bue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Winner of CVPR@2023 Ego4D STA challenge. arXiv admin note: substantial text overlap with <a href="/abs/2305.12953">arXiv:2305.12953</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16311" title="Abstract">arXiv:2305.16311</a> (replaced) [<a href="/pdf/2305.16311" title="Download PDF">pdf</a>, <a href="/format/2305.16311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Break-A-Scene: Extracting Multiple Concepts from a Single Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Avrahami%2C+O">Omri Avrahami</a>, 
<a href="/search/cs?searchtype=author&query=Aberman%2C+K">Kfir Aberman</a>, 
<a href="/search/cs?searchtype=author&query=Fried%2C+O">Ohad Fried</a>, 
<a href="/search/cs?searchtype=author&query=Cohen-Or%2C+D">Daniel Cohen-Or</a>, 
<a href="/search/cs?searchtype=author&query=Lischinski%2C+D">Dani Lischinski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGGRAPH Asia 2023. Project page: at: <a href="https://omriavrahami.com/break-a-scene/">this https URL</a> Video: <a href="https://www.youtube.com/watch?v=-9EA-BhizgM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17212" title="Abstract">arXiv:2305.17212</a> (replaced) [<a href="/pdf/2305.17212" title="Download PDF">pdf</a>, <a href="/format/2305.17212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rotational Equilibrium: How Weight Decay Balances Learning Across Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kosson%2C+A">Atli Kosson</a>, 
<a href="/search/cs?searchtype=author&query=Messmer%2C+B">Bettina Messmer</a>, 
<a href="/search/cs?searchtype=author&query=Jaggi%2C+M">Martin Jaggi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at <a href="https://github.com/epfml/rotational-optimizers">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17359" title="Abstract">arXiv:2305.17359</a> (replaced) [<a href="/pdf/2305.17359" title="Download PDF">pdf</a>, <a href="/format/2305.17359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of  GPT-Generated Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xianjun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Petzold%2C+L">Linda Petzold</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updates
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17884" title="Abstract">arXiv:2305.17884</a> (replaced) [<a href="/pdf/2305.17884" title="Download PDF">pdf</a>, <a href="/format/2305.17884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Monte Carlo and Tensor-network Methods for Partial  Differential Equations via Sketching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+Y">Yian Chen</a>, 
<a href="/search/math?searchtype=author&query=Khoo%2C+Y">Yuehaw Khoo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18171" title="Abstract">arXiv:2305.18171</a> (replaced) [<a href="/pdf/2305.18171" title="Download PDF">pdf</a>, <a href="/format/2305.18171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Probabilistic Image-Text Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chun%2C+S">Sanghyuk Chun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/naver-ai/pcmepp.">this https URL</a> Project page: <a href="https://naver-ai.github.io/pcmepp/.">this https URL</a> 26 pages, 1.2 MB
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18864" title="Abstract">arXiv:2305.18864</a> (replaced) [<a href="/pdf/2305.18864" title="Download PDF">pdf</a>, <a href="/format/2305.18864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Gradient Langevin Dynamics Based on Quantization with  Increasing Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seok%2C+J">JInwuk Seok</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+C">Changsik Cho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19685" title="Abstract">arXiv:2305.19685</a> (replaced) [<a href="/pdf/2305.19685" title="Download PDF">pdf</a>, <a href="/format/2305.19685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Stochastic Mechanics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Orlova%2C+E">Elena Orlova</a>, 
<a href="/search/cs?searchtype=author&query=Ustimenko%2C+A">Aleksei Ustimenko</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+R">Ruoxi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+P+Y">Peter Y. Lu</a>, 
<a href="/search/cs?searchtype=author&query=Willett%2C+R">Rebecca Willett</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantum Physics (quant-ph); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00978" title="Abstract">arXiv:2306.00978</a> (replaced) [<a href="/pdf/2306.00978" title="Download PDF">pdf</a>, <a href="/format/2306.00978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AWQ: Activation-aware Weight Quantization for LLM Compression and  Acceleration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Ji Lin</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiaming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Haotian Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+X">Xingyu Dang</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+C">Chuang Gan</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Song Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at: <a href="https://github.com/mit-han-lab/llm-awq">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01095" title="Abstract">arXiv:2306.01095</a> (replaced) [<a href="/pdf/2306.01095" title="Download PDF">pdf</a>, <a href="/format/2306.01095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-Batch, Iteration-Efficient Neural Bayesian Design Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ansari%2C+N">Navid Ansari</a>, 
<a href="/search/cs?searchtype=author&query=Seidel%2C+H">Hans-Peter Seidel</a>, 
<a href="/search/cs?searchtype=author&query=Babaei%2C+V">Vahid Babaei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01776" title="Abstract">arXiv:2306.01776</a> (replaced) [<a href="/pdf/2306.01776" title="Download PDF">pdf</a>, <a href="/format/2306.01776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Zero to Turbulence: Generative Modeling for 3D Flow Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Lienen%2C+M">Marten Lienen</a>, 
<a href="/search/physics?searchtype=author&query=L%C3%BCdke%2C+D">David L&#xfc;dke</a>, 
<a href="/search/physics?searchtype=author&query=Hansen-Palmus%2C+J">Jan Hansen-Palmus</a>, 
<a href="/search/physics?searchtype=author&query=G%C3%BCnnemann%2C+S">Stephan G&#xfc;nnemann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03091" title="Abstract">arXiv:2306.03091</a> (replaced) [<a href="/pdf/2306.03091" title="Download PDF">pdf</a>, <a href="/format/2306.03091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Canwen Xu</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03364" title="Abstract">arXiv:2306.03364</a> (replaced) [<a href="/pdf/2306.03364" title="Download PDF">pdf</a>, <a href="/format/2306.03364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Representations on the Unit Sphere: Investigating Angular  Gaussian and von Mises-Fisher Distributions for Online Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michel%2C+N">Nicolas Michel</a>, 
<a href="/search/cs?searchtype=author&query=Chierchia%2C+G">Giovanni Chierchia</a>, 
<a href="/search/cs?searchtype=author&query=Negrel%2C+R">Romain Negrel</a>, 
<a href="/search/cs?searchtype=author&query=Bercher%2C+J">Jean-Fran&#xe7;ois Bercher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03531" title="Abstract">arXiv:2306.03531</a> (replaced) [<a href="/pdf/2306.03531" title="Download PDF">pdf</a>, <a href="/format/2306.03531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Concept-Based System for Local, Global, and Misclassification  Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aghaeipoor%2C+F">Fatemeh Aghaeipoor</a>, 
<a href="/search/cs?searchtype=author&query=Asgarian%2C+D">Dorsa Asgarian</a>, 
<a href="/search/cs?searchtype=author&query=Sabokrou%2C+M">Mohammad Sabokrou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03872" title="Abstract">arXiv:2306.03872</a> (replaced) [<a href="/pdf/2306.03872" title="Download PDF">pdf</a>, <a href="/format/2306.03872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deductive Verification of Chain-of-Thought Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zhan Ling</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yunhao Fang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuanlin Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhiao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Mingu Lee</a>, 
<a href="/search/cs?searchtype=author&query=Memisevic%2C+R">Roland Memisevic</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hao Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05739" title="Abstract">arXiv:2306.05739</a> (replaced) [<a href="/pdf/2306.05739" title="Download PDF">pdf</a>, <a href="/format/2306.05739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leaping through tree space: continuous phylogenetic inference for rooted  and unrooted trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Penn%2C+M+J">Matthew J Penn</a>, 
<a href="/search/q-bio?searchtype=author&query=Scheidwasser%2C+N">Neil Scheidwasser</a>, 
<a href="/search/q-bio?searchtype=author&query=Penn%2C+J">Joseph Penn</a>, 
<a href="/search/q-bio?searchtype=author&query=Donnelly%2C+C+A">Christl A Donnelly</a>, 
<a href="/search/q-bio?searchtype=author&query=Duch%C3%AAne%2C+D+A">David A Duch&#xea;ne</a>, 
<a href="/search/q-bio?searchtype=author&query=Bhatt%2C+S">Samir Bhatt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures, 15 supplementary pages, 3 supplementary figures; overhaul of Methods and Results sections
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Populations and Evolution (q-bio.PE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05880" title="Abstract">arXiv:2306.05880</a> (replaced) [<a href="/pdf/2306.05880" title="Download PDF">pdf</a>, <a href="/format/2306.05880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time Series Continuous Modeling for Imputation and Forecasting with  Implicit Neural Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naour%2C+E+L">Etienne Le Naour</a>, 
<a href="/search/cs?searchtype=author&query=Serrano%2C+L">Louis Serrano</a>, 
<a href="/search/cs?searchtype=author&query=Migus%2C+L">L&#xe9;on Migus</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yuan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Agoua%2C+G">Ghislain Agoua</a>, 
<a href="/search/cs?searchtype=author&query=Baskiotis%2C+N">Nicolas Baskiotis</a>, 
<a href="/search/cs?searchtype=author&query=Gallinari%2C+P">Patrick Gallinari</a>, 
<a href="/search/cs?searchtype=author&query=Guigue%2C+V">Vincent Guigue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05956" title="Abstract">arXiv:2306.05956</a> (replaced) [<a href="/pdf/2306.05956" title="Download PDF">pdf</a>, <a href="/format/2306.05956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;My sex-related data is more sensitive than my financial data and I want  the same level of security and privacy&quot;: User Risk Perceptions and Protective  Actions in Female-oriented Technologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehrnezhad%2C+M">Maryam Mehrnezhad</a>, 
<a href="/search/cs?searchtype=author&query=Almeida%2C+T">Teresa Almeida</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06383" title="Abstract">arXiv:2306.06383</a> (replaced) [<a href="/pdf/2306.06383" title="Download PDF">pdf</a>, <a href="/ps/2306.06383" title="Download PostScript">ps</a>, <a href="/format/2306.06383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using orthogonally structured positive bases for constructing positive  $k$-spanning sets with cosine measure guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hare%2C+W">Warren Hare</a>, 
<a href="/search/math?searchtype=author&query=Jarry-Bolduc%2C+G">Gabriel Jarry-Bolduc</a>, 
<a href="/search/math?searchtype=author&query=Kerleau%2C+S">S&#xe9;bastien Kerleau</a>, 
<a href="/search/math?searchtype=author&query=Royer%2C+C+W">Cl&#xe9;ment W. Royer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06991" title="Abstract">arXiv:2306.06991</a> (replaced) [<a href="/pdf/2306.06991" title="Download PDF">pdf</a>, <a href="/format/2306.06991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zike Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanwang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09222" title="Abstract">arXiv:2306.09222</a> (replaced) [<a href="/pdf/2306.09222" title="Download PDF">pdf</a>, <a href="/format/2306.09222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Re-weighted Gradient Descent via Distributionally Robust  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Ramnath Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Majmundar%2C+K">Kushal Majmundar</a>, 
<a href="/search/cs?searchtype=author&query=Nagaraj%2C+D">Dheeraj Nagaraj</a>, 
<a href="/search/cs?searchtype=author&query=Suggala%2C+A+S">Arun Sai Suggala</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11268" title="Abstract">arXiv:2306.11268</a> (replaced) [<a href="/pdf/2306.11268" title="Download PDF">pdf</a>, <a href="/format/2306.11268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LightRidge: An End-to-end Agile Design Framework for Diffractive Optical  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruiyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+M">Minhan Lou</a>, 
<a href="/search/cs?searchtype=author&query=Sensale-Rodriguez%2C+B">Berardi Sensale-Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Weilu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Cunxi Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 13 figures, Architectural Support for Programming Languages and Operating Systems (ASPLOS'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Optics (physics.optics)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11313" title="Abstract">arXiv:2306.11313</a> (replaced) [<a href="/pdf/2306.11313" title="Download PDF">pdf</a>, <a href="/format/2306.11313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep graph kernel point processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dong%2C+Z">Zheng Dong</a>, 
<a href="/search/stat?searchtype=author&query=Repasky%2C+M">Matthew Repasky</a>, 
<a href="/search/stat?searchtype=author&query=Cheng%2C+X">Xiuyuan Cheng</a>, 
<a href="/search/stat?searchtype=author&query=Xie%2C+Y">Yao Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13196" title="Abstract">arXiv:2306.13196</a> (replaced) [<a href="/pdf/2306.13196" title="Download PDF">pdf</a>, <a href="/format/2306.13196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiMSam: Diffusion Models as Samplers for Task and Motion Planning under  Partial Observability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+X">Xiaolin Fang</a>, 
<a href="/search/cs?searchtype=author&query=Garrett%2C+C+R">Caelan Reed Garrett</a>, 
<a href="/search/cs?searchtype=author&query=Eppner%2C+C">Clemens Eppner</a>, 
<a href="/search/cs?searchtype=author&query=Lozano-P%C3%A9rez%2C+T">Tom&#xe1;s Lozano-P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Kaelbling%2C+L+P">Leslie Pack Kaelbling</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+D">Dieter Fox</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14915" title="Abstract">arXiv:2306.14915</a> (replaced) [<a href="/pdf/2306.14915" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A GPT-4 Reticular Chemist for Guiding MOF Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhiling Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Rong%2C+Z">Zichao Rong</a>, 
<a href="/search/cs?searchtype=author&query=Rampal%2C+N">Nakul Rampal</a>, 
<a href="/search/cs?searchtype=author&query=Borgs%2C+C">Christian Borgs</a>, 
<a href="/search/cs?searchtype=author&query=Chayes%2C+J+T">Jennifer T. Chayes</a>, 
<a href="/search/cs?searchtype=author&query=Yaghi%2C+O+M">Omar M. Yaghi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 173 pages (9-page manuscript and 164 pages of supporting information) Submitted to Angewandte Chemie International Edition
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Materials Science (cond-mat.mtrl-sci); Chemical Physics (physics.chem-ph)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15410" title="Abstract">arXiv:2306.15410</a> (replaced) [<a href="/pdf/2306.15410" title="Download PDF">pdf</a>, <a href="/format/2306.15410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoGraph: Predicting Lane Graphs from Traffic Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Z%C3%BCrn%2C+J">Jannik Z&#xfc;rn</a>, 
<a href="/search/cs?searchtype=author&query=Posner%2C+I">Ingmar Posner</a>, 
<a href="/search/cs?searchtype=author&query=Burgard%2C+W">Wolfram Burgard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15724" title="Abstract">arXiv:2306.15724</a> (replaced) [<a href="/pdf/2306.15724" title="Download PDF">pdf</a>, <a href="/format/2306.15724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REFLECT: Summarizing Robot Experiences for Failure Explanation and  Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zeyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bahety%2C+A">Arpit Bahety</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shuran Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CoRL 2023 camera-ready; Project website: <a href="https://robot-reflect.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15963" title="Abstract">arXiv:2306.15963</a> (replaced) [<a href="/pdf/2306.15963" title="Download PDF">pdf</a>, <a href="/format/2306.15963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fused Gromov-Wasserstein Graph Mixup for Graph-level Classifications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xinyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+X">Xu Chu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yasha Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Junfeng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Liantao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wenwu Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00238" title="Abstract">arXiv:2307.00238</a> (replaced) [<a href="/pdf/2307.00238" title="Download PDF">pdf</a>, <a href="/format/2307.00238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Transfer Learning Models for High-Dimensional Linear Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liu%2C+S+S">Shuo Shuo Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00589" title="Abstract">arXiv:2307.00589</a> (replaced) [<a href="/pdf/2307.00589" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MedCPT: Contrastive Pre-trained Transformers with Large-scale PubMed  Search Logs for Zero-shot Biomedical Information Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Won Kim</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qingyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Comeau%2C+D+C">Donald C. Comeau</a>, 
<a href="/search/cs?searchtype=author&query=Yeganova%2C+L">Lana Yeganova</a>, 
<a href="/search/cs?searchtype=author&query=Wilbur%2C+W+J">W. John Wilbur</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyong Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The MedCPT code and API are available at <a href="https://github.com/ncbi/MedCPT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00604" title="Abstract">arXiv:2307.00604</a> (replaced) [<a href="/pdf/2307.00604" title="Download PDF">pdf</a>, <a href="/format/2307.00604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Listing Small Minimal $s,t$-separators in FPT-Delay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kenig%2C+B">Batya Kenig</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Databases (cs.DB)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01430" title="Abstract">arXiv:2307.01430</a> (replaced) [<a href="/pdf/2307.01430" title="Download PDF">pdf</a>, <a href="/format/2307.01430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual Learning in Open-vocabulary Classification with Complementary  Memory Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+W">Weijie Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Hoiem%2C+D">Derek Hoiem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01918" title="Abstract">arXiv:2307.01918</a> (replaced) [<a href="/pdf/2307.01918" title="Download PDF">pdf</a>, <a href="/format/2307.01918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computational Reproducibility in Computational Social Science
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schoch%2C+D">David Schoch</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+C">Chung-hong Chan</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+C">Claudia Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Bleier%2C+A">Arnim Bleier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v1: Working Paper; v2: fixed missing citation in text; v3: fixed some minor errors and formatting; v4: shortened paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02728" title="Abstract">arXiv:2307.02728</a> (replaced) [<a href="/pdf/2307.02728" title="Download PDF">pdf</a>, <a href="/format/2307.02728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levy%2C+A">Andrew Levy</a>, 
<a href="/search/cs?searchtype=author&query=Rammohan%2C+S">Sreehari Rammohan</a>, 
<a href="/search/cs?searchtype=author&query=Allievi%2C+A">Alessandro Allievi</a>, 
<a href="/search/cs?searchtype=author&query=Niekum%2C+S">Scott Niekum</a>, 
<a href="/search/cs?searchtype=author&query=Konidaris%2C+G">George Konidaris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Additional baseline comparisons
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02863" title="Abstract">arXiv:2307.02863</a> (replaced) [<a href="/pdf/2307.02863" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ValiTex -- a unified validation framework for computational text-based  measures of social science constructs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Birkenmaier%2C+L">Lukas Birkenmaier</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+C">Claudia Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Lechner%2C+C">Clemens Lechner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03488" title="Abstract">arXiv:2307.03488</a> (replaced) [<a href="/pdf/2307.03488" title="Download PDF">pdf</a>, <a href="/format/2307.03488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Line-Constrained $k$-Semi-Obnoxious Facility Location
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singireddy%2C+V+R">Vishwanath R. Singireddy</a>, 
<a href="/search/cs?searchtype=author&query=Basappa%2C+M">Manjanna Basappa</a>, 
<a href="/search/cs?searchtype=author&query=Aravind%2C+N+R">N. R. Aravind</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04139" title="Abstract">arXiv:2307.04139</a> (replaced) [<a href="/pdf/2307.04139" title="Download PDF">pdf</a>, <a href="/ps/2307.04139" title="Download PostScript">ps</a>, <a href="/format/2307.04139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Randomized Algorithm for Single-Source Shortest Path on Undirected  Real-Weighted Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+R">Ran Duan</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Jiayi Mao</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+X">Xinkai Shu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+L">Longhui Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09237" title="Abstract">arXiv:2307.09237</a> (replaced) [<a href="/pdf/2307.09237" title="Download PDF">pdf</a>, <a href="/format/2307.09237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Quick Guide for the Iterated Extended Kalman Filter on Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huai%2C+J">Jianzhu Huai</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+X">Xiang Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages excluding references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10928" title="Abstract">arXiv:2307.10928</a> (replaced) [<a href="/pdf/2307.10928" title="Download PDF">pdf</a>, <a href="/format/2307.10928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLASK: Fine-grained Language Model Evaluation based on Alignment Skill  Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+S">Seonghyeon Ye</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Doyoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungdong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+H">Hyeonbin Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungone Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jo%2C+Y">Yongrae Jo</a>, 
<a href="/search/cs?searchtype=author&query=Thorne%2C+J">James Thorne</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11088" title="Abstract">arXiv:2307.11088</a> (replaced) [<a href="/pdf/2307.11088" title="Download PDF">pdf</a>, <a href="/format/2307.11088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> L-Eval: Instituting Standardized Evaluation for Long Context Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+C">Chenxin An</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shansan Gong</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+M">Ming Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xingjian Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mukai Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11833" title="Abstract">arXiv:2307.11833</a> (replaced) [<a href="/pdf/2307.11833" title="Download PDF">pdf</a>, <a href="/format/2307.11833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhiyuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xueying Ding</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+B+A">B. Aditya Prakash</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages (including 9 pages of main text, 3 pages of references, and 4 pages of appendix), 9 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14619" title="Abstract">arXiv:2307.14619</a> (replaced) [<a href="/pdf/2307.14619" title="Download PDF">pdf</a>, <a href="/format/2307.14619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provable Guarantees for Generative Behavior Cloning: Bridging Low-Level  Stability and High-Level Behavior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Block%2C+A">Adam Block</a>, 
<a href="/search/cs?searchtype=author&query=Pfrommer%2C+D">Daniel Pfrommer</a>, 
<a href="/search/cs?searchtype=author&query=Simchowitz%2C+M">Max Simchowitz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> updated figures, minor notational change for readability
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00258" title="Abstract">arXiv:2308.00258</a> (replaced) [<a href="/pdf/2308.00258" title="Download PDF">pdf</a>, <a href="/format/2308.00258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AQUILA: Communication Efficient Federated Learning with Adaptive  Quantization in Device Selection Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zihao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yuzhu Mao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhenpeng Shi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+T">Tian Lan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wenbo Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao-Ping Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00472" title="Abstract">arXiv:2308.00472</a> (replaced) [<a href="/pdf/2308.00472" title="Download PDF">pdf</a>, <a href="/format/2308.00472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vector Field Based Volume Peeling for Multi-Axis Machining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dutta%2C+N">Neelotpal Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+G">Guoxin Fang</a>, 
<a href="/search/cs?searchtype=author&query=Yigit%2C+I+E">Ismail E. Yigit</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C+C+L">Charlie C.L. Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ASME Journal of Computing and Information Science in Engineering, accepted, October 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01273" title="Abstract">arXiv:2308.01273</a> (replaced) [<a href="/pdf/2308.01273" title="Download PDF">pdf</a>, <a href="/format/2308.01273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Attachments to SEO: Click Here to Learn More about Clickbait PDFs!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stivala%2C+G">Giada Stivala</a>, 
<a href="/search/cs?searchtype=author&query=Abdelnabi%2C+S">Sahar Abdelnabi</a>, 
<a href="/search/cs?searchtype=author&query=Mengascini%2C+A">Andrea Mengascini</a>, 
<a href="/search/cs?searchtype=author&query=Graziano%2C+M">Mariano Graziano</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+M">Mario Fritz</a>, 
<a href="/search/cs?searchtype=author&query=Pellegrino%2C+G">Giancarlo Pellegrino</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Replaced with accepted version. To be published at ACSAC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06887" title="Abstract">arXiv:2308.06887</a> (replaced) [<a href="/pdf/2308.06887" title="Download PDF">pdf</a>, <a href="/format/2308.06887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustified ANNs Reveal Wormholes Between Human Category Percepts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gaziv%2C+G">Guy Gaziv</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M+J">Michael J. Lee</a>, 
<a href="/search/cs?searchtype=author&query=DiCarlo%2C+J+J">James J. DiCarlo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In NeurIPS 2023. Code: <a href="https://github.com/ggaziv/Wormholes">this https URL</a> Project Webpage: <a href="https://himjl.github.io/pwormholes">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> https://neurips.cc/virtual/2023/poster/72812
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07327" title="Abstract">arXiv:2308.07327</a> (replaced) [<a href="/pdf/2308.07327" title="Download PDF">pdf</a>, <a href="/format/2308.07327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant  Poker Game Simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juho Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 figure, submission to IEEE Transactions on Games
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08011" title="Abstract">arXiv:2308.08011</a> (replaced) [<a href="/pdf/2308.08011" title="Download PDF">pdf</a>, <a href="/format/2308.08011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shortcut-V2V: Compression Framework for Video-to-Video Translation based  on Temporal Redundancy Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+C">Chaeyeon Chung</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+Y">Yeojeong Park</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Seunghwan Choi</a>, 
<a href="/search/cs?searchtype=author&query=Ganbat%2C+M">Munkhsoyol Ganbat</a>, 
<a href="/search/cs?searchtype=author&query=Choo%2C+J">Jaegul Choo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08155" title="Abstract">arXiv:2308.08155</a> (replaced) [<a href="/pdf/2308.08155" title="Download PDF">pdf</a>, <a href="/format/2308.08155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingyun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+G">Gagan Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jieyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yiran Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Beibin Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+E">Erkang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Li Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaokun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiale Liu</a>, 
<a href="/search/cs?searchtype=author&query=Awadallah%2C+A+H">Ahmed Hassan Awadallah</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+R+W">Ryen W White</a>, 
<a href="/search/cs?searchtype=author&query=Burger%2C+D">Doug Burger</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages (10 pages for the main text, 3 pages for references, and 30 pages for appendices)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08303" title="Abstract">arXiv:2308.08303</a> (replaced) [<a href="/pdf/2308.08303" title="Download PDF">pdf</a>, <a href="/format/2308.08303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Next-Active Objects for Context-Aware Anticipation in  Egocentric Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thakur%2C+S">Sanket Thakur</a>, 
<a href="/search/cs?searchtype=author&query=Beyan%2C+C">Cigdem Beyan</a>, 
<a href="/search/cs?searchtype=author&query=Morerio%2C+P">Pietro Morerio</a>, 
<a href="/search/cs?searchtype=author&query=Murino%2C+V">Vittorio Murino</a>, 
<a href="/search/cs?searchtype=author&query=Del+Bue%2C+A">Alessio Del Bue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in WACV'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10792" title="Abstract">arXiv:2308.10792</a> (replaced) [<a href="/pdf/2308.10792" title="Download PDF">pdf</a>, <a href="/format/2308.10792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instruction Tuning for Large Language Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shengyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+L">Linfeng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoya Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaofei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Runyi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoyin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A Survey paper, Pre-print
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13111" title="Abstract">arXiv:2308.13111</a> (replaced) [<a href="/pdf/2308.13111" title="Download PDF">pdf</a>, <a href="/format/2308.13111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian low-rank adaptation for large language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+A+X">Adam X. Yang</a>, 
<a href="/search/cs?searchtype=author&query=Robeyns%2C+M">Maxime Robeyns</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Aitchison%2C+L">Laurence Aitchison</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13208" title="Abstract">arXiv:2308.13208</a> (replaced) [<a href="/pdf/2308.13208" title="Download PDF">pdf</a>, <a href="/format/2308.13208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-inspired Equivariant Descriptors of Non-bonded Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Huguenin-Dumittan%2C+K+K">Kevin K. Huguenin-Dumittan</a>, 
<a href="/search/physics?searchtype=author&query=Loche%2C+P">Philip Loche</a>, 
<a href="/search/physics?searchtype=author&query=Haoran%2C+N">Ni Haoran</a>, 
<a href="/search/physics?searchtype=author&query=Ceriotti%2C+M">Michele Ceriotti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16175" title="Abstract">arXiv:2308.16175</a> (replaced) [<a href="/pdf/2308.16175" title="Download PDF">pdf</a>, <a href="/format/2308.16175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying Uncertainty in Answers from any Language Model and Enhancing  their Trustworthiness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiuhai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mueller%2C+J">Jonas Mueller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16586" title="Abstract">arXiv:2308.16586</a> (replaced) [<a href="/pdf/2308.16586" title="Download PDF">pdf</a>, <a href="/format/2308.16586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Represent Patches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xunzhu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+H">Haoye Tian</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenghan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pian%2C+W">Weiguo Pian</a>, 
<a href="/search/cs?searchtype=author&query=Ezzini%2C+S">Saad Ezzini</a>, 
<a href="/search/cs?searchtype=author&query=Kabore%2C+A+K">Abdoul Kader Kabore</a>, 
<a href="/search/cs?searchtype=author&query=Habib%2C+A">Andrew Habib</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+J">Jacques Klein</a>, 
<a href="/search/cs?searchtype=author&query=Bissyande%2C+T+F">Tegawende F. Bissyande</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03006" title="Abstract">arXiv:2309.03006</a> (replaced) [<a href="/pdf/2309.03006" title="Download PDF">pdf</a>, <a href="/format/2309.03006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fuzz on the Beach: Fuzzing Solana Smart Contracts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smolka%2C+S">Sven Smolka</a> (1), 
<a href="/search/cs?searchtype=author&query=Giesen%2C+J">Jens-Rene Giesen</a> (1), 
<a href="/search/cs?searchtype=author&query=Winkler%2C+P">Pascal Winkler</a> (1), 
<a href="/search/cs?searchtype=author&query=Draissi%2C+O">Oussama Draissi</a> (1), 
<a href="/search/cs?searchtype=author&query=Davi%2C+L">Lucas Davi</a> (1), 
<a href="/search/cs?searchtype=author&query=Karame%2C+G">Ghassan Karame</a> (2), 
<a href="/search/cs?searchtype=author&query=Pohl%2C+K">Klaus Pohl</a> (1) ((1) University of Duisburg-Essen, (2) Ruhr University Bochum)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper will appear on the ACM CCS 2023 in November 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04015" title="Abstract">arXiv:2309.04015</a> (replaced) [<a href="/pdf/2309.04015" title="Download PDF">pdf</a>, <a href="/format/2309.04015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Transport with Tempered Exponential Measures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amid%2C+E">Ehsan Amid</a>, 
<a href="/search/cs?searchtype=author&query=Nielsen%2C+F">Frank Nielsen</a>, 
<a href="/search/cs?searchtype=author&query=Nock%2C+R">Richard Nock</a>, 
<a href="/search/cs?searchtype=author&query=Warmuth%2C+M+K">Manfred K. Warmuth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04142" title="Abstract">arXiv:2309.04142</a> (replaced) [<a href="/pdf/2309.04142" title="Download PDF">pdf</a>, <a href="/format/2309.04142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trustworthy and Synergistic Artificial Intelligence for Software  Engineering: Vision and Roadmaps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lo%2C+D">David Lo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is to appear in the post-proceedings of the Future of Software Engineering (FoSE) track of the 45th IEEE/ACM International Conference on Software Engineering (ICSE 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04296" title="Abstract">arXiv:2309.04296</a> (replaced) [<a href="/pdf/2309.04296" title="Download PDF">pdf</a>, <a href="/format/2309.04296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating Out-of-Distribution Electricity Load Forecasting during  COVID-19: Benchmarking energy load forecasting models without and with  continual learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prabowo%2C+A">Arian Prabowo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kaixuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+H">Hao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Sethuvenkatraman%2C+S">Subbu Sethuvenkatraman</a>, 
<a href="/search/cs?searchtype=author&query=Salim%2C+F+D">Flora D. Salim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures, 5 tables, BuildSys '23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05148" title="Abstract">arXiv:2309.05148</a> (replaced) [<a href="/pdf/2309.05148" title="Download PDF">pdf</a>, <a href="/format/2309.05148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Skin Tone: A Multidimensional Measure of Apparent Skin Color
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thong%2C+W">William Thong</a>, 
<a href="/search/cs?searchtype=author&query=Joniak%2C+P">Przemyslaw Joniak</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+A">Alice Xiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the International Conference on Computer Vision (ICCV) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05287" title="Abstract">arXiv:2309.05287</a> (replaced) [<a href="/pdf/2309.05287" title="Download PDF">pdf</a>, <a href="/format/2309.05287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Addressing Feature Imbalance in Sound Source Separation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jaechang Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+J">Jeongyeon Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+S">Soheun Yi</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Jaewoong Cho</a>, 
<a href="/search/cs?searchtype=author&query=Ok%2C+J">Jungseul Ok</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05445" title="Abstract">arXiv:2309.05445</a> (replaced) [<a href="/pdf/2309.05445" title="Download PDF">pdf</a>, <a href="/ps/2309.05445" title="Download PostScript">ps</a>, <a href="/format/2309.05445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Many Cores, Many Models: GPU Programming Model vs. Vendor Compatibility  Overview
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Herten%2C+A">Andreas Herten</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in the proceedings of the P3HPC workshop, hosted at SC23 (International Conference for High Performance Computing, Networking, Storage, and Analysis)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05613" title="Abstract">arXiv:2309.05613</a> (replaced) [<a href="/pdf/2309.05613" title="Download PDF">pdf</a>, <a href="/format/2309.05613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning the Geodesic Embedding with Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+B">Bo Pang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhongtian Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng-Shuai Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGGRAPH Asia 2023, Journal Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06256" title="Abstract">arXiv:2309.06256</a> (replaced) [<a href="/pdf/2309.06256" title="Download PDF">pdf</a>, <a href="/format/2309.06256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speciality vs Generality: An Empirical Study on Catastrophic Forgetting  in Fine-tuning Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+L">Lu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hangyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zeming Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Pi%2C+R">Renjie Pi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+S">Shizhe Diao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Han Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06606" title="Abstract">arXiv:2309.06606</a> (replaced) [<a href="/pdf/2309.06606" title="Download PDF">pdf</a>, <a href="/format/2309.06606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Differentiable Filters Enable Ubiquitous Robot Control  with Smartwatches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weigend%2C+F+C">Fabian C Weigend</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Amor%2C+H+B">Heni Ben Amor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> DiffPropRob Workshop IROS 2023 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06651" title="Abstract">arXiv:2309.06651</a> (replaced) [<a href="/pdf/2309.06651" title="Download PDF">pdf</a>, <a href="/format/2309.06651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConR: Contrastive Regularizer for Deep Imbalanced Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Keramati%2C+M">Mahsa Keramati</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+L">Lili Meng</a>, 
<a href="/search/cs?searchtype=author&query=Evans%2C+R+D">R. David Evans</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06799" title="Abstract">arXiv:2309.06799</a> (replaced) [<a href="/e-print/2309.06799" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Geoscience Meets Foundation Models: Towards General Geoscience  Artificial Intelligence System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jin-Jian Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> the manuscript is under re-writing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Geophysics (physics.geo-ph)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07627" title="Abstract">arXiv:2309.07627</a> (replaced) [<a href="/pdf/2309.07627" title="Download PDF">pdf</a>, <a href="/format/2309.07627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Reduced Basis Trust Region Methods for Parameter Identification  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kartmann%2C+M">Michael Kartmann</a>, 
<a href="/search/math?searchtype=author&query=Keil%2C+T">Tim Keil</a>, 
<a href="/search/math?searchtype=author&query=Ohlberger%2C+M">Mario Ohlberger</a>, 
<a href="/search/math?searchtype=author&query=Volkwein%2C+S">Stefan Volkwein</a>, 
<a href="/search/math?searchtype=author&query=Kaltenbacher%2C+B">Barbara Kaltenbacher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08600" title="Abstract">arXiv:2309.08600</a> (replaced) [<a href="/pdf/2309.08600" title="Download PDF">pdf</a>, <a href="/format/2309.08600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Autoencoders Find Highly Interpretable Features in Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cunningham%2C+H">Hoagy Cunningham</a>, 
<a href="/search/cs?searchtype=author&query=Ewart%2C+A">Aidan Ewart</a>, 
<a href="/search/cs?searchtype=author&query=Riggs%2C+L">Logan Riggs</a>, 
<a href="/search/cs?searchtype=author&query=Huben%2C+R">Robert Huben</a>, 
<a href="/search/cs?searchtype=author&query=Sharkey%2C+L">Lee Sharkey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 18 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10253" title="Abstract">arXiv:2309.10253</a> (replaced) [<a href="/pdf/2309.10253" title="Download PDF">pdf</a>, <a href="/format/2309.10253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPTFUZZER: Red Teaming Large Language Models with Auto-Generated  Jailbreak Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jiahao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xingwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+X">Xinyu Xing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10313" title="Abstract">arXiv:2309.10313</a> (replaced) [<a href="/pdf/2309.10313" title="Download PDF">pdf</a>, <a href="/format/2309.10313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Catastrophic Forgetting in Multimodal Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Y">Yuexiang Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+S">Shengbang Tong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+M">Mu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Q">Qing Qu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y+J">Yong Jae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yi Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10650" title="Abstract">arXiv:2309.10650</a> (replaced) [<a href="/pdf/2309.10650" title="Download PDF">pdf</a>, <a href="/format/2309.10650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MUSTANG: Multi-Stain Self-Attention Graph Multiple Instance Learning  Pipeline for Histopathology Whole Slide Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gallagher-Syed%2C+A">Amaya Gallagher-Syed</a>, 
<a href="/search/cs?searchtype=author&query=Rossi%2C+L">Luca Rossi</a>, 
<a href="/search/cs?searchtype=author&query=Rivellese%2C+F">Felice Rivellese</a>, 
<a href="/search/cs?searchtype=author&query=Pitzalis%2C+C">Costantino Pitzalis</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+M">Myles Lewis</a>, 
<a href="/search/cs?searchtype=author&query=Barnes%2C+M">Michael Barnes</a>, 
<a href="/search/cs?searchtype=author&query=Slabaugh%2C+G">Gregory Slabaugh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at BMVC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10657" title="Abstract">arXiv:2309.10657</a> (replaced) [<a href="/pdf/2309.10657" title="Download PDF">pdf</a>, <a href="/format/2309.10657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Adaptive Safety for Multi-Agent Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berducci%2C+L">Luigi Berducci</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Mangharam%2C+R">Rahul Mangharam</a>, 
<a href="/search/cs?searchtype=author&query=Grosu%2C+R">Radu Grosu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Update with appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10829" title="Abstract">arXiv:2309.10829</a> (replaced) [<a href="/pdf/2309.10829" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative study of Deep Learning Models for Binary Classification on  Combined Pulmonary Chest X-ray Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shuvo%2C+S+A">Shabbir Ahmed Shuvo</a>, 
<a href="/search/eess?searchtype=author&query=Islam%2C+M+A">Md Aminul Islam</a>, 
<a href="/search/eess?searchtype=author&query=Hoque%2C+M+M">Md. Mozammel Hoque</a>, 
<a href="/search/eess?searchtype=author&query=Sulaiman%2C+R+B">Rejwan Bin Sulaiman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12114" title="Abstract">arXiv:2309.12114</a> (replaced) [<a href="/pdf/2309.12114" title="Download PDF">pdf</a>, <a href="/format/2309.12114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoPET Challenge 2023: Sliding Window-based Optimization of U-Net
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hadlich%2C+M">Matthias Hadlich</a>, 
<a href="/search/eess?searchtype=author&query=Marinov%2C+Z">Zdravko Marinov</a>, 
<a href="/search/eess?searchtype=author&query=Stiefelhagen%2C+R">Rainer Stiefelhagen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 figure, MICCAI 2023 - AutoPET Challenge Submission Version 2: Added all results on the preliminary test set
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12594" title="Abstract">arXiv:2309.12594</a> (replaced) [<a href="/pdf/2309.12594" title="Download PDF">pdf</a>, <a href="/format/2309.12594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeFormer: Integrating Transformers with Deformable Models for 3D Shape  Abstraction from a Single Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Di Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+M">Meng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhangli%2C+Q">Qilong Zhangli</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuowei Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhixing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Metaxas%2C+D+N">Dimitris N. Metaxas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13409" title="Abstract">arXiv:2309.13409</a> (replaced) [<a href="/pdf/2309.13409" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-Series Forecasting: Unleashing Long-Term Dependencies with  Fractionally Differenced Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maitra%2C+S">Sarit Maitra</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+V">Vivek Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Dwivedi%2C+S">Srashti Dwivedi</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Sukanya Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+G+K">Goutam Kumar Kundu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Numerical Analysis (math.NA); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13438" title="Abstract">arXiv:2309.13438</a> (replaced) [<a href="/pdf/2309.13438" title="Download PDF">pdf</a>, <a href="/format/2309.13438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking superpixel segmentation from biologically inspired mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">TingYu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Bo Peng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">DaiPeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhange%2C+Z">ZhenGuang Zhange</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13570" title="Abstract">arXiv:2309.13570</a> (replaced) [<a href="/pdf/2309.13570" title="Download PDF">pdf</a>, <a href="/format/2309.13570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Mobile Digital-Twin Tracking via An RGBD-based  Transformer Model and A Comprehensive Mobile Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zixun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+K">Keling Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S+Z">Seth Z. Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Chuanyu Pan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tianjian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Weiyu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+A+Y">Allen Y. Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13575" title="Abstract">arXiv:2309.13575</a> (replaced) [<a href="/pdf/2309.13575" title="Download PDF">pdf</a>, <a href="/format/2309.13575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Weight Fixing: Large-scale training of neural network  weight uncertainties for quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Subia-Waud%2C+C">Christopher Subia-Waud</a>, 
<a href="/search/cs?searchtype=author&query=Dasmahapatra%2C+S">Srinandan Dasmahapatra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13691" title="Abstract">arXiv:2309.13691</a> (replaced) [<a href="/pdf/2309.13691" title="Download PDF">pdf</a>, <a href="/format/2309.13691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Simultaneous Information and Energy Transmission through Quantum  Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Das%2C+B+K">Bishal Kumar Das</a>, 
<a href="/search/quant-ph?searchtype=author&query=Varshney%2C+L+R">Lav R. Varshney</a>, 
<a href="/search/quant-ph?searchtype=author&query=Madhok%2C+V">Vaibhav Madhok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13698" title="Abstract">arXiv:2309.13698</a> (replaced) [<a href="/pdf/2309.13698" title="Download PDF">pdf</a>, <a href="/format/2309.13698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counting Vanishing Matrix-Vector Products
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brand%2C+C">Cornelius Brand</a>, 
<a href="/search/cs?searchtype=author&query=Korchemna%2C+V">Viktoriia Korchemna</a>, 
<a href="/search/cs?searchtype=author&query=Skotnica%2C+M">Michael Skotnica</a>, 
<a href="/search/cs?searchtype=author&query=Simonov%2C+K">Kirill Simonov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Version 2: 18 pages, 5 figures; it contains result from <a href="/abs/2209.09788">arXiv:2209.09788</a>; minor improvements, typos corrected
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14509" title="Abstract">arXiv:2309.14509</a> (replaced) [<a href="/pdf/2309.14509" title="Download PDF">pdf</a>, <a href="/format/2309.14509" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme  Long Sequence Transformer Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jacobs%2C+S+A">Sam Ade Jacobs</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+M">Masahiro Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chengming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Minjia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S+L">Shuaiwen Leon Song</a>, 
<a href="/search/cs?searchtype=author&query=Rajbhandari%2C+S">Samyam Rajbhandari</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuxiong He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14563" title="Abstract">arXiv:2309.14563</a> (replaced) [<a href="/pdf/2309.14563" title="Download PDF">pdf</a>, <a href="/format/2309.14563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a statistical theory of data selection under weak supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kolossov%2C+G">Germain Kolossov</a>, 
<a href="/search/stat?searchtype=author&query=Montanari%2C+A">Andrea Montanari</a>, 
<a href="/search/stat?searchtype=author&query=Tandon%2C+P">Pulkit Tandon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 55 pages; 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15521" title="Abstract">arXiv:2309.15521</a> (replaced) [<a href="/pdf/2309.15521" title="Download PDF">pdf</a>, <a href="/format/2309.15521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MLOps for Scarce Image Data: A Use Case in Microscopic Image Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sitcheu%2C+A+Y">Angelo Yamachui Sitcheu</a>, 
<a href="/search/cs?searchtype=author&query=Friederich%2C+N">Nils Friederich</a>, 
<a href="/search/cs?searchtype=author&query=Baeuerle%2C+S">Simon Baeuerle</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+O">Oliver Neumann</a>, 
<a href="/search/cs?searchtype=author&query=Reischl%2C+M">Markus Reischl</a>, 
<a href="/search/cs?searchtype=author&query=Mikut%2C+R">Ralf Mikut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 5 figures , 33. Workshop on Computational Intelligence Berlin Germany
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15669" title="Abstract">arXiv:2309.15669</a> (replaced) [<a href="/pdf/2309.15669" title="Download PDF">pdf</a>, <a href="/format/2309.15669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Computational Entanglement and Its Interpretation in Adversarial  Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">YenLung Lai</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xingbo Dong</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhe Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16264" title="Abstract">arXiv:2309.16264</a> (replaced) [<a href="/pdf/2309.16264" title="Download PDF">pdf</a>, <a href="/format/2309.16264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GAMMA: Generalizable Articulation Modeling and Manipulation for  Articulated Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qiaojun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenhai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+C">Ce Hao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+L">Lin Shao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Cewu Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, submitted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16282" title="Abstract">arXiv:2309.16282</a> (replaced) [<a href="/pdf/2309.16282" title="Download PDF">pdf</a>, <a href="/format/2309.16282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AgEncID: Aggregate Encryption Individual Decryption of Key for FPGA  Bitstream IP Cores in Cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Debnath%2C+M">Mukta Debnath</a>, 
<a href="/search/cs?searchtype=author&query=Guha%2C+K">Krishnendu Guha</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+D">Debasri Saha</a>, 
<a href="/search/cs?searchtype=author&query=Sur-Kolay%2C+S">Susmita Sur-Kolay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 7 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16364" title="Abstract">arXiv:2309.16364</a> (replaced) [<a href="/pdf/2309.16364" title="Download PDF">pdf</a>, <a href="/format/2309.16364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FG-NeRF: Flow-GAN based Probabilistic Neural Radiance Field for  Independence-Assumption-Free Uncertainty Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+S">Songlin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiazhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+F">Fanbo Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hao Su</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">He Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17200" title="Abstract">arXiv:2309.17200</a> (replaced) [<a href="/pdf/2309.17200" title="Download PDF">pdf</a>, <a href="/format/2309.17200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secure-by-design smart contract based on dataflow implementations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Casale-Brunet%2C+S">Simone Casale-Brunet</a>, 
<a href="/search/cs?searchtype=author&query=Mattavelli%2C+M">Marco Mattavelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17264" title="Abstract">arXiv:2309.17264</a> (replaced) [<a href="/pdf/2309.17264" title="Download PDF">pdf</a>, <a href="/format/2309.17264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Foundation Model for General Moving Object Segmentation in Medical  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhongnuo Yan</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tong Han</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Han Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiongquan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wenlong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+D">Dong Ni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 8 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17348" title="Abstract">arXiv:2309.17348</a> (replaced) [<a href="/pdf/2309.17348" title="Download PDF">pdf</a>, <a href="/format/2309.17348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Biologically Plausible Adversarial Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farinha%2C+M+T">Matilde Tristany Farinha</a>, 
<a href="/search/cs?searchtype=author&query=Ortner%2C+T">Thomas Ortner</a>, 
<a href="/search/cs?searchtype=author&query=Dellaferrera%2C+G">Giorgia Dellaferrera</a>, 
<a href="/search/cs?searchtype=author&query=Grewe%2C+B">Benjamin Grewe</a>, 
<a href="/search/cs?searchtype=author&query=Pantazi%2C+A">Angeliki Pantazi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17357" title="Abstract">arXiv:2309.17357</a> (replaced) [<a href="/pdf/2309.17357" title="Download PDF">pdf</a>, <a href="/format/2309.17357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Module-wise Training of Neural Networks via the Minimizing Movement  Scheme
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karkar%2C+S">Skander Karkar</a>, 
<a href="/search/cs?searchtype=author&query=Ayed%2C+I">Ibrahim Ayed</a>, 
<a href="/search/cs?searchtype=author&query=de+B%C3%A9zenac%2C+E">Emmanuel de B&#xe9;zenac</a>, 
<a href="/search/cs?searchtype=author&query=Gallinari%2C+P">Patrick Gallinari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. arXiv admin note: text overlap with <a href="/abs/2210.00949">arXiv:2210.00949</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17452" title="Abstract">arXiv:2309.17452</a> (replaced) [<a href="/pdf/2309.17452" title="Download PDF">pdf</a>, <a href="/format/2309.17452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gou%2C+Z">Zhibin Gou</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zhihong Shao</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yeyun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yelong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+N">Nan Duan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors equal contribution
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00199" title="Abstract">arXiv:2310.00199</a> (replaced) [<a href="/pdf/2310.00199" title="Download PDF">pdf</a>, <a href="/format/2310.00199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeformUX-Net: Exploring a 3D Foundation Backbone for Medical Image  Segmentation with Depthwise Deformable Convolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H+H">Ho Hin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Quan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+S">Shunxing Bao</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+Y">Yuankai Huo</a>, 
<a href="/search/cs?searchtype=author&query=Landman%2C+B+A">Bennett A. Landman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, the source code with our pre-trained model is available at this <a href="https://github.com/MASILab/deform-uxnet">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00270" title="Abstract">arXiv:2310.00270</a> (replaced) [<a href="/pdf/2310.00270" title="Download PDF">pdf</a>, <a href="/format/2310.00270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpatialRank: Urban Event Ranking with NDCG Optimization on  Spatiotemporal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bang An</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yongjian Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianbao Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00357" title="Abstract">arXiv:2310.00357</a> (replaced) [<a href="/pdf/2310.00357" title="Download PDF">pdf</a>, <a href="/format/2310.00357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structural Adversarial Objectives for Self-Supervised Representation  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Maire%2C+M">Michael Maire</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00526" title="Abstract">arXiv:2310.00526</a> (replaced) [<a href="/e-print/2310.00526" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Graph Neural Networks Optimal Approximation Algorithms?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yau%2C+M">Morris Yau</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+E">Eric Lu</a>, 
<a href="/search/cs?searchtype=author&query=Karalias%2C+N">Nikolaos Karalias</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jessica Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jegelka%2C+S">Stefanie Jegelka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Figure 1 pg 2. is inaccurate
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00527" title="Abstract">arXiv:2310.00527</a> (replaced) [<a href="/pdf/2310.00527" title="Download PDF">pdf</a>, <a href="/format/2310.00527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised Learning of Contextualized Local Visual Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silva%2C+T+S">Thalles Santos Silva</a>, 
<a href="/search/cs?searchtype=author&query=Pedrini%2C+H">Helio Pedrini</a>, 
<a href="/search/cs?searchtype=author&query=Rivera%2C+A+R">Ad&#xed;n Ram&#xed;rez Rivera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print. 4th Visual Inductive Priors for Data-Efficient Deep Learning Workshop ICCV 2023. Code at <a href="https://github.com/sthalles/CLoVE">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 4th Visual Inductive Priors for Data-Efficient Deep Learning
  Workshop ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00574" title="Abstract">arXiv:2310.00574</a> (replaced) [<a href="/pdf/2310.00574" title="Download PDF">pdf</a>, <a href="/format/2310.00574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SIMD Dataflow Co-optimization for Efficient Neural Networks Inferences  on CPUs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Cyrus Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hassman%2C+Z">Zack Hassman</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruize Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+D">Dhirpal Shah</a>, 
<a href="/search/cs?searchtype=author&query=Richard%2C+V">Vaugnn Richard</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanjing Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Machine Learning (cs.LG); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00648" title="Abstract">arXiv:2310.00648</a> (replaced) [<a href="/pdf/2310.00648" title="Download PDF">pdf</a>, <a href="/format/2310.00648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lauren Hong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Ting Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00673" title="Abstract">arXiv:2310.00673</a> (replaced) [<a href="/pdf/2310.00673" title="Download PDF">pdf</a>, <a href="/format/2310.00673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Type Inference for Enhanced Dataflow Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seidel%2C+L">Lukas Seidel</a>, 
<a href="/search/cs?searchtype=author&query=Effendi%2C+S+D+B">Sedick David Baker Effendi</a>, 
<a href="/search/cs?searchtype=author&query=Pinho%2C+X">Xavier Pinho</a>, 
<a href="/search/cs?searchtype=author&query=Rieck%2C+K">Konrad Rieck</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Merwe%2C+B">Brink van der Merwe</a>, 
<a href="/search/cs?searchtype=author&query=Yamaguchi%2C+F">Fabian Yamaguchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> - fixed last author's name - fixed header
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 28th European Symposium on Research in Computer Security (ESORICS)
  2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00681" title="Abstract">arXiv:2310.00681</a> (replaced) [<a href="/pdf/2310.00681" title="Download PDF">pdf</a>, <a href="/format/2310.00681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PharmacoNet: Accelerating Large-Scale Virtual Screening by Deep  Pharmacophore Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Seo%2C+S">Seonghwan Seo</a>, 
<a href="/search/q-bio?searchtype=author&query=Kim%2C+W+Y">Woo Youn Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01011" title="Abstract">arXiv:2310.01011</a> (replaced) [<a href="/pdf/2310.01011" title="Download PDF">pdf</a>, <a href="/format/2310.01011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Fixing Clever-Hans Predictors with Counterfactual Knowledge  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bender%2C+S">Sidney Bender</a>, 
<a href="/search/cs?searchtype=author&query=Anders%2C+C+J">Christopher J. Anders</a>, 
<a href="/search/cs?searchtype=author&query=Chormai%2C+P">Pattarawatt Chormai</a>, 
<a href="/search/cs?searchtype=author&query=Marxfeld%2C+H">Heike Marxfeld</a>, 
<a href="/search/cs?searchtype=author&query=Herrmann%2C+J">Jan Herrmann</a>, 
<a href="/search/cs?searchtype=author&query=Montavon%2C+G">Gr&#xe9;goire Montavon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01079" title="Abstract">arXiv:2310.01079</a> (replaced) [<a href="/pdf/2310.01079" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Approach with Monte-Carlo Simulation and Hybrid Optimization  Approach for Inventory Management with Stochastic Demand
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maitra%2C+S">Sarit Maitra</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+V">Vivek Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Sukanya Kundu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01468" title="Abstract">arXiv:2310.01468</a> (replaced) [<a href="/pdf/2310.01468" title="Download PDF">pdf</a>, <a href="/format/2310.01468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Entity-Deduction Arena: A playground for probing the conversational  reasoning and planning capabilities of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yizhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiarui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Jaitly%2C+N">Navdeep Jaitly</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01469" title="Abstract">arXiv:2310.01469</a> (replaced) [<a href="/pdf/2310.01469" title="Download PDF">pdf</a>, <a href="/format/2310.01469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Lies: Hallucinations are not Bugs, but Features as Adversarial  Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jia-Yu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+K">Kun-Peng Ning</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhen-Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+M">Mu-Nan Ning</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Li Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01557" title="Abstract">arXiv:2310.01557</a> (replaced) [<a href="/pdf/2310.01557" title="Download PDF">pdf</a>, <a href="/format/2310.01557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SmartPlay : A Benchmark for LLMs as Intelligent Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+T+M">Tom M. Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanzhi Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01653" title="Abstract">arXiv:2310.01653</a> (replaced) [<a href="/pdf/2310.01653" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Taxonomy and Evaluation of IoT Security Guidelines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jesse Chen</a>, 
<a href="/search/cs?searchtype=author&query=Anandayuvaraj%2C+D">Dharun Anandayuvaraj</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+J+C">James C Davis</a>, 
<a href="/search/cs?searchtype=author&query=Rahaman%2C+S">Sazzadur Rahaman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01737" title="Abstract">arXiv:2310.01737</a> (replaced) [<a href="/pdf/2310.01737" title="Download PDF">pdf</a>, <a href="/format/2310.01737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blending Imitation and Reinforcement Learning for Robust Policy  Improvement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuefeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yoneda%2C+T">Takuma Yoneda</a>, 
<a href="/search/cs?searchtype=author&query=Stevens%2C+R+L">Rick L. Stevens</a>, 
<a href="/search/cs?searchtype=author&query=Walter%2C+M+R">Matthew R. Walter</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxin Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01839" title="Abstract">arXiv:2310.01839</a> (replaced) [<a href="/pdf/2310.01839" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preserving Phonemic Distinctions for Ordinal Regression: A Novel Loss  Function for Automatic Pronunciation Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yan%2C+B">Bi-Cheng Yan</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Hsin-Wei Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yi-Cheng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jiun-Ting Li</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+C">Chi-Han Lin</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+B">Berlin Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01852" title="Abstract">arXiv:2310.01852</a> (replaced) [<a href="/pdf/2310.01852" title="Download PDF">pdf</a>, <a href="/format/2310.01852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LanguageBind: Extending Video-Language Pretraining to N-modality by  Language-based Semantic Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+M">Munan Ning</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jiaxi Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">HongFa Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Y">Yatian Pang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenhao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junwu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wancai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhifeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Li Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01866" title="Abstract">arXiv:2310.01866</a> (replaced) [<a href="/pdf/2310.01866" title="Download PDF">pdf</a>, <a href="/format/2310.01866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Route Design in Sheepdog System--Traveling Salesman Problem Formulation  and Evolutionary Computation Solution--
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imahayashi%2C+W">Wataru Imahayashi</a>, 
<a href="/search/cs?searchtype=author&query=Tsunoda%2C+Y">Yusuke Tsunoda</a>, 
<a href="/search/cs?searchtype=author&query=Ogura%2C+M">Masaki Ogura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01886" title="Abstract">arXiv:2310.01886</a> (replaced) [<a href="/pdf/2310.01886" title="Download PDF">pdf</a>, <a href="/format/2310.01886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective and Parameter-Efficient Reusing Fine-Tuned Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Weisen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Baijiong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Han Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J+T">James T. Kwok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01892" title="Abstract">arXiv:2310.01892</a> (replaced) [<a href="/pdf/2310.01892" title="Download PDF">pdf</a>, <a href="/ps/2310.01892" title="Download PostScript">ps</a>, <a href="/format/2310.01892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FiGURe: Simple and Efficient Unsupervised Node Representations with  Filter Augmentations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ekbote%2C+C">Chanakya Ekbote</a>, 
<a href="/search/cs?searchtype=author&query=Deshpande%2C+A+P">Ajinkya Pankaj Deshpande</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+A">Arun Iyer</a>, 
<a href="/search/cs?searchtype=author&query=Bairi%2C+R">Ramakrishna Bairi</a>, 
<a href="/search/cs?searchtype=author&query=Sellamanickam%2C+S">Sundararajan Sellamanickam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02031" title="Abstract">arXiv:2310.02031</a> (replaced) [<a href="/pdf/2310.02031" title="Download PDF">pdf</a>, <a href="/format/2310.02031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OceanGPT: A Large Language Model for Ocean Science Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+Z">Zhen Bi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yida Xue</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+Y">Yixin Ou</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+D">Daxiong Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Guozhou Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress. Project Website: <a href="https://zjunlp.github.io/project/OceanGPT/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02094" title="Abstract">arXiv:2310.02094</a> (replaced) [<a href="/pdf/2310.02094" title="Download PDF">pdf</a>, <a href="/format/2310.02094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoNO: Complex Neural Operator for Continuous Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+K">Karn Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+N+M+A">N M Anoop Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=P%2C+P+A">Prathosh A P</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02130" title="Abstract">arXiv:2310.02130</a> (replaced) [<a href="/pdf/2310.02130" title="Download PDF">pdf</a>, <a href="/format/2310.02130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustering Graphs of Bounded Treewidth to Minimize the Sum of  Radius-Dependent Costs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Drexler%2C+L">Lukas Drexler</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B6ckendorff%2C+J">Jan H&#xf6;ckendorff</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6nen%2C+J">Joshua K&#xf6;nen</a>, 
<a href="/search/cs?searchtype=author&query=Schewior%2C+K">Kevin Schewior</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> updated funding information
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02238" title="Abstract">arXiv:2310.02238</a> (replaced) [<a href="/pdf/2310.02238" title="Download PDF">pdf</a>, <a href="/ps/2310.02238" title="Download PostScript">ps</a>, <a href="/format/2310.02238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Who&#x27;s Harry Potter? Approximate Unlearning in LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eldan%2C+R">Ronen Eldan</a>, 
<a href="/search/cs?searchtype=author&query=Russinovich%2C+M">Mark Russinovich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02244" title="Abstract">arXiv:2310.02244</a> (replaced) [<a href="/pdf/2310.02244" title="Download PDF">pdf</a>, <a href="/format/2310.02244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Greg Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dingli Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hayou%2C+S">Soufiane Hayou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Probability (math.PR)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item342">Cross-lists</a></li>
<li><a href="#item390">Replacements</a></li>
</ul>
<small>[ total of 595 entries:  <b>1-595</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
