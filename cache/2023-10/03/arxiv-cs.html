<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Fri 29 Sep 23  to  Mon  2 Oct 23, announced Tue,  3 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item699">Cross-lists</a></li>
<li><a href="#item782">Replacements</a></li>
</ul>
<small>[ total of 1206 entries:  <b>1-1206</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Tue,  3 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00001" title="Abstract">arXiv:2310.00001</a> [<a href="/pdf/2310.00001" title="Download PDF">pdf</a>, <a href="/format/2310.00001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AsaPy: A Python Library for Aerospace Simulation Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dantas%2C+J+P+A">Joao P. A. Dantas</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+S+R">Samara R. Silva</a>, 
<a href="/search/cs?searchtype=author&query=Gomes%2C+V+C+F">Vitor C. F. Gomes</a>, 
<a href="/search/cs?searchtype=author&query=Costa%2C+A+N">Andre N. Costa</a>, 
<a href="/search/cs?searchtype=author&query=Samersla%2C+A+R">Adrisson R. Samersla</a>, 
<a href="/search/cs?searchtype=author&query=Geraldo%2C+D">Diego Geraldo</a>, 
<a href="/search/cs?searchtype=author&query=Maximo%2C+M+R+O+A">Marcos R. O. A. Maximo</a>, 
<a href="/search/cs?searchtype=author&query=Yoneyama%2C+T">Takashi Yoneyama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>

</div>
<p class="mathjax">AsaPy is a custom-made Python library designed to simplify and optimize the
analysis of simulation data. It offers a range of features, including the
design of experiment methods, statistical analysis techniques, machine learning
algorithms, and data visualization tools. AsaPy's flexibility and
customizability make it a viable solution for engineers and researchers who
need to quickly gain insights into constructive simulations. AsaPy is built on
top of popular scientific computing libraries, ensuring high performance and
scalability. In this work, we provide an overview of the key features and
capabilities of AsaPy, followed by an exposition of its architecture and
demonstrations of its effectiveness through some use cases applied in military
operational simulations. We also evaluate how other simulation tools deal with
data science, highlighting AsaPy's strengths and advantages. Finally, we
discuss potential use cases and applications of AsaPy and outline future
directions for the development and improvement of the library.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00003" title="Abstract">arXiv:2310.00003</a> [<a href="/pdf/2310.00003" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Derivation of a 2D PCCU-AENO method for nonconservative problems.  Theory, Method and theoretical arguments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Roland%2C+N+N+A">Ngatcha Ndengna Arno Roland</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">In this paper, we introduce a methodology to design genuinely two-dimensional
(2D) secondorder path-conservative central-upwind (PCCU) schemes. The scheme
studies dam-break with high sediment concentration over abrupt moving
topography quickly spatially variable even in the presence of resonance. This
study is possible via a 2D sediment transport model (including arbitrarily
sloping sediment beds and associated energy and entropy) in new generalized
Shallow Water equations derived with associated energy and entropy in this
work. We establish an existence theorem of global weak solutions. We show the
convergence of a sequence of solutions of the proposed model. The second-order
accuracy of the PCCU scheme is achieved using a new extension AENO (Averaging
Essentially Non-Oscillatory) reconstruction developed in the 2D version of this
work. We prove by rigorous demonstrations that the derived 2D scheme on
structured meshes is well-balanced and positivity-preserving. Several tests are
made to show the ability and superb performance of the proposed numerical
modeling. The results obtained are compared with those existing in the
literature and with experimental data. The current modeling improves some
recent results in sediment transport and shows a good ability to simulate
sediment transport in large-range environments.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00005" title="Abstract">arXiv:2310.00005</a> [<a href="/pdf/2310.00005" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated machine vision control system for technological nodes assembly  process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shtabel%2C+N">Nikolay Shtabel</a>, 
<a href="/search/cs?searchtype=author&query=Saramud%2C+M">Mikhail Saramud</a>, 
<a href="/search/cs?searchtype=author&query=Tkachev%2C+S">Stepan Tkachev</a>, 
<a href="/search/cs?searchtype=author&query=Pikalov%2C+I">Iakov Pikalov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, MIST Aerospace - V - 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The paper discusses the prerequisites for the creation, technical solutions
and implementation of an automated control system for the assembly of a small
spacecraft. Both the hardware and software implementation of the system that
provides control and logging of the assembly process of individual units at
various workplaces are analyzed. The article presents solutions to reduce the
requirements for equipment used to control the assembly technology, in
particular, to use cameras with a lower resolution, through the use of special
algorithms for the formation and processing of technological marks. A tool is
presented that allows you to control the tightening torques of threaded
connections and limit the tightening torque according to a given algorithm with
wireless control. The developed system provides the functions of not only
control, but also logging of the technological process, which can be useful in
the future when creating a digital twin of the product.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00008" title="Abstract">arXiv:2310.00008</a> [<a href="/pdf/2310.00008" title="Download PDF">pdf</a>, <a href="/format/2310.00008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Multimodal Locomotion: A Quick Overview of Hardware and Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pitroda%2C+S">Shreyansh Pitroda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Bipedal robots are a fascinating and advanced category of robots designed to
mimic human form and locomotion. The development of the bipedal robots is a
significant milestone in robotics. However, even the most advanced bipedal
robots are susceptible to changes in terrain, obstacle negotiation, payload,
and weight distribution, and the ability to recover after stumbles. These
problems can be circumvented by introducing thrusters. Thrusters will allow the
robot to stabilize on various uneven terrain. The robot can easily avoid
obstacles and will be able to recover after stumbling. Harpy is a bipedal robot
that has 6 joints and 2 thrusters and serves as a hardware platform for
implementing advanced control algorithms. This thesis explores manufacturing
harpy hardware such that the overall system can be lightweight and strong.
Also, it goes through simulation results to show thruster-assisted walking, and
at last, it shows firmware and communication network development which is
implemented on actual hardware. vii
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00009" title="Abstract">arXiv:2310.00009</a> [<a href="/pdf/2310.00009" title="Download PDF">pdf</a>, <a href="/format/2310.00009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dataset Generation for Drone Optimal Placement Using Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jialin Hao</a> (TSP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Unmanned aerial vehicle (UAV), or drone is increasingly becoming a promising
tool in communication system. This report explains the generation details of a
dataset which will be used to designing an algorithm for the optimal placement
of UAVs in the drone-assisted vehicular network (DAVN). The goal is to improve
the drones' communication and energy efficiency after our previous work. The
report is organized as followed: the first section is devoted to the delay
analysis of the vehicle requests in the DAVN using queuing theory; the second
part of the report models the energy consumption of the drones while the third
section explains the simulation scenario and dataset features. The notations
and terminologies used in this report are summarized in the last section.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00010" title="Abstract">arXiv:2310.00010</a> [<a href="/pdf/2310.00010" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial Empathy Classification: A Survey of Deep Learning Techniques,  Datasets, and Evaluation Scales
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tahir%2C+S">Sharjeel Tahir</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+S+A">Syed Afaq Shah</a>, 
<a href="/search/cs?searchtype=author&query=Abu-Khalaf%2C+J">Jumana Abu-Khalaf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">From the last decade, researchers in the field of machine learning (ML) and
assistive developmental robotics (ADR) have taken an interest in artificial
empathy (AE) as a possible future paradigm for human-robot interaction (HRI).
Humans learn empathy since birth, therefore, it is challenging to instill this
sense in robots and intelligent machines. Nevertheless, by training over a vast
amount of data and time, imitating empathy, to a certain extent, can be
possible for robots. Training techniques for AE, along with findings from the
field of empathetic AI research, are ever-evolving. The standard workflow for
artificial empathy consists of three stages: 1) Emotion Recognition (ER) using
the retrieved features from video or textual data, 2) analyzing the perceived
emotion or degree of empathy to choose the best course of action, and 3)
carrying out a response action. Recent studies that show AE being used with
virtual agents or robots often include Deep Learning (DL) techniques. For
instance, models like VGGFace are used to conduct ER. Semi-supervised models
like Autoencoders generate the corresponding emotional states and behavioral
responses. However, there has not been any study that presents an independent
approach for evaluating AE, or the degree to which a reaction was empathetic.
This paper aims to investigate and evaluate existing works for measuring and
evaluating empathy, as well as the datasets that have been collected and used
so far. Our goal is to highlight and facilitate the use of state-of-the-art
methods in the area of AE by comparing their performance. This will aid
researchers in the area of AE in selecting their approaches with precision.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00011" title="Abstract">arXiv:2310.00011</a> [<a href="/pdf/2310.00011" title="Download PDF">pdf</a>, <a href="/format/2310.00011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Self-supervised Depth and Optical Flow Estimation towards Dynamic  Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhengyang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Ying Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Significant attention has been attracted to deep learning-based depth
estimates. Dynamic objects become the most hard problems in
inter-frame-supervised depth estimates due to the uncertainty in adjacent
frames. Thus, integrating optical flow information with depth estimation is a
feasible solution, as the optical flow is an essential motion representation.
In this work, we construct a joint inter-frame-supervised depth and optical
flow estimation framework, which predicts depths in various motions by
minimizing pixel wrap errors in bilateral photometric re-projections and
optical vectors. For motion segmentation, we adaptively segment the preliminary
estimated optical flow map with large areas of connectivity. In self-supervised
depth estimation, different motion regions are predicted independently and then
composite into a complete depth. Further, the pose and depth estimations
re-synthesize the optical flow maps, serving to compute reconstruction errors
with the preliminary predictions. Our proposed joint depth and optical flow
estimation outperforms existing depth estimators on the KITTI Depth dataset,
both with and without Cityscapes pretraining. Additionally, our optical flow
results demonstrate competitive performance on the KITTI Flow 2015 dataset.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00012" title="Abstract">arXiv:2310.00012</a> [<a href="/pdf/2310.00012" title="Download PDF">pdf</a>, <a href="/format/2310.00012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Operator-free Equilibrium on the Sphere
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dai%2C+X">Xiongming Dai</a>, 
<a href="/search/math?searchtype=author&query=Baumgartner%2C+G">Gerald Baumgartner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages,3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We propose a generalized minimum discrepancy, which derives from Legendre's
ODE and spherical harmonic theoretics to provide a new criterion of
equidistributed pointsets on the sphere. A continuous and derivative kernel in
terms of elementary functions is established to simplify the computation of the
generalized minimum discrepancy. We consider the deterministic point generated
from Pycke's statistics to integrate a Franke function for the sphere and
investigate the discrepancies of points systems embedding with different
kernels. Quantitive experiments are conducted and the results are analyzed. Our
deduced model can explore latent point systems, that have the minimum
discrepancy without the involvement of pseudodifferential operators and
Beltrami operators, by the use of derivatives. Compared to the random point
generated from the Monte Carlo method, only a few points generated by our
method are required to approximate the target in arbitrary dimensions.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00013" title="Abstract">arXiv:2310.00013</a> [<a href="/pdf/2310.00013" title="Download PDF">pdf</a>, <a href="/format/2310.00013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Communications in Collaborative Perception with Domain  Alignment for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Senkang%2C+H">Hu Senkang</a>, 
<a href="/search/cs?searchtype=author&query=Zhengru%2C+F">Fang Zhengru</a>, 
<a href="/search/cs?searchtype=author&query=Haonan%2C+A">An Haonan</a>, 
<a href="/search/cs?searchtype=author&query=Guowen%2C+X">Xu Guowen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhou Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xianhao%2C+C">Chen Xianhao</a>, 
<a href="/search/cs?searchtype=author&query=Yuguang%2C+F">Fang Yuguang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Collaborative perception among multiple connected and autonomous vehicles can
greatly enhance perceptive capabilities by allowing vehicles to exchange
supplementary information via communications. Despite advances in previous
approaches, challenges still remain due to channel variations and data
heterogeneity among collaborative vehicles. To address these issues, we propose
ACC-DA, a channel-aware collaborative perception framework to dynamically
adjust the communication graph and minimize the average transmission delay
while mitigating the side effects from the data heterogeneity. Our novelties
lie in three aspects. We first design a transmission delay minimization method,
which can construct the communication graph and minimize the transmission delay
according to different channel information state. We then propose an adaptive
data reconstruction mechanism, which can dynamically adjust the rate-distortion
trade-off to enhance perception efficiency. Moreover, it minimizes the temporal
redundancy during data transmissions. Finally, we conceive a domain alignment
scheme to align the data distribution from different vehicles, which can
mitigate the domain gap between different vehicles and improve the performance
of the target task. Comprehensive experiments demonstrate the effectiveness of
our method in comparison to the existing state-of-the-art works.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00014" title="Abstract">arXiv:2310.00014</a> [<a href="/pdf/2310.00014" title="Download PDF">pdf</a>, <a href="/format/2310.00014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fewer-token Neural Speech Codec with Time-invariant Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yong Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+J">Jiangyan Yi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Le Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+J">Jianhua Tao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chuyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Junzuo Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Language model based text-to-speech (TTS) models, like VALL-E, have gained
attention for their outstanding in-context learning capability in zero-shot
scenarios. Neural speech codec is a critical component of these models, which
can convert speech into discrete token representations. However, excessive
token sequences from the codec may negatively affect prediction accuracy and
restrict the progression of Language model based TTS models. To address this
issue, this paper proposes a novel neural speech codec with time-invariant
codes named TiCodec. By encoding and quantizing time-invariant information into
a separate code, TiCodec can reduce the amount of frame-level information that
needs encoding, effectively decreasing the number of tokens as codes of speech.
Furthermore, this paper introduces a time-invariant encoding consistency loss
to enhance the consistency of time-invariant code within an utterance and force
it to capture more global information, which can benefit the zero-shot TTS
task. Experimental results demonstrate that TiCodec can not only enhance the
quality of reconstruction speech with fewer tokens but also increase the
similarity and naturalness, as well as reduce the word error rate of the
synthesized speech by the TTS model.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00015" title="Abstract">arXiv:2310.00015</a> [<a href="/pdf/2310.00015" title="Download PDF">pdf</a>, <a href="/format/2310.00015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Communication with Probability Graph: A Joint Communication and  Computation Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhouxiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhaohui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+Q">Quoc-Viet Pham</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qianqian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoyang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, we present a probability graph-based semantic information
compression system for scenarios where the base station (BS) and the user share
common background knowledge. We employ probability graphs to represent the
shared knowledge between the communicating parties. During the transmission of
specific text data, the BS first extracts semantic information from the text,
which is represented by a knowledge graph. Subsequently, the BS omits certain
relational information based on the shared probability graph to reduce the data
size. Upon receiving the compressed semantic data, the user can automatically
restore missing information using the shared probability graph and predefined
rules. This approach brings additional computational resource consumption while
effectively reducing communication resource consumption. Considering the
limitations of wireless resources, we address the problem of joint
communication and computation resource allocation design, aiming at minimizing
the total communication and computation energy consumption of the network while
adhering to latency, transmit power, and semantic constraints. Simulation
results demonstrate the effectiveness of the proposed system.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00016" title="Abstract">arXiv:2310.00016</a> [<a href="/pdf/2310.00016" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PID Optimization Using Lagrangian Mechanics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kou%2C+E">Ethan Kou</a>, 
<a href="/search/eess?searchtype=author&query=Moghadam%2C+M">Majid Moghadam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Creating a simulation of a system enables the tuning of control systems
without the need for a physical system. In this paper, we employ Lagrangian
Mechanics to derive a set of equations to simulate an inverted pendulum on a
cart. The system consists of a freely-rotating rod attached to a cart, with the
rod's balance achieved through applying the correct forces to the cart. We
manually tune the proportional, integral, and derivative gain coefficients of a
Proportional Integral Derivative controller (PID) to balance a rod. To further
improve PID performance, we can optimize an objective function to find better
gain coefficients.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00022" title="Abstract">arXiv:2310.00022</a> [<a href="/pdf/2310.00022" title="Download PDF">pdf</a>, <a href="/format/2310.00022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt-Enhanced Self-supervised Representation Learning for Remote  Sensing Image Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qingjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Learning representations through self-supervision on a large-scale, unlabeled
dataset has proven to be highly effective for understanding diverse images,
such as those used in remote sensing image analysis. However, remote sensing
images often have complex and densely populated scenes, with multiple land
objects and no clear foreground objects. This intrinsic property can lead to
false positive pairs in contrastive learning, or missing contextual information
in reconstructive learning, which can limit the effectiveness of existing
self-supervised learning methods. To address these problems, we propose a
prompt-enhanced self-supervised representation learning method that uses a
simple yet efficient pre-training pipeline. Our approach involves utilizing
original image patches as a reconstructive prompt template, and designing a
prompt-enhanced generative branch that provides contextual information through
semantic consistency constraints. We collected a dataset of over 1.28 million
remote sensing images that is comparable to the popular ImageNet dataset, but
without specific temporal or geographical constraints. Our experiments show
that our method outperforms fully supervised learning models and
state-of-the-art self-supervised learning methods on various downstream tasks,
including land cover classification, semantic segmentation, object detection,
and instance segmentation. These results demonstrate that our approach learns
impressive remote sensing representations with high generalization and
transferability.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00023" title="Abstract">arXiv:2310.00023</a> [<a href="/pdf/2310.00023" title="Download PDF">pdf</a>, <a href="/format/2310.00023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> De-SaTE: Denoising Self-attention Transformer Encoders for Li-ion  Battery Health Prognostics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shinde%2C+G">Gaurav Shinde</a>, 
<a href="/search/cs?searchtype=author&query=Mohapatra%2C+R">Rohan Mohapatra</a>, 
<a href="/search/cs?searchtype=author&query=Krishan%2C+P">Pooja Krishan</a>, 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+S">Saptarshi Sengupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, 3 tables, 17 equations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Lithium Ion (Li-ion) batteries have gained widespread popularity across
various industries, from powering portable electronic devices to propelling
electric vehicles and supporting energy storage systems. A central challenge in
managing Li-ion batteries effectively is accurately predicting their Remaining
Useful Life (RUL), which is a critical measure for proactive maintenance and
predictive analytics. This study presents a novel approach that harnesses the
power of multiple denoising modules, each trained to address specific types of
noise commonly encountered in battery data. Specifically we use a denoising
auto-encoder and a wavelet denoiser to generate encoded/decomposed
representations, which are subsequently processed through dedicated
self-attention transformer encoders. After extensive experimentation on the
NASA and CALCE datasets, we are able to characterize a broad spectrum of health
indicator estimations under a set of diverse noise patterns. We find that our
reported error metrics on these datasets are on par or better with the best
reported in recent literature.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00029" title="Abstract">arXiv:2310.00029</a> [<a href="/pdf/2310.00029" title="Download PDF">pdf</a>, <a href="/ps/2310.00029" title="Download PostScript">ps</a>, <a href="/format/2310.00029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Driving Behavior Generation Incorporating Human Risk  Cognition for Autonomous Vehicle Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Hao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+S">Shuo Cai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yunfeng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+T">Ting Qu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+X">Xun Gong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Autonomous vehicle (AV) evaluation has been the subject of increased interest
in recent years both in industry and in academia. This paper focuses on the
development of a novel framework for generating adversarial driving behavior of
background vehicle interfering against the AV to expose effective and rational
risky events. Specifically, the adversarial behavior is learned by a
reinforcement learning (RL) approach incorporated with the cumulative prospect
theory (CPT) which allows representation of human risk cognition. Then, the
extended version of deep deterministic policy gradient (DDPG) technique is
proposed for training the adversarial policy while ensuring training stability
as the CPT action-value function is leveraged. A comparative case study
regarding the cut-in scenario is conducted on a high fidelity
Hardware-in-the-Loop (HiL) platform and the results demonstrate the adversarial
effectiveness to infer the weakness of the tested AV.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00031" title="Abstract">arXiv:2310.00031</a> [<a href="/pdf/2310.00031" title="Download PDF">pdf</a>, <a href="/format/2310.00031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-image Alignment for Diffusion-based Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kondapaneni%2C+N">Neehar Kondapaneni</a>, 
<a href="/search/cs?searchtype=author&query=Marks%2C+M">Markus Marks</a>, 
<a href="/search/cs?searchtype=author&query=Knott%2C+M">Manuel Knott</a>, 
<a href="/search/cs?searchtype=author&query=Guimar%C3%A3es%2C+R">Rog&#xe9;rio Guimar&#xe3;es</a>, 
<a href="/search/cs?searchtype=author&query=Perona%2C+P">Pietro Perona</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Diffusion models are generative models with impressive text-to-image
synthesis capabilities and have spurred a new wave of creative methods for
classical machine learning tasks. However, the best way to harness the
perceptual knowledge of these generative models for visual tasks is still an
open question. Specifically, it is unclear how to use the prompting interface
when applying diffusion backbones to vision tasks. We find that automatically
generated captions can improve text-image alignment and significantly enhance a
model's cross-attention maps, leading to better perceptual performance. Our
approach improves upon the current SOTA in diffusion-based semantic
segmentation on ADE20K and the current overall SOTA in depth estimation on
NYUv2. Furthermore, our method generalizes to the cross-domain setting; we use
model personalization and caption modifications to align our model to the
target domain and find improvements over unaligned baselines. Our object
detection model, trained on Pascal VOC, achieves SOTA results on Watercolor2K.
Our segmentation method, trained on Cityscapes, achieves SOTA results on Dark
Zurich-val and Nighttime Driving.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00032" title="Abstract">arXiv:2310.00032</a> [<a href="/pdf/2310.00032" title="Download PDF">pdf</a>, <a href="/format/2310.00032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pretrain, Prompt, and Transfer: Evolving Digital Twins for Time-to-Event  Analysis in Cyber-physical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qinghua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+T">Tao Yue</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+S">Shaukat Ali</a>, 
<a href="/search/cs?searchtype=author&query=Arratibel%2C+M">Maite Arratibel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Cyber-Physical Systems (CPSs), e.g., elevator systems and autonomous driving
systems, are progressively permeating our everyday lives. To ensure their
safety, various analyses need to be conducted, such as anomaly detection and
time-to-event analysis (the focus of this paper). Recently, it has been widely
accepted that digital Twins (DTs) can serve as an efficient method to aid in
the development, maintenance, and safe and secure operation of CPSs. However,
CPSs frequently evolve, e.g., with new or updated functionalities, which demand
their corresponding DTs be co-evolved, i.e., in synchronization with the CPSs.
To that end, we propose a novel method, named PPT, utilizing an
uncertainty-aware transfer learning for DT evolution. Specifically, we first
pretrain PPT with a pretraining dataset to acquire generic knowledge about the
CPSs, followed by adapting it to a specific CPS with the help of prompt tuning.
Results highlight that PPT is effective in time-to-event analysis in both
elevator and ADSs case studies, on average, outperforming a baseline method by
7.31 and 12.58 in terms of Huber loss, respectively. The experiment results
also affirm the effectiveness of transfer learning, prompt tuning and
uncertainty quantification in terms of reducing Huber loss by at least 21.32,
3.14 and 4.08, respectively, in both case studies.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00033" title="Abstract">arXiv:2310.00033</a> [<a href="/pdf/2310.00033" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OriWheelBot: An origami-wheeled robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Z">Zufeng Pang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiyong Li</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+G">Guilin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zhoucheng Su</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junfeng He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kaiyue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+D">Dezheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zenan Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shouyan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yang Tian</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y+M">Yi Min Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenpei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhuangjian Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 papes, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Applied Physics (physics.app-ph)

</div>
<p class="mathjax">Origami-inspired robots with multiple advantages, such as being lightweight,
requiring less assembly, and exhibiting exceptional deformability, have
received substantial and sustained attention. However, the existing
origami-inspired robots are usually of limited functionalities and developing
feature-rich robots is very challenging. Here, we report an origami-wheeled
robot (OriWheelBot) with variable width and outstanding sand walking
versatility. The OriWheelBot's ability to adjust wheel width over obstacles is
achieved by origami wheels made of Miura origami. An improved version, called
iOriWheelBot, is also developed to automatically judge the width of the
obstacles. Three actions, namely direct pass, variable width pass, and direct
return, will be carried out depending on the width of the channel between the
obstacles. We have identified two motion mechanisms, i.e., sand-digging and
sand-pushing, with the latter being more conducive to walking on the sand. We
have systematically examined numerous sand walking characteristics, including
carrying loads, climbing a slope, walking on a slope, and navigating sand pits,
small rocks, and sand traps. The OriWheelBot can change its width by 40%, has a
loading-carrying ratio of 66.7% on flat sand and can climb a 17-degree sand
incline. The OriWheelBot can be useful for planetary subsurface exploration and
disaster area rescue.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00034" title="Abstract">arXiv:2310.00034</a> [<a href="/pdf/2310.00034" title="Download PDF">pdf</a>, <a href="/format/2310.00034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PB-LLM: Partially Binarized Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shang%2C+Y">Yuzhang Shang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhihang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qiang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhen Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Frist work using network binarization for large language model compression
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">This paper explores network binarization, a radical form of quantization,
compressing model weights to a single bit, specifically for Large Language
Models (LLMs) compression. Due to previous binarization methods collapsing
LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can
achieve extreme low-bit quantization while maintaining the linguistic reasoning
capacity of quantized LLMs. Specifically, our exploration first uncovers the
ineffectiveness of naive applications of existing binarization algorithms and
highlights the imperative role of salient weights in achieving low-bit
quantization. Thus, PB-LLM filters a small ratio of salient weights during
binarization, allocating them to higher-bit storage, i.e.,
partially-binarization. PB-LLM is extended to recover the capacities of
quantized LMMs, by analyzing from the perspective of post-training quantization
(PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts
from GPTQ, we reconstruct the binarized weight matrix guided by the Hessian
matrix and successfully recover the reasoning capacity of PB-LLM in low-bit.
Under QAT, we freeze the salient weights during training, explore the
derivation of optimal scaling factors crucial for minimizing the quantization
error, and propose a scaling mechanism based on this derived scaling strategy
for residual binarized weights. Those explorations and the developed
methodologies significantly contribute to rejuvenating the performance of
low-bit quantized LLMs and present substantial advancements in the field of
network binarization for LLMs.The code is available at
https://github.com/hahnyuan/BinaryLLM.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00035" title="Abstract">arXiv:2310.00035</a> [<a href="/pdf/2310.00035" title="Download PDF">pdf</a>, <a href="/format/2310.00035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoRA ensembles for large language model fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Aitchison%2C+L">Laurence Aitchison</a>, 
<a href="/search/cs?searchtype=author&query=Rudolph%2C+M">Maja Rudolph</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as
overconfidence, poor calibration, and unreliable prediction results on test
data or out-of-distribution samples. One approach commonly used in vision for
alleviating this issue is a deep ensemble, which constructs an ensemble by
training the same model multiple times using different random initializations.
However, there is a huge challenge to ensembling LLMs: the most effective LLMs
are very, very large. Keeping a single LLM in memory is already challenging
enough: keeping an ensemble of e.g. 5 LLMs in memory is impossible in many
settings. To address these issues, we propose an ensemble approach using
Low-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique.
Critically, these low-rank adapters represent a very small number of
parameters, orders of magnitude less than the underlying pre-trained model.
Thus, it is possible to construct large ensembles of LoRA adapters with almost
the same computational overhead as using the original model. We find that LoRA
ensembles, applied on its own or on top of pre-existing regularization
techniques, gives consistent improvements in predictive accuracy and
uncertainty quantification.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00036" title="Abstract">arXiv:2310.00036</a> [<a href="/pdf/2310.00036" title="Download PDF">pdf</a>, <a href="/format/2310.00036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning  Platform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shengyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+J">Jiayi Weng</a>, 
<a href="/search/cs?searchtype=author&query=Charakorn%2C+R">Rujikorn Charakorn</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Min Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhongwen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Onta%C3%B1%C3%B3n%2C+S">Santiago Onta&#xf1;&#xf3;n</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Distributed Deep Reinforcement Learning (DRL) aims to leverage more
computational resources to train autonomous agents with less training time.
Despite recent progress in the field, reproducibility issues have not been
sufficiently explored. This paper first shows that the typical actor-learner
framework can have reproducibility issues even if hyperparameters are
controlled. We then introduce Cleanba, a new open-source platform for
distributed DRL that proposes a highly reproducible architecture. Cleanba
implements highly optimized distributed variants of PPO and IMPALA. Our Atari
experiments show that these variants can obtain equivalent or higher scores
than strong IMPALA baselines in moolib and torchbeast and PPO baseline in
CleanRL. However, Cleanba variants present 1) shorter training time and 2) more
reproducible learning curves in different hardware settings. Cleanba's source
code is available at \url{https://github.com/vwxyzjn/cleanba}
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00041" title="Abstract">arXiv:2310.00041</a> [<a href="/pdf/2310.00041" title="Download PDF">pdf</a>, <a href="/format/2310.00041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning Clifford invariants of ADE Coxeter elements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dechant%2C+P">Pierre-Philippe Dechant</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yang-Hui He</a>, 
<a href="/search/cs?searchtype=author&query=Heyes%2C+E">Elli Heyes</a>, 
<a href="/search/cs?searchtype=author&query=Hirst%2C+E">Edward Hirst</a>, 
<a href="/search/cs?searchtype=author&query=Riabchenko%2C+D">Dmitrii Riabchenko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 16 Figures, 12 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; High Energy Physics - Theory (hep-th); Mathematical Physics (math-ph); Group Theory (math.GR); Representation Theory (math.RT)

</div>
<p class="mathjax">There has been recent interest in novel Clifford geometric invariants of
linear transformations. This motivates the investigation of such invariants for
a certain type of geometric transformation of interest in the context of root
systems, reflection groups, Lie groups and Lie algebras: the Coxeter
transformations. We perform exhaustive calculations of all Coxeter
transformations for $A_8$, $D_8$ and $E_8$ for a choice of basis of simple
roots and compute their invariants, using high-performance computing. This
computational algebra paradigm generates a dataset that can then be mined using
techniques from data science such as supervised and unsupervised machine
learning. In this paper we focus on neural network classification and principal
component analysis. Since the output -- the invariants -- is fully determined
by the choice of simple roots and the permutation order of the corresponding
reflections in the Coxeter element, we expect huge degeneracy in the mapping.
This provides the perfect setup for machine learning, and indeed we see that
the datasets can be machine learned to very high accuracy. This paper is a
pump-priming study in experimental mathematics using Clifford algebras, showing
that such Clifford algebraic datasets are amenable to machine learning, and
shedding light on relationships between these novel and other well-known
geometric invariants and also giving rise to analytic results.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00057" title="Abstract">arXiv:2310.00057</a> [<a href="/pdf/2310.00057" title="Download PDF">pdf</a>, <a href="/format/2310.00057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fusing simulation and monitoring data for real-time settlement  prediction during tunnel construction: A multi-fidelity deep operator network  (DeepONet)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+B+T">Ba Trung Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yong Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Meschke%2C+G">G&#xfc;nther Meschke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Ground settlement prediction during the process of mechanized tunneling is of
paramount importance and remains a challenging research topic. Typically, two
different paradigms have been created: a physics-driven approach utilizing
advanced process-oriented numerical models for settlement prediction, and a
data-driven approach employing machine learning techniques to establish
mappings between influencing factors and ground settlement. To integrate the
advantages of both approaches and assimilate the data from different sources,
we propose a multi-fidelity deep operator network (DeepONet) framework,
leveraging the recently developed operator learning methods. The presented
framework comprises two components: a low-fidelity subnet that captures the
fundamental ground settlement patterns obtained from finite element
simulations, and a high-fidelity subnet that learns the nonlinear correlation
between numerical models and real engineering monitoring data. A pre-processing
strategy for causality is adopted to consider the spatio-temporal
characteristic of the settlement during tunnel excavation. Transfer learning is
utilized to reduce the training cost for the low-fidelity subnet. The results
show that the proposed method can effectively capture the physical laws
presented by numerical simulations and accurately fit measured data as well.
Remarkably, even with very limited noisy monitoring data, our model can achieve
rapid, accurate, and robust prediction of the full-field ground settlement in
real-time mechanized tunneling.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00068" title="Abstract">arXiv:2310.00068</a> [<a href="/pdf/2310.00068" title="Download PDF">pdf</a>, <a href="/format/2310.00068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emotional Listener Portrait: Realistic Listener Motion Simulation in  Conversation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Luchuan Song</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+G">Guojun Yin</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhenchao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiaoyi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenliang Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
<p class="mathjax">Listener head generation centers on generating non-verbal behaviors (e.g.,
smile) of a listener in reference to the information delivered by a speaker. A
significant challenge when generating such responses is the non-deterministic
nature of fine-grained facial expressions during a conversation, which varies
depending on the emotions and attitudes of both the speaker and the listener.
To tackle this problem, we propose the Emotional Listener Portrait (ELP), which
treats each fine-grained facial motion as a composition of several discrete
motion-codewords and explicitly models the probability distribution of the
motions under different emotion in conversation. Benefiting from the
``explicit'' and ``discrete'' design, our ELP model can not only automatically
generate natural and diverse responses toward a given speaker via sampling from
the learned distribution but also generate controllable responses with a
predetermined attitude. Under several quantitative metrics, our ELP exhibits
significant improvements compared to previous methods.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00070" title="Abstract">arXiv:2310.00070</a> [<a href="/pdf/2310.00070" title="Download PDF">pdf</a>, <a href="/format/2310.00070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Explainability: Utilizing Explainable Machine Learning in  Bypassing IoT Botnet Detection Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alani%2C+M+M">Mohammed M. Alani</a>, 
<a href="/search/cs?searchtype=author&query=Mashatan%2C+A">Atefeh Mashatan</a>, 
<a href="/search/cs?searchtype=author&query=Miri%2C+A">Ali Miri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Botnet detection based on machine learning have witnessed significant leaps
in recent years, with the availability of large and reliable datasets that are
extracted from real-life scenarios. Consequently, adversarial attacks on
machine learning-based cybersecurity systems are posing a significant threat to
the practicality of these solutions. In this paper, we introduce a novel attack
that utilizes machine learning model's explainability in evading detection by
botnet detection systems. The proposed attack utilizes information obtained
from model's explainability to build adversarial samples that can evade
detection in a blackbox setting. The proposed attack was tested on a trained
IoT botnet detection systems and was capable of bypassing the botnet detection
with 0% detection by altering one feature only to generate the adversarial
samples.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00073" title="Abstract">arXiv:2310.00073</a> [<a href="/pdf/2310.00073" title="Download PDF">pdf</a>, <a href="/format/2310.00073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Objective Sparse Sensing with Ergodic Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rao%2C+A">Ananya Rao</a>, 
<a href="/search/cs?searchtype=author&query=Choset%2C+H">Howie Choset</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We consider a search problem where a robot has one or more types of sensors,
each suited to detecting different types of targets or target information.
Often, information in the form of a distribution of possible target locations,
or locations of interest, may be available to guide the search. When multiple
types of information exist, then a distribution for each type of information
must also exist, thereby making the search problem that uses these
distributions to guide the search a multi-objective one. In this paper, we
consider a multi-objective search problem when the cost to use a sensor is
limited. To this end, we leverage the ergodic metric, which drives agents to
spend time in regions proportional to the expected amount of information there.
We define the multi-objective sparse sensing ergodic (MO-SS-E) metric in order
to optimize when and where each sensor measurement should be taken while
planning trajectories that balance the multiple objectives. We observe that our
approach maintains coverage performance as the number of samples taken
considerably degrades. Further empirical results on different multi-agent
problem setups demonstrate the applicability of our approach for both
homogeneous and heterogeneous multi-agent teams.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00074" title="Abstract">arXiv:2310.00074</a> [<a href="/pdf/2310.00074" title="Download PDF">pdf</a>, <a href="/format/2310.00074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SocREval: Large Language Models with the Socratic Method for  Reference-Free Reasoning Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+H">Hangfeng He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+D">Dan Roth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">To comprehensively assess the capacity of current models for complex
reasoning, it is crucial to assess their step-by-step reasoning in a scalable
manner. Established reference-based evaluation metrics rely on human-annotated
reasoning chains to assess the model-derived chains. However, such
``gold-standard'' human-written reasoning chains may not be unique and their
acquisition is often labor-intensive. Existing reference-free reasoning metrics
eliminate the need for human-crafted reasoning chains as references, but they
typically require fine-tuning on datasets with human-derived reasoning chains,
which complicates the process and raises concerns regarding generalizability
across diverse datasets. To address these challenges, we harness GPT-4 to
automatically evaluate reasoning chain quality, obviating the need for
human-crafted references. Leveraging the Socratic method, we devise tailored
prompts to enhance reference-free reasoning evaluation, which we term SocREval
(Socratic method for Reasoning Evaluation). Empirical results from four human
annotated datasets reveal that SocREval significantly improves GPT-4's
performance, surpassing existing reference-free and reference-based reasoning
evaluation metrics. Beyond its demonstrated efficacy, our proposed framework,
large language models (LLMs) with the Socratic method, proves to be both
cost-efficient and robust to prompt writing and example selection, as
substantiated by our in-depth analysis.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00076" title="Abstract">arXiv:2310.00076</a> [<a href="/pdf/2310.00076" title="Download PDF">pdf</a>, <a href="/format/2310.00076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustness of AI-Image Detectors: Fundamental Limits and Practical  Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saberi%2C+M">Mehrdad Saberi</a>, 
<a href="/search/cs?searchtype=author&query=Sadasivan%2C+V+S">Vinu Sankar Sadasivan</a>, 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+K">Keivan Rezaei</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Aounon Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Chegini%2C+A">Atoosa Chegini</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In light of recent advancements in generative AI models, it has become
essential to distinguish genuine content from AI-generated one to prevent the
malicious usage of fake materials as authentic ones and vice versa. Various
techniques have been introduced for identifying AI-generated images, with
watermarking emerging as a promising approach. In this paper, we analyze the
robustness of various AI-image detectors including watermarking and
classifier-based deepfake detectors. For watermarking methods that introduce
subtle image perturbations (i.e., low perturbation budget methods), we reveal a
fundamental trade-off between the evasion error rate (i.e., the fraction of
watermarked images detected as non-watermarked ones) and the spoofing error
rate (i.e., the fraction of non-watermarked images detected as watermarked
ones) upon an application of a diffusion purification attack. In this regime,
we also empirically show that diffusion purification effectively removes
watermarks with minimal changes to images. For high perturbation watermarking
methods where notable changes are applied to images, the diffusion purification
attack is not effective. In this case, we develop a model substitution
adversarial attack that can successfully remove watermarks. Moreover, we show
that watermarking methods are vulnerable to spoofing attacks where the attacker
aims to have real images (potentially obscene) identified as watermarked ones,
damaging the reputation of the developers. In particular, by just having
black-box access to the watermarking method, we show that one can generate a
watermarked noise image which can be added to the real images to have them
falsely flagged as watermarked ones. Finally, we extend our theory to
characterize a fundamental trade-off between the robustness and reliability of
classifier-based deep fake detectors and demonstrate it through experiments.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00077" title="Abstract">arXiv:2310.00077</a> [<a href="/pdf/2310.00077" title="Download PDF">pdf</a>, <a href="/format/2310.00077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-budget Black-box Optimization Algorithms Evaluated on BBOB and  OpenAI Gym
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raponi%2C+E">Elena Raponi</a>, 
<a href="/search/cs?searchtype=author&query=Carraz%2C+N+R">Nathanael Rakotonirina Carraz</a>, 
<a href="/search/cs?searchtype=author&query=Rapin%2C+J">J&#xe9;r&#xe9;my Rapin</a>, 
<a href="/search/cs?searchtype=author&query=Doerr%2C+C">Carola Doerr</a>, 
<a href="/search/cs?searchtype=author&query=Teytaud%2C+O">Olivier Teytaud</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The growing ubiquity of machine learning (ML) has led it to enter various
areas of computer science, including black-box optimization (BBO). Recent
research is particularly concerned with Bayesian optimization (BO). BO-based
algorithms are popular in the ML community, as they are used for hyperparameter
optimization and more generally for algorithm configuration. However, their
efficiency decreases as the dimensionality of the problem and the budget of
evaluations increase. Meanwhile, derivative-free optimization methods have
evolved independently in the optimization community. Therefore, we urge to
understand whether cross-fertilization is possible between the two communities,
ML and BBO, i.e., whether algorithms that are heavily used in ML also work well
in BBO and vice versa. Comparative experiments often involve rather small
benchmarks and show visible problems in the experimental setup, such as poor
initialization of baselines, overfitting due to problem-specific setting of
hyperparameters, and low statistical significance.
<br />With this paper, we update and extend a comparative study presented by Hutter
et al. in 2013. We compare BBO tools for ML with more classical heuristics,
first on the well-known BBOB benchmark suite from the COCO environment and then
on Direct Policy Search for OpenAI Gym, a reinforcement learning benchmark. Our
results confirm that BO-based optimizers perform well on both benchmarks when
budgets are limited, albeit with a higher computational cost, while they are
often outperformed by algorithms from other families when the evaluation budget
becomes larger. We also show that some algorithms from the BBO community
perform surprisingly well on ML tasks.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00078" title="Abstract">arXiv:2310.00078</a> [<a href="/pdf/2310.00078" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing Semantic Ambiguity of the Materials Science Ontologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McClellan%2C+S">Scott McClellan</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+Y">Yuan An</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xintong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xia Lin</a>, 
<a href="/search/cs?searchtype=author&query=Greenberg%2C+J">Jane Greenberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, International Society for Knowledge Organization (ISKO) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">Growth in computational materials science and initiatives such as the
Materials Genome Initiative (MGI) and the European Materials Modelling Council
(EMMC) has motivated the development and application of ontologies. A key
factor has been increased adoption of the FAIR principles, making research data
findable, accessible, interoperable, and reusable (Wilkinson et al. 2016). This
paper characterizes semantic interoperability among a subset of materials
science ontologies in the MatPortal repository. Background context covers
semantic interoperability, ontological commitment, and the materials science
ontology landscape. The research focused on MatPortal's two interoperability
protocols: LOOM term matching and URI matching. Results report the degree of
overlap and demonstrate the different types of ambiguity among ontologies. The
discussion considers implications for FAIR and AI, and the conclusion highlight
key findings and next steps.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00080" title="Abstract">arXiv:2310.00080</a> [<a href="/pdf/2310.00080" title="Download PDF">pdf</a>, <a href="/format/2310.00080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Model Brushless Electric Motors for the Design of Lightweight  Robotic Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+U+H">Ung Hee Lee</a>, 
<a href="/search/cs?searchtype=author&query=Shepherd%2C+T">Tor Shepherd</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sangbae Kim</a>, 
<a href="/search/cs?searchtype=author&query=De%2C+A">Avik De</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hao Su</a>, 
<a href="/search/cs?searchtype=author&query=Gregg%2C+R">Robert Gregg</a>, 
<a href="/search/cs?searchtype=author&query=Mooney%2C+L">Luke Mooney</a>, 
<a href="/search/cs?searchtype=author&query=Rouse%2C+E">Elliott Rouse</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">A key step in the development of lightweight, high performance robotic
systems is the modeling and selection of permanent magnet brushless direct
current (BLDC) electric motors. Typical modeling analyses are completed a
priori, and provide insight for properly sizing a motor for an application,
specifying the required operating voltage and current, as well as assessing the
thermal response and other design attributes (e.g.transmission ratio). However,
to perform these modeling analyses, proper information about the motor's
characteristics are needed, which are often obtained from manufacturer
datasheets. Through our own experience and communications with manufacturers,
we have noticed a lack of clarity and standardization in modeling BLDC motors,
compounded by vague or inconsistent terminology used in motor datasheets. The
purpose of this tutorial is to concisely describe the governing equations for
BLDC motor analyses used in the design process, as well as highlight potential
errors that can arise from incorrect usage. We present a power-invariant
conversion from phase and line-to-line reference frames to a familiar q-axis DC
motor representation, which provides a ``brushed'' analogue of a three phase
BLDC motor that is convenient for analysis and design. We highlight potential
errors including incorrect calculations of winding resistive heat loss,
improper estimation of motor torque via the motor's torque constant, and
incorrect estimation of the required bus voltage or resulting angular velocity
limitations. A unified and condensed set of governing equations is available
for designers in the Appendix. The intent of this work is to provide a
consolidated mathematical foundation for modeling BLDC motors that addresses
existing confusion and fosters high performance designs of future robotic
systems.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00085" title="Abstract">arXiv:2310.00085</a> [<a href="/pdf/2310.00085" title="Download PDF">pdf</a>, <a href="/format/2310.00085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PEACE: Prompt Engineering Automation for CLIPSeg Enhancement in Aerial  Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bong%2C+H+M">Haechan Mark Bong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rongge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=de+Azambuja%2C+R">Ricardo de Azambuja</a>, 
<a href="/search/cs?searchtype=author&query=Beltrame%2C+G">Giovanni Beltrame</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICRA 2024. arXiv admin note: substantial text overlap with <a href="/abs/2308.11471">arXiv:2308.11471</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">From industrial to space robotics, safe landing is an essential component for
flight operations. With the growing interest in artificial intelligence, we
direct our attention to learning based safe landing approaches. This paper
extends our previous work, DOVESEI, which focused on a reactive UAV system by
harnessing the capabilities of open vocabulary image segmentation. Prompt-based
safe landing zone segmentation using an open vocabulary based model is no more
just an idea, but proven to be feasible by the work of DOVESEI. However, a
heuristic selection of words for prompt is not a reliable solution since it
cannot take the changing environment into consideration and detrimental
consequences can occur if the observed environment is not well represented by
the given prompt. Therefore, we introduce PEACE (Prompt Engineering Automation
for CLIPSeg Enhancement), powering DOVESEI to automate the prompt generation
and engineering to adapt to data distribution shifts. Our system is capable of
performing safe landing operations with collision avoidance at altitudes as low
as 20 meters using only monocular cameras and image segmentation. We take
advantage of DOVESEI's dynamic focus to circumvent abrupt fluctuations in the
terrain segmentation between frames in a video stream. PEACE shows promising
improvements in prompt generation and engineering for aerial images compared to
the standard prompt used for CLIP and CLIPSeg. Combining DOVESEI and PEACE, our
system was able improve successful safe landing zone selections by 58.62%
compared to using only DOVESEI. All the source code is open source and
available online.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00090" title="Abstract">arXiv:2310.00090</a> [<a href="/pdf/2310.00090" title="Download PDF">pdf</a>, <a href="/ps/2310.00090" title="Download PostScript">ps</a>, <a href="/format/2310.00090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Counting of Involutory MDS Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Samanta%2C+S">Susanta Samanta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">The optimal branch number of MDS matrices has established their prominence in
the design of diffusion layers for various block ciphers and hash functions.
Consequently, several matrix structures have been proposed for designing MDS
matrices, including Hadamard and circulant matrices. In this paper, we first
provide the count of Hadamard MDS matrices of order $4$ over the field
$\mathbb{F}_{2^r}$. Subsequently, we present the counts of order $2$ MDS
matrices and order $2$ involutory MDS matrices over the field
$\mathbb{F}_{2^r}$. Finally, leveraging these counts of order $2$ matrices, we
derive an upper bound for the number of all involutory MDS matrices of order
$4$ over $\mathbb{F}_{2^r}$.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00091" title="Abstract">arXiv:2310.00091</a> [<a href="/pdf/2310.00091" title="Download PDF">pdf</a>, <a href="/format/2310.00091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Automated Accessibility Report Generation for Mobile Apps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Swearngin%2C+A">Amanda Swearngin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jason Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gomez%2C+E">Esteban Gomez</a>, 
<a href="/search/cs?searchtype=author&query=Coughenour%2C+J">Jen Coughenour</a>, 
<a href="/search/cs?searchtype=author&query=Stukenborg%2C+R">Rachel Stukenborg</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+B">Bhavya Garg</a>, 
<a href="/search/cs?searchtype=author&query=Hughes%2C+G">Greg Hughes</a>, 
<a href="/search/cs?searchtype=author&query=Hilliard%2C+A">Adriana Hilliard</a>, 
<a href="/search/cs?searchtype=author&query=Bigham%2C+J">Jeffrey Bigham</a>, 
<a href="/search/cs?searchtype=author&query=Nichols%2C+J">Jeffrey Nichols</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Many apps have basic accessibility issues, like missing labels or low
contrast. Automated tools can help app developers catch basic issues, but can
be laborious or require writing dedicated tests. We propose a system, motivated
by a collaborative process with accessibility stakeholders at a large
technology company, to generate whole app accessibility reports by combining
varied data collection methods (e.g., app crawling, manual recording) with an
existing accessibility scanner. Many such scanners are based on single-screen
scanning, and a key problem in whole app accessibility reporting is to
effectively de-duplicate and summarize issues collected across an app. To this
end, we developed a screen grouping model with 96.9% accuracy (88.8% F1-score)
and UI element matching heuristics with 97% accuracy (98.2% F1-score). We
combine these technologies in a system to report and summarize unique issues
across an app, and enable a unique pixel-based ignore feature to help engineers
and testers better manage reported issues across their app's lifetime. We
conducted a qualitative evaluation with 18 accessibility-focused engineers and
testers which showed this system can enhance their existing accessibility
testing toolkit and address key limitations in current accessibility scanning
tools.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00092" title="Abstract">arXiv:2310.00092</a> [<a href="/pdf/2310.00092" title="Download PDF">pdf</a>, <a href="/format/2310.00092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Voice2Action: Language Models as Agent for Efficient Real-Time  Interaction in Virtual Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yang Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Large Language Models (LLMs) are trained and aligned to follow natural
language instructions with only a handful of examples, and they are prompted as
task-driven autonomous agents to adapt to various sources of execution
environments. However, deploying agent LLMs in virtual reality (VR) has been
challenging due to the lack of efficiency in online interactions and the
complex manipulation categories in 3D environments. In this work, we propose
Voice2Action, a framework that hierarchically analyzes customized voice signals
and textual commands through action and entity extraction and divides the
execution tasks into canonical interaction subsets in real-time with error
prevention from environment feedback. Experiment results in an urban
engineering VR environment with synthetic instruction data show that
Voice2Action can perform more efficiently and accurately than approaches
without optimizations.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00093" title="Abstract">arXiv:2310.00093</a> [<a href="/pdf/2310.00093" title="Download PDF">pdf</a>, <a href="/format/2310.00093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DataDAM: Efficient Dataset Distillation with Attention Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sajedi%2C+A">Ahmad Sajedi</a>, 
<a href="/search/cs?searchtype=author&query=Khaki%2C+S">Samir Khaki</a>, 
<a href="/search/cs?searchtype=author&query=Amjadian%2C+E">Ehsan Amjadian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L+Z">Lucy Z. Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lawryshyn%2C+Y+A">Yuri A. Lawryshyn</a>, 
<a href="/search/cs?searchtype=author&query=Plataniotis%2C+K+N">Konstantinos N. Plataniotis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in International Conference in Computer Vision (ICCV) 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> booktitle = Proceedings of the IEEE/CVF International Conference
  on Computer Vision (ICCV) month = October year = 2023 pages = 17097-17107
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Researchers have long tried to minimize training costs in deep learning while
maintaining strong generalization across diverse datasets. Emerging research on
dataset distillation aims to reduce training costs by creating a small
synthetic set that contains the information of a larger real dataset and
ultimately achieves test accuracy equivalent to a model trained on the whole
dataset. Unfortunately, the synthetic data generated by previous methods are
not guaranteed to distribute and discriminate as well as the original training
data, and they incur significant computational costs. Despite promising
results, there still exists a significant performance gap between models
trained on condensed synthetic sets and those trained on the whole dataset. In
this paper, we address these challenges using efficient Dataset Distillation
with Attention Matching (DataDAM), achieving state-of-the-art performance while
reducing training costs. Specifically, we learn synthetic images by matching
the spatial attention maps of real and synthetic data generated by different
layers within a family of randomly initialized neural networks. Our method
outperforms the prior methods on several datasets, including CIFAR10/100,
TinyImageNet, ImageNet-1K, and subsets of ImageNet-1K across most of the
settings, and achieves improvements of up to 6.5% and 4.1% on CIFAR100 and
ImageNet-1K, respectively. We also show that our high-quality distilled images
have practical benefits for downstream applications, such as continual learning
and neural architecture search.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00096" title="Abstract">arXiv:2310.00096</a> [<a href="/pdf/2310.00096" title="Download PDF">pdf</a>, <a href="/format/2310.00096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Few-Call Model Stealing via Active Self-Paced Knowledge  Distillation and Diffusion-Based Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hondru%2C+V">Vlad Hondru</a>, 
<a href="/search/cs?searchtype=author&query=Ionescu%2C+R+T">Radu Tudor Ionescu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Diffusion models showcased strong capabilities in image synthesis, being used
in many computer vision tasks with great success. To this end, we propose to
explore a new use case, namely to copy black-box classification models without
having access to the original training data, the architecture, and the weights
of the model, \ie~the model is only exposed through an inference API. More
specifically, we can only observe the (soft or hard) labels for some image
samples passed as input to the model. Furthermore, we consider an additional
constraint limiting the number of model calls, mostly focusing our research on
few-call model stealing. In order to solve the model extraction task given the
applied restrictions, we propose the following framework. As training data, we
create a synthetic data set (called proxy data set) by leveraging the ability
of diffusion models to generate realistic and diverse images. Given a maximum
number of allowed API calls, we pass the respective number of samples through
the black-box model to collect labels. Finally, we distill the knowledge of the
black-box teacher (attacked model) into a student model (copy of the attacked
model), harnessing both labeled and unlabeled data generated by the diffusion
model. We employ a novel active self-paced learning framework to make the most
of the proxy data during distillation. Our empirical results on two data sets
confirm the superiority of our framework over two state-of-the-art methods in
the few-call model extraction scenario.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00098" title="Abstract">arXiv:2310.00098</a> [<a href="/pdf/2310.00098" title="Download PDF">pdf</a>, <a href="/format/2310.00098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning with Differential Privacy for End-to-End Speech  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pelikan%2C+M">Martin Pelikan</a>, 
<a href="/search/cs?searchtype=author&query=Azam%2C+S+S">Sheikh Shams Azam</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+V">Vitaly Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Silovsky%2C+J+%22">Jan &quot;Honza&quot; Silovsky</a>, 
<a href="/search/cs?searchtype=author&query=Talwar%2C+K">Kunal Talwar</a>, 
<a href="/search/cs?searchtype=author&query=Likhomanenko%2C+T">Tatiana Likhomanenko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
<p class="mathjax">While federated learning (FL) has recently emerged as a promising approach to
train machine learning models, it is limited to only preliminary explorations
in the domain of automatic speech recognition (ASR). Moreover, FL does not
inherently guarantee user privacy and requires the use of differential privacy
(DP) for robust privacy guarantees. However, we are not aware of prior work on
applying DP to FL for ASR. In this paper, we aim to bridge this research gap by
formulating an ASR benchmark for FL with DP and establishing the first
baselines. First, we extend the existing research on FL for ASR by exploring
different aspects of recent $\textit{large end-to-end transformer models}$:
architecture design, seed models, data heterogeneity, domain shift, and impact
of cohort size. With a $\textit{practical}$ number of central aggregations we
are able to train $\textbf{FL models}$ that are \textbf{nearly optimal} even
with heterogeneous data, a seed model from another domain, or no pre-trained
seed model. Second, we apply DP to FL for ASR, which is non-trivial since DP
noise severely affects model training, especially for large transformer models,
due to highly imbalanced gradients in the attention block. We counteract the
adverse effect of DP noise by reviving per-layer clipping and explaining why
its effect is more apparent in our case than in the prior work. Remarkably, we
achieve user-level ($7.2$, $10^{-9}$)-$\textbf{DP}$ (resp. ($4.5$,
$10^{-9}$)-$\textbf{DP}$) with a 1.3% (resp. 4.6%) absolute drop in the word
error rate for extrapolation to high (resp. low) population scale for
$\textbf{FL with DP in ASR}$.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00099" title="Abstract">arXiv:2310.00099</a> [<a href="/pdf/2310.00099" title="Download PDF">pdf</a>, <a href="/format/2310.00099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising and Selecting Pseudo-Heatmaps for Semi-Supervised Human Pose  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhuoran Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Manchen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yanbei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Favaro%2C+P">Paolo Favaro</a>, 
<a href="/search/cs?searchtype=author&query=Modolo%2C+D">Davide Modolo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose a new semi-supervised learning design for human pose estimation
that revisits the popular dual-student framework and enhances it two ways.
First, we introduce a denoising scheme to generate reliable pseudo-heatmaps as
targets for learning from unlabeled data. This uses multi-view augmentations
and a threshold-and-refine procedure to produce a pool of pseudo-heatmaps.
Second, we select the learning targets from these pseudo-heatmaps guided by the
estimated cross-student uncertainty. We evaluate our proposed method on
multiple evaluation setups on the COCO benchmark. Our results show that our
model outperforms previous state-of-the-art semi-supervised pose estimators,
especially in extreme low-data regime. For example with only 0.5K labeled
images our method is capable of surpassing the best competitor by 7.22 mAP
(+25% absolute improvement). We also demonstrate that our model can learn
effectively from unlabeled data in the wild to further boost its generalization
and performance.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00100" title="Abstract">arXiv:2310.00100</a> [<a href="/pdf/2310.00100" title="Download PDF">pdf</a>, <a href="/format/2310.00100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilingual Natural Language ProcessingModel for Radiology Reports --  The Summary is all you need!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lindo%2C+M">Mariana Lindo</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+A+S">Ana Sofia Santos</a>, 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+A">Andr&#xe9; Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianning Li</a>, 
<a href="/search/cs?searchtype=author&query=Luijten%2C+G">Gijs Luijten</a>, 
<a href="/search/cs?searchtype=author&query=Correia%2C+G">Gustavo Correia</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Moon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kleesiek%2C+J">Jens Kleesiek</a>, 
<a href="/search/cs?searchtype=author&query=Egger%2C+J">Jan Egger</a>, 
<a href="/search/cs?searchtype=author&query=Alves%2C+V">Victor Alves</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 figure, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The impression section of a radiology report summarizes important radiology
findings and plays a critical role in communicating these findings to
physicians. However, the preparation of these summaries is time-consuming and
error-prone for radiologists. Recently, numerous models for radiology report
summarization have been developed. Nevertheless, there is currently no model
that can summarize these reports in multiple languages. Such a model could
greatly improve future research and the development of Deep Learning models
that incorporate data from patients with different ethnic backgrounds. In this
study, the generation of radiology impressions in different languages was
automated by fine-tuning a model, publicly available, based on a multilingual
text-to-text Transformer to summarize findings available in English,
Portuguese, and German radiology reports. In a blind test, two board-certified
radiologists indicated that for at least 70% of the system-generated summaries,
the quality matched or exceeded the corresponding human-written summaries,
suggesting substantial clinical reliability. Furthermore, this study showed
that the multilingual model outperformed other models that specialized in
summarizing radiology reports in only one language, as well as models that were
not specifically designed for summarizing radiology reports, such as ChatGPT.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00105" title="Abstract">arXiv:2310.00105</a> [<a href="/pdf/2310.00105" title="Download PDF">pdf</a>, <a href="/format/2310.00105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Space Symmetry Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianke Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dehmamy%2C+N">Nima Dehmamy</a>, 
<a href="/search/cs?searchtype=author&query=Walters%2C+R">Robin Walters</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Rose Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Equivariant neural networks require explicit knowledge of the symmetry group.
Automatic symmetry discovery methods aim to relax this constraint and learn
invariance and equivariance from data. However, existing symmetry discovery
methods are limited to linear symmetries in their search space and cannot
handle the complexity of symmetries in real-world, often high-dimensional data.
We propose a novel generative model, Latent LieGAN (LaLiGAN), which can
discover nonlinear symmetries from data. It learns a mapping from data to a
latent space where the symmetries become linear and simultaneously discovers
symmetries in the latent space. Theoretically, we show that our method can
express any nonlinear symmetry under certain conditions. Experimentally, our
method can capture the intrinsic symmetry in high-dimensional observations,
which results in a well-structured latent space that is useful for other
downstream tasks. We demonstrate the use cases for LaLiGAN in improving
equation discovery and long-term forecasting for various dynamical systems.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00106" title="Abstract">arXiv:2310.00106</a> [<a href="/pdf/2310.00106" title="Download PDF">pdf</a>, <a href="/format/2310.00106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FashionFlow: Leveraging Diffusion Models for Dynamic Fashion Video  Synthesis from Static Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Islam%2C+T">Tasin Islam</a>, 
<a href="/search/cs?searchtype=author&query=Miron%2C+A">Alina Miron</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">XiaoHui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongmin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Our study introduces a new image-to-video generator called FashionFlow. By
utilising a diffusion model, we are able to create short videos from still
images. Our approach involves developing and connecting relevant components
with the diffusion model, which sets our work apart. The components include the
use of pseudo-3D convolutional layers to generate videos efficiently. VAE and
CLIP encoders capture vital characteristics from still images to influence the
diffusion model. Our research demonstrates a successful synthesis of fashion
videos featuring models posing from various angles, showcasing the fit and
appearance of the garment. Our findings hold great promise for improving and
enhancing the shopping experience for the online fashion industry.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00108" title="Abstract">arXiv:2310.00108</a> [<a href="/pdf/2310.00108" title="Download PDF">pdf</a>, <a href="/format/2310.00108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical Membership Inference Attacks Against Large-Scale Multi-Modal  Models: A Pilot Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+M">Myeongseob Ko</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Ming Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenguang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+R">Ruoxi Jia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Computer Vision (ICCV) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Membership inference attacks (MIAs) aim to infer whether a data point has
been used to train a machine learning model. These attacks can be employed to
identify potential privacy vulnerabilities and detect unauthorized use of
personal data. While MIAs have been traditionally studied for simple
classification models, recent advancements in multi-modal pre-training, such as
CLIP, have demonstrated remarkable zero-shot performance across a range of
computer vision tasks. However, the sheer scale of data and models presents
significant computational challenges for performing the attacks.
<br />This paper takes a first step towards developing practical MIAs against
large-scale multi-modal models. We introduce a simple baseline strategy by
thresholding the cosine similarity between text and image features of a target
point and propose further enhancing the baseline by aggregating cosine
similarity across transformations of the target. We also present a new weakly
supervised attack method that leverages ground-truth non-members (e.g.,
obtained by using the publication date of a target model and the timestamps of
the open data) to further enhance the attack. Our evaluation shows that CLIP
models are susceptible to our attack strategies, with our simple baseline
achieving over $75\%$ membership identification accuracy. Furthermore, our
enhanced attacks outperform the baseline across multiple models and datasets,
with the weakly supervised attack demonstrating an average-case performance
improvement of $17\%$ and being at least $7$X more effective at low
false-positive rates. These findings highlight the importance of protecting the
privacy of multi-modal foundational models, which were previously assumed to be
less susceptible to MIAs due to less overfitting. Our code is available at
https://github.com/ruoxi-jia-group/CLIP-MIA.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00109" title="Abstract">arXiv:2310.00109</a> [<a href="/pdf/2310.00109" title="Download PDF">pdf</a>, <a href="/format/2310.00109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of  Things
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alam%2C+S">Samiul Alam</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Tiantian Feng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Hui Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhichao Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">JeongGil Ko</a>, 
<a href="/search/cs?searchtype=author&query=Somasundaram%2C+K">Kiran Somasundaram</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+S+S">Shrikanth S. Narayanan</a>, 
<a href="/search/cs?searchtype=author&query=Avestimehr%2C+S">Salman Avestimehr</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Digital Libraries (cs.DL)

</div>
<p class="mathjax">There is a significant relevance of federated learning (FL) in the realm of
Artificial Intelligence of Things (AIoT). However, most existing FL works are
not conducted on datasets collected from authentic IoT devices that capture
unique modalities and inherent challenges of IoT data. In this work, we
introduce FedAIoT, an FL benchmark for AIoT to fill this critical gap. FedAIoT
includes eight datatsets collected from a wide range of IoT devices. These
datasets cover unique IoT modalities and target representative applications of
AIoT. FedAIoT also includes a unified end-to-end FL framework for AIoT that
simplifies benchmarking the performance of the datasets. Our benchmark results
shed light on the opportunities and challenges of FL for AIoT. We hope FedAIoT
could serve as an invaluable resource to foster advancements in the important
field of FL for AIoT. The repository of FedAIoT is maintained at
https://github.com/AIoT-MLSys-Lab/FedAIoT.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00111" title="Abstract">arXiv:2310.00111</a> [<a href="/pdf/2310.00111" title="Download PDF">pdf</a>, <a href="/format/2310.00111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory-efficient compression of $\mathcal{DH}^2$-matrices for  high-frequency problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=B%C3%B6rm%2C+S">Steffen B&#xf6;rm</a>, 
<a href="/search/math?searchtype=author&query=Henningsen%2C+J">Janne Henningsen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Directional interpolation is a fast and efficient compression technique for
high-frequency Helmholtz boundary integral equations, but requires a very large
amount of storage in its original form. Algebraic recompression can
significantly reduce the storage requirements and speed up the solution process
accordingly. During the recompression process, weight matrices are required to
correctly measure the influence of different basis vectors on the final result,
and for highly accurate approximations, these weight matrices require more
storage than the final compressed matrix.
<br />We present a compression method for the weight matrices and demonstrate that
it introduces only a controllable error to the overall approximation. Numerical
experiments show that the new method leads to a significant reduction in
storage requirements.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00112" title="Abstract">arXiv:2310.00112</a> [<a href="/pdf/2310.00112" title="Download PDF">pdf</a>, <a href="/format/2310.00112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning for Node Selection in Branch-and-Bound
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mattick%2C+A">Alexander Mattick</a>, 
<a href="/search/cs?searchtype=author&query=Mutschler%2C+C">Christopher Mutschler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A big challenge in branch and bound lies in identifying the optimal node
within the search tree from which to proceed. Current state-of-the-art
selectors utilize either hand-crafted ensembles that automatically switch
between naive sub-node selectors, or learned node selectors that rely on
individual node data. We propose a novel bi-simulation technique that uses
reinforcement learning (RL) while considering the entire tree state, rather
than just isolated nodes. To achieve this, we train a graph neural network that
produces a probability distribution based on the path from the model's root to
its ``to-be-selected'' leaves. Modelling node-selection as a probability
distribution allows us to train the model using state-of-the-art RL techniques
that capture both intrinsic node-quality and node-evaluation costs. Our method
induces a high quality node selection policy on a set of varied and complex
problem sets, despite only being trained on specially designed, synthetic TSP
instances. Experiments on several benchmarks show significant improvements in
optimality gap reductions and per-node efficiency under strict time
constraints.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00113" title="Abstract">arXiv:2310.00113</a> [<a href="/pdf/2310.00113" title="Download PDF">pdf</a>, <a href="/format/2310.00113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ksi%C4%85%C5%BCek%2C+K">Kamil Ksi&#x105;&#x17c;ek</a>, 
<a href="/search/cs?searchtype=author&query=Spurek%2C+P">Przemys&#x142;aw Spurek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Artificial neural networks suffer from catastrophic forgetting when they are
sequentially trained on multiple tasks. To overcome this problem, there exist
many continual learning strategies. One of the most effective is the
hypernetwork-based approach. The hypernetwork generates the weights of a target
model based on the task's identity. The model's main limitation is that
hypernetwork can produce completely different nests for each task.
Consequently, each task is solved separately. The model does not use
information from the network dedicated to previous tasks and practically
produces new architectures when it learns the subsequent tasks. To solve such a
problem, we use the lottery ticket hypothesis, which postulates the existence
of sparse subnetworks, named winning tickets, that preserve the performance of
a full network.
<br />In the paper, we propose a method called HyperMask, which trains a single
network for all tasks. Hypernetwork produces semi-binary masks to obtain target
subnetworks dedicated to new tasks. This solution inherits the ability of the
hypernetwork to adapt to new tasks with minimal forgetting. Moreover, due to
the lottery ticket hypothesis, we can use a single network with weighted
subnets dedicated to each task.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00115" title="Abstract">arXiv:2310.00115</a> [<a href="/pdf/2310.00115" title="Download PDF">pdf</a>, <a href="/format/2310.00115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yanqiao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+J">Jeehyun Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+K">Keir Adams</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+B">Bozhao Nan</a>, 
<a href="/search/cs?searchtype=author&query=Stenfors%2C+B">Brock Stenfors</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yuanqi Du</a>, 
<a href="/search/cs?searchtype=author&query=Chauhan%2C+J">Jatin Chauhan</a>, 
<a href="/search/cs?searchtype=author&query=Wiest%2C+O">Olaf Wiest</a>, 
<a href="/search/cs?searchtype=author&query=Isayev%2C+O">Olexandr Isayev</a>, 
<a href="/search/cs?searchtype=author&query=Coley%2C+C+W">Connor W. Coley</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yizhou Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Molecular Representation Learning (MRL) has proven impactful in numerous
biochemical applications such as drug discovery and enzyme design. While Graph
Neural Networks (GNNs) are effective at learning molecular representations from
a 2D molecular graph or a single 3D structure, existing works often overlook
the flexible nature of molecules, which continuously interconvert across
conformations via chemical bond rotations and minor vibrational perturbations.
To better account for molecular flexibility, some recent works formulate MRL as
an ensemble learning problem, focusing on explicitly learning from a set of
conformer structures. However, most of these studies have limited datasets,
tasks, and models. In this work, we introduce the first MoleculAR Conformer
Ensemble Learning (MARCEL) benchmark to thoroughly evaluate the potential of
learning on conformer ensembles and suggest promising research directions.
MARCEL includes four datasets covering diverse molecule- and reaction-level
properties of chemically diverse molecules including organocatalysts and
transition-metal catalysts, extending beyond the scope of common GNN benchmarks
that are confined to drug-like molecules. In addition, we conduct a
comprehensive empirical study, which benchmarks representative 1D, 2D, and 3D
molecular representation learning models, along with two strategies that
explicitly incorporate conformer ensembles into 3D MRL models. Our findings
reveal that direct learning from an accessible conformer space can improve
performance on a variety of tasks and models.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00116" title="Abstract">arXiv:2310.00116</a> [<a href="/pdf/2310.00116" title="Download PDF">pdf</a>, <a href="/format/2310.00116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Certified Robustness via Dynamic Margin Maximization and Improved  Lipschitz Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fazlyab%2C+M">Mahyar Fazlyab</a>, 
<a href="/search/cs?searchtype=author&query=Entesari%2C+T">Taha Entesari</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+A">Aniket Roy</a>, 
<a href="/search/cs?searchtype=author&query=Chellappa%2C+R">Rama Chellappa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">To improve the robustness of deep classifiers against adversarial
perturbations, many approaches have been proposed, such as designing new
architectures with better robustness properties (e.g., Lipschitz-capped
networks), or modifying the training process itself (e.g., min-max
optimization, constrained learning, or regularization). These approaches,
however, might not be effective at increasing the margin in the input (feature)
space. As a result, there has been an increasing interest in developing
training procedures that can directly manipulate the decision boundary in the
input space. In this paper, we build upon recent developments in this category
by developing a robust training algorithm whose objective is to increase the
margin in the output (logit) space while regularizing the Lipschitz constant of
the model along vulnerable directions. We show that these two objectives can
directly promote larger margins in the input space. To this end, we develop a
scalable method for calculating guaranteed differentiable upper bounds on the
Lipschitz constant of neural networks accurately and efficiently. The relative
accuracy of the bounds prevents excessive regularization and allows for more
direct manipulation of the decision boundary. Furthermore, our Lipschitz
bounding algorithm exploits the monotonicity and Lipschitz continuity of the
activation layers, and the resulting bounds can be used to design new layers
with controllable bounds on their Lipschitz constant. Experiments on the MNIST,
CIFAR-10, and Tiny-ImageNet data sets verify that our proposed algorithm
obtains competitively improved results compared to the state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00117" title="Abstract">arXiv:2310.00117</a> [<a href="/pdf/2310.00117" title="Download PDF">pdf</a>, <a href="/format/2310.00117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI  Co-Writing Tasks using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reza%2C+M">Mohi Reza</a>, 
<a href="/search/cs?searchtype=author&query=Laundry%2C+N">Nathan Laundry</a>, 
<a href="/search/cs?searchtype=author&query=Musabirov%2C+I">Ilya Musabirov</a>, 
<a href="/search/cs?searchtype=author&query=Dushniku%2C+P">Peter Dushniku</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z+Y+%22">Zhi Yuan &quot;Michael&quot; Yu</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+K">Kashish Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Grossman%2C+T">Tovi Grossman</a>, 
<a href="/search/cs?searchtype=author&query=Liut%2C+M">Michael Liut</a>, 
<a href="/search/cs?searchtype=author&query=Kuzminykh%2C+A">Anastasia Kuzminykh</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+J+J">Joseph Jay Williams</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Exploring alternative ideas by rewriting text is integral to the writing
process. State-of-the-art large language models (LLMs) can simplify writing
variation generation. However, current interfaces pose challenges for
simultaneous consideration of multiple variations: creating new versions
without overwriting text can be difficult, and pasting them sequentially can
clutter documents, increasing workload and disrupting writers' flow. To tackle
this, we present ABScribe, an interface that supports rapid, yet visually
structured, exploration of writing variations in human-AI co-writing tasks.
With ABScribe, users can swiftly produce multiple variations using LLM prompts,
which are auto-converted into reusable buttons. Variations are stored
adjacently within text segments for rapid in-place comparisons using mouse-over
interactions on a context toolbar. Our user study with 12 writers shows that
ABScribe significantly reduces task workload (d = 1.20, p &lt; 0.001), enhances
user perceptions of the revision process (d = 2.41, p &lt; 0.001) compared to a
popular baseline workflow, and provides insights into how writers explore
variations using LLMs.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00119" title="Abstract">arXiv:2310.00119</a> [<a href="/pdf/2310.00119" title="Download PDF">pdf</a>, <a href="/format/2310.00119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fewshot learning on global multimodal embeddings for earth observation  tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Allen%2C+M">Matt Allen</a>, 
<a href="/search/cs?searchtype=author&query=Dorr%2C+F">Francisco Dorr</a>, 
<a href="/search/cs?searchtype=author&query=Gallego-Mejia%2C+J+A">Joseph A. Gallego-Mejia</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Ferrer%2C+L">Laura Mart&#xed;nez-Ferrer</a>, 
<a href="/search/cs?searchtype=author&query=Jungbluth%2C+A">Anna Jungbluth</a>, 
<a href="/search/cs?searchtype=author&query=Kalaitzis%2C+F">Freddie Kalaitzis</a>, 
<a href="/search/cs?searchtype=author&query=Ramos-Poll%C3%A1n%2C+R">Ra&#xfa;l Ramos-Poll&#xe1;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, submitted to NeurIPS workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this work we pretrain a CLIP/ViT based model using three different
modalities of satellite imagery across five AOIs covering over ~10\% of the
earth total landmass, namely Sentinel 2 RGB optical imagery, Sentinel 1 SAR
amplitude and Sentinel 1 SAR interferometric coherence. This model uses $\sim
250$ M parameters. Then, we use the embeddings produced for each modality with
a classical machine learning method to attempt different downstream tasks for
earth observation related to vegetation, built up surface, croplands and
permanent water. We consistently show how we reduce the need for labeled data
by 99\%, so that with ~200-500 randomly selected labeled examples (around
4K-10K km$^2$) we reach performance levels analogous to those achieved with the
full labeled datasets (about 150K image chips or 3M km$^2$ in each AOI) on all
modalities, AOIs and downstream tasks. This leads us to think that the model
has captured significant earth features useful in a wide variety of scenarios.
To enhance our model's usability in practice, its architecture allows inference
in contexts with missing modalities and even missing channels within each
modality. Additionally, we visually show that this embedding space, obtained
with no labels, is sensible to the different earth features represented by the
labelled datasets we selected.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00120" title="Abstract">arXiv:2310.00120</a> [<a href="/pdf/2310.00120" title="Download PDF">pdf</a>, <a href="/format/2310.00120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Grid Tensorized Fourier Neural Operator for High-Resolution PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kossaifi%2C+J">Jean Kossaifi</a>, 
<a href="/search/cs?searchtype=author&query=Kovachki%2C+N">Nikola Kovachki</a>, 
<a href="/search/cs?searchtype=author&query=Azizzadenesheli%2C+K">Kamyar Azizzadenesheli</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Memory complexity and data scarcity have so far prohibited learning solution
operators of partial differential equations (PDEs) at high resolutions. We
address these limitations by introducing a new data efficient and highly
parallelizable operator learning approach with reduced memory requirement and
better generalization, called multi-grid tensorized neural operator (MG-TFNO).
MG-TFNO scales to large resolutions by leveraging local and global structures
of full-scale, real-world phenomena, through a decomposition of both the input
domain and the operator's parameter space. Our contributions are threefold: i)
we enable parallelization over input samples with a novel multi-grid-based
domain decomposition, ii) we represent the parameters of the model in a
high-order latent subspace of the Fourier domain, through a global tensor
factorization, resulting in an extreme reduction in the number of parameters
and improved generalization, and iii) we propose architectural improvements to
the backbone FNO. Our approach can be used in any operator learning setting. We
demonstrate superior performance on the turbulent Navier-Stokes equations where
we achieve less than half the error with over 150x compression. The
tensorization combined with the domain decomposition, yields over 150x
reduction in the number of parameters and 7x reduction in the domain size
without losses in accuracy, while slightly enabling parallelism.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00127" title="Abstract">arXiv:2310.00127</a> [<a href="/pdf/2310.00127" title="Download PDF">pdf</a>, <a href="/format/2310.00127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sensor Placement for Flapping Wing Model Using Stochastic Observability  Gramians
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Boyac%C4%B1o%C4%9Flu%2C+B">Burak Boyac&#x131;o&#x11f;lu</a>, 
<a href="/search/eess?searchtype=author&query=Babaei%2C+M">Mahnoush Babaei</a>, 
<a href="/search/eess?searchtype=author&query=Mamo%2C+A+H">Amanuel H. Mamo</a>, 
<a href="/search/eess?searchtype=author&query=Bergbreiter%2C+S">Sarah Bergbreiter</a>, 
<a href="/search/eess?searchtype=author&query=Daniel%2C+T+L">Thomas L. Daniel</a>, 
<a href="/search/eess?searchtype=author&query=Morgansen%2C+K+A">Kristi A. Morgansen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Systems in nature are stochastic as well as nonlinear. In traditional
applications, engineered filters aim to minimize the stochastic effects caused
by process and measurement noise. Conversely, a previous study showed that the
process noise can reveal the observability of a system that was initially
categorized as unobservable when deterministic tools were used. In this paper,
we develop a stochastic framework to explore observability analysis and sensor
placement. This framework allows for direct studies of the effects of
stochasticity on optimal sensor placement and selection to improve filter error
covariance. Numerical results are presented for sensor selection that optimizes
stochastic empirical observability in a bioinspired setting.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00129" title="Abstract">arXiv:2310.00129</a> [<a href="/pdf/2310.00129" title="Download PDF">pdf</a>, <a href="/format/2310.00129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ILB: Graph Neural Network Enabled Emergency Demand Response Program For  Electricity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaham%2C+S">Sina Shaham</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamachari%2C+B">Bhaskar Krishnamachari</a>, 
<a href="/search/cs?searchtype=author&query=Kahn%2C+M">Matthew Kahn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Demand Response (DR) programs have become a crucial component of smart
electricity grids as they shift the flexibility of electricity consumption from
supply to demand in response to the ever-growing demand for electricity. In
particular, in times of crisis, an emergency DR program is required to manage
unexpected spikes in energy demand. In this paper, we propose the
Incentive-Driven Load Balancer (ILB), a program designed to efficiently manage
demand and response during crisis situations. By offering incentives to
flexible households likely to reduce demand, the ILB facilitates effective
demand reduction and prepares them for unexpected events. To enable ILB, we
introduce a two-step machine learning-based framework for participant
selection, which employs a graph-based approach to identify households capable
of easily adjusting their electricity consumption. This framework utilizes two
Graph Neural Networks (GNNs): one for pattern recognition and another for
household selection. Through extensive experiments on household-level
electricity consumption in California, Michigan, and Texas, we demonstrate the
ILB program's significant effectiveness in supporting communities during
emergencies.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00132" title="Abstract">arXiv:2310.00132</a> [<a href="/pdf/2310.00132" title="Download PDF">pdf</a>, <a href="/format/2310.00132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Audiovisual Segmentation with Semantic Quantization and  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinglu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaohao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xiulian Peng</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Rita Singh</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Audiovisual segmentation (AVS) is a challenging task that aims to segment
visual objects in videos based on their associated acoustic cues. With multiple
sound sources involved, establishing robust correspondences between audio and
visual contents poses unique challenges due to its (1) intricate entanglement
across sound sources and (2) frequent shift among sound events. Assuming sound
events occur independently, the multi-source semantic space (which encompasses
all possible semantic categories) can be viewed as the Cartesian product of
single-source sub-spaces. This motivates us to decompose the multi-source audio
semantics into single-source semantics, allowing for more effective interaction
with visual content. Specifically, we propose a semantic decomposition method
based on product quantization, where the multi-source semantics can be
decomposed and represented by several quantized single-source semantics.
Furthermore, we introduce a global-to-local quantization mechanism that
distills knowledge from stable global (clip-level) features into local
(frame-level) ones to handle the constant shift of audio semantics. Extensive
experiments demonstrate that semantically quantized and decomposed audio
representation significantly improves AVS performance, e.g., +21.2% mIoU on the
most challenging AVS-Semantic benchmark.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00133" title="Abstract">arXiv:2310.00133</a> [<a href="/pdf/2310.00133" title="Download PDF">pdf</a>, <a href="/format/2310.00133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prior Mismatch and Adaptation in PnP-ADMM with a Nonconvex Convergence  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shoushtari%2C+S">Shirin Shoushtari</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chandler%2C+E+P">Edward P. Chandler</a>, 
<a href="/search/cs?searchtype=author&query=Asif%2C+M+S">M. Salman Asif</a>, 
<a href="/search/cs?searchtype=author&query=Kamilov%2C+U+S">Ulugbek S. Kamilov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Plug-and-Play (PnP) priors is a widely-used family of methods for solving
imaging inverse problems by integrating physical measurement models with image
priors specified using image denoisers. PnP methods have been shown to achieve
state-of-the-art performance when the prior is obtained using powerful deep
denoisers. Despite extensive work on PnP, the topic of distribution mismatch
between the training and testing data has often been overlooked in the PnP
literature. This paper presents a set of new theoretical and numerical results
on the topic of prior distribution mismatch and domain adaptation for
alternating direction method of multipliers (ADMM) variant of PnP. Our
theoretical result provides an explicit error bound for PnP-ADMM due to the
mismatch between the desired denoiser and the one used for inference. Our
analysis contributes to the work in the area by considering the mismatch under
nonconvex data-fidelity terms and expansive denoisers. Our first set of
numerical results quantifies the impact of the prior distribution mismatch on
the performance of PnP-ADMM on the problem of image super-resolution. Our
second set of numerical results considers a simple and effective domain
adaption strategy that closes the performance gap due to the use of mismatched
denoisers. Our results suggest the relative robustness of PnP-ADMM to prior
distribution mismatch, while also showing that the performance gap can be
significantly reduced with few training samples from the desired distribution.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00136" title="Abstract">arXiv:2310.00136</a> [<a href="/pdf/2310.00136" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing performance in Basketball: A Game-Theoretic Approach to Shot  Percentage Distribution in a team
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Aditya Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 figures, 8 equations, accepted at Asia-Singapore Conference on Sport Science (ACSS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">In this paper, we propose a shot percentage distribution strategy among the
players of a basketball team to maximize the score that can be achieved by
them. The approach is based on the concepts of game theory related to network
flow.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00137" title="Abstract">arXiv:2310.00137</a> [<a href="/pdf/2310.00137" title="Download PDF">pdf</a>, <a href="/format/2310.00137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Disconnect Between Theory and Practice of Overparametrized Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wenger%2C+J">Jonathan Wenger</a>, 
<a href="/search/cs?searchtype=author&query=Dangel%2C+F">Felix Dangel</a>, 
<a href="/search/cs?searchtype=author&query=Kristiadi%2C+A">Agustinus Kristiadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The infinite-width limit of neural networks (NNs) has garnered significant
attention as a theoretical framework for analyzing the behavior of large-scale,
overparametrized networks. By approaching infinite width, NNs effectively
converge to a linear model with features characterized by the neural tangent
kernel (NTK). This establishes a connection between NNs and kernel methods, the
latter of which are well understood. Based on this link, theoretical benefits
and algorithmic improvements have been hypothesized and empirically
demonstrated in synthetic architectures. These advantages include faster
optimization, reliable uncertainty quantification and improved continual
learning. However, current results quantifying the rate of convergence to the
kernel regime suggest that exploiting these benefits requires architectures
that are orders of magnitude wider than they are deep. This assumption raises
concerns that practically relevant architectures do not exhibit behavior as
predicted via the NTK. In this work, we empirically investigate whether the
limiting regime either describes the behavior of large-width architectures used
in practice or is informative for algorithmic improvements. Our empirical
results demonstrate that this is not the case in optimization, uncertainty
quantification or continual learning. This observed disconnect between theory
and practice calls into question the practical relevance of the infinite-width
limit.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00140" title="Abstract">arXiv:2310.00140</a> [<a href="/pdf/2310.00140" title="Download PDF">pdf</a>, <a href="/ps/2310.00140" title="Download PostScript">ps</a>, <a href="/format/2310.00140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GASS: Generalizing Audio Source Separation with Large-scale Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pons%2C+J">Jordi Pons</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pascual%2C+S">Santiago Pascual</a>, 
<a href="/search/cs?searchtype=author&query=Serr%C3%A0%2C+J">Joan Serr&#xe0;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS); Signal Processing (eess.SP)

</div>
<p class="mathjax">Universal source separation targets at separating the audio sources of an
arbitrary mix, removing the constraint to operate on a specific domain like
speech or music. Yet, the potential of universal source separation is limited
because most existing works focus on mixes with predominantly sound events, and
small training datasets also limit its potential for supervised learning. Here,
we study a single general audio source separation (GASS) model trained to
separate speech, music, and sound events in a supervised fashion with a
large-scale dataset. We assess GASS models on a diverse set of tasks. Our
strong in-distribution results show the feasibility of GASS models, and the
competitive out-of-distribution performance in sound event and speech
separation shows its generalization abilities. Yet, it is challenging for GASS
models to generalize for separating out-of-distribution cinematic and music
content. We also fine-tune GASS models on each dataset and consistently
outperform the ones without pre-training. All fine-tuned models (except the
music separation one) obtain state-of-the-art results in their respective
benchmarks.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00141" title="Abstract">arXiv:2310.00141</a> [<a href="/pdf/2310.00141" title="Download PDF">pdf</a>, <a href="/format/2310.00141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Gift of Feedback: Improving ASR Model Quality by Learning from User  Corrections through Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Lillian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yuxin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingqing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Harry Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Prabhavalkar%2C+R">Rohit Prabhavalkar</a>, 
<a href="/search/cs?searchtype=author&query=Guliani%2C+D">Dhruv Guliani</a>, 
<a href="/search/cs?searchtype=author&query=Motta%2C+G">Giovanni Motta</a>, 
<a href="/search/cs?searchtype=author&query=Mathews%2C+R">Rajiv Mathews</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Automatic speech recognition (ASR) models are typically trained on large
datasets of transcribed speech. As language evolves and new terms come into
use, these models can become outdated and stale. In the context of models
trained on the server but deployed on edge devices, errors may result from the
mismatch between server training data and actual on-device usage. In this work,
we seek to continually learn from on-device user corrections through Federated
Learning (FL) to address this issue. We explore techniques to target fresh
terms that the model has not previously encountered, learn long-tail words, and
mitigate catastrophic forgetting. In experimental evaluations, we find that the
proposed techniques improve model recognition of fresh terms, while preserving
quality on the overall language distribution.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00142" title="Abstract">arXiv:2310.00142</a> [<a href="/pdf/2310.00142" title="Download PDF">pdf</a>, <a href="/format/2310.00142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aerial Interaction with Tactile Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xiaofeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+G">Guanqi He</a>, 
<a href="/search/cs?searchtype=author&query=Mousaei%2C+M">Mohammadreza Mousaei</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+J">Junyi Geng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Guanya Shi</a>, 
<a href="/search/cs?searchtype=author&query=Scherer%2C+S">Sebastian Scherer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">While autonomous Uncrewed Aerial Vehicles (UAVs) have grown rapidly, most
applications only focus on passive visual tasks. Aerial interaction aims to
execute tasks involving physical interactions, which offers a way to assist
humans in high-risk, high-altitude operations, thereby reducing cost, time, and
potential hazards. The coupled dynamics between the aerial vehicle and
manipulator, however, pose challenges for precision control. Previous research
has typically employed either position control, which often fails to meet
mission accuracy, or force control using expensive, heavy, and cumbersome
force/torque sensors that also lack local semantic information. Conversely,
tactile sensors, being both cost-effective and lightweight, are capable of
sensing contact information including force distribution, as well as
recognizing local textures. Existing work on tactile sensing mainly focuses on
tabletop manipulation tasks within a quasi-static process. In this paper, we
pioneer the use of vision-based tactile sensors on a fully-actuated UAV to
improve the accuracy of the more dynamic aerial manipulation tasks. We
introduce a pipeline utilizing tactile feedback for real-time force tracking
via a hybrid motion-force controller and a method for wall texture detection
during aerial interactions. Our experiments demonstrate that our system can
effectively replace or complement traditional force/torque sensors, improving
flight performance by approximately 16% in position tracking error when using
the fused force estimate compared to relying on a single sensor. Our tactile
sensor achieves 93.4% accuracy in real-time texture recognition and 100%
post-contact. To the best of our knowledge, this is the first work to
incorporate a vision-based tactile sensor into aerial interaction tasks.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00144" title="Abstract">arXiv:2310.00144</a> [<a href="/pdf/2310.00144" title="Download PDF">pdf</a>, <a href="/format/2310.00144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Sampling-Enhanced Temporal-Spatial GCN: A Scalable  Framework for Transaction Anomaly Detection in Ethereum Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Behfar%2C+S+K">Stefan Kambiz Behfar</a>, 
<a href="/search/cs?searchtype=author&query=Crowcroft%2C+J">Jon Crowcroft</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The rapid evolution of the Ethereum network necessitates sophisticated
techniques to ensure its robustness against potential threats and to maintain
transparency. While Graph Neural Networks (GNNs) have pioneered anomaly
detection in such platforms, capturing the intricacies of both spatial and
temporal transactional patterns has remained a challenge. This study presents a
fusion of Graph Convolutional Networks (GCNs) with Temporal Random Walks (TRW)
enhanced by probabilistic sampling to bridge this gap. Our approach, unlike
traditional GCNs, leverages the strengths of TRW to discern complex temporal
sequences in Ethereum transactions, thereby providing a more nuanced
transaction anomaly detection mechanism. Preliminary evaluations demonstrate
that our TRW-GCN framework substantially advances the performance metrics over
conventional GCNs in detecting anomalies and transaction bursts. This research
not only underscores the potential of temporal cues in Ethereum transactional
data but also offers a scalable and effective methodology for ensuring the
security and transparency of decentralized platforms. By harnessing both
spatial relationships and time-based transactional sequences as node features,
our model introduces an additional layer of granularity, making the detection
process more robust and less prone to false positives. This work lays the
foundation for future research aimed at optimizing and enhancing the
transparency of blockchain technologies, and serves as a testament to the
significance of considering both time and space dimensions in the ever-evolving
landscape of the decentralized platforms.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00145" title="Abstract">arXiv:2310.00145</a> [<a href="/pdf/2310.00145" title="Download PDF">pdf</a>, <a href="/format/2310.00145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D Reconstruction in Noisy Agricultural Environments: A Bayesian  Optimization Perspective for View Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bacharis%2C+A">Athanasios Bacharis</a>, 
<a href="/search/cs?searchtype=author&query=Polyzos%2C+K+D">Konstantinos D. Polyzos</a>, 
<a href="/search/cs?searchtype=author&query=Nelson%2C+H+J">Henry J. Nelson</a>, 
<a href="/search/cs?searchtype=author&query=Giannakis%2C+G+B">Georgios B. Giannakis</a>, 
<a href="/search/cs?searchtype=author&query=Papanikolopoulos%2C+N">Nikolaos Papanikolopoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">3D reconstruction is a fundamental task in robotics that gained attention due
to its major impact in a wide variety of practical settings, including
agriculture, underwater, and urban environments. An important approach for this
task, known as view planning, is to judiciously place a number of cameras in
positions that maximize the visual information improving the resulting 3D
reconstruction. Circumventing the need for a large number of arbitrary images,
geometric criteria can be applied to select fewer yet more informative images
to markedly improve the 3D reconstruction performance. Nonetheless,
incorporating the noise of the environment that exists in various real-world
scenarios into these criteria may be challenging, particularly when prior
information about the noise is not provided. To that end, this work advocates a
novel geometric function that accounts for the existing noise, relying solely
on a relatively small number of noise realizations without requiring its
closed-form expression. With no analytic expression of the geometric function,
this work puts forth a Bayesian optimization algorithm for accurate 3D
reconstruction in the presence of noise. Numerical tests on noisy agricultural
environments showcase the impressive merits of the proposed approach for 3D
reconstruction with even a small number of available cameras.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00146" title="Abstract">arXiv:2310.00146</a> [<a href="/pdf/2310.00146" title="Download PDF">pdf</a>, <a href="/format/2310.00146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diver Identification Using Anthropometric Data Ratios for Underwater  Multi-Human-Robot Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Jungseok Hong</a>, 
<a href="/search/cs?searchtype=author&query=Enan%2C+S+S">Sadman Sakib Enan</a>, 
<a href="/search/cs?searchtype=author&query=Sattar%2C+J">Junaed Sattar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Recent advances in efficient design, perception algorithms, and computing
hardware have made it possible to create improved human-robot interaction (HRI)
capabilities for autonomous underwater vehicles (AUVs). To conduct secure
missions as underwater human-robot teams, AUVs require the ability to
accurately identify divers. However, this remains an open problem due to
divers' challenging visual features, mainly caused by similar-looking scuba
gear. In this paper, we present a novel algorithm that can perform diver
identification using either pre-trained models or models trained during
deployment. We exploit anthropometric data obtained from diver pose estimates
to generate robust features that are invariant to changes in distance and
photometric conditions. We also propose an embedding network that maximizes
inter-class distances in the feature space and minimizes those for the
intra-class features, which significantly improves classification performance.
Furthermore, we present an end-to-end diver identification framework that
operates on an AUV and evaluate the accuracy of the proposed algorithm.
Quantitative results in controlled-water experiments show that our algorithm
achieves a high level of accuracy in diver identification.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00149" title="Abstract">arXiv:2310.00149</a> [<a href="/pdf/2310.00149" title="Download PDF">pdf</a>, <a href="/format/2310.00149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One for All: Towards Training One Graph Model for All Classification  Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiarui Feng</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lecheng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+N">Ningyue Liang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yixin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muhan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Designing a single model that addresses multiple tasks has been a
long-standing objective in artificial intelligence. Recently, large language
models have demonstrated exceptional capability in integrating and solving
different tasks within the language domain. However, a unified model for
various tasks on graphs remains underexplored, primarily due to the challenges
unique to the graph learning domain. First, graph data from different areas
carry distinct attributes and follow different distributions. Such discrepancy
makes it hard to represent graphs in a single representation space. Second,
tasks on graphs diversify into node, link, and graph tasks, requiring distinct
embedding strategies. Finally, an appropriate graph prompting paradigm for
in-context learning is unclear. Striving to handle all the aforementioned
challenges, we propose One for All (OFA), the first general framework that can
use a single graph model to address the above challenges. Specifically, OFA
proposes text-attributed graphs to unify different graph data by describing
nodes and edges with natural language and uses language models to encode the
diverse and possibly cross-domain text attributes to feature vectors in the
same embedding space. Furthermore, OFA introduces the concept of
nodes-of-interest to standardize different tasks with a single task
representation. For in-context learning on graphs, OFA introduces a novel graph
prompting paradigm that appends prompting substructures to the input graph,
which enables it to address varied tasks without fine-tuning. We train the OFA
model using graph data from multiple domains (including citation networks,
molecular graphs, knowledge graphs, etc.) simultaneously and evaluate its
ability in supervised, few-shot, and zero-shot learning scenarios. OFA performs
well across different tasks, making it the first general-purpose graph
classification model across domains.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00152" title="Abstract">arXiv:2310.00152</a> [<a href="/pdf/2310.00152" title="Download PDF">pdf</a>, <a href="/format/2310.00152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Prompt Rewriting for Personalized Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Cheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Q">Qiaozhu Mei</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+W">Weize Kong</a>, 
<a href="/search/cs?searchtype=author&query=Bendersky%2C+M">Michael Bendersky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Facilitated by large language models (LLMs), personalized text generation has
become a rapidly growing research direction. Most existing studies focus on
designing specialized models for a particular domain, or they require
fine-tuning the LLMs to generate personalized text. We consider a typical
scenario in which the large language model, which generates personalized
output, is frozen and can only be accessed through APIs. Under this constraint,
all one can do is to improve the input text (i.e., text prompts) sent to the
LLM, a procedure that is usually done manually. In this paper, we propose a
novel method to automatically revise prompts for personalized text generation.
The proposed method takes the initial prompts generated by a state-of-the-art,
multistage framework for personalized generation and rewrites a few critical
components that summarize and synthesize the personal context. The prompt
rewriter employs a training paradigm that chains together supervised learning
(SL) and reinforcement learning (RL), where SL reduces the search space of RL
and RL facilitates end-to-end training of the rewriter. Using datasets from
three representative domains, we demonstrate that the rewritten prompts
outperform both the original prompts and the prompts optimized via supervised
learning or reinforcement learning alone. In-depth analysis of the rewritten
prompts shows that they are not only human readable, but also able to guide
manual revision of prompts when there is limited resource to employ
reinforcement learning to train the prompt rewriter, or when it is costly to
deploy an automatic prompt rewriter for inference.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00154" title="Abstract">arXiv:2310.00154</a> [<a href="/pdf/2310.00154" title="Download PDF">pdf</a>, <a href="/format/2310.00154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Primal-Dual Continual Learning: Stability and Plasticity through  Lagrange Multipliers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elenter%2C+J">Juan Elenter</a>, 
<a href="/search/cs?searchtype=author&query=NaderiAlizadeh%2C+N">Navid NaderiAlizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Javidi%2C+T">Tara Javidi</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+A">Alejandro Ribeiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Continual learning is inherently a constrained learning problem. The goal is
to learn a predictor under a \emph{no-forgetting} requirement. Although several
prior studies formulate it as such, they do not solve the constrained problem
explicitly. In this work, we show that it is both possible and beneficial to
undertake the constrained optimization problem directly. To do this, we
leverage recent results in constrained learning through Lagrangian duality. We
focus on memory-based methods, where a small subset of samples from previous
tasks can be stored in a replay buffer. In this setting, we analyze two
versions of the continual learning problem: a coarse approach with constraints
at the task level and a fine approach with constraints at the sample level. We
show that dual variables indicate the sensitivity of the optimal value with
respect to constraint perturbations. We then leverage this result to partition
the buffer in the coarse approach, allocating more resources to harder tasks,
and to populate the buffer in the fine approach, including only impactful
samples. We derive sub-optimality bounds, and empirically corroborate our
theoretical results in various continual learning benchmarks. We also discuss
the limitations of these methods with respect to the amount of memory available
and the number of constraints involved in the optimization problem.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00156" title="Abstract">arXiv:2310.00156</a> [<a href="/pdf/2310.00156" title="Download PDF">pdf</a>, <a href="/format/2310.00156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Generalizable Tool-use Skills through Trajectory Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+C">Carl Qi</a>, 
<a href="/search/cs?searchtype=author&query=Shetty%2C+S">Sarthak Shetty</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xingyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Held%2C+D">David Held</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Autonomous systems that efficiently utilize tools can assist humans in
completing many common tasks such as cooking and cleaning. However, current
systems fall short of matching human-level of intelligence in terms of adapting
to novel tools. Prior works based on affordance often make strong assumptions
about the environments and cannot scale to more complex, contact-rich tasks. In
this work, we tackle this challenge and explore how agents can learn to use
previously unseen tools to manipulate deformable objects. We propose to learn a
generative model of the tool-use trajectories as a sequence of point clouds,
which generalizes to different tool shapes. Given any novel tool, we first
generate a tool-use trajectory and then optimize the sequence of tool poses to
align with the generated trajectory. We train a single model for four different
challenging deformable object manipulation tasks. Our model is trained with
demonstration data from just a single tool for each task and is able to
generalize to various novel tools, significantly outperforming baselines.
Additional materials can be found on our project website:
https://sites.google.com/view/toolgen.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00158" title="Abstract">arXiv:2310.00158</a> [<a href="/pdf/2310.00158" title="Download PDF">pdf</a>, <a href="/format/2310.00158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feedback-guided Data Synthesis for Imbalanced Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hemmat%2C+R+A">Reyhane Askari Hemmat</a>, 
<a href="/search/cs?searchtype=author&query=Pezeshki%2C+M">Mohammad Pezeshki</a>, 
<a href="/search/cs?searchtype=author&query=Bordes%2C+F">Florian Bordes</a>, 
<a href="/search/cs?searchtype=author&query=Drozdzal%2C+M">Michal Drozdzal</a>, 
<a href="/search/cs?searchtype=author&query=Romero-Soriano%2C+A">Adriana Romero-Soriano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Current status quo in machine learning is to use static datasets of real
images for training, which often come from long-tailed distributions. With the
recent advances in generative models, researchers have started augmenting these
static datasets with synthetic data, reporting moderate performance
improvements on classification tasks. We hypothesize that these performance
gains are limited by the lack of feedback from the classifier to the generative
model, which would promote the usefulness of the generated samples to improve
the classifier's performance. In this work, we introduce a framework for
augmenting static datasets with useful synthetic samples, which leverages
one-shot feedback from the classifier to drive the sampling of the generative
model. In order for the framework to be effective, we find that the samples
must be close to the support of the real data of the task at hand, and be
sufficiently diverse. We validate three feedback criteria on a long-tailed
dataset (ImageNet-LT) as well as a group-imbalanced dataset (NICO++). On
ImageNet-LT, we achieve state-of-the-art results, with over 4 percent
improvement on underrepresented classes while being twice efficient in terms of
the number of generated synthetic samples. NICO++ also enjoys marked boosts of
over 5 percent in worst group accuracy. With these results, our framework paves
the path towards effectively leveraging state-of-the-art text-to-image models
as data sources that can be queried to improve downstream applications.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00160" title="Abstract">arXiv:2310.00160</a> [<a href="/pdf/2310.00160" title="Download PDF">pdf</a>, <a href="/format/2310.00160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Specialization: Uncovering Latent Expertise within Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Junmo Kang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Hongyin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yada Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Glass%2C+J">James Glass</a>, 
<a href="/search/cs?searchtype=author&query=Cox%2C+D">David Cox</a>, 
<a href="/search/cs?searchtype=author&query=Ritter%2C+A">Alan Ritter</a>, 
<a href="/search/cs?searchtype=author&query=Feris%2C+R">Rogerio Feris</a>, 
<a href="/search/cs?searchtype=author&query=Karlinsky%2C+L">Leonid Karlinsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent works have demonstrated the effectiveness of self-alignment in which a
large language model is, by itself, aligned to follow general instructions
through the automatic generation of instructional data using a handful of
human-written seeds. Instead of general alignment, in this work, we focus on
self-alignment for expert domain specialization (e.g., biomedicine),
discovering it to be very effective for improving zero-shot and few-shot
performance in target domains of interest. As a preliminary, we first present
the benchmark results of existing aligned models within a specialized domain,
which reveals the marginal effect that "generic" instruction-following training
has on downstream expert domains' performance. To remedy this, we explore
self-specialization that leverages domain-specific unlabelled data and a few
labeled seeds for the self-alignment process. When augmented with retrieval to
reduce hallucination and enhance concurrency of the alignment,
self-specialization offers an effective (and efficient) way of "carving out" an
expert model out of a "generalist", pre-trained LLM where different domains of
expertise are originally combined in a form of "superposition". Our
experimental results on a biomedical domain show that our self-specialized
model (30B) outperforms its base model, MPT-30B by a large margin and even
surpasses larger popular models based on LLaMA-65B, highlighting its potential
and practicality for specialization, especially considering its efficiency in
terms of data and parameters.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00161" title="Abstract">arXiv:2310.00161</a> [<a href="/pdf/2310.00161" title="Download PDF">pdf</a>, <a href="/format/2310.00161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection-Oriented Image-Text Pretraining for Open-Vocabulary Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dahun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Angelova%2C+A">Anelia Angelova</a>, 
<a href="/search/cs?searchtype=author&query=Kuo%2C+W">Weicheng Kuo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tech report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a new open-vocabulary detection approach based on
detection-oriented image-text pretraining to bridge the gap between image-level
pretraining and open-vocabulary object detection. At the pretraining phase, we
replace the commonly used classification architecture with the detector
architecture, which better serves the region-level recognition needs of
detection by enabling the detector heads to learn from noisy image-text pairs.
Using only standard contrastive loss and no pseudo-labeling, our approach is a
simple yet effective extension of the contrastive learning method to learn
emergent object-semantic cues. In addition, we propose a shifted-window
learning approach upon window attention to make the backbone representation
more robust, translation-invariant, and less biased by the window pattern. On
the popular LVIS open-vocabulary detection benchmark, our approach sets a new
state of the art of 40.4 mask AP$_r$ using the common ViT-L backbone,
significantly outperforming the best existing approach by +6.5 mask AP$_r$ at
system level. On the COCO benchmark, we achieve very competitive 40.8 novel AP
without pseudo labeling or weak supervision. In addition, we evaluate our
approach on the transfer detection setup, where ours outperforms the baseline
significantly. Visualization reveals emerging object locality from the
pretraining recipes compared to the baseline. Code and models will be publicly
released.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00163" title="Abstract">arXiv:2310.00163</a> [<a href="/pdf/2310.00163" title="Download PDF">pdf</a>, <a href="/format/2310.00163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cook2LTL: Translating Cooking Recipes to LTL Formulae using Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mavrogiannis%2C+A">Angelos Mavrogiannis</a>, 
<a href="/search/cs?searchtype=author&query=Mavrogiannis%2C+C">Christoforos Mavrogiannis</a>, 
<a href="/search/cs?searchtype=author&query=Aloimonos%2C+Y">Yiannis Aloimonos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Cooking recipes are especially challenging to translate to robot plans as
they feature rich linguistic complexity, temporally-extended interconnected
tasks, and an almost infinite space of possible actions. Our key insight is
that combining a source of background cooking domain knowledge with a formalism
capable of handling the temporal richness of cooking recipes could enable the
extraction of unambiguous, robot-executable plans. In this work, we use Linear
Temporal Logic (LTL) as a formal language expressible enough to model the
temporal nature of cooking recipes. Leveraging pre-trained Large Language
Models (LLMs), we present a system that translates instruction steps from an
arbitrary cooking recipe found on the internet to a series of LTL formulae,
grounding high-level cooking actions to a set of primitive actions that are
executable by a manipulator in a kitchen environment. Our approach makes use of
a caching scheme, dynamically building a queryable action library at runtime,
significantly decreasing LLM API calls (-51%), latency (-59%) and cost (-42%)
compared to a baseline that queries the LLM for every newly encountered action
at runtime. We demonstrate the transferability of our system in a realistic
simulation platform through showcasing a set of simple cooking tasks.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00164" title="Abstract">arXiv:2310.00164</a> [<a href="/pdf/2310.00164" title="Download PDF">pdf</a>, <a href="/format/2310.00164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PRIME: Prioritizing Interpretability in Failure Mode Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+K">Keivan Rezaei</a>, 
<a href="/search/cs?searchtype=author&query=Saberi%2C+M">Mehrdad Saberi</a>, 
<a href="/search/cs?searchtype=author&query=Moayeri%2C+M">Mazda Moayeri</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this work, we study the challenge of providing human-understandable
descriptions for failure modes in trained image classification models. Existing
works address this problem by first identifying clusters (or directions) of
incorrectly classified samples in a latent space and then aiming to provide
human-understandable text descriptions for them. We observe that in some cases,
describing text does not match well with identified failure modes, partially
owing to the fact that shared interpretable attributes of failure modes may not
be captured using clustering in the feature space. To improve on these
shortcomings, we propose a novel approach that prioritizes interpretability in
this problem: we start by obtaining human-understandable concepts (tags) of
images in the dataset and then analyze the model's behavior based on the
presence or absence of combinations of these tags. Our method also ensures that
the tags describing a failure mode form a minimal set, avoiding redundant and
noisy descriptions. Through several experiments on different datasets, we show
that our method successfully identifies failure modes and generates
high-quality text descriptions associated with them. These results highlight
the importance of prioritizing interpretability in understanding model
failures.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00165" title="Abstract">arXiv:2310.00165</a> [<a href="/pdf/2310.00165" title="Download PDF">pdf</a>, <a href="/format/2310.00165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SCoRe: Submodular Combinatorial Representation Learning for Real-World  Class-Imbalanced Settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Majee%2C+A">Anay Majee</a>, 
<a href="/search/cs?searchtype=author&query=Kothawade%2C+S">Suraj Kothawade</a>, 
<a href="/search/cs?searchtype=author&query=Killiamsetty%2C+K">Krishnateja Killiamsetty</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+R">Rishabh Iyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Representation Learning in real-world class-imbalanced settings has emerged
as a challenging task in the evolution of deep learning. Lack of diversity in
visual and structural features for rare classes restricts modern neural
networks to learn discriminative feature clusters. This manifests in the form
of large inter-class bias between rare object classes and elevated intra-class
variance among abundant classes in the dataset. Although deep metric learning
approaches have shown promise in this domain, significant improvements need to
be made to overcome the challenges associated with class-imbalance in mission
critical tasks like autonomous navigation and medical diagnostics. Set-based
combinatorial functions like Submodular Information Measures exhibit properties
that allow them to simultaneously model diversity and cooperation among feature
clusters. In this paper, we introduce the SCoRe (Submodular Combinatorial
Representation Learning) framework and propose a family of Submodular
Combinatorial Loss functions to overcome these pitfalls in contrastive
learning. We also show that existing contrastive learning approaches are either
submodular or can be re-formulated to create their submodular counterparts. We
conduct experiments on the newly introduced family of combinatorial objectives
on two image classification benchmarks - pathologically imbalanced CIFAR-10,
subsets of MedMNIST and a real-world road object detection benchmark - India
Driving Dataset (IDD). Our experiments clearly show that the newly introduced
objectives like Facility Location, Graph-Cut and Log Determinant outperform
state-of-the-art metric learners by up to 7.6% for the imbalanced
classification tasks and up to 19.4% for object detection tasks.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00166" title="Abstract">arXiv:2310.00166</a> [<a href="/pdf/2310.00166" title="Download PDF">pdf</a>, <a href="/format/2310.00166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Motif: Intrinsic Motivation from Artificial Intelligence Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klissarov%2C+M">Martin Klissarov</a>, 
<a href="/search/cs?searchtype=author&query=D%27Oro%2C+P">Pierluca D&#x27;Oro</a>, 
<a href="/search/cs?searchtype=author&query=Sodhani%2C+S">Shagun Sodhani</a>, 
<a href="/search/cs?searchtype=author&query=Raileanu%2C+R">Roberta Raileanu</a>, 
<a href="/search/cs?searchtype=author&query=Bacon%2C+P">Pierre-Luc Bacon</a>, 
<a href="/search/cs?searchtype=author&query=Vincent%2C+P">Pascal Vincent</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Amy Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Henaff%2C+M">Mikael Henaff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors equally contributed - order decided by coin flip
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Exploring rich environments and evaluating one's actions without prior
knowledge is immensely challenging. In this paper, we propose Motif, a general
method to interface such prior knowledge from a Large Language Model (LLM) with
an agent. Motif is based on the idea of grounding LLMs for decision-making
without requiring them to interact with the environment: it elicits preferences
from an LLM over pairs of captions to construct an intrinsic reward, which is
then used to train agents with reinforcement learning. We evaluate Motif's
performance and behavior on the challenging, open-ended and
procedurally-generated NetHack game. Surprisingly, by only learning to maximize
its intrinsic reward, Motif achieves a higher game score than an algorithm
directly trained to maximize the score itself. When combining Motif's intrinsic
reward with the environment reward, our method significantly outperforms
existing approaches and makes progress on tasks where no advancements have ever
been made without demonstrations. Finally, we show that Motif mostly generates
intuitive human-aligned behaviors which can be steered easily through prompt
modifications, while scaling well with the LLM size and the amount of
information given in the prompt.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00167" title="Abstract">arXiv:2310.00167</a> [<a href="/pdf/2310.00167" title="Download PDF">pdf</a>, <a href="/format/2310.00167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Optimal Tabletop Rearrangement with Multiple Manipulation  Primitives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Baichuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xujia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingjin Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In practice, many types of manipulation actions (e.g., pick-n-place and push)
are needed to accomplish real-world manipulation tasks. Yet, limited research
exists that explores the synergistic integration of different manipulation
actions for optimally solving long-horizon task-and-motion planning problems.
In this study, we propose and investigate planning high-quality action
sequences for solving long-horizon tabletop rearrangement tasks in which
multiple manipulation primitives are required. Denoting the problem
rearrangement with multiple manipulation primitives (REMP), we develop two
algorithms, hierarchical best-first search (HBFS) and parallel Monte Carlo tree
search for multi-primitive rearrangement (PMMR) toward optimally resolving the
challenge. Extensive simulation and real robot experiments demonstrate that
both methods effectively tackle REMP, with HBFS excelling in planning speed and
PMMR producing human-like, high-quality solutions with a nearly 100% success
rate.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00168" title="Abstract">arXiv:2310.00168</a> [<a href="/pdf/2310.00168" title="Download PDF">pdf</a>, <a href="/format/2310.00168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LQ-OCP: Energy-Optimal Control for LQ Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Beaver%2C+L+E">Logan E. Beaver</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This article presents a method to automatically generate energy-optimal
trajectories for systems with linear dynamics, linear constraints, and a
quadratic cost functional (LQ systems). First, using recent advancements in
optimal control, we derive the optimal motion primitive generator for LQ
systems--this yields linear differential equations that describe all dynamical
motion primitives that the optimal system follows. We also derive the
optimality conditions where the system switches between motion primitives--a
system of equations that are bilinear in the unknown junction time. Finally, we
demonstrate the performance of our approach on an energy-minimizing submersible
robot with state and control constraints. We compare our approach to an
energy-optimizing Linear Quadratic Regulator (LQR), where we learn the optimal
weights of the LQR cost function to minimize energy consumption while ensuring
convergence and constraint satisfaction. Our approach converges to the optimal
solution 6,400% faster than the LQR weight optimization, and that our solution
is 350% more energy efficient. Finally, we disturb the initial state of the
submersible to show that our approach still finds energy-efficient solutions
faster than LQR when the unconstrained solution is infeasible.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00171" title="Abstract">arXiv:2310.00171</a> [<a href="/pdf/2310.00171" title="Download PDF">pdf</a>, <a href="/format/2310.00171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Degree Distribution Identifiability of Stochastic Kronecker Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alabi%2C+D">Daniel Alabi</a>, 
<a href="/search/cs?searchtype=author&query=Kalimeris%2C+D">Dimitris Kalimeris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Large-scale analysis of the distributions of the network graphs observed in
naturally-occurring phenomena has revealed that the degrees of such graphs
follow a power-law or lognormal distribution. Seshadhri, Pinar, and Kolda (J.
ACM, 2013) proved that stochastic Kronecker graph (SKG) models cannot generate
graphs with degree distribution that follows a power-law or lognormal
distribution. As a result, variants of the SKG model have been proposed to
generate graphs which approximately follow degree distributions, without any
significant oscillations. However, all existing solutions either require
significant additional parameterization or have no provable guarantees on the
degree distribution.
<br />-- In this work, we present statistical and computational identifiability
notions which imply the separation of SKG models. Specifically, we prove that
SKG models in different identifiability classes can be separated by the
existence of isolated vertices and connected components in their corresponding
generated graphs. This could explain the large (i.e., $&gt;50\%$) fraction of
isolated vertices in some popular graph generation benchmarks.
<br />-- We present and analyze an efficient algorithm that can get rid of
oscillations in the degree distribution by mixing seeds of relative prime
dimensions. For an initial $2\times 1$ and $2\times 2$ seed, a crucial
subroutine of this algorithm solves a degree-2 and degree-4 optimization
problem in the variables of the initial seed, respectively. We generalize this
approach to solving optimization problems for $m\times n$ seeds, for any $m,
n\in\mathbb{N}$.
<br />-- The use of $3\times 3$ seeds alone cannot get rid of significant
oscillations. We prove that such seeds result in degree distribution that is
bounded above by an exponential tail and thus cannot result in a power-law or
lognormal.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00172" title="Abstract">arXiv:2310.00172</a> [<a href="/pdf/2310.00172" title="Download PDF">pdf</a>, <a href="/format/2310.00172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A physics-informed deep learning approach for solving strongly  degenerate parabolic problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ambrosio%2C+P">Pasquale Ambrosio</a>, 
<a href="/search/math?searchtype=author&query=Cuomo%2C+S">Salvatore Cuomo</a>, 
<a href="/search/math?searchtype=author&query=De+Rosa%2C+M">Mariapia De Rosa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In recent years, Scientific Machine Learning (SciML) methods for solving
partial differential equations (PDEs) have gained increasing popularity. Within
such a paradigm, Physics-Informed Neural Networks (PINNs) are novel deep
learning frameworks for solving initial-boundary value problems involving
nonlinear PDEs. Recently, PINNs have shown promising results in several
application fields. Motivated by applications to gas filtration problems, here
we present and evaluate a PINN-based approach to predict solutions to
$strongly\,\,degenerate\,\,parabolic\,\,problems\,\,with\,\,asymptotic\,\,structure\,\,of\,\,Laplacian\,\,type$.
To the best of our knowledge, this is one of the first papers demonstrating the
efficacy of the PINN framework for solving such kind of problems. In
particular, we estimate an appropriate approximation error for some test
problems whose analytical solutions are fortunately known. The numerical
experiments discussed include two and three-dimensional spatial domains,
emphasizing the effectiveness of this approach in predicting accurate
solutions.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00175" title="Abstract">arXiv:2310.00175</a> [<a href="/pdf/2310.00175" title="Download PDF">pdf</a>, <a href="/format/2310.00175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Bounds for Volumetric Spanners and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhaskara%2C+A">Aditya Bhaskara</a>, 
<a href="/search/cs?searchtype=author&query=Mahabadi%2C+S">Sepideh Mahabadi</a>, 
<a href="/search/cs?searchtype=author&query=Vakilian%2C+A">Ali Vakilian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Given a set of points of interest, a volumetric spanner is a subset of the
points using which all the points can be expressed using "small" coefficients
(measured in an appropriate norm). Formally, given a set of vectors $X = \{v_1,
v_2, \dots, v_n\}$, the goal is to find $T \subseteq [n]$ such that every $v
\in X$ can be expressed as $\sum_{i\in T} \alpha_i v_i$, with $\|\alpha\|$
being small. This notion, which has also been referred to as a well-conditioned
basis, has found several applications, including bandit linear optimization,
determinant maximization, and matrix low rank approximation. In this paper, we
give almost optimal bounds on the size of volumetric spanners for all $\ell_p$
norms, and show that they can be constructed using a simple local search
procedure. We then show the applications of our result to other tasks and in
particular the problem of finding coresets for the Minimum Volume Enclosing
Ellipsoid (MVEE) problem.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00177" title="Abstract">arXiv:2310.00177</a> [<a href="/pdf/2310.00177" title="Download PDF">pdf</a>, <a href="/format/2310.00177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann  Boundary Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lan%2C+W">Weixian Lan</a>, 
<a href="/search/math?searchtype=author&query=Gueidon%2C+E">Elias Gueidon</a>, 
<a href="/search/math?searchtype=author&query=Kaneda%2C+A">Ayano Kaneda</a>, 
<a href="/search/math?searchtype=author&query=Panetta%2C+J">Julian Panetta</a>, 
<a href="/search/math?searchtype=author&query=Teran%2C+J">Joseph Teran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce a neural-preconditioned iterative solver for Poisson equations
with mixed boundary conditions. The Poisson equation is ubiquitous in
scientific computing: it governs a wide array of physical phenomena, arises as
a subproblem in many numerical algorithms, and serves as a model problem for
the broader class of elliptic PDEs. The most popular Poisson discretizations
yield large sparse linear systems. At high resolution, and for
performance-critical applications, iterative solvers can be advantageous for
these -- but only when paired with powerful preconditioners. The core of our
solver is a neural network trained to approximate the inverse of a discrete
structured-grid Laplace operator for a domain of arbitrary shape and with mixed
boundary conditions. The structure of this problem motivates a novel network
architecture that we demonstrate is highly effective as a preconditioner even
for boundary conditions outside the training set. We show that on challenging
test cases arising from an incompressible fluid simulation, our method
outperforms state-of-the-art solvers like algebraic multigrid as well as some
recent neural preconditioners.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00178" title="Abstract">arXiv:2310.00178</a> [<a href="/pdf/2310.00178" title="Download PDF">pdf</a>, <a href="/format/2310.00178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zelin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Caseiro%2C+D">Diamantino Caseiro</a>, 
<a href="/search/cs?searchtype=author&query=Munkhdalai%2C+T">Tsendsuren Munkhdalai</a>, 
<a href="/search/cs?searchtype=author&query=Sim%2C+K+C">Khe Chai Sim</a>, 
<a href="/search/cs?searchtype=author&query=Rondon%2C+P">Pat Rondon</a>, 
<a href="/search/cs?searchtype=author&query=Pundak%2C+G">Golan Pundak</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+G">Gan Song</a>, 
<a href="/search/cs?searchtype=author&query=Prabhavalkar%2C+R">Rohit Prabhavalkar</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zhong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Ding Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Sainath%2C+T">Tara Sainath</a>, 
<a href="/search/cs?searchtype=author&query=Mengibar%2C+P+M">Pedro Moreno Mengibar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Contextual biasing refers to the problem of biasing the automatic speech
recognition (ASR) systems towards rare entities that are relevant to the
specific user or application scenarios. We propose algorithms for contextual
biasing based on the Knuth-Morris-Pratt algorithm for pattern matching. During
beam search, we boost the score of a token extension if it extends matching
into a set of biasing phrases. Our method simulates the classical approaches
often implemented in the weighted finite state transducer (WFST) framework, but
avoids the FST language altogether, with careful considerations on memory
footprint and efficiency on tensor processing units (TPUs) by vectorization.
Without introducing additional model parameters, our method achieves
significant word error rate (WER) reductions on biasing test sets by itself,
and yields further performance gain when combined with a model-based biasing
method.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00179" title="Abstract">arXiv:2310.00179</a> [<a href="/pdf/2310.00179" title="Download PDF">pdf</a>, <a href="/format/2310.00179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Preference Dynamics using Lattice Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Riess%2C+H">Hans Riess</a>, 
<a href="/search/cs?searchtype=author&query=Henselman-Petrusek%2C+G">Gregory Henselman-Petrusek</a>, 
<a href="/search/cs?searchtype=author&query=Munger%2C+M+C">Michael C. Munger</a>, 
<a href="/search/cs?searchtype=author&query=Ghrist%2C+R">Robert Ghrist</a>, 
<a href="/search/cs?searchtype=author&query=Bell%2C+Z+I">Zachary I. Bell</a>, 
<a href="/search/cs?searchtype=author&query=Zavlanos%2C+M+M">Michael M. Zavlanos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Preferences, fundamental in all forms of strategic behavior and collective
decision-making, in their raw form, are an abstract ordering on a set of
alternatives. Agents, we assume, revise their preferences as they gain more
information about other agents. Exploiting the ordered algebraic structure of
preferences, we introduce a message-passing algorithm for heterogeneous agents
distributed over a network to update their preferences based on aggregations of
the preferences of their neighbors in a graph. We demonstrate the existence of
equilibrium points of the resulting global dynamical system of local preference
updates and provide a sufficient condition for trajectories to converge to
equilibria: stable preferences. Finally, we present numerical simulations
demonstrating our preliminary results.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00180" title="Abstract">arXiv:2310.00180</a> [<a href="/pdf/2310.00180" title="Download PDF">pdf</a>, <a href="/format/2310.00180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MARL: Multi-scale Archetype Representation Learning for Urban Building  Energy Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+X">Xinwei Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zixun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+W">Wentao Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Caldas%2C+L">Luisa Caldas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> *Equal Contribution
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Building archetypes, representative models of building stock, are crucial for
precise energy simulations in Urban Building Energy Modeling. The current
widely adopted building archetypes are developed on a nationwide scale,
potentially neglecting the impact of local buildings' geometric specificities.
We present Multi-scale Archetype Representation Learning (MARL), an approach
that leverages representation learning to extract geometric features from a
specific building stock. Built upon VQ-AE, MARL encodes building footprints and
purifies geometric information into latent vectors constrained by multiple
architectural downstream tasks. These tailored representations are proven
valuable for further clustering and building energy modeling. The advantages of
our algorithm are its adaptability with respect to the different building
footprint sizes, the ability for automatic generation across multi-scale
regions, and the preservation of geometric features across neighborhoods and
local ecologies. In our study spanning five regions in LA County, we show MARL
surpasses both conventional and VQ-AE extracted archetypes in performance.
Results demonstrate that geometric feature embeddings significantly improve the
accuracy and reliability of energy consumption estimates. Code, dataset and
trained models are publicly available:
https://github.com/ZixunHuang1997/MARL-BuildingEnergyEstimation
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00183" title="Abstract">arXiv:2310.00183</a> [<a href="/pdf/2310.00183" title="Download PDF">pdf</a>, <a href="/format/2310.00183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Equivalence of Graph Convolution and Mixup
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiaotian Han</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+H">Hanqing Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+S">Shaoliang Nie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingzhou Liu</a>, 
<a href="/search/cs?searchtype=author&query=Narang%2C+K">Kanika Narang</a>, 
<a href="/search/cs?searchtype=author&query=Shakeri%2C+Z">Zahra Shakeri</a>, 
<a href="/search/cs?searchtype=author&query=Sankararaman%2C+K+A">Karthik Abinav Sankararaman</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Song Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Khabsa%2C+M">Madian Khabsa</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xia Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper investigates the relationship between graph convolution and Mixup
techniques. Graph convolution in a graph neural network involves aggregating
features from neighboring samples to learn representative features for a
specific node or sample. On the other hand, Mixup is a data augmentation
technique that generates new examples by averaging features and one-hot labels
from multiple samples. One commonality between these techniques is their
utilization of information from multiple samples to derive feature
representation. This study aims to explore whether a connection exists between
these two approaches. Our investigation reveals that, under two mild
conditions, graph convolution can be viewed as a specialized form of Mixup that
is applied during both the training and testing phases. The two conditions are:
1) \textit{Homophily Relabel} - assigning the target node's label to all its
neighbors, and 2) \textit{Test-Time Mixup} - Mixup the feature during the test
time. We establish this equivalence mathematically by demonstrating that graph
convolution networks (GCN) and simplified graph convolution (SGC) can be
expressed as a form of Mixup. We also empirically verify the equivalence by
training an MLP using the two conditions to achieve comparable performance.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00184" title="Abstract">arXiv:2310.00184</a> [<a href="/pdf/2310.00184" title="Download PDF">pdf</a>, <a href="/format/2310.00184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NASU -- Novel Actuating Screw Unit: Origami-inspired Screw-based  Propulsion on Mobile Ground Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joyce%2C+C">Calvin Joyce</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+J">Jason Lim</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+R">Roger Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Owens%2C+M">Michael Owens</a>, 
<a href="/search/cs?searchtype=author&query=Wickenhiser%2C+S">Sara Wickenhiser</a>, 
<a href="/search/cs?searchtype=author&query=Peiros%2C+E">Elizabeth Peiros</a>, 
<a href="/search/cs?searchtype=author&query=Richter%2C+F">Florian Richter</a>, 
<a href="/search/cs?searchtype=author&query=Yip%2C+M+C">Michael C. Yip</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 9 Figures, submitted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Screw-based locomotion is a robust method of locomotion across a wide range
of media including water, sand, and gravel. A challenge with screws is their
significant number of impactful design parameters that affect locomotion
performance in varying environments. One crucial parameter is the angle of
attack, also referred to as the lead angle. The angle of attack has a
significant impact on the screw's performance as it creates a trade-off between
efficiency and forward velocity. This trend is consistent across various types
of media. In this work, we present a Novel Actuating Screw Unit. It is the
first screw-based propulsion design that enables the reconfiguration of the
angle of attack dynamically for optimized locomotion across multiple media. The
design is inspired by the kresling unit, which is a widespread mechanism in
origami robotics, and the angle of attack is adjusted with a linear actuator,
while the entire unit is spun on its axis as an archimedean screw. NASU is
integrated onto a mobile test-bed and experiments are conducted in a large
variety of media including gravel, grass, and sand. Our experiments show the
proposed design is a promising direction for reconfigurable screws by allowing
control to optimize for efficiency or velocity.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00188" title="Abstract">arXiv:2310.00188</a> [<a href="/pdf/2310.00188" title="Download PDF">pdf</a>, <a href="/ps/2310.00188" title="Download PostScript">ps</a>, <a href="/format/2310.00188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The adjoint double layer potential on smooth surfaces in $\mathbb{R}^3$  and the Neumann problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Beale%2C+J+T">J. Thomas Beale</a>, 
<a href="/search/math?searchtype=author&query=Storm%2C+M">Michael Storm</a>, 
<a href="/search/math?searchtype=author&query=Tlupova%2C+S">Svetlana Tlupova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present a simple yet accurate method to compute the adjoint double layer
potential, which is used to solve the Neumann boundary value problem for
Laplace's equation in three dimensions. An expansion in curvilinear coordinates
leads us to modify the expression for the adjoint double layer so that the
singularity is reduced when evaluating the integral on the surface. We then
regularize the Green's function, with a radial parameter $\delta$. We show that
a natural regularization has error $O(\delta^3)$, and a simple modification
improves the error to $O(\delta^5)$. The integral is evaluated numerically
without the need of special coordinates. We use this treatment of the adjoint
double layer to solve the classical integral equation for the interior Neumann
problem and evaluate the solution on the boundary. Choosing $\delta =
ch^{4/5}$, we find about $O(h^4)$ convergence in our examples, where $h$ is the
spacing in a background grid.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00192" title="Abstract">arXiv:2310.00192</a> [<a href="/pdf/2310.00192" title="Download PDF">pdf</a>, <a href="/format/2310.00192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tailors: Accelerating Sparse Tensor Algebra by Overbooking Buffer  Capacity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+Z+Y">Zi Yu Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y+N">Yannan Nellie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Emer%2C+J+S">Joel S. Emer</a>, 
<a href="/search/cs?searchtype=author&query=Sze%2C+V">Vivienne Sze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 13 figures, to appear in MICRO 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Sparse tensor algebra is a challenging class of workloads to accelerate due
to low arithmetic intensity and varying sparsity patterns. Prior sparse tensor
algebra accelerators have explored tiling sparse data to increase exploitable
data reuse and improve throughput, but typically allocate tile size in a given
buffer for the worst-case data occupancy. This severely limits the utilization
of available memory resources and reduces data reuse. Other accelerators employ
complex tiling during preprocessing or at runtime to determine the exact tile
size based on its occupancy. This paper proposes a speculative tensor tiling
approach, called overbooking, to improve buffer utilization by taking advantage
of the distribution of nonzero elements in sparse tensors to construct larger
tiles with greater data reuse. To ensure correctness, we propose a low-overhead
hardware mechanism, Tailors, that can tolerate data overflow by design while
ensuring reasonable data reuse. We demonstrate that Tailors can be easily
integrated into the memory hierarchy of an existing sparse tensor algebra
accelerator. To ensure high buffer utilization with minimal tiling overhead, we
introduce a statistical approach, Swiftiles, to pick a tile size so that tiles
usually fit within the buffer's capacity, but can potentially overflow, i.e.,
it overbooks the buffers. Across a suite of 22 sparse tensor algebra workloads,
we show that our proposed overbooking strategy introduces an average speedup of
$52.7\times$ and $2.3\times$ and an average energy reduction of $22.5\times$
and $2.5\times$ over ExTensor without and with optimized tiling, respectively.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00193" title="Abstract">arXiv:2310.00193</a> [<a href="/pdf/2310.00193" title="Download PDF">pdf</a>, <a href="/ps/2310.00193" title="Download PostScript">ps</a>, <a href="/format/2310.00193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When is fast, implicit squaring of $A^{-1}B$ stable?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Schneider%2C+R">Ryan Schneider</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We analyze Implicit Repeated Squaring ($\textbf{IRS}$) -- an algorithm that
implicitly computes $(A^{-1}B)^{2^p}$ for $A,B \in {\mathbb C}^{n \times n}$
and $p \in {\mathbb N}_+$. Originating with inverse-free divide-and-conquer
eigensolvers, this routine uses only matrix multiplication and QR to produce
$A_p$ and $B_p$ satisfying $A_p^{-1}B_p = (A^{-1}B)^{2^p}$. By offering the
choice to work with $A_p$ and $B_p$ individually or form $A_p^{-1}B_p$,
$\textbf{IRS}$ is applicable to both implicit and explicit computations
involving $(A^{-1}B)^{2^p}$. Moreover, it exhibits stability. In this paper, we
present the first rigorous and general analysis of the algorithm, showing that
a finite-arithmetic implementation of $\textbf{IRS}$ -- using any fast matrix
multiplication routine -- satisfies a mixed forward/backward stability bound
provided $O(\log(1/\epsilon) + \log(\mu(n)) + (p-1)
\log(\kappa_\text{IRS}(A,B,p)))$ bits of precision are used, where $\epsilon$
is the desired (sufficiently small) forward error, $\mu(n)$ is a polynomial in
$n$, and $\kappa_{\text{IRS}}$ is an appropriately chosen condition number.
Hence, fast $\textbf{IRS}$ is stable in relatively modest precision when $p$ is
logarithmic in $n$ or smaller. We also demonstrate on a handful of examples
that $\textbf{IRS}$ offers improved accuracy over a naive algorithm that
explicitly forms and squares $A^{-1}B$. Along the way, we develop a spectral
norm perturbation bound for full QR that may be of independent interest.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00194" title="Abstract">arXiv:2310.00194</a> [<a href="/pdf/2310.00194" title="Download PDF">pdf</a>, <a href="/format/2310.00194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Prefrontal Cortex-inspired Architecture for Planning in Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Webb%2C+T">Taylor Webb</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+S+S">Shanka Subhra Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Krabach%2C+B">Brian Krabach</a>, 
<a href="/search/cs?searchtype=author&query=Momennejad%2C+I">Ida Momennejad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Large language models (LLMs) demonstrate impressive performance on a wide
variety of tasks, but they often struggle with tasks that require multi-step
reasoning or goal-directed planning. To address this, we take inspiration from
the human brain, in which planning is accomplished via the recurrent
interaction of specialized modules in the prefrontal cortex (PFC). These
modules perform functions such as conflict monitoring, state prediction, state
evaluation, task decomposition, and task coordination. We find that LLMs are
sometimes capable of carrying out these functions in isolation, but struggle to
autonomously coordinate them in the service of a goal. Therefore, we propose a
black box architecture with multiple LLM-based (GPT-4) modules. The
architecture improves planning through the interaction of specialized
PFC-inspired modules that break down a larger problem into multiple brief
automated calls to the LLM. We evaluate the combined architecture on two
challenging planning tasks -- graph traversal and Tower of Hanoi -- finding
that it yields significant improvements over standard LLM methods (e.g.,
zero-shot prompting or in-context learning). These results demonstrate the
benefit of utilizing knowledge from cognitive neuroscience to improve planning
in LLMs.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00195" title="Abstract">arXiv:2310.00195</a> [<a href="/pdf/2310.00195" title="Download PDF">pdf</a>, <a href="/format/2310.00195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Strategies for Modeling Sign Language Phonology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kezar%2C+L">Lee Kezar</a>, 
<a href="/search/cs?searchtype=author&query=Carlin%2C+R">Riley Carlin</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+T">Tejas Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Sehyr%2C+Z">Zed Sehyr</a>, 
<a href="/search/cs?searchtype=author&query=Caselli%2C+N">Naomi Caselli</a>, 
<a href="/search/cs?searchtype=author&query=Thomason%2C+J">Jesse Thomason</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the European Symposium for Artificial Neural Networks (ESANN) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Like speech, signs are composed of discrete, recombinable features called
phonemes. Prior work shows that models which can recognize phonemes are better
at sign recognition, motivating deeper exploration into strategies for modeling
sign language phonemes. In this work, we learn graph convolution networks to
recognize the sixteen phoneme "types" found in ASL-LEX 2.0. Specifically, we
explore how learning strategies like multi-task and curriculum learning can
leverage mutually useful information between phoneme types to facilitate better
modeling of sign language phonemes. Results on the Sem-Lex Benchmark show that
curriculum learning yields an average accuracy of 87% across all phoneme types,
outperforming fine-tuning and multi-task strategies for most phoneme types.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00196" title="Abstract">arXiv:2310.00196</a> [<a href="/pdf/2310.00196" title="Download PDF">pdf</a>, <a href="/format/2310.00196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Sem-Lex Benchmark: Modeling ASL Signs and Their Phonemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kezar%2C+L">Lee Kezar</a>, 
<a href="/search/cs?searchtype=author&query=Pontecorvo%2C+E">Elana Pontecorvo</a>, 
<a href="/search/cs?searchtype=author&query=Daniels%2C+A">Adele Daniels</a>, 
<a href="/search/cs?searchtype=author&query=Baer%2C+C">Connor Baer</a>, 
<a href="/search/cs?searchtype=author&query=Ferster%2C+R">Ruth Ferster</a>, 
<a href="/search/cs?searchtype=author&query=Berger%2C+L">Lauren Berger</a>, 
<a href="/search/cs?searchtype=author&query=Thomason%2C+J">Jesse Thomason</a>, 
<a href="/search/cs?searchtype=author&query=Sehyr%2C+Z+S">Zed Sevcikova Sehyr</a>, 
<a href="/search/cs?searchtype=author&query=Caselli%2C+N">Naomi Caselli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the ACM Conference on Accessibility (ASSETS) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Sign language recognition and translation technologies have the potential to
increase access and inclusion of deaf signing communities, but research
progress is bottlenecked by a lack of representative data. We introduce a new
resource for American Sign Language (ASL) modeling, the Sem-Lex Benchmark. The
Benchmark is the current largest of its kind, consisting of over 84k videos of
isolated sign productions from deaf ASL signers who gave informed consent and
received compensation. Human experts aligned these videos with other sign
language resources including ASL-LEX, SignBank, and ASL Citizen, enabling
useful expansions for sign and phonological feature recognition. We present a
suite of experiments which make use of the linguistic information in ASL-LEX,
evaluating the practicality and fairness of the Sem-Lex Benchmark for isolated
sign recognition (ISR). We use an SL-GCN model to show that the phonological
features are recognizable with 85% accuracy, and that they are effective as an
auxiliary target to ISR. Learning to recognize phonological features alongside
gloss results in a 6% improvement for few-shot ISR accuracy and a 2%
improvement for ISR accuracy overall. Instructions for downloading the data can
be found at https://github.com/leekezar/SemLex.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00197" title="Abstract">arXiv:2310.00197</a> [<a href="/pdf/2310.00197" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identification, Impacts, and Opportunities of Three Common Measurement  Considerations when using Digital Trace Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muise%2C+D">Daniel Muise</a>, 
<a href="/search/cs?searchtype=author&query=Ram%2C+N">Nilam Ram</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+T">Thomas Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Reeves%2C+B">Byron Reeves</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages with refs, 1 figure, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY); Econometrics (econ.EM)

</div>
<p class="mathjax">Cataloguing specific URLs, posts, and applications with digital traces is the
new best practice for measuring media use and content consumption. Despite the
apparent accuracy that comes with greater granularity, however, digital traces
may introduce additional ambiguity and new errors into the measurement of media
use. In this note, we identify three new measurement challenges when using
Digital Trace Data that were recently uncovered using a new measurement
framework - Screenomics - that records media use at the granularity of
individual screenshots obtained every few seconds as people interact with
mobile devices. We label the considerations as follows: (1) entangling - the
common measurement error introduced by proxying exposure to content by exposure
to format; (2) flattening - aggregating unique segments of media interaction
without incorporating temporal information, most commonly intraindividually and
(3) bundling - summation of the durations of segments of media interaction,
indiscriminate with respect to variations across media segments.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00198" title="Abstract">arXiv:2310.00198</a> [<a href="/pdf/2310.00198" title="Download PDF">pdf</a>, <a href="/format/2310.00198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Non-IID Federated Learning via Heterogeneity-Guided Client  Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huancheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Vikalo%2C+H">Haris Vikalo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Statistical heterogeneity of data present at client devices in a federated
learning (FL) system renders the training of a global model in such systems
difficult. Particularly challenging are the settings where due to resource
constraints only a small fraction of clients can participate in any given round
of FL. Recent approaches to training a global model in FL systems with non-IID
data have focused on developing client selection methods that aim to sample
clients with more informative updates of the model. However, existing client
selection techniques either introduce significant computation overhead or
perform well only in the scenarios where clients have data with similar
heterogeneity profiles. In this paper, we propose HiCS-FL (Federated Learning
via Hierarchical Clustered Sampling), a novel client selection method in which
the server estimates statistical heterogeneity of a client's data using the
client's update of the network's output layer and relies on this information to
cluster and sample the clients. We analyze the ability of the proposed
techniques to compare heterogeneity of different datasets, and characterize
convergence of the training process that deploys the introduced client
selection method. Extensive experimental results demonstrate that in non-IID
settings HiCS-FL achieves faster convergence and lower training variance than
state-of-the-art FL client selection schemes. Notably, HiCS-FL drastically
reduces computation cost compared to existing selection schemes and is
adaptable to different heterogeneity scenarios.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00199" title="Abstract">arXiv:2310.00199</a> [<a href="/pdf/2310.00199" title="Download PDF">pdf</a>, <a href="/format/2310.00199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeformUX-Net: Exploring a 3D Foundation Backbone for Medical Image  Segmentation with Depthwise Deformable Convolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H+H">Ho Hin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Quan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+S">Shunxing Bao</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+Y">Yuankai Huo</a>, 
<a href="/search/cs?searchtype=author&query=Landman%2C+B+A">Bennett A. Landman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, the source code with our pre-trained model is available at this <a href="https://github.com/MASILab/deform-uxnet">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The application of 3D ViTs to medical image segmentation has seen remarkable
strides, somewhat overshadowing the budding advancements in Convolutional
Neural Network (CNN)-based models. Large kernel depthwise convolution has
emerged as a promising technique, showcasing capabilities akin to hierarchical
transformers and facilitating an expansive effective receptive field (ERF)
vital for dense predictions. Despite this, existing core operators, ranging
from global-local attention to large kernel convolution, exhibit inherent
trade-offs and limitations (e.g., global-local range trade-off, aggregating
attentional features). We hypothesize that deformable convolution can be an
exploratory alternative to combine all advantages from the previous operators,
providing long-range dependency, adaptive spatial aggregation and computational
efficiency as a foundation backbone. In this work, we introduce 3D
DeformUX-Net, a pioneering volumetric CNN model that adeptly navigates the
shortcomings traditionally associated with ViTs and large kernel convolution.
Specifically, we revisit volumetric deformable convolution in depth-wise
setting to adapt long-range dependency with computational efficiency. Inspired
by the concepts of structural re-parameterization for convolution kernel
weights, we further generate the deformable tri-planar offsets by adapting a
parallel branch (starting from $1\times1\times1$ convolution), providing
adaptive spatial aggregation across all channels. Our empirical evaluations
reveal that the 3D DeformUX-Net consistently outperforms existing
state-of-the-art ViTs and large kernel convolution models across four
challenging public datasets, spanning various scales from organs (KiTS: 0.680
to 0.720, MSD Pancreas: 0.676 to 0.717, AMOS: 0.871 to 0.902) to vessels (e.g.,
MSD hepatic vessels: 0.635 to 0.671) in mean Dice.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00204" title="Abstract">arXiv:2310.00204</a> [<a href="/pdf/2310.00204" title="Download PDF">pdf</a>, <a href="/format/2310.00204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding Pragmatic Differences Between Disciplines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kezar%2C+L">Lee Kezar</a>, 
<a href="/search/cs?searchtype=author&query=Pujara%2C+J">Jay Pujara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Proceedings of the Second Workshop on Scholarly Document Processing (at NAACL 2021) <a href="https://aclanthology.org/2021.sdp-1.10/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Scholarly documents have a great degree of variation, both in terms of
content (semantics) and structure (pragmatics). Prior work in scholarly
document understanding emphasizes semantics through document summarization and
corpus topic modeling but tends to omit pragmatics such as document
organization and flow. Using a corpus of scholarly documents across 19
disciplines and state-of-the-art language modeling techniques, we learn a fixed
set of domain-agnostic descriptors for document sections and "retrofit" the
corpus to these descriptors (also referred to as "normalization"). Then, we
analyze the position and ordering of these descriptors across documents to
understand the relationship between discipline and structure. We report
within-discipline structural archetypes, variability, and between-discipline
comparisons, supporting the hypothesis that scholarly communities, despite
their size, diversity, and breadth, share similar avenues for expressing their
work. Our findings lay the foundation for future work in assessing research
quality, domain style transfer, and further pragmatic analysis.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00205" title="Abstract">arXiv:2310.00205</a> [<a href="/pdf/2310.00205" title="Download PDF">pdf</a>, <a href="/format/2310.00205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study on the Use of Static Analysis Tools in Open Source  Embedded Software
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+M">Mingjie Shen</a>, 
<a href="/search/cs?searchtype=author&query=Pillai%2C+A">Akul Pillai</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+B+A">Brian A. Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+J+C">James C. Davis</a>, 
<a href="/search/cs?searchtype=author&query=Machiry%2C+A">Aravind Machiry</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">This paper performs the first study to understand the prevalence, challenges,
and effectiveness of using Static Application Security Testing (SAST) tools on
Open-Source Embedded Software (EMBOSS) repositories. We collect a corpus of 258
of the most popular EMBOSS projects, representing 13 distinct categories such
as real-time operating systems, network stacks, and applications. To understand
the current use of SAST tools on EMBOSS, we measured this corpus and surveyed
developers. To understand the challenges and effectiveness of using SAST tools
on EMBOSS projects, we applied these tools to the projects in our corpus. We
report that almost none of these projects (just 3%) use SAST tools beyond those
baked into the compiler, and developers give rationales such as ineffectiveness
and false positives. In applying SAST tools ourselves, we show that minimal
engineering effort and project expertise are needed to apply many tools to a
given EMBOSS project. GitHub's CodeQL was the most effective SAST tool -- using
its built-in security checks we found a total of 540 defects (with a false
positive rate of 23%) across the 258 projects, with 399 (74%) likely security
vulnerabilities, including in projects maintained by Microsoft, Amazon, and the
Apache Foundation. EMBOSS engineers have confirmed 273 (51%) of these defects,
mainly by accepting our pull requests. Two CVEs were issued. In summary, we
urge EMBOSS engineers to adopt the current generation of SAST tools, which
offer low false positive rates and are effective at finding security-relevant
defects.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00206" title="Abstract">arXiv:2310.00206</a> [<a href="/pdf/2310.00206" title="Download PDF">pdf</a>, <a href="/format/2310.00206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Investigation of Multi-feature Extraction and Super-resolution with  Fast Microphone Arrays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+E+T">Eric T. Chang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Runsheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ballentine%2C+P">Peter Ballentine</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingxi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+T">Trey Smith</a>, 
<a href="/search/cs?searchtype=author&query=Coltin%2C+B">Brian Coltin</a>, 
<a href="/search/cs?searchtype=author&query=Kymissis%2C+I">Ioannis Kymissis</a>, 
<a href="/search/cs?searchtype=author&query=Ciocarlie%2C+M">Matei Ciocarlie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, submitted to 2024 IEEE International Conference on Robotics and Automation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this work, we use MEMS microphones as vibration sensors to simultaneously
classify texture and estimate contact position and velocity. Vibration sensors
are an important facet of both human and robotic tactile sensing, providing
fast detection of contact and onset of slip. Microphones are an attractive
option for implementing vibration sensing as they offer a fast response and can
be sampled quickly, are affordable, and occupy a very small footprint. Our
prototype sensor uses only a sparse array of distributed MEMS microphones (8-9
mm spacing) embedded under an elastomer. We use transformer-based architectures
for data analysis, taking advantage of the microphones' high sampling rate to
run our models on time-series data as opposed to individual snapshots. This
approach allows us to obtain 77.3% average accuracy on 4-class texture
classification (84.2% when excluding the slowest drag velocity), 1.5 mm median
error on contact localization, and 4.5 mm/s median error on contact velocity.
We show that the learned texture and localization models are robust to varying
velocity and generalize to unseen velocities. We also report that our sensor
provides fast contact detection, an important advantage of fast transducers.
This investigation illustrates the capabilities one can achieve with a MEMS
microphone array alone, leaving valuable sensor real estate available for
integration with complementary tactile sensing modalities.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00207" title="Abstract">arXiv:2310.00207</a> [<a href="/pdf/2310.00207" title="Download PDF">pdf</a>, <a href="/ps/2310.00207" title="Download PostScript">ps</a>, <a href="/format/2310.00207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Unseen Multiword Expressions in American Sign Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kezar%2C+L">Lee Kezar</a>, 
<a href="/search/cs?searchtype=author&query=Shukla%2C+A">Aryan Shukla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report, unpublished
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multiword expressions present unique challenges in many translation tasks. In
an attempt to ultimately apply a multiword expression detection system to the
translation of American Sign Language, we built and tested two systems that
apply word embeddings from GloVe to determine whether or not the word
embeddings of lexemes can be used to predict whether or not those lexemes
compose a multiword expression. It became apparent that word embeddings carry
data that can detect non-compositionality with decent accuracy.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00212" title="Abstract">arXiv:2310.00212</a> [<a href="/pdf/2310.00212" title="Download PDF">pdf</a>, <a href="/format/2310.00212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for  LLM Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Banghua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruoyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Zhaojin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Ramchandran%2C+K">Kannan Ramchandran</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J">Jiantao Jiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Language Models (LLMs) can acquire extensive world knowledge through
pre-training on large corpora. However, due to exposure to low-quality data,
LLMs may exhibit harmful behavior without aligning with human values. The
dominant approach for steering LLMs towards beneficial behavior involves
Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy
Optimization (PPO) serving as the default RL optimizer. Despite its
effectiveness, PPO has limitations when optimizing rewards trained from
comparison-based loss. Primarily, PPO is not invariant to equivalent reward
functions containing identical preference information due to the need to
calibrate the reward scale. Additionally, PPO's necessity for token-wise
updates introduces complexity in both function approximation and algorithm
design compared to trajectory-wise optimization. This paper proposes a new
framework, reinforcement learning with relative feedback, and a novel
trajectory-wise policy gradient algorithm, Pairwise Proximal Policy
Optimization (P3O) that operates directly on comparative rewards. We show
theoretically that P3O is invariant to equivalent rewards and avoids the
complexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO
in the KL-Reward trade-off and can align with human preferences as well as or
better than prior methods. In summary, this work introduces a simpler yet
effective approach for aligning LLMs to human preferences through relative
feedback.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00213" title="Abstract">arXiv:2310.00213</a> [<a href="/pdf/2310.00213" title="Download PDF">pdf</a>, <a href="/format/2310.00213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LSOR: Longitudinally-Consistent Self-Organized Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+J">Jiahong Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qingyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Adeli%2C+E">Ehsan Adeli</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Wei Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zaharchuk%2C+G">Greg Zaharchuk</a>, 
<a href="/search/cs?searchtype=author&query=Pohl%2C+K+M">Kilian M. Pohl</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Medical Image Computing and Computer
  Assisted Intervention (MICCAI) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Interpretability is a key issue when applying deep learning models to
longitudinal brain MRIs. One way to address this issue is by visualizing the
high-dimensional latent spaces generated by deep learning via self-organizing
maps (SOM). SOM separates the latent space into clusters and then maps the
cluster centers to a discrete (typically 2D) grid preserving the
high-dimensional relationship between clusters. However, learning SOM in a
high-dimensional latent space tends to be unstable, especially in a
self-supervision setting. Furthermore, the learned SOM grid does not
necessarily capture clinically interesting information, such as brain age. To
resolve these issues, we propose the first self-supervised SOM approach that
derives a high-dimensional, interpretable representation stratified by brain
age solely based on longitudinal brain MRIs (i.e., without demographic or
cognitive information). Called Longitudinally-consistent Self-Organized
Representation learning (LSOR), the method is stable during training as it
relies on soft clustering (vs. the hard cluster assignments used by existing
SOM). Furthermore, our approach generates a latent space stratified according
to brain age by aligning trajectories inferred from longitudinal MRIs to the
reference vector associated with the corresponding SOM cluster. When applied to
longitudinal MRIs of the Alzheimer's Disease Neuroimaging Initiative (ADNI,
N=632), LSOR generates an interpretable latent space and achieves comparable or
higher accuracy than the state-of-the-art representations with respect to the
downstream tasks of classification (static vs. progressive mild cognitive
impairment) and regression (determining ADAS-Cog score of all subjects). The
code is available at
https://github.com/ouyangjiahong/longitudinal-som-single-modality.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00214" title="Abstract">arXiv:2310.00214</a> [<a href="/pdf/2310.00214" title="Download PDF">pdf</a>, <a href="/ps/2310.00214" title="Download PostScript">ps</a>, <a href="/format/2310.00214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum MDS Codes with length $n\equiv 0,1($mod$\,\frac{q\pm1}{2})$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+R">Ruhao Wan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">An important family of quantum codes is the quantum
maximum-distance-separable (MDS) codes. In this paper, we construct some new
classes of quantum MDS codes by generalized Reed-Solomon (GRS) codes and
Hermitian construction. In addition, the length $n$ of most of the quantum MDS
codes we constructed satisfies $n\equiv 0,1($mod$\,\frac{q\pm1}{2})$, which is
different from previously known code lengths. At the same time, the quantum MDS
codes we construct have large minimum distances that are greater than $q/2+1$.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00215" title="Abstract">arXiv:2310.00215</a> [<a href="/pdf/2310.00215" title="Download PDF">pdf</a>, <a href="/format/2310.00215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit collaboration with a drawing machine through dance movements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grinberg%2C+I">Itay Grinberg</a>, 
<a href="/search/cs?searchtype=author&query=Bremers%2C+A">Alexandra Bremers</a>, 
<a href="/search/cs?searchtype=author&query=Pancoast%2C+L">Louisa Pancoast</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+W">Wendy Ju</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">In this demonstration, we exhibit the initial results of an ongoing body of
exploratory work, investigating the potential for creative machines to
communicate and collaborate with people through movement as a form of implicit
interaction. The paper describes a Wizard-of-Oz demo, where a hidden wizard
controls an AxiDraw drawing robot while a participant collaborates with it to
draw a custom postcard. This demonstration aims to gather perspectives from the
computational fabrication community regarding how practitioners of fabrication
with machines experience interacting with a mixed-initiative collaborative
machine.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00221" title="Abstract">arXiv:2310.00221</a> [<a href="/pdf/2310.00221" title="Download PDF">pdf</a>, <a href="/format/2310.00221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Random Noise: Insights on Anonymization Strategies from a Latent  Bandit Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galozy%2C+A">Alexander Galozy</a>, 
<a href="/search/cs?searchtype=author&query=Alawadi%2C+S">Sadi Alawadi</a>, 
<a href="/search/cs?searchtype=author&query=Kebande%2C+V">Victor Kebande</a>, 
<a href="/search/cs?searchtype=author&query=Nowaczyk%2C+S">S&#x142;awomir Nowaczyk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper investigates the issue of privacy in a learning scenario where
users share knowledge for a recommendation task. Our study contributes to the
growing body of research on privacy-preserving machine learning and underscores
the need for tailored privacy techniques that address specific attack patterns
rather than relying on one-size-fits-all solutions. We use the latent bandit
setting to evaluate the trade-off between privacy and recommender performance
by employing various aggregation strategies, such as averaging, nearest
neighbor, and clustering combined with noise injection. More specifically, we
simulate a linkage attack scenario leveraging publicly available auxiliary
information acquired by the adversary. Our results on three open real-world
datasets reveal that adding noise using the Laplace mechanism to an individual
user's data record is a poor choice. It provides the highest regret for any
noise level, relative to de-anonymization probability and the ADS metric.
Instead, one should combine noise with appropriate aggregation strategies. For
example, using averages from clusters of different sizes provides flexibility
not achievable by varying the amount of noise alone. Generally, no single
aggregation strategy can consistently achieve the optimum regret for a given
desired level of privacy.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00222" title="Abstract">arXiv:2310.00222</a> [<a href="/pdf/2310.00222" title="Download PDF">pdf</a>, <a href="/format/2310.00222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Source Inference Attacks: Beyond Membership Inference Attacks in  Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hongsheng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Salcic%2C+Z">Zoran Salcic</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Choo%2C+K+R">Kim-Kwang Raymond Choo</a>, 
<a href="/search/cs?searchtype=author&query=Dobbie%2C+G">Gillian Dobbie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Dependable and Secure Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Federated learning (FL) is a popular approach to facilitate privacy-aware
machine learning since it allows multiple clients to collaboratively train a
global model without granting others access to their private data. It is,
however, known that FL can be vulnerable to membership inference attacks
(MIAs), where the training records of the global model can be distinguished
from the testing records. Surprisingly, research focusing on the investigation
of the source inference problem appears to be lacking. We also observe that
identifying a training record's source client can result in privacy breaches
extending beyond MIAs. For example, consider an FL application where multiple
hospitals jointly train a COVID-19 diagnosis model, membership inference
attackers can identify the medical records that have been used for training,
and any additional identification of the source hospital can result the patient
from the particular hospital more prone to discrimination. Seeking to
contribute to the literature gap, we take the first step to investigate source
privacy in FL. Specifically, we propose a new inference attack (hereafter
referred to as source inference attack -- SIA), designed to facilitate an
honest-but-curious server to identify the training record's source client. The
proposed SIAs leverage the Bayesian theorem to allow the server to implement
the attack in a non-intrusive manner without deviating from the defined FL
protocol. We then evaluate SIAs in three different FL frameworks to show that
in existing FL frameworks, the clients sharing gradients, model parameters, or
predictions on a public dataset will leak such source information to the
server. We also conduct extensive experiments on various datasets to
investigate the key factors in an SIA. The experimental results validate the
efficacy of the proposed SIAs.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00224" title="Abstract">arXiv:2310.00224</a> [<a href="/pdf/2310.00224" title="Download PDF">pdf</a>, <a href="/format/2310.00224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional  Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nair%2C+N+G">Nithin Gopalakrishnan Nair</a>, 
<a href="/search/cs?searchtype=author&query=Cherian%2C+A">Anoop Cherian</a>, 
<a href="/search/cs?searchtype=author&query=Lohit%2C+S">Suhas Lohit</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ye Wang</a>, 
<a href="/search/cs?searchtype=author&query=Koike-Akino%2C+T">Toshiaki Koike-Akino</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+V+M">Vishal M. Patel</a>, 
<a href="/search/cs?searchtype=author&query=Marks%2C+T+K">Tim K. Marks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Conditional generative models typically demand large annotated training sets
to achieve high-quality synthesis. As a result, there has been significant
interest in designing models that perform plug-and-play generation, i.e., to
use a predefined or pretrained model, which is not explicitly trained on the
generative task, to guide the generative process (e.g., using language).
However, such guidance is typically useful only towards synthesizing high-level
semantics rather than editing fine-grained details as in image-to-image
translation tasks. To this end, and capitalizing on the powerful fine-grained
generative control offered by the recent diffusion-based generative models, we
introduce Steered Diffusion, a generalized framework for photorealistic
zero-shot conditional image generation using a diffusion model trained for
unconditional generation. The key idea is to steer the image generation of the
diffusion model at inference time via designing a loss using a pre-trained
inverse model that characterizes the conditional task. This loss modulates the
sampling trajectory of the diffusion process. Our framework allows for easy
incorporation of multiple conditions during inference. We present experiments
using steered diffusion on several tasks including inpainting, colorization,
text-guided semantic editing, and image super-resolution. Our results
demonstrate clear qualitative and quantitative improvements over
state-of-the-art diffusion-based plug-and-play models while adding negligible
additional computational cost.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00226" title="Abstract">arXiv:2310.00226</a> [<a href="/pdf/2310.00226" title="Download PDF">pdf</a>, <a href="/format/2310.00226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A simple GPU implementation of spectral-element methods for solving 3D  Poisson type equations on cartesian meshes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+X">Xinyu Liu</a>, 
<a href="/search/math?searchtype=author&query=Shen%2C+J">Jie Shen</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+X">Xiangxiong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">It is well known since 1960s that by exploring the tensor product structure
of the discrete Laplacian on Cartesian meshes, one can develop a simple direct
Poisson solver with an $\mathcal O(N^{\frac{d+1}d})$ complexity in
$d$-dimension. The GPU acceleration of numerically solving PDEs has been
explored successfully around fifteen years ago and become more and more popular
in the past decade, driven by significant advancement in both hardware and
software technologies, especially in the recent few years. We present in this
paper a simple but extremely fast MATLAB implementation on a modern GPU, which
can be easily reproduced, for solving 3D Poisson type equations using a
spectral-element method. In particular, it costs less than one second on a
Nvidia A100 for solving a Poisson equation with one billion degree of freedoms.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00227" title="Abstract">arXiv:2310.00227</a> [<a href="/pdf/2310.00227" title="Download PDF">pdf</a>, <a href="/format/2310.00227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling for Training Time and Post-hoc Out-of-distribution Detection  Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Rongyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Franchi%2C+G">Gianni Franchi</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+A">Angela Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The capacity of a modern deep learning system to determine if a sample falls
within its realm of knowledge is fundamental and important. In this paper, we
offer insights and analyses of recent state-of-the-art out-of-distribution
(OOD) detection methods - extremely simple activation shaping (ASH). We
demonstrate that activation pruning has a detrimental effect on OOD detection,
while activation scaling enhances it. Moreover, we propose SCALE, a simple yet
effective post-hoc network enhancement method for OOD detection, which attains
state-of-the-art OOD detection performance without compromising in-distribution
(ID) accuracy. By integrating scaling concepts into the training process to
capture a sample's ID characteristics, we propose Intermediate Tensor SHaping
(ISH), a lightweight method for training time OOD detection enhancement. We
achieve AUROC scores of +1.85\% for near-OOD and +0.74\% for far-OOD datasets
on the OpenOOD v1.5 ImageNet-1K benchmark. Our code and models are available at
https://github.com/kai422/SCALE.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00228" title="Abstract">arXiv:2310.00228</a> [<a href="/pdf/2310.00228" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> King of the Hill: C2 for Next Generation Swarm Warfare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adams%2C+T">Takuma Adams</a>, 
<a href="/search/cs?searchtype=author&query=McLennan-Smith%2C+T">Timothy McLennan-Smith</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Adaptation and Self-Organizing Systems (nlin.AO)

</div>
<p class="mathjax">As the reliability of cheap, off-the-shelf autonomous platforms increases, so
does the risk posed by intelligent multi-agent systems to military operations.
In the contemporary context of the Russo-Ukrainian war alone, we have seen
autonomous aerial vehicles and surface vessels deployed both individually and
in multitude to deliver critical effects to both sides. While there is a large
body of literature on tactical level communications and interactions between
agents, the exploration of high-level command and control (C2) structures that
will underpin future autonomous multi-agent military operations is a less
explored area of research. We propose a quantitative game-theoretic framework
to study effective C2 structures in cooperative and competitive multi-agent
swarming scenarios. To test our framework, we construct a virtual environment
where two adversarial swarms compete to achieve outcomes comparable to
real-world scenarios. The framework we present in this paper enables us to
quickly test and interrogate different C2 configurations in multi-agent systems
to explore C2 as a force multiplier when at a force disadvantage.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00229" title="Abstract">arXiv:2310.00229</a> [<a href="/pdf/2310.00229" title="Download PDF">pdf</a>, <a href="/format/2310.00229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Spatial and Temporal Abstraction in Planning for Better  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+M">Mingde Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Alver%2C+S">Safa Alver</a>, 
<a href="/search/cs?searchtype=author&query=van+Seijen%2C+H">Harm van Seijen</a>, 
<a href="/search/cs?searchtype=author&query=Laroche%2C+R">Romain Laroche</a>, 
<a href="/search/cs?searchtype=author&query=Precup%2C+D">Doina Precup</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Inspired by human conscious planning, we propose Skipper, a model-based
reinforcement learning agent that utilizes spatial and temporal abstractions to
generalize learned skills in novel situations. It automatically decomposes the
task at hand into smaller-scale, more manageable subtasks and hence enables
sparse decision-making and focuses its computation on the relevant parts of the
environment. This relies on the definition of a high-level proxy problem
represented as a directed graph, in which vertices and edges are learned
end-to-end using hindsight. Our theoretical analyses provide performance
guarantees under appropriate assumptions and establish where our approach is
expected to be helpful. Generalization-focused experiments validate Skipper's
significant advantage in zero-shot generalization, compared to existing
state-of-the-art hierarchical planning methods.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00230" title="Abstract">arXiv:2310.00230</a> [<a href="/pdf/2310.00230" title="Download PDF">pdf</a>, <a href="/format/2310.00230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SLM: Bridge the thin gap between speech and text foundation models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingqiu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Wei Han</a>, 
<a href="/search/cs?searchtype=author&query=Shafran%2C+I">Izhak Shafran</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zelin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+C">Chung-Cheng Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yuan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N">Nanxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Soltau%2C+H">Hagen Soltau</a>, 
<a href="/search/cs?searchtype=author&query=Rubenstein%2C+P">Paul Rubenstein</a>, 
<a href="/search/cs?searchtype=author&query=Zilka%2C+L">Lukas Zilka</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dian Yu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zhong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Pundak%2C+G">Golan Pundak</a>, 
<a href="/search/cs?searchtype=author&query=Siddhartha%2C+N">Nikhil Siddhartha</a>, 
<a href="/search/cs?searchtype=author&query=Schalkwyk%2C+J">Johan Schalkwyk</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yonghui Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We present a joint Speech and Language Model (SLM), a multitask,
multilingual, and dual-modal model that takes advantage of pretrained
foundational speech and language models. SLM freezes the pretrained foundation
models to maximally preserves their capabilities, and only trains a simple
adapter with just 1\% (156M) of the foundation models' parameters. This
adaptation not only leads SLM to achieve strong performance on conventional
tasks such as speech recognition (ASR) and speech translation (AST), but also
introduces the novel capability of zero-shot instruction-following for more
diverse tasks: given a speech input and a text instruction, SLM is able to
perform unseen generation tasks including contextual biasing ASR using
real-time context, dialog generation, speech continuation, and question
answering, etc. Our approach demonstrates that the representational gap between
pretrained speech and language models might be narrower than one would expect,
and can be bridged by a simple adaptation mechanism. As a result, SLM is not
only efficient to train, but also inherits strong capabilities already acquired
in foundation models of different modalities.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00233" title="Abstract">arXiv:2310.00233</a> [<a href="/pdf/2310.00233" title="Download PDF">pdf</a>, <a href="/format/2310.00233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> causalimages: An R Package for Causal Inference with Earth Observation,  Bio-medical, and Social Science Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jerzak%2C+C+T">Connor T. Jerzak</a>, 
<a href="/search/cs?searchtype=author&query=Daoud%2C+A">Adel Daoud</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> For accompanying software, see <a href="https://github.com/AIandGlobalDevelopmentLab/causalimages-software">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">The causalimages R package enables causal inference with image and image
sequence data, providing new tools for integrating novel data sources like
satellite and bio-medical imagery into the study of cause and effect. One set
of functions enables image-based causal inference analyses. For example, one
key function decomposes treatment effect heterogeneity by images using an
interpretable Bayesian framework. This allows for determining which types of
images or image sequences are most responsive to interventions. A second
modeling function allows researchers to control for confounding using images.
The package also allows investigators to produce embeddings that serve as
vector summaries of the image or video content. Finally, infrastructural
functions are also provided, such as tools for writing large-scale image and
image sequence data as sequentialized byte strings for more rapid image
analysis. causalimages therefore opens new capabilities for causal inference in
R, letting researchers use informative imagery in substantive analyses in a
fast and accessible manner.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00234" title="Abstract">arXiv:2310.00234</a> [<a href="/pdf/2310.00234" title="Download PDF">pdf</a>, <a href="/format/2310.00234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pixel-Inconsistency Modeling for Image Manipulation Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+C">Chenqi Kong</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+A">Anwei Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Rocha%2C+A">Anderson Rocha</a>, 
<a href="/search/cs?searchtype=author&query=Kot%2C+A+C">Alex C. Kot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Digital image forensics plays a crucial role in image authentication and
manipulation localization. Despite the progress powered by deep neural
networks, existing forgery localization methodologies exhibit limitations when
deployed to unseen datasets and perturbed images (i.e., lack of generalization
and robustness to real-world applications). To circumvent these problems and
aid image integrity, this paper presents a generalized and robust manipulation
localization model through the analysis of pixel inconsistency artifacts. The
rationale is grounded on the observation that most image signal processors
(ISP) involve the demosaicing process, which introduces pixel correlations in
pristine images. Moreover, manipulating operations, including splicing,
copy-move, and inpainting, directly affect such pixel regularity. We,
therefore, first split the input image into several blocks and design masked
self-attention mechanisms to model the global pixel dependency in input images.
Simultaneously, we optimize another local pixel dependency stream to mine local
manipulation clues within input forgery images. In addition, we design novel
Learning-to-Weight Modules (LWM) to combine features from the two streams,
thereby enhancing the final forgery localization performance. To improve the
training process, we propose a novel Pixel-Inconsistency Data Augmentation
(PIDA) strategy, driving the model to focus on capturing inherent pixel-level
artifacts instead of mining semantic forgery traces. This work establishes a
comprehensive benchmark integrating 15 representative detection models across
12 datasets. Extensive experiments show that our method successfully extracts
inherent pixel-inconsistency forgery fingerprints and achieve state-of-the-art
generalization and robustness performances in image manipulation localization.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00236" title="Abstract">arXiv:2310.00236</a> [<a href="/pdf/2310.00236" title="Download PDF">pdf</a>, <a href="/format/2310.00236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compensated sum and delayed update for time dependent wave simulations  at half precision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gao%2C+L">Longfei Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">On modern hardware, the speed of memory operation is often the limiting
factor for execution time for many scientific applications, particularly for
those related to PDE discretizations. This motivates us to explore the
possibility of operating at half precision to reduce memory footprint and hence
utilize the memory bandwidth more effectively. Specifically, we study the
viability of half precision simulations for time dependent wave equations in
this work. Potential pitfalls when naively switching to half precision in these
simulations are illustrated. We then demonstrate that replacing the standard
floating point sum with the compensated sum for solution updates can
significantly improve the quality of the simulation results.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00239" title="Abstract">arXiv:2310.00239</a> [<a href="/pdf/2310.00239" title="Download PDF">pdf</a>, <a href="/format/2310.00239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaptNet: Policy Adaptation for Physics-Based Character Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Pei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+K">Kaixiang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Andrews%2C+S">Sheldon Andrews</a>, 
<a href="/search/cs?searchtype=author&query=Kry%2C+P+G">Paul G. Kry</a>, 
<a href="/search/cs?searchtype=author&query=Neff%2C+M">Michael Neff</a>, 
<a href="/search/cs?searchtype=author&query=McGuire%2C+M">Morgan McGuire</a>, 
<a href="/search/cs?searchtype=author&query=Karamouzas%2C+I">Ioannis Karamouzas</a>, 
<a href="/search/cs?searchtype=author&query=Zordan%2C+V">Victor Zordan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGGRAPH Asia 2023. Video: <a href="https://youtu.be/WxmJSCNFb28">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ACM Transactions on Graphics 42, 6, Article 112.1522 (December
  2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Motivated by humans' ability to adapt skills in the learning of new ones,
this paper presents AdaptNet, an approach for modifying the latent space of
existing policies to allow new behaviors to be quickly learned from like tasks
in comparison to learning from scratch. Building on top of a given
reinforcement learning controller, AdaptNet uses a two-tier hierarchy that
augments the original state embedding to support modest changes in a behavior
and further modifies the policy network layers to make more substantive
changes. The technique is shown to be effective for adapting existing
physics-based controllers to a wide range of new styles for locomotion, new
task targets, changes in character morphology and extensive changes in
environment. Furthermore, it exhibits significant increase in learning
efficiency, as indicated by greatly reduced training times when compared to
training from scratch or using other approaches that modify existing policies.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00240" title="Abstract">arXiv:2310.00240</a> [<a href="/pdf/2310.00240" title="Download PDF">pdf</a>, <a href="/format/2310.00240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Mask-aware CLIP Representations for Zero-Shot Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiao%2C+S">Siyu Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yunchao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Humphrey Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Recently, pre-trained vision-language models have been increasingly used to
tackle the challenging zero-shot segmentation task. Typical solutions follow
the paradigm of first generating mask proposals and then adopting CLIP to
classify them. To maintain the CLIP's zero-shot transferability, previous
practices favour to freeze CLIP during training. However, in the paper, we
reveal that CLIP is insensitive to different mask proposals and tends to
produce similar predictions for various mask proposals of the same image. This
insensitivity results in numerous false positives when classifying mask
proposals. This issue mainly relates to the fact that CLIP is trained with
image-level supervision. To alleviate this issue, we propose a simple yet
effective method, named Mask-aware Fine-tuning (MAFT). Specifically,
Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary
numbers of image and mask proposals simultaneously. Then, mask-aware loss and
self-distillation loss are designed to fine-tune IP-CLIP Encoder, ensuring CLIP
is responsive to different mask proposals while not sacrificing
transferability. In this way, mask-aware representations can be easily learned
to make the true positives stand out. Notably, our solution can seamlessly plug
into most existing methods without introducing any new parameters during the
fine-tuning process. We conduct extensive experiments on the popular zero-shot
benchmarks. With MAFT, the performance of the state-of-the-art methods is
promoted by a large margin: 50.4% (+ 8.2%) on COCO, 81.8% (+ 3.2%) on
Pascal-VOC, and 8.7% (+4.3%) on ADE20K in terms of mIoU for unseen classes. The
code is available at https://github.com/jiaosiyu1999/MAFT.git.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00241" title="Abstract">arXiv:2310.00241</a> [<a href="/pdf/2310.00241" title="Download PDF">pdf</a>, <a href="/format/2310.00241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Complexity of Distance-$r$ Dominating Set Reconfiguration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+N">Niranka Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Hoang%2C+D+A">Duc A. Hoang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">For a fixed integer $r \geq 1$, a distance-$r$ dominating set of a graph $G =
(V, E)$ is a vertex subset $D \subseteq V$ such that every vertex in $V$ is
within distance $r$ from some member of $D$. Given two distance-$r$ dominating
sets $D_s, D_t$ of $G$, the Distance-$r$ Dominating Set Reconfiguration
(D$r$DSR) problem asks if there is a sequence of distance-$r$ dominating sets
that transforms $D_s$ into $D_t$ (or vice versa) such that each intermediate
member is obtained from its predecessor by applying a given reconfiguration
rule exactly once. The problem for $r = 1$ has been well-studied in the
literature. We consider D$r$DSR for $r \geq 2$ under two well-known
reconfiguration rules: Token Jumping ($\mathsf{TJ}$, which involves replacing a
member of the current D$r$DS by a non-member) and Token Sliding ($\mathsf{TS}$,
which involves replacing a member of the current D$r$DS by an adjacent
non-member). We show that D$r$DSR ($r \geq 2$) is $\mathtt{PSPACE}$-complete
under both $\mathsf{TJ}$ and $\mathsf{TS}$ on bipartite graphs, planar graphs
of maximum degree six and bounded bandwidth, and chordal graphs. On the
positive side, we show that D$r$DSR ($r \geq 2$) can be solved in polynomial
time on split graphs and cographs under both $\mathsf{TS}$ and $\mathsf{TJ}$
and on trees and interval graphs under $\mathsf{TJ}$. Along the way, we observe
some properties of a shortest reconfiguration sequence in split graphs when $r
= 2$, which may be of independent interest.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00242" title="Abstract">arXiv:2310.00242</a> [<a href="/pdf/2310.00242" title="Download PDF">pdf</a>, <a href="/ps/2310.00242" title="Download PostScript">ps</a>, <a href="/format/2310.00242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Walking = Traversable? : Traversability Prediction via Multiple Human  Object Tracking under Occlusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+J+T+Y">Jonathan Tay Yu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+K">Kanji Tanaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 figures, technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The emerging ``Floor plan from human trails (PfH)" technique has great
potential for improving indoor robot navigation by predicting the
traversability of occluded floors. This study presents an innovative approach
that replaces first-person-view sensors with a third-person-view monocular
camera mounted on the observer robot. This approach can gather measurements
from multiple humans, expanding its range of applications. The key idea is to
use two types of trackers, SLAM and MOT, to monitor stationary objects and
moving humans and assess their interactions. This method achieves stable
predictions of traversability even in challenging visual scenarios, such as
occlusions, nonlinear perspectives, depth uncertainty, and intersections
involving multiple humans. Additionally, we extend map quality metrics to apply
to traversability maps, facilitating future research. We validate our proposed
method through fusion and comparison with established techniques.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00243" title="Abstract">arXiv:2310.00243</a> [<a href="/pdf/2310.00243" title="Download PDF">pdf</a>, <a href="/format/2310.00243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Age-Optimal Multi-Flow Status Updating with Errors: A Sample-Path  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Kompella%2C+S">Sastry Kompella</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the Journal of Communications and Networks (JCN) Special Issue. arXiv admin note: substantial text overlap with <a href="/abs/1801.02394">arXiv:1801.02394</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Information Theory (cs.IT); Performance (cs.PF); Social and Information Networks (cs.SI); Systems and Control (eess.SY)

</div>
<p class="mathjax">In this paper, we study an age of information minimization problem in
continuous-time and discrete-time status updating systems that involve multiple
packet flows, multiple servers, and transmission errors. Four scheduling
policies are proposed. We develop a unifying sample-path approach and use it to
show that, when the packet generation and arrival times are synchronized across
the flows, the proposed policies are (near) optimal for minimizing any
time-dependent, symmetric, and non-decreasing penalty function of the ages of
the flows over time in a stochastic ordering sense.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00244" title="Abstract">arXiv:2310.00244</a> [<a href="/pdf/2310.00244" title="Download PDF">pdf</a>, <a href="/format/2310.00244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coordinated Rate-Splitting Multiple Access for Integrated  Satellite-Terrestrial Networks with Super-Common Message
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Juhwan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jungwoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+L">Longfei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+W">Wonjae Shin</a>, 
<a href="/search/cs?searchtype=author&query=Clerckx%2C+B">Bruno Clerckx</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Rate-splitting multiple access (RSMA) is an emerging multiple access
technique for multi-antenna networks that splits messages into common and
private parts for flexible interference mitigation. Motivated by its robustness
and scalability, it is promising to employ RSMA in integrated
satellite-terrestrial networks (ISTN), where a satellite serves satellite users
(SUs) broadly with a multibeam multicast transmission while terrestrial base
station (BS) serves cellular users (CUs) with a unicast transmission, operating
in the same frequency band. To avoid the data exchange between
satellite/cellular networks via backhaul, we assume a coordinated ISTN relying
on imperfect channel state information. We put forth a coordinated RSMA
framework tailored to the coordinated ISTN by applying inter-network
rate-splitting (RS) with a super-common message on top of intra-network RS with
common/private messages. With the unified RS design for inter- and
intra-networks, we jointly optimize the precoding and power allocation of the
private/common/super-common messages to achieve max-min fairness among all SUs
and CUs through successive convex approximation. By doing so, the power of the
super-common message can be adjusted according to interference levels of the
satellite towards CUs, thereby potentially mitigating inter-network
interference. Simulation results demonstrate the superiority and robustness of
our approach to cope with various interference and propagation conditions.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00246" title="Abstract">arXiv:2310.00246</a> [<a href="/pdf/2310.00246" title="Download PDF">pdf</a>, <a href="/format/2310.00246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A hybrid quantum-classical conditional generative adversarial network  algorithm for human-centered paradigm in cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhiliang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiaojiao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+L">Lian Tong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 9 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EURASIP Journal on Wireless Communications and Networking, 2021.
  2021(1): p. 37
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Emerging Technologies (cs.ET); Quantum Physics (quant-ph)

</div>
<p class="mathjax">As an emerging field that aims to bridge the gap between human activities and
computing systems, human-centered computing (HCC) in cloud, edge, fog has had a
huge impact on the artificial intelligence algorithms. The quantum generative
adversarial network (QGAN) is considered to be one of the quantum machine
learning algorithms with great application prospects, which also should be
improved to conform to the human-centered paradigm. The generation process of
QGAN is relatively random and the generated model does not conform to the
human-centered concept, so it is not quite suitable for real scenarios. In
order to solve these problems, a hybrid quantum-classical conditional
generative adversarial network (QCGAN) algorithm is proposed, which is a
knowledge-driven human-computer interaction computing mode that can be
implemented in cloud. The purposes of stabilizing the generation process and
realizing the interaction between human and computing process are achieved by
inputting artificial conditional information in the generator and
discriminator. The generator uses the parameterized quantum circuit with an
all-to-all connected topology, which facilitates the tuning of network
parameters during the training process. The discriminator uses the classical
neural network, which effectively avoids the "input bottleneck" of quantum
machine learning. Finally, the BAS training set is selected to conduct
experiment on the quantum cloud computing platform. The result shows that the
QCGAN algorithm can effectively converge to the Nash equilibrium point after
training and perform human-centered classification generation tasks.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00247" title="Abstract">arXiv:2310.00247</a> [<a href="/pdf/2310.00247" title="Download PDF">pdf</a>, <a href="/format/2310.00247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging the Gap Between Foundation Models and Heterogeneous Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Sixing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Mu%C3%B1oz%2C+J+P">J. Pablo Mu&#xf1;oz</a>, 
<a href="/search/cs?searchtype=author&query=Jannesari%2C+A">Ali Jannesari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated learning (FL) offers privacy-preserving decentralized machine
learning, optimizing models at edge clients without sharing private data.
Simultaneously, foundation models (FMs) have gained traction in the artificial
intelligence (AI) community due to their exceptional performance across various
tasks. However, integrating FMs into FL presents challenges, primarily due to
their substantial size and intensive resource requirements. This is especially
true when considering the resource heterogeneity in edge FL systems. We present
an adaptive framework for Resource-aware Federated Foundation Models (RaFFM) to
address these challenges. RaFFM introduces specialized model compression
algorithms tailored for FL scenarios, such as salient parameter prioritization
and high-performance subnetwork extraction. These algorithms enable dynamic
scaling of given transformer-based FMs to fit heterogeneous resource
constraints at the network edge during both FL's optimization and deployment
stages. Experimental results demonstrate that RaFFM shows significant
superiority in resource utilization efficiency and uses fewer resources to
deploy FMs to FL. Despite the lower resource consumption, target models
optimized by RaFFM achieve performance on par with traditional FL methods
applied to full-sized FMs. This is evident across tasks in both natural
language processing and computer vision domains.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00248" title="Abstract">arXiv:2310.00248</a> [<a href="/pdf/2310.00248" title="Download PDF">pdf</a>, <a href="/format/2310.00248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning State-Augmented Policies for Information Routing in  Communication Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Sourajit Das</a>, 
<a href="/search/cs?searchtype=author&query=NaderiAlizadeh%2C+N">Navid NaderiAlizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+A">Alejandro Ribeiro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper examines the problem of information routing in a large-scale
communication network, which can be formulated as a constrained statistical
learning problem having access to only local information. We delineate a novel
State Augmentation (SA) strategy to maximize the aggregate information at
source nodes using graph neural network (GNN) architectures, by deploying graph
convolutions over the topological links of the communication network. The
proposed technique leverages only the local information available at each node
and efficiently routes desired information to the destination nodes. We
leverage an unsupervised learning procedure to convert the output of the GNN
architecture to optimal information routing strategies. In the experiments, we
perform the evaluation on real-time network topologies to validate our
algorithms. Numerical simulations depict the improved performance of the
proposed method in training a GNN parameterization as compared to baseline
algorithms.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00249" title="Abstract">arXiv:2310.00249</a> [<a href="/pdf/2310.00249" title="Download PDF">pdf</a>, <a href="/format/2310.00249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MMPI: a Flexible Radiance Field Representation by Multiple Multi-plane  Images Blending
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuze He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yubin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+R">Ran Yi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong-Jin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenping Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents a flexible representation of neural radiance fields based
on multi-plane images (MPI), for high-quality view synthesis of complex scenes.
MPI with Normalized Device Coordinate (NDC) parameterization is widely used in
NeRF learning for its simple definition, easy calculation, and powerful ability
to represent unbounded scenes. However, existing NeRF works that adopt MPI
representation for novel view synthesis can only handle simple forward-facing
unbounded scenes, where the input cameras are all observing in similar
directions with small relative translations. Hence, extending these MPI-based
methods to more complex scenes like large-range or even 360-degree scenes is
very challenging. In this paper, we explore the potential of MPI and show that
MPI can synthesize high-quality novel views of complex scenes with diverse
camera distributions and view directions, which are not only limited to simple
forward-facing scenes. Our key idea is to encode the neural radiance field with
multiple MPIs facing different directions and blend them with an adaptive
blending operation. For each region of the scene, the blending operation gives
larger blending weights to those advantaged MPIs with stronger local
representation abilities while giving lower weights to those with weaker
representation abilities. Such blending operation automatically modulates the
multiple MPIs to appropriately represent the diverse local density and color
information. Experiments on the KITTI dataset and ScanNet dataset demonstrate
that our proposed MMPI synthesizes high-quality images from diverse camera pose
distributions and is fast to train, outperforming the previous fast-training
NeRF methods for novel view synthesis. Moreover, we show that MMPI can encode
extremely long trajectories and produce novel view renderings, demonstrating
its potential in applications like autonomous driving.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00254" title="Abstract">arXiv:2310.00254</a> [<a href="/pdf/2310.00254" title="Download PDF">pdf</a>, <a href="/format/2310.00254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Distributed Efficient Blockchain Oracle Scheme for Internet of Things
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xian%2C+Y">Youquan Xian</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Lianghaojie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jianyong Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+H">Hao Huo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peng Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Emerging Technologies (cs.ET)

</div>
<p class="mathjax">In recent years, blockchain has been widely applied in the Internet of Things
(IoT). Blockchain oracle, as a bridge for data communication between blockchain
and off-chain, has also received significant attention. However, the numerous
and heterogeneous devices in the IoT pose great challenges to the efficiency
and security of data acquisition for oracles. We find that the matching
relationship between data sources and oracle nodes greatly affects the
efficiency and service quality of the entire oracle system. To address these
issues, this paper proposes a distributed and efficient oracle solution
tailored for the IoT, enabling fast acquisition of real-time off-chain data.
Specifically, we first design a distributed oracle architecture that combines
both Trusted Execution Environment (TEE) devices and ordinary devices to
improve system scalability, considering the heterogeneity of IoT devices.
Secondly, based on the trusted node information provided by TEE, we determine
the matching relationship between nodes and data sources, assigning appropriate
nodes for tasks to enhance system efficiency. Through simulation experiments,
our proposed solution has been shown to effectively improve the efficiency and
service quality of the system, reducing the average response time by
approximately 9.92\% compared to conventional approaches.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00256" title="Abstract">arXiv:2310.00256</a> [<a href="/pdf/2310.00256" title="Download PDF">pdf</a>, <a href="/format/2310.00256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Performance Analysis of Graphcore IPUs: Analyzing Squared and Skewed  Matrix Multiplication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shekofteh%2C+S+-">S.-Kazem Shekofteh</a>, 
<a href="/search/cs?searchtype=author&query=Alles%2C+C">Christian Alles</a>, 
<a href="/search/cs?searchtype=author&query=Kochend%C3%B6rfer%2C+N">Nils Kochend&#xf6;rfer</a>, 
<a href="/search/cs?searchtype=author&query=Fr%C3%B6ning%2C+H">Holger Fr&#xf6;ning</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">In recent decades, High Performance Computing (HPC) has undergone significant
enhancements, particularly in the realm of hardware platforms, aimed at
delivering increased processing power while keeping power consumption within
reasonable limits. The Intelligence Processing Unit (IPU) represents an
entirely novel category of massively parallel processors, meticulously designed
to expedite parallel computations through a multitude of processing cores and
on-chip memory components interconnected via high-speed fabrics. While IPUs are
primarily tailored for machine learning applications and come equipped with
several libraries for the seamless implementation of neural networks, they also
retain the capability to execute traditional parallel programs like matrix
multiplication. However, it is essential to acknowledge that there are certain
considerations and limitations when utilizing IPUs for such tasks. This paper
embarks on an extensive analytical examination of matrix multiplications (MM)
executed on an IPU, focusing on aspects such as execution efficiency and memory
usage. Additionally, a comparative analysis is conducted, pitting the IPU
against a GPU. Our findings indicate that IPUs can outperform modern GPUs,
especially in handling the consistently challenging skewed matrix
multiplication operations. For a more comprehensive understanding, we
scrutinize various aspect ratios of matrices for these operations on an IPU and
a Turing-class GPU (RTX 2080TI), revealing that the IPU consistently delivers
more robust performance when dealing with skewed matrices compared to a GPU.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00258" title="Abstract">arXiv:2310.00258</a> [<a href="/pdf/2310.00258" title="Download PDF">pdf</a>, <a href="/format/2310.00258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleash Data Generation for Efficient and Effective Data-free Knowledge  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+M">Minh-Tuan Tran</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Trung Le</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+X">Xuan-May Le</a>, 
<a href="/search/cs?searchtype=author&query=Harandi%2C+M">Mehrtash Harandi</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+Q+H">Quan Hung Tran</a>, 
<a href="/search/cs?searchtype=author&query=Phung%2C+D">Dinh Phung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Data-Free Knowledge Distillation (DFKD) has recently made remarkable
advancements with its core principle of transferring knowledge from a teacher
neural network to a student neural network without requiring access to the
original data. Nonetheless, existing approaches encounter a significant
challenge when attempting to generate samples from random noise inputs, which
inherently lack meaningful information. Consequently, these models struggle to
effectively map this noise to the ground-truth sample distribution, resulting
in the production of low-quality data and imposing substantial time
requirements for training the generator. In this paper, we propose a novel
Noisy Layer Generation method (NAYER) which relocates the randomness source
from the input to a noisy layer and utilizes the meaningful label-text
embedding (LTE) as the input. The significance of LTE lies in its ability to
contain substantial meaningful inter-class information, enabling the generation
of high-quality samples with only a few training steps. Simultaneously, the
noisy layer plays a key role in addressing the issue of diversity in sample
generation by preventing the model from overemphasizing the constrained label
information. By reinitializing the noisy layer in each iteration, we aim to
facilitate the generation of diverse samples while still retaining the method's
efficiency, thanks to the ease of learning provided by LTE. Experiments carried
out on multiple datasets demonstrate that our NAYER not only outperforms the
state-of-the-art methods but also achieves speeds 5 to 15 times faster than
previous approaches.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00259" title="Abstract">arXiv:2310.00259</a> [<a href="/pdf/2310.00259" title="Download PDF">pdf</a>, <a href="/format/2310.00259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoHall: Automated Hallucination Dataset Generation for Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zouying Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While Large language models (LLMs) have garnered widespread applications
across various domains due to their powerful language understanding and
generation capabilities, the detection of non-factual or hallucinatory content
generated by LLMs remains scarce. Currently, one significant challenge in
hallucination detection is the laborious task of time-consuming and expensive
manual annotation of the hallucinatory generation. To address this issue, this
paper first introduces a method for automatically constructing model-specific
hallucination datasets based on existing fact-checking datasets called
AutoHall. Furthermore, we propose a zero-resource and black-box hallucination
detection method based on self-contradiction. We conduct experiments towards
prevalent open-/closed-source LLMs, achieving superior hallucination detection
performance compared to extant baselines. Moreover, our experiments reveal
variations in hallucination proportions and types among different models.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00262" title="Abstract">arXiv:2310.00262</a> [<a href="/pdf/2310.00262" title="Download PDF">pdf</a>, <a href="/ps/2310.00262" title="Download PostScript">ps</a>, <a href="/format/2310.00262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Integral Consensus Control of Multi-Agent Networks Perturbed by  Matched and Unmatched Disturbances: The Case of Directed Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Romero%2C+J+G">Jose Guadalupe Romero</a>, 
<a href="/search/eess?searchtype=author&query=Navarro-Alarcon%2C+D">David Navarro-Alarcon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Multiagent Systems (cs.MA); Robotics (cs.RO)

</div>
<p class="mathjax">This work presents a new method to design consensus controllers for perturbed
double integrator systems whose interconnection is described by a directed
graph containing a rooted spanning tree. We propose new robust controllers to
solve the consensus and synchronization problems when the systems are under the
effects of matched and unmatched disturbances. In both problems, we present
simple continuous controllers, whose integral actions allow us to handle the
disturbances. A rigorous stability analysis based on Lyapunov's direct method
for unperturbed networked systems is presented. To assess the performance of
our result, a representative simulation study is presented.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00263" title="Abstract">arXiv:2310.00263</a> [<a href="/pdf/2310.00263" title="Download PDF">pdf</a>, <a href="/ps/2310.00263" title="Download PostScript">ps</a>, <a href="/format/2310.00263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RIS-Aided Cell-Free Massive MIMO Systems for 6G: Fundamentals, System  Design, and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+E">Enyu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiayi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hongyang Du</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+B">Bo Ai</a>, 
<a href="/search/cs?searchtype=author&query=Yuen%2C+C">Chau Yuen</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>, 
<a href="/search/cs?searchtype=author&query=Xuemin">Xuemin</a> (Sherman)
<a href="/search/cs?searchtype=author&query=Shen">Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">An introduction of intelligent interconnectivity for people and things has
posed higher demands and more challenges for sixth-generation (6G) networks,
such as high spectral efficiency and energy efficiency, ultra-low latency, and
ultra-high reliability. Cell-free (CF) massive multiple-input multiple-output
(mMIMO) and reconfigurable intelligent surface (RIS), also called intelligent
reflecting surface (IRS), are two promising technologies for coping with these
unprecedented demands. Given their distinct capabilities, integrating the two
technologies to further enhance wireless network performances has received
great research and development attention. In this paper, we provide a
comprehensive survey of research on RIS-aided CF mMIMO wireless communication
systems. We first introduce system models focusing on system architecture and
application scenarios, channel models, and communication protocols.
Subsequently, we summarize the relevant studies on system operation and
resource allocation, providing in-depth analyses and discussions. Following
this, we present practical challenges faced by RIS-aided CF mMIMO systems,
particularly those introduced by RIS, such as hardware impairments and
electromagnetic interference. We summarize corresponding analyses and solutions
to further facilitate the implementation of RIS-aided CF mMIMO systems.
Furthermore, we explore an interplay between RIS-aided CF mMIMO and other
emerging 6G technologies, such as next-generation multiple-access (NGMA),
simultaneous wireless information and power transfer (SWIPT), and millimeter
wave (mmWave). Finally, we outline several research directions for future
RIS-aided CF mMIMO systems.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00264" title="Abstract">arXiv:2310.00264</a> [<a href="/pdf/2310.00264" title="Download PDF">pdf</a>, <a href="/format/2310.00264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Epistemic Logic over Similarity Graphs: Common, Distributed and Mutual  Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaolong Liang</a>, 
<a href="/search/cs?searchtype=author&query=W%C3%A1ng%2C+Y+N">Y&#xec; N. W&#xe1;ng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Computational Complexity (cs.CC); Logic (math.LO)

</div>
<p class="mathjax">In this paper, we delve into the study of epistemic logics, interpreted
through similarity models based on weighted graphs. We explore eight languages
that extend the traditional epistemic language by incorporating modalities of
common, distributed, and mutual knowledge. The concept of individual knowledge
is redefined under these similarity models. It is no longer just a matter of
personal knowledge, but is now enriched and understood as knowledge under the
individual's epistemic ability. Common knowledge is presented as higher-order
knowledge that is universally known to any degree, a definition that aligns
with existing literature. We reframe distributed knowledge as a form of
knowledge acquired by collectively leveraging the abilities of a group of
agents. In contrast, mutual knowledge is defined as the knowledge obtained
through the shared abilities of a group. We then focus on the resulting logics,
examining their relative expressivity, semantic correspondence to the classical
epistemic logic, proof systems and the computational complexity associated with
the model checking problem and the satisfiability/validity problem. This paper
offers significant insights into the logical analysis and understanding of
these enriched forms of knowledge, contributing to the broader discourse on
epistemic logic.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00265" title="Abstract">arXiv:2310.00265</a> [<a href="/pdf/2310.00265" title="Download PDF">pdf</a>, <a href="/ps/2310.00265" title="Download PostScript">ps</a>, <a href="/format/2310.00265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Describing weighted safety with weighted LTL over product  $&#x3c9;$-valuation monoids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mandrali%2C+E">Eleni Mandrali</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We define the notion of $k$-safe infinitary series over idempotent ordered
totally generalized product $\omega $-valuation monoids that satisfy specific
properties. For each element $k$ of the underlying structure (different from
the neutral elements of the additive, and the multiplicative operation) we
determine two syntactic fragments of the weighted $LTL$ with the property that
the semantics of the formulas in these fragments are $k$ -safe infinitary
series. For specific idempotent ordered totally generalized product $\omega
$-valuation monoids we provide algorithms that given a weighted B\"{u}chi
automaton and a weighted $LTL$ formula in these fragments, decide whether the
behavior of the automaton coincides with the semantics of the formula.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00268" title="Abstract">arXiv:2310.00268</a> [<a href="/pdf/2310.00268" title="Download PDF">pdf</a>, <a href="/format/2310.00268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unravel Anomalies: An End-to-end Seasonal-Trend Decomposition Approach  for Time Series Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+R">Ran Ding</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuantao Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Traditional Time-series Anomaly Detection (TAD) methods often struggle with
the composite nature of complex time-series data and a diverse array of
anomalies. We introduce TADNet, an end-to-end TAD model that leverages
Seasonal-Trend Decomposition to link various types of anomalies to specific
decomposition components, thereby simplifying the analysis of complex
time-series and enhancing detection performance. Our training methodology,
which includes pre-training on a synthetic dataset followed by fine-tuning,
strikes a balance between effective decomposition and precise anomaly
detection. Experimental validation on real-world datasets confirms TADNet's
state-of-the-art performance across a diverse range of anomalies.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00270" title="Abstract">arXiv:2310.00270</a> [<a href="/pdf/2310.00270" title="Download PDF">pdf</a>, <a href="/format/2310.00270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpatialRank: Urban Event Ranking with NDCG Optimization on  Spatiotemporal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bang An</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yongjian Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianbao Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The problem of urban event ranking aims at predicting the top-k most risky
locations of future events such as traffic accidents and crimes. This problem
is of fundamental importance to public safety and urban administration
especially when limited resources are available. The problem is, however,
challenging due to complex and dynamic spatio-temporal correlations between
locations, uneven distribution of urban events in space, and the difficulty to
correctly rank nearby locations with similar features. Prior works on event
forecasting mostly aim at accurately predicting the actual risk score or counts
of events for all the locations. Rankings obtained as such usually have low
quality due to prediction errors. Learning-to-rank methods directly optimize
measures such as Normalized Discounted Cumulative Gain (NDCG), but cannot
handle the spatiotemporal autocorrelation existing among locations. In this
paper, we bridge the gap by proposing a novel spatial event ranking approach
named SpatialRank. SpatialRank features adaptive graph convolution layers that
dynamically learn the spatiotemporal dependencies across locations from data.
In addition, the model optimizes through surrogates a hybrid NDCG loss with a
spatial component to better rank neighboring spatial locations. We design an
importance-sampling with a spatial filtering algorithm to effectively evaluate
the loss during training. Comprehensive experiments on three real-world
datasets demonstrate that SpatialRank can effectively identify the top riskiest
locations of crimes and traffic accidents and outperform state-of-art methods
in terms of NDCG by up to 12.7%.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00272" title="Abstract">arXiv:2310.00272</a> [<a href="/pdf/2310.00272" title="Download PDF">pdf</a>, <a href="/format/2310.00272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Efficacy of Large Language Models in Reflective  Assessment Methods through Chain of Thoughts Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masikisiki%2C+B">Baphumelele Masikisiki</a>, 
<a href="/search/cs?searchtype=author&query=Marivate%2C+V">Vukosi Marivate</a>, 
<a href="/search/cs?searchtype=author&query=Hlope%2C+Y">Yvette Hlope</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the Associate Computer Machinery
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models, such as Generative Pre-trained Transformer 3 (aka.
GPT-3), have been developed to understand language through the analysis of
extensive text data, allowing them to identify patterns and connections between
words. While LLMs have demonstrated impressive performance across various
text-related tasks, they encounter challenges in tasks associated with
reasoning. To address this challenge, Chain of Thought(CoT) prompting method
has been proposed as a means to enhance LLMs' proficiency in complex reasoning
tasks like solving math word problems and answering questions based on logical
argumentative reasoning. The primary aim of this research is to assess how well
four language models can grade reflective essays of third-year medical
students. The assessment will specifically target the evaluation of critical
thinking skills using CoT prompting.
<br />The research will provide the following contributions; to introduce and
educate on the process of instructing models to evaluate reflective essays from
a dataset they have not been previously trained on; to illustrate the use of
CoT prompting as an instructional approach for training large models to carry
out particular tasks. Our results suggest that among all the models, Llama-7b
performs the least effectively, displaying the highest mean squared error.
Conversely, ChatGPT emerges as the superior model, boasting a higher Cohen
kappa score value of 0.53. Lastly, it's important to note that the selected
models do prioritise user privacy by allowing users to delete their own
conducted conversations.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00273" title="Abstract">arXiv:2310.00273</a> [<a href="/pdf/2310.00273" title="Download PDF">pdf</a>, <a href="/format/2310.00273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe Stabilizing Control for Polygonal Robots in Dynamic Elliptical  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+K">Kehan Long</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+K">Khoa Tran</a>, 
<a href="/search/cs?searchtype=author&query=Leok%2C+M">Melvin Leok</a>, 
<a href="/search/cs?searchtype=author&query=Atanasov%2C+N">Nikolay Atanasov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper addresses the challenge of safe navigation for rigid-body mobile
robots in dynamic environments. We introduce an analytic approach to compute
the distance between a polygon and an ellipse, and employ it to construct a
control barrier function (CBF) for safe control synthesis. Existing CBF design
methods for mobile robot obstacle avoidance usually assume point or circular
robots, preventing their applicability to more realistic robot body geometries.
Our work enables CBF designs that capture complex robot and obstacle shapes. We
demonstrate the effectiveness of our approach in simulations highlighting
real-time obstacle avoidance in constrained and dynamic environments for both
mobile robots and multi-joint robot arms.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00274" title="Abstract">arXiv:2310.00274</a> [<a href="/pdf/2310.00274" title="Download PDF">pdf</a>, <a href="/format/2310.00274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AfriSpeech-200: Pan-African Accented Speech Dataset for Clinical and  General Domain ASR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olatunji%2C+T">Tobi Olatunji</a>, 
<a href="/search/cs?searchtype=author&query=Afonja%2C+T">Tejumade Afonja</a>, 
<a href="/search/cs?searchtype=author&query=Yadavalli%2C+A">Aditya Yadavalli</a>, 
<a href="/search/cs?searchtype=author&query=Emezue%2C+C+C">Chris Chinenye Emezue</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sahib Singh</a>, 
<a href="/search/cs?searchtype=author&query=Dossou%2C+B+F+P">Bonaventure F.P. Dossou</a>, 
<a href="/search/cs?searchtype=author&query=Osuchukwu%2C+J">Joanne Osuchukwu</a>, 
<a href="/search/cs?searchtype=author&query=Osei%2C+S">Salomey Osei</a>, 
<a href="/search/cs?searchtype=author&query=Tonja%2C+A+L">Atnafu Lambebo Tonja</a>, 
<a href="/search/cs?searchtype=author&query=Etori%2C+N">Naome Etori</a>, 
<a href="/search/cs?searchtype=author&query=Mbataku%2C+C">Clinton Mbataku</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to TACL 2023. This is a pre-MIT Press publication version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Africa has a very low doctor-to-patient ratio. At very busy clinics, doctors
could see 30+ patients per day -- a heavy patient burden compared with
developed countries -- but productivity tools such as clinical automatic speech
recognition (ASR) are lacking for these overworked clinicians. However,
clinical ASR is mature, even ubiquitous, in developed nations, and
clinician-reported performance of commercial clinical ASR systems is generally
satisfactory. Furthermore, the recent performance of general domain ASR is
approaching human accuracy. However, several gaps exist. Several publications
have highlighted racial bias with speech-to-text algorithms and performance on
minority accents lags significantly. To our knowledge, there is no publicly
available research or benchmark on accented African clinical ASR, and speech
data is non-existent for the majority of African accents. We release
AfriSpeech, 200hrs of Pan-African English speech, 67,577 clips from 2,463
unique speakers across 120 indigenous accents from 13 countries for clinical
and general domain ASR, a benchmark test set, with publicly available
pre-trained models with SOTA performance on the AfriSpeech benchmark.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00276" title="Abstract">arXiv:2310.00276</a> [<a href="/pdf/2310.00276" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Obstacles and Opportunities for Learning from Demonstration in Practical  Industrial Assembly: A Systematic Literature Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moreno%2C+V+H">V. Hernandez Moreno</a>, 
<a href="/search/cs?searchtype=author&query=Jansing%2C+S">S. Jansing</a>, 
<a href="/search/cs?searchtype=author&query=Polikarpov%2C+M">M. Polikarpov</a>, 
<a href="/search/cs?searchtype=author&query=Carmichael%2C+M+G">M. G. Carmichael</a>, 
<a href="/search/cs?searchtype=author&query=Deuse%2C+J">J. Deuse</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 8 figures, 3 tables, accepted to Robotics and Computer-Integrated Manufacturing Journal Volume 86 April 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Learning from demonstration is one of the most promising methods to
counteract the challenging long-term trends in repetitive industrial assembly.
It offers not only a programming technique that is accessible to workers on the
shop floor, reducing the need for robot experts and the associated costs but
also a possible solution to the observable shift from mass-production to
mass-customisation through flexible and generalising systems. Since the
emergence of the learning from demonstration idea in the 1980s, its
methodologies, capabilities, and achievements have constantly evolved. However,
despite reports of continued progress in academic publications, the concept has
not yet robustly emerged across the assembly industry. In light of its great
potential, this paper presents the findings from a systematic literature review
following the updated Preferred Reporting Items for Systematic Reviews (PRISMA)
guidelines. It aims to provide an overview of the state-of-the-art learning
from demonstration solutions developed for assembly-related tasks and offer a
critical discussion of remaining obstacles in order to drive its progression
towards meaningful deployments. The analysis includes a total of 61 papers over
the period of 2013-2023 sourced from Scopus and Web of Science databases.
Findings indicate that learning from demonstration has attained a significant
level of maturity within the research environment, as evidenced by thorough
experimental achievements, proving its great promise for industrial assembly
applications. However, critical obstacles exist in the area of proven
practicability, task complexity and diversity, generalisation, performance
evaluation and integration concepts that require attention to promote its
widespread adoption and create a seamless transition into industrial practices.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00277" title="Abstract">arXiv:2310.00277</a> [<a href="/pdf/2310.00277" title="Download PDF">pdf</a>, <a href="/format/2310.00277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Framework for Generative Data Augmentation: A Comprehensive  Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zihui Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yunjie Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Generative data augmentation (GDA) has emerged as a promising technique to
alleviate data scarcity in machine learning applications. This thesis presents
a comprehensive survey and unified framework of the GDA landscape. We first
provide an overview of GDA, discussing its motivation, taxonomy, and key
distinctions from synthetic data generation. We then systematically analyze the
critical aspects of GDA - selection of generative models, techniques to utilize
them, data selection methodologies, validation approaches, and diverse
applications. Our proposed unified framework categorizes the extensive GDA
literature, revealing gaps such as the lack of universal benchmarks. The thesis
summarises promising research directions, including , effective data selection,
theoretical development for large-scale models' application in GDA and
establishing a benchmark for GDA. By laying a structured foundation, this
thesis aims to nurture more cohesive development and accelerate progress in the
vital arena of generative data augmentation.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00278" title="Abstract">arXiv:2310.00278</a> [<a href="/pdf/2310.00278" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A bibliometric Analysis on Spectrum Sensing in Wireless Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tamuka%2C+N">Nyashadzashe Tamuka</a>, 
<a href="/search/cs?searchtype=author&query=Sibanda%2C+K">Khulumani Sibanda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Digital Libraries (cs.DL)

</div>
<p class="mathjax">Spectrum scarcity is a prevalent problem in wireless networks due to the
strict allotment of the spectrum (frequency bands) to licensed users by network
regulatory bodies. Such an operation implies that the unlicensed users
(secondary wireless spectrum users) have to evacuate the spectrum when the
primary wireless spectrum users (licensed users) are utilizing the frequency
bands to avoid interference. Cognitive radio alleviates the spectrum shortage
by detecting unoccupied frequency bands. This reduces the underutilization of
frequency bands in wireless networks. There have been numerous related studies
on spectrum sensing, however, few studies have conducted a bibliometric
analysis on this subject. The goal of this study was to conduct a bibliometric
analysis on the optimization of spectrum sensing. The PRISMA methodology was
the basis for the bibliometric analysis to identify the limitations of the
existing spectrum sensing techniques. The findings revealed that various
machine learning or hybrid models outperformed the traditional techniques such
as matched filter and energy detectors at the lowest signal to noise ratio
(SNR). SNR is the ratio of the desired signal magnitude to the background noise
magnitude. This study, therefore, recommends researchers propose alternative
techniques to optimize (improve) spectrum sensing in wireless networks. More
work should be done to develop models that optimize spectrum sensing at low
SNR.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00280" title="Abstract">arXiv:2310.00280</a> [<a href="/pdf/2310.00280" title="Download PDF">pdf</a>, <a href="/format/2310.00280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model  Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qiushi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhangyue Yin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Language Models (LLMs) are evolving at an unprecedented pace and have
exhibited considerable capability in the realm of natural language processing
(NLP) with world knowledge. Benefiting from ultra-large-scale training corpora,
a single LLM can manage typical NLP tasks competently. However, its performance
in executing reasoning tasks is still confined by the limitations of its
internal representations. To push this boundary further, we introduce Corex in
this paper, a suite of novel general-purpose strategies that transform LLMs
into autonomous agents pioneering multi-model collaborations for complex
task-solving. Inspired by human behaviors, Corex is constituted by diverse
collaboration paradigms including Debate, Review, and Retrieve modes, which
collectively work towards enhancing the factuality, faithfulness, and
reliability of the reasoning process. These paradigms foster task-agnostic
approaches that enable LLMs to ''think outside the box,'' thereby overcoming
hallucinations and providing better solutions. Through extensive experiments
across four different types of reasoning tasks, we demonstrate that
orchestrating multiple LLMs to work in concert yields substantially better
performance compared to existing methods. Further results and in-depth analysis
demonstrate the cost-effectiveness of our method, facilitating collaboration
among different LLMs and promoting annotation efficiency.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00283" title="Abstract">arXiv:2310.00283</a> [<a href="/pdf/2310.00283" title="Download PDF">pdf</a>, <a href="/format/2310.00283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Learning Based Fine-Tuning Framework for Speech Emotion  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yusong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Funakoshi%2C+K">Kotaro Funakoshi</a>, 
<a href="/search/cs?searchtype=author&query=Okumura%2C+M">Manabu Okumura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Speech emotion recognition (SER) has drawn increasing attention for its
applications in human-machine interaction. However, existing SER methods ignore
the information gap between the pre-training speech recognition task and the
downstream SER task, leading to sub-optimal performance. Moreover, they require
much time to fine-tune on each specific speech dataset, restricting their
effectiveness in real-world scenes with large-scale noisy data. To address
these issues, we propose an active learning (AL) based Fine-Tuning framework
for SER that leverages task adaptation pre-training (TAPT) and AL methods to
enhance performance and efficiency. Specifically, we first use TAPT to minimize
the information gap between the pre-training and the downstream task. Then, AL
methods are used to iteratively select a subset of the most informative and
diverse samples for fine-tuning, reducing time consumption. Experiments
demonstrate that using only 20\%pt. samples improves 8.45\%pt. accuracy and
reduces 79\%pt. time consumption.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00287" title="Abstract">arXiv:2310.00287</a> [<a href="/pdf/2310.00287" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InFER: A Multi-Ethnic Indian Facial Expression Recognition Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rizvi%2C+S+S+A">Syed Sameen Ahmad Rizvi</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Preyansh Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Challa%2C+J+S">Jagat Sesh Challa</a>, 
<a href="/search/cs?searchtype=author&query=Narang%2C+P">Pratik Narang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the 15th International Conference on Agents and Artificial Intelligence Volume 3: ICAART; ISBN 978-989-758-623-1; ISSN 2184-433X, SciTePress, pages 550-557. DOI: 10.5220/0011699400003393
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Volume 3: ICAART, 2023, pages - 550-557
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The rapid advancement in deep learning over the past decade has transformed
Facial Expression Recognition (FER) systems, as newer methods have been
proposed that outperform the existing traditional handcrafted techniques.
However, such a supervised learning approach requires a sufficiently large
training dataset covering all the possible scenarios. And since most people
exhibit facial expressions based upon their age group, gender, and ethnicity, a
diverse facial expression dataset is needed. This becomes even more crucial
while developing a FER system for the Indian subcontinent, which comprises of a
diverse multi-ethnic population. In this work, we present InFER, a real-world
multi-ethnic Indian Facial Expression Recognition dataset consisting of 10,200
images and 4,200 short videos of seven basic facial expressions. The dataset
has posed expressions of 600 human subjects, and spontaneous/acted expressions
of 6000 images crowd-sourced from the internet. To the best of our knowledge
InFER is the first of its kind consisting of images from 600 subjects from very
diverse ethnicity of the Indian Subcontinent. We also present the experimental
results of baseline &amp; deep FER methods on our dataset to substantiate its
usability in real-world practical applications.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00288" title="Abstract">arXiv:2310.00288</a> [<a href="/pdf/2310.00288" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel in-memory wireless computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+G">Gong-Jie Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zai-Zheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yangdong%2C+X">Xing-Jian Yangdong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yixiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Liang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yingmeng Ge</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yichen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Chen Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Li-Bo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+B">Bin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zaichen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Shi-Jun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+F">Feng Miao</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Nat Electron 6, 381-389 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Emerging Technologies (cs.ET); Systems and Control (eess.SY); Applied Physics (physics.app-ph)

</div>
<p class="mathjax">Parallel wireless digital communication with ultralow power consumption is
critical for emerging edge technologies such as 5G and Internet of Things.
However, the physical separation between digital computing units and analogue
transmission units in traditional wireless technology leads to high power
consumption. Here we report a parallel in-memory wireless computing scheme. The
approach combines in-memory computing with wireless communication using
memristive crossbar arrays. We show that the system can be used for the radio
transmission of a binary stream of 480 bits with a bit error rate of 0. The
in-memory wireless computing uses two orders of magnitude less power than
conventional technology (based on digital-to-analogue and analogue-to-digital
converters). We also show that the approach can be applied to acoustic and
optical wireless communications
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00290" title="Abstract">arXiv:2310.00290</a> [<a href="/pdf/2310.00290" title="Download PDF">pdf</a>, <a href="/ps/2310.00290" title="Download PostScript">ps</a>, <a href="/format/2310.00290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mathematical structure of perfect predictive reservoir computing for  autoregressive type of time series data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoneda%2C+T">Tsuyoshi Yoneda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Analysis of PDEs (math.AP); Dynamical Systems (math.DS); Functional Analysis (math.FA)

</div>
<p class="mathjax">Reservoir Computing (RC) is a type of recursive neural network (RNN), and
there can be no doubt that the RC will be more and more widely used for
building future prediction models for time-series data, with low training cost,
high speed and high computational power. However, research into the
mathematical structure of RC neural networks has only recently begun. Bollt
(2021) clarified the necessity of the autoregressive (AR) model for gaining the
insight into the mathematical structure of RC neural networks, and indicated
that the Wold decomposition theorem is the milestone for understanding of
these. Keeping this celebrated result in mind, in this paper, we clarify hidden
structures of input and recurrent weight matrices in RC neural networks, and
show that such structures attain perfect prediction for the AR type of time
series data.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00294" title="Abstract">arXiv:2310.00294</a> [<a href="/pdf/2310.00294" title="Download PDF">pdf</a>, <a href="/ps/2310.00294" title="Download PostScript">ps</a>, <a href="/format/2310.00294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RIS-aided Near-Field MIMO Communications: Codebook and Beam Training  Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+S">Suyu Lv</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaodong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Nallanathan%2C+A">Arumugam Nallanathan</a>, 
<a href="/search/cs?searchtype=author&query=Swindlehurst%2C+A+L">A. Lee Swindlehurst</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Downlink reconfigurable intelligent surface (RIS)-assisted
multi-input-multi-output (MIMO) systems are considered with far-field,
near-field, and hybrid-far-near-field channels. According to the angular or
distance information contained in the received signals, 1) a distance-based
codebook is designed for near-field MIMO channels, based on which a
hierarchical beam training scheme is proposed to reduce the training overhead;
2) a combined angular-distance codebook is designed for mixed-far-near-field
MIMO channels, based on which a two-stage beam training scheme is proposed to
achieve alignment in the angular and distance domains separately. For
maximizing the achievable rate while reducing the complexity, an alternating
optimization algorithm is proposed to carry out the joint optimization
iteratively. Specifically, the RIS coefficient matrix is optimized through the
beam training process, the optimal combining matrix is obtained from the
closed-form solution for the mean square error (MSE) minimization problem, and
the active beamforming matrix is optimized by exploiting the relationship
between the achievable rate and MSE. Numerical results reveal that: 1) the
proposed beam training schemes achieve near-optimal performance with a
significantly decreased training overhead; 2) compared to the angular-only
far-field channel model, taking the additional distance information into
consideration will effectively improve the achievable rate when carrying out
beam design for near-field communications.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00296" title="Abstract">arXiv:2310.00296</a> [<a href="/pdf/2310.00296" title="Download PDF">pdf</a>, <a href="/format/2310.00296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QUIZ: An Arbitrary Volumetric Point Matching Method for Medical Image  Registration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xinxin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haoyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chulong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+W">Weibin Kong</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Jingjing Dai</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuming Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yaoqin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaokun Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Rigid pre-registration involving local-global matching or other large
deformation scenarios is crucial. Current popular methods rely on unsupervised
learning based on grayscale similarity, but under circumstances where different
poses lead to varying tissue structures, or where image quality is poor, these
methods tend to exhibit instability and inaccuracies. In this study, we propose
a novel method for medical image registration based on arbitrary voxel point of
interest matching, called query point quizzer (QUIZ). QUIZ focuses on the
correspondence between local-global matching points, specifically employing CNN
for feature extraction and utilizing the Transformer architecture for global
point matching queries, followed by applying average displacement for local
image rigid transformation. We have validated this approach on a large
deformation dataset of cervical cancer patients, with results indicating
substantially smaller deviations compared to state-of-the-art methods.
Remarkably, even for cross-modality subjects, it achieves results surpassing
the current state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00297" title="Abstract">arXiv:2310.00297</a> [<a href="/pdf/2310.00297" title="Download PDF">pdf</a>, <a href="/format/2310.00297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding In-Context Learning from Repetitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jianhao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chiyu Song</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chenming Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yafu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper explores the elusive mechanism underpinning in-context learning in
Large Language Models (LLMs). Our work provides a novel perspective by
examining in-context learning via the lens of surface repetitions. We
quantitatively investigate the role of surface features in text generation, and
empirically establish the existence of \emph{token co-occurrence
reinforcement}, a principle that strengthens the relationship between two
tokens based on their contextual co-occurrences. By investigating the dual
impacts of these features, our research illuminates the internal workings of
in-context learning and expounds on the reasons for its failures. This paper
provides an essential contribution to the understanding of in-context learning
and its potential limitations, providing a fresh perspective on this exciting
capability.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00298" title="Abstract">arXiv:2310.00298</a> [<a href="/pdf/2310.00298" title="Download PDF">pdf</a>, <a href="/format/2310.00298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compilation Semantics for a Programming Language with Versions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tanabe%2C+Y">Yudai Tanabe</a>, 
<a href="/search/cs?searchtype=author&query=Lubis%2C+L+A">Luthfan Anshar Lubis</a>, 
<a href="/search/cs?searchtype=author&query=Aotani%2C+T">Tomoyuki Aotani</a>, 
<a href="/search/cs?searchtype=author&query=Masuhara%2C+H">Hidehiko Masuhara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is the full version of the paper accepted by APLAS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Programming with versions is a paradigm that allows a program to use multiple
versions of a module so that the programmer can selectively use functions from
both older and newer versions of a single module. Previous work formalized
$\lambda_{\mathrm{VL}}$, a core calculus for programming with versions, but it
has not been integrated into practical programming languages. In this paper, we
propose VL, a Haskell-subset surface language for $\lambda_{\mathrm{VL}}$ along
with its compilation method. We formally describe the core part of the VL
compiler, which translates from the surface language to the core language by
leveraging Girard's translation, soundly infers the consistent version of
expressions along with their types, and generates a multi-version interface by
bundling specific-version interfaces. We conduct a case study to show how VL
supports practical software evolution scenarios and discuss the method's
scalability.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00299" title="Abstract">arXiv:2310.00299</a> [<a href="/pdf/2310.00299" title="Download PDF">pdf</a>, <a href="/format/2310.00299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RelBERT: Embedding Relations with Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ushio%2C+A">Asahi Ushio</a>, 
<a href="/search/cs?searchtype=author&query=Camacho-Collados%2C+J">Jose Camacho-Collados</a>, 
<a href="/search/cs?searchtype=author&query=Schockaert%2C+S">Steven Schockaert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Many applications need access to background knowledge about how different
concepts and entities are related. Although Knowledge Graphs (KG) and Large
Language Models (LLM) can address this need to some extent, KGs are inevitably
incomplete and their relational schema is often too coarse-grained, while LLMs
are inefficient and difficult to control. As an alternative, we propose to
extract relation embeddings from relatively small language models. In
particular, we show that masked language models such as RoBERTa can be
straightforwardly fine-tuned for this purpose, using only a small amount of
training data. The resulting model, which we call RelBERT, captures relational
similarity in a surprisingly fine-grained way, allowing us to set a new
state-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of
modelling relations that go well beyond what the model has seen during
training. For instance, we obtained strong results on relations between named
entities with a model that was only trained on lexical relations between
concepts, and we observed that RelBERT can recognise morphological analogies
despite not being trained on such examples. Overall, we find that RelBERT
significantly outperforms strategies based on prompting language models that
are several orders of magnitude larger, including recent GPT-based models and
open source models.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00301" title="Abstract">arXiv:2310.00301</a> [<a href="/pdf/2310.00301" title="Download PDF">pdf</a>, <a href="/format/2310.00301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hierarchical Approach to Environment Design with Generative Trajectory  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dexun Li</a>, 
<a href="/search/cs?searchtype=author&query=Varakantham%2C+P">Pradeep Varakantham</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Unsupervised Environment Design (UED) is a paradigm for training generally
capable agents to achieve good zero-shot transfer performance. This paradigm
hinges on automatically generating a curriculum of training environments.
Leading approaches for UED predominantly use randomly generated environment
instances to train the agent. While these methods exhibit good zero-shot
transfer performance, they often encounter challenges in effectively exploring
large design spaces or leveraging previously discovered underlying structures,
To address these challenges, we introduce a novel framework based on
Hierarchical MDP (Markov Decision Processes). Our approach includes an
upper-level teacher's MDP responsible for training a lower-level MDP student
agent, guided by the student's performance. To expedite the learning of the
upper leavel MDP, we leverage recent advancements in generative modeling to
generate synthetic experience dataset for training the teacher agent. Our
algorithm, called Synthetically-enhanced Hierarchical Environment Design
(SHED), significantly reduces the resource-intensive interactions between the
agent and the environment. To validate the effectiveness of SHED, we conduct
empirical experiments across various domains, with the goal of developing an
efficient and robust agent under limited training resources. Our results show
the manifold advantages of SHED and highlight its effectiveness as a potent
instrument for curriculum-based learning within the UED framework. This work
contributes to exploring the next generation of RL agents capable of adeptly
handling an ever-expanding range of complex tasks.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00305" title="Abstract">arXiv:2310.00305</a> [<a href="/pdf/2310.00305" title="Download PDF">pdf</a>, <a href="/format/2310.00305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards LLM-based Fact Verification on News Claims with a Hierarchical  Step-by-Step Prompting Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wei Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While large pre-trained language models (LLMs) have shown their impressive
capabilities in various NLP tasks, they are still under-explored in the
misinformation domain. In this paper, we examine LLMs with in-context learning
(ICL) for news claim verification, and find that only with 4-shot demonstration
examples, the performance of several prompting methods can be comparable with
previous supervised models. To further boost performance, we introduce a
Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to
separate a claim into several subclaims and then verify each of them via
multiple questions-answering steps progressively. Experiment results on two
public misinformation datasets show that HiSS prompting outperforms
state-of-the-art fully-supervised approach and strong few-shot ICL-enabled
baselines.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00307" title="Abstract">arXiv:2310.00307</a> [<a href="/pdf/2310.00307" title="Download PDF">pdf</a>, <a href="/format/2310.00307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual-Augmented Transformer Network for Weakly Supervised Semantic  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jingliang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zonghan Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Weakly supervised semantic segmentation (WSSS), a fundamental computer vision
task, which aims to segment out the object within only class-level labels. The
traditional methods adopt the CNN-based network and utilize the class
activation map (CAM) strategy to discover the object regions. However, such
methods only focus on the most discriminative region of the object, resulting
in incomplete segmentation. An alternative is to explore vision transformers
(ViT) to encode the image to acquire the global semantic information. Yet, the
lack of transductive bias to objects is a flaw of ViT. In this paper, we
explore the dual-augmented transformer network with self-regularization
constraints for WSSS. Specifically, we propose a dual network with both
CNN-based and transformer networks for mutually complementary learning, where
both networks augment the final output for enhancement. Massive systemic
evaluations on the challenging PASCAL VOC 2012 benchmark demonstrate the
effectiveness of our method, outperforming previous state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00309" title="Abstract">arXiv:2310.00309</a> [<a href="/pdf/2310.00309" title="Download PDF">pdf</a>, <a href="/format/2310.00309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Adaptation of the AAA-Interpolation Algorithm for Model Reduction of  MIMO Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jonas%2C+J">Jared Jonas</a>, 
<a href="/search/eess?searchtype=author&query=Bamieh%2C+B">Bassam Bamieh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">We consider the Adaptive Antoulas-Anderson (AAA) rational interpolation
algorithm recently developed by Trefethen and co-authors, which can be viewed
as a type of moment-matching technique for system realization and
approximation. We consider variations on this algorithm that are suitable for
model reduction of linear time invariant systems while addressing some of the
shortcomings of the block-AAA variant of the algorithm for MIMO systems. In
particular, we develop state-space formulas and keep track of the state-space
dimension at every step of the adaptive block-AAA algorithm, showing an
unfavorable increase of the state dimension. We propose a new low-rank adaptive
interpolation algorithm that addresses this shortcoming. Comparative
computational results are included for the algorithms above, together with
comparisons to balanced reduction.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00310" title="Abstract">arXiv:2310.00310</a> [<a href="/pdf/2310.00310" title="Download PDF">pdf</a>, <a href="/format/2310.00310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An easy zero-shot learning combination: Texture Sensitive Semantic  Segmentation IceHrNet and Advanced Style Transfer Learning Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhiyong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuelong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xiaoqin Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zong%2C+J">Jun Zong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiuheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+R">Ran Tao</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+X">Xiaofei Cong</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yufeng Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 11 figures, submitted to Journal of Hydrology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We proposed an easy method of Zero-Shot semantic segmentation by using style
transfer. In this case, we successfully used a medical imaging dataset (Blood
Cell Imagery) to train a model for river ice semantic segmentation. First, we
built a river ice semantic segmentation dataset IPC_RI_SEG using a fixed camera
and covering the entire ice melting process of the river. Second, a
high-resolution texture fusion semantic segmentation network named IceHrNet is
proposed. The network used HRNet as the backbone and added ASPP and Decoder
segmentation heads to retain low-level texture features for fine semantic
segmentation. Finally, a simple and effective advanced style transfer learning
strategy was proposed, which can perform zero-shot transfer learning based on
cross-domain semantic segmentation datasets, achieving a practical effect of
87% mIoU for semantic segmentation of river ice without target training dataset
(25% mIoU for None Stylized, 65% mIoU for Conventional Stylized, our strategy
improved by 22%). Experiments showed that the IceHrNet outperformed the
state-of-the-art methods on the texture-focused dataset IPC_RI_SEG, and
achieved an excellent result on the shape-focused river ice datasets. In
zero-shot transfer learning, IceHrNet achieved an increase of 2 percentage
points compared to other methods. Our code and model are published on
https://github.com/PL23K/IceHrNet.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00311" title="Abstract">arXiv:2310.00311</a> [<a href="/pdf/2310.00311" title="Download PDF">pdf</a>, <a href="/format/2310.00311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Planning with Latent Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenhao Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Temporal abstraction and efficient planning pose significant challenges in
offline reinforcement learning, mainly when dealing with domains that involve
temporally extended tasks and delayed sparse rewards. Existing methods
typically plan in the raw action space and can be inefficient and inflexible.
Latent action spaces offer a more flexible paradigm, capturing only possible
actions within the behavior policy support and decoupling the temporal
structure between planning and modeling. However, current latent-action-based
methods are limited to discrete spaces and require expensive planning. This
paper presents a unified framework for continuous latent action space
representation learning and planning by leveraging latent, score-based
diffusion models. We establish the theoretical equivalence between planning in
the latent action space and energy-guided sampling with a pretrained diffusion
model and incorporate a novel sequence-level exact sampling method. Our
proposed method, $\texttt{LatentDiffuser}$, demonstrates competitive
performance on low-dimensional locomotion control tasks and surpasses existing
methods in higher-dimensional tasks.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00313" title="Abstract">arXiv:2310.00313</a> [<a href="/pdf/2310.00313" title="Download PDF">pdf</a>, <a href="/format/2310.00313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Learning in Large Language Models: A Neuroscience-inspired  Analysis of Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yousefi%2C+S">Safoora Yousefi</a>, 
<a href="/search/cs?searchtype=author&query=Betthauser%2C+L">Leo Betthauser</a>, 
<a href="/search/cs?searchtype=author&query=Hasanbeig%2C+H">Hosein Hasanbeig</a>, 
<a href="/search/cs?searchtype=author&query=Saran%2C+A">Akanksha Saran</a>, 
<a href="/search/cs?searchtype=author&query=Milli%C3%A8re%2C+R">Rapha&#xeb;l Milli&#xe8;re</a>, 
<a href="/search/cs?searchtype=author&query=Momennejad%2C+I">Ida Momennejad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) exhibit remarkable performance improvement
through in-context learning (ICL) by leveraging task-specific examples in the
input. However, the mechanisms behind this improvement remain elusive. In this
work, we investigate how LLM embeddings and attention representations change
following in-context-learning, and how these changes mediate improvement in
behavior. We employ neuroscience-inspired techniques such as representational
similarity analysis (RSA) and propose novel methods for parameterized probing
and measuring ratio of attention to relevant vs. irrelevant information in
Llama-2 70B and Vicuna 13B. We designed three tasks with a priori relationships
among their conditions: reading comprehension, linear regression, and
adversarial prompt injection. We formed hypotheses about expected similarities
in task representations to investigate latent changes in embeddings and
attention. Our analyses revealed a meaningful correlation between changes in
both embeddings and attention representations with improvements in behavioral
performance after ICL. This empirical framework empowers a nuanced
understanding of how latent representations affect LLM behavior with and
without ICL, offering valuable tools and insights for future research and
practical applications.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00318" title="Abstract">arXiv:2310.00318</a> [<a href="/pdf/2310.00318" title="Download PDF">pdf</a>, <a href="/format/2310.00318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding Realistic Images from Brain Activity with Contrastive  Self-supervision and Latent Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingyuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingxiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Moens%2C+M">Marie-Francine Moens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages,5 figures,
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Reconstructing visual stimuli from human brain activities provides a
promising opportunity to advance our understanding of the brain's visual system
and its connection with computer vision models. Although deep generative models
have been employed for this task, the challenge of generating high-quality
images with accurate semantics persists due to the intricate underlying
representations of brain signals and the limited availability of parallel data.
In this paper, we propose a two-phase framework named Contrast and Diffuse
(CnD) to decode realistic images from functional magnetic resonance imaging
(fMRI) recordings. In the first phase, we acquire representations of fMRI data
through self-supervised contrastive learning. In the second phase, the encoded
fMRI representations condition the diffusion model to reconstruct visual
stimulus through our proposed concept-aware conditioning method. Experimental
results show that CnD reconstructs highly plausible images on challenging
benchmarks. We also provide a quantitative interpretation of the connection
between the latent diffusion model (LDM) components and the human brain's
visual system. In summary, we present an effective approach for reconstructing
visual stimuli based on human brain activity and offer a novel framework to
understand the relationship between the diffusion model and the human brain
visual system.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00322" title="Abstract">arXiv:2310.00322</a> [<a href="/pdf/2310.00322" title="Download PDF">pdf</a>, <a href="/format/2310.00322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chengdong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziran Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Minquan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ci%2C+H">Hai Ci</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xuehai Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Deployable Large Language Models (LLMs) must conform to the criterion of
helpfulness and harmlessness, thereby achieving consistency between LLMs
outputs and human values. Red-teaming techniques constitute a critical way
towards this criterion. Existing work rely solely on manual red team designs
and heuristic adversarial prompts for vulnerability detection and optimization.
These approaches lack rigorous mathematical formulation, thus limiting the
exploration of diverse attack strategy within quantifiable measure and
optimization of LLMs under convergence guarantees. In this paper, we present
Red-teaming Game (RTG), a general game-theoretic framework without manual
annotation. RTG is designed for analyzing the multi-turn attack and defense
interactions between Red-team language Models (RLMs) and Blue-team Language
Model (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) with
diversity measure of the semantic space. GRTS is an automated red teaming
technique to solve RTG towards Nash equilibrium through meta-game analysis,
which corresponds to the theoretically guaranteed optimization direction of
both RLMs and BLM. Empirical results in multi-turn attacks with RLMs show that
GRTS autonomously discovered diverse attack strategies and effectively improved
security of LLMs, outperforming existing heuristic red-team designs. Overall,
RTG has established a foundational framework for red teaming tasks and
constructed a new scalable oversight technique for alignment.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00328" title="Abstract">arXiv:2310.00328</a> [<a href="/pdf/2310.00328" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deployment Corrections: An incident response framework for frontier AI  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=O%27Brien%2C+J">Joe O&#x27;Brien</a>, 
<a href="/search/cs?searchtype=author&query=Ee%2C+S">Shaun Ee</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+Z">Zoe Williams</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 53 pages; 1 figure; 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">A comprehensive approach to addressing catastrophic risks from AI models
should cover the full model lifecycle. This paper explores contingency plans
for cases where pre-deployment risk management falls short: where either very
dangerous models are deployed, or deployed models become very dangerous.
<br />Informed by incident response practices from industries including
cybersecurity, we describe a toolkit of deployment corrections that AI
developers can use to respond to dangerous capabilities, behaviors, or use
cases of AI models that develop or are detected after deployment. We also
provide a framework for AI developers to prepare and implement this toolkit.
<br />We conclude by recommending that frontier AI developers should (1) maintain
control over model access, (2) establish or grow dedicated teams to design and
maintain processes for deployment corrections, including incident response
plans, and (3) establish these deployment corrections as allowable actions with
downstream users. We also recommend frontier AI developers, standard-setting
organizations, and regulators should collaborate to define a standardized
industry-wide approach to the use of deployment corrections in incident
response.
<br />Caveat: This work applies to frontier AI models that are made available
through interfaces (e.g., API) that provide the AI developer or another
upstream party means of maintaining control over access (e.g., GPT-4 or
Claude). It does not apply to management of catastrophic risk from open-source
models (e.g., BLOOM or Llama-2), for which the restrictions we discuss are
largely unenforceable.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00330" title="Abstract">arXiv:2310.00330</a> [<a href="/pdf/2310.00330" title="Download PDF">pdf</a>, <a href="/ps/2310.00330" title="Download PostScript">ps</a>, <a href="/format/2310.00330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A DSP shared is a DSP earned: HLS Task-Level Multi-Pumping for  High-Performance Low-Resource Designs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brignone%2C+G">Giovanni Brignone</a>, 
<a href="/search/cs?searchtype=author&query=Lazarescu%2C+M+T">Mihai T. Lazarescu</a>, 
<a href="/search/cs?searchtype=author&query=Lavagno%2C+L">Luciano Lavagno</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">High-level synthesis (HLS) enhances digital hardware design productivity
through a high abstraction level. Even if the HLS abstraction prevents
fine-grained manual register-transfer level (RTL) optimizations, it also
enables automatable optimizations that would be unfeasible or hard to automate
at RTL. Specifically, we propose a task-level multi-pumping methodology to
reduce resource utilization, particularly digital signal processors (DSPs),
while preserving the throughput of HLS kernels modeled as dataflow graphs
(DFGs) targeting field-programmable gate arrays. The methodology exploits the
HLS resource sharing to automatically insert the logic for reusing the same
functional unit for different operations. In addition, it relies on multi-clock
DFG s to run the multi-pumped tasks at higher frequencies. The methodology
scales the pipeline initiation interval (II) and the clock frequency
constraints of resource-intensive tasks by a multi-pumping factor (M). The
looser II allows sharing the same resource among M different operations, while
the tighter clock frequency preserves the throughput. We verified that our
methodology opens a new Pareto front in the throughput and resource space by
applying it to open-source HLS designs using state-of-the-art commercial HLS
and implementation tools by Xilinx. The multi-pumped designs require up to 40%
fewer DSP resources at the same throughput as the original designs optimized
for performance (i.e., running at the maximum clock frequency) and achieve up
to 50% better throughput using the same DSP s as the original designs optimized
for resources with a single clock.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00332" title="Abstract">arXiv:2310.00332</a> [<a href="/pdf/2310.00332" title="Download PDF">pdf</a>, <a href="/format/2310.00332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MFL Data Preprocessing and CNN-based Oil Pipeline Defects Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Katser%2C+I">Iurii Katser</a>, 
<a href="/search/cs?searchtype=author&query=Kozitsin%2C+V">Vyacheslav Kozitsin</a>, 
<a href="/search/cs?searchtype=author&query=Mozolin%2C+I">Igor Mozolin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures, 5 tables, 14 references. arXiv admin note: text overlap with <a href="/abs/2009.10163">arXiv:2009.10163</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Systems and Control (eess.SY)

</div>
<p class="mathjax">Recently, the application of computer vision for anomaly detection has been
under attention in several industrial fields. An important example is oil
pipeline defect detection. Failure of one oil pipeline can interrupt the
operation of the entire transportation system or cause a far-reaching failure.
The automated defect detection could significantly decrease the inspection time
and the related costs. However, there is a gap in the related literature when
it comes to dealing with this task. The existing studies do not sufficiently
cover the research of the Magnetic Flux Leakage data and the preprocessing
techniques that allow overcoming the limitations set by the available data.
This work focuses on alleviating these issues. Moreover, in doing so, we
exploited the recent convolutional neural network structures and proposed
robust approaches, aiming to acquire high performance considering the related
metrics. The proposed approaches and their applicability were verified using
real-world data.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00333" title="Abstract">arXiv:2310.00333</a> [<a href="/html/2310.00333" title="Download HTML">html</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proceedings of the Third Workshop on Agents and Robots for reliable  Engineered Autonomy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferrando%2C+A">Angelo Ferrando</a> (University of Genoa, Italy), 
<a href="/search/cs?searchtype=author&query=Cardoso%2C+R">Rafael Cardoso</a> (University of Aberdeen, United Kingdom)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 391, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">The volume comprises the proceedings of the Third Workshop on Agents and
Robots for reliable Engineered Autonomy (AREA 2023), held alongside the 26th
European Conference on Artificial Intelligence (ECAI 2023). It explores the
convergence of autonomous agents and robotics, emphasizing the practical
application of agents in real-world scenarios with physical interactions. The
workshop highlights the growing importance of enhanced autonomy and reliable
behavior in robotic systems and the need for novel verification and validation
methods. Its primary objective is to promote collaboration between researchers
in these fields, aiming to address complex challenges in autonomous robotic
systems. The volume includes 7 full papers and 5 short papers.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00334" title="Abstract">arXiv:2310.00334</a> [<a href="/pdf/2310.00334" title="Download PDF">pdf</a>, <a href="/ps/2310.00334" title="Download PostScript">ps</a>, <a href="/format/2310.00334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bounded Simultaneous Messages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bogdanov%2C+A">Andrej Bogdanov</a>, 
<a href="/search/cs?searchtype=author&query=Dinesh%2C+K">Krishnamoorthy Dinesh</a>, 
<a href="/search/cs?searchtype=author&query=Filmus%2C+Y">Yuval Filmus</a>, 
<a href="/search/cs?searchtype=author&query=Ishai%2C+Y">Yuval Ishai</a>, 
<a href="/search/cs?searchtype=author&query=Kaplan%2C+A">Avi Kaplan</a>, 
<a href="/search/cs?searchtype=author&query=Sekar%2C+S">Sruthi Sekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 1 table, accepted to FSTTCS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">We consider the following question of bounded simultaneous messages (BSM)
protocols: Can computationally unbounded Alice and Bob evaluate a function
$f(x,y)$ of their inputs by sending polynomial-size messages to a
computationally bounded Carol? The special case where $f$ is the mod-2
inner-product function and Carol is bounded to AC$^0$ has been studied in
previous works. The general question can be broadly motivated by applications
in which distributed computation is more costly than local computation,
including secure two-party computation.
<br />In this work, we initiate a more systematic study of the BSM model, with
different functions $f$ and computational bounds on Carol. In particular, we
give evidence against the existence of BSM protocols with polynomial-size Carol
for naturally distributed variants of NP-complete languages.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00335" title="Abstract">arXiv:2310.00335</a> [<a href="/pdf/2310.00335" title="Download PDF">pdf</a>, <a href="/format/2310.00335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anomaly Detection in Power Generation Plants with Generative Adversarial  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Atemkeng%2C+M">Marcellin Atemkeng</a>, 
<a href="/search/cs?searchtype=author&query=Jimoh%2C+T+A">Toheeb Aduramomi Jimoh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Anomaly detection is a critical task that involves the identification of data
points that deviate from a predefined pattern, useful for fraud detection and
related activities. Various techniques are employed for anomaly detection, but
recent research indicates that deep learning methods, with their ability to
discern intricate data patterns, are well-suited for this task. This study
explores the use of Generative Adversarial Networks (GANs) for anomaly
detection in power generation plants. The dataset used in this investigation
comprises fuel consumption records obtained from power generation plants
operated by a telecommunications company. The data was initially collected in
response to observed irregularities in the fuel consumption patterns of the
generating sets situated at the company's base stations. The dataset was
divided into anomalous and normal data points based on specific variables, with
64.88% classified as normal and 35.12% as anomalous. An analysis of feature
importance, employing the random forest classifier, revealed that Running Time
Per Day exhibited the highest relative importance. A GANs model was trained and
fine-tuned both with and without data augmentation, with the goal of increasing
the dataset size to enhance performance. The generator model consisted of five
dense layers using the tanh activation function, while the discriminator
comprised six dense layers, each integrated with a dropout layer to prevent
overfitting. Following data augmentation, the model achieved an accuracy rate
of 98.99%, compared to 66.45% before augmentation. This demonstrates that the
model nearly perfectly classified data points into normal and anomalous
categories, with the augmented data significantly enhancing the GANs'
performance in anomaly detection. Consequently, this study recommends the use
of GANs, particularly when using large datasets, for effective anomaly
detection.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00336" title="Abstract">arXiv:2310.00336</a> [<a href="/pdf/2310.00336" title="Download PDF">pdf</a>, <a href="/format/2310.00336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DURENDAL: Graph deep learning framework for temporal heterogeneous  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dileo%2C+M">Manuel Dileo</a>, 
<a href="/search/cs?searchtype=author&query=Zignani%2C+M">Matteo Zignani</a>, 
<a href="/search/cs?searchtype=author&query=Gaito%2C+S">Sabrina Gaito</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Temporal heterogeneous networks (THNs) are evolving networks that
characterize many real-world applications such as citation and events networks,
recommender systems, and knowledge graphs. Although different Graph Neural
Networks (GNNs) have been successfully applied to dynamic graphs, most of them
only support homogeneous graphs or suffer from model design heavily influenced
by specific THNs prediction tasks. Furthermore, there is a lack of temporal
heterogeneous networked data in current standard graph benchmark datasets.
Hence, in this work, we propose DURENDAL, a graph deep learning framework for
THNs. DURENDAL can help to easily repurpose any heterogeneous graph learning
model to evolving networks by combining design principles from snapshot-based
and multirelational message-passing graph learning models. We introduce two
different schemes to update embedding representations for THNs, discussing the
strengths and weaknesses of both strategies. We also extend the set of
benchmarks for TNHs by introducing two novel high-resolution temporal
heterogeneous graph datasets derived from an emerging Web3 platform and a
well-established e-commerce website. Overall, we conducted the experimental
evaluation of the framework over four temporal heterogeneous network datasets
on future link prediction tasks in an evaluation setting that takes into
account the evolving nature of the data. Experiments show the prediction power
of DURENDAL compared to current solutions for evolving and dynamic graphs, and
the effectiveness of its model design.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00337" title="Abstract">arXiv:2310.00337</a> [<a href="/pdf/2310.00337" title="Download PDF">pdf</a>, <a href="/format/2310.00337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantization of Deep Neural Networks to facilitate self-correction of  weights on Phase Change Memory-based analog hardware
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ivanov%2C+A">Arseni Ivanov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">In recent years, hardware-accelerated neural networks have gained significant
attention for edge computing applications. Among various hardware options,
crossbar arrays, offer a promising avenue for efficient storage and
manipulation of neural network weights. However, the transition from trained
floating-point models to hardware-constrained analog architectures remains a
challenge. In this work, we combine a quantization technique specifically
designed for such architectures with a novel self-correcting mechanism. By
utilizing dual crossbar connections to represent both the positive and negative
parts of a single weight, we develop an algorithm to approximate a set of
multiplicative weights. These weights, along with their differences, aim to
represent the original network's weights with minimal loss in performance. We
implement the models using IBM's aihwkit and evaluate their efficacy over time.
Our results demonstrate that, when paired with an on-chip pulse generator, our
self-correcting neural network performs comparably to those trained with
analog-aware algorithms.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00338" title="Abstract">arXiv:2310.00338</a> [<a href="/pdf/2310.00338" title="Download PDF">pdf</a>, <a href="/ps/2310.00338" title="Download PostScript">ps</a>, <a href="/format/2310.00338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Complete Metamorphic Testing Pipeline
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duque-Torres%2C+A">Alejandra Duque-Torres</a>, 
<a href="/search/cs?searchtype=author&query=Pfahl%2C+D">Dietmar Pfahl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Metamorphic Testing (MT) addresses the test oracle problem by examining the
relationships between input-output pairs in consecutive executions of the
System Under Test (SUT). These relations, known as Metamorphic Relations (MRs),
specify the expected output changes resulting from specific input changes.
However, achieving full automation in generating, selecting, and understanding
MR violations poses challenges. Our research aims to develop methods and tools
that assist testers in generating MRs, defining constraints, and providing
explainability for MR outcomes. In the MR generation phase, we explore
automated techniques that utilise a domain-specific language to generate and
describe MRs. The MR constraint definition focuses on capturing the nuances of
MR applicability by defining constraints. These constraints help identify the
specific conditions under which MRs are expected to hold. The evaluation and
validation involve conducting empirical studies to assess the effectiveness of
the developed methods and validate their applicability in real-world regression
testing scenarios. Through this research, we aim to advance the automation of
MR generation, enhance the understanding of MR violations, and facilitate their
effective application in regression testing.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00339" title="Abstract">arXiv:2310.00339</a> [<a href="/pdf/2310.00339" title="Download PDF">pdf</a>, <a href="/format/2310.00339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedLPA: Personalized One-shot Federated Learning with Layer-Wise  Posterior Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liangxi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+F">Feiyang Ye</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yunheng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xia Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Linshan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jialin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Efficiently aggregating trained neural networks from local clients into a
global model on a server is a widely researched topic in federated learning.
Recently, motivated by diminishing privacy concerns, mitigating potential
attacks, and reducing the overhead of communication, one-shot federated
learning (i.e., limiting client-server communication into a single round) has
gained popularity among researchers. However, the one-shot aggregation
performances are sensitively affected by the non-identical training data
distribution, which exhibits high statistical heterogeneity in some real-world
scenarios. To address this issue, we propose a novel one-shot aggregation
method with Layer-wise Posterior Aggregation, named FedLPA. FedLPA aggregates
local models to obtain a more accurate global model without requiring extra
auxiliary datasets or exposing any confidential local information, e.g., label
distributions. To effectively capture the statistics maintained in the biased
local datasets in the practical non-IID scenario, we efficiently infer the
posteriors of each layer in each local model using layer-wise Laplace
approximation and aggregate them to train the global parameters. Extensive
experimental results demonstrate that FedLPA significantly improves learning
performance over state-of-the-art methods across several metrics.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00340" title="Abstract">arXiv:2310.00340</a> [<a href="/pdf/2310.00340" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regulating Dark Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brenncke%2C+M">Martin Brenncke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> forthcoming in Notre Dame Journal of International &amp; Comparative Law
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Dark patterns have become increasingly pervasive in online choice
architectures, encompassing practices like subscription traps, hiding
information about fees, pre-selecting options by default, nagging, and drip
pricing. Regulators around the world have started to express concerns that such
practices are causing substantial consumer detriment. This Article focuses on
the legal response to dark patterns in the European Union. It provides the
first comprehensive mapping of European Union regulations expressly addressing
dark patterns. The Article argues that these regulations protect biased
consumers and adopt autonomy as a normative lens to assess dark patterns.
Consequently, regulating dark patterns in European Union law means regulating
for autonomy. This normative lens is under-researched.
<br />This Article addresses this gap in research with two principle contributions.
First, it works out a specific conception of autonomous decision-making, rooted
in the paradigm that providing consumers with information enables consumers to
make an informed decision. Second, the Article offers a novel normative
classification for dark patterns in online choice architectures. It develops a
taxonomy encompassing six categories of autonomy violations, specifically
tailored for the assessment and regulation of dark patterns that exploit
consumer behavioral biases. These categories serve multiple purposes. They
uncover and make explicit the autonomy violations addressed by existing
European Union regulations. They delineate the contentious line between
acceptable influences on consumer decision-making and autonomy violations that
may warrant regulation in online choice architectures. They also provide
policymakers in the EU and elsewhere with a framework when deliberating the
regulation of other instances of dark patterns.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00341" title="Abstract">arXiv:2310.00341</a> [<a href="/pdf/2310.00341" title="Download PDF">pdf</a>, <a href="/format/2310.00341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mathematical Model of Dating Apps&#x27; Influence on Sexually Transmitted  Diseases Spread
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fenster%2C+A">Ariel Fenster</a>, 
<a href="/search/cs?searchtype=author&query=Lazebnik%2C+T">Teddy Lazebnik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Sexually transmitted diseases (STDs) are a group of pathogens infecting new
hosts through sexual interactions. Due to its social and economic burden,
multiple models have been proposed to study the spreading of pathogens. In
parallel, in the ever-evolving landscape of digital social interactions, the
pervasive utilization of dating apps has become a prominent facet of modern
society. Despite the surge in popularity and the profound impact on
relationship formation, a crucial gap in the literature persists regarding the
potential ramifications of dating apps usage on the dynamics of STDs. In this
paper, we address this gap by presenting a novel mathematical framework - an
extended Susceptible-Infected-Susceptible (SIS) epidemiological model to
elucidate the intricate interplay between dating apps engagement and the
propagation of STDs. Namely, as dating apps are designed to make users revisit
them and have mainly casual sexual interactions with other users, they increase
the number of causal partners, which increases the overall spread of STDS.
Using extensive simulation, based on real-world data, explore the effect of
dating apps adoption and control on the STD spread. We show that an increased
adoption of dating apps can result in an STD outbreak if not handled
appropriately.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00342" title="Abstract">arXiv:2310.00342</a> [<a href="/pdf/2310.00342" title="Download PDF">pdf</a>, <a href="/format/2310.00342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RBF Weighted Hyper-Involution for RGB-D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+A">Mehfuz A Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Peethambaran%2C+J">Jiju Peethambaran</a>, 
<a href="/search/cs?searchtype=author&query=London%2C+N">Neil London</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">A vast majority of conventional augmented reality devices are equipped with
depth sensors. Depth images produced by such sensors contain complementary
information for object detection when used with color images. Despite the
benefits, it remains a complex task to simultaneously extract photometric and
depth features in real time due to the immanent difference between depth and
color images. Moreover, standard convolution operations are not sufficient to
properly extract information directly from raw depth images leading to
intermediate representations of depth which is inefficient. To address these
issues, we propose a real-time and two stream RGBD object detection model. The
proposed model consists of two new components: a depth guided hyper-involution
that adapts dynamically based on the spatial interaction pattern in the raw
depth map and an up-sampling based trainable fusion layer that combines the
extracted depth and color image features without blocking the information
transfer between them. We show that the proposed model outperforms other RGB-D
based object detection models on NYU Depth v2 dataset and achieves comparable
(second best) results on SUN RGB-D. Additionally, we introduce a new outdoor
RGB-D object detection dataset where our proposed model outperforms other
models. The performance evaluation on diverse synthetic data generated from CAD
models and images shows the potential of the proposed model to be adapted to
augmented reality based applications.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00344" title="Abstract">arXiv:2310.00344</a> [<a href="/pdf/2310.00344" title="Download PDF">pdf</a>, <a href="/format/2310.00344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harmony World Models: Boosting Sample Efficiency for Model-based  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Haoyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jialong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+N">Ningya Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianmin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+M">Mingsheng Long</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Model-based reinforcement learning (MBRL) holds the promise of
sample-efficient learning by utilizing a world model, which models how the
environment works and typically encompasses components for two tasks:
observation modeling and reward modeling. In this paper, through a dedicated
empirical investigation, we gain a deeper understanding of the role each task
plays in world models and uncover the overlooked potential of more efficient
MBRL by harmonizing the interference between observation and reward modeling.
Our key insight is that while prevalent approaches of explicit MBRL attempt to
restore abundant details of the environment through observation models, it is
difficult due to the environment's complexity and limited model capacity. On
the other hand, reward models, while dominating in implicit MBRL and adept at
learning task-centric dynamics, are inadequate for sample-efficient learning
without richer learning signals. Capitalizing on these insights and
discoveries, we propose a simple yet effective method, Harmony World Models
(HarmonyWM), that introduces a lightweight harmonizer to maintain a dynamic
equilibrium between the two tasks in world model learning. Our experiments on
three visual control domains show that the base MBRL method equipped with
HarmonyWM gains 10%-55% absolute performance boosts.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00346" title="Abstract">arXiv:2310.00346</a> [<a href="/pdf/2310.00346" title="Download PDF">pdf</a>, <a href="/format/2310.00346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fostering new Vertical and Horizontal IoT Applications with Intelligence  Everywhere
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+H">Hung Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wachowicz%2C+M">Monica Wachowicz</a>, 
<a href="/search/cs?searchtype=author&query=Richard%2C+R">Rene Richard</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Ching-Hsien Hsu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Keywords: IoT collective intelligence, machine learning, edge intelligence, cloud computing, learning models, vertical IoT, IoT network, horizontal IoT applications, Society 5.0
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Computers and Society (cs.CY); Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Intelligence Everywhere is predicated on the seamless integration of IoT
networks transporting a vast amount of data streams through many computing
resources across an edge-to-cloud continuum, relying on the orchestration of
distributed machine learning models. The result is an interconnected and
collective intelligent ecosystem where devices, systems, services, and users
work together to support IoT applications. This paper discusses the
state-of-the-art research and the principles of the Intelligence Everywhere
framework for enhancing IoT applications in vertical sectors such as Digital
Health, Infrastructure, and Transportation/Mobility in the context of
intelligent society (Society 5.0). It also introduces a novel perspective for
the development of horizontal IoT applications, capable of running across
various IoT networks while fostering collective intelligence across diverse
sectors. Finally, this paper provides comprehensive insights into the
challenges and opportunities for harnessing collective knowledge from real-time
insights, leading to optimised processes and better overall collaboration
across different IoT sectors.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00347" title="Abstract">arXiv:2310.00347</a> [<a href="/pdf/2310.00347" title="Download PDF">pdf</a>, <a href="/format/2310.00347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking Bias Detection: Leveraging Transformer-Based Models for  Content Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raza%2C+S">Shaina Raza</a>, 
<a href="/search/cs?searchtype=author&query=Bamgbose%2C+O">Oluwanifemi Bamgbose</a>, 
<a href="/search/cs?searchtype=author&query=Chatrath%2C+V">Veronica Chatrath</a>, 
<a href="/search/cs?searchtype=author&query=Sidyakin%2C+Y">Yan Sidyakin</a>, 
<a href="/search/cs?searchtype=author&query=Ghuge%2C+S">Shardul Ghuge</a>, 
<a href="/search/cs?searchtype=author&query=Muaad%2C+A+Y">Abdullah Y Muaad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> UNDER REVIEW
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Detecting bias in text is crucial due to its potential implications in
perpetuating harmful stereotypes, spreading misinformation, and influencing
decision-making. Existing language models often struggle to generalize beyond
their training data. To address this challenge, we propose the Contextualized
Bi-Directional Dual Transformer (CBDT) Classifier that leverages two
interconnected transformer networks, the Context Transformer and the Entity
Transformer, to detect bias in text. Experimental results on diverse datasets
demonstrate the superiority of the CBDT classifier in accurately classifying
biased and non-biased sentences, as well as identifying specific biased words
and phrases. We get a performance gain of about 2-4% over the baselines. Future
research can extend the model to different languages and cultural contexts
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00348" title="Abstract">arXiv:2310.00348</a> [<a href="/pdf/2310.00348" title="Download PDF">pdf</a>, <a href="/format/2310.00348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Age of Information in Slotted ALOHA With Energy Harvesting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ngo%2C+K">Khac-Hoang Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Durisi%2C+G">Giuseppe Durisi</a>, 
<a href="/search/cs?searchtype=author&query=Amat%2C+A+G+i">Alexandre Graell i Amat</a>, 
<a href="/search/cs?searchtype=author&query=Munari%2C+A">Andrea Munari</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%A1zaro%2C+F">Francisco L&#xe1;zaro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE Globecom 2023; simulation code: <a href="https://github.com/khachoang1412/AoI_slottedALOHA_energyHarvesting">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We examine the age of information (AoI) of a status update system that
incorporates energy harvesting and uses the slotted ALOHA protocol. We derive
analytically the average AoI and the probability that the AoI exceeds a given
threshold. Via numerical results, we investigate two strategies to minimize the
age of information (AoI): transmitting a new update whenever possible to
exploit every chance to reduce the AoI, and transmitting only when sufficient
energy is available to increase the chance of successful delivery. The two
strategies are beneficial for low and high update generation rates,
respectively. However, an optimized approach that balances the two strategies
outperforms them significantly in terms of both AoI and throughput.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00349" title="Abstract">arXiv:2310.00349</a> [<a href="/pdf/2310.00349" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Political Communication in a Polarized Society: A Longitudinal  Study of Brazilian Presidential Elections on Instagram
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de-Lima-Santos%2C+M">Mathias-Felipe de-Lima-Santos</a>, 
<a href="/search/cs?searchtype=author&query=Gon%C3%A7alves%2C+I">Isabella Gon&#xe7;alves</a>, 
<a href="/search/cs?searchtype=author&query=Quiles%2C+M+G">Marcos G. Quiles</a>, 
<a href="/search/cs?searchtype=author&query=Mesquita%2C+L">Lucia Mesquita</a>, 
<a href="/search/cs?searchtype=author&query=Ceron%2C+W">Wilson Ceron</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">In today's digital age, images have emerged as powerful tools for politicians
to engage with their voters on social media platforms. Visual content possesses
a unique emotional appeal that often leads to increased user engagement.
However, research on visual communication remains relatively limited,
particularly in the Global South. This study aims to bridge this gap by
employing a combination of computational methods and qualitative approach to
investigate the visual communication strategies employed in a dataset of 11,263
Instagram posts by 19 Brazilian presidential candidates in 2018 and 2022
national elections. Through two studies, we observed consistent patterns across
these candidates on their use of visual political communication. Notably, we
identify a prevalence of celebratory and positively toned images. They also
exhibit a strong sense of personalization, portraying candidates connected with
their voters on a more emotional level. Our research also uncovers unique
contextual nuances specific to the Brazilian political landscape. We note a
substantial presence of screenshots from news websites and other social media
platforms. Furthermore, text-edited images with portrayals emerge as a
prominent feature. In light of these results, we engage in a discussion
regarding the implications for the broader field of visual political
communication. This article serves as a testament to the pivotal role that
Instagram has played in shaping the narrative of two fiercely polarized
Brazilian elections, casting a revealing light on the ever-evolving dynamics of
visual political communication in the digital age. Finally, we propose avenues
for future research in the realm of visual political communication.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00351" title="Abstract">arXiv:2310.00351</a> [<a href="/pdf/2310.00351" title="Download PDF">pdf</a>, <a href="/format/2310.00351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neuroadaptation in Physical Human-Robot Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Avinash Singh</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dikai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chin-Teng Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Robots for physical Human-Robot Collaboration (pHRC) systems need to change
their behavior and how they operate in consideration of several factors, such
as the performance and intention of a human co-worker and the capabilities of
different human-co-workers in collision avoidance and singularity of the robot
operation. As the system's admittance becomes variable throughout the
workspace, a potential solution is to tune the interaction forces and control
the parameters based on the operator's requirements. To overcome this issue, we
have demonstrated a novel closed-loop-neuroadaptive framework for pHRC. We have
applied cognitive conflict information in a closed-loop manner, with the help
of reinforcement learning, to adapt to robot strategy and compare this with
open-loop settings. The experiment results show that the closed-loop-based
neuroadaptive framework successfully reduces the level of cognitive conflict
during pHRC, consequently increasing the smoothness and intuitiveness of
human-robot collaboration. These results suggest the feasibility of a
neuroadaptive approach for future pHRC control systems through
electroencephalogram (EEG) signals.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00353" title="Abstract">arXiv:2310.00353</a> [<a href="/pdf/2310.00353" title="Download PDF">pdf</a>, <a href="/format/2310.00353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy stable schemes for the shear shallow water model Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yadav%2C+A">Anshu Yadav</a>, 
<a href="/search/math?searchtype=author&query=Bhoriya%2C+D">Deepak Bhoriya</a>, 
<a href="/search/math?searchtype=author&query=Kumar%2C+H">Harish Kumar</a>, 
<a href="/search/math?searchtype=author&query=Chandrashekar%2C+P">Praveen Chandrashekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Journal of Scientific Computing (JOMP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">The shear shallow water model is an extension of the classical shallow water
model to include the effects of vertical shear. It is a system of six
non-linear hyperbolic PDE with non-conservative products. We develop a
high-order entropy stable finite difference scheme for this model in one
dimension and extend it to two dimensions on rectangular grids. The key idea is
to rewrite the system so that non-conservative terms do not contribute to the
entropy evolution. Then, we first develop an entropy conservative scheme for
the conservative part, which is then extended to the complete system using the
fact that the non-conservative terms do not contribute to the entropy
production. The entropy dissipative scheme, which leads to an entropy
inequality, is then obtained by carefully adding dissipative flux terms. The
proposed schemes are then tested on several one and two-dimensional problems to
demonstrate their stability and accuracy.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00354" title="Abstract">arXiv:2310.00354</a> [<a href="/pdf/2310.00354" title="Download PDF">pdf</a>, <a href="/format/2310.00354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-Dentify: Deep learning for proximal caries detection on bitewing  x-ray -- HUNT4 Oral Health Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Frutos%2C+J+P">Javier P&#xe9;rez de Frutos</a>, 
<a href="/search/cs?searchtype=author&query=Helland%2C+R+H">Ragnhild Holden Helland</a>, 
<a href="/search/cs?searchtype=author&query=Desai%2C+S">Shreya Desai</a>, 
<a href="/search/cs?searchtype=author&query=Nymoen%2C+L+C">Line Cathrine Nymoen</a>, 
<a href="/search/cs?searchtype=author&query=Lang%C3%B8%2C+T">Thomas Lang&#xf8;</a>, 
<a href="/search/cs?searchtype=author&query=Remman%2C+T">Theodor Remman</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+A">Abhijit Sen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 4 figure, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Background: Dental caries diagnosis requires the manual inspection of
diagnostic bitewing images of the patient, followed by a visual inspection and
probing of the identified dental pieces with potential lesions. Yet the use of
artificial intelligence, and in particular deep-learning, has the potential to
aid in the diagnosis by providing a quick and informative analysis of the
bitewing images.
<br />Methods: A dataset of 13,887 bitewings from the HUNT4 Oral Health Study were
annotated individually by six different experts, and used to train three
different object detection deep-learning architectures: RetinaNet (ResNet50),
YOLOv5 (M size), and EfficientDet (D0 and D1 sizes). A consensus dataset of 197
images, annotated jointly by the same six dentist, was used for evaluation. A
five-fold cross validation scheme was used to evaluate the performance of the
AI models.
<br />Results: the trained models show an increase in average precision and
F1-score, and decrease of false negative rate, with respect to the dental
clinicians. Out of the three architectures studied, YOLOv5 shows the largest
improvement, reporting 0.647 mean average precision, 0.548 mean F1-score, and
0.149 mean false negative rate. Whereas the best annotators on each of these
metrics reported 0.299, 0.495, and 0.164 respectively.
<br />Conclusion: Deep-learning models have shown the potential to assist dental
professionals in the diagnosis of caries. Yet, the task remains challenging due
to the artifacts natural to the bitewings.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00355" title="Abstract">arXiv:2310.00355</a> [<a href="/pdf/2310.00355" title="Download PDF">pdf</a>, <a href="/format/2310.00355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gaze-Driven Sentence Simplification for Language Learners: Enhancing  Comprehension and Readability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Higasa%2C+T">Taichi Higasa</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+K">Keitaro Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Q">Qi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Morishima%2C+S">Shigeo Morishima</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM ICMI 2023 workshops (Multimodal, Interactive Interfaces for Education)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Language learners should regularly engage in reading challenging materials as
part of their study routine. Nevertheless, constantly referring to dictionaries
is time-consuming and distracting. This paper presents a novel gaze-driven
sentence simplification system designed to enhance reading comprehension while
maintaining their focus on the content. Our system incorporates machine
learning models tailored to individual learners, combining eye gaze features
and linguistic features to assess sentence comprehension. When the system
identifies comprehension difficulties, it provides simplified versions by
replacing complex vocabulary and grammar with simpler alternatives via GPT-3.5.
We conducted an experiment with 19 English learners, collecting data on their
eye movements while reading English text. The results demonstrated that our
system is capable of accurately estimating sentence-level comprehension.
Additionally, we found that GPT-3.5 simplification improved readability in
terms of traditional readability metrics and individual word difficulty,
paraphrasing across different linguistic levels.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00357" title="Abstract">arXiv:2310.00357</a> [<a href="/pdf/2310.00357" title="Download PDF">pdf</a>, <a href="/format/2310.00357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structural Adversarial Objectives for\\Self-Supervised Representation  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Maire%2C+M">Michael Maire</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Within the framework of generative adversarial networks (GANs), we propose
objectives that task the discriminator for self-supervised representation
learning via additional structural modeling responsibilities. In combination
with an efficient smoothness regularizer imposed on the network, these
objectives guide the discriminator to learn to extract informative
representations, while maintaining a generator capable of sampling from the
domain. Specifically, our objectives encourage the discriminator to structure
features at two levels of granularity: aligning distribution characteristics,
such as mean and variance, at coarse scales, and grouping features into local
clusters at finer scales. Operating as a feature learner within the GAN
framework frees our self-supervised system from the reliance on hand-crafted
data augmentation schemes that are prevalent across contrastive representation
learning methods. Across CIFAR-10/100 and an ImageNet subset, experiments
demonstrate that equipping GANs with our self-supervised objectives suffices to
produce discriminators which, evaluated in terms of representation learning,
compete with networks trained by contrastive learning approaches.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00359" title="Abstract">arXiv:2310.00359</a> [<a href="/pdf/2310.00359" title="Download PDF">pdf</a>, <a href="/format/2310.00359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Cross-dataset Deepfake Detection with Deep Information  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shanmin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Ying Fu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+S">Siwei Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deepfake technology poses a significant threat to security and social trust.
Although existing detection methods have demonstrated high performance in
identifying forgeries within datasets using the same techniques for training
and testing, they suffer from sharp performance degradation when faced with
cross-dataset scenarios where unseen deepfake techniques are tested. To address
this challenge, we propose a deep information decomposition (DID) framework in
this paper. Unlike most existing deepfake detection methods, our framework
prioritizes high-level semantic features over visual artifacts. Specifically,
it decomposes facial features into deepfake-related and irrelevant information
and optimizes the deepfake information for real/fake discrimination to be
independent of other factors. Our approach improves the robustness of deepfake
detection against various irrelevant information changes and enhances the
generalization ability of the framework to detect unseen forgery methods.
Extensive experimental comparisons with existing state-of-the-art detection
methods validate the effectiveness and superiority of the DID framework on
cross-dataset deepfake detection.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00362" title="Abstract">arXiv:2310.00362</a> [<a href="/pdf/2310.00362" title="Download PDF">pdf</a>, <a href="/format/2310.00362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Posterior Illumination for Ambiguity-aware Inverse Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+L">Linjie Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Tewari%2C+A">Ayush Tewari</a>, 
<a href="/search/cs?searchtype=author&query=Habermann%2C+M">Marc Habermann</a>, 
<a href="/search/cs?searchtype=author&query=Saito%2C+S">Shunsuke Saito</a>, 
<a href="/search/cs?searchtype=author&query=Zollh%C3%B6fer%2C+M">Michael Zollh&#xf6;fer</a>, 
<a href="/search/cs?searchtype=author&query=Leimk%C3%BChler%2C+T">Thomas Leimk&#xfc;hler</a>, 
<a href="/search/cs?searchtype=author&query=Theobalt%2C+C">Christian Theobalt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGGRAPH Asia 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Inverse rendering, the process of inferring scene properties from images, is
a challenging inverse problem. The task is ill-posed, as many different scene
configurations can give rise to the same image. Most existing solutions
incorporate priors into the inverse-rendering pipeline to encourage plausible
solutions, but they do not consider the inherent ambiguities and the
multi-modal distribution of possible decompositions. In this work, we propose a
novel scheme that integrates a denoising diffusion probabilistic model
pre-trained on natural illumination maps into an optimization framework
involving a differentiable path tracer. The proposed method allows sampling
from combinations of illumination and spatially-varying surface materials that
are, both, natural and explain the image observations. We further conduct an
extensive comparative study of different priors on illumination used in
previous work on inverse rendering. Our method excels in recovering materials
and producing highly realistic and diverse environment map samples that
faithfully explain the illumination of the input images.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00363" title="Abstract">arXiv:2310.00363</a> [<a href="/pdf/2310.00363" title="Download PDF">pdf</a>, <a href="/format/2310.00363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Composition of Control Barrier Functions With Differing Relative Degrees  for Safety Under Input Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rabiee%2C+P">Pedram Rabiee</a>, 
<a href="/search/eess?searchtype=author&query=Hoagg%2C+J+B">Jesse B. Hoagg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to 2024 American Control Conference (ACC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper presents a new approach for guaranteed safety subject to input
constraints (e.g., actuator limits) using a composition of multiple control
barrier functions (CBFs). First, we present a method for constructing a single
CBF from multiple CBFs, which can have different relative degrees. This
construction relies on a soft minimum function and yields a CBF whose
$0$-superlevel set is a subset of the union of the $0$-superlevel sets of all
the CBFs used in the construction. Next, we extend the approach to systems with
input constraints. Specifically, we introduce control dynamics that allow us to
express the input constraints as CBFs in the closed-loop state (i.e., the state
of the system and the controller). The CBFs constructed from input constraints
do not have the same relative degree as the safety constraints. Thus, the
composite soft-minimum CBF construction is used to combine the input-constraint
CBFs with the safety-constraint CBFs. Finally, we present a feasible
real-time-optimization control that guarantees that the state remains in the
$0$-superlevel set of the composite soft-minimum CBF. We demonstrate these
approaches on a nonholonomic ground robot example.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00367" title="Abstract">arXiv:2310.00367</a> [<a href="/pdf/2310.00367" title="Download PDF">pdf</a>, <a href="/format/2310.00367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with  TikZ
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belouadi%2C+J">Jonas Belouadi</a>, 
<a href="/search/cs?searchtype=author&query=Lauscher%2C+A">Anne Lauscher</a>, 
<a href="/search/cs?searchtype=author&query=Eger%2C+S">Steffen Eger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Generating bitmap graphics from text has gained considerable attention, yet
for scientific figures, vector graphics are often preferred. Given that vector
graphics are typically encoded using low-level graphics primitives, generating
them directly is difficult. To address this, we propose the use of TikZ, a
well-known abstract graphics language that can be compiled to vector graphics,
as an intermediate representation of scientific figures. TikZ offers
human-oriented, high-level commands, thereby facilitating conditional language
modeling with any large language model. To this end, we introduce DaTikZ the
first large-scale TikZ dataset, consisting of 120k TikZ drawings aligned with
captions. We fine-tune LLaMA on DaTikZ, as well as our new model CLiMA, which
augments LLaMA with multimodal CLIP embeddings. In both human and automatic
evaluation, CLiMA and LLaMA outperform commercial GPT-4 and Claude 2 in terms
of similarity to human-created figures, with CLiMA additionally improving
text-image alignment. Our detailed analysis shows that all models generalize
well and are not susceptible to memorization. GPT-4 and Claude 2, however, tend
to generate more simplistic figures compared to both humans and our models. We
make our framework, AutomaTikZ, along with model weights and datasets, publicly
available.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00369" title="Abstract">arXiv:2310.00369</a> [<a href="/pdf/2310.00369" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distilling Inductive Bias: Knowledge Distillation Beyond Model  Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Habib%2C+G">Gousia Habib</a>, 
<a href="/search/cs?searchtype=author&query=Saleem%2C+T+J">Tausifa Jan Saleem</a>, 
<a href="/search/cs?searchtype=author&query=Lall%2C+B">Brejesh Lall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With the rapid development of computer vision, Vision Transformers (ViTs)
offer the tantalizing prospect of unified information processing across visual
and textual domains. But due to the lack of inherent inductive biases in ViTs,
they require enormous amount of data for training. To make their applications
practical, we introduce an innovative ensemble-based distillation approach
distilling inductive bias from complementary lightweight teacher models. Prior
systems relied solely on convolution-based teaching. However, this method
incorporates an ensemble of light teachers with different architectural
tendencies, such as convolution and involution, to instruct the student
transformer jointly. Because of these unique inductive biases, instructors can
accumulate a wide range of knowledge, even from readily identifiable stored
datasets, which leads to enhanced student performance. Our proposed framework
also involves precomputing and storing logits in advance, essentially the
unnormalized predictions of the model. This optimization can accelerate the
distillation process by eliminating the need for repeated forward passes during
knowledge distillation, significantly reducing the computational burden and
enhancing efficiency.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00371" title="Abstract">arXiv:2310.00371</a> [<a href="/pdf/2310.00371" title="Download PDF">pdf</a>, <a href="/format/2310.00371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConSOR: A Context-Aware Semantic Object Rearrangement Framework for  Partially Arranged Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramachandruni%2C+K">Kartik Ramachandruni</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+M">Max Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Chernova%2C+S">Sonia Chernova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IROS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Object rearrangement is the problem of enabling a robot to identify the
correct object placement in a complex environment. Prior work on object
rearrangement has explored a diverse set of techniques for following user
instructions to achieve some desired goal state. Logical predicates, images of
the goal scene, and natural language descriptions have all been used to
instruct a robot in how to arrange objects. In this work, we argue that
burdening the user with specifying goal scenes is not necessary in
partially-arranged environments, such as common household settings. Instead, we
show that contextual cues from partially arranged scenes (i.e., the placement
of some number of pre-arranged objects in the environment) provide sufficient
context to enable robots to perform object rearrangement \textit{without any
explicit user goal specification}. We introduce ConSOR, a Context-aware
Semantic Object Rearrangement framework that utilizes contextual cues from a
partially arranged initial state of the environment to complete the arrangement
of new objects, without explicit goal specification from the user. We
demonstrate that ConSOR strongly outperforms two baselines in generalizing to
novel object arrangements and unseen object categories. The code and data can
be found at https://github.com/kartikvrama/consor.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00372" title="Abstract">arXiv:2310.00372</a> [<a href="/pdf/2310.00372" title="Download PDF">pdf</a>, <a href="/format/2310.00372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Active Learning with Noisy Oracle in Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schubert%2C+M">Marius Schubert</a>, 
<a href="/search/cs?searchtype=author&query=Riedlinger%2C+T">Tobias Riedlinger</a>, 
<a href="/search/cs?searchtype=author&query=Kahl%2C+K">Karsten Kahl</a>, 
<a href="/search/cs?searchtype=author&query=Rottmann%2C+M">Matthias Rottmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Obtaining annotations for complex computer vision tasks such as object
detection is an expensive and time-intense endeavor involving a large number of
human workers or expert opinions. Reducing the amount of annotations required
while maintaining algorithm performance is, therefore, desirable for machine
learning practitioners and has been successfully achieved by active learning
algorithms. However, it is not merely the amount of annotations which
influences model performance but also the annotation quality. In practice, the
oracles that are queried for new annotations frequently contain significant
amounts of noise. Therefore, cleansing procedures are oftentimes necessary to
review and correct given labels. This process is subject to the same budget as
the initial annotation itself since it requires human workers or even domain
experts. Here, we propose a composite active learning framework including a
label review module for deep object detection. We show that utilizing part of
the annotation budget to correct the noisy annotations partially in the active
dataset leads to early improvements in model performance, especially when
coupled with uncertainty-based query strategies. The precision of the label
error proposals has a significant influence on the measured effect of the label
review. In our experiments we achieve improvements of up to 4.5 mAP points of
object detection performance by incorporating label reviews at equal annotation
budget.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00374" title="Abstract">arXiv:2310.00374</a> [<a href="/pdf/2310.00374" title="Download PDF">pdf</a>, <a href="/format/2310.00374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coordinated pausing: An evaluation-based coordination scheme for  frontier AI developers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alaga%2C+J">Jide Alaga</a>, 
<a href="/search/cs?searchtype=author&query=Schuett%2C+J">Jonas Schuett</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 3 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">As artificial intelligence (AI) models are scaled up, new capabilities can
emerge unintentionally and unpredictably, some of which might be dangerous. In
response, dangerous capabilities evaluations have emerged as a new risk
assessment tool. But what should frontier AI developers do if sufficiently
dangerous capabilities are in fact discovered? This paper focuses on one
possible response: coordinated pausing. It proposes an evaluation-based
coordination scheme that consists of five main steps: (1) Frontier AI models
are evaluated for dangerous capabilities. (2) Whenever, and each time, a model
fails a set of evaluations, the developer pauses certain research and
development activities. (3) Other developers are notified whenever a model with
dangerous capabilities has been discovered. They also pause related research
and development activities. (4) The discovered capabilities are analyzed and
adequate safety precautions are put in place. (5) Developers only resume their
paused activities if certain safety thresholds are reached. The paper also
discusses four concrete versions of that scheme. In the first version, pausing
is completely voluntary and relies on public pressure on developers. In the
second version, participating developers collectively agree to pause under
certain conditions. In the third version, a single auditor evaluates models of
multiple developers who agree to pause if any model fails a set of evaluations.
In the fourth version, developers are legally required to run evaluations and
pause if dangerous capabilities are discovered. Finally, the paper discusses
the desirability and feasibility of our proposed coordination scheme. It
concludes that coordinated pausing is a promising mechanism for tackling
emerging risks from frontier AI models. However, a number of practical and
legal obstacles need to be overcome, especially how to avoid violations of
antitrust law.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00377" title="Abstract">arXiv:2310.00377</a> [<a href="/pdf/2310.00377" title="Download PDF">pdf</a>, <a href="/format/2310.00377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating the Effect of Incidental Correlations on Part-based Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhatt%2C+G">Gaurav Bhatt</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+D">Deepayan Das</a>, 
<a href="/search/cs?searchtype=author&query=Sigal%2C+L">Leonid Sigal</a>, 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+V+N">Vineeth N Balasubramanian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in 37th Conference on Neural Information Processing Systems (NeurIPS'2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Intelligent systems possess a crucial characteristic of breaking complicated
problems into smaller reusable components or parts and adjusting to new tasks
using these part representations. However, current part-learners encounter
difficulties in dealing with incidental correlations resulting from the limited
observations of objects that may appear only in specific arrangements or with
specific backgrounds. These incidental correlations may have a detrimental
impact on the generalization and interpretability of learned part
representations. This study asserts that part-based representations could be
more interpretable and generalize better with limited data, employing two
innovative regularization methods. The first regularization separates
foreground and background information's generative process via a unique
mixture-of-parts formulation. Structural constraints are imposed on the parts
using a weakly-supervised loss, guaranteeing that the mixture-of-parts for
foreground and background entails soft, object-agnostic masks. The second
regularization assumes the form of a distillation loss, ensuring the invariance
of the learned parts to the incidental background correlations. Furthermore, we
incorporate sparse and orthogonal constraints to facilitate learning
high-quality part representations. By reducing the impact of incidental
background correlations on the learned parts, we exhibit state-of-the-art
(SoTA) performance on few-shot learning tasks on benchmark datasets, including
MiniImagenet, TieredImageNet, and FC100. We also demonstrate that the
part-based representations acquired through our approach generalize better than
existing techniques, even under domain shifts of the background and common data
corruption on the ImageNet-9 dataset. The implementation is available on
GitHub: https://github.com/GauravBh1010tt/DPViT.git
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00378" title="Abstract">arXiv:2310.00378</a> [<a href="/pdf/2310.00378" title="Download PDF">pdf</a>, <a href="/format/2310.00378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Value Understanding in Language Models through  Discriminator-Critique Gap
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaowei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+F">Fengshuo Bai</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Recent advancements in Large Language Models (LLMs) have heightened concerns
about their potential misalignment with human values. However, evaluating their
grasp of these values is complex due to their intricate and adaptable nature.
We argue that truly understanding values in LLMs requires considering both
"know what" and "know why". To this end, we present the Value Understanding
Measurement (VUM) framework that quantitatively assess both "know what" and
"know why" by measuring the discriminator-critique gap related to human values.
Using the Schwartz Value Survey, we specify our evaluation values and develop a
thousand-level dialogue dataset with GPT-4. Our assessment looks at both the
value alignment of LLM's outputs compared to baseline answers and how LLM
responses align with reasons for value recognition versus GPT-4's annotations.
We evaluate five representative LLMs and provide strong evidence that the
scaling law significantly impacts "know what" but not much on "know why", which
has consistently maintained a high level. This may further suggest that LLMs
might craft plausible explanations based on the provided context without truly
understanding their inherent value, indicating potential risks.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00381" title="Abstract">arXiv:2310.00381</a> [<a href="/pdf/2310.00381" title="Download PDF">pdf</a>, <a href="/format/2310.00381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quadratic constraint consistency in the projection-free approximation of  harmonic maps and bending isometries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Akrivis%2C+G">Georgios Akrivis</a>, 
<a href="/search/math?searchtype=author&query=Bartels%2C+S">S&#xf6;ren Bartels</a>, 
<a href="/search/math?searchtype=author&query=Palus%2C+C">Christian Palus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We devise a projection-free iterative scheme for the approximation of
harmonic maps that provides a second-order accuracy of the constraint violation
and is unconditionally energy stable. A corresponding error estimate is valid
under a mild but necessary discrete regularity condition. The method is based
on the application of a BDF2 scheme and the considered problem serves as a
model for partial differential equations with holonomic constraint. The
performance of the method is illustrated via the computation of stationary
harmonic maps and bending isometries.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00382" title="Abstract">arXiv:2310.00382</a> [<a href="/pdf/2310.00382" title="Download PDF">pdf</a>, <a href="/format/2310.00382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A geographic information system-based modelling, analysing and  visualising of low voltage networks: The potential of demand time-shifting in  the power quality improvement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Antic%2C+T">Tomislav Antic</a>, 
<a href="/search/eess?searchtype=author&query=Capuder%2C+T">Tomislav Capuder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The challenges of power quality are an emerging topic for the past couple of
years due to massive changes occurring in low voltage distribution networks,
being even more emphasized in the years marked by the novel COVID-19 disease
affecting people's behaviour and energy crisis increasing the awareness and
need of end-users energy independence. Both of these phenomena additionally
stress the need for changes in distribution networks as the traditional
consumption patterns of the end-users are significantly different. To overcome
these challenges it is necessary to develop tools and methods that will help
Distribution System Operators (DSOs). In this paper, we present a geographic
information system (GIS)-based tool that, by using open source technologies,
identifies and removes errors both in the GIS data, representing a distribution
network, and in the consumption data collected from the smart meters. After
processing the initial data, a mathematical model of the network is created,
and the impact of different electricity consumption scenarios on power quality
(PQ) indicators voltage magnitude, voltage unbalance factor, and total voltage
harmonic distortion are calculated. The results of simulations are visualised
using a GIS tool, and based on the results, time periods that are most affected
by the change of end-users behaviour are detected. The potential of the
end-users in the PQ improvement is investigated and an algorithm that shifts
consumption to more adequate time periods is implemented. The results show that
the pandemic negatively affect all analysed PQ indicators since the change in
the average value of PQ disturbances increased both during the hard and
post-lockdown period. The time-shifting of consumption shows significant
potential in how the end-users can not only reduce their own energy costs but
create power quality benefits by reducing all relevant indicators.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00384" title="Abstract">arXiv:2310.00384</a> [<a href="/pdf/2310.00384" title="Download PDF">pdf</a>, <a href="/ps/2310.00384" title="Download PostScript">ps</a>, <a href="/format/2310.00384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Power and 3D Trajectory Optimization for UAV-enabled Wireless  Powered Communication Networks with Obstacles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pan%2C+H">Hongyang Pan</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yanheng Liu</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+G">Geng Sun</a>, 
<a href="/search/eess?searchtype=author&query=Fan%2C+J">Junsong Fan</a>, 
<a href="/search/eess?searchtype=author&query=Liang%2C+S">Shuang Liang</a>, 
<a href="/search/eess?searchtype=author&query=Yuen%2C+C">Chau Yuen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Unmanned aerial vehicle (UAV)-enabled wireless powered communication networks
(WPCNs) are promising technologies in 5G/6G wireless communications, while
there are several challenges about UAV power allocation and scheduling to
enhance the energy utilization efficiency, considering the existence of
obstacles. In this work, we consider a UAV-enabled WPCN scenario that a UAV
needs to cover the ground wireless devices (WDs). During the coverage process,
the UAV needs to collect data from the WDs and charge them simultaneously. To
this end, we formulate a joint-UAV power and three-dimensional (3D) trajectory
optimization problem (JUPTTOP) to simultaneously increase the total number of
the covered WDs, increase the time efficiency, and reduce the total flying
distance of UAV so as to improve the energy utilization efficiency in the
network. Due to the difficulties and complexities, we decompose it into two sub
optimization problems, which are the UAV power allocation optimization problem
(UPAOP) and UAV 3D trajectory optimization problem (UTTOP), respectively. Then,
we propose an improved non-dominated sorting genetic algorithm-II with K-means
initialization operator and Variable dimension mechanism (NSGA-II-KV) for
solving the UPAOP. For UTTOP, we first introduce a pretreatment method, and
then use an improved particle swarm optimization with Normal distribution
initialization, Genetic mechanism, Differential mechanism and Pursuit operator
(PSO-NGDP) to deal with this sub optimization problem. Simulation results
verify the effectiveness of the proposed strategies under different scales and
settings of the networks.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00385" title="Abstract">arXiv:2310.00385</a> [<a href="/pdf/2310.00385" title="Download PDF">pdf</a>, <a href="/format/2310.00385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Demonstrations Controller for In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Fei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+T">Taotian Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shujian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xinyu Dai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In-Context Learning (ICL) is a new paradigm for natural language processing
(NLP), where a large language model (LLM) observes a small number of
demonstrations and a test instance as its input, and directly makes predictions
without updating model parameters. Previous studies have revealed that ICL is
sensitive to the selection and the ordering of demonstrations. However, there
are few studies regarding the impact of the demonstration number on the ICL
performance within a limited input length of LLM, because it is commonly
believed that the number of demonstrations is positively correlated with model
performance. In this paper, we found this conclusion does not always hold true.
Through pilot experiments, we discover that increasing the number of
demonstrations does not necessarily lead to improved performance. Building upon
this insight, we propose a Dynamic Demonstrations Controller (D$^2$Controller),
which can improve the ICL performance by adjusting the number of demonstrations
dynamically. The experimental results show that D$^2$Controller yields a 5.4%
relative improvement on eight different sizes of LLMs across ten datasets.
Moreover, we also extend our method to previous ICL models and achieve
competitive results.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00386" title="Abstract">arXiv:2310.00386</a> [<a href="/pdf/2310.00386" title="Download PDF">pdf</a>, <a href="/format/2310.00386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Order-Preserving GFlowNets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yihang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mauch%2C+L">Lukas Mauch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Generative Flow Networks (GFlowNets) have been introduced as a method to
sample a diverse set of candidates with probabilities proportional to a given
reward. However, GFlowNets can only be used with a predefined scalar reward,
which can be either computationally expensive or not directly accessible, in
the case of multi-objective optimization (MOO) tasks for example. Moreover, to
prioritize identifying high-reward candidates, the conventional practice is to
raise the reward to a higher exponent, the optimal choice of which may vary
across different environments. To address these issues, we propose
Order-Preserving GFlowNets (OP-GFNs), which sample with probabilities in
proportion to a learned reward function that is consistent with a provided
(partial) order on the candidates, thus eliminating the need for an explicit
formulation of the reward function. We theoretically prove that the training
process of OP-GFNs gradually sparsifies the learned reward landscape in
single-objective maximization tasks. The sparsification concentrates on
candidates of a higher hierarchy in the ordering, ensuring exploration at the
beginning and exploitation towards the end of the training. We demonstrate
OP-GFN's state-of-the-art performance in single-objective maximization (totally
ordered) and multi-objective Pareto front approximation (partially ordered)
tasks, including synthetic datasets, molecule generation, and neural
architecture search.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00387" title="Abstract">arXiv:2310.00387</a> [<a href="/pdf/2310.00387" title="Download PDF">pdf</a>, <a href="/format/2310.00387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving Distributed Market Mechanism for Active Distribution  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Franke%2C+M">Matthias Franke</a>, 
<a href="/search/eess?searchtype=author&query=Stanojev%2C+O">Ognjen Stanojev</a>, 
<a href="/search/eess?searchtype=author&query=Mitridati%2C+L">Lesia Mitridati</a>, 
<a href="/search/eess?searchtype=author&query=Hug%2C+G">Gabriela Hug</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Amidst the worldwide efforts to decarbonize power networks, Local Electricity
Markets (LEMs) in distribution networks are gaining importance due to the
increased adoption of renewable energy sources and prosumers. Considering that
LEMs involve data exchange among independent entities, privacy and
cybersecurity are some of the main practical challenges in LEM design. This
paper proposes a secure market protocol using innovations from distributed
optimization and Secure MultiParty Computation (SMPC). The considered LEM is
formulated as an uncertainty-aware joint market for energy and reserves with
affine balancing policies. To achieve scalability and enable the use of SMPC,
market clearing is solved using the Consensus ADMM algorithm. Subsequently, the
data exchange among participants via ADMM iterations is protected using the
Shamir secret-sharing scheme to ensure privacy. The market protocol is further
reinforced by a secure and verifiable settlement process that uses SMPC and
ElGamal commitments to verify market quantities and by a secure recovery scheme
for missing network measurements. Finally, the feasibility and performance of
the proposed LEM are evaluated on a 15-bus test network.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00390" title="Abstract">arXiv:2310.00390</a> [<a href="/pdf/2310.00390" title="Download PDF">pdf</a>, <a href="/format/2310.00390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision  Generalists
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gan%2C+Y">Yulu Gan</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sungwoo Park</a>, 
<a href="/search/cs?searchtype=author&query=Schubert%2C+A">Alexander Schubert</a>, 
<a href="/search/cs?searchtype=author&query=Philippakis%2C+A">Anthony Philippakis</a>, 
<a href="/search/cs?searchtype=author&query=Alaa%2C+A+M">Ahmed M. Alaa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advances in generative diffusion models have enabled text-controlled
synthesis of realistic and diverse images with impressive quality. Despite
these remarkable advances, the application of text-to-image generative models
in computer vision for standard visual recognition tasks remains limited. The
current de facto approach for these tasks is to design model architectures and
loss functions that are tailored to the task at hand. In this paper, we develop
a unified language interface for computer vision tasks that abstracts away
task-specific design choices and enables task execution by following natural
language instructions. Our approach involves casting multiple computer vision
tasks as text-to-image generation problems. Here, the text represents an
instruction describing the task, and the resulting image is a visually-encoded
task output. To train our model, we pool commonly-used computer vision datasets
covering a range of tasks, including segmentation, object detection, depth
estimation, and classification. We then use a large language model to
paraphrase prompt templates that convey the specific tasks to be conducted on
each image, and through this process, we create a multi-modal and multi-task
training dataset comprising input and output images along with annotated
instructions. Following the InstructPix2Pix architecture, we apply
instruction-tuning to a text-to-image diffusion model using our constructed
dataset, steering its functionality from a generative model to an
instruction-guided multi-task vision learner. Experiments demonstrate that our
model, dubbed InstructCV, performs competitively compared to other generalist
and task-specific vision models. Moreover, it exhibits compelling
generalization capabilities to unseen data, categories, and user instructions.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00393" title="Abstract">arXiv:2310.00393</a> [<a href="/pdf/2310.00393" title="Download PDF">pdf</a>, <a href="/ps/2310.00393" title="Download PostScript">ps</a>, <a href="/format/2310.00393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New SDP Roundings and Certifiable Approximation for Cubic Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+J">Jun-Ting Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Kothari%2C+P+K">Pravesh K. Kothari</a>, 
<a href="/search/cs?searchtype=author&query=Pesenti%2C+L">Lucas Pesenti</a>, 
<a href="/search/cs?searchtype=author&query=Trevisan%2C+L">Luca Trevisan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">We give new rounding schemes for SDP relaxations for the problems of
maximizing cubic polynomials over the unit sphere and the $n$-dimensional
hypercube. In both cases, the resulting algorithms yield a $O(\sqrt{n/k})$
multiplicative approximation in $2^{O(k)} \text{poly}(n)$ time. In particular,
we obtain a $O(\sqrt{n/\log n})$ approximation in polynomial time. For the unit
sphere, this improves on the rounding algorithms of Bhattiprolu et. al.
[BGG+17] that need quasi-polynomial time to obtain a similar approximation
guarantee. Over the $n$-dimensional hypercube, our results match the guarantee
of a search algorithm of Khot and Naor [KN08] that obtains a similar
approximation ratio via techniques from convex geometry. Unlike their method,
our algorithm obtains an upper bound on the integrality gap of SDP relaxations
for the problem and as a result, also yields a certificate on the optimum value
of the input instance. Our results naturally generalize to homogeneous
polynomials of higher degree and imply improved algorithms for approximating
satisfiable instances of Max-3SAT.
<br />Our main motivation is the stark lack of rounding techniques for SDP
relaxations of higher degree polynomial optimization in sharp contrast to a
rich theory of SDP roundings for the quadratic case. Our rounding algorithms
introduce two new ideas: 1) a new polynomial reweighting based method to round
sum-of-squares relaxations of higher degree polynomial maximization problems,
and 2) a general technique to compress such relaxations down to substantially
smaller SDPs by relying on an explicit construction of certain hitting sets. We
hope that our work will inspire improved rounding algorithms for polynomial
optimization and related problems.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00394" title="Abstract">arXiv:2310.00394</a> [<a href="/pdf/2310.00394" title="Download PDF">pdf</a>, <a href="/format/2310.00394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy Efficient and Connectivity Aware Self-Organizing Distributed IoT  Topology Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seyyedi%2C+A">Azra Seyyedi</a>, 
<a href="/search/cs?searchtype=author&query=Dortaj%2C+S">Sina Dortaj</a>, 
<a href="/search/cs?searchtype=author&query=Bohlouli%2C+M">Mahdi Bohlouli</a>, 
<a href="/search/cs?searchtype=author&query=Oskoee%2C+S+N">SeyedEhsan Nedaaee Oskoee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Internet of Things has pervaded every area of modern life. From a research
and industry standpoint, there has been an increasing demand and desire in
recent years to develop Internet of Things networks with distributed structure.
Wireless communication under emergency circumstances is one of the important
applications that distributed Internet of Things can have. In order for a
network to be functional in this scenario, it must be developed without the aid
of a pre-established or centralized structure and operated in a self-organized
manner to accommodate the communication requirements of the time. Although the
design and development of such networks can be highly advantageous, they
frequently confront difficulties, the most significant of which is attaining
and maintaining effective connectivity to have reliable communications despite
the requirement to optimize energy usage. In this study, we present a model for
self-organizing topology control for ad hoc-based Internet of Things networks
that can address the aforementioned challenges. The model that will be
presented employs the notion of the Hamiltonian function in classical mechanics
and has two key objectives: regulating the network's topology and dynamics to
enhance connectivity to a desirable level while requiring the least amount of
energy possible. The results of the simulation indicate that the proposed model
satisfactorily fulfills the goals of the problem.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00395" title="Abstract">arXiv:2310.00395</a> [<a href="/pdf/2310.00395" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of system capacity and spectral efficiency of fixed-grid  network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%2C+A">Adarsha M</a>, 
<a href="/search/cs?searchtype=author&query=Malathi%2C+S">S. Malathi</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Santosh Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this article, the performance of a fixed grid network is examined for
various modulation formats to estimate the system's capacity and spectral
efficiency. The optical In-phase Quadrature Modulator structure is used to
build a fixed grid network modulation, and the homodyne detection approach is
used for the receiver. Data multiplexing is accomplished using the Polarization
Division Multiplexed technology. 100 Gbps, 150 Gbps, and 200 Gbps data rates
are transmitted under these circumstances utilizing various modulation formats.
Various pre-processing and signal recovery steps are explained by using modern
digital signal processing systems. The achieved spectrum efficiencies for
PM-QPSK, PM-8 QAM, and PM-16 QAM, respectively, were 2, 3, and 4 bits/s/Hz.
Different modulation like PM-QPSK, PM-8-QAM, and PM-16-QAM each has system
capacities of 8-9, 12-13.5, and 16-18 Tbps and it reaches transmission
distances of 3000, 1300, and 700 kilometers with acceptable Bit Error Rate less
than equal to 2*10-3 respectively. Peak optical power for received signal
detection and full width at half maximum is noted for the different modulations
under a fixed grind network.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00396" title="Abstract">arXiv:2310.00396</a> [<a href="/pdf/2310.00396" title="Download PDF">pdf</a>, <a href="/format/2310.00396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Scheduling and Trajectory Optimization of Charging UAV in Wireless  Rechargeable Sensor Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yanheng Liu</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+H">Hongyang Pan</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+G">Geng Sun</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+A">Aimin Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jiahui Li</a>, 
<a href="/search/eess?searchtype=author&query=Liang%2C+S">Shuang Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Wireless rechargeable sensor networks with a charging unmanned aerial vehicle
(CUAV) have the broad application prospects in the power supply of the
rechargeable sensor nodes (SNs). However, how to schedule a CUAV and design the
trajectory to improve the charging efficiency of the entire system is still a
vital problem. In this paper, we formulate a joint-CUAV scheduling and
trajectory optimization problem (JSTOP) to simultaneously minimize the hovering
points of CUAV, the number of the repeatedly covered SNs and the flying
distance of CUAV for charging all SNs. Due to the complexity of JSTOP, it is
decomposed into two optimization subproblems that are CUAV scheduling
optimization problem (CSOP) and CUAV trajectory optimization problem (CTOP).
CSOP is a hybrid optimization problem that consists of the continuous and
discrete solution space, and the solution dimension in CSOP is not fixed since
it should be changed with the number of hovering points of CUAV. Moreover, CTOP
is a completely discrete optimization problem. Thus, we propose a particle
swarm optimization (PSO) with a flexible dimension mechanism, a K-means
operator and a punishment-compensation mechanism (PSOFKP) and a PSO with a
discretization factor, a 2-opt operator and a path crossover reduction
mechanism (PSOD2P) to solve the converted CSOP and CTOP, respectively.
Simulation results evaluate the benefits of PSOFKP and PSOD2P under different
scales and settings of the network, and the stability of the proposed
algorithms is verified.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00399" title="Abstract">arXiv:2310.00399</a> [<a href="/pdf/2310.00399" title="Download PDF">pdf</a>, <a href="/format/2310.00399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empirical Study on Transformer-based Techniques for Software Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yan Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+X">Xinyue Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+L">Lei Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kailong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J+S">Jin Song Dong</a>, 
<a href="/search/cs?searchtype=author&query=Beschastnikh%2C+I">Ivan Beschastnikh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Many Transformer-based pre-trained models for code have been developed and
applied to code-related tasks. In this paper, we review the existing
literature, examine the suitability of model architectures for different tasks,
and look at the generalization ability of models on different datasets, and
their resource consumption.
<br />We examine three very representative pre-trained models for code: CodeBERT,
CodeGPT, and CodeT5, and conduct experiments on the top-4 most targeted
software engineering tasks that we found in our literature survey: Code
Summarization, Bug Fixing, Bug Detection, and Code Search. In our study, we
showcase the capability of decoder-only models (CodeGPT) for specific
generation tasks under state-of-the-art evaluation metrics and contest the
common belief that the encoder-decoder architecture is optimal for
general-purpose coding tasks. Additionally, we found that the most frequently
used models are not necessarily the most suitable for certain applications and
the developers' needs are not adequately addressed by current research. As
well, we found that the benchmark and frequent dataset for Bug Fixing and Code
Summarization both fail to enable models to generalize onto other datasets for
the same task (the frequent dataset refers to the dataset with the highest
frequency used in literature other than the benchmark). We use statistical
testing to support our conclusions from experiments. Finally, CodeBERT is
highly efficient for understanding tasks, whereas CodeT5's efficiency for
generation tasks is in doubt, as the highest resource consumption does not
guarantee a consistent better performance on different metrics. We also discuss
the numerous practical issues in advancing future research on transformer-based
models for code-related tasks.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00400" title="Abstract">arXiv:2310.00400</a> [<a href="/pdf/2310.00400" title="Download PDF">pdf</a>, <a href="/format/2310.00400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MonoGAE: Roadside Monocular 3D Object Detection with Ground-Aware  Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jiaxin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Li Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chuang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiming Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Although the majority of recent autonomous driving systems concentrate on
developing perception methods based on ego-vehicle sensors, there is an
overlooked alternative approach that involves leveraging intelligent roadside
cameras to help extend the ego-vehicle perception ability beyond the visual
range. We discover that most existing monocular 3D object detectors rely on the
ego-vehicle prior assumption that the optical axis of the camera is parallel to
the ground. However, the roadside camera is installed on a pole with a pitched
angle, which makes the existing methods not optimal for roadside scenes. In
this paper, we introduce a novel framework for Roadside Monocular 3D object
detection with ground-aware embeddings, named MonoGAE. Specifically, the ground
plane is a stable and strong prior knowledge due to the fixed installation of
cameras in roadside scenarios. In order to reduce the domain gap between the
ground geometry information and high-dimensional image features, we employ a
supervised training paradigm with a ground plane to predict high-dimensional
ground-aware embeddings. These embeddings are subsequently integrated with
image features through cross-attention mechanisms. Furthermore, to improve the
detector's robustness to the divergences in cameras' installation poses, we
replace the ground plane depth map with a novel pixel-level refined ground
plane equation map. Our approach demonstrates a substantial performance
advantage over all previous monocular 3D object detectors on widely recognized
3D detection benchmarks for roadside cameras. The code and pre-trained models
will be released soon.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00401" title="Abstract">arXiv:2310.00401</a> [<a href="/pdf/2310.00401" title="Download PDF">pdf</a>, <a href="/format/2310.00401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Better Situational Graphs by Inferring High-level Semantic-Relational  Concepts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Millan-Romera%2C+J+A">Jose Andres Millan-Romera</a>, 
<a href="/search/cs?searchtype=author&query=Bavle%2C+H">Hriday Bavle</a>, 
<a href="/search/cs?searchtype=author&query=Shaheer%2C+M">Muhammad Shaheer</a>, 
<a href="/search/cs?searchtype=author&query=Oswald%2C+M+R">Martin R. Oswald</a>, 
<a href="/search/cs?searchtype=author&query=Voos%2C+H">Holger Voos</a>, 
<a href="/search/cs?searchtype=author&query=Sanchez-Lopez%2C+J+L">Jose Luis Sanchez-Lopez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Recent works on SLAM extend their pose graphs with higher-level semantic
concepts exploiting relationships between them, to provide, not only a richer
representation of the situation/environment but also to improve the accuracy of
its estimation. Concretely, our previous work, Situational Graphs (S-Graphs), a
pioneer in jointly leveraging semantic relationships in the factor optimization
process, relies on semantic entities such as wall surfaces and rooms, whose
relationship is mathematically defined. Nevertheless, excerpting these
high-level concepts relying exclusively on the lower-level factor-graph remains
a challenge and it is currently done with ad-hoc algorithms, which limits its
capability to include new semantic-relational concepts. To overcome this
limitation, in this work, we propose a Graph Neural Network (GNN) for learning
high-level semantic-relational concepts that can be inferred from the low-level
factor graph. We have demonstrated that we can infer room entities and their
relationship to the mapped wall surfaces, more accurately and more
computationally efficient than the baseline algorithm. Additionally, to
demonstrate the versatility of our method, we provide a new semantic concept,
i.e. wall, and its relationship with its wall surfaces. Our proposed method has
been integrated into S-Graphs+, and it has been validated in both simulated and
real datasets. A docker container with our software will be made available to
the scientific community.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00402" title="Abstract">arXiv:2310.00402</a> [<a href="/pdf/2310.00402" title="Download PDF">pdf</a>, <a href="/format/2310.00402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiskANN++: Efficient Page-based Search over Isomorphic Mapped Graph  Index using Query-sensitivity Entry Vertex
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jiongkang Ni</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaoliang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Can Li</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiajie Yao</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shihai Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuecang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages including references, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Databases (cs.DB)

</div>
<p class="mathjax">Given a vector dataset $\mathcal{X}$ and a query vector $\vec{x}_q$,
graph-based Approximate Nearest Neighbor Search (ANNS) aims to build a graph
index $G$ and approximately return vectors with minimum distances to
$\vec{x}_q$ by searching over $G$. The main drawback of graph-based ANNS is
that a graph index would be too large to fit into the memory especially for a
large-scale $\mathcal{X}$. To solve this, a Product Quantization (PQ)-based
hybrid method called DiskANN is proposed to store a low-dimensional PQ index in
memory and retain a graph index in SSD, thus reducing memory overhead while
ensuring a high search accuracy. However, it suffers from two I/O issues that
significantly affect the overall efficiency: (1) long routing path from an
entry vertex to the query's neighborhood that results in large number of I/O
requests and (2) redundant I/O requests during the routing process. We propose
an optimized DiskANN++ to overcome above issues. Specifically, for the first
issue, we present a query-sensitive entry vertex selection strategy to replace
DiskANN's static graph-central entry vertex by a dynamically determined entry
vertex that is close to the query. For the second I/O issue, we present an
isomorphic mapping on DiskANN's graph index to optimize the SSD layout and
propose an asynchronously optimized Pagesearch based on the optimized SSD
layout as an alternative to DiskANN's beamsearch. Comprehensive experimental
studies on eight real-world datasets demonstrate our DiskANN++'s superiority on
efficiency. We achieve a notable 1.5 X to 2.2 X improvement on QPS compared to
DiskANN, given the same accuracy constraint.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00405" title="Abstract">arXiv:2310.00405</a> [<a href="/pdf/2310.00405" title="Download PDF">pdf</a>, <a href="/format/2310.00405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlling Neural Style Transfer with Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chengming Feng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jing Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hongtu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+S">Siwei Lyu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IJCAI 2023. The contributions of Chengming Feng and Jing Hu to this paper were equal. arXiv admin note: text overlap with <a href="/abs/2309.13672">arXiv:2309.13672</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Controlling the degree of stylization in the Neural Style Transfer (NST) is a
little tricky since it usually needs hand-engineering on hyper-parameters. In
this paper, we propose the first deep Reinforcement Learning (RL) based
architecture that splits one-step style transfer into a step-wise process for
the NST task. Our RL-based method tends to preserve more details and structures
of the content image in early steps, and synthesize more style patterns in
later steps. It is a user-easily-controlled style-transfer method.
Additionally, as our RL-based model performs the stylization progressively, it
is lightweight and has lower computational complexity than existing one-step
Deep Learning (DL) based models. Experimental results demonstrate the
effectiveness and robustness of our method.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00406" title="Abstract">arXiv:2310.00406</a> [<a href="/pdf/2310.00406" title="Download PDF">pdf</a>, <a href="/format/2310.00406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> mmWave Beam Selection in Analog Beamforming Using Personalized Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Isaksson%2C+M">Martin Isaksson</a>, 
<a href="/search/cs?searchtype=author&query=Vannella%2C+F">Filippo Vannella</a>, 
<a href="/search/cs?searchtype=author&query=Sandberg%2C+D">David Sandberg</a>, 
<a href="/search/cs?searchtype=author&query=C%C3%B6ster%2C+R">Rickard C&#xf6;ster</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, to be published in IEEE Future Networks World Forum 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Using analog beamforming in mmWave frequency bands we can focus the energy
towards a receiver to achieve high throughput. However, this requires the
network to quickly find the best downlink beam configuration in the face of
non-IID data. We propose a personalized Federated Learning (FL) method to
address this challenge, where we learn a mapping between uplink Sub-6GHz
channel estimates and the best downlink beam in heterogeneous scenarios with
non-IID characteristics. We also devise FedLion, a FL implementation of the
Lion optimization algorithm. Our approach reduces the signaling overhead and
provides superior performance, up to 33.6% higher accuracy than a single FL
model and 6% higher than a local model.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00410" title="Abstract">arXiv:2310.00410</a> [<a href="/pdf/2310.00410" title="Download PDF">pdf</a>, <a href="/format/2310.00410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-Domain Dialogue Quality Evaluation: Deriving Nugget-level Scores  from Turn-level Scores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takehi%2C+R">Rikiya Takehi</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+A">Akihisa Watanabe</a>, 
<a href="/search/cs?searchtype=author&query=Sakai%2C+T">Tetsuya Sakai</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Annual International ACM SIGIR Conference on Research and
  Development in Information Retrieval in the Asia Pacific Region (SIGIR-AP
  `23), November 26-28, 2023, Beijing, China. ACM, New York, NY, USA, 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Existing dialogue quality evaluation systems can return a score for a given
system turn from a particular viewpoint, e.g., engagingness. However, to
improve dialogue systems by locating exactly where in a system turn potential
problems lie, a more fine-grained evaluation may be necessary. We therefore
propose an evaluation approach where a turn is decomposed into nuggets (i.e.,
expressions associated with a dialogue act), and nugget-level evaluation is
enabled by leveraging an existing turn-level evaluation system. We demonstrate
the potential effectiveness of our evaluation method through a case study.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00413" title="Abstract">arXiv:2310.00413</a> [<a href="/pdf/2310.00413" title="Download PDF">pdf</a>, <a href="/format/2310.00413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SSIF: Learning Continuous Image Representation for Spatial-Spectral  Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mai%2C+G">Gengchen Mai</a>, 
<a href="/search/cs?searchtype=author&query=Lao%2C+N">Ni Lao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Weiwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuchi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiaming Song</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+C">Chenlin Meng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Hongxu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+J">Jinmeng Rao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Existing digital sensors capture images at fixed spatial and spectral
resolutions (e.g., RGB, multispectral, and hyperspectral images), and each
combination requires bespoke machine learning models. Neural Implicit Functions
partially overcome the spatial resolution challenge by representing an image in
a resolution-independent way. However, they still operate at fixed, pre-defined
spectral resolutions. To address this challenge, we propose Spatial-Spectral
Implicit Function (SSIF), a neural implicit model that represents an image as a
function of both continuous pixel coordinates in the spatial domain and
continuous wavelengths in the spectral domain. We empirically demonstrate the
effectiveness of SSIF on two challenging spatio-spectral super-resolution
benchmarks. We observe that SSIF consistently outperforms state-of-the-art
baselines even when the baselines are allowed to train separate models at each
spectral resolution. We show that SSIF generalizes well to both unseen spatial
resolutions and spectral resolutions. Moreover, SSIF can generate
high-resolution images that improve the performance of downstream tasks (e.g.,
land use classification) by 1.7%-7%.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00416" title="Abstract">arXiv:2310.00416</a> [<a href="/pdf/2310.00416" title="Download PDF">pdf</a>, <a href="/format/2310.00416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Refutation of Shapley Values for XAI -- Additional Evidence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanxiang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Marques-Silva%2C+J">Joao Marques-Silva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2307.07514">arXiv:2307.07514</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent work demonstrated the inadequacy of Shapley values for explainable
artificial intelligence (XAI). Although to disprove a theory a single
counterexample suffices, a possible criticism of earlier work is that the focus
was solely on Boolean classifiers. To address such possible criticism, this
paper demonstrates the inadequacy of Shapley values for families of classifiers
where features are not boolean, but also for families of classifiers for which
multiple classes can be picked. Furthermore, the paper shows that the features
changed in any minimal $l_0$ distance adversarial examples do not include
irrelevant features, thus offering further arguments regarding the inadequacy
of Shapley values for XAI.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00418" title="Abstract">arXiv:2310.00418</a> [<a href="/pdf/2310.00418" title="Download PDF">pdf</a>, <a href="/format/2310.00418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MVC: A Multi-Task Vision Transformer Network for COVID-19 Diagnosis from  Chest X-ray Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tran%2C+H">Huyen Tran</a>, 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+D+T">Duc Thanh Nguyen</a>, 
<a href="/search/eess?searchtype=author&query=Yearwood%2C+J">John Yearwood</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Medical image analysis using computer-based algorithms has attracted
considerable attention from the research community and achieved tremendous
progress in the last decade. With recent advances in computing resources and
availability of large-scale medical image datasets, many deep learning models
have been developed for disease diagnosis from medical images. However,
existing techniques focus on sub-tasks, e.g., disease classification and
identification, individually, while there is a lack of a unified framework
enabling multi-task diagnosis. Inspired by the capability of Vision
Transformers in both local and global representation learning, we propose in
this paper a new method, namely Multi-task Vision Transformer (MVC) for
simultaneously classifying chest X-ray images and identifying affected regions
from the input data. Our method is built upon the Vision Transformer but
extends its learning capability in a multi-task setting. We evaluated our
proposed method and compared it with existing baselines on a benchmark dataset
of COVID-19 chest X-ray images. Experimental results verified the superiority
of the proposed method over the baselines on both the image classification and
affected region identification tasks.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00426" title="Abstract">arXiv:2310.00426</a> [<a href="/pdf/2310.00426" title="Download PDF">pdf</a>, <a href="/format/2310.00426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PixArt-$&#x3b1;$: Fast Training of Diffusion Transformer for  Photorealistic Text-to-Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junsong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jincheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+C">Chongjian Ge</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Lewei Yao</a>, 
<a href="/search/cs?searchtype=author&query=Xie1%2C+E">Enze Xie1</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongdao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J">James Kwok</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Huchuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://pixart-alpha.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The most advanced text-to-image (T2I) models require significant training
costs (e.g., millions of GPU hours), seriously hindering the fundamental
innovation for the AIGC community while increasing CO2 emissions. This paper
introduces PIXART-$\alpha$, a Transformer-based T2I diffusion model whose image
generation quality is competitive with state-of-the-art image generators (e.g.,
Imagen, SDXL, and even Midjourney), reaching near-commercial application
standards. Additionally, it supports high-resolution image synthesis up to
1024px resolution with low training cost, as shown in Figure 1 and 2. To
achieve this goal, three core designs are proposed: (1) Training strategy
decomposition: We devise three distinct training steps that separately optimize
pixel dependency, text-image alignment, and image aesthetic quality; (2)
Efficient T2I Transformer: We incorporate cross-attention modules into
Diffusion Transformer (DiT) to inject text conditions and streamline the
computation-intensive class-condition branch; (3) High-informative data: We
emphasize the significance of concept density in text-image pairs and leverage
a large Vision-Language model to auto-label dense pseudo-captions to assist
text-image alignment learning. As a result, PIXART-$\alpha$'s training speed
markedly surpasses existing large-scale T2I models, e.g., PIXART-$\alpha$ only
takes 10.8% of Stable Diffusion v1.5's training time (675 vs. 6,250 A100 GPU
days), saving nearly \$300,000 (\$26,000 vs. \$320,000) and reducing 90% CO2
emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training
cost is merely 1%. Extensive experiments demonstrate that PIXART-$\alpha$
excels in image quality, artistry, and semantic control. We hope
PIXART-$\alpha$ will provide new insights to the AIGC community and startups to
accelerate building their own high-quality yet low-cost generative models from
scratch.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00427" title="Abstract">arXiv:2310.00427</a> [<a href="/pdf/2310.00427" title="Download PDF">pdf</a>, <a href="/format/2310.00427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Technical Report of 2023 ABO Fine-grained Semantic Segmentation  Competition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zeyu Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this report, we describe the technical details of our submission to the
2023 ABO Fine-grained Semantic Segmentation Competition, by Team "Zeyu\_Dong"
(username:ZeyuDong). The task is to predicate the semantic labels for the
convex shape of five categories, which consist of high-quality, standardized 3D
models of real products available for purchase online. By using DGCNN as the
backbone to classify different structures of five classes, We carried out
numerous experiments and found learning rate stochastic gradient descent with
warm restarts and setting different rate of factors for various categories
contribute most to the performance of the model. The appropriate method helps
us rank 3rd place in the Dev phase of the 2023 ICCV 3DVeComm Workshop
Challenge.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00428" title="Abstract">arXiv:2310.00428</a> [<a href="/pdf/2310.00428" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The First Principles of Big Memory Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yu Hua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Operating Systems (cs.OS)</span>

</div>
<p class="mathjax">In this paper, we comprehensively analyze the vertical and horizontal
extensions of existing memory hierarchy. The difference between memory and big
memory is well reported. We present the state-of-the-art studies upon the big
memory systems, together with design methodology and implementations.
Persistence is the first principle of big memory systems. We further show the
full-stack and moving persistence.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00429" title="Abstract">arXiv:2310.00429</a> [<a href="/pdf/2310.00429" title="Download PDF">pdf</a>, <a href="/format/2310.00429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Stability of Iterative Retraining of Generative Models on their  own Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bertrand%2C+Q">Quentin Bertrand</a>, 
<a href="/search/cs?searchtype=author&query=Bose%2C+A+J">Avishek Joey Bose</a>, 
<a href="/search/cs?searchtype=author&query=Duplessis%2C+A">Alexandre Duplessis</a>, 
<a href="/search/cs?searchtype=author&query=Jiralerspong%2C+M">Marco Jiralerspong</a>, 
<a href="/search/cs?searchtype=author&query=Gidel%2C+G">Gauthier Gidel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Deep generative models have made tremendous progress in modeling complex
data, often exhibiting generation quality that surpasses a typical human's
ability to discern the authenticity of samples. Undeniably, a key driver of
this success is enabled by the massive amounts of web-scale data consumed by
these models. Due to these models' striking performance and ease of
availability, the web will inevitably be increasingly populated with synthetic
content. Such a fact directly implies that future iterations of generative
models must contend with the reality that their training is curated from both
clean data and artificially generated data from past models. In this paper, we
develop a framework to rigorously study the impact of training generative
models on mixed datasets (of real and synthetic data) on their stability. We
first prove the stability of iterative training under the condition that the
initial generative models approximate the data distribution well enough and the
proportion of clean training data (w.r.t. synthetic data) is large enough. We
empirically validate our theory on both synthetic and natural images by
iteratively training normalizing flows and state-of-the-art diffusion models on
CIFAR10 and FFHQ.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00430" title="Abstract">arXiv:2310.00430</a> [<a href="/pdf/2310.00430" title="Download PDF">pdf</a>, <a href="/format/2310.00430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Berkeley Open Extended Reality Recordings 2023 (BOXRR-23): 4.7 Million  Motion Capture Recordings from 105,852 Extended Reality Device Users
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nair%2C+V">Vivek Nair</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Wenbo Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=O%27Brien%2C+J+F">James F. O&#x27;Brien</a>, 
<a href="/search/cs?searchtype=author&query=Rosenberg%2C+L">Louis Rosenberg</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dawn Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Learn more at <a href="https://rdi.berkeley.edu/metaverse/boxrr-23">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Extended reality (XR) devices such as the Meta Quest and Apple Vision Pro
have seen a recent surge in attention, with motion tracking "telemetry" data
lying at the core of nearly all XR and metaverse experiences. Researchers are
just beginning to understand the implications of this data for security,
privacy, usability, and more, but currently lack large-scale human motion
datasets to study. The BOXRR-23 dataset contains 4,717,215 motion capture
recordings, voluntarily submitted by 105,852 XR device users from over 50
countries. BOXRR-23 is over 200 times larger than the largest existing motion
capture research dataset and uses a new, highly efficient purpose-built XR Open
Recording (XROR) file format.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00431" title="Abstract">arXiv:2310.00431</a> [<a href="/pdf/2310.00431" title="Download PDF">pdf</a>, <a href="/format/2310.00431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ResolvNet: A Graph Convolutional Network with multi-scale Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koke%2C+C">Christian Koke</a>, 
<a href="/search/cs?searchtype=author&query=Saroha%2C+A">Abhishek Saroha</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yuesong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Eisenberger%2C+M">Marvin Eisenberger</a>, 
<a href="/search/cs?searchtype=author&query=Cremers%2C+D">Daniel Cremers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">It is by now a well known fact in the graph learning community that the
presence of bottlenecks severely limits the ability of graph neural networks to
propagate information over long distances. What so far has not been appreciated
is that, counter-intuitively, also the presence of strongly connected
sub-graphs may severely restrict information flow in common architectures.
Motivated by this observation, we introduce the concept of multi-scale
consistency. At the node level this concept refers to the retention of a
connected propagation graph even if connectivity varies over a given graph. At
the graph-level, multi-scale consistency refers to the fact that distinct
graphs describing the same object at different resolutions should be assigned
similar feature vectors. As we show, both properties are not satisfied by
poular graph neural network architectures. To remedy these shortcomings, we
introduce ResolvNet, a flexible graph neural network based on the mathematical
concept of resolvents. We rigorously establish its multi-scale consistency
theoretically and verify it in extensive experiments on real world data: Here
networks based on this ResolvNet architecture prove expressive; out-performing
baselines significantly on many tasks; in- and outside the multi-scale setting.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00433" title="Abstract">arXiv:2310.00433</a> [<a href="/pdf/2310.00433" title="Download PDF">pdf</a>, <a href="/format/2310.00433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active-Perceptive Motion Generation for Mobile Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jauhri%2C+S">Snehal Jauhri</a>, 
<a href="/search/cs?searchtype=author&query=Lueth%2C+S">Sophie Lueth</a>, 
<a href="/search/cs?searchtype=author&query=Chalvatzaki%2C+G">Georgia Chalvatzaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print. Project page: <a href="https://sites.google.com/view/actpermoma">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Mobile Manipulation (MoMa) systems incorporate the benefits of mobility and
dexterity, thanks to the enlarged space in which they can move and interact
with their environment. MoMa robots can also continuously perceive their
environment when equipped with onboard sensors, e.g., an embodied camera.
However, extracting task-relevant visual information in unstructured and
cluttered environments such as households remains a challenge. In this work, we
introduce an active perception pipeline for mobile manipulators to generate
motions that are informative toward manipulation tasks such as grasping, in
initially unknown, cluttered scenes. Our proposed approach ActPerMoMa generates
robot trajectories in a receding horizon fashion, sampling trajectories and
computing path-wise utilities that trade-off reconstructing the unknown scene
by maximizing the visual information gain and the taskoriented objective, e.g.,
grasp success by maximizing grasp reachability efficiently. We demonstrate the
efficacy of our method in simulated experiments with a dual-arm TIAGo++ MoMa
robot performing mobile grasping in cluttered scenes and when its path is
obstructed by external obstacles. We empirically analyze the contribution of
various utilities and hyperparameters, and compare against representative
baselines both with and without active perception objectives. Finally, we
demonstrate the transfer of our mobile grasping strategy to the real world,
showing a promising direction for active-perceptive MoMa.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00434" title="Abstract">arXiv:2310.00434</a> [<a href="/pdf/2310.00434" title="Download PDF">pdf</a>, <a href="/format/2310.00434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose  Generation via Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhiyao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+T">Tian Lv</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+S">Sheng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M+G">Matthieu Gaetan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+J">Jenny Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Yu-Hui Wen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Minjing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong-jin Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://raineggplant.github.io/DiffPoseTalk/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">The generation of stylistic 3D facial animations driven by speech poses a
significant challenge as it requires learning a many-to-many mapping between
speech, style, and the corresponding natural facial motion. However, existing
methods either employ a deterministic model for speech-to-motion mapping or
encode the style using a one-hot encoding scheme. Notably, the one-hot encoding
approach fails to capture the complexity of the style and thus limits
generalization ability. In this paper, we propose DiffPoseTalk, a generative
framework based on the diffusion model combined with a style encoder that
extracts style embeddings from short reference videos. During inference, we
employ classifier-free guidance to guide the generation process based on the
speech and style. We extend this to include the generation of head poses,
thereby enhancing user perception. Additionally, we address the shortage of
scanned 3D talking face data by training our model on reconstructed 3DMM
parameters from a high-quality, in-the-wild audio-visual dataset. Our extensive
experiments and user study demonstrate that our approach outperforms
state-of-the-art methods. The code and dataset will be made publicly available.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00435" title="Abstract">arXiv:2310.00435</a> [<a href="/pdf/2310.00435" title="Download PDF">pdf</a>, <a href="/format/2310.00435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistent Aggregation of Objectives with Diverse Time Preferences  Requires Non-Markovian Rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pitis%2C+S">Silviu Pitis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of NeurIPS 2023. 10 pages (+4 references, +3 appendix)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As the capabilities of artificial agents improve, they are being increasingly
deployed to service multiple diverse objectives and stakeholders. However, the
composition of these objectives is often performed ad hoc, with no clear
justification. This paper takes a normative approach to multi-objective agency:
from a set of intuitively appealing axioms, it is shown that Markovian
aggregation of Markovian reward functions is not possible when the time
preference (discount factor) for each objective may vary. It follows that
optimal multi-objective agents must admit rewards that are non-Markovian with
respect to the individual objectives. To this end, a practical non-Markovian
aggregation scheme is proposed, which overcomes the impossibility with only one
additional parameter for each objective. This work offers new insights into
sequential, multi-objective agency and intertemporal choice, and has practical
implications for the design of AI systems deployed to serve multiple
generations of principals with varying time preference.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00436" title="Abstract">arXiv:2310.00436</a> [<a href="/pdf/2310.00436" title="Download PDF">pdf</a>, <a href="/format/2310.00436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Representation Generalization in Authorship Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haining Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Authorship identification ascertains the authorship of texts whose origins
remain undisclosed. That authorship identification techniques work as reliably
as they do has been attributed to the fact that authorial style is properly
captured and represented. Although modern authorship identification methods
have evolved significantly over the years and have proven effective in
distinguishing authorial styles, the generalization of stylistic features
across domains has not been systematically reviewed. The presented work
addresses the challenge of enhancing the generalization of stylistic
representations in authorship identification, particularly when there are
discrepancies between training and testing samples. A comprehensive review of
empirical studies was conducted, focusing on various stylistic features and
their effectiveness in representing an author's style. The influencing factors
such as topic, genre, and register on writing style were also explored, along
with strategies to mitigate their impact. While some stylistic features, like
character n-grams and function words, have proven to be robust and
discriminative, others, such as content words, can introduce biases and hinder
cross-domain generalization. Representations learned using deep learning
models, especially those incorporating character n-grams and syntactic
information, show promise in enhancing representation generalization. The
findings underscore the importance of selecting appropriate stylistic features
for authorship identification, especially in cross-domain scenarios. The
recognition of the strengths and weaknesses of various linguistic features
paves the way for more accurate authorship identification in diverse contexts.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00438" title="Abstract">arXiv:2310.00438</a> [<a href="/pdf/2310.00438" title="Download PDF">pdf</a>, <a href="/format/2310.00438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-Producible Adversarial Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khachaturov%2C+D">David Khachaturov</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yue Gao</a>, 
<a href="/search/cs?searchtype=author&query=Shumailov%2C+I">Ilia Shumailov</a>, 
<a href="/search/cs?searchtype=author&query=Mullins%2C+R">Robert Mullins</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+R">Ross Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Fawaz%2C+K">Kassem Fawaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Visual adversarial examples have so far been restricted to pixel-level image
manipulations in the digital world, or have required sophisticated equipment
such as 2D or 3D printers to be produced in the physical real world. We present
the first ever method of generating human-producible adversarial examples for
the real world that requires nothing more complicated than a marker pen. We
call them $\textbf{adversarial tags}$. First, building on top of differential
rendering, we demonstrate that it is possible to build potent adversarial
examples with just lines. We find that by drawing just $4$ lines we can disrupt
a YOLO-based model in $54.8\%$ of cases; increasing this to $9$ lines disrupts
$81.8\%$ of the cases tested. Next, we devise an improved method for line
placement to be invariant to human drawing error. We evaluate our system
thoroughly in both digital and analogue worlds and demonstrate that our tags
can be applied by untrained humans. We demonstrate the effectiveness of our
method for producing real-world adversarial examples by conducting a user study
where participants were asked to draw over printed images using digital
equivalents as guides. We further evaluate the effectiveness of both targeted
and untargeted attacks, and discuss various trade-offs and method limitations,
as well as the practical and ethical implications of our work. The source code
will be released publicly.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00441" title="Abstract">arXiv:2310.00441</a> [<a href="/pdf/2310.00441" title="Download PDF">pdf</a>, <a href="/format/2310.00441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Human Color Discrimination for Memory- and Energy-Efficient  Image Encoding in Virtual Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ujjainkar%2C+N">Nisarg Ujjainkar</a>, 
<a href="/search/cs?searchtype=author&query=Shahan%2C+E">Ethan Shahan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kenneth Chen</a>, 
<a href="/search/cs?searchtype=author&query=Duinkharjav%2C+B">Budmonde Duinkharjav</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuhao Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ASPLOS '24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Virtual Reality (VR) has the potential of becoming the next ubiquitous
computing platform. Continued progress in the burgeoning field of VR depends
critically on an efficient computing substrate. In particular, DRAM access
energy is known to contribute to a significant portion of system energy.
Today's framebuffer compression system alleviates the DRAM traffic by using a
numerically lossless compression algorithm. Being numerically lossless,
however, is unnecessary to preserve perceptual quality for humans. This paper
proposes a perceptually lossless, but numerically lossy, system to compress
DRAM traffic. Our idea builds on top of long-established psychophysical studies
that show that humans cannot discriminate colors that are close to each other.
The discrimination ability becomes even weaker (i.e., more colors are
perceptually indistinguishable) in our peripheral vision. Leveraging the color
discrimination (in)ability, we propose an algorithm that adjusts pixel colors
to minimize the bit encoding cost without introducing visible artifacts. The
algorithm is coupled with lightweight architectural support that, in real-time,
reduces the DRAM traffic by 66.9\% and outperforms existing framebuffer
compression mechanisms by up to 20.4\%. Psychophysical studies on human
participants show that our system introduce little to no perceptual fidelity
degradation.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00447" title="Abstract">arXiv:2310.00447</a> [<a href="/pdf/2310.00447" title="Download PDF">pdf</a>, <a href="/format/2310.00447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Parameters of the DC Power Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Taheri%2C+B">Babak Taheri</a>, 
<a href="/search/eess?searchtype=author&query=Molzahn%2C+D+K">Daniel K. Molzahn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Many power system operation and planning problems use the DC power flow
approximation to address computational challenges from the nonlinearity of the
AC power flow equations. The DC power flow simplifies the AC power flow
equations to a linear form that relates active power flows to phase angle
differences across branches, parameterized by coefficients based on the
branches' susceptances. Inspired by techniques for training machine learning
models, this paper proposes an algorithm that seeks optimal coefficient and
bias parameters to improve the DC power flow approximation's accuracy.
Specifically, the proposed algorithm selects the coefficient and bias parameter
values that minimize the discrepancy, across a specified set of operational
scenarios, between the power flows given by the DC approximation and the power
flows from the AC equations. Gradient-based optimization methods like
Broyden-Fletcher-Goldfarb-Shanno (BFGS), Limited-Memory BFGS (L-BFGS), and
Truncated Newton Conjugate-Gradient (TNC) enable solution of the proposed
algorithm for large systems. After an off-line training phase, the optimized
parameters are used to improve the accuracy of the DC power flow during on-line
computations. Numerical results show several orders of magnitude improvements
in accuracy relative to a hot-start DC power flow approximation across a range
of test cases.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00448" title="Abstract">arXiv:2310.00448</a> [<a href="/pdf/2310.00448" title="Download PDF">pdf</a>, <a href="/format/2310.00448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Question-Answering Model for Schizophrenia Symptoms and Their Impact on  Daily Life using Mental Health Forums Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Intern%C3%B2%2C+C">Christian Intern&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Ambrosini%2C+E">Eloisa Ambrosini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In recent years, there is strong emphasis on mining medical data using
machine learning techniques. A common problem is to obtain a noiseless set of
textual documents, with a relevant content for the research question, and
developing a Question Answering (QA) model for a specific medical field. The
purpose of this paper is to present a new methodology for building a medical
dataset and obtain a QA model for analysis of symptoms and impact on daily life
for a specific disease domain. The ``Mental Health'' forum was used, a forum
dedicated to people suffering from schizophrenia and different mental
disorders. Relevant posts of active users, who regularly participate, were
extrapolated providing a new method of obtaining low-bias content and without
privacy issues. Furthermore, it is shown how to pre-process the dataset to
convert it into a QA dataset. The Bidirectional Encoder Representations from
Transformers (BERT), DistilBERT, RoBERTa, and BioBERT models were fine-tuned
and evaluated via F1-Score, Exact Match, Precision and Recall. Accurate
empirical experiments demonstrated the effectiveness of the proposed method for
obtaining an accurate dataset for QA model implementation. By fine-tuning the
BioBERT QA model, we achieved an F1 score of 0.885, showing a considerable
improvement and outperforming the state-of-the-art model for mental disorders
domain.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00451" title="Abstract">arXiv:2310.00451</a> [<a href="/pdf/2310.00451" title="Download PDF">pdf</a>, <a href="/format/2310.00451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Role of Neural Collapse in Meta Learning Models for Few-shot  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Medepalli%2C+S">Saaketh Medepalli</a>, 
<a href="/search/cs?searchtype=author&query=Doraiswamy%2C+N">Naren Doraiswamy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Meta-learning frameworks for few-shot learning aims to learn models that can
learn new skills or adapt to new environments rapidly with a few training
examples. This has led to the generalizability of the developed model towards
new classes with just a few labelled samples. However these networks are seen
as black-box models and understanding the representations learnt under
different learning scenarios is crucial. Neural collapse ($\mathcal{NC}$) is a
recently discovered phenomenon which showcases unique properties at the network
proceeds towards zero loss. The input features collapse to their respective
class means, the class means form a Simplex equiangular tight frame (ETF) where
the class means are maximally distant and linearly separable, and the
classifier acts as a simple nearest neighbor classifier. While these phenomena
have been observed in simple classification networks, this study is the first
to explore and understand the properties of neural collapse in meta learning
frameworks for few-shot learning. We perform studies on the Omniglot dataset in
the few-shot setting and study the neural collapse phenomenon. We observe that
the learnt features indeed have the trend of neural collapse, especially as
model size grows, but to do not necessarily showcase the complete collapse as
measured by the $\mathcal{NC}$ properties.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00454" title="Abstract">arXiv:2310.00454</a> [<a href="/pdf/2310.00454" title="Download PDF">pdf</a>, <a href="/format/2310.00454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniLVSeg: Unified Left Ventricular Segmentation with Sparsely Annotated  Echocardiogram Videos through Self-Supervised Temporal Masking and Weakly  Supervised Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maani%2C+F">Fadillah Maani</a>, 
<a href="/search/cs?searchtype=author&query=Ukaye%2C+A">Asim Ukaye</a>, 
<a href="/search/cs?searchtype=author&query=Saadi%2C+N">Nada Saadi</a>, 
<a href="/search/cs?searchtype=author&query=Saeed%2C+N">Numan Saeed</a>, 
<a href="/search/cs?searchtype=author&query=Yaqub%2C+M">Mohammad Yaqub</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Echocardiography has become an indispensable clinical imaging modality for
general heart health assessment. From calculating biomarkers such as ejection
fraction to the probability of a patient's heart failure, accurate segmentation
of the heart and its structures allows doctors to plan and execute treatments
with greater precision and accuracy. However, achieving accurate and robust
left ventricle segmentation is time-consuming and challenging due to different
reasons. This work introduces a novel approach for consistent left ventricular
(LV) segmentation from sparsely annotated echocardiogram videos. We achieve
this through (1) self-supervised learning (SSL) using temporal masking followed
by (2) weakly supervised training. We investigate two different segmentation
approaches: 3D segmentation and a novel 2D superimage (SI). We demonstrate how
our proposed method outperforms the state-of-the-art solutions by achieving a
93.32% (95%CI 93.21-93.43%) dice score on a large-scale dataset
(EchoNet-Dynamic) while being more efficient. To show the effectiveness of our
approach, we provide extensive ablation studies, including pre-training
settings and various deep learning backbones. Additionally, we discuss how our
proposed methodology achieves high data utility by incorporating unlabeled
frames in the training process. To help support the AI in medicine community,
the complete solution with the source code will be made publicly available upon
acceptance.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00455" title="Abstract">arXiv:2310.00455</a> [<a href="/pdf/2310.00455" title="Download PDF">pdf</a>, <a href="/format/2310.00455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Music- and Lyrics-driven Dance Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+W">Wenjie Yin</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Q">Qingyuan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Kragic%2C+D">Danica Kragic</a>, 
<a href="/search/cs?searchtype=author&query=Bj%C3%B6rkman%2C+M">M&#xe5;rten Bj&#xf6;rkman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Graphics (cs.GR); Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Lyrics often convey information about the songs that are beyond the auditory
dimension, enriching the semantic meaning of movements and musical themes. Such
insights are important in the dance choreography domain. However, most existing
dance synthesis methods mainly focus on music-to-dance generation, without
considering the semantic information. To complement it, we introduce JustLMD, a
new multimodal dataset of 3D dance motion with music and lyrics. To the best of
our knowledge, this is the first dataset with triplet information including
dance motion, music, and lyrics. Additionally, we showcase a cross-modal
diffusion-based network designed to generate 3D dance motion conditioned on
music and lyrics. The proposed JustLMD dataset encompasses 4.6 hours of 3D
dance motion in 1867 sequences, accompanied by musical tracks and their
corresponding English lyrics.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00457" title="Abstract">arXiv:2310.00457</a> [<a href="/pdf/2310.00457" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Mortality Prediction in Heart Failure Patients: Exploring  Preprocessing Methods for Imbalanced Clinical Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kia%2C+H">Hanif Kia</a>, 
<a href="/search/cs?searchtype=author&query=Vali%2C+M">Mansour Vali</a>, 
<a href="/search/cs?searchtype=author&query=Sabahi%2C+H">Hadi Sabahi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, accepted in ICMBE2023 Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Heart failure (HF) is a critical condition in which the accurate prediction
of mortality plays a vital role in guiding patient management decisions.
However, clinical datasets used for mortality prediction in HF often suffer
from an imbalanced distribution of classes, posing significant challenges. In
this paper, we explore preprocessing methods for enhancing one-month mortality
prediction in HF patients. We present a comprehensive preprocessing framework
including scaling, outliers processing and resampling as key techniques. We
also employed an aware encoding approach to effectively handle missing values
in clinical datasets. Our study utilizes a comprehensive dataset from the
Persian Registry Of cardio Vascular disease (PROVE) with a significant class
imbalance. By leveraging appropriate preprocessing techniques and Machine
Learning (ML) algorithms, we aim to improve mortality prediction performance
for HF patients. The results reveal an average enhancement of approximately
3.6% in F1 score and 2.7% in MCC for tree-based models, specifically Random
Forest (RF) and XGBoost (XGB). This demonstrates the efficiency of our
preprocessing approach in effectively handling Imbalanced Clinical Datasets
(ICD). Our findings hold promise in guiding healthcare professionals to make
informed decisions and improve patient outcomes in HF management.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00458" title="Abstract">arXiv:2310.00458</a> [<a href="/pdf/2310.00458" title="Download PDF">pdf</a>, <a href="/format/2310.00458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forced oscillation source localization from generator measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tyloo%2C+M">Melvyn Tyloo</a>, 
<a href="/search/eess?searchtype=author&query=Vuffray%2C+M">Marc Vuffray</a>, 
<a href="/search/eess?searchtype=author&query=Lokhov%2C+A+Y">Andrey Y. Lokhov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Adaptation and Self-Organizing Systems (nlin.AO)

</div>
<p class="mathjax">Malfunctioning equipment or erroneous operating conditions can cause periodic
disturbances that would persist over time, creating an undesirable transfer of
energy across the system -- an effect referred to as forced oscillations.
Wide-area oscillations may damage assets, trigger inadvertent tripping or
control actions, and be the cause of equipment failure. Unfortunately, for
wide-area oscillations, the location, frequency, and amplitude of these forced
oscillations may be hard to determine. Recently, a data-driven
maximum-likelihood-based method was proposed to perform source localization in
transmission grids under wide-area response scenarios. However, this method
relies on full PMU coverage and all buses having inertia and damping. Here, we
extend this method to realistic scenarios which includes loads and
inverter-based generation buses. Incorporating Kron reduction directly into the
maximum likelihood estimator, we are able to identify the location and
frequency of forcing applied at both traditional generators and load or
inverter-based resources.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00463" title="Abstract">arXiv:2310.00463</a> [<a href="/pdf/2310.00463" title="Download PDF">pdf</a>, <a href="/format/2310.00463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diff-DOPE: Differentiable Deep Object Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tremblay%2C+J">Jonathan Tremblay</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+B">Bowen Wen</a>, 
<a href="/search/cs?searchtype=author&query=Blukis%2C+V">Valts Blukis</a>, 
<a href="/search/cs?searchtype=author&query=Sundaralingam%2C+B">Balakumar Sundaralingam</a>, 
<a href="/search/cs?searchtype=author&query=Tyree%2C+S">Stephen Tyree</a>, 
<a href="/search/cs?searchtype=author&query=Birchfield%2C+S">Stan Birchfield</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICRA 2023. Project page is at <a href="https://diffdope.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">We introduce Diff-DOPE, a 6-DoF pose refiner that takes as input an image, a
3D textured model of an object, and an initial pose of the object. The method
uses differentiable rendering to update the object pose to minimize the visual
error between the image and the projection of the model. We show that this
simple, yet effective, idea is able to achieve state-of-the-art results on pose
estimation datasets. Our approach is a departure from recent methods in which
the pose refiner is a deep neural network trained on a large synthetic dataset
to map inputs to refinement steps. Rather, our use of differentiable rendering
allows us to avoid training altogether. Our approach performs multiple gradient
descent optimizations in parallel with different random learning rates to avoid
local minima from symmetric objects, similar appearances, or wrong step size.
Various modalities can be used, e.g., RGB, depth, intensity edges, and object
segmentation masks. We present experiments examining the effect of various
choices, showing that the best results are found when the RGB image is
accompanied by an object mask and depth image to guide the optimization
process.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00465" title="Abstract">arXiv:2310.00465</a> [<a href="/pdf/2310.00465" title="Download PDF">pdf</a>, <a href="/format/2310.00465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expressing and Inferring Action Carefulness in Human-to-Robot Handovers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lastrico%2C+L">Linda Lastrico</a>, 
<a href="/search/cs?searchtype=author&query=Duarte%2C+N+F">Nuno Ferreira Duarte</a>, 
<a href="/search/cs?searchtype=author&query=Carf%C3%AC%2C+A">Alessandro Carf&#xec;</a>, 
<a href="/search/cs?searchtype=author&query=Rea%2C+F">Francesco Rea</a>, 
<a href="/search/cs?searchtype=author&query=Sciutti%2C+A">Alessandra Sciutti</a>, 
<a href="/search/cs?searchtype=author&query=Mastrogiovanni%2C+F">Fulvio Mastrogiovanni</a>, 
<a href="/search/cs?searchtype=author&query=Santos-Victor%2C+J">Jos&#xe9; Santos-Victor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PREPRINT VERSION Accepted for publication at the 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Implicit communication plays such a crucial role during social exchanges that
it must be considered for a good experience in human-robot interaction. This
work addresses implicit communication associated with the detection of physical
properties, transport, and manipulation of objects. We propose an ecological
approach to infer object characteristics from subtle modulations of the natural
kinematics occurring during human object manipulation. Similarly, we take
inspiration from human strategies to shape robot movements to be communicative
of the object properties while pursuing the action goals. In a realistic HRI
scenario, participants handed over cups - filled with water or empty - to a
robotic manipulator that sorted them. We implemented an online classifier to
differentiate careful/not careful human movements, associated with the cups'
content. We compared our proposed "expressive" controller, which modulates the
movements according to the cup filling, against a neutral motion controller.
Results show that human kinematics is adjusted during the task, as a function
of the cup content, even in reach-to-grasp motion. Moreover, the carefulness
during the handover of full cups can be reliably inferred online, well before
action completion. Finally, although questionnaires did not reveal explicit
preferences from participants, the expressive robot condition improved task
efficiency.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00467" title="Abstract">arXiv:2310.00467</a> [<a href="/pdf/2310.00467" title="Download PDF">pdf</a>, <a href="/ps/2310.00467" title="Download PostScript">ps</a>, <a href="/format/2310.00467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New results on Erasure Combinatorial Batch Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+P">Phuc-Lu Le</a>, 
<a href="/search/cs?searchtype=author&query=Dau%2C+S+H">Son Hoang Dau</a>, 
<a href="/search/cs?searchtype=author&query=Ngo%2C+H+D">Hy Dinh Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+D">Thuc D. Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Allerton conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">We investigate in this work the problem of Erasure Combinatorial Batch Codes,
in which $n$ files are stored on $m$ servers so that every set of $n-r$ servers
allows a client to retrieve at most $k$ distinct files by downloading at most
$t$ files from each server. Previous studies have solved this problem for the
special case of $t=1$ using Combinatorial Batch Codes. We tackle the general
case $t \geq 1$ using a generalization of Hall's theorem. Additionally, we
address a realistic scenario in which the retrieved files are consecutive
according to some order and provide a simple and optimal solution for this
case.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00468" title="Abstract">arXiv:2310.00468</a> [<a href="/pdf/2310.00468" title="Download PDF">pdf</a>, <a href="/ps/2310.00468" title="Download PostScript">ps</a>, <a href="/format/2310.00468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Encouraging Inferable Behavior for Autonomy: Repeated Bimatrix  Stackelberg Games with Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karabag%2C+M+O">Mustafa O. Karabag</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+S">Sophia Smith</a>, 
<a href="/search/cs?searchtype=author&query=Fridovich-Keil%2C+D">David Fridovich-Keil</a>, 
<a href="/search/cs?searchtype=author&query=Topcu%2C+U">Ufuk Topcu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">When interacting with other non-competitive decision-making agents, it is
critical for an autonomous agent to have inferable behavior: Their actions must
convey their intention and strategy. For example, an autonomous car's strategy
must be inferable by the pedestrians interacting with the car. We model the
inferability problem using a repeated bimatrix Stackelberg game with
observations where a leader and a follower repeatedly interact. During the
interactions, the leader uses a fixed, potentially mixed strategy. The
follower, on the other hand, does not know the leader's strategy and
dynamically reacts based on observations that are the leader's previous
actions. In the setting with observations, the leader may suffer from an
inferability loss, i.e., the performance compared to the setting where the
follower has perfect information of the leader's strategy. We show that the
inferability loss is upper-bounded by a function of the number of interactions
and the stochasticity level of the leader's strategy, encouraging the use of
inferable strategies with lower stochasticity levels. As a converse result, we
also provide a game where the required number of interactions is lower bounded
by a function of the desired inferability loss.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00469" title="Abstract">arXiv:2310.00469</a> [<a href="/pdf/2310.00469" title="Download PDF">pdf</a>, <a href="/format/2310.00469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> State of In Situ Visualization in Simulations: We are fast. But are we  inspiring?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huebl%2C+A">Axel Huebl</a>, 
<a href="/search/cs?searchtype=author&query=Formenti%2C+A">Arianna Formenti</a>, 
<a href="/search/cs?searchtype=author&query=Garten%2C+M">Marco Garten</a>, 
<a href="/search/cs?searchtype=author&query=Vay%2C+J">Jean-Luc Vay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages + references, 1 figure, accepted lightning talk abstract for ISAV23 (in conjunction with SC23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Visualization of dynamic processes in scientific high-performance computing
is an immensely data intensive endeavor. Application codes have recently
demonstrated scaling to full-size Exascale machines, and generating
high-quality data for visualization is consequently on the machine-scale,
easily spanning 100s of TBytes of input to generate a single video frame. In
situ visualization, the technique to consume the many-node decomposed data
in-memory, as exposed by applications, is the dominant workflow. Although in
situ visualization has achieved tremendous progress in the last decade, scaling
to system-size together with the application codes that produce its data, there
is one important question that we cannot skip: is what we produce insightful
and inspiring?
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00473" title="Abstract">arXiv:2310.00473</a> [<a href="/pdf/2310.00473" title="Download PDF">pdf</a>, <a href="/ps/2310.00473" title="Download PostScript">ps</a>, <a href="/format/2310.00473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Control of Grid-Interfacing Inverters With Current Magnitude  Limits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Joswig-Jones%2C+T">Trager Joswig-Jones</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+B">Baosen Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, 1 table. Submitted to ACC'2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Grid-interfacing inverters act as the interface between renewable resources
and the electric grid, and have the potential to offer fast and programmable
controls compared to synchronous generators. With this flexibility there has
been significant research efforts into determining the best way to control
these inverters. Inverters are limited in their maximum current output in order
to protect semiconductor devices, presenting a nonlinear constraint that needs
to be accounted for in their control algorithms. Existing approaches either
simply saturate a controller that is designed for unconstrained systems, or
assume small perturbations and linearize a saturated system. These approaches
can lead to stability issues or limiting the control actions to be too
conservative.
<br />In this paper, we directly focus on a nonlinear system that explicitly
accounts for the saturation of the current magnitude. We use a Lyapunov
stability approach to determine a stability condition for the system,
guaranteeing that a class of controllers would be stabilizing if they satisfy a
simple SDP condition. With this condition we fit a linear-feedback controller
by sampling the output (offline) model predictive control problems. This
learned controller has improved performances with existing designs.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00481" title="Abstract">arXiv:2310.00481</a> [<a href="/pdf/2310.00481" title="Download PDF">pdf</a>, <a href="/format/2310.00481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LANCAR: Leveraging Language for Context-Aware Robot Locomotion in  Unstructured Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shek%2C+C+L">Chak Lam Shek</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>, 
<a href="/search/cs?searchtype=author&query=Tokekar%2C+P">Pratap Tokekar</a>, 
<a href="/search/cs?searchtype=author&query=Bedi%2C+A+S">Amrit Singh Bedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Robotic locomotion is a challenging task, especially in unstructured
terrains. In practice, the optimal locomotion policy can be context-dependent
by using the contextual information of encountered terrains in decision-making.
Humans can interpret the environmental context for robots, but the ambiguity of
human language makes it challenging to use in robot locomotion directly. In
this paper, we propose a novel approach, LANCAR, that introduces a context
translator that works with reinforcement learning (RL) agents for context-aware
locomotion. Our formulation allows a robot to interpret the contextual
information from environments generated by human observers or Vision-Language
Models (VLM) with Large Language Models (LLM) and use this information to
generate contextual embeddings. We incorporate the contextual embeddings with
the robot's internal environmental observations as the input to the RL agent's
decision neural network. We evaluate LANCAR with contextual information in
varying ambiguity levels and compare its performance using several alternative
approaches. Our experimental results demonstrate that our approach exhibits
good generalizability and adaptability across diverse terrains, by achieving at
least 10% of performance improvement in episodic reward over baselines. The
experiment video can be found at the following link:
https://raaslab.org/projects/LLM_Context_Estimation/.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00483" title="Abstract">arXiv:2310.00483</a> [<a href="/pdf/2310.00483" title="Download PDF">pdf</a>, <a href="/ps/2310.00483" title="Download PostScript">ps</a>, <a href="/format/2310.00483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Code Interpreter to Write Better Unit Tests on Quixbugs  Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+V">Vincent Li</a>, 
<a href="/search/cs?searchtype=author&query=Doiron%2C+N">Nick Doiron</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages (including appendices), 0 figures, 1 table. First authored by Vincent Li; edited by Nick Doiron
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Unit testing is a commonly-used approach in software engineering to test the
correctness and robustness of written code. Unit tests are tests designed to
test small components of a codebase in isolation, such as an individual
function or method. Although unit tests have historically been written by human
programmers, recent advancements in AI, particularly LLMs, have shown
corresponding advances in automatic unit test generation. In this study, we
explore the effect of different prompts on the quality of unit tests generated
by Code Interpreter, a GPT-4-based LLM, on Python functions provided by the
Quixbugs dataset, and we focus on prompting due to the ease with which users
can make use of our findings and observations. We find that the quality of the
generated unit tests is not sensitive to changes in minor details in the
prompts provided. However, we observe that Code Interpreter is often able to
effectively identify and correct mistakes in code that it writes, suggesting
that providing it runnable code to check the correctness of its outputs would
be beneficial, even though we find that it is already often able to generate
correctly-formatted unit tests. Our findings suggest that, when prompting
models similar to Code Interpreter, it is important to include the basic
information necessary to generate unit tests, but minor details are not as
important.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00486" title="Abstract">arXiv:2310.00486</a> [<a href="/pdf/2310.00486" title="Download PDF">pdf</a>, <a href="/format/2310.00486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> It HAS to be Subjective: Human Annotator Simulation via Zero-shot  Density Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenlin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Woodland%2C+P+C">Philip C. Woodland</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at: <a href="https://github.com/W-Wu/HAS_CNF">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Human annotator simulation (HAS) serves as a cost-effective substitute for
human evaluation such as data annotation and system assessment. Human
perception and behaviour during human evaluation exhibit inherent variability
due to diverse cognitive processes and subjective interpretations, which should
be taken into account in modelling to better mimic the way people perceive and
interact with the world. This paper introduces a novel meta-learning framework
that treats HAS as a zero-shot density estimation problem, which incorporates
human variability and allows for the efficient generation of human-like
annotations for unlabelled test inputs. Under this framework, we propose two
new model classes, conditional integer flows and conditional softmax flows, to
account for ordinal and categorical annotations, respectively. The proposed
method is evaluated on three real-world human evaluation tasks and shows
superior capability and efficiency to predict the aggregated behaviours of
human annotators, match the distribution of human annotations, and simulate the
inter-annotator disagreements.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00488" title="Abstract">arXiv:2310.00488</a> [<a href="/pdf/2310.00488" title="Download PDF">pdf</a>, <a href="/format/2310.00488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Memorization and Privacy risks of Sharpness Aware Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y+I">Young In Kim</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Pratiksha Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Royset%2C+J+O">Johannes O. Royset</a>, 
<a href="/search/cs?searchtype=author&query=Khanna%2C+R">Rajiv Khanna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In many recent works, there is an increased focus on designing algorithms
that seek flatter optima for neural network loss optimization as there is
empirical evidence that it leads to better generalization performance in many
datasets. In this work, we dissect these performance gains through the lens of
data memorization in overparameterized models. We define a new metric that
helps us identify which data points specifically do algorithms seeking flatter
optima do better when compared to vanilla SGD. We find that the generalization
gains achieved by Sharpness Aware Minimization (SAM) are particularly
pronounced for atypical data points, which necessitate memorization. This
insight helps us unearth higher privacy risks associated with SAM, which we
verify through exhaustive empirical evaluations. Finally, we propose mitigation
strategies to achieve a more desirable accuracy vs privacy tradeoff.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00489" title="Abstract">arXiv:2310.00489</a> [<a href="/pdf/2310.00489" title="Download PDF">pdf</a>, <a href="/format/2310.00489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic DAG Discovery for Interpretable Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+i">ianxiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenchao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuncong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanchi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Imitation learning, which learns agent policy by mimicking expert
demonstration, has shown promising results in many applications such as medical
treatment regimes and self-driving vehicles. However, it remains a difficult
task to interpret control policies learned by the agent. Difficulties mainly
come from two aspects: 1) agents in imitation learning are usually implemented
as deep neural networks, which are black-box models and lack interpretability;
2) the latent causal mechanism behind agents' decisions may vary along the
trajectory, rather than staying static throughout time steps. To increase
transparency and offer better interpretability of the neural agent, we propose
to expose its captured knowledge in the form of a directed acyclic causal
graph, with nodes being action and state variables and edges denoting the
causal relations behind predictions. Furthermore, we design this causal
discovery process to be state-dependent, enabling it to model the dynamics in
latent causal graphs. Concretely, we conduct causal discovery from the
perspective of Granger causality and propose a self-explainable imitation
learning framework, {\method}. The proposed framework is composed of three
parts: a dynamic causal discovery module, a causality encoding module, and a
prediction module, and is trained in an end-to-end manner. After the model is
learned, we can obtain causal relations among states and action variables
behind its decisions, exposing policies learned by it. Experimental results on
both synthetic and real-world datasets demonstrate the effectiveness of the
proposed {\method} in learning the dynamic causal graphs for understanding the
decision-making of imitation learning meanwhile maintaining high prediction
accuracy.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00491" title="Abstract">arXiv:2310.00491</a> [<a href="/pdf/2310.00491" title="Download PDF">pdf</a>, <a href="/format/2310.00491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StreetNav: Leveraging Street Cameras to Support Precise Outdoor  Navigation for Blind Pedestrians
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+G">Gaurav Jain</a>, 
<a href="/search/cs?searchtype=author&query=Hindi%2C+B">Basel Hindi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zihao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasula%2C+K">Koushik Srinivasula</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+M">Mingyu Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ghasemi%2C+M">Mahshid Ghasemi</a>, 
<a href="/search/cs?searchtype=author&query=Weiner%2C+D">Daniel Weiner</a>, 
<a href="/search/cs?searchtype=author&query=Paris%2C+S+A">Sophie Ana Paris</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X+Y+T">Xin Yi Therese Xu</a>, 
<a href="/search/cs?searchtype=author&query=Malcolm%2C+M">Michael Malcolm</a>, 
<a href="/search/cs?searchtype=author&query=Turkcan%2C+M">Mehmet Turkcan</a>, 
<a href="/search/cs?searchtype=author&query=Ghaderi%2C+J">Javad Ghaderi</a>, 
<a href="/search/cs?searchtype=author&query=Kostic%2C+Z">Zoran Kostic</a>, 
<a href="/search/cs?searchtype=author&query=Zussman%2C+G">Gil Zussman</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+B+A">Brian A. Smith</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Blind and low-vision (BLV) people rely on GPS-based systems for outdoor
navigation. GPS's inaccuracy, however, causes them to veer off track, run into
unexpected obstacles, and struggle to reach precise destinations. While prior
work has made precise navigation possible indoors via additional hardware
installations, enabling precise navigation outdoors remains a challenge.
Ironically, many outdoor environments of interest such as downtown districts
are already instrumented with hardware such as street cameras. In this work, we
explore the idea of repurposing street cameras for outdoor navigation, and
investigate the effectiveness of such an approach. Our resulting system,
StreetNav, processes the cameras' video feeds using computer vision and gives
BLV pedestrians real-time navigation assistance. Our user evaluations in the
COSMOS testbed with eight BLV pedestrians show that StreetNav guides them more
precisely than GPS, but its performance is sensitive to lighting conditions and
environmental occlusions. We discuss future implications for deploying such
systems at scale.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00492" title="Abstract">arXiv:2310.00492</a> [<a href="/pdf/2310.00492" title="Download PDF">pdf</a>, <a href="/format/2310.00492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Language Modeling to Instruction Following: Understanding the  Behavior Shift in LLMs after Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuansheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Wenlin Yao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianshu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xiaoman Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ninghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 13 figures, 12 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) have achieved remarkable success, demonstrating
powerful instruction-following capabilities across diverse tasks. Instruction
fine-tuning is critical in enabling LLMs to align with user intentions and
effectively follow instructions. In this work, we investigate how instruction
fine-tuning modifies pre-trained models, focusing on two perspectives:
instruction recognition and knowledge evolution. To study the behavior shift of
LLMs, we employ a suite of local and global explanation methods, including a
gradient-based approach for input-output attribution and techniques for
interpreting patterns and concepts in self-attention and feed-forward layers.
Our findings reveal three significant impacts of instruction fine-tuning: 1) It
empowers LLMs to better recognize the instruction parts from user prompts,
thereby facilitating high-quality response generation and addressing the
``lost-in-the-middle'' issue observed in pre-trained models; 2) It aligns the
knowledge stored in feed-forward layers with user-oriented tasks, exhibiting
minimal shifts across linguistic levels. 3) It facilitates the learning of
word-word relations with instruction verbs through the self-attention
mechanism, particularly in the lower and middle layers, indicating enhanced
recognition of instruction words. These insights contribute to a deeper
understanding of the behavior shifts in LLMs after instruction fine-tuning and
lay the groundwork for future research aimed at interpreting and optimizing
LLMs for various applications. We will release our code and data soon.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00496" title="Abstract">arXiv:2310.00496</a> [<a href="/pdf/2310.00496" title="Download PDF">pdf</a>, <a href="/format/2310.00496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Sparsity Roofline: Understanding the Hardware Limits of Sparse  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shinn%2C+C">Cameron Shinn</a>, 
<a href="/search/cs?searchtype=author&query=McCarthy%2C+C">Collin McCarthy</a>, 
<a href="/search/cs?searchtype=author&query=Muralidharan%2C+S">Saurav Muralidharan</a>, 
<a href="/search/cs?searchtype=author&query=Osama%2C+M">Muhammad Osama</a>, 
<a href="/search/cs?searchtype=author&query=Owens%2C+J+D">John D. Owens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce the Sparsity Roofline, a visual performance model for evaluating
sparsity in neural networks. The Sparsity Roofline jointly models network
accuracy, sparsity, and predicted inference speedup. Our approach does not
require implementing and benchmarking optimized kernels, and the predicted
speedup is equal to what would be measured when the corresponding dense and
sparse kernels are equally well-optimized. We achieve this through a novel
analytical model for predicting sparse network performance, and validate the
predicted speedup using several real-world computer vision architectures pruned
across a range of sparsity patterns and degrees. We demonstrate the utility and
ease-of-use of our model through two case studies: (1) we show how machine
learning researchers can predict the performance of unimplemented or
unoptimized block-structured sparsity patterns, and (2) we show how hardware
designers can predict the performance implications of new sparsity patterns and
sparse data formats in hardware. In both scenarios, the Sparsity Roofline helps
performance experts identify sparsity regimes with the highest performance
potential.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00498" title="Abstract">arXiv:2310.00498</a> [<a href="/pdf/2310.00498" title="Download PDF">pdf</a>, <a href="/format/2310.00498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Gait Generation For Walking, Soft Robotic Quadrupeds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ketchum%2C+J">Jake Ketchum</a>, 
<a href="/search/cs?searchtype=author&query=Schiffer%2C+S">Sophia Schiffer</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Muchen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Kaarthik%2C+P">Pranav Kaarthik</a>, 
<a href="/search/cs?searchtype=author&query=Truby%2C+R+L">Ryan L. Truby</a>, 
<a href="/search/cs?searchtype=author&query=Murphey%2C+T+D">Todd D. Murphey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 Pages, 6 Figures, Published at IROS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Gait generation for soft robots is challenging due to the nonlinear dynamics
and high dimensional input spaces of soft actuators. Limitations in soft
robotic control and perception force researchers to hand-craft open loop
controllers for gait sequences, which is a non-trivial process. Moreover, short
soft actuator lifespans and natural variations in actuator behavior limit
machine learning techniques to settings that can be learned on the same time
scales as robot deployment. Lastly, simulation is not always possible, due to
heterogeneity and nonlinearity in soft robotic materials and their dynamics
change due to wear. We present a sample-efficient, simulation free, method for
self-generating soft robot gaits, using very minimal computation. This
technique is demonstrated on a motorized soft robotic quadruped that walks
using four legs constructed from 16 ``handed shearing auxetic" (HSA) actuators.
To manage the dimension of the search space, gaits are composed of two
sequential sets of leg motions selected from 7 possible primitives. Pairs of
primitives are executed on one leg at a time; we then select the
best-performing pair to execute while moving on to subsequent legs. This method
-- which uses no simulation, sophisticated computation, or user input --
consistently generates good translation and rotation gaits in as low as 4
minutes of hardware experimentation, outperforming hand-crafted gaits. This is
the first demonstration of completely autonomous gait generation in a soft
robot.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00500" title="Abstract">arXiv:2310.00500</a> [<a href="/pdf/2310.00500" title="Download PDF">pdf</a>, <a href="/format/2310.00500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Small Visual Language Models can also be Open-Ended Few-Shot Learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Derakhshani%2C+M+M">Mohammad Mahdi Derakhshani</a>, 
<a href="/search/cs?searchtype=author&query=Najdenkoska%2C+I">Ivona Najdenkoska</a>, 
<a href="/search/cs?searchtype=author&query=Snoek%2C+C+G+M">Cees G. M. Snoek</a>, 
<a href="/search/cs?searchtype=author&query=Worring%2C+M">Marcel Worring</a>, 
<a href="/search/cs?searchtype=author&query=Asano%2C+Y+M">Yuki M. Asano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present Self-Context Adaptation (SeCAt), a self-supervised approach that
unlocks open-ended few-shot abilities of small visual language models. Our
proposed adaptation algorithm explicitly learns from symbolic, yet
self-supervised training tasks. Specifically, our approach imitates image
captions in a self-supervised way based on clustering a large pool of images
followed by assigning semantically-unrelated names to clusters. By doing so, we
construct the `self-context', a training signal consisting of interleaved
sequences of image and pseudo-caption pairs and a query image for which the
model is trained to produce the right pseudo-caption. We demonstrate the
performance and flexibility of SeCAt on several multimodal few-shot datasets,
spanning various granularities. By using models with approximately 1B
parameters we outperform the few-shot abilities of much larger models, such as
Frozen and FROMAGe. SeCAt opens new possibilities for research in open-ended
few-shot learning that otherwise requires access to large or proprietary
models.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00503" title="Abstract">arXiv:2310.00503</a> [<a href="/pdf/2310.00503" title="Download PDF">pdf</a>, <a href="/format/2310.00503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Black-box Attacks on Image Activity Prediction and its Natural Language  Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baia%2C+A+E">Alina Elena Baia</a>, 
<a href="/search/cs?searchtype=author&query=Poggioni%2C+V">Valentina Poggioni</a>, 
<a href="/search/cs?searchtype=author&query=Cavallaro%2C+A">Andrea Cavallaro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICCV2023 AROW Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Explainable AI (XAI) methods aim to describe the decision process of deep
neural networks. Early XAI methods produced visual explanations, whereas more
recent techniques generate multimodal explanations that include textual
information and visual representations. Visual XAI methods have been shown to
be vulnerable to white-box and gray-box adversarial attacks, with an attacker
having full or partial knowledge of and access to the target system. As the
vulnerabilities of multimodal XAI models have not been examined, in this paper
we assess for the first time the robustness to black-box attacks of the natural
language explanations generated by a self-rationalizing image-based activity
recognition model. We generate unrestricted, spatially variant perturbations
that disrupt the association between the predictions and the corresponding
explanations to mislead the model into generating unfaithful explanations. We
show that we can create adversarial images that manipulate the explanations of
an activity recognition model by having access only to its final output.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00504" title="Abstract">arXiv:2310.00504</a> [<a href="/pdf/2310.00504" title="Download PDF">pdf</a>, <a href="/format/2310.00504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring SAM Ablations for Enhancing Medical Segmentation in Radiology  and Pathology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ranem%2C+A">Amin Ranem</a>, 
<a href="/search/cs?searchtype=author&query=Babendererde%2C+N">Niklas Babendererde</a>, 
<a href="/search/cs?searchtype=author&query=Fuchs%2C+M">Moritz Fuchs</a>, 
<a href="/search/cs?searchtype=author&query=Mukhopadhyay%2C+A">Anirban Mukhopadhyay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Medical imaging plays a critical role in the diagnosis and treatment planning
of various medical conditions, with radiology and pathology heavily reliant on
precise image segmentation. The Segment Anything Model (SAM) has emerged as a
promising framework for addressing segmentation challenges across different
domains. In this white paper, we delve into SAM, breaking down its fundamental
components and uncovering the intricate interactions between them. We also
explore the fine-tuning of SAM and assess its profound impact on the accuracy
and reliability of segmentation results, focusing on applications in radiology
(specifically, brain tumor segmentation) and pathology (specifically, breast
cancer segmentation). Through a series of carefully designed experiments, we
analyze SAM's potential application in the field of medical imaging. We aim to
bridge the gap between advanced segmentation techniques and the demanding
requirements of healthcare, shedding light on SAM's transformative
capabilities.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00505" title="Abstract">arXiv:2310.00505</a> [<a href="/pdf/2310.00505" title="Download PDF">pdf</a>, <a href="/format/2310.00505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling the Unborn: Advancing Fetal Health Classification through  Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mandala%2C+S+K">Sujith K Mandala</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Fetal health classification is a critical task in obstetrics, enabling early
identification and management of potential health problems. However, it remains
challenging due to data complexity and limited labeled samples. This research
paper presents a novel machine-learning approach for fetal health
classification, leveraging a LightGBM classifier trained on a comprehensive
dataset. The proposed model achieves an impressive accuracy of 98.31% on a test
set. Our findings demonstrate the potential of machine learning in enhancing
fetal health classification, offering a more objective and accurate assessment.
Notably, our approach combines various features, such as fetal heart rate,
uterine contractions, and maternal blood pressure, to provide a comprehensive
evaluation. This methodology holds promise for improving early detection and
treatment of fetal health issues, ensuring better outcomes for both mothers and
babies. Beyond the high accuracy achieved, the novelty of our approach lies in
its comprehensive feature selection and assessment methodology. By
incorporating multiple data points, our model offers a more holistic and
reliable evaluation compared to traditional methods. This research has
significant implications in the field of obstetrics, paving the way for
advancements in early detection and intervention of fetal health concerns.
Future work involves validating the model on a larger dataset and developing a
clinical application. Ultimately, we anticipate that our research will
revolutionize the assessment and management of fetal health, contributing to
improved healthcare outcomes for expectant mothers and their babies.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00508" title="Abstract">arXiv:2310.00508</a> [<a href="/pdf/2310.00508" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analytical Modeling of Parameter Imbalance in Permanent Magnet  Synchronous Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pramod%2C+P">Prerit Pramod</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper presents a systematic and comprehensive analysis of the impact of
parameter imbalance in permanent magnet synchronous machines. Analytical models
that reveal the effects of imbalance are obtained for each parameter.
Thereafter, the models are verified for accuracy by comparison with complex
simulations that closely represent true machine behavior. Such models may be
utilized for developing (general) algorithms for detection, learning and
mitigation of the negative effects of parameter imbalance including current
(and thus torque) pulsations during real-time operation.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00510" title="Abstract">arXiv:2310.00510</a> [<a href="/pdf/2310.00510" title="Download PDF">pdf</a>, <a href="/format/2310.00510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Benchmarks for Self-Driving Labs using Color Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ginsburg%2C+T">Tobias Ginsburg</a>, 
<a href="/search/cs?searchtype=author&query=Hippe%2C+K">Kyle Hippe</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+R">Ryan Lewis</a>, 
<a href="/search/cs?searchtype=author&query=Ozgulbas%2C+D">Doga Ozgulbas</a>, 
<a href="/search/cs?searchtype=author&query=Cleary%2C+A">Aileen Cleary</a>, 
<a href="/search/cs?searchtype=author&query=Butler%2C+R">Rory Butler</a>, 
<a href="/search/cs?searchtype=author&query=Stone%2C+C">Casey Stone</a>, 
<a href="/search/cs?searchtype=author&query=Stroka%2C+A">Abraham Stroka</a>, 
<a href="/search/cs?searchtype=author&query=Foster%2C+I">Ian Foster</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Self Driving Labs (SDLs) that combine automation of experimental procedures
with autonomous decision making are gaining popularity as a means of increasing
the throughput of scientific workflows. The task of identifying quantities of
supplied colored pigments that match a target color, the color matching
problem, provides a simple and flexible SDL test case, as it requires
experiment proposal, sample creation, and sample analysis, three common
components in autonomous discovery applications. We present a robotic solution
to the color matching problem that allows for fully autonomous execution of a
color matching protocol. Our solution leverages the WEI science factory
platform to enable portability across different robotic hardware, the use of
alternative optimization methods for continuous refinement, and automated
publication of results for experiment tracking and post-hoc analysis.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00511" title="Abstract">arXiv:2310.00511</a> [<a href="/pdf/2310.00511" title="Download PDF">pdf</a>, <a href="/ps/2310.00511" title="Download PostScript">ps</a>, <a href="/format/2310.00511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonparametric active learning for cost-sensitive classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Njike%2C+B+N">Boris Ndjia Njike</a>, 
<a href="/search/cs?searchtype=author&query=Siebert%2C+X">Xavier Siebert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST)

</div>
<p class="mathjax">Cost-sensitive learning is a common type of machine learning problem where
different errors of prediction incur different costs. In this paper, we design
a generic nonparametric active learning algorithm for cost-sensitive
classification. Based on the construction of confidence bounds for the expected
prediction cost functions of each label, our algorithm sequentially selects the
most informative vector points. Then it interacts with them by only querying
the costs of prediction that could be the smallest. We prove that our algorithm
attains optimal rate of convergence in terms of the number of interactions with
the feature vector space. Furthermore, in terms of a general version of
Tsybakov's noise assumption, the gain over the corresponding passive learning
is explicitly characterized by the probability-mass of the boundary decision.
Additionally, we prove the near-optimality of obtained upper bounds by
providing matching (up to logarithmic factor) lower bounds.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00513" title="Abstract">arXiv:2310.00513</a> [<a href="/pdf/2310.00513" title="Download PDF">pdf</a>, <a href="/ps/2310.00513" title="Download PostScript">ps</a>, <a href="/format/2310.00513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formal Probabilistic Methods for Combinatorial Structures in  Isabelle/HOL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Edmonds%2C+C">Chelsea Edmonds</a>, 
<a href="/search/cs?searchtype=author&query=Paulson%2C+L+C">Lawrence C. Paulson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Initial paper preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">Formalised libraries of combinatorial mathematics have rapidly expanded over
the last five years, but few use one of the most important tools: probability.
How can often intuitive probabilistic arguments be translated into a formal
text? We present a modular framework in Isabelle/HOL to formalise combinatorial
proofs using probabilistic methods such as the Lov\'asz local lemma, a
fundamental result in probability which is particularly important for existence
proofs. We apply the framework to formalise several classic lemmas on
hypergraph colourings, revealing how intuitive probabilistic reasoning can lead
mathematicians astray.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00516" title="Abstract">arXiv:2310.00516</a> [<a href="/pdf/2310.00516" title="Download PDF">pdf</a>, <a href="/format/2310.00516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Efficiency and Privacy in Memory-Based Malware Classification  through Feature Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sazzed%2C+S">Salim Sazzed</a>, 
<a href="/search/cs?searchtype=author&query=Ullah%2C+S">Sharif Ullah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE ICMLA-2023 Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Malware poses a significant security risk to individuals, organizations, and
critical infrastructure by compromising systems and data. Leveraging memory
dumps that offer snapshots of computer memory can aid the analysis and
detection of malicious content, including malware. To improve the efficacy and
address privacy concerns in malware classification systems, feature selection
can play a critical role as it is capable of identifying the most relevant
features, thus, minimizing the amount of data fed to classifiers. In this
study, we employ three feature selection approaches to identify significant
features from memory content and use them with a diverse set of classifiers to
enhance the performance and privacy of the classification task. Comprehensive
experiments are conducted across three levels of malware classification tasks:
i) binary-level benign or malware classification, ii) malware type
classification (including Trojan horse, ransomware, and spyware), and iii)
malware family classification within each family (with varying numbers of
classes). Results demonstrate that the feature selection strategy,
incorporating mutual information and other methods, enhances classifier
performance for all tasks. Notably, selecting only 25\% and 50\% of input
features using Mutual Information and then employing the Random Forest
classifier yields the best results. Our findings reinforce the importance of
feature selection for malware classification and provide valuable insights for
identifying appropriate approaches. By advancing the effectiveness and privacy
of malware classification systems, this research contributes to safeguarding
against security threats posed by malicious software.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00517" title="Abstract">arXiv:2310.00517</a> [<a href="/pdf/2310.00517" title="Download PDF">pdf</a>, <a href="/format/2310.00517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing the Generalizability of Deep Neural Networks-Based Models for  Black Skin Lesions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barros%2C+L">Luana Barros</a>, 
<a href="/search/cs?searchtype=author&query=Chaves%2C+L">Levy Chaves</a>, 
<a href="/search/cs?searchtype=author&query=Avila%2C+S">Sandra Avila</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 3 figures, 7 tables. Accepted at CIARP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Melanoma is the most severe type of skin cancer due to its ability to cause
metastasis. It is more common in black people, often affecting acral regions:
palms, soles, and nails. Deep neural networks have shown tremendous potential
for improving clinical care and skin cancer diagnosis. Nevertheless, prevailing
studies predominantly rely on datasets of white skin tones, neglecting to
report diagnostic outcomes for diverse patient skin tones. In this work, we
evaluate supervised and self-supervised models in skin lesion images extracted
from acral regions commonly observed in black individuals. Also, we carefully
curate a dataset containing skin lesions in acral regions and assess the
datasets concerning the Fitzpatrick scale to verify performance on black skin.
Our results expose the poor generalizability of these models, revealing their
favorable performance for lesions on white skin. Neglecting to create diverse
datasets, which necessitates the development of specialized models, is
unacceptable. Deep neural networks have great potential to improve diagnosis,
particularly for populations with limited access to dermatology. However,
including black skin lesions is necessary to ensure these populations can
access the benefits of inclusive technology.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00519" title="Abstract">arXiv:2310.00519</a> [<a href="/pdf/2310.00519" title="Download PDF">pdf</a>, <a href="/format/2310.00519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite element analysis of a generalized Robin boundary value problem in  curved domains based on the extension method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kashiwabara%2C+T">Takahito Kashiwabara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A theoretical analysis of the finite element method for a generalized Robin
boundary value problem, which involves a second-order differential operator on
the boundary, is presented. If $\Omega$ is a general smooth domain with a
curved boundary, we need to introduce an approximate domain $\Omega_h$ and to
address issues owing to the domain perturbation $\Omega \neq \Omega_h$. In
contrast to the transformation approach used in existing studies, we employ the
extension approach, which is easier to handle in practical computation, in
order to construct a numerical scheme. Assuming that approximate domains and
function spaces are given by isoparametric finite elements of order $k$, we
prove the optimal rate of convergence in the $H^1$- and $L^2$-norms. A
numerical example is given for the piecewise linear case $k = 1$.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00522" title="Abstract">arXiv:2310.00522</a> [<a href="/pdf/2310.00522" title="Download PDF">pdf</a>, <a href="/format/2310.00522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mapping of Internet &quot;Coastlines&quot; via Large Scale Anonymized Network  Source Correlations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jananthan%2C+H">Hayden Jananthan</a>, 
<a href="/search/cs?searchtype=author&query=Kepner%2C+J">Jeremy Kepner</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+M">Michael Jones</a>, 
<a href="/search/cs?searchtype=author&query=Arcand%2C+W">William Arcand</a>, 
<a href="/search/cs?searchtype=author&query=Bestor%2C+D">David Bestor</a>, 
<a href="/search/cs?searchtype=author&query=Bergeron%2C+W">William Bergeron</a>, 
<a href="/search/cs?searchtype=author&query=Byun%2C+C">Chansup Byun</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+T">Timothy Davis</a>, 
<a href="/search/cs?searchtype=author&query=Gadepally%2C+V">Vijay Gadepally</a>, 
<a href="/search/cs?searchtype=author&query=Grant%2C+D">Daniel Grant</a>, 
<a href="/search/cs?searchtype=author&query=Houle%2C+M">Michael Houle</a>, 
<a href="/search/cs?searchtype=author&query=Hubbell%2C+M">Matthew Hubbell</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+A">Anna Klein</a>, 
<a href="/search/cs?searchtype=author&query=Milechin%2C+L">Lauren Milechin</a>, 
<a href="/search/cs?searchtype=author&query=Morales%2C+G">Guillermo Morales</a>, 
<a href="/search/cs?searchtype=author&query=Morris%2C+A">Andrew Morris</a>, 
<a href="/search/cs?searchtype=author&query=Mullen%2C+J">Julie Mullen</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+R">Ritesh Patel</a>, 
<a href="/search/cs?searchtype=author&query=Pentland%2C+A">Alex Pentland</a>, 
<a href="/search/cs?searchtype=author&query=Pisharody%2C+S">Sandeep Pisharody</a>, 
<a href="/search/cs?searchtype=author&query=Prout%2C+A">Andrew Prout</a>, 
<a href="/search/cs?searchtype=author&query=Reuther%2C+A">Albert Reuther</a>, 
<a href="/search/cs?searchtype=author&query=Rosa%2C+A">Antonio Rosa</a>, 
<a href="/search/cs?searchtype=author&query=Samsi%2C+S">Siddharth Samsi</a>, 
<a href="/search/cs?searchtype=author&query=Trigg%2C+T">Tyler Trigg</a>, 
<a href="/search/cs?searchtype=author&query=Wachman%2C+G">Gabriel Wachman</a>, 
<a href="/search/cs?searchtype=author&query=Yee%2C+C">Charles Yee</a>, 
<a href="/search/cs?searchtype=author&query=Michaleas%2C+P">Peter Michaleas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 figures, IEEE HPEC 2023 (accepted)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Expanding the scientific tools available to protect computer networks can be
aided by a deeper understanding of the underlying statistical distributions of
network traffic and their potential geometric interpretations. Analyses of
large scale network observations provide a unique window into studying those
underlying statistics. Newly developed GraphBLAS hypersparse matrices and D4M
associative array technologies enable the efficient anonymized analysis of
network traffic on the scale of trillions of events. This work analyzes over
100,000,000,000 anonymized packets from the largest Internet telescope (CAIDA)
and over 10,000,000 anonymized sources from the largest commercial honeyfarm
(GreyNoise). Neither CAIDA nor GreyNoise actively emit Internet traffic and
provide distinct observations of unsolicited Internet traffic (primarily
botnets and scanners). Analysis of these observations confirms the previously
observed Cauchy-like distributions describing temporal correlations between
Internet sources. The Gull lighthouse problem is a well-known geometric
characterization of the standard Cauchy distribution and motivates a potential
geometric interpretation for Internet observations. This work generalizes the
Gull lighthouse problem to accommodate larger classes of coastlines, deriving a
closed-form solution for the resulting probability distributions, stating and
examining the inverse problem of identifying an appropriate coastline given a
continuous probability distribution, identifying a geometric heuristic for
solving this problem computationally, and applying that heuristic to examine
the temporal geometry of different subsets of network observations. Application
of this method to the CAIDA and GreyNoise data reveals a several orders of
magnitude difference between known benign and other traffic which can lead to
potentially novel ways to protect networks.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00525" title="Abstract">arXiv:2310.00525</a> [<a href="/pdf/2310.00525" title="Download PDF">pdf</a>, <a href="/format/2310.00525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement learning adaptive fuzzy controller for lighting systems:  application to aircraft cabin
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vashishtha%2C+K">Kritika Vashishtha</a>, 
<a href="/search/cs?searchtype=author&query=Saad%2C+A">Anas Saad</a>, 
<a href="/search/cs?searchtype=author&query=Faieghi%2C+R">Reza Faieghi</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+F">Fengfeng Xi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The lighting requirements are subjective and one light setting cannot work
for all. However, there is little work on developing smart lighting algorithms
that can adapt to user preferences. To address this gap, this paper uses fuzzy
logic and reinforcement learning to develop an adaptive lighting algorithm. In
particular, we develop a baseline fuzzy inference system (FIS) using the domain
knowledge. We use the existing literature to create a FIS that generates
lighting setting recommendations based on environmental conditions i.e. daily
glare index, and user information including age, activity, and chronotype.
Through a feedback mechanism, the user interacts with the algorithm, correcting
the algorithm output to their preferences. We interpret these corrections as
rewards to a Q-learning agent, which tunes the FIS parameters online to match
the user preferences. We implement the algorithm in an aircraft cabin mockup
and conduct an extensive user study to evaluate the effectiveness of the
algorithm and understand its learning behavior. Our implementation results
demonstrate that the developed algorithm possesses the capability to learn user
preferences while successfully adapting to a wide range of environmental
conditions and user characteristics. and can deal with a diverse spectrum of
environmental conditions and user characteristics. This underscores its
viability as a potent solution for intelligent light management, featuring
advanced learning capabilities.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00526" title="Abstract">arXiv:2310.00526</a> [<a href="/pdf/2310.00526" title="Download PDF">pdf</a>, <a href="/format/2310.00526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Graph Neural Networks Optimal Approximation Algorithms?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yau%2C+M">Morris Yau</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+E">Eric Lu</a>, 
<a href="/search/cs?searchtype=author&query=Karalias%2C+N">Nikolaos Karalias</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jessica Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jegelka%2C+S">Stefanie Jegelka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">In this work we design graph neural network architectures that can be used to
obtain optimal approximation algorithms for a large class of combinatorial
optimization problems using powerful algorithmic tools from semidefinite
programming (SDP). Concretely, we prove that polynomial-sized message passing
algorithms can represent the most powerful polynomial time algorithms for Max
Constraint Satisfaction Problems assuming the Unique Games Conjecture. We
leverage this result to construct efficient graph neural network architectures,
OptGNN, that obtain high-quality approximate solutions on landmark
combinatorial optimization problems such as Max Cut and maximum independent
set. Our approach achieves strong empirical results across a wide range of
real-world and synthetic datasets against both neural baselines and classical
algorithms. Finally, we take advantage of OptGNN's ability to capture convex
relaxations to design an algorithm for producing dual certificates of
optimality (bounds on the optimal solution) from the learned embeddings of
OptGNN.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00527" title="Abstract">arXiv:2310.00527</a> [<a href="/pdf/2310.00527" title="Download PDF">pdf</a>, <a href="/format/2310.00527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised Learning of Contextualized Local Visual Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silva%2C+T+S">Thalles Santos Silva</a>, 
<a href="/search/cs?searchtype=author&query=Pedrini%2C+H">Helio Pedrini</a>, 
<a href="/search/cs?searchtype=author&query=Rivera%2C+A+R">Ad&#xed;n Ram&#xed;rez Rivera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print. 4th Visual Inductive Priors for Data-Efficient Deep Learning Workshop ICCV 2023. Code at $\href{<a href="https://github.com/sthalles/CLoVE">this https URL</a>}{\text{this link}}$
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 4th Visual Inductive Priors for Data-Efficient Deep Learning
  Workshop ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present Contextualized Local Visual Embeddings (CLoVE), a self-supervised
convolutional-based method that learns representations suited for dense
prediction tasks. CLoVE deviates from current methods and optimizes a single
loss function that operates at the level of contextualized local embeddings
learned from output feature maps of convolution neural network (CNN) encoders.
To learn contextualized embeddings, CLoVE proposes a normalized mult-head
self-attention layer that combines local features from different parts of an
image based on similarity. We extensively benchmark CLoVE's pre-trained
representations on multiple datasets. CLoVE reaches state-of-the-art
performance for CNN-based architectures in 4 dense prediction downstream tasks,
including object detection, instance segmentation, keypoint detection, and
dense pose estimation. Code:
$\href{https://github.com/sthalles/CLoVE}{https://github.com/sthalles/CLoVE}$.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00530" title="Abstract">arXiv:2310.00530</a> [<a href="/pdf/2310.00530" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Neural Radiance Fields (NeRF) for Large-scale Aerial Images --  A Multi-tiling Approaching and the Geometry Assessment of NeRF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Ningli Xu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+R">Rongjun Qin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Debao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Remondino%2C+F">Fabio Remondino</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 Figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Neural Radiance Fields (NeRF) offer the potential to benefit 3D
reconstruction tasks, including aerial photogrammetry. However, the scalability
and accuracy of the inferred geometry are not well-documented for large-scale
aerial assets,since such datasets usually result in very high memory
consumption and slow convergence.. In this paper, we aim to scale the NeRF on
large-scael aerial datasets and provide a thorough geometry assessment of NeRF.
Specifically, we introduce a location-specific sampling technique as well as a
multi-camera tiling (MCT) strategy to reduce memory consumption during image
loading for RAM, representation training for GPU memory, and increase the
convergence rate within tiles. MCT decomposes a large-frame image into multiple
tiled images with different camera models, allowing these small-frame images to
be fed into the training process as needed for specific locations without a
loss of accuracy. We implement our method on a representative approach,
Mip-NeRF, and compare its geometry performance with threephotgrammetric MVS
pipelines on two typical aerial datasets against LiDAR reference data. Both
qualitative and quantitative results suggest that the proposed NeRF approach
produces better completeness and object details than traditional approaches,
although as of now, it still falls short in terms of accuracy.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00533" title="Abstract">arXiv:2310.00533</a> [<a href="/pdf/2310.00533" title="Download PDF">pdf</a>, <a href="/format/2310.00533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SELF: Language-Driven Self-Evolution for Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jianqiao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+W">Wanjun Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenyong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yufei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+F">Fei Mi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baojun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, 6 tables. Due to the limitation "The abstract field cannot be longer than 1,920 characters", the abstract appearing here is slightly shorter than that in the PDF file
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have showcased remarkable versatility across
diverse domains. However, the pathway toward autonomous model development, a
cornerstone for achieving human-level learning and advancing autonomous AI,
remains largely uncharted. We introduce an innovative approach, termed "SELF"
(Self-Evolution with Language Feedback). This methodology empowers LLMs to
undergo continual self-evolution. Furthermore, SELF employs language-based
feedback as a versatile and comprehensive evaluative tool, pinpointing areas
for response refinement and bolstering the stability of self-evolutionary
training. Initiating with meta-skill learning, SELF acquires foundational
meta-skills with a focus on self-feedback and self-refinement. These
meta-skills are critical, guiding the model's subsequent self-evolution through
a cycle of perpetual training with self-curated data, thereby enhancing its
intrinsic abilities. Given unlabeled instructions, SELF equips the model with
the capability to autonomously generate and interactively refine responses.
This synthesized training data is subsequently filtered and utilized for
iterative fine-tuning, enhancing the model's capabilities. Experimental results
on representative benchmarks substantiate that SELF can progressively advance
its inherent abilities without the requirement of human intervention, thereby
indicating a viable pathway for autonomous model evolution. Additionally, SELF
can employ online self-refinement strategy to produce responses of superior
quality. In essence, the SELF framework signifies a progressive step towards
autonomous LLM development, transforming the LLM from a mere passive recipient
of information into an active participant in its own evolution.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00534" title="Abstract">arXiv:2310.00534</a> [<a href="/pdf/2310.00534" title="Download PDF">pdf</a>, <a href="/format/2310.00534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe Optimal Interactions Between Automated and Human-Driven Vehicles in  Mixed Traffic with Event-triggered Control Barrier Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+A">Anni Li</a>, 
<a href="/search/eess?searchtype=author&query=Cassandras%2C+C+G">Christos G. Cassandras</a>, 
<a href="/search/eess?searchtype=author&query=Xiao%2C+W">Wei Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper studies safe driving interactions between Human-Driven Vehicles
(HDVs) and Connected and Automated Vehicles (CAVs) in mixed traffic where the
dynamics and control policies of HDVs are unknown and hard to predict. In order
to address this challenge, we employ event-triggered Control Barrier Functions
(CBFs) to estimate the HDV model online, construct data-driven and
state-feedback safety controllers, and transform constrained optimal control
problems for CAVs into a sequence of event-triggered quadratic programs. We
show that we can ensure collision-free between HDVs and CAVs and demonstrate
the robustness and flexibility of our framework on different types of human
drivers in lane-changing scenarios while guaranteeing safety with
human-in-the-loop interactions.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00535" title="Abstract">arXiv:2310.00535</a> [<a href="/pdf/2310.00535" title="Download PDF">pdf</a>, <a href="/format/2310.00535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and  Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuandong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Beidi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S">Simon Du</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical
framework to understand the training procedure of multilayer Transformer
architectures. This is achieved by integrating out the self-attention layer in
Transformers, producing a modified dynamics of MLP layers only. JoMA removes
unrealistic assumptions in previous analysis (e.g., lack of residual
connection), and predicts that the attention first becomes sparse (to learn
salient tokens), then dense (to learn less salient tokens) in the presence of
nonlinear activations, while in the linear case, it is consistent with existing
works. We leverage JoMA to qualitatively explains how tokens are combined to
form hierarchies in multilayer Transformers, when the input tokens are
generated by a latent hierarchical generative model. Experiments on models
trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained
models (OPT, Pythia) verify our theoretical findings.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00540" title="Abstract">arXiv:2310.00540</a> [<a href="/pdf/2310.00540" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning-driven Analysis of Gastrointestinal Symptoms in  Post-COVID-19 Patients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yousif%2C+M+G">Maitham G. Yousif</a>, 
<a href="/search/cs?searchtype=author&query=Al-Amran%2C+F+G">Fadhil G. Al-Amran</a>, 
<a href="/search/cs?searchtype=author&query=Rawaf%2C+S">Salman Rawaf</a>, 
<a href="/search/cs?searchtype=author&query=Grmt%2C+M+A">Mohammad Abdulla Grmt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2309.16736">arXiv:2309.16736</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The COVID-19 pandemic, caused by the novel coronavirus SARS-CoV-2, has posed
significant health challenges worldwide. While respiratory symptoms have been
the primary focus, emerging evidence has highlighted the impact of COVID-19 on
various organ systems, including the gastrointestinal (GI) tract. This study,
based on data from 913 post-COVID-19 patients in Iraq collected during 2022 and
2023, investigates the prevalence and patterns of GI symptoms in individuals
recovering from COVID-19 and leverages machine learning algorithms to identify
predictive factors for these symptoms. The research findings reveal that a
notable percentage of post-COVID-19 patients experience GI symptoms during
their recovery phase. Diarrhea emerged as the most frequently reported symptom,
followed by abdominal pain and nausea. Machine learning analysis uncovered
significant predictive factors for GI symptoms, including age, gender, disease
severity, comorbidities, and the duration of COVID-19 illness. These findings
underscore the importance of monitoring and addressing GI symptoms in
post-COVID-19 care, with machine learning offering valuable tools for early
identification and personalized intervention. This study contributes to the
understanding of the long-term consequences of COVID-19 on GI health and
emphasizes the potential benefits of utilizing machine learning-driven analysis
in predicting and managing these symptoms. Further research is warranted to
delve into the mechanisms underlying GI symptoms in COVID-19 survivors and to
develop targeted interventions for symptom management. Keywords: COVID-19,
gastrointestinal symptoms, machine learning, predictive factors, post-COVID-19
care, long COVID.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00542" title="Abstract">arXiv:2310.00542</a> [<a href="/pdf/2310.00542" title="Download PDF">pdf</a>, <a href="/format/2310.00542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Horizontal Class Backdoor to Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Hua Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yansong Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">All existing backdoor attacks to deep learning (DL) models belong to the
vertical class backdoor (VCB). That is, any sample from a class will activate
the implanted backdoor in the presence of the secret trigger, regardless of
source-class-agnostic or source-class-specific backdoor. Current trends of
existing defenses are overwhelmingly devised for VCB attacks especially the
source-class-agnostic backdoor, which essentially neglects other potential
simple but general backdoor types, thus giving false security implications. It
is thus urgent to discover unknown backdoor types.
<br />This work reveals a new, simple, and general horizontal class backdoor (HCB)
attack. We show that the backdoor can be naturally bounded with innocuous
natural features that are common and pervasive in the real world. Note that an
innocuous feature (e.g., expression) is irrelevant to the main task of the
model (e.g., recognizing a person from one to another). The innocuous feature
spans across classes horizontally but is exhibited by partial samples per class
-- satisfying the horizontal class (HC) property. Only when the trigger is
concurrently presented with the HC innocuous feature, can the backdoor be
effectively activated. Extensive experiments on attacking performance in terms
of high attack success rates with tasks of 1) MNIST, 2) facial recognition, 3)
traffic sign recognition, and 4) object detection demonstrate that the HCB is
highly efficient and effective. We extensively evaluate the HCB evasiveness
against a (chronologically) series of 9 influential countermeasures of
Fine-Pruning (RAID 18'), STRIP (ACSAC 19'), Neural Cleanse (Oakland 19'), ABS
(CCS 19'), Februus (ACSAC 20'), MNTD (Oakland 21'), SCAn (USENIX SEC 21'), MOTH
(Oakland 22'), and Beatrix (NDSS 23'), where none of them can succeed even when
a simplest trigger is used.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00544" title="Abstract">arXiv:2310.00544</a> [<a href="/pdf/2310.00544" title="Download PDF">pdf</a>, <a href="/format/2310.00544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving stationary nonlinear Fokker-Planck equations via sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/math?searchtype=author&query=Tang%2C+Y">Yijia Tang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+J">Jingtong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Solving the stationary nonlinear Fokker-Planck equations is important in
applications and examples include the Poisson-Boltzmann equation and the two
layer neural networks. Making use of the connection between the interacting
particle systems and the nonlinear Fokker-Planck equations, we propose to solve
the stationary solution by sampling from the $N$-body Gibbs distribution. This
avoids simulation of the $N$-body system for long time and more importantly
such a method can avoid the requirement of uniform propagation of chaos from
direct simulation of the particle systems. We establish the convergence of the
Gibbs measure to the stationary solution when the interaction kernel is bounded
(not necessarily continuous) and the temperature is not very small. Numerical
experiments are performed for the Poisson-Boltzmann equations and the two-layer
neural networks to validate the method and the theory.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00546" title="Abstract">arXiv:2310.00546</a> [<a href="/pdf/2310.00546" title="Download PDF">pdf</a>, <a href="/format/2310.00546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seal2Real: Prompt Prior Learning on Diffusion Model for Unsupervised  Document Seal Data Generation and Realisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiancheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yifan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shifeng Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In document processing, seal-related tasks have very large commercial
applications, such as seal segmentation, seal authenticity discrimination, seal
removal, and text recognition under seals. However, these seal-related tasks
are highly dependent on labelled document seal datasets, resulting in very
little work on these tasks. To address the lack of labelled datasets for these
seal-related tasks, we propose Seal2Real, a generative method that generates a
large amount of labelled document seal data, and construct a Seal-DB dataset
containing 20K images with labels. In Seal2Real, we propose a prompt prior
learning architecture based on a pre-trained Stable Diffusion Model that
migrates the prior generative power of to our seal generation task with
unsupervised training. The realistic seal generation capability greatly
facilitates the performance of downstream seal-related tasks on real data.
Experimental results on the Seal-DB dataset demonstrate the effectiveness of
Seal2Real.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00551" title="Abstract">arXiv:2310.00551</a> [<a href="/pdf/2310.00551" title="Download PDF">pdf</a>, <a href="/format/2310.00551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Derivative based global sensitivity analysis and its entropic link
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yang%2C+J">Jiannan Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 page, 3 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Probability (math.PR); Computation (stat.CO)

</div>
<p class="mathjax">Distribution-based global sensitivity analysis (GSA), such as variance-based
and entropy-based approaches, can provide quantitative sensitivity information.
However, they can be expensive to evaluate and are thus limited to low
dimensional problems. Derivative-based GSA, on the other hand, require much
fewer model evaluations. It is known that derivative-based GSA is closely
linked to variance-based total sensitivity index, while its relationship with
the entropy-based measure is unclear. To fill this gap, we introduce a
log-derivative based functional to demonstrate that the entropy-based and
derivative-based sensitivity measures are strongly connected. In particular, we
give proofs that, similar to the case with variance-based GSA, there is an
inequality relationship between entropy-based and derivative-based important
measures. Both analytical and numerical verifications are provided. Examples
show that the derivative-based methods give similar variable rankings as
entropy-based index and can thus be potentially used as a proxy for both
variance-based and entropy-based distribution-type GSA.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00552" title="Abstract">arXiv:2310.00552</a> [<a href="/pdf/2310.00552" title="Download PDF">pdf</a>, <a href="/format/2310.00552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Siamese Representation Learning for Unsupervised Relation Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guangxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26th European Conference on Artificial Intelligence ECAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Unsupervised relation extraction (URE) aims at discovering underlying
relations between named entity pairs from open-domain plain text without prior
information on relational distribution. Existing URE models utilizing
contrastive learning, which attract positive samples and repulse negative
samples to promote better separation, have got decent effect. However,
fine-grained relational semantic in relationship makes spurious negative
samples, damaging the inherent hierarchical structure and hindering
performances. To tackle this problem, we propose Siamese Representation
Learning for Unsupervised Relation Extraction -- a novel framework to simply
leverage positive pairs to representation learning, possessing the capability
to effectively optimize relation representation of instances and retain
hierarchical information in relational feature space. Experimental results show
that our model significantly advances the state-of-the-art results on two
benchmark datasets and detailed analyses demonstrate the effectiveness and
robustness of our proposed model on unsupervised relation extraction.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00558" title="Abstract">arXiv:2310.00558</a> [<a href="/pdf/2310.00558" title="Download PDF">pdf</a>, <a href="/format/2310.00558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diving into the Depths of Spotting Text in Multi-Domain Noisy Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Alloy Das</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+S">Sanket Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+U">Umapada Pal</a>, 
<a href="/search/cs?searchtype=author&query=Llad%C3%B3s%2C+J">Josep Llad&#xf3;s</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">When used in a real-world noisy environment, the capacity to generalize to
multiple domains is essential for any autonomous scene text spotting system.
However, existing state-of-the-art methods employ pretraining and fine-tuning
strategies on natural scene datasets, which do not exploit the feature
interaction across other complex domains. In this work, we explore and
investigate the problem of domain-agnostic scene text spotting, i.e., training
a model on multi-domain source data such that it can directly generalize to
target domains rather than being specialized for a specific domain or scenario.
In this regard, we present the community a text spotting validation benchmark
called Under-Water Text (UWT) for noisy underwater scenes to establish an
important case study. Moreover, we also design an efficient super-resolution
based end-to-end transformer baseline called DA-TextSpotter which achieves
comparable or superior performance over existing text spotting architectures
for both regular and arbitrary-shaped scene text spotting benchmarks in terms
of both accuracy and model efficiency. The dataset, code and pre-trained models
will be released upon acceptance.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00559" title="Abstract">arXiv:2310.00559</a> [<a href="/pdf/2310.00559" title="Download PDF">pdf</a>, <a href="/format/2310.00559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CPIPS: Learning to Preserve Perceptual Distances in End-to-End Image  Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chen-Hsiu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Ja-Ling Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures; accepted by APSIPA ASC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Lossy image coding standards such as JPEG and MPEG have successfully achieved
high compression rates for human consumption of multimedia data. However, with
the increasing prevalence of IoT devices, drones, and self-driving cars,
machines rather than humans are processing a greater portion of captured visual
content. Consequently, it is crucial to pursue an efficient compressed
representation that caters not only to human vision but also to image
processing and machine vision tasks. Drawing inspiration from the efficient
coding hypothesis in biological systems and the modeling of the sensory cortex
in neural science, we repurpose the compressed latent representation to
prioritize semantic relevance while preserving perceptual distance. Our
proposed method, Compressed Perceptual Image Patch Similarity (CPIPS), can be
derived at a minimal cost from a learned neural codec and computed
significantly faster than DNN-based perceptual metrics such as LPIPS and DISTS.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00560" title="Abstract">arXiv:2310.00560</a> [<a href="/pdf/2310.00560" title="Download PDF">pdf</a>, <a href="/format/2310.00560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Task Scheduling and Container Image Caching in Edge Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mou%2C+F">Fangyi Mou</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zhiqing Tang</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">Jiong Lou</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jianxiong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhua Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tian Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">In Edge Computing (EC), containers have been increasingly used to deploy
applications to provide mobile users services. Each container must run based on
a container image file that exists locally. However, it has been conspicuously
neglected by existing work that effective task scheduling combined with dynamic
container image caching is a promising way to reduce the container image
download time with the limited bandwidth resource of edge nodes. To fill in
such gaps, in this paper, we propose novel joint Task Scheduling and Image
Caching (TSIC) algorithms, specifically: 1) We consider the joint task
scheduling and image caching problem and formulate it as a Markov Decision
Process (MDP), taking the communication delay, waiting delay, and computation
delay into consideration; 2) To solve the MDP problem, a TSIC algorithm based
on deep reinforcement learning is proposed with the customized state and action
spaces and combined with an adaptive caching update algorithm. 3) A real
container system is implemented to validate our algorithms. The experiments
show that our strategy outperforms the existing baseline approaches by 23\% and
35\% on average in terms of total delay and waiting delay, respectively.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00564" title="Abstract">arXiv:2310.00564</a> [<a href="/pdf/2310.00564" title="Download PDF">pdf</a>, <a href="/format/2310.00564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous  spiking neural network processor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Richter%2C+O">Ole Richter</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chenxi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Whatley%2C+A+M">Adrian M. Whatley</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6stinger%2C+G">German K&#xf6;stinger</a>, 
<a href="/search/cs?searchtype=author&query=Nielsen%2C+C">Carsten Nielsen</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+N">Ning Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Indiveri%2C+G">Giacomo Indiveri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> *Ole Richter and Chenxi Wu contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the remarkable progress that technology has made, the need for
processing data near the sensors at the edge has increased dramatically. The
electronic systems used in these applications must process data continuously,
in real-time, and extract relevant information using the smallest possible
energy budgets. A promising approach for implementing always-on processing of
sensory signals that supports on-demand, sparse, and edge-computing is to take
inspiration from biological nervous system. Following this approach, we present
a brain-inspired platform for prototyping real-time event-based Spiking Neural
Networks (SNNs). The system proposed supports the direct emulation of dynamic
and realistic neural processing phenomena such as short-term plasticity, NMDA
gating, AMPA diffusion, homeostasis, spike frequency adaptation,
conductance-based dendritic compartments and spike transmission delays. The
analog circuits that implement such primitives are paired with a low latency
asynchronous digital circuits for routing and mapping events. This asynchronous
infrastructure enables the definition of different network architectures, and
provides direct event-based interfaces to convert and encode data from
event-based and continuous-signal sensors. Here we describe the overall system
architecture, we characterize the mixed signal analog-digital circuits that
emulate neural dynamics, demonstrate their features with experimental
measurements, and present a low- and high-level software ecosystem that can be
used for configuring the system. The flexibility to emulate different
biologically plausible neural networks, and the chip's ability to monitor both
population and single neuron signals in real-time, allow to develop and
validate complex models of neural processing for both basic research and
edge-computing applications.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00566" title="Abstract">arXiv:2310.00566</a> [<a href="/pdf/2310.00566" title="Download PDF">pdf</a>, <a href="/format/2310.00566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empowering Many, Biasing a Few: Generalist Credit Scoring through Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+D">Duanyu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yongfu Dai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jimin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Weiguang Han</a>, 
<a href="/search/cs?searchtype=author&query=Lopez-Lira%2C+A">Alejandro Lopez-Lira</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
<p class="mathjax">Credit and risk assessments are cornerstones of the financial landscape,
impacting both individual futures and broader societal constructs. Existing
credit scoring models often exhibit limitations stemming from knowledge myopia
and task isolation. In response, we formulate three hypotheses and undertake an
extensive case study to investigate LLMs' viability in credit assessment. Our
empirical investigations unveil LLMs' ability to overcome the limitations
inherent in conventional models. We introduce a novel benchmark curated for
credit assessment purposes, fine-tune a specialized Credit and Risk Assessment
Large Language Model (CALM), and rigorously examine the biases that LLMs may
harbor. Our findings underscore LLMs' potential in revolutionizing credit
assessment, showcasing their adaptability across diverse financial evaluations,
and emphasizing the critical importance of impartial decision-making in the
financial sector. Our datasets, models, and benchmarks are open-sourced for
other researchers.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00567" title="Abstract">arXiv:2310.00567</a> [<a href="/pdf/2310.00567" title="Download PDF">pdf</a>, <a href="/format/2310.00567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Robustness of Randomized Feature Defense Against  Query-Based Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Q+H">Quang H. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Lao%2C+Y">Yingjie Lao</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+T">Tung Pham</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kok-Seng Wong</a>, 
<a href="/search/cs?searchtype=author&query=Doan%2C+K+D">Khoa D. Doan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recent works have shown that deep neural networks are vulnerable to
adversarial examples that find samples close to the original image but can make
the model misclassify. Even with access only to the model's output, an attacker
can employ black-box attacks to generate such adversarial examples. In this
work, we propose a simple and lightweight defense against black-box attacks by
adding random noise to hidden features at intermediate layers of the model at
inference time. Our theoretical analysis confirms that this method effectively
enhances the model's resilience against both score-based and decision-based
black-box attacks. Importantly, our defense does not necessitate adversarial
training and has minimal impact on accuracy, rendering it applicable to any
pre-trained model. Our analysis also reveals the significance of selectively
adding noise to different parts of the model based on the gradient of the
adversarial objective function, which can be varied during the attack. We
demonstrate the robustness of our defense against multiple black-box attacks
through extensive empirical experiments involving diverse models with various
architectures.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00568" title="Abstract">arXiv:2310.00568</a> [<a href="/pdf/2310.00568" title="Download PDF">pdf</a>, <a href="/format/2310.00568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Data Hiding in Neural Compressed Latent Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chen-Hsiu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Ja-Ling Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figure; accepted by VCIP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We propose an end-to-end learned image data hiding framework that embeds and
extracts secrets in the latent representations of a generic neural compressor.
By leveraging a perceptual loss function in conjunction with our proposed
message encoder and decoder, our approach simultaneously achieves high image
quality and high bit accuracy. Compared to existing techniques, our framework
offers superior image secrecy and competitive watermarking robustness in the
compressed domain while accelerating the embedding speed by over 50 times.
These results demonstrate the potential of combining data hiding techniques and
neural compression and offer new insights into developing neural compression
techniques and their applications.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00569" title="Abstract">arXiv:2310.00569</a> [<a href="/pdf/2310.00569" title="Download PDF">pdf</a>, <a href="/format/2310.00569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TDCGL: Two-Level Debiased Contrastive Graph Learning for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yubo Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haotian Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">knowledge graph-based recommendation methods have achieved great success in
the field of recommender systems. However, over-reliance on high-quality
knowledge graphs is a bottleneck for such methods. Specifically, the
long-tailed distribution of entities of KG and noise issues in the real world
will make item-entity dependent relations deviate from reflecting true
characteristics and significantly harm the performance of modeling user
preference. Contrastive learning, as a novel method that is employed for data
augmentation and denoising, provides inspiration to fill this research gap.
However, the mainstream work only focuses on the long-tail properties of the
number of items clicked, while ignoring that the long-tail properties of total
number of clicks per user may also affect the performance of the recommendation
model. Therefore, to tackle these problems, motivated by the Debiased
Contrastive Learning of Unsupervised Sentence Representations (DCLR), we
propose Two-Level Debiased Contrastive Graph Learning (TDCGL) model.
Specifically, we design the Two-Level Debiased Contrastive Learning (TDCL) and
deploy it in the KG, which is conducted not only on User-Item pairs but also on
User-User pairs for modeling higher-order relations. Also, to reduce the bias
caused by random sampling in contrastive learning, with the exception of the
negative samples obtained by random sampling, we add a noise-based generation
of negation to ensure spatial uniformity. Considerable experiments on
open-source datasets demonstrate that our method has excellent anti-noise
capability and significantly outperforms state-of-the-art baselines. In
addition, ablation studies about the necessity for each level of TDCL are
conducted.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00570" title="Abstract">arXiv:2310.00570</a> [<a href="/pdf/2310.00570" title="Download PDF">pdf</a>, <a href="/format/2310.00570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LaPLACE: Probabilistic Local Model-Agnostic Causal Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Minn%2C+S">Sein Minn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Machine learning models have undeniably achieved impressive performance
across a range of applications. However, their often perceived black-box
nature, and lack of transparency in decision-making, have raised concerns about
understanding their predictions. To tackle this challenge, researchers have
developed methods to provide explanations for machine learning models. In this
paper, we introduce LaPLACE-explainer, designed to provide probabilistic
cause-and-effect explanations for any classifier operating on tabular data, in
a human-understandable manner. The LaPLACE-Explainer component leverages the
concept of a Markov blanket to establish statistical boundaries between
relevant and non-relevant features automatically. This approach results in the
automatic generation of optimal feature subsets, serving as explanations for
predictions. Importantly, this eliminates the need to predetermine a fixed
number N of top features as explanations, enhancing the flexibility and
adaptability of our methodology. Through the incorporation of conditional
probabilities, our approach offers probabilistic causal explanations and
outperforms LIME and SHAP (well-known model-agnostic explainers) in terms of
local accuracy and consistency of explained features. LaPLACE's soundness,
consistency, local accuracy, and adaptability are rigorously validated across
various classification models. Furthermore, we demonstrate the practical
utility of these explanations via experiments with both simulated and
real-world datasets. This encompasses addressing trust-related issues, such as
evaluating prediction reliability, facilitating model selection, enhancing
trustworthiness, and identifying fairness-related concerns within classifiers.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00571" title="Abstract">arXiv:2310.00571</a> [<a href="/pdf/2310.00571" title="Download PDF">pdf</a>, <a href="/format/2310.00571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deriving Loss Function for Value-oriented Renewable Energy Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yufan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wen%2C+H">Honglin Wen</a>, 
<a href="/search/eess?searchtype=author&query=Bian%2C+Y">Yuexin Bian</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+Y">Yuanyuan Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to PSCC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Renewable energy forecasting is the workhorse for efficient energy dispatch.
However, forecasts with small mean squared errors (MSE) may not necessarily
lead to low operation costs. Here, we propose a forecasting approach
specifically tailored for operational purposes, by incorporating operational
problems into the estimation of forecast models via designing a loss function.
We formulate a bilevel program, where the operation problem is at the lower
level, and the forecast model estimation is at the upper level. We establish
the relationship between the lower-level optimal solutions and forecasts
through multiparametric programming. By integrating it into the upper-level
objective for minimizing expected operation cost, we convert the bilevel
problem to a single-level one and derive the loss function for training the
model. It is proved to be piecewise linear, for linear operation problem.
Compared to the commonly used loss functions, e.g. MSE, our approach achieves
lower operation costs.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00572" title="Abstract">arXiv:2310.00572</a> [<a href="/pdf/2310.00572" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Colloquial Persian POS (CPPOS) Corpus: A Novel Corpus for Colloquial  Persian Part of Speech Tagging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rabiei%2C+L">Leyla Rabiei</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+F">Farzaneh Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Khansari%2C+M">Mohammad Khansari</a>, 
<a href="/search/cs?searchtype=author&query=Rajabi%2C+Z">Zeinab Rajabi</a>, 
<a href="/search/cs?searchtype=author&query=Salimi%2C+M">Moein Salimi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Introduction: Part-of-Speech (POS) Tagging, the process of classifying words
into their respective parts of speech (e.g., verb or noun), is essential in
various natural language processing applications. POS tagging is a crucial
preprocessing task for applications like machine translation, question
answering, sentiment analysis, etc. However, existing corpora for POS tagging
in Persian mainly consist of formal texts, such as daily news and newspapers.
As a result, smart POS tools, machine learning models, and deep learning models
trained on these corpora may not perform optimally for processing colloquial
text in social network analysis. Method: This paper introduces a novel corpus,
"Colloquial Persian POS" (CPPOS), specifically designed to support colloquial
Persian text. The corpus includes formal and informal text collected from
various domains such as political, social, and commercial on Telegram, Twitter,
and Instagram more than 520K labeled tokens. After collecting posts from these
social platforms for one year, special preprocessing steps were conducted,
including normalization, sentence tokenizing, and word tokenizing for social
text. The tokens and sentences were then manually annotated and verified by a
team of linguistic experts. This study also defines a POS tagging guideline for
annotating the data and conducting the annotation process. Results: To evaluate
the quality of CPPOS, various deep learning models, such as the RNN family,
were trained using the constructed corpus. A comparison with another well-known
Persian POS corpus named "Bijankhan" and the Persian Hazm POS tool trained on
Bijankhan revealed that our model trained on CPPOS outperforms them. With the
new corpus and the BiLSTM deep neural model, we achieved a 14% improvement over
the previous dataset.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00574" title="Abstract">arXiv:2310.00574</a> [<a href="/pdf/2310.00574" title="Download PDF">pdf</a>, <a href="/format/2310.00574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SIMD Dataflow Co-optimization for Efficient Neural Networks Inferences  on CPUs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Cyrus Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hassman%2C+Z">Zack Hassman</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruize Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+D">Dhirpal Shah</a>, 
<a href="/search/cs?searchtype=author&query=Richard%2C+V">Vaugnn Richard</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanjing Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Machine Learning (cs.LG); Performance (cs.PF)

</div>
<p class="mathjax">We address the challenges associated with deploying neural networks on CPUs,
with a particular focus on minimizing inference time while maintaining
accuracy. Our novel approach is to use the dataflow (i.e., computation order)
of a neural network to explore data reuse opportunities using heuristic-guided
analysis and a code generation framework, which enables exploration of various
Single Instruction, Multiple Data (SIMD) implementations to achieve optimized
neural network execution. Our results demonstrate that the dataflow that keeps
outputs in SIMD registers while also maximizing both input and weight reuse
consistently yields the best performance for a wide variety of inference
workloads, achieving up to 3x speedup for 8-bit neural networks, and up to 4.8x
speedup for binary neural networks, respectively, over the optimized
implementations of neural networks today.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00576" title="Abstract">arXiv:2310.00576</a> [<a href="/pdf/2310.00576" title="Download PDF">pdf</a>, <a href="/format/2310.00576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GrowLength: Accelerating LLMs Pretraining by Progressively Growing  Training Length
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Hongye Jin</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiaotian Han</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingfeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhimeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+C">Chia-Yuan Chang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xia Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The evolving sophistication and intricacies of Large Language Models (LLMs)
yield unprecedented advancements, yet they simultaneously demand considerable
computational resources and incur significant costs. To alleviate these
challenges, this paper introduces a novel, simple, and effective method named
``\growlength'' to accelerate the pretraining process of LLMs. Our method
progressively increases the training length throughout the pretraining phase,
thereby mitigating computational costs and enhancing efficiency. For instance,
it begins with a sequence length of 128 and progressively extends to 4096. This
approach enables models to process a larger number of tokens within limited
time frames, potentially boosting their performance. In other words, the
efficiency gain is derived from training with shorter sequences optimizing the
utilization of resources. Our extensive experiments with various
state-of-the-art LLMs have revealed that models trained using our method not
only converge more swiftly but also exhibit superior performance metrics
compared to those trained with existing methods. Furthermore, our method for
LLMs pretraining acceleration does not require any additional engineering
efforts, making it a practical solution in the realm of LLMs.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00578" title="Abstract">arXiv:2310.00578</a> [<a href="/pdf/2310.00578" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nine-year-old children outperformed ChatGPT in emotion: Evidence from  Chinese writing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+S">Siyi Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tongquan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Siruo Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">ChatGPT has been demonstrated to possess significant capabilities in
generating intricate, human-like text, and recent studies have established that
its performance in theory of mind tasks is comparable to that of a
nine-year-old child. However, it remains uncertain whether ChatGPT surpasses
nine-year-old children in Chinese writing proficiency. To explore this, our
study juxtaposed the Chinese writing performance of ChatGPT and nine-year-old
children on both narrative and scientific topics, aiming to uncover the
relative strengths and weaknesses of ChatGPT in writing.
<br />The collected data were analyzed across five linguistic dimensions: fluency,
accuracy, complexity, cohesion, and emotion. Each dimension underwent
assessment through precise indices. The findings revealed that nine-year-old
children excelled beyond ChatGPT in terms of fluency and cohesion within their
writing. In contrast, ChatGPT manifested a superior performance in accuracy
compared to the children. Concerning complexity, children exhibited superior
skills in science-themed writing, while ChatGPT prevailed in nature-themed
writing. Significantly, this research is pioneering in revealing that
nine-year-old children convey stronger emotions than ChatGPT in their Chinese
compositions.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00582" title="Abstract">arXiv:2310.00582</a> [<a href="/pdf/2310.00582" title="Download PDF">pdf</a>, <a href="/format/2310.00582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pink: Unveiling the Power of Referential Comprehension for Multi-modal  LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xuan%2C+S">Shiyu Xuan</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qingpei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiliang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities
in many vision-language tasks. Nevertheless, most MLLMs still lack the
Referential Comprehension (RC) ability to identify a specific object or area in
images, limiting their application in fine-grained perception tasks. This paper
proposes a novel method to enhance the RC capability for MLLMs. Our model
represents the referring object in the image using the coordinates of its
bounding box and converts the coordinates into texts in a specific format. This
allows the model to treat the coordinates as natural language. Moreover, we
construct the instruction tuning dataset with various designed RC tasks at a
low cost by unleashing the potential of annotations in existing datasets. To
further boost the RC ability of the model, we propose a self-consistent
bootstrapping method that extends dense object annotations of a dataset into
high-quality referring-expression-bounding-box pairs. The model is trained
end-to-end with a parameter-efficient tuning framework that allows both
modalities to benefit from multi-modal instruction tuning. This framework
requires fewer trainable parameters and less training data. Experimental
results on conventional vision-language and RC tasks demonstrate the superior
performance of our method. For instance, our model exhibits a 12.0% absolute
accuracy improvement over Instruct-BLIP on VSR and surpasses Kosmos-2 by 24.7%
on RefCOCO_val under zero-shot settings. We also attain the top position on the
leaderboard of MMBench. The models, datasets, and codes are publicly available
at https://github.com/SY-Xuan/Pink
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00583" title="Abstract">arXiv:2310.00583</a> [<a href="/pdf/2310.00583" title="Download PDF">pdf</a>, <a href="/format/2310.00583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CityFM: City Foundation Models to Solve Urban Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balsebre%2C+P">Pasquale Balsebre</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weiming Huang</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+G">Gao Cong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pre-trained Foundation Models (PFMs) have ushered in a paradigm-shift in
Artificial Intelligence, due to their ability to learn general-purpose
representations that can be readily employed in a wide range of downstream
tasks. While PFMs have been successfully adopted in various fields such as
Natural Language Processing and Computer Vision, their capacity in handling
geospatial data and answering urban questions remains limited. This can be
attributed to the intrinsic heterogeneity of geospatial data, which encompasses
different data types, including points, segments and regions, as well as
multiple information modalities, such as a spatial position, visual
characteristics and textual annotations. The proliferation of Volunteered
Geographic Information initiatives, and the ever-increasing availability of
open geospatial data sources, like OpenStreetMap, which is freely accessible
globally, unveil a promising opportunity to bridge this gap. In this paper, we
present CityFM, a self-supervised framework to train a foundation model within
a selected geographical area of interest, such as a city. CityFM relies solely
on open data from OSM, and produces multimodal representations of entities of
different types, incorporating spatial, visual, and textual information. We
analyse the entity representations generated using our foundation models from a
qualitative perspective, and conduct quantitative experiments on road,
building, and region-level downstream tasks. We compare its results to
algorithms tailored specifically for the respective applications. In all the
experiments, CityFM achieves performance superior to, or on par with, the
baselines.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00588" title="Abstract">arXiv:2310.00588</a> [<a href="/pdf/2310.00588" title="Download PDF">pdf</a>, <a href="/format/2310.00588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Anomaly Detection in Confined Spaces Using Ergodic Traversal of  Directed Region Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wong%2C+B">Benjamin Wong</a>, 
<a href="/search/cs?searchtype=author&query=Paine%2C+T+M">Tyler M. Paine</a>, 
<a href="/search/cs?searchtype=author&query=Devasia%2C+S">Santosh Devasia</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+A+G">Ashis G. Banerjee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We provide the first step toward developing a hierarchical control-estimation
framework to actively plan robot trajectories for anomaly detection in confined
spaces. The space is represented globally using a directed region graph, where
a region is a landmark that needs to be visited (inspected). We devise a fast
mixing Markov chain to find an ergodic route that traverses this graph so that
the region visitation frequency is proportional to its anomaly detection
uncertainty, while satisfying the edge directionality (region transition)
constraint(s). Preliminary simulation results show fast convergence to the
ergodic solution and confident estimation of the presence of anomalies in the
inspected regions.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00594" title="Abstract">arXiv:2310.00594</a> [<a href="/pdf/2310.00594" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance evaluation of Machine learning algorithms for Intrusion  Detection System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tripathy%2C+S+S">Sudhanshu Sekhar Tripathy</a>, 
<a href="/search/cs?searchtype=author&query=Behera%2C+B">Bichitrananda Behera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The escalation of hazards to safety and hijacking of digital networks are
among the strongest perilous difficulties that must be addressed in the present
day. Numerous safety procedures were set up to track and recognize any illicit
activity on the network's infrastructure. IDS are the best way to resist and
recognize intrusions on internet connections and digital technologies. To
classify network traffic as normal or anomalous, Machine Learning (ML)
classifiers are increasingly utilized. An IDS with machine learning increases
the accuracy with which security attacks are detected. This paper focuses on
intrusion detection systems (IDSs) analysis using ML techniques. IDSs utilizing
ML techniques are efficient and precise at identifying network assaults. In
data with large dimensional spaces, however, the efficacy of these systems
degrades. correspondingly, the case is essential to execute a feasible feature
removal technique capable of getting rid of characteristics that have little
effect on the classification process. In this paper, we analyze the KDD
CUP-'99' intrusion detection dataset used for training and validating ML
models. Then, we implement ML classifiers such as Logistic Regression, Decision
Tree, K-Nearest Neighbour, Naive Bayes, Bernoulli Naive Bayes, Multinomial
Naive Bayes, XG-Boost Classifier, Ada-Boost, Random Forest, SVM, Rocchio
classifier, Ridge, Passive-Aggressive classifier, ANN besides Perceptron (PPN),
the optimal classifiers are determined by comparing the results of Stochastic
Gradient Descent and back-propagation neural networks for IDS, Conventional
categorization indicators, such as "accuracy, precision, recall, and the
f1-measure, have been used to evaluate the performance of the ML classification
algorithms.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00597" title="Abstract">arXiv:2310.00597</a> [<a href="/pdf/2310.00597" title="Download PDF">pdf</a>, <a href="/format/2310.00597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Task-oriented Dialog Model with Task-progressive and Policy-aware  Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+L">Lucen Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hengtong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Caixia Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaojie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiashen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+K">Ke Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+G">Guanglu Wan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NLPCC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pre-trained conversation models (PCMs) have achieved promising progress in
recent years. However, existing PCMs for Task-oriented dialog (TOD) are
insufficient for capturing the sequential nature of the TOD-related tasks, as
well as for learning dialog policy information. To alleviate these problems,
this paper proposes a task-progressive PCM with two policy-aware pre-training
tasks. The model is pre-trained through three stages where TOD-related tasks
are progressively employed according to the task logic of the TOD system. A
global policy consistency task is designed to capture the multi-turn dialog
policy sequential relation, and an act-based contrastive learning task is
designed to capture similarities among samples with the same dialog policy. Our
model achieves better results on both MultiWOZ and In-Car end-to-end dialog
modeling benchmarks with only 18\% parameters and 25\% pre-training data
compared to the previous state-of-the-art PCM, GALAXY.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00598" title="Abstract">arXiv:2310.00598</a> [<a href="/pdf/2310.00598" title="Download PDF">pdf</a>, <a href="/format/2310.00598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Computational and Modeling Foundation for Automatic Coherence  Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maimon%2C+A">Aviya Maimon</a>, 
<a href="/search/cs?searchtype=author&query=Tsarfaty%2C+R">Reut Tsarfaty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Coherence is an essential property of well-written texts, that refers to the
way textual units relate to one another. In the era of generative AI, coherence
assessment is essential for many NLP tasks; summarization, generation,
long-form question-answering, and more. However, in NLP {coherence} is an
ill-defined notion, not having a formal definition or evaluation metrics, that
would allow for large-scale automatic and systematic coherence assessment. To
bridge this gap, in this work we employ the formal linguistic definition of
\citet{Reinhart:1980} of what makes a discourse coherent, consisting of three
conditions -- {\em cohesion, consistency} and {\em relevance} -- and formalize
these conditions as respective computational tasks. We hypothesize that (i) a
model trained on all of these tasks will learn the features required for
coherence detection, and that (ii) a joint model for all tasks will exceed the
performance of models trained on each task individually. On two benchmarks for
coherence scoring rated by humans, one containing 500 automatically-generated
short stories and another containing 4k real-world texts, our experiments
confirm that jointly training on the proposed tasks leads to better performance
on each task compared with task-specific models, and to better performance on
assessing coherence overall, compared with strong baselines. We conclude that
the formal and computational setup of coherence as proposed here provides a
solid foundation for advanced methods of large-scale automatic assessment of
coherence.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00600" title="Abstract">arXiv:2310.00600</a> [<a href="/pdf/2310.00600" title="Download PDF">pdf</a>, <a href="/format/2310.00600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Complexity of the Eigenvalue Deletion Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Misra%2C+N">Neeldhara Misra</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+H">Harshil Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Saurabh%2C+S">Saket Saurabh</a>, 
<a href="/search/cs?searchtype=author&query=Thakkar%2C+D">Dhara Thakkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages; this is the full version of a paper accepted for presentation at the 34th International Symposium on Algorithms and Computation (ISAAC 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">For any fixed positive integer $r$ and a given budget $k$, the
$r$-\textsc{Eigenvalue Vertex Deletion} ($r$-EVD) problem asks if a graph $G$
admits a subset $S$ of at most $k$ vertices such that the adjacency matrix of
$G\setminus S$ has at most $r$ distinct eigenvalues. The edge deletion, edge
addition, and edge editing variants are defined analogously. For $r = 1$,
$r$-EVD is equivalent to the Vertex Cover problem. For $r = 2$, it turns out
that $r$-EVD amounts to removing a subset $S$ of at most $k$ vertices so that
$G\setminus S$ is a cluster graph where all connected components have the same
size.
<br />We show that $r$-EVD is NP-complete even on bipartite graphs with maximum
degree four for every fixed $r &gt; 2$, and FPT when parameterized by the solution
size and the maximum degree of the graph. We also establish several results for
the special case when $r = 2$. For the vertex deletion variant, we show that
$2$-EVD is NP-complete even on triangle-free and $3d$-regular graphs for any
$d\geq 2$, and also NP-complete on $d$-regular graphs for any $d\geq 8$. The
edge deletion, addition, and editing variants are all NP-complete for $r = 2$.
The edge deletion problem admits a polynomial time algorithm if the input is a
cluster graph, while the edge addition variant is hard even when the input is a
cluster graph. We show that the edge addition variant has a quadratic kernel.
The edge deletion and vertex deletion variants are FPT when parameterized by
the solution size alone.
<br />Our main contribution is to develop the complexity landscape for the problem
of modifying a graph with the aim of reducing the number of distinct
eigenvalues in the spectrum of its adjacency matrix. It turns out that this
captures, apart from Vertex Cover, also a natural variation of the problem of
modifying to a cluster graph as a special case, which we believe may be of
independent interest.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00603" title="Abstract">arXiv:2310.00603</a> [<a href="/pdf/2310.00603" title="Download PDF">pdf</a>, <a href="/format/2310.00603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faithful Explanations of Black-box NLP Models Using LLM-generated  Counterfactuals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gat%2C+Y">Yair Gat</a>, 
<a href="/search/cs?searchtype=author&query=Calderon%2C+N">Nitay Calderon</a>, 
<a href="/search/cs?searchtype=author&query=Feder%2C+A">Amir Feder</a>, 
<a href="/search/cs?searchtype=author&query=Chapanin%2C+A">Alexander Chapanin</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Amit Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Reichart%2C+R">Roi Reichart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Causal explanations of the predictions of NLP systems are essential to ensure
safety and establish trust. Yet, existing methods often fall short of
explaining model predictions effectively or efficiently and are often
model-specific. In this paper, we address model-agnostic explanations,
proposing two approaches for counterfactual (CF) approximation. The first
approach is CF generation, where a large language model (LLM) is prompted to
change a specific text concept while keeping confounding concepts unchanged.
While this approach is demonstrated to be very effective, applying LLM at
inference-time is costly. We hence present a second approach based on matching,
and propose a method that is guided by an LLM at training-time and learns a
dedicated embedding space. This space is faithful to a given causal graph and
effectively serves to identify matches that approximate CFs. After showing
theoretically that approximating CFs is required in order to construct faithful
explanations, we benchmark our approaches and explain several models, including
LLMs with billions of parameters. Our empirical results demonstrate the
excellent performance of CF generation models as model-agnostic explainers.
Moreover, our matching approach, which requires far less test-time resources,
also provides effective explanations, surpassing many baselines. We also find
that Top-K techniques universally improve every tested method. Finally, we
showcase the potential of LLMs in constructing new benchmarks for model
explanation and subsequently validate our conclusions. Our work illuminates new
pathways for efficient and accurate approaches to interpreting NLP systems.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00604" title="Abstract">arXiv:2310.00604</a> [<a href="/pdf/2310.00604" title="Download PDF">pdf</a>, <a href="/format/2310.00604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Path Structured Multimarginal Schr&#xf6;dinger Bridge for Probabilistic  Learning of Hardware Resource Usage by Control Software
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bondar%2C+G+A">Georgiy A. Bondar</a>, 
<a href="/search/eess?searchtype=author&query=Gifford%2C+R">Robert Gifford</a>, 
<a href="/search/eess?searchtype=author&query=Phan%2C+L+T+X">Linh Thi Xuan Phan</a>, 
<a href="/search/eess?searchtype=author&query=Halder%2C+A">Abhishek Halder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures. Submitted to American Control Conference (ACC) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">The solution of the path structured multimarginal Schr\"{o}dinger bridge
problem (MSBP) is the most-likely measure-valued trajectory consistent with a
sequence of observed probability measures or distributional snapshots. We
leverage recent algorithmic advances in solving such structured MSBPs for
learning stochastic hardware resource usage by control software. The solution
enables predicting the time-varying distribution of hardware resource
availability at a desired time with guaranteed linear convergence. We
demonstrate the efficacy of our probabilistic learning approach in a model
predictive control software execution case study. The method exhibits rapid
convergence to an accurate prediction of hardware resource utilization of the
controller. The method can be broadly applied to any software to predict
cyber-physical context-dependent performance at arbitrary time.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00605" title="Abstract">arXiv:2310.00605</a> [<a href="/pdf/2310.00605" title="Download PDF">pdf</a>, <a href="/format/2310.00605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Generalized Matrix Norm Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kulmburg%2C+A">Adrian Kulmburg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We investigate the problem of computing the operator norm of a matrix with
respect to norms induced by linear operators. We show that the dual formulation
of this problem can be expressed as a max-min problem which can, for some
specific cases, be solved in polynomial time. In other instances, we show that
the problem is approximable. Along the way, we develop the concept of
push-forward and pull-back of seminorms, and deduce new duality results when
optimizing over the unit ball of various norms.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00607" title="Abstract">arXiv:2310.00607</a> [<a href="/pdf/2310.00607" title="Download PDF">pdf</a>, <a href="/format/2310.00607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Onset of Robust Overfitting in Adversarial Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chaojian Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiaolong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Adversarial Training (AT) is a widely-used algorithm for building robust
neural networks, but it suffers from the issue of robust overfitting, the
fundamental mechanism of which remains unclear. In this work, we consider
normal data and adversarial perturbation as separate factors, and identify that
the underlying causes of robust overfitting stem from the normal data through
factor ablation in AT. Furthermore, we explain the onset of robust overfitting
as a result of the model learning features that lack robust generalization,
which we refer to as non-effective features. Specifically, we provide a
detailed analysis of the generation of non-effective features and how they lead
to robust overfitting. Additionally, we explain various empirical behaviors
observed in robust overfitting and revisit different techniques to mitigate
robust overfitting from the perspective of non-effective features, providing a
comprehensive understanding of the robust overfitting phenomenon. This
understanding inspires us to propose two measures, attack strength and data
augmentation, to hinder the learning of non-effective features by the neural
network, thereby alleviating robust overfitting. Extensive experiments
conducted on benchmark datasets demonstrate the effectiveness of the proposed
methods in mitigating robust overfitting and enhancing adversarial robustness.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00608" title="Abstract">arXiv:2310.00608</a> [<a href="/pdf/2310.00608" title="Download PDF">pdf</a>, <a href="/format/2310.00608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skip-Plan: Procedure Planning in Instructional Videos via Condensed  Action Space Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+W">Wenjia Geng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Muheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yansong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiwen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we propose Skip-Plan, a condensed action space learning method
for procedure planning in instructional videos. Current procedure planning
methods all stick to the state-action pair prediction at every timestep and
generate actions adjacently. Although it coincides with human intuition, such a
methodology consistently struggles with high-dimensional state supervision and
error accumulation on action sequences. In this work, we abstract the procedure
planning problem as a mathematical chain model. By skipping uncertain nodes and
edges in action chains, we transfer long and complex sequence functions into
short but reliable ones in two ways. First, we skip all the intermediate state
supervision and only focus on action predictions. Second, we decompose
relatively long chains into multiple short sub-chains by skipping unreliable
intermediate actions. By this means, our model explores all sorts of reliable
sub-relations within an action sequence in the condensed action space.
Extensive experiments show Skip-Plan achieves state-of-the-art performance on
the CrossTask and COIN benchmarks for procedure planning.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00614" title="Abstract">arXiv:2310.00614</a> [<a href="/pdf/2310.00614" title="Download PDF">pdf</a>, <a href="/format/2310.00614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Adaptation with Hypernetworks for Few-shot Molecular  Property Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shiguang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Q">Quanming Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Molecular property prediction (MPP) is important in biomedical applications,
which naturally suffers from a lack of labels, thus forming a few-shot learning
problem. State-of-the-art approaches are usually based on gradient-based meta
learning strategy, which ignore difference in model parameter and molecule's
learning difficulty. To address above problems, we propose a novel hierarchical
adaptation mechanism for few-shot MPP (HiMPP). The model follows a
encoder-predictor framework. First, to make molecular representation
property-adaptive, we selectively adapt encoder's parameter by designing a
hypernetwork to modulate node embeddings during message propagation. Next, we
make molecule-level adaptation by design another hypernetwork, which assigns
larger propagating steps for harder molecules in predictor. In this way,
molecular representation is transformed by HiMPP hierarchically from
property-level to molecular level. Extensive results show that HiMPP obtains
the state-of-the-art performance in few-shot MPP problems, and our proposed
hierarchical adaptation mechanism is rational and effective.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00615" title="Abstract">arXiv:2310.00615</a> [<a href="/pdf/2310.00615" title="Download PDF">pdf</a>, <a href="/format/2310.00615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scene-aware Human Motion Forecasting via Mutual Distance Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+C">Chaoyue Xing</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+W">Wei Mao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Miaomiao Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we tackle the problem of scene-aware 3D human motion
forecasting. A key challenge of this task is to predict future human motions
that are consistent with the scene, by modelling the human-scene interactions.
While recent works have demonstrated that explicit constraints on human-scene
interactions can prevent the occurrence of ghost motion, they only provide
constraints on partial human motion e.g., the global motion of the human or a
few joints contacting the scene, leaving the rest motion unconstrained. To
address this limitation, we propose to model the human-scene interaction with
the mutual distance between the human body and the scene. Such mutual distances
constrain both the local and global human motion, resulting in a whole-body
motion constrained prediction. In particular, mutual distance constraints
consist of two components, the signed distance of each vertex on the human mesh
to the scene surface, and the distance of basis scene points to the human mesh.
We develop a pipeline with two prediction steps that first predicts the future
mutual distances from the past human motion sequence and the scene, and then
forecasts the future human motion conditioning on the predicted mutual
distances. During training, we explicitly encourage consistency between the
predicted poses and the mutual distances. Our approach outperforms the
state-of-the-art methods on both synthetic and real datasets.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00616" title="Abstract">arXiv:2310.00616</a> [<a href="/pdf/2310.00616" title="Download PDF">pdf</a>, <a href="/format/2310.00616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Adversarial Transferability in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yijiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Ying Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haohan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages of the main paper. 21 pages in total
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We investigate the robustness and security issues from a novel and practical
setting: a group of malicious clients has impacted the model during training by
disguising their identities and acting as benign clients, and only revealing
their adversary position after the training to conduct transferable adversarial
attacks with their data, which is usually a subset of the data that FL system
is trained with. Our aim is to offer a full understanding of the challenges the
FL system faces in this practical setting across a spectrum of configurations.
We notice that such an attack is possible, but the federated model is more
robust compared with its centralized counterpart when the accuracy on clean
images is comparable. Through our study, we hypothesized the robustness is from
two factors: the decentralized training on distributed data and the averaging
operation. We provide evidence from both the perspective of empirical
experiments and theoretical analysis. Our work has implications for
understanding the robustness of federated learning systems and poses a
practical question for federated learning applications.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00618" title="Abstract">arXiv:2310.00618</a> [<a href="/pdf/2310.00618" title="Download PDF">pdf</a>, <a href="/format/2310.00618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GNRK: Graph Neural Runge-Kutta method for solving partial differential  equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+H">Hoyun Choi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sungyeop Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kahng%2C+B">B. Kahng</a>, 
<a href="/search/cs?searchtype=author&query=Jo%2C+J">Junghyo Jo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Neural networks have proven to be efficient surrogate models for tackling
partial differential equations (PDEs). However, their applicability is often
confined to specific PDEs under certain constraints, in contrast to classical
PDE solvers that rely on numerical differentiation. Striking a balance between
efficiency and versatility, this study introduces a novel approach called Graph
Neural Runge-Kutta (GNRK), which integrates graph neural network modules with a
recurrent structure inspired by the classical solvers. The GNRK operates on
graph structures, ensuring its resilience to changes in spatial and temporal
resolutions during domain discretization. Moreover, it demonstrates the
capability to address general PDEs, irrespective of initial conditions or PDE
coefficients. To assess its performance, we benchmark the GNRK against existing
neural network based PDE solvers using the 2-dimensional Burgers' equation,
revealing the GNRK's superiority in terms of model size and accuracy.
Additionally, this graph-based methodology offers a straightforward extension
for solving coupled differential equations, typically necessitating more
intricate models.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00623" title="Abstract">arXiv:2310.00623</a> [<a href="/pdf/2310.00623" title="Download PDF">pdf</a>, <a href="/format/2310.00623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speed and Density Planning for a Speed-Constrained Robot Swarm Through a  Virtual Tube
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+W">Wenqi Song</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+Q">Quan Quan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The planning and control of a robot swarm in a complex environment have
attracted increasing attention. To this end, the idea of virtual tubes has been
taken up in our previous work. Specifically, a virtual tube with varying widths
has been planned to avoid collisions with obstacles in a complex environment.
Based on the planned virtual tube for a large number of speed-constrained
robots, the average forward speed and density along the virtual tube are
further planned in this paper to ensure safety and improve efficiency. Compared
with the existing methods, the proposed method is based on global information
and can be applied to traversing narrow spaces for speed-constrained robot
swarms. Numerical simulations and experiments are conducted to show that the
safety and efficiency of the passing-through process are improved. A video
about simulations and experiments is available on https://youtu.be/lJHdMQMqSpc.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00625" title="Abstract">arXiv:2310.00625</a> [<a href="/pdf/2310.00625" title="Download PDF">pdf</a>, <a href="/format/2310.00625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reduced basis stabilization and post-processing for the virtual element  method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Credali%2C+F">Fabio Credali</a>, 
<a href="/search/math?searchtype=author&query=Bertoluzza%2C+S">Silvia Bertoluzza</a>, 
<a href="/search/math?searchtype=author&query=Prada%2C+D">Daniele Prada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 4 algorithms, 14 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present a reduced basis method for cheaply constructing (possibly rough)
approximations to the nodal basis functions of the virtual element space, and
propose to use such approximations for the design of the stabilization term in
the virtual element method and for the post-processing of the solution.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00626" title="Abstract">arXiv:2310.00626</a> [<a href="/pdf/2310.00626" title="Download PDF">pdf</a>, <a href="/format/2310.00626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GhostEncoder: Stealthy Backdoor Attacks with Dynamic Triggers to  Pre-trained Encoders in Self-supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiannan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+C">Changchun Yin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+L">Liming Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Run Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenhao Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages,8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Within the realm of computer vision, self-supervised learning (SSL) pertains
to training pre-trained image encoders utilizing a substantial quantity of
unlabeled images. Pre-trained image encoders can serve as feature extractors,
facilitating the construction of downstream classifiers for various tasks.
However, the use of SSL has led to an increase in security research related to
various backdoor attacks. Currently, the trigger patterns used in backdoor
attacks on SSL are mostly visible or static (sample-agnostic), making backdoors
less covert and significantly affecting the attack performance. In this work,
we propose GhostEncoder, the first dynamic invisible backdoor attack on SSL.
Unlike existing backdoor attacks on SSL, which use visible or static trigger
patterns, GhostEncoder utilizes image steganography techniques to encode hidden
information into benign images and generate backdoor samples. We then fine-tune
the pre-trained image encoder on a manipulation dataset to inject the backdoor,
enabling downstream classifiers built upon the backdoored encoder to inherit
the backdoor behavior for target downstream tasks. We evaluate GhostEncoder on
three downstream tasks and results demonstrate that GhostEncoder provides
practical stealthiness on images and deceives the victim model with a high
attack success rate without compromising its utility. Furthermore, GhostEncoder
withstands state-of-the-art defenses, including STRIP, STRIP-Cl, and
SSL-Cleanse.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00627" title="Abstract">arXiv:2310.00627</a> [<a href="/pdf/2310.00627" title="Download PDF">pdf</a>, <a href="/format/2310.00627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Client Selection for Federated Learning using Cellular  Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pavlidis%2C+N">Nikolaos Pavlidis</a>, 
<a href="/search/cs?searchtype=author&query=Perifanis%2C+V">Vasileios Perifanis</a>, 
<a href="/search/cs?searchtype=author&query=Chatzinikolaou%2C+T+P">Theodoros Panagiotis Chatzinikolaou</a>, 
<a href="/search/cs?searchtype=author&query=Sirakoulis%2C+G+C">Georgios Ch. Sirakoulis</a>, 
<a href="/search/cs?searchtype=author&query=Efraimidis%2C+P+S">Pavlos S. Efraimidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18th IEEE International Workshop on Cellular Nanoscale Networks and their Applications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated Learning (FL) has emerged as a promising solution for
privacy-enhancement and latency minimization in various real-world
applications, such as transportation, communications, and healthcare. FL
endeavors to bring Machine Learning (ML) down to the edge by harnessing data
from million of devices and IoT sensors, thus enabling rapid responses to
dynamic environments and yielding highly personalized results. However, the
increased amount of sensors across diverse applications poses challenges in
terms of communication and resource allocation, hindering the participation of
all devices in the federated process and prompting the need for effective FL
client selection. To address this issue, we propose Cellular Automaton-based
Client Selection (CA-CS), a novel client selection algorithm, which leverages
Cellular Automata (CA) as models to effectively capture spatio-temporal changes
in a fast-evolving environment. CA-CS considers the computational resources and
communication capacity of each participating client, while also accounting for
inter-client interactions between neighbors during the client selection
process, enabling intelligent client selection for online FL processes on data
streams that closely resemble real-world scenarios. In this paper, we present a
thorough evaluation of the proposed CA-CS algorithm using MNIST and CIFAR-10
datasets, while making a direct comparison against a uniformly random client
selection scheme. Our results demonstrate that CA-CS achieves comparable
accuracy to the random selection approach, while effectively avoiding
high-latency clients.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00629" title="Abstract">arXiv:2310.00629</a> [<a href="/pdf/2310.00629" title="Download PDF">pdf</a>, <a href="/format/2310.00629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finger-UNet: A U-Net based Multi-Task Architecture for Deep Fingerprint  Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gavas%2C+E">Ekta Gavas</a>, 
<a href="/search/cs?searchtype=author&query=Namboodiri%2C+A">Anoop Namboodiri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, Accepted at 18th VISIGRAPP 2023: VISAPP conference
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 18th International Joint Conference on Computer
  Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP
  2023) - Volume 4: VISAPP; ISBN 978-989-758-634-7; ISSN 2184-4321, SciTePress,
  pages 309-316
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">For decades, fingerprint recognition has been prevalent for security,
forensics, and other biometric applications. However, the availability of
good-quality fingerprints is challenging, making recognition difficult.
Fingerprint images might be degraded with a poor ridge structure and noisy or
less contrasting backgrounds. Hence, fingerprint enhancement plays a vital role
in the early stages of the fingerprint recognition/verification pipeline. In
this paper, we investigate and improvise the encoder-decoder style architecture
and suggest intuitive modifications to U-Net to enhance low-quality
fingerprints effectively. We investigate the use of Discrete Wavelet Transform
(DWT) for fingerprint enhancement and use a wavelet attention module instead of
max pooling which proves advantageous for our task. Moreover, we replace
regular convolutions with depthwise separable convolutions, which significantly
reduces the memory footprint of the model without degrading the performance. We
also demonstrate that incorporating domain knowledge with fingerprint minutiae
prediction task can improve fingerprint reconstruction through multi-task
learning. Furthermore, we also integrate the orientation estimation task to
propagate the knowledge of ridge orientations to enhance the performance
further. We present the experimental results and evaluate our model on FVC 2002
and NIST SD302 databases to show the effectiveness of our approach compared to
previous works.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00632" title="Abstract">arXiv:2310.00632</a> [<a href="/pdf/2310.00632" title="Download PDF">pdf</a>, <a href="/format/2310.00632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Win-Win: Training High-Resolution Vision Transformers from Two Windows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leroy%2C+V">Vincent Leroy</a>, 
<a href="/search/cs?searchtype=author&query=Revaud%2C+J">Jerome Revaud</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+T">Thomas Lucas</a>, 
<a href="/search/cs?searchtype=author&query=Weinzaepfel%2C+P">Philippe Weinzaepfel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Transformers have become the standard in state-of-the-art vision
architectures, achieving impressive performance on both image-level and dense
pixelwise tasks. However, training vision transformers for high-resolution
pixelwise tasks has a prohibitive cost. Typical solutions boil down to
hierarchical architectures, fast and approximate attention, or training on
low-resolution crops. This latter solution does not constrain architectural
choices, but it leads to a clear performance drop when testing at resolutions
significantly higher than that used for training, thus requiring ad-hoc and
slow post-processing schemes. In this paper, we propose a novel strategy for
efficient training and inference of high-resolution vision transformers: the
key principle is to mask out most of the high-resolution inputs during
training, keeping only N random windows. This allows the model to learn local
interactions between tokens inside each window, and global interactions between
tokens from different windows. As a result, the model can directly process the
high-resolution input at test time without any special trick. We show that this
strategy is effective when using relative positional embedding such as rotary
embeddings. It is 4 times faster to train than a full-resolution network, and
it is straightforward to use at test time compared to existing approaches. We
apply this strategy to the dense monocular task of semantic segmentation, and
find that a simple setting with 2 windows performs best, hence the name of our
method: Win-Win. To demonstrate the generality of our contribution, we further
extend it to the binocular task of optical flow, reaching state-of-the-art
performance on the Spring benchmark that contains Full-HD images with an
inference time an order of magnitude faster than the best competitor.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00633" title="Abstract">arXiv:2310.00633</a> [<a href="/pdf/2310.00633" title="Download PDF">pdf</a>, <a href="/format/2310.00633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Robustness and Safety of 2D and 3D Deep Learning Models  Against Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+B">Bin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Songtao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuanyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+B">Bin Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to CSUR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Benefiting from the rapid development of deep learning, 2D and 3D computer
vision applications are deployed in many safe-critical systems, such as
autopilot and identity authentication. However, deep learning models are not
trustworthy enough because of their limited robustness against adversarial
attacks. The physically realizable adversarial attacks further pose fatal
threats to the application and human safety. Lots of papers have emerged to
investigate the robustness and safety of deep learning models against
adversarial attacks. To lead to trustworthy AI, we first construct a general
threat model from different perspectives and then comprehensively review the
latest progress of both 2D and 3D adversarial attacks. We extend the concept of
adversarial examples beyond imperceptive perturbations and collate over 170
papers to give an overview of deep learning model robustness against various
adversarial attacks. To the best of our knowledge, we are the first to
systematically investigate adversarial attacks for 3D models, a flourishing
field applied to many real-world applications. In addition, we examine physical
adversarial attacks that lead to safety violations. Last but not least, we
summarize present popular topics, give insights on challenges, and shed light
on future research on trustworthy AI.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00635" title="Abstract">arXiv:2310.00635</a> [<a href="/pdf/2310.00635" title="Download PDF">pdf</a>, <a href="/format/2310.00635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning Based Neighbour Selection for VANET with Adaptive  Trust Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarker%2C+O">Orvila Sarker</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Hong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Babar%2C+M+A">M. Ali Babar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article is accepted at the 22nd IEEE International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Successful information propagation from source to destination in Vehicular
Adhoc Network (VANET) can be hampered by the presence of neighbouring attacker
nodes causing unwanted packet dropping. Potential attackers change their
behaviour over time and remain undetected due to the ad-hoc nature of VANET.
Capturing the dynamic attacker behaviour and updating the corresponding
neighbourhood information without compromising the quality of service
requirements is an ongoing challenge. This work proposes a Reinforcement
Learning (RL) based neighbour selection framework for VANET with an adaptive
trust management system to capture the behavioural changes of potential
attackers and to dynamically update the neighbourhood information. In contrast
to existing works, we consider trust and link-life time in unison as neighbour
selection criteria to achieve trustworthy communication. Our adaptive trust
model takes into account the social relationship, time and confidence in trust
observation to avoid four types of attackers. To update the neighbourhood
information, our framework sets the learning rate of the RL agent according to
the velocities of the neighbour nodes to improve the model's adaptability to
network topology changes. Results demonstrate that our method can take less
number of hops to the destination for large network sizes while can response is
up to 54 percent faster compared to a baseline method. Also, the proposed model
can outperform the other baseline method by reducing the packet dropping rate
up to 57 percent caused by the attacker.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00636" title="Abstract">arXiv:2310.00636</a> [<a href="/pdf/2310.00636" title="Download PDF">pdf</a>, <a href="/ps/2310.00636" title="Download PostScript">ps</a>, <a href="/format/2310.00636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A DEIM-CUR factorization with iterative SVDs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gidisu%2C+P+Y">Perfect Y. Gidisu</a>, 
<a href="/search/math?searchtype=author&query=Hochstenbach%2C+M+E">Michiel E. Hochstenbach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A CUR factorization is often utilized as a substitute for the singular value
decomposition (SVD), especially when a concrete interpretation of the singular
vectors is challenging. Moreover, if the original data matrix possesses
properties like nonnegativity and sparsity, a CUR decomposition can better
preserve them compared to the SVD. An essential aspect of this approach is the
methodology used for selecting a subset of columns and rows from the original
matrix. This study investigates the effectiveness of \emph{one-round sampling}
and iterative subselection techniques and introduces new iterative subselection
strategies based on iterative SVDs. One provably appropriate technique for
index selection in constructing a CUR factorization is the discrete empirical
interpolation method (DEIM). Our contribution aims to improve the approximation
quality of the DEIM scheme by iteratively invoking it in several rounds, in the
sense that we select subsequent columns and rows based on the previously
selected ones. That is, we modify $A$ after each iteration by removing the
information that has been captured by the previously selected columns and rows.
We also discuss how iterative procedures for computing a few singular vectors
of large data matrices can be integrated with the new iterative subselection
strategies. We present the results of numerical experiments, providing a
comparison of one-round sampling and iterative subselection techniques, and
demonstrating the improved approximation quality associated with using the
latter.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00637" title="Abstract">arXiv:2310.00637</a> [<a href="/pdf/2310.00637" title="Download PDF">pdf</a>, <a href="/format/2310.00637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Engineering using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Allen%2C+B+P">Bradley P. Allen</a>, 
<a href="/search/cs?searchtype=author&query=Stork%2C+L">Lise Stork</a>, 
<a href="/search/cs?searchtype=author&query=Groth%2C+P">Paul Groth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 2 figures, accepted in Transactions on Graph Data and Knowledge
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Knowledge engineering is a discipline that focuses on the creation and
maintenance of processes that generate and apply knowledge. Traditionally,
knowledge engineering approaches have focused on knowledge expressed in formal
languages. The emergence of large language models and their capabilities to
effectively work with natural language, in its broadest sense, raises questions
about the foundations and practice of knowledge engineering. Here, we outline
the potential role of LLMs in knowledge engineering, identifying two central
directions: 1) creating hybrid neuro-symbolic knowledge systems; and 2)
enabling knowledge engineering in natural language. Additionally, we formulate
key open research questions to tackle these directions.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00638" title="Abstract">arXiv:2310.00638</a> [<a href="/pdf/2310.00638" title="Download PDF">pdf</a>, <a href="/format/2310.00638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A primal-dual perspective for distributed TD-learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lim%2C+H">Han-Dong Lim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Donghwan Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">The goal of this paper is to investigate distributed temporal difference (TD)
learning for a networked multi-agent Markov decision process. The proposed
approach is based on distributed optimization algorithms, which can be
interpreted as primal-dual Ordinary differential equation (ODE) dynamics
subject to null-space constraints. Based on the exponential convergence
behavior of the primal-dual ODE dynamics subject to null-space constraints, we
examine the behavior of the final iterate in various distributed TD-learning
scenarios, considering both constant and diminishing step-sizes and
incorporating both i.i.d. and Markovian observation models. Unlike existing
methods, the proposed algorithm does not require the assumption that the
underlying communication network structure is characterized by a doubly
stochastic matrix.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00641" title="Abstract">arXiv:2310.00641</a> [<a href="/pdf/2310.00641" title="Download PDF">pdf</a>, <a href="/format/2310.00641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RegBN: Batch Normalization of Multimodal Data with Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghahremani%2C+M">Morteza Ghahremani</a>, 
<a href="/search/cs?searchtype=author&query=Wachinger%2C+C">Christian Wachinger</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent years have witnessed a surge of interest in integrating
high-dimensional data captured by multisource sensors, driven by the impressive
success of neural networks in the integration of multimodal data. However, the
integration of heterogeneous multimodal data poses a significant challenge, as
confounding effects and dependencies among such heterogeneous data sources
introduce unwanted variability and bias, leading to suboptimal performance of
multimodal models. Therefore, it becomes crucial to normalize the low- or
high-level features extracted from data modalities before their fusion takes
place. This paper introduces a novel approach for the normalization of
multimodal data, called RegBN, that incorporates regularization. RegBN uses the
Frobenius norm as a regularizer term to address the side effects of confounders
and underlying dependencies among different data sources. The proposed method
generalizes well across multiple modalities and eliminates the need for
learnable parameters, simplifying training and inference. We validate the
effectiveness of RegBN on eight databases from five research areas,
encompassing diverse modalities such as language, audio, image, video, depth,
tabular, and 3D MRI. The proposed method demonstrates broad applicability
across different architectures such as multilayer perceptrons, convolutional
neural networks, and vision transformers, enabling effective normalization of
both low- and high-level features in multimodal neural networks. RegBN is
available at \url{https://github.com/mogvision/regbn}.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00642" title="Abstract">arXiv:2310.00642</a> [<a href="/pdf/2310.00642" title="Download PDF">pdf</a>, <a href="/format/2310.00642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Bandits Model to Deep Deterministic Policy Gradient, Reinforcement  Learning with Contextual Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhendong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xiaoli Wei</a>, 
<a href="/search/cs?searchtype=author&query=Kuruoglu%2C+E+E">Ercan E. Kuruoglu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">The problem of how to take the right actions to make profits in sequential
process continues to be difficult due to the quick dynamics and a significant
amount of uncertainty in many application scenarios. In such complicated
environments, reinforcement learning (RL), a reward-oriented strategy for
optimum control, has emerged as a potential technique to address this strategic
decision-making issue. However, reinforcement learning also has some
shortcomings that make it unsuitable for solving many financial problems,
excessive resource consumption, and inability to quickly obtain optimal
solutions, making it unsuitable for quantitative trading markets. In this
study, we use two methods to overcome the issue with contextual information:
contextual Thompson sampling and reinforcement learning under supervision which
can accelerate the iterations in search of the best answer. In order to
investigate strategic trading in quantitative markets, we merged the earlier
financial trading strategy known as constant proportion portfolio insurance
(CPPI) into deep deterministic policy gradient (DDPG). The experimental results
show that both methods can accelerate the progress of reinforcement learning to
obtain the optimal solution.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00646" title="Abstract">arXiv:2310.00646</a> [<a href="/pdf/2310.00646" title="Download PDF">pdf</a>, <a href="/format/2310.00646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WASA: WAtermark-based Source Attribution for Large Language  Model-Generated Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingtan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xinyang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zitong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Z">Zhongxiang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Foo%2C+C">Chuan-Sheng Foo</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-Kiong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Low%2C+B+K+H">Bryan Kian Hsiang Low</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">The impressive performances of large language models (LLMs) and their immense
potential for commercialization have given rise to serious concerns over the
intellectual property (IP) of their training data. In particular, the synthetic
texts generated by LLMs may infringe the IP of the data being used to train the
LLMs. To this end, it is imperative to be able to (a) identify the data
provider who contributed to the generation of a synthetic text by an LLM
(source attribution) and (b) verify whether the text data from a data provider
has been used to train an LLM (data provenance). In this paper, we show that
both problems can be solved by watermarking, i.e., by enabling an LLM to
generate synthetic texts with embedded watermarks that contain information
about their source(s). We identify the key properties of such watermarking
frameworks (e.g., source attribution accuracy, robustness against adversaries),
and propose a WAtermarking for Source Attribution (WASA) framework that
satisfies these key properties due to our algorithmic designs. Our WASA
framework enables an LLM to learn an accurate mapping from the texts of
different data providers to their corresponding unique watermarks, which sets
the foundation for effective source attribution (and hence data provenance).
Extensive empirical evaluations show that our WASA framework achieves effective
source attribution and data provenance.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00647" title="Abstract">arXiv:2310.00647</a> [<a href="/pdf/2310.00647" title="Download PDF">pdf</a>, <a href="/format/2310.00647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Task Performance: Evaluating and Reducing the Flaws of Large  Multimodal Models with In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shukor%2C+M">Mustafa Shukor</a>, 
<a href="/search/cs?searchtype=author&query=Rame%2C+A">Alexandre Rame</a>, 
<a href="/search/cs?searchtype=author&query=Dancette%2C+C">Corentin Dancette</a>, 
<a href="/search/cs?searchtype=author&query=Cord%2C+M">Matthieu Cord</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://evalign-icl.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Following the success of Large Language Models (LLMs), Large Multimodal
Models (LMMs), such as the Flamingo model and its subsequent competitors, have
started to emerge as natural steps towards generalist agents. However,
interacting with recent LMMs reveals major limitations that are hardly captured
by the current evaluation benchmarks. Indeed, task performances (e.g., VQA
accuracy) alone do not provide enough clues to understand their real
capabilities, limitations, and to which extent such models are aligned to human
expectations. To refine our understanding of those flaws, we deviate from the
current evaluation paradigm and propose the EvALign-ICL framework, in which we
(1) evaluate 8 recent open-source LMMs (based on the Flamingo architecture such
as OpenFlamingo and IDEFICS) on 5 different axes; hallucinations, abstention,
compositionality, explainability and instruction following. Our evaluation on
these axes reveals major flaws in LMMs. To efficiently address these problems,
and inspired by the success of in-context learning (ICL) in LLMs, (2) we
explore ICL as a solution and study how it affects these limitations. Based on
our ICL study, (3) we push ICL further and propose new multimodal ICL
approaches such as; Multitask-ICL, Chain-of-Hindsight-ICL, and
Self-Correcting-ICL. Our findings are as follows; (1) Despite their success,
LMMs have flaws that remain unsolved with scaling alone. (2) The effect of ICL
on LMMs flaws is nuanced; despite its effectiveness for improved
explainability, abstention, and instruction following, ICL does not improve
compositional abilities, and actually even amplifies hallucinations. (3) The
proposed ICL variants are promising as post-hoc approaches to efficiently
tackle some of those flaws. The code is available here:
https://evalign-icl.github.io/
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00648" title="Abstract">arXiv:2310.00648</a> [<a href="/pdf/2310.00648" title="Download PDF">pdf</a>, <a href="/format/2310.00648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lauren Hong</a> (1), 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Ting Wang</a> (1) ((1) Stony Brook University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of
pre-trained language models (PLMs) to specific tasks. By tuning only a minimal
set of (extra) parameters, PEFT achieves performance comparable to full
fine-tuning. However, despite its prevalent use, the security implications of
PEFT remain largely unexplored. In this paper, we conduct a pilot study
revealing that PEFT exhibits unique vulnerability to trojan attacks.
Specifically, we present PETA, a novel attack that accounts for downstream
adaptation through bilevel optimization: the upper-level objective embeds the
backdoor into a PLM while the lower-level objective simulates PEFT to retain
the PLM's task-specific performance. With extensive evaluation across a variety
of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in
terms of both attack success rate and unaffected clean accuracy, even after the
victim user performs PEFT over the backdoored PLM using untainted data.
Moreover, we empirically provide possible explanations for PETA's efficacy: the
bilevel optimization inherently 'orthogonalizes' the backdoor and PEFT modules,
thereby retaining the backdoor throughout PEFT. Based on this insight, we
explore a simple defense that omits PEFT in selected layers of the backdoored
PLM and unfreezes a subset of these layers' parameters, which is shown to
effectively neutralize PETA.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00650" title="Abstract">arXiv:2310.00650</a> [<a href="/pdf/2310.00650" title="Download PDF">pdf</a>, <a href="/format/2310.00650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quasi-Monte Carlo for unbounded integrands with importance sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ouyang%2C+D">Du Ouyang</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+X">Xiaoqun Wang</a>, 
<a href="/search/math?searchtype=author&query=He%2C+Z">Zhijian He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We consider the problem of estimating an expectation $ \mathbb{E}\left[
h(W)\right]$ by quasi-Monte Carlo (QMC) methods, where $ h $ is an unbounded
smooth function on $ \mathbb{R}^d $ and $ W$ is a standard normal distributed
random variable. To study rates of convergence for QMC on unbounded integrands,
we use a smoothed projection operator to project the output of $W$ to a bounded
region, which differs from the strategy of avoiding the singularities along the
boundary of the unit cube $ [0,1]^d $ in 10.1137/S0036144504441573. The error
is then bounded by the quadrature error of the transformed integrand and the
projection error. Under certain growth conditions on the function $h$, we
obtain an error rate of $ O(n^{-1+\epsilon}) $ for QMC and randomized QMC with
a sample size $ n $ and an arbitrarily small $ \epsilon&gt;0 $. Furthermore, we
find that importance sampling can improve the root mean squared error of
randomized QMC from $ O(n^{-1+\epsilon}) $ to $ O( n^{-3/2+\epsilon}) $.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00653" title="Abstract">arXiv:2310.00653</a> [<a href="/pdf/2310.00653" title="Download PDF">pdf</a>, <a href="/format/2310.00653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reformulating Vision-Language Foundation Models and Datasets Towards  Universal Multimodal Assistants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tianyu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jinyi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoye Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yue Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chongyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yinxv Pan</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jiao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dahai Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Hai-Tao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities
to perceive images and follow open-ended instructions. The capabilities of
MLLMs depend on two crucial factors: the model architecture to facilitate the
feature alignment of visual modules and large language models; the multimodal
instruction tuning datasets for human instruction following. (i) For the model
architecture, most existing models introduce an external bridge module to
connect vision encoders with language models, which needs an additional
feature-alignment pre-training. In this work, we discover that compact
pre-trained vision language models can inherently serve as ``out-of-the-box''
bridges between vision and language. Based on this, we propose Muffin
framework, which directly employs pre-trained vision-language models to act as
providers of visual signals. (ii) For the multimodal instruction tuning
datasets, existing methods omit the complementary relationship between
different datasets and simply mix datasets from different tasks. Instead, we
propose UniMM-Chat dataset which explores the complementarities of datasets to
generate 1.1M high-quality and diverse multimodal instructions. We merge
information describing the same image from diverse datasets and transforms it
into more knowledge-intensive conversation data. Experimental results
demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset.
Muffin achieves state-of-the-art performance on a wide range of vision-language
tasks, significantly surpassing state-of-the-art models like LLaVA and
InstructBLIP. Our model and dataset are all accessible at
https://github.com/thunlp/muffin.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00654" title="Abstract">arXiv:2310.00654</a> [<a href="/pdf/2310.00654" title="Download PDF">pdf</a>, <a href="/format/2310.00654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Streamlining Attack Tree Generation: A Fragment-Based Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pekaric%2C+I">Irdin Pekaric</a>, 
<a href="/search/cs?searchtype=author&query=Frick%2C+M">Markus Frick</a>, 
<a href="/search/cs?searchtype=author&query=Adigun%2C+J+G">Jubril Gbolahan Adigun</a>, 
<a href="/search/cs?searchtype=author&query=Groner%2C+R">Raffaela Groner</a>, 
<a href="/search/cs?searchtype=author&query=Witte%2C+T">Thomas Witte</a>, 
<a href="/search/cs?searchtype=author&query=Raschke%2C+A">Alexander Raschke</a>, 
<a href="/search/cs?searchtype=author&query=Felderer%2C+M">Michael Felderer</a>, 
<a href="/search/cs?searchtype=author&query=Tichy%2C+M">Matthias Tichy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at the 57th Hawaii International Conference on Social Systems (HICSS-57), Honolulu, Hawaii. 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Attack graphs are a tool for analyzing security vulnerabilities that capture
different and prospective attacks on a system. As a threat modeling tool, it
shows possible paths that an attacker can exploit to achieve a particular goal.
However, due to the large number of vulnerabilities that are published on a
daily basis, they have the potential to rapidly expand in size. Consequently,
this necessitates a significant amount of resources to generate attack graphs.
In addition, generating composited attack models for complex systems such as
self-adaptive or AI is very difficult due to their nature to continuously
change. In this paper, we present a novel fragment-based attack graph
generation approach that utilizes information from publicly available
information security databases. Furthermore, we also propose a domain-specific
language for attack modeling, which we employ in the proposed attack graph
generation approach. Finally, we present a demonstrator example showcasing the
attack generator's capability to replicate a verified attack chain, as
previously confirmed by security experts.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00655" title="Abstract">arXiv:2310.00655</a> [<a href="/pdf/2310.00655" title="Download PDF">pdf</a>, <a href="/format/2310.00655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PatchMixer: A Patch-Mixing Architecture for Long-Term Time Series  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+Z">Zeying Gong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yujin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Junwei Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Although the Transformer has been the dominant architecture for time series
forecasting tasks in recent years, a fundamental challenge remains: the
permutation-invariant self-attention mechanism within Transformers leads to a
loss of temporal information. To tackle these challenges, we propose
PatchMixer, a novel CNN-based model. It introduces a permutation-variant
convolutional structure to preserve temporal information. Diverging from
conventional CNNs in this field, which often employ multiple scales or numerous
branches, our method relies exclusively on depthwise separable convolutions.
This allows us to extract both local features and global correlations using a
single-scale architecture. Furthermore, we employ dual forecasting heads that
encompass both linear and nonlinear components to better model future curve
trends and details. Our experimental results on seven time-series forecasting
benchmarks indicate that compared with the state-of-the-art method and the
best-performing CNN, PatchMixer yields $3.9\%$ and $21.2\%$ relative
improvements, respectively, while being 2-3x faster than the most advanced
method. We will release our code and model.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00656" title="Abstract">arXiv:2310.00656</a> [<a href="/pdf/2310.00656" title="Download PDF">pdf</a>, <a href="/format/2310.00656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEGO-Prover: Neural Theorem Proving with Growing Libraries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xin%2C+H">Huajian Xin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chuanyang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qingxing Cao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yinya Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jing Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Han Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jian Yin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaodan Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Despite the success of large language models (LLMs), the task of theorem
proving still remains one of the hardest reasoning tasks that is far from being
fully solved. Prior methods using language models have demonstrated promising
results, but they still struggle to prove even middle school level theorems.
One common limitation of these methods is that they assume a fixed theorem
library during the whole theorem proving process. However, as we all know,
creating new useful theorems or even new theories is not only helpful but
crucial and necessary for advancing mathematics and proving harder and deeper
results. In this work, we present LEGO-Prover, which employs a growing skill
library containing verified lemmas as skills to augment the capability of LLMs
used in theorem proving. By constructing the proof modularly, LEGO-Prover
enables LLMs to utilize existing skills retrieved from the library and to
create new skills during the proving process. These skills are further evolved
(by prompting an LLM) to enrich the library on another scale. Modular and
reusable skills are constantly added to the library to enable tackling
increasingly intricate mathematical problems. Moreover, the learned library
further bridges the gap between human proofs and formal proofs by making it
easier to impute missing steps. LEGO-Prover advances the state-of-the-art pass
rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 47.1%).
During the proving process, LEGO-Prover also manages to generate over 20,000
skills (theorems/lemmas) and adds them to the growing library. Our ablation
study indicates that these newly added skills are indeed helpful for proving
theorems, resulting in an improvement from a success rate of 47.1% to 50.4%. We
also release our code and all the generated skills.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00658" title="Abstract">arXiv:2310.00658</a> [<a href="/pdf/2310.00658" title="Download PDF">pdf</a>, <a href="/format/2310.00658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Robots are Here: Navigating the Generative AI Revolution in  Computing Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prather%2C+J">James Prather</a>, 
<a href="/search/cs?searchtype=author&query=Denny%2C+P">Paul Denny</a>, 
<a href="/search/cs?searchtype=author&query=Leinonen%2C+J">Juho Leinonen</a>, 
<a href="/search/cs?searchtype=author&query=Becker%2C+B+A">Brett A. Becker</a>, 
<a href="/search/cs?searchtype=author&query=Albluwi%2C+I">Ibrahim Albluwi</a>, 
<a href="/search/cs?searchtype=author&query=Craig%2C+M">Michelle Craig</a>, 
<a href="/search/cs?searchtype=author&query=Keuning%2C+H">Hieke Keuning</a>, 
<a href="/search/cs?searchtype=author&query=Kiesler%2C+N">Natalie Kiesler</a>, 
<a href="/search/cs?searchtype=author&query=Kohn%2C+T">Tobias Kohn</a>, 
<a href="/search/cs?searchtype=author&query=Luxton-Reilly%2C+A">Andrew Luxton-Reilly</a>, 
<a href="/search/cs?searchtype=author&query=MacNeil%2C+S">Stephen MacNeil</a>, 
<a href="/search/cs?searchtype=author&query=Peterson%2C+A">Andrew Peterson</a>, 
<a href="/search/cs?searchtype=author&query=Pettit%2C+R">Raymond Pettit</a>, 
<a href="/search/cs?searchtype=author&query=Reeves%2C+B+N">Brent N. Reeves</a>, 
<a href="/search/cs?searchtype=author&query=Savelka%2C+J">Jaromir Savelka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages of content + 12 pages of references and appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Recent advancements in artificial intelligence (AI) are fundamentally
reshaping computing, with large language models (LLMs) now effectively being
able to generate and interpret source code and natural language instructions.
These emergent capabilities have sparked urgent questions in the computing
education community around how educators should adapt their pedagogy to address
the challenges and to leverage the opportunities presented by this new
technology. In this working group report, we undertake a comprehensive
exploration of LLMs in the context of computing education and make five
significant contributions. First, we provide a detailed review of the
literature on LLMs in computing education and synthesise findings from 71
primary articles. Second, we report the findings of a survey of computing
students and instructors from across 20 countries, capturing prevailing
attitudes towards LLMs and their use in computing education contexts. Third, to
understand how pedagogy is already changing, we offer insights collected from
in-depth interviews with 22 computing educators from five continents who have
already adapted their curricula and assessments. Fourth, we use the ACM Code of
Ethics to frame a discussion of ethical issues raised by the use of large
language models in computing education, and we provide concrete advice for
policy makers, educators, and students. Finally, we benchmark the performance
of LLMs on various computing education datasets, and highlight the extent to
which the capabilities of current models are rapidly improving. Our aim is that
this report will serve as a focal point for both researchers and practitioners
who are exploring, adapting, using, and evaluating LLMs and LLM-based tools in
computing classrooms.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00659" title="Abstract">arXiv:2310.00659</a> [<a href="/pdf/2310.00659" title="Download PDF">pdf</a>, <a href="/format/2310.00659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Liveness Detection Competition -- Noncontact-based Fingerprint  Algorithms and Systems (LivDet-2023 Noncontact Fingerprint)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Purnapatra%2C+S">Sandip Purnapatra</a>, 
<a href="/search/cs?searchtype=author&query=Rezaie%2C+H">Humaira Rezaie</a>, 
<a href="/search/cs?searchtype=author&query=Jawade%2C+B">Bhavin Jawade</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yue Pan</a>, 
<a href="/search/cs?searchtype=author&query=Brosell%2C+L">Luke Brosell</a>, 
<a href="/search/cs?searchtype=author&query=Sumi%2C+M+R">Mst Rumana Sumi</a>, 
<a href="/search/cs?searchtype=author&query=Igene%2C+L">Lambert Igene</a>, 
<a href="/search/cs?searchtype=author&query=Dimarco%2C+A">Alden Dimarco</a>, 
<a href="/search/cs?searchtype=author&query=Setlur%2C+S">Srirangaraj Setlur</a>, 
<a href="/search/cs?searchtype=author&query=Dey%2C+S">Soumyabrata Dey</a>, 
<a href="/search/cs?searchtype=author&query=Schuckers%2C+S">Stephanie Schuckers</a>, 
<a href="/search/cs?searchtype=author&query=Huber%2C+M">Marco Huber</a>, 
<a href="/search/cs?searchtype=author&query=Kolf%2C+J+N">Jan Niklas Kolf</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Meiling Fang</a>, 
<a href="/search/cs?searchtype=author&query=Damer%2C+N">Naser Damer</a>, 
<a href="/search/cs?searchtype=author&query=Adami%2C+B">Banafsheh Adami</a>, 
<a href="/search/cs?searchtype=author&query=Chitic%2C+R">Raul Chitic</a>, 
<a href="/search/cs?searchtype=author&query=Seelert%2C+K">Karsten Seelert</a>, 
<a href="/search/cs?searchtype=author&query=Mistry%2C+V">Vishesh Mistry</a>, 
<a href="/search/cs?searchtype=author&query=Parthe%2C+R">Rahul Parthe</a>, 
<a href="/search/cs?searchtype=author&query=Kacar%2C+U">Umit Kacar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Liveness Detection (LivDet) is an international competition series open to
academia and industry with the objec-tive to assess and report state-of-the-art
in Presentation Attack Detection (PAD). LivDet-2023 Noncontact Fingerprint is
the first edition of the noncontact fingerprint-based PAD competition for
algorithms and systems. The competition serves as an important benchmark in
noncontact-based fingerprint PAD, offering (a) independent assessment of the
state-of-the-art in noncontact-based fingerprint PAD for algorithms and
systems, and (b) common evaluation protocol, which includes finger photos of a
variety of Presentation Attack Instruments (PAIs) and live fingers to the
biometric research community (c) provides standard algorithm and system
evaluation protocols, along with the comparative analysis of state-of-the-art
algorithms from academia and industry with both old and new android
smartphones. The winning algorithm achieved an APCER of 11.35% averaged overall
PAIs and a BPCER of 0.62%. The winning system achieved an APCER of 13.0.4%,
averaged over all PAIs tested over all the smartphones, and a BPCER of 1.68%
over all smartphones tested. Four-finger systems that make individual
finger-based PAD decisions were also tested. The dataset used for competition
will be available 1 to all researchers as per data share protocol
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00664" title="Abstract">arXiv:2310.00664</a> [<a href="/pdf/2310.00664" title="Download PDF">pdf</a>, <a href="/format/2310.00664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Twin Neural Network Improved k-Nearest Neighbor Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wetzel%2C+S+J">Sebastian J. Wetzel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2301.01383">arXiv:2301.01383</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Twin neural network regression is trained to predict differences between
regression targets rather than the targets themselves. A solution to the
original regression problem can be obtained by ensembling predicted differences
between the targets of an unknown data point and multiple known anchor data
points. Choosing the anchors to be the nearest neighbors of the unknown data
point leads to a neural network-based improvement of k-nearest neighbor
regression. This algorithm is shown to outperform both neural networks and
k-nearest neighbor regression on small to medium-sized data sets.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00665" title="Abstract">arXiv:2310.00665</a> [<a href="/pdf/2310.00665" title="Download PDF">pdf</a>, <a href="/format/2310.00665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balancing Efficiency vs. Effectiveness and Providing Missing Label  Robustness in Multi-Label Stream Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bakhshi%2C+S">Sepehr Bakhshi</a>, 
<a href="/search/cs?searchtype=author&query=Can%2C+F">Fazli Can</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Available works addressing multi-label classification in a data stream
environment focus on proposing accurate models; however, these models often
exhibit inefficiency and cannot balance effectiveness and efficiency. In this
work, we propose a neural network-based approach that tackles this issue and is
suitable for high-dimensional multi-label classification. Our model uses a
selective concept drift adaptation mechanism that makes it suitable for a
non-stationary environment. Additionally, we adapt our model to an environment
with missing labels using a simple yet effective imputation strategy and
demonstrate that it outperforms a vast majority of the state-of-the-art
supervised models. To achieve our purposes, we introduce a weighted binary
relevance-based approach named ML-BELS using the Broad Ensemble Learning System
(BELS) as its base classifier. Instead of a chain of stacked classifiers, our
model employs independent weighted ensembles, with the weights generated by the
predictions of a BELS classifier. We show that using the weighting strategy on
datasets with low label cardinality negatively impacts the accuracy of the
model; with this in mind, we use the label cardinality as a trigger for
applying the weights. We present an extensive assessment of our model using 11
state-of-the-art baselines, five synthetics, and 13 real-world datasets, all
with different characteristics. Our results demonstrate that the proposed
approach ML-BELS is successful in balancing effectiveness and efficiency, and
is robust to missing labels and concept drift.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00670" title="Abstract">arXiv:2310.00670</a> [<a href="/pdf/2310.00670" title="Download PDF">pdf</a>, <a href="/format/2310.00670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hierarchical Graph-based Approach for Recognition and Description  Generation of Bimanual Actions in Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziaeetabar%2C+F">Fatemeh Ziaeetabar</a>, 
<a href="/search/cs?searchtype=author&query=Safabakhsh%2C+R">Reza Safabakhsh</a>, 
<a href="/search/cs?searchtype=author&query=Momtazi%2C+S">Saeedeh Momtazi</a>, 
<a href="/search/cs?searchtype=author&query=Tamosiunaite%2C+M">Minija Tamosiunaite</a>, 
<a href="/search/cs?searchtype=author&query=W%C3%B6rg%C3%B6tter%2C+F">Florentin W&#xf6;rg&#xf6;tter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Nuanced understanding and the generation of detailed descriptive content for
(bimanual) manipulation actions in videos is important for disciplines such as
robotics, human-computer interaction, and video content analysis. This study
describes a novel method, integrating graph based modeling with layered
hierarchical attention mechanisms, resulting in higher precision and better
comprehensiveness of video descriptions. To achieve this, we encode, first, the
spatio-temporal inter dependencies between objects and actions with scene
graphs and we combine this, in a second step, with a novel 3-level architecture
creating a hierarchical attention mechanism using Graph Attention Networks
(GATs). The 3-level GAT architecture allows recognizing local, but also global
contextual elements. This way several descriptions with different semantic
complexity can be generated in parallel for the same video clip, enhancing the
discriminative accuracy of action recognition and action description. The
performance of our approach is empirically tested using several 2D and 3D
datasets. By comparing our method to the state of the art we consistently
obtain better performance concerning accuracy, precision, and contextual
relevance when evaluating action recognition as well as description generation.
In a large set of ablation experiments we also assess the role of the different
components of our model. With our multi-level approach the system obtains
different semantic description depths, often observed in descriptions made by
different people, too. Furthermore, better insight into bimanual hand-object
interactions as achieved by our model may portend advancements in the field of
robotics, enabling the emulation of intricate human actions with heightened
precision.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00672" title="Abstract">arXiv:2310.00672</a> [<a href="/pdf/2310.00672" title="Download PDF">pdf</a>, <a href="/format/2310.00672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GeRA: Label-Efficient Geometrically Regularized Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klebe%2C+D">Dustin Klebe</a>, 
<a href="/search/cs?searchtype=author&query=Shnitzer%2C+T">Tal Shnitzer</a>, 
<a href="/search/cs?searchtype=author&query=Yurochkin%2C+M">Mikhail Yurochkin</a>, 
<a href="/search/cs?searchtype=author&query=Karlinsky%2C+L">Leonid Karlinsky</a>, 
<a href="/search/cs?searchtype=author&query=Solomon%2C+J">Justin Solomon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Pretrained unimodal encoders incorporate rich semantic information into
embedding space structures. To be similarly informative, multi-modal encoders
typically require massive amounts of paired data for alignment and training. We
introduce a semi-supervised Geometrically Regularized Alignment (GeRA) method
to align the embedding spaces of pretrained unimodal encoders in a
label-efficient way. Our method leverages the manifold geometry of unpaired
(unlabeled) data to improve alignment performance. To prevent distortions to
local geometry during the alignment process, potentially disrupting semantic
neighborhood structures and causing misalignment of unobserved pairs, we
introduce a geometric loss term. This term is built upon a diffusion operator
that captures the local manifold geometry of the unimodal pretrained encoders.
GeRA is modality-agnostic and thus can be used to align pretrained encoders
from any data modalities. We provide empirical evidence to the effectiveness of
our method in the domains of speech-text and image-text alignment. Our
experiments demonstrate significant improvement in alignment quality compared
to a variaty of leading baselines, especially with a small amount of paired
data, using our proposed geometric regularization.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00673" title="Abstract">arXiv:2310.00673</a> [<a href="/pdf/2310.00673" title="Download PDF">pdf</a>, <a href="/format/2310.00673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Type Inference for Enhanced Dataflow Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seidel%2C+L">Lukas Seidel</a>, 
<a href="/search/cs?searchtype=author&query=Effendi%2C+S+D+B">Sedick David Baker Effendi</a>, 
<a href="/search/cs?searchtype=author&query=Pinho%2C+X">Xavier Pinho</a>, 
<a href="/search/cs?searchtype=author&query=Rieck%2C+K">Konrad Rieck</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Merwe%2C+B">Brink van der Merwe</a>, 
<a href="/search/cs?searchtype=author&query=Yamaguch%2C+F">Fabian Yamaguch</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 28th European Symposium on Research in Computer Security (ESORICS)
  2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Statically analyzing dynamically-typed code is a challenging endeavor, as
even seemingly trivial tasks such as determining the targets of procedure calls
are non-trivial without knowing the types of objects at compile time.
Addressing this challenge, gradual typing is increasingly added to
dynamically-typed languages, a prominent example being TypeScript that
introduces static typing to JavaScript. Gradual typing improves the developer's
ability to verify program behavior, contributing to robust, secure and
debuggable programs. In practice, however, users only sparsely annotate types
directly. At the same time, conventional type inference faces
performance-related challenges as program size grows. Statistical techniques
based on machine learning offer faster inference, but although recent
approaches demonstrate overall improved accuracy, they still perform
significantly worse on user-defined types than on the most common built-in
types. Limiting their real-world usefulness even more, they rarely integrate
with user-facing applications. We propose CodeTIDAL5, a Transformer-based model
trained to reliably predict type annotations. For effective result retrieval
and re-integration, we extract usage slices from a program's code property
graph. Comparing our approach against recent neural type inference systems, our
model outperforms the current state-of-the-art by 7.85% on the
ManyTypes4TypeScript benchmark, achieving 71.27% accuracy overall. Furthermore,
we present JoernTI, an integration of our approach into Joern, an open source
static analysis tool, and demonstrate that the analysis benefits from the
additional type information. As our model allows for fast inference times even
on commodity CPUs, making our system available through Joern leads to high
accessibility and facilitates security research.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00675" title="Abstract">arXiv:2310.00675</a> [<a href="/pdf/2310.00675" title="Download PDF">pdf</a>, <a href="/format/2310.00675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimization or Architecture: How to Hack Kalman Filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Greenberg%2C+I">Ido Greenberg</a>, 
<a href="/search/cs?searchtype=author&query=Yannay%2C+N">Netanel Yannay</a>, 
<a href="/search/cs?searchtype=author&query=Mannor%2C+S">Shie Mannor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In non-linear filtering, it is traditional to compare non-linear
architectures such as neural networks to the standard linear Kalman Filter
(KF). We observe that this mixes the evaluation of two separate components: the
non-linear architecture, and the parameters optimization method. In particular,
the non-linear model is often optimized, whereas the reference KF model is not.
We argue that both should be optimized similarly, and to that end present the
Optimized KF (OKF). We demonstrate that the KF may become competitive to neural
models - if optimized using OKF. This implies that experimental conclusions of
certain previous studies were derived from a flawed process. The advantage of
OKF over the standard KF is further studied theoretically and empirically, in a
variety of problems. Conveniently, OKF can replace the KF in real-world systems
by merely updating the parameters.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00677" title="Abstract">arXiv:2310.00677</a> [<a href="/pdf/2310.00677" title="Download PDF">pdf</a>, <a href="/format/2310.00677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Roadmap towards Intelligent Operations for Reliable Cloud Computing  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huo%2C+Y">Yintong Huo</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Cheryl Lee</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+M+R">Michael R. Lyu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by ICDM AIOPS workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The increasing complexity and usage of cloud systems have made it challenging
for service providers to ensure reliability. This paper highlights two main
challenges, namely internal and external factors, that affect the reliability
of cloud microservices. Afterward, we discuss the data-driven approach that can
resolve these challenges from four key aspects: ticket management, log
management, multimodal analysis, and the microservice resilience testing
approach. The experiments conducted show that the proposed data-driven AIOps
solution significantly enhances system reliability from multiple angles.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00678" title="Abstract">arXiv:2310.00678</a> [<a href="/pdf/2310.00678" title="Download PDF">pdf</a>, <a href="/format/2310.00678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A General Offline Reinforcement Learning Framework for Interactive  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Teng Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Donglin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">This paper studies the problem of learning interactive recommender systems
from logged feedbacks without any exploration in online environments. We
address the problem by proposing a general offline reinforcement learning
framework for recommendation, which enables maximizing cumulative user rewards
without online exploration. Specifically, we first introduce a probabilistic
generative model for interactive recommendation, and then propose an effective
inference algorithm for discrete and stochastic policy learning based on logged
feedbacks. In order to perform offline learning more effectively, we propose
five approaches to minimize the distribution mismatch between the logging
policy and recommendation policy: support constraints, supervised
regularization, policy constraints, dual constraints and reward extrapolation.
We conduct extensive experiments on two public real-world datasets,
demonstrating that the proposed methods can achieve superior performance over
existing supervised learning and reinforcement learning methods for
recommendation.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00679" title="Abstract">arXiv:2310.00679</a> [<a href="/pdf/2310.00679" title="Download PDF">pdf</a>, <a href="/format/2310.00679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CebuaNER: A New Baseline Cebuano Named Entity Recognition Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pilar%2C+M+B+E">Ma. Beatrice Emanuela Pilar</a>, 
<a href="/search/cs?searchtype=author&query=Papas%2C+E+M">Ellyza Mari Papas</a>, 
<a href="/search/cs?searchtype=author&query=Buenaventura%2C+M+L">Mary Loise Buenaventura</a>, 
<a href="/search/cs?searchtype=author&query=Dedoroy%2C+D">Dane Dedoroy</a>, 
<a href="/search/cs?searchtype=author&query=Montefalcon%2C+M+D">Myron Darrel Montefalcon</a>, 
<a href="/search/cs?searchtype=author&query=Padilla%2C+J+R">Jay Rhald Padilla</a>, 
<a href="/search/cs?searchtype=author&query=Maceda%2C+L">Lany Maceda</a>, 
<a href="/search/cs?searchtype=author&query=Abisado%2C+M">Mideth Abisado</a>, 
<a href="/search/cs?searchtype=author&query=Imperial%2C+J+M">Joseph Marvin Imperial</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for PACLIC2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite being one of the most linguistically diverse groups of countries,
computational linguistics and language processing research in Southeast Asia
has struggled to match the level of countries from the Global North. Thus,
initiatives such as open-sourcing corpora and the development of baseline
models for basic language processing tasks are important stepping stones to
encourage the growth of research efforts in the field. To answer this call, we
introduce CebuaNER, a new baseline model for named entity recognition (NER) in
the Cebuano language. Cebuano is the second most-used native language in the
Philippines, with over 20 million speakers. To build the model, we collected
and annotated over 4,000 news articles, the largest of any work in the
language, retrieved from online local Cebuano platforms to train algorithms
such as Conditional Random Field and Bidirectional LSTM. Our findings show
promising results as a new baseline model, achieving over 70% performance on
precision, recall, and F1 across all entity tags, as well as potential efficacy
in a crosslingual setup with Tagalog.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00683" title="Abstract">arXiv:2310.00683</a> [<a href="/pdf/2310.00683" title="Download PDF">pdf</a>, <a href="/format/2310.00683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Active Flux method for the Euler equations on Cartesian grids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Abgrall%2C+R">R&#xe9;mi Abgrall</a>, 
<a href="/search/math?searchtype=author&query=Barsukow%2C+W">Wasilij Barsukow</a>, 
<a href="/search/math?searchtype=author&query=Klingenberg%2C+C">Christian Klingenberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Active Flux is an extension of the Finite Volume method and additionally
incorporates point values located at cell boundaries. This gives rise to a
globally continuous approximation of the solution. The method is third-order
accurate. We demonstrate that a new semi-discrete Active Flux method (first
described in Abgrall&amp;Barsukow, 2023 for one space dimension) can easily be used
to solve nonlinear hyperbolic systems in multiple dimensions, such as the
compressible Euler equations of inviscid hydrodynamics. Originally, the Active
Flux method emerged as a fully discrete method, and required an exact or
approximate evolution operator for the point value update. For nonlinear
problems such an operator is often difficult to obtain, in particular for
multiple spatial dimensions. With the new approach it becomes possible to leave
behind these difficulties. We introduce a multi-dimensional limiting strategy
and demonstrate the performance of the new method on both Riemann problems and
subsonic flows.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00684" title="Abstract">arXiv:2310.00684</a> [<a href="/pdf/2310.00684" title="Download PDF">pdf</a>, <a href="/format/2310.00684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Many Views Are Needed to Reconstruct an Unknown Object Using NeRF?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Sicong Pan</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Liren Jin</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Popovi%C4%87%2C+M">Marija Popovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Bennewitz%2C+M">Maren Bennewitz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Neural Radiance Fields (NeRFs) are gaining significant interest for online
active object reconstruction due to their exceptional memory efficiency and
requirement for only posed RGB inputs. Previous NeRF-based view planning
methods exhibit computational inefficiency since they rely on an iterative
paradigm, consisting of (1) retraining the NeRF when new images arrive; and (2)
planning a path to the next best view only. To address these limitations, we
propose a non-iterative pipeline based on the Prediction of the Required number
of Views (PRV). The key idea behind our approach is that the required number of
views to reconstruct an object depends on its complexity. Therefore, we design
a deep neural network, named PRVNet, to predict the required number of views,
allowing us to tailor the data acquisition based on the object complexity and
plan a globally shortest path. To train our PRVNet, we generate supervision
labels using the ShapeNet dataset. Simulated experiments show that our
PRV-based view planning method outperforms baselines, achieving good
reconstruction quality while significantly reducing movement cost and planning
time. We further justify the generalization ability of our approach in a
real-world experiment.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00685" title="Abstract">arXiv:2310.00685</a> [<a href="/pdf/2310.00685" title="Download PDF">pdf</a>, <a href="/format/2310.00685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Implicit Reconstruction Using One-Shot View Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Sicong Pan</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Liren Jin</a>, 
<a href="/search/cs?searchtype=author&query=Popovi%C4%87%2C+M">Marija Popovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Bennewitz%2C+M">Maren Bennewitz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Active object reconstruction using autonomous robots is gaining great
interest. A primary goal in this task is to maximize the information of the
object to be reconstructed, given limited on-board resources. Previous view
planning methods exhibit inefficiency since they rely on an iterative paradigm
based on explicit representations, consisting of (1) planning a path to the
next-best view only; and (2) requiring a considerable number of less-gain views
in terms of surface coverage. To address these limitations, we integrated
implicit representations into the One-Shot View Planning (OSVP). The key idea
behind our approach is to use implicit representations to obtain the small
missing surface areas instead of observing them with extra views. Therefore, we
design a deep neural network, named OSVP, to directly predict a set of views
given a dense point cloud refined from an initial sparse observation. To train
our OSVP network, we generate supervision labels using dense point clouds
refined by implicit representations and set covering optimization problems.
Simulated experiments show that our method achieves sufficient reconstruction
quality, outperforming several baselines under limited view and movement
budgets. We further demonstrate the applicability of our approach in a
real-world object reconstruction scenario.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00688" title="Abstract">arXiv:2310.00688</a> [<a href="/pdf/2310.00688" title="Download PDF">pdf</a>, <a href="/format/2310.00688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Constrained Dynamics Algorithms based on an Equivalent LQR  Formulation using Gauss&#x27; Principle of Least Constraint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sathya%2C+A+S">Ajay Suresha Sathya</a>, 
<a href="/search/cs?searchtype=author&query=Bruyninckx%2C+H">Herman Bruyninckx</a>, 
<a href="/search/cs?searchtype=author&query=Decre%2C+W">Wilm Decre</a>, 
<a href="/search/cs?searchtype=author&query=Pipeleers%2C+G">Goele Pipeleers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We derive a family of efficient constrained dynamics algorithms by
formulating an equivalent linear quadratic regulator (LQR) problem using Gauss
principle of least constraint and solving it using dynamic programming. Our
approach builds upon the pioneering (but largely unknown) O(n + m^2d + m^3)
solver by Popov and Vereshchagin (PV), where n, m and d are the number of
joints, number of constraints and the kinematic tree depth respectively. We
provide an expository derivation for the original PV solver and extend it to
floating-base kinematic trees with constraints allowed on any link. We make new
connections between the LQR's dual Hessian and the inverse operational space
inertia matrix (OSIM), permitting efficient OSIM computation, which we further
accelerate using matrix inversion lemma. By generalizing the elimination
ordering and accounting for MUJOCO-type soft constraints, we derive two
original O(n + m) complexity solvers. Our numerical results indicate that
significant simulation speed-up can be achieved for high dimensional robots
like quadrupeds and humanoids using our algorithms as they scale better than
the widely used O(nd^2 + m^2d + d^2m) LTL algorithm of Featherstone. The
derivation through the LQR-constrained dynamics connection can make our
algorithm accessible to a wider audience and enable cross-fertilization of
software and research results between the fields
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00689" title="Abstract">arXiv:2310.00689</a> [<a href="/pdf/2310.00689" title="Download PDF">pdf</a>, <a href="/format/2310.00689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exchange means change: an unsupervised single-temporal change detection  framework based on intra- and inter-image patch exchange
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongruixuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jian Song</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bo Du</a>, 
<a href="/search/cs?searchtype=author&query=Yokoya%2C+N">Naoto Yokoya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Change detection (CD) is a critical task in studying the dynamics of
ecosystems and human activities using multi-temporal remote sensing images.
While deep learning has shown promising results in CD tasks, it requires a
large number of labeled and paired multi-temporal images to achieve high
performance. Pairing and annotating large-scale multi-temporal remote sensing
images is both expensive and time-consuming. To make deep learning-based CD
techniques more practical and cost-effective, we propose an unsupervised
single-temporal CD framework based on intra- and inter-image patch exchange
(I3PE). The I3PE framework allows for training deep change detectors on
unpaired and unlabeled single-temporal remote sensing images that are readily
available in real-world applications. The I3PE framework comprises four steps:
1) intra-image patch exchange method is based on an object-based image analysis
method and adaptive clustering algorithm, which generates pseudo-bi-temporal
image pairs and corresponding change labels from single-temporal images by
exchanging patches within the image; 2) inter-image patch exchange method can
generate more types of land-cover changes by exchanging patches between images;
3) a simulation pipeline consisting of several image enhancement methods is
proposed to simulate the radiometric difference between pre- and post-event
images caused by different imaging conditions in real situations; 4)
self-supervised learning based on pseudo-labels is applied to further improve
the performance of the change detectors in both unsupervised and
semi-supervised cases. Extensive experiments on two large-scale datasets
demonstrate that I3PE outperforms representative unsupervised approaches and
achieves F1 value improvements of 10.65% and 6.99% to the SOTA method.
Moreover, I3PE can improve the performance of the ... (see the original article
for full abstract)
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00691" title="Abstract">arXiv:2310.00691</a> [<a href="/pdf/2310.00691" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Validation of a Low-Speed and Reverse Driving Articulated Vehicle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gosar%2C+V">Viral Gosar</a>, 
<a href="/search/cs?searchtype=author&query=Alirezaei%2C+M">Mohsen Alirezaei</a>, 
<a href="/search/cs?searchtype=author&query=Besselink%2C+I">Igo Besselink</a>, 
<a href="/search/cs?searchtype=author&query=Nijmeijer%2C+H">Henk Nijmeijer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the 15th International Symposium on Advanced Vehicle Control
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">For the autonomous operation of articulated vehicles at distribution centers,
accurate positioning of the vehicle is of the utmost importance. Automation of
these vehicle poses several challenges, e.g. large swept path, asymmetric
steering response, large slide slip angles of non-steered trailer axles and
trailer instability while reversing. Therefore, a validated vehicle model is
required that accurately and efficiently predicts the states of the vehicle.
Unlike forward driving, open-loop validation methods can not be used for
reverse driving of articulated vehicles due to their unstable dynamics. This
paper proposes an approach to stabilize the unstable pole of the system and
compares three vehicle models (kinematic, non-linear single track and multibody
dynamics model) against real-world test data obtained from low-speed
experiments at a distribution center. It is concluded that single track
non-linear model has a better performance in comparison to other models for
large articulation angles and reverse driving maneuvers.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00692" title="Abstract">arXiv:2310.00692</a> [<a href="/pdf/2310.00692" title="Download PDF">pdf</a>, <a href="/format/2310.00692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Noise Geometry of Stochastic Gradient Descent: A Quantitative and  Analytical Characterization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingze Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lei Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Empirical studies have demonstrated that the noise in stochastic gradient
descent (SGD) aligns favorably with the local geometry of loss landscape.
However, theoretical and quantitative explanations for this phenomenon remain
sparse. In this paper, we offer a comprehensive theoretical investigation into
the aforementioned {\em noise geometry} for over-parameterized linear (OLMs)
models and two-layer neural networks. We scrutinize both average and
directional alignments, paying special attention to how factors like sample
size and input data degeneracy affect the alignment strength. As a specific
application, we leverage our noise geometry characterizations to study how SGD
escapes from sharp minima, revealing that the escape direction has significant
components along flat directions. This is in stark contrast to GD, which
escapes only along the sharpest directions. To substantiate our theoretical
findings, both synthetic and real-world experiments are provided.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00696" title="Abstract">arXiv:2310.00696</a> [<a href="/pdf/2310.00696" title="Download PDF">pdf</a>, <a href="/format/2310.00696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do the Benefits of Joint Models for Relation Extraction Extend to  Document-level Tasks?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saini%2C+P">Pratik Saini</a>, 
<a href="/search/cs?searchtype=author&query=Nayak%2C+T">Tapas Nayak</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+I">Indrajit Bhattacharya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IJCNLP-AACL 2023 (Short)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Two distinct approaches have been proposed for relational triple extraction -
pipeline and joint. Joint models, which capture interactions across triples,
are the more recent development, and have been shown to outperform pipeline
models for sentence-level extraction tasks. Document-level extraction is a more
challenging setting where interactions across triples can be long-range, and
individual triples can also span across sentences. Joint models have not been
applied for document-level tasks so far. In this paper, we benchmark
state-of-the-art pipeline and joint extraction models on sentence-level as well
as document-level datasets. Our experiments show that while joint models
outperform pipeline models significantly for sentence-level extraction, their
performance drops sharply below that of pipeline models for the document-level
dataset.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00697" title="Abstract">arXiv:2310.00697</a> [<a href="/pdf/2310.00697" title="Download PDF">pdf</a>, <a href="/format/2310.00697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning How to Propagate Messages in Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Teng Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Donglin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> KDD2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper studies the problem of learning message propagation strategies for
graph neural networks (GNNs). One of the challenges for graph neural networks
is that of defining the propagation strategy. For instance, the choices of
propagation steps are often specialized to a single graph and are not
personalized to different nodes. To compensate for this, in this paper, we
present learning to propagate, a general learning framework that not only
learns the GNN parameters for prediction but more importantly, can explicitly
learn the interpretable and personalized propagate strategies for different
nodes and various types of graphs. We introduce the optimal propagation steps
as latent variables to help find the maximum-likelihood estimation of the GNN
parameters in a variational Expectation-Maximization (VEM) framework. Extensive
experiments on various types of graph benchmarks demonstrate that our proposed
framework can significantly achieve better performance compared with the
state-of-the-art methods, and can effectively learn personalized and
interpretable propagate strategies of messages in GNNs.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00698" title="Abstract">arXiv:2310.00698</a> [<a href="/pdf/2310.00698" title="Download PDF">pdf</a>, <a href="/format/2310.00698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comics for Everyone: Generating Accessible Text Descriptions for Comic  Strips
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramaprasad%2C+R">Reshma Ramaprasad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CLVL: 5th Workshop On Closing The Loop Between Vision And Language (ICCV 2023 Workshop)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Comic strips are a popular and expressive form of visual storytelling that
can convey humor, emotion, and information. However, they are inaccessible to
the BLV (Blind or Low Vision) community, who cannot perceive the images,
layouts, and text of comics. Our goal in this paper is to create natural
language descriptions of comic strips that are accessible to the visually
impaired community. Our method consists of two steps: first, we use computer
vision techniques to extract information about the panels, characters, and text
of the comic images; second, we use this information as additional context to
prompt a multimodal large language model (MLLM) to produce the descriptions. We
test our method on a collection of comics that have been annotated by human
experts and measure its performance using both quantitative and qualitative
metrics. The outcomes of our experiments are encouraging and promising.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00699" title="Abstract">arXiv:2310.00699</a> [<a href="/pdf/2310.00699" title="Download PDF">pdf</a>, <a href="/format/2310.00699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pianist Identification Using Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jingjing Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wiggins%2C+G">Geraint Wiggins</a>, 
<a href="/search/cs?searchtype=author&query=Fazekas%2C+G">Gyorgy Fazekas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, accepted by the 4th International Symposium on the Internet of Sounds, IS2 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper presents a comprehensive study of automatic performer
identification in expressive piano performances using convolutional neural
networks (CNNs) and expressive features. Our work addresses the challenging
multi-class classification task of identifying virtuoso pianists, which has
substantial implications for building dynamic musical instruments with
intelligence and smart musical systems. Incorporating recent advancements, we
leveraged large-scale expressive piano performance datasets and deep learning
techniques. We refined the scores by expanding repetitions and ornaments for
more accurate feature extraction. We demonstrated the capability of
one-dimensional CNNs for identifying pianists based on expressive features and
analyzed the impact of the input sequence lengths and different features. The
proposed model outperforms the baseline, achieving 85.3% accuracy in a 6-way
identification task. Our refined dataset proved more apt for training a robust
pianist identifier, making a substantial contribution to the field of automatic
performer identification. Our codes have been released at
https://github.com/BetsyTang/PID-CNN.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00702" title="Abstract">arXiv:2310.00702</a> [<a href="/pdf/2310.00702" title="Download PDF">pdf</a>, <a href="/format/2310.00702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> You Do Not Need Additional Priors in Camouflage Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuchen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Heng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Junjie Xie</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yongqiang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhongbo Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Camouflage object detection (COD) poses a significant challenge due to the
high resemblance between camouflaged objects and their surroundings. Although
current deep learning methods have made significant progress in detecting
camouflaged objects, many of them heavily rely on additional prior information.
However, acquiring such additional prior information is both expensive and
impractical in real-world scenarios. Therefore, there is a need to develop a
network for camouflage object detection that does not depend on additional
priors. In this paper, we propose a novel adaptive feature aggregation method
that effectively combines multi-layer feature information to generate guidance
information. In contrast to previous approaches that rely on edge or ranking
priors, our method directly leverages information extracted from image features
to guide model training. Through extensive experimental results, we demonstrate
that our proposed method achieves comparable or superior performance when
compared to state-of-the-art approaches.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00703" title="Abstract">arXiv:2310.00703</a> [<a href="/pdf/2310.00703" title="Download PDF">pdf</a>, <a href="/format/2310.00703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Study of Training Objectives for Clarification Facet  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+S">Shiyu Ni</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+K">Keping Bi</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiafeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Due to the ambiguity and vagueness of a user query, it is essential to
identify the query facets for the clarification of user intents. Existing work
on query facet generation has achieved compelling performance by sequentially
predicting the next facet given previously generated facets based on
pre-trained language generation models such as BART. Given a query, there are
mainly two types of training objectives to guide the facet generation models.
One is to generate the default sequence of ground-truth facets, and the other
is to enumerate all the permutations of ground-truth facets and use the
sequence that has the minimum loss for model updates. The second is
permutation-invariant while the first is not. In this paper, we aim to conduct
a systematic comparative study of various types of training objectives, with
different properties of not only whether it is permutation-invariant but also
whether it conducts sequential prediction and whether it can control the count
of output facets. To this end, we propose another three training objectives of
different aforementioned properties. For comprehensive comparisons, besides the
commonly used evaluation that measures the matching with ground-truth facets,
we also introduce two diversity metrics to measure the diversity of the
generated facets. Based on an open-domain query facet dataset, i.e., MIMICS, we
conduct extensive analyses and show the pros and cons of each method, which
could shed light on model training for clarification facet generation. The code
can be found at \url{https://github.com/ShiyuNee/Facet-Generation}
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00704" title="Abstract">arXiv:2310.00704</a> [<a href="/pdf/2310.00704" title="Download PDF">pdf</a>, <a href="/format/2310.00704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniAudio: An Audio Foundation Model Toward Universal Audio Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Dongchao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Jinchuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Rongjie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Songxiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xuankai Chang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiatong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Sheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xixin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhou Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+H">Helen Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Language models (LMs) have demonstrated the capability to handle a variety of
generative tasks. This paper presents the UniAudio system, which, unlike prior
task-specific approaches, leverages LMs techniques to generate multiple types
of audio (including speech, sounds, music, and singing) with given input
conditions. UniAudio 1) first tokenizes all types of target audio along with
other condition modalities, 2) concatenates source-target pair as a single
sequence, and 3) performs next-token prediction using LMs. Also, a multi-scale
Transformer model is proposed to handle the overly long sequences caused by the
residual vector quantization based neural codec in tokenization. Training of
UniAudio is scaled up to 165K hours of audio and 1B parameters, based on all
generative tasks, aiming to obtain sufficient prior knowledge not only in the
intrinsic properties of audio but also the inter-relationship between audio and
other modalities. Therefore, the trained UniAudio model has the potential to
become a foundation model for universal audio generation: it shows strong
capability in all trained tasks and can seamlessly support new audio generation
tasks after simple fine-tuning. Experiments demonstrate that UniAudio achieves
state-of-the-art or at least competitive results on most of the 11 tasks. Demo
and code are released at https://github.com/yangdongchao/UniAudio
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00705" title="Abstract">arXiv:2310.00705</a> [<a href="/pdf/2310.00705" title="Download PDF">pdf</a>, <a href="/ps/2310.00705" title="Download PostScript">ps</a>, <a href="/format/2310.00705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concurrent Hyperproperties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Finkbeiner%2C+B">Bernd Finkbeiner</a>, 
<a href="/search/cs?searchtype=author&query=Olderog%2C+E">Ernst-R&#xfc;diger Olderog</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Theories of Programming and Formal Methods 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Trace properties, which are sets of execution traces, are often used to
analyze systems, but their expressiveness is limited. Clarkson and Schneider
defined hyperproperties as a generalization of trace properties to sets of sets
of traces. Typical applications of hyperproperties are found in information
flow security. We introduce an analogous definition of concurrent
hyperproperties, by generalizing traces to concurrent traces, which we define
as partially ordered multisets. We take Petri nets as the basic semantic model.
Concurrent traces are formalized via causal nets. To check concurrent
hyperproperties, we define may and must testing of sets of concurrent traces in
the style of DeNicola and Hennessy, using the parallel composition of Petri
nets. In our approach, we thus distinguish nondeterministic and concurrent
behavior. We discuss examples where concurrent hyperproperties are needed.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00706" title="Abstract">arXiv:2310.00706</a> [<a href="/pdf/2310.00706" title="Download PDF">pdf</a>, <a href="/format/2310.00706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alharthi%2C+D">Dareen Alharthi</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Roshan Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Dhamyal%2C+H">Hira Dhamyal</a>, 
<a href="/search/cs?searchtype=author&query=Maiti%2C+S">Soumi Maiti</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Rita Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Modern speech synthesis systems have improved significantly, with synthetic
speech being indistinguishable from real speech. However, efficient and
holistic evaluation of synthetic speech still remains a significant challenge.
Human evaluation using Mean Opinion Score (MOS) is ideal, but inefficient due
to high costs. Therefore, researchers have developed auxiliary automatic
metrics like Word Error Rate (WER) to measure intelligibility. Prior works
focus on evaluating synthetic speech based on pre-trained speech recognition
models, however, this can be limiting since this approach primarily measures
speech intelligibility. In this paper, we propose an evaluation technique
involving the training of an ASR model on synthetic speech and assessing its
performance on real speech. Our main assumption is that by training the ASR
model on the synthetic speech, the WER on real speech reflects the similarity
between distributions, a broader assessment of synthetic speech quality beyond
intelligibility. Our proposed metric demonstrates a strong correlation with
both MOS naturalness and MOS intelligibility when compared to SpeechLMScore and
MOSNet on three recent Text-to-Speech (TTS) systems: MQTTS, StyleTTS, and
YourTTS.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00708" title="Abstract">arXiv:2310.00708</a> [<a href="/pdf/2310.00708" title="Download PDF">pdf</a>, <a href="/format/2310.00708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple Yet Effective Strategy to Robustify the Meta Learning Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Y">Yiqin Lv</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yanghe Feng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zheng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jincai Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Meta learning is a promising paradigm to enable skill transfer across tasks.
Most previous methods employ the empirical risk minimization principle in
optimization. However, the resulting worst fast adaptation to a subset of tasks
can be catastrophic in risk-sensitive scenarios. To robustify fast adaptation,
this paper optimizes meta learning pipelines from a distributionally robust
perspective and meta trains models with the measure of expected tail risk. We
take the two-stage strategy as heuristics to solve the robust meta learning
problem, controlling the worst fast adaptation cases at a certain probabilistic
level. Experimental results show that our simple method can improve the
robustness of meta learning to task distributions and reduce the conditional
expectation of the worst fast adaptation risk.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00710" title="Abstract">arXiv:2310.00710</a> [<a href="/pdf/2310.00710" title="Download PDF">pdf</a>, <a href="/format/2310.00710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How well does LLM generate security tests?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+W">Wenjia Song</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Zhengjie Ji</a>, 
<a href="/search/cs?searchtype=author&query=Danfeng">Danfeng</a> (Daphne)Yao, 
<a href="/search/cs?searchtype=author&query=Meng%2C+N">Na Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Developers often build software on top of third-party libraries (Libs) to
improve programmer productivity and software quality. The libraries may contain
vulnerabilities exploitable by hackers to attack the applications (Apps) built
on top of them. People refer to such attacks as \textbf{supply chain attacks},
the documented number of which has increased 742% in 2022. People created tools
to mitigate such attacks, by scanning the library dependencies of Apps,
identifying the usage of vulnerable library versions, and suggesting secure
alternatives to vulnerable dependencies. However, recent studies show that many
developers do not trust the reports by these tools; they ask for code or
evidence to demonstrate how library vulnerabilities lead to security exploits,
in order to assess vulnerability severity and modification necessity.
Unfortunately, manually crafting demos of application-specific attacks is
challenging and time-consuming, and there is insufficient tool support to
automate that procedure.
<br />In this study, we used ChatGPT-4.0 to generate security tests, and to
demonstrate how vulnerable library dependencies facilitate the supply chain
attacks to given Apps. We explored various prompt styles/templates, and found
that \tool generated tests for all \totalClient Apps, demonstrating
\completeECount attacks successfully. It outperformed two state-of-the-art
security test generators -- TRANSFER and SIEGE -- by generating a lot more
tests and achieving more exploits. \tool worked better when prompts described
more on the vulnerabilities, possible exploits, and code context. Our research
will shed light on new research in security test generation. The generated
tests will help developers create secure by design and secure by default
software.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00711" title="Abstract">arXiv:2310.00711</a> [<a href="/pdf/2310.00711" title="Download PDF">pdf</a>, <a href="/format/2310.00711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Data Repair: Are We Ready to Deploy?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+W">Wei Ni</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+X">Xiaoye Miao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yangyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jianwei Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 51 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Data quality is paramount in today's data-driven world, especially in the era
of generative AI. Dirty data with errors and inconsistencies usually leads to
flawed insights, unreliable decision-making, and biased or low-quality outputs
from generative models. The study of repairing erroneous data has gained
significant importance. Existing data repair algorithms differ in information
utilization, problem settings, and are tested in limited scenarios. In this
paper, we initially compare and summarize these algorithms using a new guided
information-based taxonomy. We then systematically conduct a comprehensive
evaluation of 12 mainstream data repair algorithms under the settings of
various data error rates, error types, and downstream analysis tasks, assessing
their error reduction performance with a novel metric. Also, we develop an
effective and unified repair optimization strategy that substantially benefits
the state of the arts, as empirically confirmed. We demonstrate that, the pure
clean data may not necessarily yield the best performance in data analysis
tasks and data is always worth repairing regardless of error rate. Based on the
found observations and insights, we provide some practical guidelines for 5
scenarios and 2 main data analysis tasks. We anticipate this paper enabling
researchers and users to well understand and deploy data repair algorithms in
practice. Finally, we outline research challenges and promising future
directions in the data repair field.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00712" title="Abstract">arXiv:2310.00712</a> [<a href="/pdf/2310.00712" title="Download PDF">pdf</a>, <a href="/format/2310.00712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logical Bias Learning for Object Relation Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xinyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Zihan Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+A">Anna Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Scene graph generation (SGG) aims to automatically map an image into a
semantic structural graph for better scene understanding. It has attracted
significant attention for its ability to provide object and relation
information, enabling graph reasoning for downstream tasks. However, it faces
severe limitations in practice due to the biased data and training method. In
this paper, we present a more rational and effective strategy based on causal
inference for object relation prediction. To further evaluate the superiority
of our strategy, we propose an object enhancement module to conduct ablation
studies. Experimental results on the Visual Gnome 150 (VG-150) dataset
demonstrate the effectiveness of our proposed method. These contributions can
provide great potential for foundation models for decision-making.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00713" title="Abstract">arXiv:2310.00713</a> [<a href="/pdf/2310.00713" title="Download PDF">pdf</a>, <a href="/format/2310.00713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safety-Critical Control of Nonholonomic Vehicles in Dynamic Environments  using Velocity Obstacles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Haraldsen%2C+A">Aurora Haraldsen</a>, 
<a href="/search/eess?searchtype=author&query=Wiig%2C+M+S">Martin S. Wiig</a>, 
<a href="/search/eess?searchtype=author&query=Ames%2C+A+D">Aaron D. Ames</a>, 
<a href="/search/eess?searchtype=author&query=Pettersen%2C+K+Y">Kristin Y. Pettersen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to 2024 American Control Conference (ACC), 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">This paper considers collision avoidance for vehicles with first-order
nonholonomic constraints maintaining nonzero forward speeds, moving within
dynamic environments. We leverage the concept of control barrier functions
(CBFs) to synthesize control inputs that prioritize safety, where the safety
criteria are derived from the velocity obstacle principle. Existing
instantiations of CBFs for collision avoidance, e.g., based on maintaining a
minimal distance, can result in control inputs that make the vehicle stop or
even reverse. The proposed formulation effectively separates speed control from
steering, allowing the vehicle to maintain a forward motion without
compromising safety. This is beneficial for ensuring that the vehicle advances
towards its desired destination, and it is moreover an underlying requirement
for certain vehicles such as marine vessels and fixed-wing UAVs. Theoretical
safety guarantees are provided, and numerical simulations demonstrate the
efficiency of the strategy in environments containing moving obstacles.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00715" title="Abstract">arXiv:2310.00715</a> [<a href="/pdf/2310.00715" title="Download PDF">pdf</a>, <a href="/format/2310.00715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient MPC for Emergency Evasive Maneuvers, Part I: Hybridization of  the Nonlinear Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gharavi%2C+L">Leila Gharavi</a>, 
<a href="/search/eess?searchtype=author&query=De+Schutter%2C+B">Bart De Schutter</a>, 
<a href="/search/eess?searchtype=author&query=Baldi%2C+S">Simone Baldi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures, submitted to IEEE Transactions on Intelligent Vehicles
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Despite the extensive application of nonlinear Model Predictive Control (MPC)
in automated driving, balancing its computational efficiency with respect to
the control performance and constraint satisfaction remains a challenge in
emergency scenarios: in such situations, sub-optimal but computationally fast
responses are more valuable than optimal responses obtained after long
computations. In this paper, we introduce a hybridization approach for
efficient approximation of nonlinear vehicle dynamics and non-convex
constraints using a hybrid systems modeling framework. Hybridization allows to
reformulate the nonlinear MPC problem during emergency evasive maneuvers as a
hybrid MPC problem. In this regard, Max-Min-Plus-Scaling (MMPS) hybrid modeling
is used to approximate the nonlinear vehicle dynamics. Meanwhile, different
formulations for constraint approximation are presented, and various
grid-generation methods are compared to solve these approximation problems.
Among these, two novel grid types are introduced to structurally include the
influence of the system dynamics on the grid point distributions in the state
domain. Overall, the work presents and compares three hybrid models and four
hybrid constraints for efficient MPC synthesis and offers guidelines for
implementation of the presented hybridization framework in other applications.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00716" title="Abstract">arXiv:2310.00716</a> [<a href="/pdf/2310.00716" title="Download PDF">pdf</a>, <a href="/format/2310.00716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient MPC for Emergency Evasive Maneuvers, Part II: Comparative  Assessment for Hybrid Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gharavi%2C+L">Leila Gharavi</a>, 
<a href="/search/eess?searchtype=author&query=De+Schutter%2C+B">Bart De Schutter</a>, 
<a href="/search/eess?searchtype=author&query=Baldi%2C+S">Simone Baldi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures, submitted to IEEE Transactions on Intelligent Vehicles
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Optimization-based approaches such as Model Predictive Control (MPC) are
promising approaches in proactive control for safety-critical applications with
changing environments such as automated driving systems. However, the
computational complexity of the MPC optimization problem coupled with the need
for real-time control in hazardous scenarios is the main bottleneck in
realization of automation levels four and five for driving systems. In this
paper, we construct hybrid formulations of the nonlinear MPC problem for
tracking control during emergency evasive maneuvers and assess their
computational efficiency in terms of accuracy and solution time. To hybridize
the MPC problem, we combine three hybrid approximations of the prediction model
and four approximations of the nonlinear stability and tire saturation
constraints and simulate the closed-loop behavior of the resulting controllers
during five emergency maneuvers for different prediction horizons. Further, we
compare the robustness of the controllers in the presence of friction
uncertainty as well to assess the accuracy-time trade-off in cases where the
friction of the road is either unknown or has an offset error with respect to
the prediction model. This robustness is studied for different levels of
friction uncertainty, as well as investigated with respect to the proximity to
the vehicle handling limits. We show that the hybridization of the MPC problem
is an efficient approach for real-time implementation of MPC during emergency
evasive maneuvers, paving the way for implementation of high levels of
automation.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00718" title="Abstract">arXiv:2310.00718</a> [<a href="/pdf/2310.00718" title="Download PDF">pdf</a>, <a href="/format/2310.00718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LintQ: A Static Analysis Framework for Qiskit Quantum Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paltenghi%2C+M">Matteo Paltenghi</a>, 
<a href="/search/cs?searchtype=author&query=Pradel%2C+M">Michael Pradel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">As quantum computing is rising in popularity, the amount of quantum programs
and the number of developers writing them are increasing rapidly.
Unfortunately, writing correct quantum programs is challenging due to various
subtle rules developers need to be aware of. Empirical studies show that 40-82%
of all bugs in quantum software are specific to the quantum domain. Yet,
existing static bug detection frameworks are mostly unaware of quantum-specific
concepts, such as circuits, gates, and qubits, and hence miss many bugs. This
paper presents LintQ, a comprehensive static analysis framework for detecting
bugs in quantum programs. Our approach is enabled by a set of abstractions
designed to reason about common concepts in quantum computing without referring
to the details of the underlying quantum computing platform. Built on top of
these abstractions, LintQ offers an extensible set of nine analyses that detect
likely bugs, such as operating on corrupted quantum states, redundant
measurements, and incorrect compositions of sub-circuits. We apply the approach
to a newly collected dataset of 7,568 real-world Qiskit-based quantum programs,
showing that LintQ effectively identifies various programming problems with a
precision of 80.5%. Comparing to a general-purpose linter and two existing,
quantum-aware techniques shows that all problems found by LintQ during our
evaluation are missed by prior work. LintQ hence takes an important step toward
reliable software in the growing field of quantum computing.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00723" title="Abstract">arXiv:2310.00723</a> [<a href="/pdf/2310.00723" title="Download PDF">pdf</a>, <a href="/format/2310.00723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HOH: Markerless Multimodal Human-Object-Human Handover Dataset with  Large Object Count
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wiederhold%2C+N">Noah Wiederhold</a>, 
<a href="/search/cs?searchtype=author&query=Megyeri%2C+A">Ava Megyeri</a>, 
<a href="/search/cs?searchtype=author&query=Paris%2C+D">DiMaggio Paris</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Sean Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+N+K">Natasha Kholgade Banerjee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">We present the HOH (Human-Object-Human) Handover Dataset, a large object
count dataset with 136 objects, to accelerate data-driven research on handover
studies, human-robot handover implementation, and artificial intelligence (AI)
on handover parameter estimation from 2D and 3D data of person interactions.
HOH contains multi-view RGB and depth data, skeletons, fused point clouds,
grasp type and handedness labels, object, giver hand, and receiver hand 2D and
3D segmentations, giver and receiver comfort ratings, and paired object
metadata and aligned 3D models for 2,720 handover interactions spanning 136
objects and 20 giver-receiver pairs-40 with role-reversal-organized from 40
participants. We also show experimental results of neural networks trained
using HOH to perform grasp, orientation, and trajectory prediction. As the only
fully markerless handover capture dataset, HOH represents natural human-human
handover interactions, overcoming challenges with markered datasets that
require specific suiting for body tracking, and lack high-resolution hand
tracking. To date, HOH is the largest handover dataset in number of objects,
participants, pairs with role reversal accounted for, and total interactions
captured.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00724" title="Abstract">arXiv:2310.00724</a> [<a href="/pdf/2310.00724" title="Download PDF">pdf</a>, <a href="/format/2310.00724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subtractive Mixture Models via Squaring: Representation and Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Loconte%2C+L">Lorenzo Loconte</a>, 
<a href="/search/cs?searchtype=author&query=Sladek%2C+A+M">Aleksanteri M. Sladek</a>, 
<a href="/search/cs?searchtype=author&query=Mengel%2C+S">Stefan Mengel</a>, 
<a href="/search/cs?searchtype=author&query=Trapp%2C+M">Martin Trapp</a>, 
<a href="/search/cs?searchtype=author&query=Solin%2C+A">Arno Solin</a>, 
<a href="/search/cs?searchtype=author&query=Gillis%2C+N">Nicolas Gillis</a>, 
<a href="/search/cs?searchtype=author&query=Vergari%2C+A">Antonio Vergari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Mixture models are traditionally represented and learned by adding several
distributions as components. Allowing mixtures to subtract probability mass or
density can drastically reduce the number of components needed to model complex
distributions. However, learning such subtractive mixtures while ensuring they
still encode a non-negative function is challenging. We investigate how to
learn and perform inference on deep subtractive mixtures by squaring them. We
do this in the framework of probabilistic circuits, which enable us to
represent tensorized mixtures and generalize several other subtractive models.
We theoretically prove that the class of squared circuits allowing subtractions
can be exponentially more expressive than traditional additive mixtures; and,
we empirically show this increased expressiveness on a series of real-world
distribution estimation tasks.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00725" title="Abstract">arXiv:2310.00725</a> [<a href="/pdf/2310.00725" title="Download PDF">pdf</a>, <a href="/ps/2310.00725" title="Download PostScript">ps</a>, <a href="/format/2310.00725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Averaging Property of Wedge Product and Naturality in Discrete Exterior  Calculus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Berwick-Evans%2C+D">Daniel Berwick-Evans</a>, 
<a href="/search/math?searchtype=author&query=Hirani%2C+A+N">Anil N. Hirani</a>, 
<a href="/search/math?searchtype=author&query=Schubel%2C+M+D">Mark D. Schubel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2104.10277">arXiv:2104.10277</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Differential Geometry (math.DG)

</div>
<p class="mathjax">In exterior calculus on smooth manifolds, the exterior derivative and wedge
product are natural with respect to smooth maps between manifolds, that is,
these operations commute with pullback. In discrete exterior calculus (DEC),
simplicial cochains play the role of discrete forms, the coboundary operator
serves as the discrete exterior derivative, and the antisymmetrized cup product
provides a discrete wedge product. In this paper we show that these discrete
operations in DEC are natural with respect to abstract simplicial maps. A
second contribution is a new combinatorial averaging interpretation of the
discrete wedge product in DEC.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00726" title="Abstract">arXiv:2310.00726</a> [<a href="/pdf/2310.00726" title="Download PDF">pdf</a>, <a href="/format/2310.00726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Length-Generalization in Transformers via Task Hinting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Awasthi%2C+P">Pranjal Awasthi</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Anupam Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">It has been observed in recent years that transformers have problems with
length generalization for certain types of reasoning and arithmetic tasks. In
particular, the performance of a transformer model trained on tasks (say
addition) up to a certain length (e.g., 5 digit numbers) drops sharply when
applied to longer instances of the same problem. This work proposes an approach
based on task hinting towards addressing length generalization. Our key idea is
that while training the model on task-specific data, it is helpful to
simultaneously train the model to solve a simpler but related auxiliary task as
well.
<br />We study the classical sorting problem as a canonical example to evaluate our
approach. We design a multitask training framework and show that task hinting
significantly improve length generalization. For sorting we show that it is
possible to train models on data consisting of sequences having length at most
$20$, and improve the test accuracy on sequences of length $100$ from less than
1% (for standard training) to more than 92% (via task hinting).
<br />Our study uncovers several interesting aspects of length generalization. We
observe that while several auxiliary tasks may seem natural a priori, their
effectiveness in improving length generalization differs dramatically. We
further use probing and visualization-based techniques to understand the
internal mechanisms via which the model performs the task, and propose a
theoretical construction consistent with the observed learning behaviors of the
model. Based on our construction, we show that introducing a small number of
length dependent parameters into the training procedure can further boost the
performance on unseen lengths. Finally, we also show the efficacy of our task
hinting based approach beyond sorting, giving hope that these techniques will
be applicable in broader contexts.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00727" title="Abstract">arXiv:2310.00727</a> [<a href="/pdf/2310.00727" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Review of deep learning in healthcare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zargar%2C+H+H">Hasan Hejbari Zargar</a>, 
<a href="/search/cs?searchtype=author&query=Zargar%2C+S+H">Saha Hejbari Zargar</a>, 
<a href="/search/cs?searchtype=author&query=Mehri%2C+R">Raziye Mehri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Given the growing complexity of healthcare data over the last several years,
using machine learning techniques like Deep Neural Network (DNN) models has
gained increased appeal. In order to extract hidden patterns and other valuable
information from the huge quantity of health data, which traditional analytics
are unable to do in a reasonable length of time, machine learning (ML)
techniques are used. Deep Learning (DL) algorithms in particular have been
shown as potential approaches to pattern identification in healthcare systems.
This thought has led to the contribution of this research, which examines deep
learning methods used in healthcare systems via an examination of cutting-edge
network designs, applications, and market trends. To connect deep learning
methodologies and human healthcare interpretability, the initial objective is
to provide in-depth insight into the deployment of deep learning models in
healthcare solutions. And last, to outline the current unresolved issues and
potential directions.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00728" title="Abstract">arXiv:2310.00728</a> [<a href="/pdf/2310.00728" title="Download PDF">pdf</a>, <a href="/format/2310.00728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Informed Graph Neural Network for Dynamic Reconfiguration of  Power Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Authier%2C+J">Jules Authier</a>, 
<a href="/search/cs?searchtype=author&query=Haider%2C+R">Rabab Haider</a>, 
<a href="/search/cs?searchtype=author&query=Annaswamy%2C+A">Anuradha Annaswamy</a>, 
<a href="/search/cs?searchtype=author&query=Dorfler%2C+F">Florian Dorfler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">To maintain a reliable grid we need fast decision-making algorithms for
complex problems like Dynamic Reconfiguration (DyR). DyR optimizes distribution
grid switch settings in real-time to minimize grid losses and dispatches
resources to supply loads with available generation. DyR is a mixed-integer
problem and can be computationally intractable to solve for large grids and at
fast timescales. We propose GraPhyR, a Physics-Informed Graph Neural Network
(GNNs) framework tailored for DyR. We incorporate essential operational and
connectivity constraints directly within the GNN framework and train it
end-to-end. Our results show that GraPhyR is able to learn to optimize the DyR
task.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00729" title="Abstract">arXiv:2310.00729</a> [<a href="/pdf/2310.00729" title="Download PDF">pdf</a>, <a href="/format/2310.00729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral Neural Networks: Approximation Theory and Optimization  Landscape
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenghui Li</a>, 
<a href="/search/cs?searchtype=author&query=Sonthalia%2C+R">Rishi Sonthalia</a>, 
<a href="/search/cs?searchtype=author&query=Trillos%2C+N+G">Nicolas Garcia Trillos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Analysis of PDEs (math.AP); Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
<p class="mathjax">There is a large variety of machine learning methodologies that are based on
the extraction of spectral geometric information from data. However, the
implementations of many of these methods often depend on traditional
eigensolvers, which present limitations when applied in practical online big
data scenarios. To address some of these challenges, researchers have proposed
different strategies for training neural networks as alternatives to
traditional eigensolvers, with one such approach known as Spectral Neural
Network (SNN). In this paper, we investigate key theoretical aspects of SNN.
First, we present quantitative insights into the tradeoff between the number of
neurons and the amount of spectral geometric information a neural network
learns. Second, we initiate a theoretical exploration of the optimization
landscape of SNN's objective to shed light on the training dynamics of SNN.
Unlike typical studies of convergence to global solutions of NN training
dynamics, SNN presents an additional complexity due to its non-convex ambient
loss function.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00731" title="Abstract">arXiv:2310.00731</a> [<a href="/pdf/2310.00731" title="Download PDF">pdf</a>, <a href="/format/2310.00731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ranked Enumeration for MSO on Trees via Knowledge Compilation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amarilli%2C+A">Antoine Amarilli</a>, 
<a href="/search/cs?searchtype=author&query=Bourhis%2C+P">Pierre Bourhis</a>, 
<a href="/search/cs?searchtype=author&query=Capelli%2C+F">Florent Capelli</a>, 
<a href="/search/cs?searchtype=author&query=Monet%2C+M">Mika&#xeb;l Monet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages; submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Data Structures and Algorithms (cs.DS); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We study the problem of enumerating the satisfying assignments for circuit
classes from knowledge compilation, where assignments are ranked in a specific
order. In particular, we show how this problem can be used to efficiently
perform ranked enumeration of the answers to MSO queries over trees, with the
order being given by a ranking function satisfying a subset-monotonicity
property.
<br />Assuming that the number of variables is constant, we show that we can
enumerate the satisfying assignments in ranked order for so-called multivalued
circuits that are smooth, decomposable, and in negation normal form (smooth
multivalued DNNF). There is no preprocessing and the enumeration delay is
linear in the size of the circuit times the number of values, plus a
logarithmic term in the number of assignments produced so far. If we further
assume that the circuit is deterministic (smooth multivalued d-DNNF), we can
achieve linear-time preprocessing in the circuit, and the delay only features
the logarithmic term.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00734" title="Abstract">arXiv:2310.00734</a> [<a href="/pdf/2310.00734" title="Download PDF">pdf</a>, <a href="/format/2310.00734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Sentiment Analysis for Low Resource languages Using Data  Augmentation Approaches: A Case Study in Marathi
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pingle%2C+A">Aabha Pingle</a>, 
<a href="/search/cs?searchtype=author&query=Vyawahare%2C+A">Aditya Vyawahare</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+I">Isha Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Tangsali%2C+R">Rahul Tangsali</a>, 
<a href="/search/cs?searchtype=author&query=Kale%2C+G">Geetanjali Kale</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+R">Raviraj Joshi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Sentiment analysis plays a crucial role in understanding the sentiment
expressed in text data. While sentiment analysis research has been extensively
conducted in English and other Western languages, there exists a significant
gap in research efforts for sentiment analysis in low-resource languages.
Limited resources, including datasets and NLP research, hinder the progress in
this area. In this work, we present an exhaustive study of data augmentation
approaches for the low-resource Indic language Marathi. Although
domain-specific datasets for sentiment analysis in Marathi exist, they often
fall short when applied to generalized and variable-length inputs. To address
this challenge, this research paper proposes four data augmentation techniques
for sentiment analysis in Marathi. The paper focuses on augmenting existing
datasets to compensate for the lack of sufficient resources. The primary
objective is to enhance sentiment analysis model performance in both in-domain
and cross-domain scenarios by leveraging data augmentation strategies. The data
augmentation approaches proposed showed a significant performance improvement
for cross-domain accuracies. The augmentation methods include paraphrasing,
back-translation; BERT-based random token replacement, named entity
replacement, and pseudo-label generation; GPT-based text and label generation.
Furthermore, these techniques can be extended to other low-resource languages
and for general text classification tasks.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00737" title="Abstract">arXiv:2310.00737</a> [<a href="/pdf/2310.00737" title="Download PDF">pdf</a>, <a href="/format/2310.00737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenAI Against Humanity: Nefarious Applications of Generative Artificial  Intelligence and Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to CACM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)
are marvels of technology; celebrated for their prowess in natural language
processing and multimodal content generation, they promise a transformative
future. But as with all powerful tools, they come with their shadows. Picture
living in a world where deepfakes are indistinguishable from reality, where
synthetic identities orchestrate malicious campaigns, and where targeted
misinformation or scams are crafted with unparalleled precision. Welcome to the
darker side of GenAI applications.
<br />This article is not just a journey through the meanders of potential misuse
of GenAI and LLMs, but also a call to recognize the urgency of the challenges
ahead. As we navigate the seas of misinformation campaigns, malicious content
generation, and the eerie creation of sophisticated malware, we'll uncover the
societal implications that ripple through the GenAI revolution we are
witnessing. From AI-powered botnets on social media platforms to the unnerving
potential of AI to generate fabricated identities, or alibis made of synthetic
realities, the stakes have never been higher.
<br />The lines between the virtual and the real worlds are blurring, and the
consequences of potential GenAI's nefarious applications impact us all. This
article serves both as a synthesis of rigorous research presented on the risks
of GenAI and misuse of LLMs and as a thought-provoking vision of the different
types of harmful GenAI applications we might encounter in the near future, and
some ways we can prepare for them.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00740" title="Abstract">arXiv:2310.00740</a> [<a href="/pdf/2310.00740" title="Download PDF">pdf</a>, <a href="/format/2310.00740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Top-down Green-ups: Satellite Sensing and Deep Models to Predict  Buffelgrass Phenology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rosenblatt%2C+L">Lucas Rosenblatt</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bin Han</a>, 
<a href="/search/cs?searchtype=author&query=Posthumus%2C+E">Erin Posthumus</a>, 
<a href="/search/cs?searchtype=author&query=Crimmins%2C+T">Theresa Crimmins</a>, 
<a href="/search/cs?searchtype=author&query=Howe%2C+B">Bill Howe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">An invasive species of grass known as "buffelgrass" contributes to severe
wildfires and biodiversity loss in the Southwest United States. We tackle the
problem of predicting buffelgrass "green-ups" (i.e. readiness for herbicidal
treatment). To make our predictions, we explore temporal, visual and
multi-modal models that combine satellite sensing and deep learning. We find
that all of our neural-based approaches improve over conventional buffelgrass
green-up models, and discuss how neural model deployment promises significant
resource savings.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00741" title="Abstract">arXiv:2310.00741</a> [<a href="/pdf/2310.00741" title="Download PDF">pdf</a>, <a href="/format/2310.00741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FELM: Benchmarking Factuality Evaluation of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shiqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yiran Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinghan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chern%2C+I">I-Chun Chern</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Siyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junxian He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023 Track on Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Assessing factuality of text generated by large language models (LLMs) is an
emerging yet crucial research area, aimed at alerting users to potential errors
and guiding the development of more reliable LLMs. Nonetheless, the evaluators
assessing factuality necessitate suitable evaluation themselves to gauge
progress and foster advancements. This direction remains under-explored,
resulting in substantial impediments to the progress of factuality evaluators.
To mitigate this issue, we introduce a benchmark for Factuality Evaluation of
large Language Models, referred to as felm. In this benchmark, we collect
responses generated from LLMs and annotate factuality labels in a fine-grained
manner. Contrary to previous studies that primarily concentrate on the
factuality of world knowledge (e.g.~information from Wikipedia), felm focuses
on factuality across diverse domains, spanning from world knowledge to math and
reasoning. Our annotation is based on text segments, which can help pinpoint
specific factual errors. The factuality annotations are further supplemented by
predefined error types and reference links that either support or contradict
the statement. In our experiments, we investigate the performance of several
LLM-based factuality evaluators on felm, including both vanilla LLMs and those
augmented with retrieval mechanisms and chain-of-thought processes. Our
findings reveal that while retrieval aids factuality evaluation, current LLMs
are far from satisfactory to faithfully detect factual errors.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00744" title="Abstract">arXiv:2310.00744</a> [<a href="/pdf/2310.00744" title="Download PDF">pdf</a>, <a href="/format/2310.00744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Feedback Control of Power Systems with Solar Plants and Composite  Loads
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nadeem%2C+M">Muhammad Nadeem</a>, 
<a href="/search/eess?searchtype=author&query=Bahavarnia%2C+M">MirSaleh Bahavarnia</a>, 
<a href="/search/eess?searchtype=author&query=Taha%2C+A+F">Ahmad F. Taha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Due to the rapid developments in synchronized measurement technologies, there
exist enormous opportunities to attenuate disturbances in future power grids
with high penetration of renewables and complex load demands. To that end, this
paper investigates the effectiveness of new robust feedback controllers for
interconnected power systems with advanced power electronics-based models of
photovoltaic (PV) power plants, composite load dynamics, and detailed
higher-order synchronous generator models. Specifically, we design new,
advanced control-theoretic wide-area controllers to improve the transient
stability of nonlinear differential-algebraic models. Thorough simulation
studies are carried out to assess the performance of the proposed controllers.
Several fundamental questions on the proposed controllers' computational
complexity and disturbance attenuation performance are raised and addressed.
Simulation results demonstrate that with the proposed controllers as a
secondary control layer, the overall transient stability and system robustness
against load and renewables disturbances/uncertainties can be significantly
improved compared to the state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00745" title="Abstract">arXiv:2310.00745</a> [<a href="/pdf/2310.00745" title="Download PDF">pdf</a>, <a href="/format/2310.00745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deterministic Langevin Unconstrained Optimization with Normalizing Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sullivan%2C+J+M">James M. Sullivan</a>, 
<a href="/search/cs?searchtype=author&query=Seljak%2C+U">Uros Seljak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 9 figures, 1 table, submitted to JOGO
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We introduce a global, gradient-free surrogate optimization strategy for
expensive black-box functions inspired by the Fokker-Planck and Langevin
equations. These can be written as an optimization problem where the objective
is the target function to maximize minus the logarithm of the current density
of evaluated samples. This objective balances exploitation of the target
objective with exploration of low-density regions. The method, Deterministic
Langevin Optimization (DLO), relies on a Normalizing Flow density estimate to
perform active learning and select proposal points for evaluation. This
strategy differs qualitatively from the widely-used acquisition functions
employed by Bayesian Optimization methods, and can accommodate a range of
surrogate choices. We demonstrate superior or competitive progress toward
objective optima on standard synthetic test functions, as well as on non-convex
and multi-modal posteriors of moderate dimension. On real-world objectives,
such as scientific and neural network hyperparameter optimization, DLO is
competitive with state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00746" title="Abstract">arXiv:2310.00746</a> [<a href="/pdf/2310.00746" title="Download PDF">pdf</a>, <a href="/format/2310.00746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities  of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z+M">Zekun Moore Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Z">Zhongyuan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Que%2C+H">Haoran Que</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wangchunshu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuhan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hongcheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+R">Ruitong Gan</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+Z">Zehao Ni</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Man Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoxiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Ke Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+J">Junran Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, repo at <a href="https://github.com/InteractiveNLP-Team/RoleLLM-public">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The advent of Large Language Models (LLMs) has paved the way for complex
tasks such as role-playing, which enhances user interactions by enabling models
to imitate various characters. However, the closed-source nature of
state-of-the-art LLMs and their general-purpose training limit role-playing
optimization. In this paper, we introduce RoleLLM, a framework to benchmark,
elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four
stages: (1) Role Profile Construction for 100 roles; (2) Context-Based
Instruction Generation (Context-Instruct) for role-specific knowledge
extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style
imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning
open-source models along with role customization. By Context-Instruct and
RoleGPT, we create RoleBench, the first systematic and fine-grained
character-level benchmark dataset for role-playing with 168,093 samples.
Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese),
significantly enhancing role-playing abilities and even achieving comparable
results with RoleGPT (using GPT-4).
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00749" title="Abstract">arXiv:2310.00749</a> [<a href="/pdf/2310.00749" title="Download PDF">pdf</a>, <a href="/format/2310.00749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEED: Simple, Efficient, and Effective Data Management via Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=CHen%2C+Z">Zui CHen</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Lei Cao</a>, 
<a href="/search/cs?searchtype=author&query=Madden%2C+S">Sam Madden</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Ju Fan</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+N">Nan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Z">Zihui Gu</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+Z">Zeyuan Shang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chunwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cafarella%2C+M">Michael Cafarella</a>, 
<a href="/search/cs?searchtype=author&query=Kraska%2C+T">Tim Kraska</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint, 17 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce SEED, an LLM-centric system that allows users to easily create
efficient, and effective data management applications. SEED comprises three
main components: code generation, model generation, and augmented LLM query to
address the challenges that LLM services are computationally and economically
expensive and do not always work well on all cases for a given data management
task. SEED addresses the expense challenge by localizing LLM computation as
much as possible. This includes replacing most of LLM calls with local code,
local models, and augmenting LLM queries with batching and data access tools,
etc. To ensure effectiveness, SEED features a bunch of optimization techniques
to enhance the localized solution and the LLM queries, including automatic code
validation, code ensemble, model representatives selection, selective tool
usages, etc. Moreover, with SEED users are able to easily construct a data
management solution customized to their applications. It allows the users to
configure each component and compose an execution pipeline in natural language.
SEED then automatically compiles it into an executable program. We showcase the
efficiency and effectiveness of SEED using diverse data management tasks such
as data imputation, NL2SQL translation, etc., achieving state-of-the-art
few-shot performance while significantly reducing the number of required LLM
calls.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00750" title="Abstract">arXiv:2310.00750</a> [<a href="/pdf/2310.00750" title="Download PDF">pdf</a>, <a href="/ps/2310.00750" title="Download PostScript">ps</a>, <a href="/format/2310.00750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Copeland Winners in Dueling Bandits with Indifferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bengs%2C+V">Viktor Bengs</a>, 
<a href="/search/cs?searchtype=author&query=Haddenhorst%2C+B">Bj&#xf6;rn Haddenhorst</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCllermeier%2C+E">Eyke H&#xfc;llermeier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider the task of identifying the Copeland winner(s) in a dueling
bandits problem with ternary feedback. This is an underexplored but practically
relevant variant of the conventional dueling bandits problem, in which, in
addition to strict preference between two arms, one may observe feedback in the
form of an indifference. We provide a lower bound on the sample complexity for
any learning algorithm finding the Copeland winner(s) with a fixed error
probability. Moreover, we propose POCOWISTA, an algorithm with a sample
complexity that almost matches this lower bound, and which shows excellent
empirical performance, even for the conventional dueling bandits problem. For
the case where the preference probabilities satisfy a specific type of
stochastic transitivity, we provide a refined version with an improved worst
case sample complexity.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00752" title="Abstract">arXiv:2310.00752</a> [<a href="/pdf/2310.00752" title="Download PDF">pdf</a>, <a href="/format/2310.00752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TIGERScore: Towards Building Explainable Metric for All Text Generation  Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+D">Dongfu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yishan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B+Y">Bill Yuchen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 3 figures, 16 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present TIGERScore, a \textbf{T}rained metric that follows
\textbf{I}nstruction \textbf{G}uidance to perform \textbf{E}xplainable, and
\textbf{R}eference-free evaluation over a wide spectrum of text generation
tasks. Different from other automatic evaluation methods that only provide
arcane scores, TIGERScore is guided by the natural language instruction to
provide error analysis to pinpoint the mistakes in the generated text. Our
metric is based on LLaMA, trained on our meticulously curated
instruction-tuning dataset MetricInstruct which covers 6 text generation tasks
and 23 text generation datasets. The dataset consists of 48K quadruple in the
form of (instruction, input, system output $\rightarrow$ error analysis). We
collected the `system outputs' through diverse channels to cover different
types of errors. To quantitatively assess our metric, we evaluate its
correlation with human ratings on 5 held-in datasets, 2 held-out datasets and
show that TIGERScore can achieve the highest overall Spearman's correlation
with human ratings across these datasets and outperforms other metrics
significantly. As a reference-free metric, its correlation can even surpass the
best existing reference-based metrics. To further qualitatively assess the
rationale generated by our metric, we conduct human evaluation on the generated
explanations and found that the explanations are 70.8\% accurate. Through these
experimental results, we believe TIGERScore demonstrates the possibility of
building universal explainable metrics to evaluate any text generation task.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00754" title="Abstract">arXiv:2310.00754</a> [<a href="/pdf/2310.00754" title="Download PDF">pdf</a>, <a href="/format/2310.00754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing and Mitigating Object Hallucination in Large Vision-Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yiyang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+C">Chenhang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jaehong Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Linjun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhun Deng</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Huaxiu Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Large vision-language models (LVLMs) have shown remarkable abilities in
understanding visual information with human languages. However, LVLMs still
suffer from object hallucination, which is the problem of generating
descriptions that include objects that do not actually exist in the images.
This can negatively impact many vision-language tasks, such as visual
summarization and reasoning. To address this issue, we propose a simple yet
powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify
object hallucination in LVLMs by reconstructing less hallucinatory
descriptions. LURE is grounded in a rigorous statistical analysis of the key
factors underlying object hallucination, including co-occurrence (the frequent
appearance of certain objects alongside others in images), uncertainty (objects
with higher uncertainty during LVLM decoding), and object position
(hallucination often appears in the later part of the generated text). LURE can
also be seamlessly integrated with any LVLMs. We evaluate LURE on six
open-source LVLMs, achieving a 23% improvement in general object hallucination
evaluation metrics over the previous best approach. In both GPT and human
evaluations, LURE consistently ranks at the top. Our data and code are
available at https://github.com/YiyangZhou/LURE.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00757" title="Abstract">arXiv:2310.00757</a> [<a href="/pdf/2310.00757" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mind the Gap: Federated Learning Broadens Domain Generalization in  Diagnostic AI Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arasteh%2C+S+T">Soroosh Tayebi Arasteh</a>, 
<a href="/search/cs?searchtype=author&query=Kuhl%2C+C">Christiane Kuhl</a>, 
<a href="/search/cs?searchtype=author&query=Saehn%2C+M">Marwin-Jonathan Saehn</a>, 
<a href="/search/cs?searchtype=author&query=Isfort%2C+P">Peter Isfort</a>, 
<a href="/search/cs?searchtype=author&query=Truhn%2C+D">Daniel Truhn</a>, 
<a href="/search/cs?searchtype=author&query=Nebelung%2C+S">Sven Nebelung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Developing robust artificial intelligence (AI) models that generalize well to
unseen datasets is challenging and usually requires large and variable
datasets, preferably from multiple institutions. In federated learning (FL), a
model is trained collaboratively at numerous sites that hold local datasets
without exchanging them. So far, the impact of training strategy, i.e., local
versus collaborative, on the diagnostic on-domain and off-domain performance of
AI models interpreting chest radiographs has not been assessed. Consequently,
using 610,000 chest radiographs from five institutions across the globe, we
assessed diagnostic performance as a function of training strategy (i.e., local
vs. collaborative), network architecture (i.e., convolutional vs.
transformer-based), generalization performance (i.e., on-domain vs.
off-domain), imaging finding (i.e., cardiomegaly, pleural effusion, pneumonia,
atelectasis, consolidation, pneumothorax, and no abnormality), dataset size
(i.e., from n=18,000 to 213,921 radiographs), and dataset diversity. Large
datasets not only showed minimal performance gains with FL but, in some
instances, even exhibited decreases. In contrast, smaller datasets revealed
marked improvements. Thus, on-domain performance was mainly driven by training
data size. However, off-domain performance leaned more on training diversity.
When trained collaboratively across diverse external institutions, AI models
consistently surpassed models trained locally for off-domain tasks, emphasizing
FL's potential in leveraging data diversity. In conclusion, FL can bolster
diagnostic privacy, reproducibility, and off-domain reliability of AI models
and, potentially, optimize healthcare outcomes.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00758" title="Abstract">arXiv:2310.00758</a> [<a href="/pdf/2310.00758" title="Download PDF">pdf</a>, <a href="/format/2310.00758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven adaptive building thermal controller tuning with  constraints: A primal-dual contextual Bayesian optimization approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xu%2C+W">Wenjie Xu</a>, 
<a href="/search/eess?searchtype=author&query=Svetozarevic%2C+B">Bratislav Svetozarevic</a>, 
<a href="/search/eess?searchtype=author&query=Di+Natale%2C+L">Loris Di Natale</a>, 
<a href="/search/eess?searchtype=author&query=Heer%2C+P">Philipp Heer</a>, 
<a href="/search/eess?searchtype=author&query=Jones%2C+C+N">Colin N Jones</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the problem of tuning the parameters of a room temperature
controller to minimize its energy consumption, subject to the constraint that
the daily cumulative thermal discomfort of the occupants is below a given
threshold. We formulate it as an online constrained black-box optimization
problem where, on each day, we observe some relevant environmental context and
adaptively select the controller parameters. In this paper, we propose to use a
data-driven Primal-Dual Contextual Bayesian Optimization (PDCBO) approach to
solve this problem. In a simulation case study on a single room, we apply our
algorithm to tune the parameters of a Proportional Integral (PI) heating
controller and the pre-heating time. Our results show that PDCBO can save up to
4.7% energy consumption compared to other state-of-the-art Bayesian
optimization-based methods while keeping the daily thermal discomfort below the
given tolerable threshold on average. Additionally, PDCBO can automatically
track time-varying tolerable thresholds while existing methods fail to do so.
We then study an alternative constrained tuning problem where we aim to
minimize the thermal discomfort with a given energy budget. With this
formulation, PDCBO reduces the average discomfort by up to 63% compared to
state-of-the-art safe optimization methods while keeping the average daily
energy consumption below the required threshold.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00760" title="Abstract">arXiv:2310.00760</a> [<a href="/pdf/2310.00760" title="Download PDF">pdf</a>, <a href="/format/2310.00760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-aware hybrid paradigm of nonlinear MPC and model-based RL  for offroad navigation: Exploration of transformers in the predictive model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lotfi%2C+F">Faraz Lotfi</a>, 
<a href="/search/cs?searchtype=author&query=Virji%2C+K">Khalil Virji</a>, 
<a href="/search/cs?searchtype=author&query=Faraji%2C+F">Farnoosh Faraji</a>, 
<a href="/search/cs?searchtype=author&query=Berry%2C+L">Lucas Berry</a>, 
<a href="/search/cs?searchtype=author&query=Holliday%2C+A">Andrew Holliday</a>, 
<a href="/search/cs?searchtype=author&query=Meger%2C+D">David Meger</a>, 
<a href="/search/cs?searchtype=author&query=Dudek%2C+G">Gregory Dudek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this paper, we investigate a hybrid scheme that combines nonlinear model
predictive control (MPC) and model-based reinforcement learning (RL) for
navigation planning of an autonomous model car across offroad, unstructured
terrains without relying on predefined maps. Our innovative approach takes
inspiration from BADGR, an LSTM-based network that primarily concentrates on
environment modeling, but distinguishes itself by substituting LSTM modules
with transformers to greatly elevate the performance our model. Addressing
uncertainty within the system, we train an ensemble of predictive models and
estimate the mutual information between model weights and outputs, facilitating
dynamic horizon planning through the introduction of variable speeds. Further
enhancing our methodology, we incorporate a nonlinear MPC controller that
accounts for the intricacies of the vehicle's model and states. The model-based
RL facet produces steering angles and quantifies inherent uncertainty. At the
same time, the nonlinear MPC suggests optimal throttle settings, striking a
balance between goal attainment speed and managing model uncertainty influenced
by velocity. In the conducted studies, our approach excels over the existing
baseline by consistently achieving higher metric values in predicting future
events and seamlessly integrating the vehicle's kinematic model for enhanced
decision-making. The code and the evaluation data are available at
https://github.com/FARAZLOTFI/offroad_autonomous_navigation/).
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00761" title="Abstract">arXiv:2310.00761</a> [<a href="/pdf/2310.00761" title="Download PDF">pdf</a>, <a href="/format/2310.00761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Image Generation for adversarially robust and  interpretable Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bischof%2C+R">Rafael Bischof</a>, 
<a href="/search/cs?searchtype=author&query=Scheidegger%2C+F">Florian Scheidegger</a>, 
<a href="/search/cs?searchtype=author&query=Kraus%2C+M+A">Michael A. Kraus</a>, 
<a href="/search/cs?searchtype=author&query=Malossi%2C+A+C+I">A. Cristiano I. Malossi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Neural Image Classifiers are effective but inherently hard to interpret and
susceptible to adversarial attacks. Solutions to both problems exist, among
others, in the form of counterfactual examples generation to enhance
explainability or adversarially augment training datasets for improved
robustness. However, existing methods exclusively address only one of the
issues. We propose a unified framework leveraging image-to-image translation
Generative Adversarial Networks (GANs) to produce counterfactual samples that
highlight salient regions for interpretability and act as adversarial samples
to augment the dataset for more robustness. This is achieved by combining the
classifier and discriminator into a single model that attributes real images to
their respective classes and flags generated images as "fake". We assess the
method's effectiveness by evaluating (i) the produced explainability masks on a
semantic segmentation task for concrete cracks and (ii) the model's resilience
against the Projected Gradient Descent (PGD) attack on a fruit defects
detection problem. Our produced saliency maps are highly descriptive, achieving
competitive IoU values compared to classical segmentation models despite being
trained exclusively on classification labels. Furthermore, the model exhibits
improved robustness to adversarial attacks, and we show how the discriminator's
"fakeness" value serves as an uncertainty measure of the predictions.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00762" title="Abstract">arXiv:2310.00762</a> [<a href="/pdf/2310.00762" title="Download PDF">pdf</a>, <a href="/ps/2310.00762" title="Download PostScript">ps</a>, <a href="/format/2310.00762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A note on the stabilizer formalism via noncommutative graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Araiza%2C+R">Roy Araiza</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jihong Cai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yushan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Holtermann%2C+A">Abraham Holtermann</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chieh Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Mohan%2C+T">Tushar Mohan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Peixue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zeyuan Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Operator Algebras (math.OA); Quantum Physics (quant-ph)

</div>
<p class="mathjax">In this short note we formulate a stabilizer formalism in the language of
noncommutative graphs. The classes of noncommutative graphs we consider are
obtained via unitary representations of compact groups, and suitably chosen
operators on finite-dimensional Hilbert spaces. Furthermore, in this framework,
we generalize previous results in this area for determining when such
noncommutative graphs have anticliques.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00763" title="Abstract">arXiv:2310.00763</a> [<a href="/pdf/2310.00763" title="Download PDF">pdf</a>, <a href="/format/2310.00763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Efficient Power Flow Learning for Network Contingencies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pareek%2C+P">Parikshit Pareek</a>, 
<a href="/search/cs?searchtype=author&query=Deka%2C+D">Deepjyoti Deka</a>, 
<a href="/search/cs?searchtype=author&query=Misra%2C+S">Sidhant Misra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This work presents an efficient data-driven method to learn power flows in
grids with network contingencies and to estimate corresponding probabilistic
voltage envelopes (PVE). First, a network-aware Gaussian process (GP) termed
Vertex-Degree Kernel (VDK-GP), developed in prior work, is used to estimate
voltage-power functions for a few network configurations. The paper introduces
a novel multi-task vertex degree kernel (MT-VDK) that amalgamates the learned
VDK-GPs to determine power flows for unseen networks, with a significant
reduction in the computational complexity and hyperparameter requirements
compared to alternate approaches. Simulations on the IEEE 30-Bus network
demonstrate the retention and transfer of power flow knowledge in both N-1 and
N-2 contingency scenarios. The MT-VDK-GP approach achieves over 50% reduction
in mean prediction error for novel N-1 contingency network configurations in
low training data regimes (50-250 samples) over VDK-GP. Additionally, MT-VDK-GP
outperforms a hyper-parameter based transfer learning approach in over 75% of
N-2 contingency network structures, even without historical N-2 outage data.
The proposed method demonstrates the ability to achieve PVEs using sixteen
times fewer power flow solutions compared to Monte-Carlo sampling-based
methods.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00766" title="Abstract">arXiv:2310.00766</a> [<a href="/pdf/2310.00766" title="Download PDF">pdf</a>, <a href="/ps/2310.00766" title="Download PostScript">ps</a>, <a href="/format/2310.00766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Preview of Open-Loop and Feedback Nash Trajectories in Racing  Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rowold%2C+M">Matthias Rowold</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was presented at the IROS 2023 Workshop on Multi-agent Dynamic Games on October 1st, 2023 in Detroit as a virtual presentation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Trajectory planning for autonomous race cars poses special challenges due to
the highly interactive and competitive environment. Prior work has applied game
theory as it provides equilibria for such non-cooperative dynamic problems.
This contribution introduces a framework to assess the suitability of the Nash
equilibrium for racing scenarios. To achieve this, we employ a variant of iLQR,
called iLQGame, to find trajectories that satisfy the equilibrium conditions
for a linear-quadratic approximation of the original game. In particular, we
are interested in the difference between the behavioral outcomes of the
open-loop and the feedback Nash equilibria and show how iLQGame can generate
both types of equilibria. We provide an overview of open problems and upcoming
research, including convergence properties of iLQGame in racing games, cost
function parameterization, and moving horizon implementations.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00768" title="Abstract">arXiv:2310.00768</a> [<a href="/pdf/2310.00768" title="Download PDF">pdf</a>, <a href="/format/2310.00768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anywhere &amp; Everywhere: A Mobile, Immersive, and Ubiquitous Vision for  Data Analytics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elmqvist%2C+N">Niklas Elmqvist</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Communications of the ACM, December 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Data is collected everywhere in our increasingly instrumented world and
people are increasingly wanting to access this data from anywhere in it. This
kind of anywhere &amp; everywhere data present new challenges and opportunities for
data-driven sensemaking and decision-making that will require leveraging novel
mobile, immersive, and ubiquitous technologies undergirded by recent advances
in human cognition. In this paper, we examine these emerging forms of analytics
that are transforming how data analysis will be conducted in the future: in an
ecosystem of connected devices, interactive visualizations, and collaborating
users with vast amounts of data and analytical mechanisms available at their
fingertips.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00769" title="Abstract">arXiv:2310.00769</a> [<a href="/pdf/2310.00769" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Critical Analysis and Countermeasures Tactics, Techniques and Procedures  (TTPs) that targeting civilians: A case study On Pegasus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hussien%2C+O">Osama Hussien</a>, 
<a href="/search/cs?searchtype=author&query=Butt%2C+U">Usman Butt</a>, 
<a href="/search/cs?searchtype=author&query=Sulaiman%2C+R+B">Rejwan Bin Sulaiman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Individuals, businesses, and governments all face additional difficulties
because of the rise of sophisticated cyberattack attacks. This paper
investigates the targeting of journalists and activists by the malware Pegasus.
To gain a deeper understanding of the tactics utilized by cybercriminals and
the vulnerabilities that facilitate their scope, this research looks on
numerous occurrences and identifies recurring patterns in the strategies,
methods, and practices employed. In this paper, a comprehensive analysis is
conducted on the far-reaching consequences of these attacks for cybersecurity
policy, encompassing the pressing need for enhanced threat intelligence sharing
mechanisms, the implementation of more resilient incident response protocols,
and the allocation of greater financial resources towards the advancement of
cybersecurity research and development initiatives. The research also discusses
how Pegasus will affect SCADA systems and critical infrastructure, and it
describes some of the most important tactics that businesses may use to reduce
the danger of cyberattacks and safeguard themselves against the 21st century's
growing threats. The extent of Pegasus spyware, which can access various data
and communications on mobile devices running iOS and Android potentially
jeopardise the civil rights and privacy of journalists, activists, and
political leaders throughout the world, was found to be worrying
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00771" title="Abstract">arXiv:2310.00771</a> [<a href="/pdf/2310.00771" title="Download PDF">pdf</a>, <a href="/format/2310.00771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-training with Synthetic Data Helps Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zecheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Che Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zixuan Dong</a>, 
<a href="/search/cs?searchtype=author&query=Ross%2C+K">Keith Ross</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, it has been shown that for offline deep reinforcement learning
(DRL), pre-training Decision Transformer with a large language corpus can
improve downstream performance (Reid et al., 2022). A natural question to ask
is whether this performance gain can only be achieved with language
pre-training, or can be achieved with simpler pre-training schemes which do not
involve language. In this paper, we first show that language is not essential
for improved performance, and indeed pre-training with synthetic IID data for a
small number of updates can match the performance gains from pre-training with
a large language corpus; moreover, pre-training with data generated by a
one-step Markov chain can further improve the performance. Inspired by these
experimental results, we then consider pre-training Conservative Q-Learning
(CQL), a popular offline DRL algorithm, which is Q-learning-based and typically
employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training
with simple synthetic data for a small number of updates can also improve CQL,
providing consistent performance improvement on D4RL Gym locomotion datasets.
The results of this paper not only illustrate the importance of pre-training
for offline DRL but also show that the pre-training data can be synthetic and
generated with remarkably simple mechanisms.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00772" title="Abstract">arXiv:2310.00772</a> [<a href="/pdf/2310.00772" title="Download PDF">pdf</a>, <a href="/format/2310.00772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMOOT: Saliency Guided Mask Optimized Online Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karkehabadi%2C+A">Ali Karkehabadi</a>, 
<a href="/search/cs?searchtype=author&query=Sasan%2C+A">Avesta Sasan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep Neural Networks are powerful tools for understanding complex patterns
and making decisions. However, their black-box nature impedes a complete
understanding of their inner workings. Saliency-Guided Training (SGT) methods
try to highlight the prominent features in the model's training based on the
output to alleviate this problem. These methods use back-propagation and
modified gradients to guide the model toward the most relevant features while
keeping the impact on the prediction accuracy negligible. SGT makes the model's
final result more interpretable by masking input partially. In this way,
considering the model's output, we can infer how each segment of the input
affects the output. In the particular case of image as the input, masking is
applied to the input pixels. However, the masking strategy and number of pixels
which we mask, are considered as a hyperparameter. Appropriate setting of
masking strategy can directly affect the model's training. In this paper, we
focus on this issue and present our contribution. We propose a novel method to
determine the optimal number of masked images based on input, accuracy, and
model loss during the training. The strategy prevents information loss which
leads to better accuracy values. Also, by integrating the model's performance
in the strategy formula, we show that our model represents the salient features
more meaningful. Our experimental results demonstrate a substantial improvement
in both model accuracy and the prominence of saliency, thereby affirming the
effectiveness of our proposed solution.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00773" title="Abstract">arXiv:2310.00773</a> [<a href="/pdf/2310.00773" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Categorizing Flight Paths using Data Visualization and Clustering  Methodologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yifan Song</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Keyang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Young%2C+S">Seth Young</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in the 9th International Conference on Research in Air Transportation (ICRAT'20): <a href="https://www.icrat.org/previous-conferences/9th-international-conference/papers/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This work leverages the U.S. Federal Aviation Administration's Traffic Flow
Management System dataset and DV8, a recently developed tool for highly
interactive visualization of air traffic data, to develop clustering algorithms
for categorizing air traffic by their varying flight paths. Two clustering
methodologies, a spatial-based geographic distance model, and a vector-based
cosine similarity model, are demonstrated and compared for their clustering
effectiveness. Examples of their applications reveal successful, realistic
clustering based on automated clustering result determination and
human-in-the-loop processes, with geographic distance algorithms performing
better for enroute portions of flight paths and cosine similarity algorithms
performing better for near-terminal operations, such as arrival paths. A point
extraction technique is applied to improve computation efficiency.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00781" title="Abstract">arXiv:2310.00781</a> [<a href="/pdf/2310.00781" title="Download PDF">pdf</a>, <a href="/format/2310.00781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mining Java Memory Errors using Subjective Interesting Subgroups with  Hierarchical Targets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Remil%2C+Y">Youcef Remil</a>, 
<a href="/search/cs?searchtype=author&query=Bendimerad%2C+A">Anes Bendimerad</a>, 
<a href="/search/cs?searchtype=author&query=Chambard%2C+M">Mathieu Chambard</a>, 
<a href="/search/cs?searchtype=author&query=Mathonat%2C+R">Romain Mathonat</a>, 
<a href="/search/cs?searchtype=author&query=Plantevit%2C+M">Marc Plantevit</a>, 
<a href="/search/cs?searchtype=author&query=Kaytoue%2C+M">Mehdi Kaytoue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
<p class="mathjax">Software applications, especially Enterprise Resource Planning (ERP) systems,
are crucial to the day-to-day operations of many industries. Therefore, it is
essential to maintain these systems effectively using tools that can identify,
diagnose, and mitigate their incidents. One promising data-driven approach is
the Subgroup Discovery (SD) technique, a data mining method that can
automatically mine incident datasets and extract discriminant patterns to
identify the root causes of issues. However, current SD solutions have
limitations in handling complex target concepts with multiple attributes
organized hierarchically. To illustrate this scenario, we examine the case of
Java out-of-memory incidents among several possible applications. We have a
dataset that describes these incidents, including their context and the types
of Java objects occupying memory when it reaches saturation, with these types
arranged hierarchically. This scenario inspires us to propose a novel Subgroup
Discovery approach that can handle complex target concepts with hierarchies. To
achieve this, we design a pattern syntax and a quality measure that ensure the
identified subgroups are relevant, non-redundant, and resilient to noise. To
achieve the desired quality measure, we use the Subjective Interestingness
model that incorporates prior knowledge about the data and promotes patterns
that are both informative and surprising relative to that knowledge. We apply
this framework to investigate out-of-memory errors and demonstrate its
usefulness in incident diagnosis. To validate the effectiveness of our approach
and the quality of the identified patterns, we present an empirical study. The
source code and data used in the evaluation are publicly accessible, ensuring
transparency and reproducibility.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00782" title="Abstract">arXiv:2310.00782</a> [<a href="/pdf/2310.00782" title="Download PDF">pdf</a>, <a href="/ps/2310.00782" title="Download PostScript">ps</a>, <a href="/format/2310.00782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Deterministic Exact Minimum Weight Cycle and Multi Source  Shortest Paths in Near Linear Rounds in CONGEST model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+U">Udit Agarwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We present new deterministic algorithms for computing distributed weighted
minimum weight cycle (MWC) in undirected and directed graphs and distributed
weighted all nodes shortest cycle (ANSC) in directed graphs. Our algorithms for
these problems run in $\tilde{O}(n)$ rounds in the CONGEST model on graphs with
arbitrary non-negative edge weights, matching the lower bound up to
polylogarithmic factors. Before our work, no near linear rounds deterministic
algorithms were known for these problems. The previous best bound for solving
these problems deterministically requires an initial computation of all pairs
shortest paths (APSP) on the given graph, followed by post-processing of $O(n)$
rounds, and in total takes $\tilde{O}(n^{4/3})$ rounds, using deterministic
APSP~\cite{AR-SPAA20}.
<br />The main component of our new $\tilde{O}(n)$ rounds algorithms is a
deterministic technique for constructing a sequence of successive blocker sets.
These blocker sets are then treated as source nodes to compute $h$-hop shortest
paths, which can then be used to compute candidate shortest cycles whose hop
length lies in a particular range. The shortest cycles can then be obtained by
selecting the cycle with the minimum weight from all these candidate cycles.
<br />Additionally using the above blocker set sequence technique, we also obtain
$\tilde{O}(n)$ rounds deterministic algorithm for the multi-source shortest
paths problem (MSSP) for both directed and undirected graphs, given that the
size of the source set is at most $\sqrt{n}$. This new result for MSSP can be a
step towards obtaining a $o(n^{4/3})$ rounds algorithm for deterministic APSP.
We also believe that our new blocker set sequence technique may have potential
applications for other distributed algorithms.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00783" title="Abstract">arXiv:2310.00783</a> [<a href="/pdf/2310.00783" title="Download PDF">pdf</a>, <a href="/format/2310.00783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Propagating Semantic Labels in Video Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balaban%2C+D">David Balaban</a>, 
<a href="/search/cs?searchtype=author&query=Medich%2C+J">Justin Medich</a>, 
<a href="/search/cs?searchtype=author&query=Gosar%2C+P">Pranay Gosar</a>, 
<a href="/search/cs?searchtype=author&query=Hart%2C+J">Justin Hart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Semantic Segmentation combines two sub-tasks: the identification of
pixel-level image masks and the application of semantic labels to those masks.
Recently, so-called Foundation Models have been introduced; general models
trained on very large datasets which can be specialized and applied to more
specific tasks. One such model, the Segment Anything Model (SAM), performs
image segmentation. Semantic segmentation systems such as CLIPSeg and MaskRCNN
are trained on datasets of paired segments and semantic labels. Manual labeling
of custom data, however, is time-consuming. This work presents a method for
performing segmentation for objects in video. Once an object has been found in
a frame of video, the segment can then be propagated to future frames; thus
reducing manual annotation effort. The method works by combining SAM with
Structure from Motion (SfM). The video input to the system is first
reconstructed into 3D geometry using SfM. A frame of video is then segmented
using SAM. Segments identified by SAM are then projected onto the the
reconstructed 3D geometry. In subsequent video frames, the labeled 3D geometry
is reprojected into the new perspective, allowing SAM to be invoked fewer
times. System performance is evaluated, including the contributions of the SAM
and SfM components. Performance is evaluated over three main metrics:
computation time, mask IOU with manual labels, and the number of tracking
losses. Results demonstrate that the system has substantial computation time
improvements over human performance for tracking objects over video frames, but
suffers in performance.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00785" title="Abstract">arXiv:2310.00785</a> [<a href="/pdf/2310.00785" title="Download PDF">pdf</a>, <a href="/format/2310.00785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BooookScore: A systematic exploration of book-length summarization in  the era of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yapei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+K">Kyle Lo</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+T">Tanya Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Iyyer%2C+M">Mohit Iyyer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Summarizing book-length documents (&gt;100K tokens) that exceed the context
window size of large language models (LLMs) requires first breaking the input
document into smaller chunks and then prompting an LLM to merge, update, and
compress chunk-level summaries. Despite the complexity and importance of this
task, it has yet to be meaningfully studied due to the challenges of
evaluation: existing book-length summarization datasets (e.g., BookSum) are in
the pretraining data of most public LLMs, and existing evaluation methods
struggle to capture errors made by modern LLM summarizers. In this paper, we
present the first study of the coherence of LLM-based book-length summarizers
implemented via two prompting workflows: (1) hierarchically merging chunk-level
summaries, and (2) incrementally updating a running summary. We obtain 1193
fine-grained human annotations on GPT-4 generated summaries of 100
recently-published books and identify eight common types of coherence errors
made by LLMs. Because human evaluation is expensive and time-consuming, we
develop an automatic metric, BooookScore, that measures the proportion of
sentences in a summary that do not contain any of the identified error types.
BooookScore has high agreement with human annotations and allows us to
systematically evaluate the impact of many other critical parameters (e.g.,
chunk size, base LLM) while saving $15K and 500 hours in human evaluation
costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce
summaries with higher BooookScore than the oft-repetitive ones generated by
LLaMA 2. Incremental updating yields lower BooookScore but higher level of
detail than hierarchical merging, a trade-off sometimes preferred by human
annotators. We release code and annotations after blind review to spur more
principled research on book-length summarization.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00788" title="Abstract">arXiv:2310.00788</a> [<a href="/pdf/2310.00788" title="Download PDF">pdf</a>, <a href="/format/2310.00788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Web Image Formats: Assessment of Their Real-World-Usage and Performance  across Popular Web Browsers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dornauer%2C+B">Benedikt Dornauer</a>, 
<a href="/search/cs?searchtype=author&query=Felderer%2C+M">Michael Felderer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint: Product-Focused Software Process Improvement 24th International Conference, PROFES 2023, Dornbirn, Austria , Dezember 10-13, 2023, Proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">In 2023, images on the web make up 41% of transmitted data, significantly
impacting the performance of web apps. Fortunately, image formats like WEBP and
AVIF could offer advanced compression and faster page loading, but may face
performance disparities across browsers. Therefore, we conducted performance
evaluations on five major browsers - Chrome, Edge, Safari, Opera, and Firefox -
while comparing four image formats. The results indicate that the newer formats
exhibited notable performance enhancements across all browsers, leading to
shorter loading times. Compared to the compressed JPEG format, WEBP and AVIF
improved the Page Load Time by 21% and 15%, respectively. However, web scraping
revealed that JPEG and PNG still dominate web image choices, with WEBP at 4% as
the most used new format. Through the web scraping and web performance
evaluation, this research serves to (1) explore image format preferences in web
applications and analyze distribution and characteristics across
frequently-visited sites in 2023 and (2) assess the performance impact of
distinct web image formats on application load times across popular web
browsers.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00789" title="Abstract">arXiv:2310.00789</a> [<a href="/pdf/2310.00789" title="Download PDF">pdf</a>, <a href="/ps/2310.00789" title="Download PostScript">ps</a>, <a href="/format/2310.00789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Testing the Limits of Unified Sequence to Sequence LLM Pretraining on  Diverse Table Data Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Soumajyoti Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Lausen%2C+L">Leonard Lausen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Tables stored in databases and tables which are present in web pages and
articles account for a large part of semi-structured data that is available on
the internet. It then becomes pertinent to develop a modeling approach with
large language models (LLMs) that can be used to solve diverse table tasks such
as semantic parsing, question answering as well as classification problems.
Traditionally, there existed separate models specialized for each task
individually. It raises the question of how far can we go to build a unified
model that works well on some table tasks without significant degradation on
others. To that end, we attempt at creating a shared modeling approach in the
pretraining stage with encoder-decoder style LLMs that can cater to diverse
tasks. We evaluate our approach that continually pretrains and finetunes
different model families of T5 with data from tables and surrounding context,
on these downstream tasks at different model scales. Through multiple ablation
studies, we observe that our pretraining with self-supervised objectives can
significantly boost the performance of the models on these tasks. As an example
of one improvement, we observe that the instruction finetuned public models
which come specialized on text question answering (QA) and have been trained on
table data still have room for improvement when it comes to table specific QA.
Our work is the first attempt at studying the advantages of a unified approach
to table specific pretraining when scaled from 770M to 11B sequence to sequence
models while also comparing the instruction finetuned variants of the models.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00791" title="Abstract">arXiv:2310.00791</a> [<a href="/pdf/2310.00791" title="Download PDF">pdf</a>, <a href="/format/2310.00791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Universal Understanding of Color Harmony: Fuzzy Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shamoi%2C+P">Pakizar Shamoi</a>, 
<a href="/search/cs?searchtype=author&query=Muratbekova%2C+M">Muragul Muratbekova</a>, 
<a href="/search/cs?searchtype=author&query=Izbassar%2C+A">Assylzhan Izbassar</a>, 
<a href="/search/cs?searchtype=author&query=Inoue%2C+A">Atsushi Inoue</a>, 
<a href="/search/cs?searchtype=author&query=Kawanaka%2C+H">Hiroharu Kawanaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to FSDM 2023 - The 9th International Conference on Fuzzy Systems and Data Mining
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Harmony level prediction is receiving increasing attention nowadays. Color
plays a crucial role in affecting human aesthetic responses. In this paper, we
explore color harmony using a fuzzy-based color model and address the question
of its universality. For our experiments, we utilize a dataset containing
attractive images from five different domains: fashion, art, nature, interior
design, and brand logos. We aim to identify harmony patterns and dominant color
palettes within these images using a fuzzy approach. It is well-suited for this
task because it can handle the inherent subjectivity and contextual variability
associated with aesthetics and color harmony evaluation. Our experimental
results suggest that color harmony is largely universal. Additionally, our
findings reveal that color harmony is not solely influenced by hue
relationships on the color wheel but also by the saturation and intensity of
colors. In palettes with high harmony levels, we observed a prevalent adherence
to color wheel principles while maintaining moderate levels of saturation and
intensity. These findings contribute to ongoing research on color harmony and
its underlying principles, offering valuable insights for designers, artists,
and researchers in the field of aesthetics.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00793" title="Abstract">arXiv:2310.00793</a> [<a href="/pdf/2310.00793" title="Download PDF">pdf</a>, <a href="/format/2310.00793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Link Prediction: A Data Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Haitao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Shomer%2C+H">Harry Shomer</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bingheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wenqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Neil Shah</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Link prediction, a fundamental task on graphs, has proven indispensable in
various applications, e.g., friend recommendation, protein analysis, and drug
interaction prediction. However, since datasets span a multitude of domains,
they could have distinct underlying mechanisms of link formation. Evidence in
existing literature underscores the absence of a universally best algorithm
suitable for all datasets. In this paper, we endeavor to explore principles of
link prediction across diverse datasets from a data-centric perspective. We
recognize three fundamental factors critical to link prediction: local
structural proximity, global structural proximity, and feature proximity. We
then unearth relationships among those factors where (i) global structural
proximity only shows effectiveness when local structural proximity is
deficient. (ii) The incompatibility can be found between feature and structural
proximity. Such incompatibility leads to GNNs for Link Prediction (GNN4LP)
consistently underperforming on edges where the feature proximity factor
dominates. Inspired by these new insights from a data perspective, we offer
practical instruction for GNN4LP model design and guidelines for selecting
appropriate benchmark datasets for more comprehensive evaluations.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00795" title="Abstract">arXiv:2310.00795</a> [<a href="/pdf/2310.00795" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Review of Generative AI in Healthcare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shokrollahi%2C+Y">Yasin Shokrollahi</a>, 
<a href="/search/cs?searchtype=author&query=Yarmohammadtoosky%2C+S">Sahar Yarmohammadtoosky</a>, 
<a href="/search/cs?searchtype=author&query=Nikahd%2C+M+M">Matthew M. Nikahd</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+P">Pengfei Dong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xianqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+L">Linxia Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages, 16 figures, 1table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The advancement of Artificial Intelligence (AI) has catalyzed revolutionary
changes across various sectors, notably in healthcare. Among the significant
developments in this field are the applications of generative AI models,
specifically transformers and diffusion models. These models have played a
crucial role in analyzing diverse forms of data, including medical imaging
(encompassing image reconstruction, image-to-image translation, image
generation, and image classification), protein structure prediction, clinical
documentation, diagnostic assistance, radiology interpretation, clinical
decision support, medical coding, and billing, as well as drug design and
molecular representation. Such applications have enhanced clinical diagnosis,
data reconstruction, and drug synthesis. This review paper aims to offer a
thorough overview of the generative AI applications in healthcare, focusing on
transformers and diffusion models. Additionally, we propose potential
directions for future research to tackle the existing limitations and meet the
evolving demands of the healthcare sector. Intended to serve as a comprehensive
guide for researchers and practitioners interested in the healthcare
applications of generative AI, this review provides valuable insights into the
current state of the art, challenges faced, and prospective future directions.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00796" title="Abstract">arXiv:2310.00796</a> [<a href="/pdf/2310.00796" title="Download PDF">pdf</a>, <a href="/format/2310.00796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lindemann%2C+M">Matthias Lindemann</a>, 
<a href="/search/cs?searchtype=author&query=Koller%2C+A">Alexander Koller</a>, 
<a href="/search/cs?searchtype=author&query=Titov%2C+I">Ivan Titov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Strong inductive biases enable learning from little data and help
generalization outside of the training distribution. Popular neural
architectures such as Transformers lack strong structural inductive biases for
seq2seq NLP tasks on their own. Consequently, they struggle with systematic
generalization beyond the training distribution, e.g. with extrapolating to
longer inputs, even when pre-trained on large amounts of text. We show how a
structural inductive bias can be injected into a seq2seq model by pre-training
it to simulate structural transformations on synthetic data. Specifically, we
inject an inductive bias towards Finite State Transducers (FSTs) into a
Transformer by pre-training it to simulate FSTs given their descriptions. Our
experiments show that our method imparts the desired inductive bias, resulting
in improved systematic generalization and better few-shot learning for FST-like
tasks.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00797" title="Abstract">arXiv:2310.00797</a> [<a href="/pdf/2310.00797" title="Download PDF">pdf</a>, <a href="/format/2310.00797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Going Beyond Familiar Features for Deep Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sivaprasad%2C+S">Sarath Sivaprasad</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+M">Mario Fritz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Anomaly Detection (AD) is a critical task that involves identifying
observations that do not conform to a learned model of normality. Prior work in
deep AD is predominantly based on a familiarity hypothesis, where familiar
features serve as the reference in a pre-trained embedding space. While this
strategy has proven highly successful, it turns out that it causes consistent
false negatives when anomalies consist of truly novel features that are not
well captured by the pre-trained encoding. We propose a novel approach to AD
using explainability to capture novel features as unexplained observations in
the input space. We achieve strong performance across a wide range of anomaly
benchmarks by combining similarity and novelty in a hybrid approach. Our
approach establishes a new state-of-the-art across multiple benchmarks,
handling diverse anomaly types while eliminating the need for expensive
background models and dense matching. In particular, we show that by taking
account of novel features, we reduce false negative anomalies by up to 40% on
challenging benchmarks compared to the state-of-the-art. Our method gives
visually inspectable explanations for pixel-level anomalies.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00798" title="Abstract">arXiv:2310.00798</a> [<a href="/pdf/2310.00798" title="Download PDF">pdf</a>, <a href="/format/2310.00798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Object manipulation through contact configuration regulation: multiple  and intermittent contacts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taylor%2C+O">Orion Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Doshi%2C+N">Neel Doshi</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+A">Alberto Rodriguez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this work, we build on our method for manipulating unknown objects via
contact configuration regulation: the estimation and control of the location,
geometry, and mode of all contacts between the robot, object, and environment.
We further develop our estimator and controller to enable manipulation through
more complex contact interactions, including intermittent contact between the
robot/object, and multiple contacts between the object/environment. In
addition, we support a larger set of contact geometries at each interface. This
is accomplished through a factor graph based estimation framework that reasons
about the complementary kinematic and wrench constraints of contact to predict
the current contact configuration. We are aided by the incorporation of a
limited amount of visual feedback; which when combined with the available F/T
sensing and robot proprioception, allows us to differentiate contact modes that
were previously indistinguishable. We implement this revamped framework on our
manipulation platform, and demonstrate that it allows the robot to perform a
wider set of manipulation tasks. This includes, using a wall as a support to
re-orient an object, or regulating the contact geometry between the object and
the ground. Finally, we conduct ablation studies to understand the
contributions from visual and tactile feedback in our manipulation framework.
Our code can be found at: https://github.com/mcubelab/pbal.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00800" title="Abstract">arXiv:2310.00800</a> [<a href="/pdf/2310.00800" title="Download PDF">pdf</a>, <a href="/format/2310.00800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphPatcher: Mitigating Degree Bias for Graph Neural Networks via  Test-time Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ju%2C+M">Mingxuan Ju</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Neil Shah</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yanfang Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent studies have shown that graph neural networks (GNNs) exhibit strong
biases towards the node degree: they usually perform satisfactorily on
high-degree nodes with rich neighbor information but struggle with low-degree
nodes. Existing works tackle this problem by deriving either designated GNN
architectures or training strategies specifically for low-degree nodes. Though
effective, these approaches unintentionally create an artificial
out-of-distribution scenario, where models mainly or even only observe
low-degree nodes during the training, leading to a downgraded performance for
high-degree nodes that GNNs originally perform well at. In light of this, we
propose a test-time augmentation framework, namely GraphPatcher, to enhance
test-time generalization of any GNNs on low-degree nodes. Specifically,
GraphPatcher iteratively generates virtual nodes to patch artificially created
low-degree nodes via corruptions, aiming at progressively reconstructing target
GNN's predictions over a sequence of increasingly corrupted nodes. Through this
scheme, GraphPatcher not only learns how to enhance low-degree nodes (when the
neighborhoods are heavily corrupted) but also preserves the original superior
performance of GNNs on high-degree nodes (when lightly corrupted).
Additionally, GraphPatcher is model-agnostic and can also mitigate the degree
bias for either self-supervised or supervised GNNs. Comprehensive experiments
are conducted over seven benchmark datasets and GraphPatcher consistently
enhances common GNNs' overall performance by up to 3.6% and low-degree
performance by up to 6.5%, significantly outperforming state-of-the-art
baselines. The source code is publicly available at
https://github.com/jumxglhf/GraphPatcher.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00804" title="Abstract">arXiv:2310.00804</a> [<a href="/pdf/2310.00804" title="Download PDF">pdf</a>, <a href="/format/2310.00804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Engineering for Wind Energy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marykovskiy%2C+Y">Yuriy Marykovskiy</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+T">Thomas Clark</a>, 
<a href="/search/cs?searchtype=author&query=Day%2C+J">Justin Day</a>, 
<a href="/search/cs?searchtype=author&query=Wiens%2C+M">Marcus Wiens</a>, 
<a href="/search/cs?searchtype=author&query=Henderson%2C+C">Charles Henderson</a>, 
<a href="/search/cs?searchtype=author&query=Quick%2C+J">Julian Quick</a>, 
<a href="/search/cs?searchtype=author&query=Abdallah%2C+I">Imad Abdallah</a>, 
<a href="/search/cs?searchtype=author&query=Sempreviva%2C+A+M">Anna Maria Sempreviva</a>, 
<a href="/search/cs?searchtype=author&query=Calbimonte%2C+J">Jean-Paul Calbimonte</a>, 
<a href="/search/cs?searchtype=author&query=Chatzi%2C+E">Eleni Chatzi</a>, 
<a href="/search/cs?searchtype=author&query=Barber%2C+S">Sarah Barber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">With the rapid evolution of the wind energy sector, there is an
ever-increasing need to create value from the vast amounts of data made
available both from within the domain, as well as from other sectors. This
article addresses the challenges faced by wind energy domain experts in
converting data into domain knowledge, connecting and integrating it with other
sources of knowledge, and making it available for use in next generation
artificially intelligent systems. To this end, this article highlights the role
that knowledge engineering can play in the process of digital transformation of
the wind energy sector. It presents the main concepts underpinning
Knowledge-Based Systems and summarises previous work in the areas of knowledge
engineering and knowledge representation in a manner that is relevant and
accessible to domain experts. A systematic analysis of the current
state-of-the-art on knowledge engineering in the wind energy domain is
performed, with available tools put into perspective by establishing the main
domain actors and their needs and identifying key problematic areas. Finally,
guidelines for further development and improvement are provided.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00806" title="Abstract">arXiv:2310.00806</a> [<a href="/pdf/2310.00806" title="Download PDF">pdf</a>, <a href="/format/2310.00806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Design Principles for Frequentist Sequential Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yunbei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zeevi%2C+A">Assaf Zeevi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Statistics Theory (math.ST)

</div>
<p class="mathjax">We develop a general theory to optimize the frequentist regret for sequential
learning problems, where efficient bandit and reinforcement learning algorithms
can be derived from unified Bayesian principles. We propose a novel
optimization approach to generate "algorithmic beliefs" at each round, and use
Bayesian posteriors to make decisions. The optimization objective to create
"algorithmic beliefs," which we term "Algorithmic Information Ratio,"
represents an intrinsic complexity measure that effectively characterizes the
frequentist regret of any algorithm. To the best of our knowledge, this is the
first systematical approach to make Bayesian-type algorithms prior-free and
applicable to adversarial settings, in a generic and optimal manner. Moreover,
the algorithms are simple and often efficient to implement. As a major
application, we present a novel algorithm for multi-armed bandits that achieves
the "best-of-all-worlds" empirical performance in the stochastic, adversarial,
and non-stationary environments. And we illustrate how these principles can be
used in linear bandits, bandit convex optimization, and reinforcement learning.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00808" title="Abstract">arXiv:2310.00808</a> [<a href="/pdf/2310.00808" title="Download PDF">pdf</a>, <a href="/format/2310.00808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Completing Visual Objects via Bridging Generation and Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yinpeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chung-Ching Lin</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Rita Singh</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents a novel approach to object completion, with the primary
goal of reconstructing a complete object from its partially visible components.
Our method, named MaskComp, delineates the completion process through iterative
stages of generation and segmentation. In each iteration, the object mask is
provided as an additional condition to boost image generation, and, in return,
the generated images can lead to a more accurate mask by fusing the
segmentation of images. We demonstrate that the combination of one generation
and one segmentation stage effectively functions as a mask denoiser. Through
alternation between the generation and segmentation stages, the partial object
mask is progressively refined, providing precise shape guidance and yielding
superior object completion results. Our experiments demonstrate the superiority
of MaskComp over existing approaches, e.g., ControlNet and Stable Diffusion,
establishing it as an effective solution for object completion.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00809" title="Abstract">arXiv:2310.00809</a> [<a href="/pdf/2310.00809" title="Download PDF">pdf</a>, <a href="/format/2310.00809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Causal Foundation Model: on Duality between Causal Inference and  Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jennings%2C+J">Joel Jennings</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chao Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">Foundation models have brought changes to the landscape of machine learning,
demonstrating sparks of human-level intelligence across a diverse array of
tasks. However, a gap persists in complex tasks such as causal inference,
primarily due to challenges associated with intricate reasoning steps and high
numerical precision requirements. In this work, we take a first step towards
building causally-aware foundation models for complex tasks. We propose a
novel, theoretically sound method called Causal Inference with Attention
(CInA), which utilizes multiple unlabeled datasets to perform self-supervised
causal learning, and subsequently enables zero-shot causal inference on unseen
tasks with new data. This is based on our theoretical results that demonstrate
the primal-dual connection between optimal covariate balancing and
self-attention, facilitating zero-shot causal inference through the final layer
of a trained transformer-type architecture. We demonstrate empirically that our
approach CInA effectively generalizes to out-of-distribution datasets and
various real-world datasets, matching or even surpassing traditional
per-dataset causal inference methodologies.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00811" title="Abstract">arXiv:2310.00811</a> [<a href="/pdf/2310.00811" title="Download PDF">pdf</a>, <a href="/format/2310.00811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Backpropagation for MoE Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">One defining characteristic of Mixture-of-Expert (MoE) models is their
capacity for conducting sparse computation via expert routing, leading to
remarkable scalability. However, backpropagation, the cornerstone of deep
learning, requires dense computation, thereby posting challenges in MoE
gradient computations. Here, we introduce SparseMixer, a scalable gradient
estimator that bridges the gap between backpropagation and sparse expert
routing. Unlike typical MoE training which strategically neglects certain
gradient terms for the sake of sparse computation and scalability, SparseMixer
provides scalable gradient approximations for these terms, enabling reliable
gradient estimation in MoE training. Grounded in a numerical ODE framework,
SparseMixer harnesses the mid-point method, a second-order ODE solver, to
deliver precise gradient approximations with negligible computational overhead.
Applying SparseMixer to Switch Transformer on both pre-training and machine
translation tasks, SparseMixer showcases considerable performance gain,
accelerating training convergence up to 2 times.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00813" title="Abstract">arXiv:2310.00813</a> [<a href="/pdf/2310.00813" title="Download PDF">pdf</a>, <a href="/format/2310.00813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OceanNet: A principled neural operator-based digital twin for regional  oceans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chattopadhyay%2C+A">Ashesh Chattopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Gray%2C+M">Michael Gray</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianning Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lowe%2C+A+B">Anna B. Lowe</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ruoying He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Supplementary information can be found in: <a href="https://drive.google.com/file/d/1NoxJLa967naJT787a5-IfZ7f_MmRuZMP/view?usp=sharing">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Chaotic Dynamics (nlin.CD); Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">While data-driven approaches demonstrate great potential in atmospheric
modeling and weather forecasting, ocean modeling poses distinct challenges due
to complex bathymetry, land, vertical structure, and flow non-linearity. This
study introduces OceanNet, a principled neural operator-based digital twin for
ocean circulation. OceanNet uses a Fourier neural operator and
predictor-evaluate-corrector integration scheme to mitigate autoregressive
error growth and enhance stability over extended time scales. A spectral
regularizer counteracts spectral bias at smaller scales. OceanNet is applied to
the northwest Atlantic Ocean western boundary current (the Gulf Stream),
focusing on the task of seasonal prediction for Loop Current eddies and the
Gulf Stream meander. Trained using historical sea surface height (SSH) data,
OceanNet demonstrates competitive forecast skill by outperforming SSH
predictions by an uncoupled, state-of-the-art dynamical ocean model forecast,
reducing computation by 500,000 times. These accomplishments demonstrate the
potential of physics-inspired deep neural operators as cost-effective
alternatives to high-resolution numerical ocean models.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00815" title="Abstract">arXiv:2310.00815</a> [<a href="/pdf/2310.00815" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReAcTable: Enhancing ReAct for Table Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunjia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Henkel%2C+J">Jordan Henkel</a>, 
<a href="/search/cs?searchtype=author&query=Floratou%2C+A">Avrilia Floratou</a>, 
<a href="/search/cs?searchtype=author&query=Cahoon%2C+J">Joyce Cahoon</a>, 
<a href="/search/cs?searchtype=author&query=Deep%2C+S">Shaleen Deep</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+J+M">Jignesh M. Patel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Table Question Answering (TQA) presents a substantial challenge at the
intersection of natural language processing and data analytics. This task
involves answering natural language (NL) questions on top of tabular data,
demanding proficiency in logical reasoning, understanding of data semantics,
and fundamental analytical capabilities. Due to its significance, a substantial
volume of research has been dedicated to exploring a wide range of strategies
aimed at tackling this challenge including approaches that leverage Large
Language Models (LLMs) through in-context learning or Chain-of-Thought (CoT)
prompting as well as approaches that train and fine-tune custom models.
<br />Nonetheless, a conspicuous gap exists in the research landscape, where there
is limited exploration of how innovative foundational research, which
integrates incremental reasoning with external tools in the context of LLMs, as
exemplified by the ReAct paradigm, could potentially bring advantages to the
TQA task. In this paper, we aim to fill this gap, by introducing ReAcTable
(ReAct for Table Question Answering tasks), a framework inspired by the ReAct
paradigm that is carefully enhanced to address the challenges uniquely
appearing in TQA tasks such as interpreting complex data semantics, dealing
with errors generated by inconsistent data and generating intricate data
transformations. ReAcTable relies on external tools such as SQL and Python code
executors, to progressively enhance the data by generating intermediate data
representations, ultimately transforming it into a more accessible format for
answering the questions with greater ease. We demonstrate that ReAcTable
achieves remarkable performance even when compared to fine-tuned approaches. In
particular, it outperforms the best prior result on the WikiTQ benchmark,
achieving an accuracy of 68.0% without requiring training a new model or
fine-tuning.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00816" title="Abstract">arXiv:2310.00816</a> [<a href="/pdf/2310.00816" title="Download PDF">pdf</a>, <a href="/format/2310.00816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sharingan: A Transformer-based Architecture for Gaze Following
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tafasca%2C+S">Samy Tafasca</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Anshul Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Odobez%2C+J">Jean-Marc Odobez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Gaze is a powerful form of non-verbal communication and social interaction
that humans develop from an early age. As such, modeling this behavior is an
important task that can benefit a broad set of application domains ranging from
robotics to sociology. In particular, Gaze Following is defined as the
prediction of the pixel-wise 2D location where a person in the image is
looking. Prior efforts in this direction have focused primarily on CNN-based
architectures to perform the task. In this paper, we introduce a novel
transformer-based architecture for 2D gaze prediction. We experiment with 2
variants: the first one retains the same task formulation of predicting a gaze
heatmap for one person at a time, while the second one casts the problem as a
2D point regression and allows us to perform multi-person gaze prediction with
a single forward pass. This new architecture achieves state-of-the-art results
on the GazeFollow and VideoAttentionTarget datasets. The code for this paper
will be made publicly available.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00818" title="Abstract">arXiv:2310.00818</a> [<a href="/pdf/2310.00818" title="Download PDF">pdf</a>, <a href="/format/2310.00818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ECG-SL: Electrocardiogram(ECG) Segment Learning, a deep learning method  for ECG signal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Han Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huiyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sano%2C+A">Akane Sano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Electrocardiogram (ECG) is an essential signal in monitoring human heart
activities. Researchers have achieved promising results in leveraging ECGs in
clinical applications with deep learning models. However, the mainstream deep
learning approaches usually neglect the periodic and formative attribute of the
ECG heartbeat waveform. In this work, we propose a novel ECG-Segment based
Learning (ECG-SL) framework to explicitly model the periodic nature of ECG
signals. More specifically, ECG signals are first split into heartbeat
segments, and then structural features are extracted from each of the segments.
Based on the structural features, a temporal model is designed to learn the
temporal information for various clinical tasks. Further, due to the fact that
massive ECG signals are available but the labeled data are very limited, we
also explore self-supervised learning strategy to pre-train the models,
resulting significant improvement for downstream tasks. The proposed method
outperforms the baseline model and shows competitive performances compared with
task-specific methods in three clinical applications: cardiac condition
diagnosis, sleep apnea detection, and arrhythmia classification. Further, we
find that the ECG-SL tends to focus more on each heartbeat's peak and ST range
than ResNet by visualizing the saliency maps.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00819" title="Abstract">arXiv:2310.00819</a> [<a href="/pdf/2310.00819" title="Download PDF">pdf</a>, <a href="/format/2310.00819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter-Efficient Tuning Helps Language Model Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+T">Tianci Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 11 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Aligning large language models (LLMs) with human preferences is essential for
safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF)
and direct preference optimization (DPO) with human feedback for alignment.
Nevertheless, they have certain drawbacks. One such limitation is that they can
only align models with one preference at the training time (e.g., they cannot
learn to generate concise responses when the preference data prefers detailed
responses), or have certain constraints for the data format (e.g., DPO only
supports pairwise preference data). To this end, prior works incorporate
controllable generations for alignment to make language models learn multiple
preferences and provide outputs with different preferences during inference if
asked. Controllable generation also offers more flexibility with regard to data
format (e.g., it supports pointwise preference data). Specifically, it uses
different control tokens for different preferences during training and
inference, making LLMs behave differently when required. Current controllable
generation methods either use a special token or hand-crafted prompts as
control tokens, and optimize them together with LLMs. As control tokens are
typically much lighter than LLMs, this optimization strategy may not
effectively optimize control tokens. To this end, we first use
parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to
optimize control tokens and then fine-tune models for controllable generations,
similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning
(MEET), improves the quality of control tokens, thus improving controllable
generation quality consistently by an apparent margin on two well-recognized
datasets compared with prior works.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00820" title="Abstract">arXiv:2310.00820</a> [<a href="/pdf/2310.00820" title="Download PDF">pdf</a>, <a href="/format/2310.00820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Determining the Optimal Number of Clusters for Time Series Datasets with  Symbolic Pattern Forest
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raihan%2C+M+N">Md Nishat Raihan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Clustering algorithms are among the most widely used data mining methods due
to their exploratory power and being an initial preprocessing step that paves
the way for other techniques. But the problem of calculating the optimal number
of clusters (say k) is one of the significant challenges for such methods. The
most widely used clustering algorithms like k-means and k-shape in time series
data mining also need the ground truth for the number of clusters that need to
be generated. In this work, we extended the Symbolic Pattern Forest algorithm,
another time series clustering algorithm, to determine the optimal number of
clusters for the time series datasets. We used SPF to generate the clusters
from the datasets and chose the optimal number of clusters based on the
Silhouette Coefficient, a metric used to calculate the goodness of a clustering
technique. Silhouette was calculated on both the bag of word vectors and the
tf-idf vectors generated from the SAX words of each time series. We tested our
approach on the UCR archive datasets, and our experimental results so far
showed significant improvement over the baseline.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00824" title="Abstract">arXiv:2310.00824</a> [<a href="/pdf/2310.00824" title="Download PDF">pdf</a>, <a href="/ps/2310.00824" title="Download PostScript">ps</a>, <a href="/format/2310.00824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-dissipative spectral renormalization exponential integrator  method for gradient flow problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hou%2C+D">Dianming Hou</a>, 
<a href="/search/math?searchtype=author&query=Ju%2C+L">Lili Ju</a>, 
<a href="/search/math?searchtype=author&query=Qiao%2C+Z">Zhonghua Qiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we present a novel spectral renormalization exponential
integrator method for solving gradient flow problems. Our method is
specifically designed to simultaneously satisfy discrete analogues of the
energy dissipation laws and achieve high-order accuracy in time. To accomplish
this, our method first incorporates the energy dissipation law into the target
gradient flow equation by introducing a time-dependent spectral renormalization
(TDSR) factor. Then, the coupled equations are discretized using the spectral
approximation in space and the exponential time differencing (ETD) in time.
Finally, the resulting fully discrete nonlinear system is decoupled and solved
using the Picard iteration at each time step. Furthermore, we introduce an
extra enforcing term into the system for updating the TDSR factor, which
greatly relaxes the time step size restriction of the proposed method and
enhances its computational efficiency. Extensive numerical tests with various
gradient flows are also presented to demonstrate the accuracy and effectiveness
of our method as well as its high efficiency when combined with an adaptive
time-stepping strategy for long-term simulations.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00826" title="Abstract">arXiv:2310.00826</a> [<a href="/pdf/2310.00826" title="Download PDF">pdf</a>, <a href="/format/2310.00826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Scale Masked Autoencoding for Reducing Label Requirements on SAR  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Allen%2C+M">Matt Allen</a>, 
<a href="/search/cs?searchtype=author&query=Dorr%2C+F">Francisco Dorr</a>, 
<a href="/search/cs?searchtype=author&query=Gallego-Mejia%2C+J+A">Joseph A. Gallego-Mejia</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Ferrer%2C+L">Laura Mart&#xed;nez-Ferrer</a>, 
<a href="/search/cs?searchtype=author&query=Jungbluth%2C+A">Anna Jungbluth</a>, 
<a href="/search/cs?searchtype=author&query=Kalaitzis%2C+F">Freddie Kalaitzis</a>, 
<a href="/search/cs?searchtype=author&query=Ramos-Poll%C3%A1n%2C+R">Ra&#xfa;l Ramos-Poll&#xe1;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Satellite-based remote sensing is instrumental in the monitoring and
mitigation of the effects of anthropogenic climate change. Large scale, high
resolution data derived from these sensors can be used to inform intervention
and policy decision making, but the timeliness and accuracy of these
interventions is limited by use of optical data, which cannot operate at night
and is affected by adverse weather conditions. Synthetic Aperture Radar (SAR)
offers a robust alternative to optical data, but its associated complexities
limit the scope of labelled data generation for traditional deep learning. In
this work, we apply a self-supervised pretraining scheme, masked autoencoding,
to SAR amplitude data covering 8.7\% of the Earth's land surface area, and tune
the pretrained weights on two downstream tasks crucial to monitoring climate
change - vegetation cover prediction and land cover classification. We show
that the use of this pretraining scheme reduces labelling requirements for the
downstream tasks by more than an order of magnitude, and that this pretraining
generalises geographically, with the performance gain increasing when tuned
downstream on regions outside the pretraining set. Our findings significantly
advance climate change mitigation by facilitating the development of task and
region-specific SAR models, allowing local communities and organizations to
deploy tailored solutions for rapid, accurate monitoring of climate change
effects.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00828" title="Abstract">arXiv:2310.00828</a> [<a href="/pdf/2310.00828" title="Download PDF">pdf</a>, <a href="/ps/2310.00828" title="Download PostScript">ps</a>, <a href="/format/2310.00828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Model for Calculating Cost of Applying Electronic Governance and  Robotic Process Automation to a Distributed Management System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+B">Bonny Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Pahune%2C+S">Saurabh Pahune</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Electronic Governance (eGov) and Robotic Process Automation (RPA) are two
technological advancements that have the potential to revolutionize the way
organizations manage their operations. When applied to Distributed Management
(DM), these technologies can further enhance organizational efficiency and
effectiveness. In this brief article, we present a mathematical model for
calculating the cost of accomplishing a task by applying eGov and RPA in a DM
system. This model is one of the first of its kind, and is expected to spark
further research on cost analysis for organizational efficiency given the
unprecedented advancements in electronic and automation technologies.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00829" title="Abstract">arXiv:2310.00829</a> [<a href="/pdf/2310.00829" title="Download PDF">pdf</a>, <a href="/format/2310.00829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Sensitivity Optimization in Differentially Private Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galli%2C+F">Filippo Galli</a>, 
<a href="/search/cs?searchtype=author&query=Palamidessi%2C+C">Catuscia Palamidessi</a>, 
<a href="/search/cs?searchtype=author&query=Cucinotta%2C+T">Tommaso Cucinotta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Training differentially private machine learning models requires constraining
an individual's contribution to the optimization process. This is achieved by
clipping the $2$-norm of their gradient at a predetermined threshold prior to
averaging and batch sanitization. This selection adversely influences
optimization in two opposing ways: it either exacerbates the bias due to
excessive clipping at lower values, or augments sanitization noise at higher
values. The choice significantly hinges on factors such as the dataset, model
architecture, and even varies within the same optimization, demanding
meticulous tuning usually accomplished through a grid search. In order to
circumvent the privacy expenses incurred in hyperparameter tuning, we present a
novel approach to dynamically optimize the clipping threshold. We treat this
threshold as an additional learnable parameter, establishing a clean
relationship between the threshold and the cost function. This allows us to
optimize the former with gradient descent, with minimal repercussions on the
overall privacy analysis. Our method is thoroughly assessed against alternative
fixed and adaptive strategies across diverse datasets, tasks, model dimensions,
and privacy levels. Our results demonstrate its comparable or superior
performance in all evaluated scenarios, given the same privacy requirements.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00831" title="Abstract">arXiv:2310.00831</a> [<a href="/pdf/2310.00831" title="Download PDF">pdf</a>, <a href="/format/2310.00831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Action Recognition Utilizing YGAR Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ranjan%2C+A">Amiya Ranjan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Lawrence Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The scarcity of high quality actions video data is a bottleneck in the
research and application of action recognition. Although significant effort has
been made in this area, there still exist gaps in the range of available data
types a more flexible and comprehensive data set could help bridge. In this
paper, we present a new 3D actions data simulation engine and generate 3 sets
of sample data to demonstrate its current functionalities. With the new data
generation process, we demonstrate its applications to image classifications,
action recognitions and potential to evolve into a system that would allow the
exploration of much more complex action recognition tasks. In order to show off
these capabilities, we also train and test a list of commonly used models for
image recognition to demonstrate the potential applications and capabilities of
the data sets and their generation process.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00832" title="Abstract">arXiv:2310.00832</a> [<a href="/pdf/2310.00832" title="Download PDF">pdf</a>, <a href="/format/2310.00832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Language Models for Data Visualization Utilizing nvBench Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Crespo-Quinones%2C+C">Carlos Crespo-Quinones</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Translation of natural language into syntactically correct commands for data
visualization is an important application of natural language models and could
be leveraged to many different tasks. A closely related effort is the task of
translating natural languages into SQL queries, which in turn could be
translated into visualization with additional information from the natural
language query supplied\cite{Zhong:2017qr}. Contributing to the progress in
this area of research, we built natural language translation models to
construct simplified versions of data and visualization queries in a language
called Vega Zero. In this paper, we explore the design and performance of these
sequence to sequence transformer based machine learning model architectures
using large language models such as BERT as encoders to predict visualization
commands from natural language queries, as well as apply available T5 sequence
to sequence models to the problem for comparison.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00833" title="Abstract">arXiv:2310.00833</a> [<a href="/pdf/2310.00833" title="Download PDF">pdf</a>, <a href="/format/2310.00833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Necessary and Sufficient Watermark for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takezawa%2C+Y">Yuki Takezawa</a>, 
<a href="/search/cs?searchtype=author&query=Sato%2C+R">Ryoma Sato</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+H">Han Bao</a>, 
<a href="/search/cs?searchtype=author&query=Niwa%2C+K">Kenta Niwa</a>, 
<a href="/search/cs?searchtype=author&query=Yamada%2C+M">Makoto Yamada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, large language models (LLMs) have achieved remarkable
performances in various NLP tasks. They can generate texts that are
indistinguishable from those written by humans. Such remarkable performance of
LLMs increases their risk of being used for malicious purposes, such as
generating fake news articles. Therefore, it is necessary to develop methods
for distinguishing texts written by LLMs from those written by humans.
Watermarking is one of the most powerful methods for achieving this. Although
existing watermarking methods have successfully detected texts generated by
LLMs, they significantly degrade the quality of the generated texts. In this
study, we propose the Necessary and Sufficient Watermark (NS-Watermark) for
inserting watermarks into generated texts without degrading the text quality.
More specifically, we derive minimum constraints required to be imposed on the
generated texts to distinguish whether LLMs or humans write the texts. Then, we
formulate the NS-Watermark as a constrained optimization problem and propose an
efficient algorithm to solve it. Through the experiments, we demonstrate that
the NS-Watermark can generate more natural texts than existing watermarking
methods and distinguish more accurately between texts written by LLMs and those
written by humans. Especially in machine translation tasks, the NS-Watermark
can outperform the existing watermarking method by up to 30 BLEU scores.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00834" title="Abstract">arXiv:2310.00834</a> [<a href="/pdf/2310.00834" title="Download PDF">pdf</a>, <a href="/format/2310.00834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Aware Route Planning for a Battery-Constrained Robot with  Multiple Charging Depots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asghar%2C+A+B">Ahmad Bilal Asghar</a>, 
<a href="/search/cs?searchtype=author&query=Tokekar%2C+P">Pratap Tokekar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper considers energy-aware route planning for a battery-constrained
robot operating in environments with multiple recharging depots. The robot has
a battery discharge time $D$, and it should visit the recharging depots at most
every $D$ time units to not run out of charge. The objective is to minimize
robot's travel time while ensuring it visits all task locations in the
environment. We present a $O(\log D)$ approximation algorithm for this problem.
We also present heuristic improvements to the approximation algorithm and
assess its performance on instances from TSPLIB, comparing it to an optimal
solution obtained through Integer Linear Programming (ILP). The simulation
results demonstrate that, despite a more than $20$-fold reduction in runtime,
the proposed algorithm provides solutions that are, on average, within $31\%$
of the ILP solution.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00835" title="Abstract">arXiv:2310.00835</a> [<a href="/pdf/2310.00835" title="Download PDF">pdf</a>, <a href="/format/2310.00835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TRAM: Benchmarking Temporal Reasoning for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yun Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, in submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Reasoning about time is essential for understanding the nuances of events
described in natural language. Previous research on this topic has been limited
in scope, characterized by a lack of standardized benchmarks that would allow
for consistent evaluations across different studies. In this paper, we
introduce TRAM, a temporal reasoning benchmark composed of ten datasets,
encompassing various temporal aspects of events such as order, arithmetic,
frequency, and duration, designed to facilitate a comprehensive evaluation of
the temporal reasoning capabilities of large language models (LLMs). We conduct
an extensive evaluation using popular LLMs, such as GPT-4 and Llama2, in both
zero-shot and few-shot learning scenarios. Additionally, we employ BERT-based
models to establish the baseline evaluations. Our findings indicate that these
models still trail human performance in temporal reasoning tasks. It is our
aspiration that TRAM will spur further progress in enhancing the temporal
reasoning abilities of LLMs. Our data is available at
\url{https://github.com/EternityYW/TRAM-Benchmark}.
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00836" title="Abstract">arXiv:2310.00836</a> [<a href="/pdf/2310.00836" title="Download PDF">pdf</a>, <a href="/format/2310.00836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical  Reasoning Capabilities of Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+M">Man Luo</a>, 
<a href="/search/cs?searchtype=author&query=Kumbhar%2C+S">Shrinidhi Kumbhar</a>, 
<a href="/search/cs?searchtype=author&query=shen%2C+M">Ming shen</a>, 
<a href="/search/cs?searchtype=author&query=Parmar%2C+M">Mihir Parmar</a>, 
<a href="/search/cs?searchtype=author&query=Varshney%2C+N">Neeraj Varshney</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+P">Pratyay Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Aditya%2C+S">Somak Aditya</a>, 
<a href="/search/cs?searchtype=author&query=Baral%2C+C">Chitta Baral</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Logical reasoning is fundamental for humans yet presents a substantial
challenge in the domain of Artificial Intelligence. Initially, researchers used
Knowledge Representation and Reasoning (KR) systems that did not scale and
required non trivial manual effort. Recently, the emergence of large language
models (LLMs) has demonstrated the ability to overcome various limitations of
formal Knowledge Representation (KR) systems. Consequently, there is a growing
interest in using LLMs for logical reasoning via natural language. This work
strives to understand the proficiency of LLMs in logical reasoning by offering
a brief review of the latest progress in this area; with a focus on the logical
reasoning datasets, tasks, and the methods adopted to utilize LLMs for
reasoning. To offer a thorough analysis, we have compiled a benchmark titled
LogiGLUE. This includes 24 varied datasets encompassing deductive, abductive,
and inductive reasoning. We have standardized these datasets into Seq2Seq tasks
to facilitate straightforward training and evaluation for future research.
Utilizing LogiGLUE as a foundation, we have trained an instruction fine tuned
language model, resulting in LogiT5. We study single task training, multi task
training, and a chain of thought knowledge distillation fine tuning technique
to assess the performance of model across the different logical reasoning
categories. By this comprehensive process, we aim to shed light on the
capabilities and potential pathways for enhancing logical reasoning proficiency
in LLMs, paving the way for more advanced and nuanced developments in this
critical field.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00837" title="Abstract">arXiv:2310.00837</a> [<a href="/pdf/2310.00837" title="Download PDF">pdf</a>, <a href="/format/2310.00837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Helios: An Efficient Out-of-core GNN Training System on Terabyte-scale  Graphs with In-memory Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jie Sun</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mo Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jun Xie</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zuocheng Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zihan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zeke Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Training graph neural networks (GNNs) on large-scale graph data holds immense
promise for numerous real-world applications but remains a great challenge.
Several disk-based GNN systems have been built to train large-scale graphs in a
single machine. However, they often fall short in terms of performance,
especially when training on terabyte-scale graphs. This is because existing
disk-based systems either overly focus on minimizing the number of SSD accesses
or do not fully overlap SSD accesses with GNN training, thus resulting in
substantial unnecessary overhead on the CPU side and then low GPU utilization.
To this end, we propose Helios, a system that can train GNN on terabyte graphs
in a single machine while achieving throughput comparable with in-memory
systems. To achieve this, we first present a GPU-initiated asynchronous disk IO
stack, allowing the GPU to directly access graph data on SSD. This design only
requires about 30% GPU cores to reach the almost maximal disk IO throughput and
wastes no GPU cores between IO submission and IO completion such that the
majority of GPU cores are left for other GNN kernels. Second, we design a
GPU-managed heterogeneous cache that extends the cache hierarchy to
heterogeneous CPU and GPU memory and thus enhances cache lookup throughput
significantly by GPU parallelism. Finally, we build a deep GNN-aware pipeline
that seamlessly integrates the computation and communication phases of the
entire GNN training process, maximizing the utility of GPU computation cycles.
Experimental results demonstrate that Helios can match the training throughput
of in-memory GNN systems, even for terabyte-scale graphs. Remarkably, Helios
surpasses the state-of-the-art GPU-managed baselines by up to 6.43x and exceeds
CPU-managed baselines by over 182x on all terabyte-scale graphs.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00839" title="Abstract">arXiv:2310.00839</a> [<a href="/pdf/2310.00839" title="Download PDF">pdf</a>, <a href="/format/2310.00839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subsurface Characterization using Ensemble-based Approaches with Deep  Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+J">Jichao Bao</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+H">Hongkyu Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jonghyun Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation (stat.CO); Machine Learning (stat.ML)

</div>
<p class="mathjax">Estimating spatially distributed properties such as hydraulic conductivity
(K) from available sparse measurements is a great challenge in subsurface
characterization. However, the use of inverse modeling is limited for
ill-posed, high-dimensional applications due to computational costs and poor
prediction accuracy with sparse datasets. In this paper, we combine Wasserstein
Generative Adversarial Network with Gradient Penalty (WGAN-GP), a deep
generative model that can accurately capture complex subsurface structure, and
Ensemble Smoother with Multiple Data Assimilation (ES-MDA), an ensemble-based
inversion method, for accurate and accelerated subsurface characterization.
WGAN-GP is trained to generate high-dimensional K fields from a low-dimensional
latent space and ES-MDA then updates the latent variables by assimilating
available measurements. Several subsurface examples are used to evaluate the
accuracy and efficiency of the proposed method and the main features of the
unknown K fields are characterized accurately with reliable uncertainty
quantification
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00840" title="Abstract">arXiv:2310.00840</a> [<a href="/pdf/2310.00840" title="Download PDF">pdf</a>, <a href="/format/2310.00840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error Norm Truncation: Robust Training in the Presence of Data Noise for  Text Generation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianjian Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haoran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Koehn%2C+P">Philipp Koehn</a>, 
<a href="/search/cs?searchtype=author&query=Khashabi%2C+D">Daniel Khashabi</a>, 
<a href="/search/cs?searchtype=author&query=Murray%2C+K">Kenton Murray</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Text generation models are notoriously vulnerable to errors in the training
data. With the wide-spread availability of massive amounts of web-crawled data
becoming more commonplace, how can we enhance the robustness of models trained
on a massive amount of noisy web-crawled text? In our work, we propose Error
Norm Truncation (ENT), a robust enhancement method to the standard training
objective that truncates noisy data. Compared to methods that only uses the
negative log-likelihood loss to estimate data quality, our method provides a
more accurate estimation by considering the distribution of non-target tokens,
which is often overlooked by previous work. Through comprehensive experiments
across language modeling, machine translation, and text summarization, we show
that equipping text generation models with ENT improves generation quality over
standard training and previous soft and hard truncation methods. Furthermore,
we show that our method improves the robustness of models against two of the
most detrimental types of noise in machine translation, resulting in an
increase of more than 2 BLEU points over the MLE baseline when up to 50% of
noise is added to the data.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00841" title="Abstract">arXiv:2310.00841</a> [<a href="/pdf/2310.00841" title="Download PDF">pdf</a>, <a href="/format/2310.00841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Drug Discovery with Dynamic Goal-aware Fragments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seul Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seanie Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Fragment-based drug discovery is an effective strategy for discovering drug
candidates in the vast chemical space, and has been widely employed in
molecular generative models. However, many existing fragment extraction methods
in such models do not take the target chemical properties into account or rely
on heuristic rules. Additionally, the existing fragment-based generative models
cannot update the fragment vocabulary with goal-aware fragments newly
discovered during the generation. To this end, we propose a molecular
generative framework for drug discovery, named Goal-aware fragment Extraction,
Assembly, and Modification (GEAM). GEAM consists of three modules, each
responsible for goal-aware fragment extraction, fragment assembly, and fragment
modification. The fragment extraction module identifies important fragments
that contribute to the desired target properties with the information
bottleneck principle, thereby constructing an effective goal-aware fragment
vocabulary. Moreover, GEAM can explore beyond the initial vocabulary with the
fragment modification module, and the exploration is further enhanced through
the dynamic goal-aware vocabulary update. We experimentally demonstrate that
GEAM effectively discovers drug candidates through the generative cycle of the
three modules in various drug discovery tasks.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00843" title="Abstract">arXiv:2310.00843</a> [<a href="/pdf/2310.00843" title="Download PDF">pdf</a>, <a href="/format/2310.00843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prov2vec: Learning Provenance Graph Representation for Unsupervised APT  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattarai%2C+B">Bibek Bhattarai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H+H">H. Howie Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Modern cyber attackers use advanced zero-day exploits, highly targeted spear
phishing, and other social engineering techniques to gain access and also use
evasion techniques to maintain a prolonged presence within the victim network
while working gradually towards the objective. To minimize the damage, it is
necessary to detect these Advanced Persistent Threats as early in the campaign
as possible. This paper proposes, Prov2Vec, a system for the continuous
monitoring of enterprise host's behavior to detect attackers' activities. It
leverages the data provenance graph built using system event logs to get
complete visibility into the execution state of an enterprise host and the
causal relationship between system entities. It proposes a novel provenance
graph kernel to obtain the canonical representation of the system behavior,
which is compared against its historical behaviors and that of other hosts to
detect the deviation from the normality. These representations are used in
several machine learning models to evaluate their ability to capture the
underlying behavior of an endpoint host. We have empirically demonstrated that
the provenance graph kernel produces a much more compact representation
compared to existing methods while improving prediction ability.
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00845" title="Abstract">arXiv:2310.00845</a> [<a href="/pdf/2310.00845" title="Download PDF">pdf</a>, <a href="/format/2310.00845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Application of frozen large-scale models to multimodal task-oriented  dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kawamoto%2C+T">Tatsuki Kawamoto</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+T">Takuma Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Miyama%2C+K">Ko Miyama</a>, 
<a href="/search/cs?searchtype=author&query=Meguro%2C+T">Takumi Meguro</a>, 
<a href="/search/cs?searchtype=author&query=Takagi%2C+T">Tomohiro Takagi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this study, we use the existing Large Language Models ENnhanced to See
Framework (LENS Framework) to test the feasibility of multimodal task-oriented
dialogues. The LENS Framework has been proposed as a method to solve computer
vision tasks without additional training and with fixed parameters of
pre-trained models. We used the Multimodal Dialogs (MMD) dataset, a multimodal
task-oriented dialogue benchmark dataset from the fashion field, and for the
evaluation, we used the ChatGPT-based G-EVAL, which only accepts textual
modalities, with arrangements to handle multimodal data. Compared to
Transformer-based models in previous studies, our method demonstrated an
absolute lift of 10.8% in fluency, 8.8% in usefulness, and 5.2% in relevance
and coherence. The results show that using large-scale models with fixed
parameters rather than using models trained on a dataset from scratch improves
performance in multimodal task-oriented dialogues. At the same time, we show
that Large Language Models (LLMs) are effective for multimodal task-oriented
dialogues. This is expected to lead to efficient applications to existing
systems.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00847" title="Abstract">arXiv:2310.00847</a> [<a href="/pdf/2310.00847" title="Download PDF">pdf</a>, <a href="/format/2310.00847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Pre-trained Networks Detect Familiar Out-of-Distribution Data?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miyai%2C+A">Atsuyuki Miyai</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Irie%2C+G">Go Irie</a>, 
<a href="/search/cs?searchtype=author&query=Aizawa%2C+K">Kiyoharu Aizawa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Out-of-distribution (OOD) detection is critical for safety-sensitive machine
learning applications and has been extensively studied, yielding a plethora of
methods developed in the literature. However, most studies for OOD detection
did not use pre-trained models and trained a backbone from scratch. In recent
years, transferring knowledge from large pre-trained models to downstream tasks
by lightweight tuning has become mainstream for training in-distribution (ID)
classifiers. To bridge the gap between the practice of OOD detection and
current classifiers, the unique and crucial problem is that the samples whose
information networks know often come as OOD input. We consider that such data
may significantly affect the performance of large pre-trained networks because
the discriminability of these OOD data depends on the pre-training algorithm.
Here, we define such OOD data as PT-OOD (Pre-Trained OOD) data. In this paper,
we aim to reveal the effect of PT-OOD on the OOD detection performance of
pre-trained networks from the perspective of pre-training algorithms. To
achieve this, we explore the PT-OOD detection performance of supervised and
self-supervised pre-training algorithms with linear-probing tuning, the most
common efficient tuning method. Through our experiments and analysis, we find
that the low linear separability of PT-OOD in the feature space heavily
degrades the PT-OOD detection performance, and self-supervised models are more
vulnerable to PT-OOD than supervised pre-trained models, even with
state-of-the-art detection methods. To solve this vulnerability, we further
propose a unique solution to large-scale pre-trained models: Leveraging
powerful instance-by-instance discriminative representations of pre-trained
models and detecting OOD in the feature space independent of the ID decision
boundaries. The code will be available via https://github.com/AtsuMiyai/PT-OOD.
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00851" title="Abstract">arXiv:2310.00851</a> [<a href="/pdf/2310.00851" title="Download PDF">pdf</a>, <a href="/format/2310.00851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-curvature, high-force, vine robot for inspection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mendoza%2C+M+J">Mija&#xed;l Ja&#xe9;n Mendoza</a>, 
<a href="/search/cs?searchtype=author&query=Naclerio%2C+N+D">Nicholas D. Naclerio</a>, 
<a href="/search/cs?searchtype=author&query=Hawkes%2C+E+W">Elliot W. Hawkes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Robot performance has advanced considerably both in and out of the factory,
however in tightly constrained, unknown environments such as inside a jet
engine or the human heart, current robots are less adept. In such cases where a
borescope or endoscope can't reach, disassembly or surgery are costly. One
promising inspection device inspired by plant growth are "vine robots" that can
navigate cluttered environments by extending from their tip. Yet, these vine
robots are currently limited in their ability to simultaneously steer into
tight curvatures and apply substantial forces to the environment. Here, we
propose a plant-inspired method of steering by asymmetrically lengthening one
side of the vine robot to enable high curvature and large force application.
Our key development is the introduction of an extremely anisotropic, composite,
wrinkled film with elastic moduli 400x different in orthogonal directions. The
film is used as the vine robot body, oriented such that it can stretch over
120% axially, but only 3% circumferentially. With the addition of controlled
layer jamming, this film enables a steering method inspired by plants in which
the circumference of the robot is inextensible, but the sides can stretch to
allow turns. This steering method and body pressure do not work against each
other, allowing the robot to exhibit higher forces and tighter curvatures than
previous vine robot architectures. This work advances the abilities of vine
robots--and robots more generally--to not only access tightly constrained
environments, but perform useful work once accessed.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00854" title="Abstract">arXiv:2310.00854</a> [<a href="/pdf/2310.00854" title="Download PDF">pdf</a>, <a href="/format/2310.00854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regulating CPU Temperature With Thermal-Aware Scheduling Using a Reduced  Order Learning Thermal Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dowling%2C+A">Anthony Dowling</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+L">Lin Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Cheng%2C+M">Ming-Cheng Cheng</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yu Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Modern real-time systems utilize considerable amounts of power while
executing computation-intensive tasks. The execution of these tasks leads to
significant power dissipation and heating of the device. It therefore results
in severe thermal issues like temperature escalation, high thermal gradients,
and excessive hot spot formation, which may result in degrading chip
performance, accelerating device aging, and premature failure. Thermal-Aware
Scheduling (TAS) enables the optimization of thermal dissipation to maintain a
safe thermal state. In this work, we implement a new TAS algorithm, POD-TAS,
which manages the thermal behavior of the cores based on a defined set of
states and their transitions. We compare the performances of a dynamic
Resistor-Capacitor (RC) thermal circuit simulator (HotSpot) and a reduced order
Proper Orthogonal Decomposition (POD)-based thermal model and we select the
latter for use in our POD-TAS algorithm. We implement a novel simulation-based
evaluation methodology to compare TAS algorithms. This methodology is used to
evaluate the performance of the proposed POD-TAS algorithm with high
spatiotemporal resolution. Additionally, we compare the performance of a state
of the art TAS algorithm, RT-TAS, to our proposed POD-TAS algorithm.
Furthermore, we utilize the Clarkson Open-source Multi-physics Benchmark Suite
(COMBS) to provide CPU workloads for task scheduling. Our experimental results
on a multi-core processor using a set of 4 benchmarks demonstrate that the
proposed POD-TAS method can improve thermal performance by decreasing the peak
thermal variance by 53.0% and the peak chip temperature by 29.01%. Using a set
of 8 benchmarks, the comparison of the algorithms demonstrates that POD-TAS
decreases the peak spatial variance of the chip temperature and the peak chip
temperature by 29.57% and 26.26% respectively.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00856" title="Abstract">arXiv:2310.00856</a> [<a href="/pdf/2310.00856" title="Download PDF">pdf</a>, <a href="/format/2310.00856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-triplet Feature Augmentation for Ponzi Scheme Detection in  Ethereum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Chengxiang Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiajun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shengbo Gong</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Chenxuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Xuan%2C+Q">Qi Xuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 2023 IEEE International Conference on Data Mining Workshops (ICDMW)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Blockchain technology revolutionizes the Internet, but also poses increasing
risks, particularly in cryptocurrency finance. On the Ethereum platform, Ponzi
schemes, phishing scams, and a variety of other frauds emerge. Existing Ponzi
scheme detection approaches based on heterogeneous transaction graph modeling
leverages semantic information between node (account) pairs to establish
connections, overlooking the semantic attributes inherent to the edges
(interactions). To overcome this, we construct heterogeneous Ethereum
interaction graphs with multiple triplet interaction patterns to better depict
the real Ethereum environment. Based on this, we design a new framework named
multi-triplet augmented heterogeneous graph neural network (MAHGNN) for Ponzi
scheme detection. We introduce the Conditional Variational Auto Encoder (CVAE)
to capture the semantic information of different triplet interaction patterns,
which facilitates the characterization on account features. Extensive
experiments demonstrate that MAHGNN is capable of addressing the problem of
multi-edge interactions in heterogeneous Ethereum interaction graphs and
achieving state-of-the-art performance in Ponzi scheme detection.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00863" title="Abstract">arXiv:2310.00863</a> [<a href="/pdf/2310.00863" title="Download PDF">pdf</a>, <a href="/format/2310.00863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Melody-conditioned lyrics generation via fine-tuning language model and  its evaluation with ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lasocki%2C+K">Karol Lasocki</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Takasu%2C+A">Atsuhiro Takasu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We leverage character-level language models for syllable-level lyrics
generation from symbolic melody. By fine-tuning a character-level pre-trained
model, we integrate language knowledge into the beam search of a syllable-level
Transformer generator. Using ChatGPT-based evaluations, we demonstrate enhanced
coherence and correctness in the generated lyrics.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00867" title="Abstract">arXiv:2310.00867</a> [<a href="/pdf/2310.00867" title="Download PDF">pdf</a>, <a href="/format/2310.00867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> (Dynamic) Prompting might be all you need to repair Compressed LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoang%2C+D+N+M">Duc N.M Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+M">Minsik Cho</a>, 
<a href="/search/cs?searchtype=author&query=Merth%2C+T">Thomas Merth</a>, 
<a href="/search/cs?searchtype=author&query=Rastegari%2C+M">Mohammad Rastegari</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs), while transformative for NLP, come with
significant computational demands, underlining the need for efficient,
training-free compression. Notably, the reliability of perplexity as a
benchmark for compressed model efficacy is in question, as our tests using
LLaMA-7B and OPT-6.7b reveal a significant performance drop in several
realistic downstream tasks, underscoring the disparity between perplexity as a
performance indicator and real-world performance. Investigation into the
trade-off between resource-intensive post-compression re-training highlights
the prospect of prompt-driven recovery as a lightweight adaption tool. However,
existing studies, confined mainly to perplexity evaluations and simple tasks,
fail to offer unequivocal confidence in the scalability and generalizability of
prompting. We tackle this uncertainty in two key ways. First, we uncover the
vulnerability of naive prompts in LLM compression as an over-reliance on a
singular prompt per input. In response, we propose inference-time dynamic
prompting (IDP), a mechanism that autonomously chooses from a set of curated
prompts based on the context of each individual input. Second, we delve into a
scientific understanding of why ``prompting might be all you need post-LLM
compression". Our findings suggest that compression doesn't irretrievably erase
LLM model knowledge but displace it, necessitating a new inference path. IDP
effectively redirects this path, enabling the model to tap into its inherent
yet displaced knowledge and thereby recover performance. Empirical tests affirm
the value of IDP, demonstrating an average performance improvement of 1.24%
across nine varied tasks spanning multiple knowledge domains.
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00868" title="Abstract">arXiv:2310.00868</a> [<a href="/pdf/2310.00868" title="Download PDF">pdf</a>, <a href="/format/2310.00868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RT-GAN: Recurrent Temporal GAN for Adding Lightweight Temporal  Consistency to Frame-Based Domain Translation Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mathew%2C+S">Shawn Mathew</a>, 
<a href="/search/cs?searchtype=author&query=Nadeem%2C+S">Saad Nadeem</a>, 
<a href="/search/cs?searchtype=author&query=Goh%2C+A+C">Alvin C. Goh</a>, 
<a href="/search/cs?searchtype=author&query=Kaufman%2C+A">Arie Kaufman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">While developing new unsupervised domain translation methods for endoscopy
videos, it is typical to start with approaches that initially work for
individual frames without temporal consistency. Once an individual-frame model
has been finalized, additional contiguous frames are added with a modified deep
learning architecture to train a new model for temporal consistency. This
transition to temporally-consistent deep learning models, however, requires
significantly more computational and memory resources for training. In this
paper, we present a lightweight solution with a tunable temporal parameter,
RT-GAN (Recurrent Temporal GAN), for adding temporal consistency to individual
frame-based approaches that reduces training requirements by a factor of 5. We
demonstrate the effectiveness of our approach on two challenging use cases in
colonoscopy: haustral fold segmentation (indicative of missed surface) and
realistic colonoscopy simulator video generation. The datasets, accompanying
code, and pretrained models will be made available at
\url{https://github.com/nadeemlab/CEP}.
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00870" title="Abstract">arXiv:2310.00870</a> [<a href="/pdf/2310.00870" title="Download PDF">pdf</a>, <a href="/format/2310.00870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> F0 analysis of Ghanaian pop singing reveals progressive alignment with  equal temperament over the past three decades: a case study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roman%2C+I+R">Iran R. Roman</a>, 
<a href="/search/cs?searchtype=author&query=Faronbi%2C+D">Daniel Faronbi</a>, 
<a href="/search/cs?searchtype=author&query=Burger-Weiser%2C+I">Isabelle Burger-Weiser</a>, 
<a href="/search/cs?searchtype=author&query=Adu-Gilmore%2C+L">Leila Adu-Gilmore</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pages 27-33
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Information Retrieval (cs.IR); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Contemporary Ghanaian popular singing combines European and traditional
Ghanaian influences. We hypothesize that access to technology embedded with
equal temperament catalyzed a progressive alignment of Ghanaian singing with
equal-tempered scales over time. To test this, we study the Ghanaian singer
Daddy Lumba, whose work spans from the earliest Ghanaian electronic style in
the late 1980s to the present. Studying a singular musician as a case study
allows us to refine our analysis without over-interpreting the findings. We
curated a collection of his songs, distributed between 1989 and 2016, to
extract F0 values from isolated vocals. We used Gaussian mixture modeling (GMM)
to approximate each song's scale and found that the pitch variance has been
decreasing over time. We also determined whether the GMM components follow the
arithmetic relationships observed in equal-tempered scales, and observed that
Daddy Lumba's singing better aligns with equal temperament in recent years.
Together, results reveal the impact of exposure to equal-tempered scales,
resulting in lessened microtonal content in Daddy Lumba's singing. Our study
highlights a potential vulnerability of Ghanaian musical scales and implies a
need for research that maps and archives singing styles.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00871" title="Abstract">arXiv:2310.00871</a> [<a href="/pdf/2310.00871" title="Download PDF">pdf</a>, <a href="/format/2310.00871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COMPOSER: Scalable and Robust Modular Policies for Snake Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuyou Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Y">Yaru Niu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xingyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Ding Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Snake robots have showcased remarkable compliance and adaptability in their
interaction with environments, mirroring the traits of their natural
counterparts. While their hyper-redundant and high-dimensional characteristics
add to this adaptability, they also pose great challenges to robot control.
Instead of perceiving the hyper-redundancy and flexibility of snake robots as
mere challenges, there lies an unexplored potential in leveraging these traits
to enhance robustness and generalizability at the control policy level. We seek
to develop a control policy that effectively breaks down the high
dimensionality of snake robots while harnessing their redundancy. In this work,
we consider the snake robot as a modular robot and formulate the control of the
snake robot as a cooperative Multi-Agent Reinforcement Learning (MARL) problem.
Each segment of the snake robot functions as an individual agent. Specifically,
we incorporate a self-attention mechanism to enhance the cooperative behavior
between agents. A high-level imagination policy is proposed to provide
additional rewards to guide the low-level control policy. We validate the
proposed method COMPOSER with five snake robot tasks, including goal reaching,
wall climbing, shape formation, tube crossing, and block pushing. COMPOSER
achieves the highest success rate across all tasks when compared to a
centralized baseline and four modular policy baselines. Additionally, we show
enhanced robustness against module corruption and significantly superior
zero-shot generalizability in our proposed method. The videos of this work are
available on our project page: https://sites.google.com/view/composer-snake/.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00873" title="Abstract">arXiv:2310.00873</a> [<a href="/pdf/2310.00873" title="Download PDF">pdf</a>, <a href="/format/2310.00873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Neural Networks Tend To Extrapolate Predictably
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+K">Katie Kang</a>, 
<a href="/search/cs?searchtype=author&query=Setlur%2C+A">Amrith Setlur</a>, 
<a href="/search/cs?searchtype=author&query=Tomlin%2C+C">Claire Tomlin</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Conventional wisdom suggests that neural network predictions tend to be
unpredictable and overconfident when faced with out-of-distribution (OOD)
inputs. Our work reassesses this assumption for neural networks with
high-dimensional inputs. Rather than extrapolating in arbitrary ways, we
observe that neural network predictions often tend towards a constant value as
input data becomes increasingly OOD. Moreover, we find that this value often
closely approximates the optimal constant solution (OCS), i.e., the prediction
that minimizes the average loss over the training data without observing the
input. We present results showing this phenomenon across 8 datasets with
different distributional shifts (including CIFAR10-C and ImageNet-R, S),
different loss functions (cross entropy, MSE, and Gaussian NLL), and different
architectures (CNNs and transformers). Furthermore, we present an explanation
for this behavior, which we first validate empirically and then study
theoretically in a simplified setting involving deep homogeneous networks with
ReLU activations. Finally, we show how one can leverage our insights in
practice to enable risk-sensitive decision-making in the presence of OOD
inputs.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00874" title="Abstract">arXiv:2310.00874</a> [<a href="/pdf/2310.00874" title="Download PDF">pdf</a>, <a href="/format/2310.00874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PC-NeRF: Parent-Child Neural Radiance Fields under Partial Sensor Data  Loss in Autonomous Driving Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiuzhong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+G">Guangming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zang%2C+Z">Zheng Zang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+P">Peng Jia</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yuxuan Han</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Junyi Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Reconstructing large-scale 3D scenes is essential for autonomous vehicles,
especially when partial sensor data is lost. Although the recently developed
neural radiance fields (NeRF) have shown compelling results in implicit
representations, the large-scale 3D scene reconstruction using partially lost
LiDAR point cloud data still needs to be explored. To bridge this gap, we
propose a novel 3D scene reconstruction framework called parent-child neural
radiance field (PC-NeRF). The framework comprises two modules, the parent NeRF
and the child NeRF, to simultaneously optimize scene-level, segment-level, and
point-level scene representations. Sensor data can be utilized more efficiently
by leveraging the segment-level representation capabilities of child NeRFs, and
an approximate volumetric representation of the scene can be quickly obtained
even with limited observations. With extensive experiments, our proposed
PC-NeRF is proven to achieve high-precision 3D reconstruction in large-scale
scenes. Moreover, PC-NeRF can effectively tackle situations where partial
sensor data is lost and has high deployment efficiency with limited training
time. Our approach implementation and the pre-trained models will be available
at https://github.com/biter0088/pc-nerf.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00877" title="Abstract">arXiv:2310.00877</a> [<a href="/pdf/2310.00877" title="Download PDF">pdf</a>, <a href="/format/2310.00877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QCFE: An efficient Feature engineering for query cost estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yu Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Junfang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+D">Dake Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Man Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaixin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+T">Tianqing Wan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Query cost estimation is a classical task for database management. Recently,
researchers apply the AI-driven model to implement query cost estimation for
achieving high accuracy. However, two defects of feature design lead to poor
cost estimation accuracy-time efficiency. On the one hand, existing works only
encode the query plan and data statistics while ignoring some other important
variables, like storage structure, hardware, database knobs, etc. These
variables also have significant impact on the query cost. On the other hand,
due to the straightforward encoding design, existing works suffer heavy
representation learning burden on ineffective dimensions of input. To meet the
above two problems, we first propose an efficient feature engineering for query
cost estimation, called QCFE. Specifically, we design a novel feature called
feature snapshot to efficiently integrate the influences of the ignored
variables. Further, we propose a difference-propagation feature reduction
method for query cost estimation to filter the useless features. The
experimental results demonstrate our QCFE could largely improve the
time-accuracy efficiency on extensive benchmarks.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00879" title="Abstract">arXiv:2310.00879</a> [<a href="/pdf/2310.00879" title="Download PDF">pdf</a>, <a href="/format/2310.00879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Temporal Fusion Based Free Space Segmentation for Autonomous  Surface Vessels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xueyao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yuwei Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The use of Autonomous Surface Vessels (ASVs) is growing rapidly. For safe and
efficient surface auto-driving, a reliable perception system is crucial. Such
systems allow the vessels to sense their surroundings and make decisions based
on the information gathered. During the perception process, free space
segmentation is essential to distinguish the safe mission zone and segment the
operational waterways. However, ASVs face particular challenges in free space
segmentation due to nearshore reflection interference, complex water textures,
and random motion vibrations caused by the water surface conditions. To deal
with these challenges, we propose a visual temporal fusion based free space
segmentation model to utilize the previous vision information. In addition, we
also introduce a new evaluation procedure and a contour position based loss
calculation function, which are more suitable for surface free space
segmentation tasks. The proposed model and process are tested on a continuous
video segmentation dataset and achieve both high-accuracy and robust results.
The dataset is also made available along with this paper.
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00887" title="Abstract">arXiv:2310.00887</a> [<a href="/pdf/2310.00887" title="Download PDF">pdf</a>, <a href="/format/2310.00887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRID: A Platform for General Robot Intelligence Development
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vemprala%2C+S">Sai Vemprala</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shuhang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shukla%2C+A">Abhinav Shukla</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+D">Dinesh Narayanan</a>, 
<a href="/search/cs?searchtype=author&query=Kapoor%2C+A">Ashish Kapoor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Developing machine intelligence abilities in robots and autonomous systems is
an expensive and time consuming process. Existing solutions are tailored to
specific applications and are harder to generalize. Furthermore, scarcity of
training data adds a layer of complexity in deploying deep machine learning
models. We present a new platform for General Robot Intelligence Development
(GRID) to address both of these issues. The platform enables robots to learn,
compose and adapt skills to their physical capabilities, environmental
constraints and goals. The platform addresses AI problems in robotics via
foundation models that know the physical world. GRID is designed from the
ground up to be extensible to accommodate new types of robots, vehicles,
hardware platforms and software protocols. In addition, the modular design
enables various deep ML components and existing foundation models to be easily
usable in a wider variety of robot-centric problems. We demonstrate the
platform in various aerial robotics scenarios and demonstrate how the platform
dramatically accelerates development of machine intelligent robots.
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00889" title="Abstract">arXiv:2310.00889</a> [<a href="/pdf/2310.00889" title="Download PDF">pdf</a>, <a href="/format/2310.00889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Convergent Boundary Integral Methods for Slender Bodies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Malhotra%2C+D">Dhairya Malhotra</a>, 
<a href="/search/math?searchtype=author&query=Barnett%2C+A">Alex Barnett</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">The interaction of fibers in a viscous (Stokes) fluid plays a crucial role in
industrial and biological processes, such as sedimentation, rheology,
transport, cell division, and locomotion. Numerical simulations generally rely
on slender body theory (SBT), an asymptotic, nonconvergent approximation whose
error blows up as fibers approach each other. Yet convergent boundary integral
equation (BIE) methods which completely resolve the fiber surface have so far
been impractical due to the prohibitive cost of layer-potential quadratures in
such high aspect-ratio 3D geometries. We present a high-order Nystr\"om
quadrature scheme with aspect-ratio independent cost, making such BIEs
practical. It combines centerline panels (each with a small number of poloidal
Fourier modes), toroidal Green's functions, generalized Chebyshev quadratures,
HPC parallel implementation, and FMM acceleration. We also present new BIE
formulations for slender bodies that lead to well conditioned linear systems
upon discretization. We test Laplace and Stokes Dirichlet problems, and Stokes
mobility problems, for slender rigid closed fibers with (possibly varying)
circular cross-section, at separations down to $1/20$ of the slender radius,
reporting convergence typically to at least 10 digits. We use this to quantify
the breakdown of numerical SBT for close-to-touching rigid fibers. We also
apply the methods to time-step the sedimentation of 512 loops with up to $1.65$
million unknowns at around 7 digits of accuracy.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00892" title="Abstract">arXiv:2310.00892</a> [<a href="/pdf/2310.00892" title="Download PDF">pdf</a>, <a href="/format/2310.00892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No Offense Taken: Eliciting Offensiveness from Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+A">Anugya Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+R">Rahul Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Mukku%2C+R">Rohith Mukku</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This work was completed in May 2022.
<br />For safe and reliable deployment of language models in the real world,
testing needs to be robust. This robustness can be characterized by the
difficulty and diversity of the test cases we evaluate these models on.
Limitations in human-in-the-loop test case generation has prompted an advent of
automated test case generation approaches. In particular, we focus on Red
Teaming Language Models with Language Models by Perez et al.(2022). Our
contributions include developing a pipeline for automated test case generation
via red teaming that leverages publicly available smaller language models
(LMs), experimenting with different target LMs and red classifiers, and
generating a corpus of test cases that can help in eliciting offensive
responses from widely deployed LMs and identifying their failure modes.
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00893" title="Abstract">arXiv:2310.00893</a> [<a href="/pdf/2310.00893" title="Download PDF">pdf</a>, <a href="/format/2310.00893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Engineering the Neural Collapse Geometry of Supervised-Contrastive Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gill%2C+J">Jaidev Gill</a>, 
<a href="/search/cs?searchtype=author&query=Vakilian%2C+V">Vala Vakilian</a>, 
<a href="/search/cs?searchtype=author&query=Thrampoulidis%2C+C">Christos Thrampoulidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Supervised-contrastive loss (SCL) is an alternative to cross-entropy (CE) for
classification tasks that makes use of similarities in the embedding space to
allow for richer representations. In this work, we propose methods to engineer
the geometry of these learnt feature embeddings by modifying the contrastive
loss. In pursuit of adjusting the geometry we explore the impact of prototypes,
fixed embeddings included during training to alter the final feature geometry.
Specifically, through empirical findings, we demonstrate that the inclusion of
prototypes in every batch induces the geometry of the learnt embeddings to
align with that of the prototypes. We gain further insights by considering a
limiting scenario where the number of prototypes far outnumber the original
batch size. Through this, we establish a connection to cross-entropy (CE) loss
with a fixed classifier and normalized embeddings. We validate our findings by
conducting a series of experiments with deep neural networks on benchmark
vision datasets.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00896" title="Abstract">arXiv:2310.00896</a> [<a href="/pdf/2310.00896" title="Download PDF">pdf</a>, <a href="/ps/2310.00896" title="Download PostScript">ps</a>, <a href="/format/2310.00896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Organized Event Participant Prediction Enhanced by Social Media  Retweeting Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yihong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hara%2C+T">Takahiro Hara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in WI-IAT 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Nowadays, many platforms on the Web offer organized events, allowing users to
be organizers or participants. For such platforms, it is beneficial to predict
potential event participants. Existing work on this problem tends to borrow
recommendation techniques. However, compared to e-commerce items and purchases,
events and participation are usually of a much smaller frequency, and the data
may be insufficient to learn an accurate model. In this paper, we propose to
utilize social media retweeting activity data to enhance the learning of event
participant prediction models. We create a joint knowledge graph to bridge the
social media and the target domain, assuming that event descriptions and tweets
are written in the same language. Furthermore, we propose a learning model that
utilizes retweeting information for the target domain prediction more
effectively. We conduct comprehensive experiments in two scenarios with
real-world data. In each scenario, we set up training data of different sizes,
as well as warm and cold test cases. The evaluation results show that our
approach consistently outperforms several baseline models, especially with the
warm test cases, and when target domain data is limited.
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00897" title="Abstract">arXiv:2310.00897</a> [<a href="/pdf/2310.00897" title="Download PDF">pdf</a>, <a href="/format/2310.00897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical Radar Sensing Using Two Stage Neural Network for Denoising  OTFS Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A+S">Ashok S Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Kalyani%2C+S">Sheetal Kalyani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Noise contamination affects the performance of orthogonal time frequency
space (OTFS) signals in real-world environments, making radar sensing
challenging. Our objective is to derive the range and velocity from the
delay-Doppler (DD) domain for radar sensing by using OTFS signaling. This work
introduces a two-stage approach to tackle this issue. In the first stage, we
use a convolutional neural network (CNN) model to classify the noise levels as
moderate or severe. Subsequently, if the noise level is severe, the OTFS
samples are denoised using a generative adversarial network (GAN). The proposed
approach achieves notable levels of accuracy in the classification of noisy
signals and mean absolute error (MAE) for the entire system even in low
signal-to-noise ratio (SNR) scenarios.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00898" title="Abstract">arXiv:2310.00898</a> [<a href="/pdf/2310.00898" title="Download PDF">pdf</a>, <a href="/format/2310.00898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enable Language Models to Implicitly Learn Self-Improvement From Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Le Hou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+T">Tianjian Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuexin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hongkun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 5 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated remarkable capabilities in
open-ended text generation tasks. However, the inherent open-ended nature of
these tasks implies that there is always room for improvement in the quality of
model responses. To address this challenge, various approaches have been
proposed to enhance the performance of LLMs. There has been a growing focus on
enabling LLMs to self-improve their response quality, thereby reducing the
reliance on extensive human annotation efforts for collecting diverse and
high-quality training data. Recently, prompting-based methods have been widely
explored among self-improvement methods owing to their effectiveness,
efficiency, and convenience. However, those methods usually require explicitly
and thoroughly written rubrics as inputs to LLMs. It is expensive and
challenging to manually derive and provide all necessary rubrics with a
real-world complex goal for improvement (e.g., being more helpful and less
harmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework
that implicitly learns the improvement goal from human preference data. PIT
only requires preference data that are used to train reward models without
extra human efforts. Specifically, we reformulate the training objective of
reinforcement learning from human feedback (RLHF) -- instead of maximizing
response quality for a given input, we maximize the quality gap of the response
conditioned on a reference response. In this way, PIT is implicitly trained
with the improvement goal of better aligning with human preferences.
Experiments on two real-world datasets and one synthetic dataset show that our
method significantly outperforms prompting-based methods.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00900" title="Abstract">arXiv:2310.00900</a> [<a href="/pdf/2310.00900" title="Download PDF">pdf</a>, <a href="/format/2310.00900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> uSee: Unified Speech Enhancement and Editing with Conditional Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Muqiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chunlei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhongweiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Heming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Speech enhancement aims to improve the quality of speech signals in terms of
quality and intelligibility, and speech editing refers to the process of
editing the speech according to specific user needs. In this paper, we propose
a Unified Speech Enhancement and Editing (uSee) model with conditional
diffusion models to handle various tasks at the same time in a generative
manner. Specifically, by providing multiple types of conditions including
self-supervised learning embeddings and proper text prompts to the score-based
diffusion model, we can enable controllable generation of the unified speech
enhancement and editing model to perform corresponding actions on the source
speech. Our experiments show that our proposed uSee model can achieve superior
performance in both speech denoising and dereverberation compared to other
related generative speech enhancement models, and can perform speech editing
given desired environmental sound text description, signal-to-noise ratios
(SNR), and room impulse responses (RIR). Demos of the generated speech are
available at https://muqiaoy.github.io/usee.
</p>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00901" title="Abstract">arXiv:2310.00901</a> [<a href="/pdf/2310.00901" title="Download PDF">pdf</a>, <a href="/format/2310.00901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TADIS: Steering Models for Deep-Thinking about Demonstration Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+T">Tianci Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yixia Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guanhua Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Instruction tuning has been demonstrated that could significantly improve the
zero-shot generalization capability to unseen tasks by an apparent margin. By
incorporating additional context (e.g., task definition, examples) during the
fine-tuning process, Large Language Models (LLMs) achieved much higher
performance than before. However, recent work reported that delusive task
examples can achieve almost the same performance as correct task examples,
indicating the input-label correspondence is less important than previously
thought. Intrigued by this counter-intuitive observation, we suspect models
have the same illusion of competence as humans. Therefore, we propose a novel
method called TADIS that steers LLMs for "Deep-Thinking'' about demonstration
examples instead of merely seeing. To alleviate the illusion of competence of
models, we first ask the model to verify the correctness of shown examples.
Then, using the verification results as conditions to elicit models for a
better answer. Our experimental results show that TADIS consistently
outperforms competitive baselines on in-domain and out-domain tasks (improving
2.79 and 4.03 average ROUGLE-L on out-domain and in-domain datasets,
respectively). Despite the presence of generated examples (not all of the
thinking labels are accurate), TADIS can notably enhance performance in
zero-shot and few-shot settings. This also suggests that our approach can be
adopted on a large scale to improve the instruction following capabilities of
models without any manual labor. Moreover, we construct three types of thinking
labels with different model sizes and find that small models learn from the
format of TADIS but larger models can be steered for "Deep-Thinking''.
</p>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00902" title="Abstract">arXiv:2310.00902</a> [<a href="/pdf/2310.00902" title="Download PDF">pdf</a>, <a href="/format/2310.00902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+Y">Yongchan Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+E">Eric Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kevin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">James Zou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Quantifying the impact of training data points is crucial for understanding
the outputs of machine learning models and for improving the transparency of
the AI pipeline. The influence function is a principled and popular data
attribution method, but its computational cost often makes it challenging to
use. This issue becomes more pronounced in the setting of large language models
and text-to-image models. In this work, we propose DataInf, an efficient
influence approximation method that is practical for large-scale generative AI
models. Leveraging an easy-to-compute closed-form expression, DataInf
outperforms existing influence computation algorithms in terms of computational
and memory efficiency. Our theoretical analysis shows that DataInf is
particularly well-suited for parameter-efficient fine-tuning techniques such as
LoRA. Through systematic empirical evaluations, we show that DataInf accurately
approximates influence scores and is orders of magnitude faster than existing
methods. In applications to RoBERTa-large, Llama-2-13B-chat, and
stable-diffusion-v1.5 models, DataInf effectively identifies the most
influential fine-tuning examples better than other approximate influence
scores. Moreover, it can help to identify which data points are mislabeled.
</p>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00905" title="Abstract">arXiv:2310.00905</a> [<a href="/pdf/2310.00905" title="Download PDF">pdf</a>, <a href="/format/2310.00905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> All Languages Matter: On the Multilingual Safety of Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhaopeng Tu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Youliang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jen-tse Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+W">Wenxiang Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+M+R">Michael R. Lyu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first multilingual safety benchmark for large language models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Safety lies at the core of developing and deploying large language models
(LLMs). However, previous safety benchmarks only concern the safety in one
language, e.g. the majority language in the pretraining data such as English.
In this work, we build the first multilingual safety benchmark for LLMs,
XSafety, in response to the global deployment of LLMs in practice. XSafety
covers 14 kinds of commonly used safety issues across 10 languages that span
several language families. We utilize XSafety to empirically study the
multilingual safety for 4 widely-used LLMs, including both close-API and
open-source models. Experimental results show that all LLMs produce
significantly more unsafe responses for non-English queries than English ones,
indicating the necessity of developing safety alignment for non-English
languages. In addition, we propose several simple and effective prompting
methods to improve the multilingual safety of ChatGPT by evoking safety
knowledge and improving cross-lingual generalization of safety alignment. Our
prompting method can significantly reduce the ratio of unsafe responses from
19.1% to 9.7% for non-English queries. We release our data at
https://github.com/Jarviswang94/Multilingual_safety_benchmark.
</p>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00906" title="Abstract">arXiv:2310.00906</a> [<a href="/pdf/2310.00906" title="Download PDF">pdf</a>, <a href="/format/2310.00906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Decentralized Cooperative Navigation Approach for Visual Homing  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahouti%2C+M">Mohamed Rahouti</a>, 
<a href="/search/cs?searchtype=author&query=Lyons%2C+D">Damian Lyons</a>, 
<a href="/search/cs?searchtype=author&query=Jagatheesaperumal%2C+S+K">Senthil Kumar Jagatheesaperumal</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+K">Kaiqi Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)

</div>
<p class="mathjax">Visual homing is a lightweight approach to visual navigation. Given the
stored information of an initial 'home' location, the navigation task back to
this location is achieved from any other location by comparing the stored home
information to the current image and extracting a motion vector. A challenge
that constrains the applicability of visual homing is that the home location
must be within the robot's field of view to initiate the homing process. Thus,
we propose a blockchain approach to visual navigation for a heterogeneous robot
team over a wide area of visual navigation. Because it does not require map
data structures, the approach is useful for robot platforms with a small
computational footprint, and because it leverages current visual information,
it supports a resilient and adaptive path selection. Further, we present a
lightweight Proof-of-Work (PoW) mechanism for reaching consensus in the
untrustworthy visual homing network.
</p>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00907" title="Abstract">arXiv:2310.00907</a> [<a href="/pdf/2310.00907" title="Download PDF">pdf</a>, <a href="/format/2310.00907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Participatory Turn in AI Design: Theoretical Foundations and the  Current State of Practice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delgado%2C+F">Fernando Delgado</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Stephen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Madaio%2C+M">Michael Madaio</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qian Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite the growing consensus that stakeholders affected by AI systems should
participate in their design, enormous variation and implicit disagreements
exist among current approaches. For researchers and practitioners who are
interested in taking a participatory approach to AI design and development, it
remains challenging to assess the extent to which any participatory approach
grants substantive agency to stakeholders. This article thus aims to ground
what we dub the "participatory turn" in AI design by synthesizing existing
theoretical literature on participation and through empirical investigation and
critique of its current practices. Specifically, we derive a conceptual
framework through synthesis of literature across technology design, political
theory, and the social sciences that researchers and practitioners can leverage
to evaluate approaches to participation in AI design. Additionally, we
articulate empirical findings concerning the current state of participatory
practice in AI design based on an analysis of recently published research and
semi-structured interviews with 12 AI researchers and practitioners. We use
these empirical findings to understand the current state of participatory
practice and subsequently provide guidance to better align participatory goals
and methods in a way that accounts for practical constraints.
</p>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00911" title="Abstract">arXiv:2310.00911</a> [<a href="/pdf/2310.00911" title="Download PDF">pdf</a>, <a href="/format/2310.00911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Manipulation of a Deformable Linear Object: Simulation and  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q+J">Qi Jing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bretl%2C+T">Timothy Bretl</a>, 
<a href="/search/cs?searchtype=author&query=Vuong%2C+N">Nghia Vuong</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+Q">Quang-Cuong Pham</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We show that it is possible to learn an open-loop policy in simulation for
the dynamic manipulation of a deformable linear object (DLO) -- e.g., a rope,
wire, or cable -- that can be executed by a real robot without additional
training. Our method is enabled by integrating an existing state-of-the-art DLO
model (Discrete Elastic Rods) with MuJoCo, a robot simulator. We describe how
this integration was done, check that validation results produced in simulation
match what we expect from analysis of the physics, and apply policy
optimization to train an open-loop policy from data collected only in
simulation that uses a robot arm to fling a wire precisely between two
obstacles. This policy achieves a success rate of 76.7% when executed by a real
robot in hardware experiments without additional training on the real task.
</p>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00912" title="Abstract">arXiv:2310.00912</a> [<a href="/pdf/2310.00912" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Resource-efficient FIR Filter Design Based on an RAG Improved  Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+M">Mengwei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhengxiong Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xianyang Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 3 figures, Conference paper for ICCS (International Conference on Circuits and Systems) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In modern digital filter chip design, efficient resource utilization is a hot
topic. Due to the linear phase characteristics of FIR filters, a pulsed fully
parallel structure can be applied to address the problem. To further reduce
hardware resource consumption, especially related to multiplication functions,
an improved RAG algorithm has been proposed. Filters with different orders and
for different algorithms have been compared, and the experimental results show
that the improved RAG algorithm excels in terms of logic resource utilization,
resource allocation, running speed, and power consumption under various
application scenarios. The proposed algorithm introduces a better circuit
structure for FIR filters, fully leveraging resource allocation strategies to
reduce logic resource consumption. The proposed circuit is faster and more
stable, making it suitable for a variety of complex application scenarios.
</p>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00917" title="Abstract">arXiv:2310.00917</a> [<a href="/pdf/2310.00917" title="Download PDF">pdf</a>, <a href="/format/2310.00917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing the Power of Multi-Lingual Datasets for Pre-training: Towards  Enhancing Text Spotting Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Alloy Das</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+S">Sanket Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+A">Ayan Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S">Saumik Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Llad%C3%B3s%2C+J">Josep Llad&#xf3;s</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+U">Umapada Pal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The adaptation capability to a wide range of domains is crucial for scene
text spotting models when deployed to real-world conditions. However, existing
state-of-the-art (SOTA) approaches usually incorporate scene text detection and
recognition simply by pretraining on natural scene text datasets, which do not
directly exploit the intermediate feature representations between multiple
domains. Here, we investigate the problem of domain-adaptive scene text
spotting, i.e., training a model on multi-domain source data such that it can
directly adapt to target domains rather than being specialized for a specific
domain or scenario. Further, we investigate a transformer baseline called
Swin-TESTR to focus on solving scene-text spotting for both regular and
arbitrary-shaped scene text along with an exhaustive evaluation. The results
clearly demonstrate the potential of intermediate representations to achieve
significant performance on text spotting benchmarks across multiple domains
(e.g. language, synth-to-real, and documents). both in terms of accuracy and
efficiency.
</p>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00920" title="Abstract">arXiv:2310.00920</a> [<a href="/pdf/2310.00920" title="Download PDF">pdf</a>, <a href="/format/2310.00920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Every Dataset Counts: Scaling up Monocular 3D Object Detection with  Joint Datasets Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fulong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xiaoyang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuxuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Ming Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Monocular 3D object detection plays a crucial role in autonomous driving.
However, existing monocular 3D detection algorithms depend on 3D labels derived
from LiDAR measurements, which are costly to acquire for new datasets and
challenging to deploy in novel environments. Specifically, this study
investigates the pipeline for training a monocular 3D object detection model on
a diverse collection of 3D and 2D datasets. The proposed framework comprises
three components: (1) a robust monocular 3D model capable of functioning across
various camera settings, (2) a selective-training strategy to accommodate
datasets with differing class annotations, and (3) a pseudo 3D training
approach using 2D labels to enhance detection performance in scenes containing
only 2D labels. With this framework, we could train models on a joint set of
various open 3D/2D datasets to obtain models with significantly stronger
generalization capability and enhanced performance on new dataset with only 2D
labels. We conduct extensive experiments on
KITTI/nuScenes/ONCE/Cityscapes/BDD100K datasets to demonstrate the scaling
ability of the proposed method.
</p>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00922" title="Abstract">arXiv:2310.00922</a> [<a href="/pdf/2310.00922" title="Download PDF">pdf</a>, <a href="/format/2310.00922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Close are Other Computer Vision Tasks to Deepfake Detection?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H+H">Huy H. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Yamagishi%2C+J">Junichi Yamagishi</a>, 
<a href="/search/cs?searchtype=author&query=Echizen%2C+I">Isao Echizen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to be Published in Proceedings of the IEEE International Joint Conference on Biometrics (IJCB 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we challenge the conventional belief that supervised
ImageNet-trained models have strong generalizability and are suitable for use
as feature extractors in deepfake detection. We present a new measurement,
"model separability," for visually and quantitatively assessing a model's raw
capacity to separate data in an unsupervised manner. We also present a
systematic benchmark for determining the correlation between deepfake detection
and other computer vision tasks using pre-trained models. Our analysis shows
that pre-trained face recognition models are more closely related to deepfake
detection than other models. Additionally, models trained using self-supervised
methods are more effective in separation than those trained using supervised
methods. After fine-tuning all models on a small deepfake dataset, we found
that self-supervised models deliver the best results, but there is a risk of
overfitting. Our results provide valuable insights that should help researchers
and practitioners develop more effective deepfake detection models.
</p>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00923" title="Abstract">arXiv:2310.00923</a> [<a href="/pdf/2310.00923" title="Download PDF">pdf</a>, <a href="/format/2310.00923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Winter Road Surface Condition Monitoring with Computer Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ojala%2C+R">Risto Ojala</a>, 
<a href="/search/cs?searchtype=author&query=Sepp%C3%A4nen%2C+A">Alvari Sepp&#xe4;nen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Winter conditions pose several challenges for automated driving applications.
A key challenge during winter is accurate assessment of road surface condition,
as its impact on friction is a critical parameter for safely and reliably
controlling a vehicle. This paper proposes a deep learning regression model,
SIWNet, capable of estimating road surface friction properties from camera
images. SIWNet extends state of the art by including an uncertainty estimation
mechanism in the architecture. This is achieved by including an additional head
in the network, which estimates a prediction interval. The prediction interval
head is trained with a maximum likelihood loss function. The model was trained
and tested with the SeeingThroughFog dataset, which features corresponding road
friction sensor readings and images from an instrumented vehicle. Acquired
results highlight the functionality of the prediction interval estimation of
SIWNet, while the network also achieved similar point estimate accuracy as the
previous state of the art. Furthermore, the SIWNet architecture is several
times more lightweight than the previously applied state-of-the-art model,
resulting in more practical and efficient deployment.
</p>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00924" title="Abstract">arXiv:2310.00924</a> [<a href="/pdf/2310.00924" title="Download PDF">pdf</a>, <a href="/format/2310.00924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulation Assessment Guidelines towards Independent Safety Assurance of  Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cherian%2C+J">Jim Cherian</a>, 
<a href="/search/cs?searchtype=author&query=Slavik%2C+M">Martin Slavik</a>, 
<a href="/search/cs?searchtype=author&query=Piazzoni%2C+A">Andrea Piazzoni</a>, 
<a href="/search/cs?searchtype=author&query=Vijay%2C+R">Roshan Vijay</a>, 
<a href="/search/cs?searchtype=author&query=Azhar%2C+M">Mohamed Azhar</a>, 
<a href="/search/cs?searchtype=author&query=de+Boer%2C+N">Niels de Boer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 54 pages, 23 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This Simulation Assessment Guidelines document is a public guidelines
document developed by the Centre of Excellence for Testing &amp; Research of AVs -
NTU (CETRAN) in collaboration with the Land Transport Authority (LTA) of
Singapore. It is primarily intended to help the developers of Autonomous
Vehicles (AVs) in Singapore to prepare their software simulations and provide
recommendations that can ensure their readiness for independent assessment of
their virtual simulation results according to the Milestone-testing framework
adopted by the assessor and the local authority in Singapore, namely, CETRAN
and LTA respectively.
</p>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00926" title="Abstract">arXiv:2310.00926</a> [<a href="/pdf/2310.00926" title="Download PDF">pdf</a>, <a href="/format/2310.00926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integration of Graph Neural Network and Neural-ODEs for Tumor Dynamic  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bazgir%2C+O">Omid Bazgir</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zichen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hafner%2C+M">Marc Hafner</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">James Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In anti-cancer drug development, a major scientific challenge is
disentangling the complex relationships between high-dimensional genomics data
from patient tumor samples, the corresponding tumor's organ of origin, the drug
targets associated with given treatments and the resulting treatment response.
Furthermore, to realize the aspirations of precision medicine in identifying
and adjusting treatments for patients depending on the therapeutic response,
there is a need for building tumor dynamic models that can integrate both
longitudinal tumor size as well as multimodal, high-content data. In this work,
we take a step towards enhancing personalized tumor dynamic predictions by
proposing a heterogeneous graph encoder that utilizes a bipartite Graph
Convolutional Neural network (GCN) combined with Neural Ordinary Differential
Equations (Neural-ODEs). We applied the methodology to a large collection of
patient-derived xenograft (PDX) data, spanning a wide variety of treatments (as
well as their combinations) on tumors that originated from a number of
different organs. We first show that the methodology is able to discover a
tumor dynamic model that significantly improves upon an empirical model which
is in current use. Additionally, we show that the graph encoder is able to
effectively utilize multimodal data to enhance tumor predictions. Our findings
indicate that the methodology holds significant promise and offers potential
applications in pre-clinical settings.
</p>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00927" title="Abstract">arXiv:2310.00927</a> [<a href="/pdf/2310.00927" title="Download PDF">pdf</a>, <a href="/format/2310.00927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Transferable Representation Learning and Zero-shot  Transfer in CLIP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zixiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yihe Deng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 4 tables, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Multi-modal learning has become increasingly popular due to its ability to
leverage information from different data sources (e.g., text and images) to
improve the model performance. Recently, CLIP has emerged as an effective
approach that employs vision-language contrastive pretraining to learn joint
image and text representations and exhibits remarkable performance in zero-shot
learning and text-guided natural image generation. Despite the huge practical
success of CLIP, its theoretical understanding remains elusive. In this paper,
we formally study transferrable representation learning underlying CLIP and
demonstrate how features from different modalities get aligned. We also analyze
its zero-shot transfer performance on the downstream tasks. Inspired by our
analysis, we propose a new CLIP-type approach, which achieves better
performance than CLIP and other state-of-the-art methods on benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00933" title="Abstract">arXiv:2310.00933</a> [<a href="/pdf/2310.00933" title="Download PDF">pdf</a>, <a href="/format/2310.00933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Case Study: Securing Embedded Linux Using CHERI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almatary%2C+H">Hesham Almatary</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Operating Systems (cs.OS)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The current embedded Linux variant lacks security as it does not have or use
MMU support. It does not also use MPUs as they do not fit with its software
model because of the design drawbacks of MPUs (i.e., coarse-grained protection
with fixed number of protected regions). We secure the existing embedded Linux
version of the RISC-V port using CHERI. CHERI is hardware-software
capability-based system that leverages the ISA, toolchain, programming
lanaguages, operating systems, and applications in order to provide complete
pointer and memory safety. We believe that CHERI could provide significant
security guarantees for high-end dynamic embedded systems at lower costs,
compared to MMUs and MPUs, by: 1) building the entire software stack in
pure-capability CHERI C mode which provides complete spatial memory safety at
the kernel and user-level, 2) isolating user programs as separate ELFs, each
with its own CHERI-based capability table; this provides spatial memory safety
similar to what the MMU offers (i.e., user programs cannot access each other's
memory), 3) isolating user programs from the kernel as the kernel has its own
capability table from the users and vice versa, and 4) compartmentalising
kernel modules using CompartOS' linkage-based compartmentalisation. This offers
a new security front that is not possible using the current MMU-based Linux,
where vulnerable/malicious kernel modules (e.g., device drivers) executing in
the kernel space would not compromise or take down the entire system. These are
the four main contributions of this paper, presenting novel CHERI-based
mechanisms to secure embedded Linux.
</p>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00934" title="Abstract">arXiv:2310.00934</a> [<a href="/pdf/2310.00934" title="Download PDF">pdf</a>, <a href="/ps/2310.00934" title="Download PostScript">ps</a>, <a href="/format/2310.00934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a control-theoretic trivialization of ABR video streaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fliess%2C+M">Michel Fliess</a>, 
<a href="/search/eess?searchtype=author&query=Join%2C+C">C&#xe9;dric Join</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Networking and Internet Architecture (cs.NI); Optimization and Control (math.OC)

</div>
<p class="mathjax">Combining flatness-based control, model-free control and algebraic estimation
techniques permits to trivialize several key issues in the adaptive bitrate
(ABR) setting, now the dominant industry approach in video streaming: 1) A
straightforward open-loop strategy for the bitrate and the buffer level; 2)
closing the loop permits not only to take into account unavoidable mismatches
and disturbances, like Internet fluctuations, but also the inherently discrete
nature of the bitrate; 3) an easily implementable closed-form estimate for a
bandwidth allows to handle severe variations of the channel capacity. Several
computer experiments and metrics for evaluating the Quality of Experience (QoE)
are displayed and discussed.
</p>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00935" title="Abstract">arXiv:2310.00935</a> [<a href="/pdf/2310.00935" title="Download PDF">pdf</a>, <a href="/format/2310.00935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resolving Knowledge Conflicts in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yike Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shangbin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Heng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weijia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Balachandran%2C+V">Vidhisha Balachandran</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianxing He</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) often encounter knowledge conflicts, scenarios
where discrepancy arises between the internal parametric knowledge of LLMs and
non-parametric information provided in the prompt context. In this work we ask
what are the desiderata for LLMs when a knowledge conflict arises and whether
existing LLMs fulfill them. We posit that LLMs should 1) identify knowledge
conflicts, 2) pinpoint conflicting information segments, and 3) provide
distinct answers or viewpoints in conflicting scenarios. To this end, we
introduce KNOWLEDGE CONFLICT, an evaluation framework for simulating contextual
knowledge conflicts and quantitatively evaluating to what extent LLMs achieve
these goals. KNOWLEDGE CONFLICT includes diverse and complex situations of
knowledge conflict, knowledge from diverse entities and domains, two synthetic
conflict creation methods, and settings with progressively increasing
difficulty to reflect realistic knowledge conflicts. Extensive experiments with
the KNOWLEDGE CONFLICT framework reveal that while LLMs perform well in
identifying the existence of knowledge conflicts, they struggle to determine
the specific conflicting knowledge and produce a response with distinct answers
amidst conflicting information. To address these challenges, we propose new
instruction-based approaches that augment LLMs to better achieve the three
goals. Further analysis shows that abilities to tackle knowledge conflicts are
greatly impacted by factors such as knowledge domain and prompt text, while
generating robust responses to knowledge conflict scenarios remains an open
research question.
</p>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00936" title="Abstract">arXiv:2310.00936</a> [<a href="/pdf/2310.00936" title="Download PDF">pdf</a>, <a href="/ps/2310.00936" title="Download PostScript">ps</a>, <a href="/format/2310.00936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trained Latent Space Navigation to Prevent Lack of Photorealism in  Generated Images on Style-based Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harada%2C+T">Takumi Harada</a>, 
<a href="/search/cs?searchtype=author&query=Aihara%2C+K">Kazuyuki Aihara</a>, 
<a href="/search/cs?searchtype=author&query=Sakai%2C+H">Hiroyuki Sakai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent studies on StyleGAN variants show promising performances for various
generation tasks. In these models, latent codes have traditionally been
manipulated and searched for the desired images. However, this approach
sometimes suffers from a lack of photorealism in generated images due to a lack
of knowledge about the geometry of the trained latent space. In this paper, we
show a simple unsupervised method that provides well-trained local latent
subspace, enabling latent code navigation while preserving the photorealism of
the generated images. Specifically, the method identifies densely mapped latent
spaces and restricts latent manipulations within the local latent subspace.
Experimental results demonstrate that images generated within the local latent
subspace maintain photorealism even when the latent codes are significantly and
repeatedly manipulated. Moreover, experiments show that the method can be
applied to latent code optimization for various types of style-based models.
Our empirical evidence of the method will benefit applications in style-based
models.
</p>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00937" title="Abstract">arXiv:2310.00937</a> [<a href="/pdf/2310.00937" title="Download PDF">pdf</a>, <a href="/format/2310.00937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Efficient Training of a U-Net Based Architecture for Structured  Documents Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kabeshova%2C+A">Anastasiia Kabeshova</a>, 
<a href="/search/cs?searchtype=author&query=Betmont%2C+G">Guillaume Betmont</a>, 
<a href="/search/cs?searchtype=author&query=Lerouge%2C+J">Julien Lerouge</a>, 
<a href="/search/cs?searchtype=author&query=Stepankevich%2C+E">Evgeny Stepankevich</a>, 
<a href="/search/cs?searchtype=author&query=Berg%C3%A8s%2C+A">Alexis Berg&#xe8;s</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Structured documents analysis and recognition are essential for modern online
on-boarding processes, and document localization is a crucial step to achieve
reliable key information extraction. While deep-learning has become the
standard technique used to solve document analysis problems, real-world
applications in industry still face the limited availability of labelled data
and of computational resources when training or fine-tuning deep-learning
models. To tackle these challenges, we propose SDL-Net: a novel U-Net like
encoder-decoder architecture for the localization of structured documents. Our
approach allows pre-training the encoder of SDL-Net on a generic dataset
containing samples of various document classes, and enables fast and
data-efficient fine-tuning of decoders to support the localization of new
document classes. We conduct extensive experiments on a proprietary dataset of
structured document images to demonstrate the effectiveness and the
generalization capabilities of the proposed approach.
</p>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00938" title="Abstract">arXiv:2310.00938</a> [<a href="/pdf/2310.00938" title="Download PDF">pdf</a>, <a href="/ps/2310.00938" title="Download PostScript">ps</a>, <a href="/format/2310.00938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An FPRAS for two terminal reliability in directed acyclic graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Weiming Feng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Heng Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We give a fully polynomial-time randomized approximation scheme (FPRAS) for
two terminal reliability in directed acyclic graphs.
</p>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00941" title="Abstract">arXiv:2310.00941</a> [<a href="/pdf/2310.00941" title="Download PDF">pdf</a>, <a href="/format/2310.00941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Variational Bayesian Phylogenetic Inference using Mixtures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kviman%2C+O">Oskar Kviman</a>, 
<a href="/search/cs?searchtype=author&query=Mol%C3%A9n%2C+R">Ricky Mol&#xe9;n</a>, 
<a href="/search/cs?searchtype=author&query=Lagergren%2C+J">Jens Lagergren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We present VBPI-Mixtures, an algorithm designed to enhance the accuracy of
phylogenetic posterior distributions, particularly for tree-topology and
branch-length approximations. Despite the Variational Bayesian Phylogenetic
Inference (VBPI), a leading-edge black-box variational inference (BBVI)
framework, achieving remarkable approximations of these distributions, the
multimodality of the tree-topology posterior presents a formidable challenge to
sampling-based learning techniques such as BBVI. Advanced deep learning
methodologies such as normalizing flows and graph neural networks have been
explored to refine the branch-length posterior approximation, yet efforts to
ameliorate the posterior approximation over tree topologies have been lacking.
Our novel VBPI-Mixtures algorithm bridges this gap by harnessing the latest
breakthroughs in mixture learning within the BBVI domain. As a result,
VBPI-Mixtures is capable of capturing distributions over tree-topologies that
VBPI fails to model. We deliver state-of-the-art performance on difficult
density estimation tasks across numerous real phylogenetic datasets.
</p>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00942" title="Abstract">arXiv:2310.00942</a> [<a href="/pdf/2310.00942" title="Download PDF">pdf</a>, <a href="/format/2310.00942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Easier Said Than Done: The Failure of Top-Level Cybersecurity Advice for  Consumer IoT Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Harten%2C+V">Veerle van Harten</a>, 
<a href="/search/cs?searchtype=author&query=Ga%C3%B1%C3%A1n%2C+C+H">Carlos Hern&#xe1;ndez Ga&#xf1;&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=van+Eeten%2C+M">Michel van Eeten</a>, 
<a href="/search/cs?searchtype=author&query=Parkin%2C+S">Simon Parkin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 1 figure, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Consumer IoT devices are generally assumed to lack adequate default security,
thus requiring user action. However, it may not be immediately clear to users
what action to take and how. This uncertainty begs the question of what the
minimum is that the user-base can reliably be asked to do as a prompt to secure
their devices. To explore this question, we analyze security actions advocated
at a national level and how these connect to user materials for a range of
specific devices. We identify four pieces of converging advice across three
nation-level initiatives. We then assess the extent to which these pieces of
advice are aligned with instruction materials for 40 different IoT devices
across five device classes (including device manuals and manufacturer
websites). We expose a disconnect between the advice and the device materials.
A stunning finding is that there is not a single assessed device to which all
four top pieces of converging advice can be applied. At best, the supporting
materials for 36 of the 40 devices provide sufficient information to apply just
two of the four pieces of advice, typically the installation and enabling of
(auto)updates. As something of a contradiction, it is necessary for a
non-expert user to assess whether expert advice applies to a device. This risks
additional user burden and proxy changes being made without the proposed
security benefits. We propose recommendations, including that governments and
researchers alike should declare their own working models of IoT devices when
considering the user view.
</p>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00943" title="Abstract">arXiv:2310.00943</a> [<a href="/pdf/2310.00943" title="Download PDF">pdf</a>, <a href="/format/2310.00943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Blind Image Deblurring Based on Framelet Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zarebnia%2C+M">M. Zarebnia</a>, 
<a href="/search/cs?searchtype=author&query=Parvaz%2C+R">R. Parvaz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The problem of image blurring is one of the most studied topics in the field
of image processing. Image blurring is caused by various factors such as hand
or camera shake. To restore the blurred image, it is necessary to know
information about the point spread function (PSF). And because in the most
cases it is not possible to accurately calculate the PSF, we are dealing with
an approximate kernel. In this paper, the semi-blind image deblurring problem
are studied. Due to the fact that the model of the deblurring problems is an
ill-conditioned problem, it is not possible to solve this problem directly. One
of the most efficient ways to solve this problem is to use the total variation
(TV) method. In the proposed algorithm, by using the framelet transform and
fractional calculations, the TV method is improved. The proposed method is used
on different types of images and is compared with existing methods with
different types of tests.
</p>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00944" title="Abstract">arXiv:2310.00944</a> [<a href="/pdf/2310.00944" title="Download PDF">pdf</a>, <a href="/format/2310.00944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust 3D Object Detection In Rainy Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piroli%2C+A">Aldi Piroli</a>, 
<a href="/search/cs?searchtype=author&query=Dallabetta%2C+V">Vinzenz Dallabetta</a>, 
<a href="/search/cs?searchtype=author&query=Kopp%2C+J">Johannes Kopp</a>, 
<a href="/search/cs?searchtype=author&query=Walessa%2C+M">Marc Walessa</a>, 
<a href="/search/cs?searchtype=author&query=Meissner%2C+D">Daniel Meissner</a>, 
<a href="/search/cs?searchtype=author&query=Dietmayer%2C+K">Klaus Dietmayer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at IEEE International Conference on Intelligent Transportation Systems ITSC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">LiDAR sensors are used in autonomous driving applications to accurately
perceive the environment. However, they are affected by adverse weather
conditions such as snow, fog, and rain. These everyday phenomena introduce
unwanted noise into the measurements, severely degrading the performance of
LiDAR-based perception systems. In this work, we propose a framework for
improving the robustness of LiDAR-based 3D object detectors against road spray.
Our approach uses a state-of-the-art adverse weather detection network to
filter out spray from the LiDAR point cloud, which is then used as input for
the object detector. In this way, the detected objects are less affected by the
adverse weather in the scene, resulting in a more accurate perception of the
environment. In addition to adverse weather filtering, we explore the use of
radar targets to further filter false positive detections. Tests on real-world
data show that our approach improves the robustness to road spray of several
popular 3D object detectors.
</p>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00946" title="Abstract">arXiv:2310.00946</a> [<a href="/pdf/2310.00946" title="Download PDF">pdf</a>, <a href="/format/2310.00946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distilling Influences to Mitigate Prediction Churn in Graph Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roth%2C+A">Andreas Roth</a>, 
<a href="/search/cs?searchtype=author&query=Liebig%2C+T">Thomas Liebig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ACML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Models with similar performances exhibit significant disagreement in the
predictions of individual samples, referred to as prediction churn. Our work
explores this phenomenon in graph neural networks by investigating differences
between models differing only in their initializations in their utilized
features for predictions. We propose a novel metric called Influence Difference
(ID) to quantify the variation in reasons used by nodes across models by
comparing their influence distribution. Additionally, we consider the
differences between nodes with a stable and an unstable prediction, positing
that both equally utilize different reasons and thus provide a meaningful
gradient signal to closely match two models even when the predictions for nodes
are similar. Based on our analysis, we propose to minimize this ID in Knowledge
Distillation, a domain where a new model should closely match an established
one. As an efficient approximation, we introduce DropDistillation (DD) that
matches the output for a graph perturbed by edge deletions. Our empirical
evaluation of six benchmark datasets for node classification validates the
differences in utilized features. DD outperforms previous methods regarding
prediction stability and overall performance in all considered Knowledge
Distillation experiments.
</p>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00949" title="Abstract">arXiv:2310.00949</a> [<a href="/pdf/2310.00949" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling Technorelief: Enhancing Neurodiverse Collaboration with Media  Capabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saigot%2C+M">Maylis Saigot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages. To be published in the Hawaii International Conference on System Sciences (HICSS) 2024 Proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">As the workforce settles into flexible work arrangements, researchers have
focused on the collaborative and psychological consequences of the shift. While
nearly a fifth of the world's population is estimated to be neurodivergent, the
implications of remote collaboration on the cognitive, sensory, and
socio-affective experiences of autistic workers are poorly understood. Prior
literature suggests that information and communication technologies (ICTs)
introduce major psychological stressors. Theoretically, these stressors ought
to be exceptionally straining considering autistic traits $\unicode{x2013}$
yet, studies describe a strong attraction to ICTs. We thus ask: how do digital
technologies alleviate autistic workers' experiences of their collaborative
work environment? Thirty-three interviews were conducted to address this
question. Findings suggest that digital media present capabilities that filter
input from the environment, turning it into a virtual stage that lets workers
"time out". The resulting "technorelief" enables autistic workers to tune into
their perceptions and regain control of their collaborative experiences.
</p>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00950" title="Abstract">arXiv:2310.00950</a> [<a href="/pdf/2310.00950" title="Download PDF">pdf</a>, <a href="/format/2310.00950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous Navigation of Micro Air Vehicles in Warehouses Using  Vision-based Line Following
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soh%2C+L+S">Ling Shuang Soh</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+H+W">Hann Woei Ho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this paper, we propose a vision-based solution for indoor Micro Air
Vehicle (MAV) navigation, with a primary focus on its application within
autonomous warehouses. Our work centers on the utilization of a single camera
as the primary sensor for tasks such as detection, localization, and path
planning. To achieve these objectives, we implement the HSV color detection and
the Hough Line Transform for effective line detection within warehouse
environments. The integration of a Kalman filter into our system enables the
camera to track yellow lines reliably. We evaluated the performance of our
vision-based line following algorithm through various MAV flight tests
conducted in the Gazebo 11 platform, utilizing ROS Noetic. The results of these
simulations demonstrate the system capability to successfully navigate narrow
indoor spaces. Our proposed system has the potential to significantly reduce
labor costs and enhance overall productivity in warehouse operations. This work
contributes to the growing field of MAV applications in autonomous warehouses,
addressing the need for efficient logistics and supply chain solutions.
</p>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00952" title="Abstract">arXiv:2310.00952</a> [<a href="/pdf/2310.00952" title="Download PDF">pdf</a>, <a href="/format/2310.00952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LS-VOS: Identifying Outliers in 3D Object Detections Using Latent Space  Virtual Outlier Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piroli%2C+A">Aldi Piroli</a>, 
<a href="/search/cs?searchtype=author&query=Dallabetta%2C+V">Vinzenz Dallabetta</a>, 
<a href="/search/cs?searchtype=author&query=Kopp%2C+J">Johannes Kopp</a>, 
<a href="/search/cs?searchtype=author&query=Walessa%2C+M">Marc Walessa</a>, 
<a href="/search/cs?searchtype=author&query=Meissner%2C+D">Daniel Meissner</a>, 
<a href="/search/cs?searchtype=author&query=Dietmayer%2C+K">Klaus Dietmayer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at IEEE International Conference on Intelligent Transportation Systems ITSC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">LiDAR-based 3D object detectors have achieved unprecedented speed and
accuracy in autonomous driving applications. However, similar to other neural
networks, they are often biased toward high-confidence predictions or return
detections where no real object is present. These types of detections can lead
to a less reliable environment perception, severely affecting the functionality
and safety of autonomous vehicles. We address this problem by proposing LS-VOS,
a framework for identifying outliers in 3D object detections. Our approach
builds on the idea of Virtual Outlier Synthesis (VOS), which incorporates
outlier knowledge during training, enabling the model to learn more compact
decision boundaries. In particular, we propose a new synthesis approach that
relies on the latent space of an auto-encoder network to generate outlier
features with a parametrizable degree of similarity to in-distribution
features. In extensive experiments, we show that our approach improves the
outlier detection capabilities of a state-of-the-art object detector while
maintaining high 3D object detection performance.
</p>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00953" title="Abstract">arXiv:2310.00953</a> [<a href="/pdf/2310.00953" title="Download PDF">pdf</a>, <a href="/format/2310.00953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel IoT Trust Model Leveraging Fully Distributed Behavioral  Fingerprinting and Secure Delegation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arazzi%2C+M">Marco Arazzi</a>, 
<a href="/search/cs?searchtype=author&query=Nicolazzo%2C+S">Serena Nicolazzo</a>, 
<a href="/search/cs?searchtype=author&query=Nocera%2C+A">Antonino Nocera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With the number of connected smart devices expected to constantly grow in the
next years, Internet of Things (IoT) solutions are experimenting a booming
demand to make data collection and processing easier. The ability of IoT
appliances to provide pervasive and better support to everyday tasks, in most
cases transparently to humans, is also achieved through the high degree of
autonomy of such devices. However, the higher the number of new capabilities
and services provided in an autonomous way, the wider the attack surface that
exposes users to data hacking and lost. In this scenario, many critical
challenges arise also because IoT devices have heterogeneous computational
capabilities (i.e., in the same network there might be simple sensors/actuators
as well as more complex and smart nodes). In this paper, we try to provide a
contribution in this setting, tackling the non-trivial issues of equipping
smart things with a strategy to evaluate, also through their neighbors, the
trustworthiness of an object in the network before interacting with it. To do
so, we design a novel and fully distributed trust model exploiting devices'
behavioral fingerprints, a distributed consensus mechanism and the Blockchain
technology. Beyond the detailed description of our framework, we also
illustrate the security model associated with it and the tests carried out to
evaluate its correctness and performance.
</p>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00954" title="Abstract">arXiv:2310.00954</a> [<a href="/pdf/2310.00954" title="Download PDF">pdf</a>, <a href="/format/2310.00954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Helpful do Novice Programmers Find the Feedback of an Automated  Repair Tool?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurniawan%2C+O">Oka Kurniawan</a>, 
<a href="/search/cs?searchtype=author&query=Poskitt%2C+C+M">Christopher M. Poskitt</a>, 
<a href="/search/cs?searchtype=author&query=Hoque%2C+I+A">Ismam Al Hoque</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+N+T+S">Norman Tiong Seng Lee</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%A9gourel%2C+C">Cyrille J&#xe9;gourel</a>, 
<a href="/search/cs?searchtype=author&query=Sockalingam%2C+N">Nachamma Sockalingam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Experience report accepted by the International Conference on Teaching, Assessment, and Learning for Engineering (TALE'23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Immediate feedback has been shown to improve student learning. In programming
courses, immediate, automated feedback is typically provided in the form of
pre-defined test cases run by a submission platform. While these are excellent
for highlighting the presence of logical errors, they do not provide novice
programmers enough scaffolding to help them identify where an error is or how
to fix it. To address this, several tools have been developed that provide
richer feedback in the form of program repairs. Studies of such tools, however,
tend to focus more on whether correct repairs can be generated, rather than how
novices are using them. In this paper, we describe our experience of using
CLARA, an automated repair tool, to provide feedback to novices. First, we
extended CLARA to support a larger subset of the Python language, before
integrating it with the Jupyter Notebooks used for our programming exercises.
Second, we devised a preliminary study in which students tackled programming
problems with and without support of the tool using the 'think aloud' protocol.
We found that novices often struggled to understand the proposed repairs,
echoing the well-known challenge to understand compiler/interpreter messages.
Furthermore, we found that students valued being told where a fix was needed -
without necessarily the fix itself - suggesting that 'less may be more' from a
pedagogical perspective.
</p>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00955" title="Abstract">arXiv:2310.00955</a> [<a href="/pdf/2310.00955" title="Download PDF">pdf</a>, <a href="/ps/2310.00955" title="Download PostScript">ps</a>, <a href="/format/2310.00955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimally truncated WKB approximation for the highly oscillatory  stationary 1D Schr&#xf6;dinger equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=K%C3%B6rner%2C+J">Jannis K&#xf6;rner</a>, 
<a href="/search/math?searchtype=author&query=Arnold%2C+A">Anton Arnold</a>, 
<a href="/search/math?searchtype=author&query=Klein%2C+C">Christian Klein</a>, 
<a href="/search/math?searchtype=author&query=Melenk%2C+J+M">Jens Markus Melenk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We discuss the numerical solution of initial value problems for
$\varepsilon^2\,\varphi''+a(x)\,\varphi=0$ in the highly oscillatory regime,
i.e., with $a(x)&gt;0$ and $0&lt;\varepsilon\ll 1$. We analyze and implement an
approximate solution based on the well-known WKB-ansatz. The resulting
approximation error is of magnitude $\mathcal{O}(\varepsilon^{N})$ where $N$
refers to the truncation order of the underlying asymptotic series. When the
optimal truncation order $N_{opt}$ is chosen, the error behaves like
$\mathcal{O}(\varepsilon^{-2}\exp(-c\varepsilon^{-1}))$ with some $c&gt;0$.
</p>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00956" title="Abstract">arXiv:2310.00956</a> [<a href="/pdf/2310.00956" title="Download PDF">pdf</a>, <a href="/format/2310.00956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semiframes: algebras of heterogeneous consensus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gabbay%2C+M">Murdoch Gabbay</a>, 
<a href="/search/cs?searchtype=author&query=Losa%2C+G">Giuliano Losa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2303.09287">arXiv:2303.09287</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Category Theory (math.CT); General Topology (math.GN)

</div>
<p class="mathjax">Semitopologies model consensus in distributed system by equating the notion
of a quorum -- a set of participants sufficient to make local progress -- with
that of an open set. This yields a topology-like theory of consensus, but
semitopologies generalise topologies, since the intersection of two quorums
need not necessarily be a quorum. The semitopological model of consensus is
naturally heterogeneous and local, just like topologies can be heterogenous and
local, and for the same reasons: points may have different quorums and there is
no restriction that open sets / quorums be uniformly generated (e.g. open sets
can be something other than two-thirds majorities of the points in the space).
<br />Semiframes are an algebraic abstraction of semitopologies. They are to
semitopologies as frames are to topologies. We give a notion of semifilter,
which plays a role analogous to filters, and show how to build a semiframe out
of the open sets of a semitopology, and a semitopology out of the semifilters
of a semiframe.
<br />We define suitable notions of category and morphism and prove a categorical
duality between (sober) semiframes and (spatial) semitopologies, and
investigate well-behavedness properties on semitopologies and semiframes across
the duality. Surprisingly, the structure of semiframes is not what one might
initially expect just from looking at semitopologies, and the canonical
structure required for the duality result -- a compatibility relation *,
generalising sets intersection -- is also canonical for expressing
well-behavedness properties.
<br />Overall, we deliver a new categorical, algebraic, abstract framework within
which to study consensus on distributed systems, and which is also simply
interesting to consider as a mathematical theory in its own right.
</p>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00958" title="Abstract">arXiv:2310.00958</a> [<a href="/pdf/2310.00958" title="Download PDF">pdf</a>, <a href="/format/2310.00958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constant Approximation for Private Interdependent Valuations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eden%2C+A">Alon Eden</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+M">Michal Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Goldner%2C+K">Kira Goldner</a>, 
<a href="/search/cs?searchtype=author&query=Mauras%2C+S">Simon Mauras</a>, 
<a href="/search/cs?searchtype=author&query=Mohan%2C+D">Divyarthi Mohan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In 64th IEEE Symposium on Foundations of Computer Science (FOCS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">The celebrated model of auctions with interdependent valuations, introduced
by Milgrom and Weber in 1982, has been studied almost exclusively under private
signals $s_1, \ldots, s_n$ of the $n$ bidders and public valuation functions
$v_i(s_1, \ldots, s_n)$. Recent work in TCS has shown that this setting admits
a constant approximation to the optimal social welfare if the valuations
satisfy a natural property called submodularity over signals (SOS). More
recently, Eden et al. (2022) have extended the analysis of interdependent
valuations to include settings with private signals and private valuations, and
established $O(\log^2 n)$-approximation for SOS valuations. In this paper we
show that this setting admits a constant factor approximation, settling the
open question raised by Eden et al. (2022).
</p>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00960" title="Abstract">arXiv:2310.00960</a> [<a href="/pdf/2310.00960" title="Download PDF">pdf</a>, <a href="/format/2310.00960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A pragmatic workflow for research software engineering in computational  science
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mari%C4%87%2C+T">Tomislav Mari&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Gl%C3%A4ser%2C+D">Dennis Gl&#xe4;ser</a>, 
<a href="/search/cs?searchtype=author&query=Lehr%2C+J">Jan-Patrick Lehr</a>, 
<a href="/search/cs?searchtype=author&query=Papagiannidis%2C+I">Ioannis Papagiannidis</a>, 
<a href="/search/cs?searchtype=author&query=Lambie%2C+B">Benjamin Lambie</a>, 
<a href="/search/cs?searchtype=author&query=Bischof%2C+C">Christian Bischof</a>, 
<a href="/search/cs?searchtype=author&query=Bothe%2C+D">Dieter Bothe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2208.07460">arXiv:2208.07460</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">University research groups in Computational Science and Engineering (CSE)
generally lack dedicated funding and personnel for Research Software
Engineering (RSE), which, combined with the pressure to maximize the number of
scientific publications, shifts the focus away from sustainable research
software development and reproducible results. The neglect of RSE in CSE at
University research groups negatively impacts the scientific output: research
data - including research software - related to a CSE publication cannot be
found, reproduced, or re-used, different ideas are not combined easily into new
ideas, and published methods must very often be re-implemented to be
investigated further. This slows down CSE research significantly, resulting in
considerable losses in time and, consequentially, public funding.
<br />We propose a RSE workflow for Computational Science and Engineering (CSE)
that addresses these challenges, that improves the quality of research output
in CSE. Our workflow applies established software engineering practices adapted
for CSE: software testing, result visualization, and periodical cross-linking
of software with reports/publications and data, timed by milestones in the
scientific publication process. The workflow introduces minimal work overhead,
crucial for university research groups, and delivers modular and tested
software linked to publications whose results can easily be reproduced. We
define research software quality from a perspective of a pragmatic researcher:
the ability to quickly find the publication, data, and software related to a
published research idea, quickly reproduce results, understand or re-use a CSE
method, and finally extend the method with new research ideas.
</p>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00962" title="Abstract">arXiv:2310.00962</a> [<a href="/pdf/2310.00962" title="Download PDF">pdf</a>, <a href="/ps/2310.00962" title="Download PostScript">ps</a>, <a href="/format/2310.00962" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Agent Bayesian Optimization with Coupled Black-Box and Affine  Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenjie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuning Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Svetozarevic%2C+B">Bratislav Svetozarevic</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+C+N">Colin N. Jones</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper studies the problem of distributed multi-agent Bayesian
optimization with both coupled black-box constraints and known affine
constraints. A primal-dual distributed algorithm is proposed that achieves
similar regret/violation bounds as those in the single-agent case for the
black-box objective and constraint functions. Additionally, the algorithm
guarantees an $\mathcal{O}(N\sqrt{T})$ bound on the cumulative violation for
the known affine constraints, where $N$ is the number of agents. Hence, it is
ensured that the average of the samples satisfies the affine constraints up to
the error $\mathcal{O}({N}/{\sqrt{T}})$. Furthermore, we characterize certain
conditions under which our algorithm can bound a stronger metric of cumulative
violation and provide best-iterate convergence without affine constraint. The
method is then applied to both sampled instances from Gaussian processes and a
real-world optimal power allocation problem for wireless communication; the
results show that our method simultaneously provides close-to-optimal
performance and maintains minor violations on average, corroborating our
theoretical analysis.
</p>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00963" title="Abstract">arXiv:2310.00963</a> [<a href="/pdf/2310.00963" title="Download PDF">pdf</a>, <a href="/ps/2310.00963" title="Download PostScript">ps</a>, <a href="/format/2310.00963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-order WKB-based Method For The 1D Stationary Schr&#xf6;dinger Equation  In The Semi-classical Limit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Arnold%2C+A">Anton Arnold</a>, 
<a href="/search/math?searchtype=author&query=K%C3%B6rner%2C+J">Jannis K&#xf6;rner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We consider initial value problems for
$\varepsilon^2\,\varphi''+a(x)\,\varphi=0$ in the highly oscillatory regime,
i.e., with $a(x)&gt;0$ and $0&lt;\varepsilon\ll 1$. We discuss their efficient
numerical integration on coarse grids, but still yielding accurate solutions.
The $\mathcal{O}(h^2)$ one-step method from [2] is based on an analytic
WKB-preprocessing of the equation. Here we extend this method to
$\mathcal{O}(h^3)$ accuracy.
</p>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00964" title="Abstract">arXiv:2310.00964</a> [<a href="/pdf/2310.00964" title="Download PDF">pdf</a>, <a href="/format/2310.00964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> All by Myself: Learning Individualized Competitive Behaviour with a  Contrastive Reinforcement Learning optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barros%2C+P">Pablo Barros</a>, 
<a href="/search/cs?searchtype=author&query=Sciutti%2C+A">Alessandra Sciutti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In a competitive game scenario, a set of agents have to learn decisions that
maximize their goals and minimize their adversaries' goals at the same time.
Besides dealing with the increased dynamics of the scenarios due to the
opponents' actions, they usually have to understand how to overcome the
opponent's strategies. Most of the common solutions, usually based on continual
learning or centralized multi-agent experiences, however, do not allow the
development of personalized strategies to face individual opponents. In this
paper, we propose a novel model composed of three neural layers that learn a
representation of a competitive game, learn how to map the strategy of specific
opponents, and how to disrupt them. The entire model is trained online, using a
composed loss based on a contrastive optimization, to learn competitive and
multiplayer games. We evaluate our model on a pokemon duel scenario and the
four-player competitive Chef's Hat card game. Our experiments demonstrate that
our model achieves better performance when playing against offline, online, and
competitive-specific models, in particular when playing against the same
opponent multiple times. We also present a discussion on the impact of our
model, in particular on how well it deals with on specific strategy learning
for each of the two scenarios.
</p>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00965" title="Abstract">arXiv:2310.00965</a> [<a href="/pdf/2310.00965" title="Download PDF">pdf</a>, <a href="/format/2310.00965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective Learning with Node Perturbation in Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dalm%2C+S">Sander Dalm</a>, 
<a href="/search/cs?searchtype=author&query=van+Gerven%2C+M">Marcel van Gerven</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+N">Nasir Ahmad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Backpropagation (BP) is the dominant and most successful method for training
parameters of deep neural network models. However, BP relies on two
computationally distinct phases, does not provide a satisfactory explanation of
biological learning, and can be challenging to apply for training of networks
with discontinuities or noisy node dynamics. By comparison, node perturbation
(NP) proposes learning by the injection of noise into the network activations,
and subsequent measurement of the induced loss change. NP relies on two forward
(inference) passes, does not make use of network derivatives, and has been
proposed as a model for learning in biological systems. However, standard NP is
highly data inefficient and unstable due to its unguided, noise-based, activity
search. In this work, we investigate different formulations of NP and relate it
to the concept of directional derivatives as well as combining it with a
decorrelating mechanism for layer-wise inputs. We find that a closer alignment
with directional derivatives, and induction of decorrelation of inputs at every
layer significantly enhances performance of NP learning making it competitive
with BP.
</p>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00967" title="Abstract">arXiv:2310.00967</a> [<a href="/pdf/2310.00967" title="Download PDF">pdf</a>, <a href="/format/2310.00967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MiCRO: Near-Zero Cost Gradient Sparsification for Scaling and  Accelerating Distributed DNN Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+D">Daegun Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Sangyoon Oh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30th IEEE International Conference on High Performance Computing, Data, and Analytics (HiPC 2023). Code: <a href="https://github.com/kljp/micro">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Gradient sparsification is a communication optimisation technique for scaling
and accelerating distributed deep neural network (DNN) training. It reduces the
increasing communication traffic for gradient aggregation. However, existing
sparsifiers have poor scalability because of the high computational cost of
gradient selection and/or increase in communication traffic. In particular, an
increase in communication traffic is caused by gradient build-up and
inappropriate threshold for gradient selection.
<br />To address these challenges, we propose a novel gradient sparsification
method called MiCRO. In MiCRO, the gradient vector is partitioned, and each
partition is assigned to the corresponding worker. Each worker then selects
gradients from its partition, and the aggregated gradients are free from
gradient build-up. Moreover, MiCRO estimates the accurate threshold to maintain
the communication traffic as per user requirement by minimising the compression
ratio error. MiCRO enables near-zero cost gradient sparsification by solving
existing problems that hinder the scalability and acceleration of distributed
DNN training. In our extensive experiments, MiCRO outperformed state-of-the-art
sparsifiers with an outstanding convergence rate.
</p>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00968" title="Abstract">arXiv:2310.00968</a> [<a href="/pdf/2310.00968" title="Download PDF">pdf</a>, <a href="/format/2310.00968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variance-Aware Regret Bounds for Stochastic Contextual Dueling Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di%2C+Q">Qiwei Di</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+T">Tao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Heyang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Farnoud%2C+F">Farzad Farnoud</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Dueling bandits is a prominent framework for decision-making involving
preferential feedback, a valuable feature that fits various applications
involving human interaction, such as ranking, information retrieval, and
recommendation systems. While substantial efforts have been made to minimize
the cumulative regret in dueling bandits, a notable gap in the current research
is the absence of regret bounds that account for the inherent uncertainty in
pairwise comparisons between the dueling arms. Intuitively, greater uncertainty
suggests a higher level of difficulty in the problem. To bridge this gap, this
paper studies the problem of contextual dueling bandits, where the binary
comparison of dueling arms is generated from a generalized linear model (GLM).
We propose a new SupLinUCB-type algorithm that enjoys computational efficiency
and a variance-aware regret bound $\tilde O\big(d\sqrt{\sum_{t=1}^T\sigma_t^2}
+ d\big)$, where $\sigma_t$ is the variance of the pairwise comparison in round
$t$, $d$ is the dimension of the context vectors, and $T$ is the time horizon.
Our regret bound naturally aligns with the intuitive expectation in scenarios
where the comparison is deterministic, the algorithm only suffers from an
$\tilde O(d)$ regret. We perform empirical experiments on synthetic data to
confirm the advantage of our method over previous variance-agnostic algorithms.
</p>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00970" title="Abstract">arXiv:2310.00970</a> [<a href="/pdf/2310.00970" title="Download PDF">pdf</a>, <a href="/format/2310.00970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EALM: Introducing Multidimensional Ethical Alignment in Conversational  Information Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yiyao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuxiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sakai%2C+T">Tetsuya Sakai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Artificial intelligence (AI) technologies should adhere to human norms to
better serve our society and avoid disseminating harmful or misleading
information, particularly in Conversational Information Retrieval (CIR).
Previous work, including approaches and datasets, has not always been
successful or sufficiently robust in taking human norms into consideration. To
this end, we introduce a workflow that integrates ethical alignment, with an
initial ethical judgment stage for efficient data screening. To address the
need for ethical judgment in CIR, we present the QA-ETHICS dataset, adapted
from the ETHICS benchmark, which serves as an evaluation tool by unifying
scenarios and label meanings. However, each scenario only considers one ethical
concept. Therefore, we introduce the MP-ETHICS dataset to evaluate a scenario
under multiple ethical concepts, such as justice and Deontology. In addition,
we suggest a new approach that achieves top performance in both binary and
multi-label ethical judgment tasks. Our research provides a practical method
for introducing ethical alignment into the CIR workflow. The data and code are
available at https://github.com/wanng-ide/ealm .
</p>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00971" title="Abstract">arXiv:2310.00971</a> [<a href="/pdf/2310.00971" title="Download PDF">pdf</a>, <a href="/format/2310.00971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BeBOP -- Combining Reactive Planning and Bayesian Optimization to Solve  Robotic Manipulation Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Styrud%2C+J">Jonathan Styrud</a>, 
<a href="/search/cs?searchtype=author&query=Mayr%2C+M">Matthias Mayr</a>, 
<a href="/search/cs?searchtype=author&query=Hellsten%2C+E">Erik Hellsten</a>, 
<a href="/search/cs?searchtype=author&query=Krueger%2C+V">Volker Krueger</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+C">Christian Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Robotic systems for manipulation tasks are increasingly expected to be easy
to configure for new tasks. While in the past, robot programs were often
written statically and tuned manually, the current, faster transition times
call for robust, modular and interpretable solutions that also allow a robotic
system to learn how to perform a task. We propose the method Behavior-based
Bayesian Optimization and Planning (BeBOP) that combines two approaches for
generating behavior trees: we build the structure using a reactive planner and
learn specific parameters with Bayesian optimization. The method is evaluated
on a set of robotic manipulation benchmarks and is shown to outperform
state-of-the-art reinforcement learning algorithms by being up to 46 times
faster while simultaneously being less dependent on reward shaping. We also
propose a modification to the uncertainty estimate for the random forest
surrogate models that drastically improves the results.
</p>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00972" title="Abstract">arXiv:2310.00972</a> [<a href="/pdf/2310.00972" title="Download PDF">pdf</a>, <a href="/format/2310.00972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the completely positive kernels for nonuniform meshes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Feng%2C+Y">Yuanyuan Feng</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2304.06293">arXiv:2304.06293</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Classical Analysis and ODEs (math.CA)

</div>
<p class="mathjax">The complete positivity, i.e., positivity of the resolvent kernels, for
convolutional kernels is an important property for the positivity property and
asymptotic behaviors of Volterra equations. We inverstigate the discrete
analogue of the complete positivity properties, especially for convolutional
kernels on nonuniform meshes. Through an operation which we call
pseudo-convolution, we introduce the complete positivity property for discrete
kernels on nonuniform meshes and establish the criterion for the complete
positivity. Lastly, we apply our theory to the L1 discretization of time
fractional differential equations on nonuniform meshes.
</p>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00973" title="Abstract">arXiv:2310.00973</a> [<a href="/pdf/2310.00973" title="Download PDF">pdf</a>, <a href="/format/2310.00973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-Checking in the Loop Model-Based Testing for Automotive Operating  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aoki%2C+T">Toshiaki Aoki</a> (1), 
<a href="/search/cs?searchtype=author&query=Hata%2C+A">Aritoshi Hata</a> (2), 
<a href="/search/cs?searchtype=author&query=Kanamori%2C+K">Kazusato Kanamori</a> (2), 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+S">Satoshi Tanaka</a> (2), 
<a href="/search/cs?searchtype=author&query=Kawamoto%2C+Y">Yuta Kawamoto</a> (3), 
<a href="/search/cs?searchtype=author&query=Tanase%2C+Y">Yasuhiro Tanase</a> (3), 
<a href="/search/cs?searchtype=author&query=Imai%2C+M">Masumi Imai</a> (3), 
<a href="/search/cs?searchtype=author&query=Shigemitsu%2C+F">Fumiya Shigemitsu</a> (4), 
<a href="/search/cs?searchtype=author&query=Gondo%2C+M">Masaki Gondo</a> (4), 
<a href="/search/cs?searchtype=author&query=Kishi%2C+T">Tomoji Kishi</a> (5) ((1) JAIST, (2) DENSO CORPORATION, (3) DENSO CREATE INC., (4) eSOL Co., Ltd, (5) Waseda University)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">While vehicles have primarily been controlled through mechanical means in
years past, an increasing number of embedded control systems are being
installed and used, keeping pace with advances in electronic control technology
and performance. Automotive systems consist of multiple components developed by
a range of vendors. To accelerate developments in embedded control systems,
industrial standards such as AUTOSAR are being defined for automotive systems,
including the design of operating system and middleware technologies. Crucial
to ensuring the safety of automotive systems, the operating system is
foundational software on which many automotive applications are executed. In
this paper, we propose an integrated model-based method for verifying
automotive operating systems; our method is called Model-Checking in the Loop
Model-Based Testing (MCIL-MBT). In MCIL-MBT, we create a model that formalizes
specifications of automotive operating systems and verifies the specifications
via model-checking. Next, we conduct model-based testing with the verified
model to ensure that a specific operating system implementation conforms to the
model. These verification and testing stages are iterated over until no flaws
are detected. Our method has already been introduced to an automotive system
supplier and an operating system vendor. Through our approach, we successfully
identified flaws that were not detected by conventional review and testing
methods.
</p>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00975" title="Abstract">arXiv:2310.00975</a> [<a href="/pdf/2310.00975" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Analysis of Synchronous Motor Drives under Concurrent Errors  in Position and Current Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pramod%2C+P">Prerit Pramod</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Field oriented control of permanent magnet synchronous motor drives involves
the closed-loop regulation of currents in the synchronous reference frame. The
current feedback is directly affected by errors in both position and stationary
frame current measurements. This paper presents the exact analytical expression
for estimated synchronous frame currents under simultaneous errors in both
sensors along with a detailed analysis of the incorrect estimation on the
closed-loop current control performance.
</p>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00976" title="Abstract">arXiv:2310.00976</a> [<a href="/pdf/2310.00976" title="Download PDF">pdf</a>, <a href="/ps/2310.00976" title="Download PostScript">ps</a>, <a href="/format/2310.00976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Division with Subjective Divisibility
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bei%2C+X">Xiaohui Bei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xinhang Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appears in the 19th Conference on Web and Internet Economics (WINE), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">The classic fair division problems assume the resources to be allocated are
either divisible or indivisible, or contain a mixture of both, but the agents
always have a predetermined and uncontroversial agreement on the
(in)divisibility of the resources. In this paper, we propose and study a new
model for fair division in which agents have their own subjective divisibility
over the goods to be allocated. That is, some agents may find a good to be
indivisible and get utilities only if they receive the whole good, while others
may consider the same good to be divisible and thus can extract utilities
according to the fraction of the good they receive. We investigate fairness
properties that can be achieved when agents have subjective divisibility.
First, we consider the maximin share (MMS) guarantee and show that the
worst-case MMS approximation guarantee is at most $2/3$ for $n \geq 2$ agents
and this ratio is tight in the two- and three-agent cases. This is in contrast
to the classic fair division settings involving two or three agents. We also
give an algorithm that produces a $1/2$-MMS allocation for an arbitrary number
of agents. Second, we adapt the notion of envy-freeness for mixed goods (EFM)
to our model and show that EFM is incompatible with non-wastefulness, a rather
weak economic efficiency notion. On the positive side, we prove that an EFM and
non-wasteful allocation always exists for two agents if at most one good is
discarded.
</p>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00977" title="Abstract">arXiv:2310.00977</a> [<a href="/pdf/2310.00977" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position Sensing Errors in Synchronous Motor Drives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pramod%2C+P">Prerit Pramod</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Non-ideal position estimation results in degraded performance of synchronous
motor drive systems due to reduction of the average capability of the drive as
well as torque harmonics of different orders. The signature and extent of the
performance degradation is further dependent, quite significantly, on the
current control architecture, i.e., feedforward or feedback control, employed.
This paper presents a comprehensive analysis of non-idealities or errors in
position estimation and their effects on the control performance of synchronous
motor drives. Analytical models capturing the error in various signals caused
by position sensing errors in the drive system for different control
architectures are presented and are validated with simulation and experimental
results on a prototype permanent magnet synchronous motor drive.
</p>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00981" title="Abstract">arXiv:2310.00981</a> [<a href="/pdf/2310.00981" title="Download PDF">pdf</a>, <a href="/format/2310.00981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Reinforcement Learning to Optimize Responses in Care Processes: A  Case Study on Aggression Incidents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verhoef%2C+B+J">Bart J. Verhoef</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xixi Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Previous studies have used prescriptive process monitoring to find actionable
policies in business processes and conducted case studies in similar domains,
such as the loan application process and the traffic fine process. However,
care processes tend to be more dynamic and complex. For example, at any stage
of a care process, a multitude of actions is possible. In this paper, we follow
the reinforcement approach and train a Markov decision process using event data
from a care process. The goal was to find optimal policies for staff members
when clients are displaying any type of aggressive behavior. We used the
reinforcement learning algorithms Q-learning and SARSA to find optimal
policies. Results showed that the policies derived from these algorithms are
similar to the most frequent actions currently used but provide the staff
members with a few more options in certain situations.
</p>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00982" title="Abstract">arXiv:2310.00982</a> [<a href="/pdf/2310.00982" title="Download PDF">pdf</a>, <a href="/format/2310.00982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViPlanner: Visual Semantic Imperative Learning for Local Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roth%2C+P">Pascal Roth</a>, 
<a href="/search/cs?searchtype=author&query=Nubert%2C+J">Julian Nubert</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+M">Mayank Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+M">Marco Hutter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Real-time path planning in outdoor environments still challenges modern
robotic systems due to differences in terrain traversability, diverse
obstacles, and the necessity for fast decision-making. Established approaches
have primarily focused on geometric navigation solutions, which work well for
structured geometric obstacles but have limitations regarding the semantic
interpretation of different terrain types and their affordances. Moreover,
these methods fail to identify traversable geometric occurrences, such as
stairs. To overcome these issues, we introduce ViPlanner, a learned local path
planning approach that generates local plans based on geometric and semantic
information. The system is trained using the Imperative Learning paradigm, for
which the network weights are optimized end-to-end based on the planning task
objective. This optimization uses a differentiable formulation of a semantic
costmap, which enables the planner to distinguish between the traversability of
different terrains and accurately identify obstacles. The semantic information
is represented in 30 classes using an RGB colorspace that can effectively
encode the multiple levels of traversability. We show that the planner can
adapt to diverse real-world environments without requiring any real-world
training. In fact, the planner is trained purely in simulation, enabling a
highly scalable training data generation. Experimental results demonstrate
resistance to noise, zero-shot sim-to-real transfer, and a decrease of 38.02%
in terms of traversability cost compared to purely geometric-based approaches.
Code and models are made publicly available:
https://github.com/leggedrobotics/viplanner.
</p>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00986" title="Abstract">arXiv:2310.00986</a> [<a href="/pdf/2310.00986" title="Download PDF">pdf</a>, <a href="/format/2310.00986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-task Learning with 3D-Aware Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei-Hong Li</a>, 
<a href="/search/cs?searchtype=author&query=McDonagh%2C+S">Steven McDonagh</a>, 
<a href="/search/cs?searchtype=author&query=Leonardis%2C+A">Ales Leonardis</a>, 
<a href="/search/cs?searchtype=author&query=Bilen%2C+H">Hakan Bilen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3D-aware Multi-task Learning, Code will be available at <a href="https://github.com/VICO-UoE/MTPSL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep neural networks have become a standard building block for designing
models that can perform multiple dense computer vision tasks such as depth
estimation and semantic segmentation thanks to their ability to capture complex
correlations in high dimensional feature space across tasks. However, the
cross-task correlations that are learned in the unstructured feature space can
be extremely noisy and susceptible to overfitting, consequently hurting
performance. We propose to address this problem by introducing a structured
3D-aware regularizer which interfaces multiple tasks through the projection of
features extracted from an image encoder to a shared 3D feature space and
decodes them into their task output space through differentiable rendering. We
show that the proposed method is architecture agnostic and can be plugged into
various prior multi-task backbones to improve their performance; as we evidence
using standard benchmarks NYUv2 and PASCAL-Context.
</p>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00987" title="Abstract">arXiv:2310.00987</a> [<a href="/pdf/2310.00987" title="Download PDF">pdf</a>, <a href="/format/2310.00987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge  Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+T+S">Tin Sum Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Lucchi%2C+A">Aurelien Lucchi</a>, 
<a href="/search/cs?searchtype=author&query=Dokmani%C4%87%2C+I">Ivan Dokmani&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Kratsios%2C+A">Anastasis Kratsios</a>, 
<a href="/search/cs?searchtype=author&query=Balius%2C+D">David Balius</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Existing statistical learning guarantees for general kernel regressors often
yield loose bounds when used with finite-rank kernels. Yet, finite-rank kernels
naturally appear in several machine learning problems, e.g.\ when fine-tuning a
pre-trained deep neural network's last layer to adapt it to a novel task when
performing transfer learning. We address this gap for finite-rank kernel ridge
regression (KRR) by deriving sharp non-asymptotic upper and lower bounds for
the KRR test error of any finite-rank KRR. Our bounds are tighter than
previously derived bounds on finite-rank KRR, and unlike comparable results,
they also remain valid for any regularization parameters.
</p>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00989" title="Abstract">arXiv:2310.00989</a> [<a href="/pdf/2310.00989" title="Download PDF">pdf</a>, <a href="/ps/2310.00989" title="Download PostScript">ps</a>, <a href="/format/2310.00989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Two- and Three-valued Semantics for Impure Simplicial Complexes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Ditmarsch%2C+H">Hans van Ditmarsch</a>, 
<a href="/search/cs?searchtype=author&query=Kuznets%2C+R">Roman Kuznets</a>, 
<a href="/search/cs?searchtype=author&query=Randrianomentsoa%2C+R">Rojo Randrianomentsoa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings GandALF 2023, <a href="/abs/2309.17318">arXiv:2309.17318</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 390, 2023, pp. 50-66
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Simplicial complexes are a convenient semantic primitive to reason about
processes (agents) communicating with each other in synchronous and
asynchronous computation. Impure simplicial complexes distinguish active
processes from crashed ones, in other words, agents that are alive from agents
that are dead. In order to rule out that dead agents reason about themselves
and about other agents, three-valued epistemic semantics have been proposed
where, in addition to the usual values true and false, the third value stands
for undefined: the knowledge of dead agents is undefined and so are the
propositional variables describing their local state. Other semantics for
impure complexes are two-valued where a dead agent knows everything. Different
choices in designing a semantics produce different three-valued semantics, and
also different two-valued semantics. In this work, we categorize the available
choices by discounting the bad ones, identifying the equivalent ones, and
connecting the non-equivalent ones via a translation. The main result of the
paper is identifying the main relevant distinction to be the number of truth
values and bridging this difference by means of a novel embedding from three-
into two-valued semantics. This translation also enables us to highlight quite
fundamental modeling differences underpinning various two- and three-valued
approaches in this area of combinatorial topology. In particular, pure
complexes can be defined as those invariant under the translation.
</p>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00990" title="Abstract">arXiv:2310.00990</a> [<a href="/pdf/2310.00990" title="Download PDF">pdf</a>, <a href="/ps/2310.00990" title="Download PostScript">ps</a>, <a href="/format/2310.00990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TSO Games -- On the decidability of safety games under the total store  order semantics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Spengler%2C+S">Stephan Spengler</a> (Uppsala University), 
<a href="/search/cs?searchtype=author&query=Sil%2C+S">Sanchari Sil</a> (Chennai Mathematical Institute)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings GandALF 2023, <a href="/abs/2309.17318">arXiv:2309.17318</a>. A full version of this paper, containing the appendix, appears at <a href="/abs/2309.02862">arXiv:2309.02862</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 390, 2023, pp. 82-98
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We consider an extension of the classical Total Store Order (TSO) semantics
by expanding it to turn-based 2-player safety games. During her turn, a player
can select any of the communicating processes and perform its next transition.
We consider different formulations of the safety game problem depending on
whether one player or both of them transfer messages from the process buffers
to the shared memory. We give the complete decidability picture for all the
possible alternatives.
</p>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00994" title="Abstract">arXiv:2310.00994</a> [<a href="/pdf/2310.00994" title="Download PDF">pdf</a>, <a href="/ps/2310.00994" title="Download PostScript">ps</a>, <a href="/format/2310.00994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Uniform One-Dimensional Fragment with Alternation of Quantifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kiero%C5%84ski%2C+E">Emanuel Kiero&#x144;ski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings GandALF 2023, <a href="/abs/2309.17318">arXiv:2309.17318</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 390, 2023, pp. 1-15
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">The uniform one-dimensional fragment of first-order logic was introduced a
few years ago as a generalization of the two-variable fragment of first-order
logic to contexts involving relations of arity greater than two. Quantifiers in
this logic are used in blocks, each block consisting only of existential
quantifiers or only of universal quantifiers. In this paper we consider the
possibility of mixing quantifiers in blocks. We identify a non-trivial
variation of the logic with mixed blocks of quantifiers which retains some good
properties of the two-variable fragment and of the uniform one-dimensional
fragment: it has the finite (exponential) model property and hence decidable,
NExpTime-complete satisfiability problem.
</p>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00995" title="Abstract">arXiv:2310.00995</a> [<a href="/pdf/2310.00995" title="Download PDF">pdf</a>, <a href="/format/2310.00995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FMplex: A Novel Method for Solving Linear Real Arithmetic Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nalbach%2C+J">Jasper Nalbach</a> (RWTH Aachen University, Germany), 
<a href="/search/cs?searchtype=author&query=Promies%2C+V">Valentin Promies</a> (RWTH Aachen University, Germany), 
<a href="/search/cs?searchtype=author&query=%C3%81brah%C3%A1m%2C+E">Erika &#xc1;brah&#xe1;m</a> (RWTH Aachen University, Germany), 
<a href="/search/cs?searchtype=author&query=Kobialka%2C+P">Paul Kobialka</a> (University of Oslo, Norway)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings GandALF 2023, <a href="/abs/2309.17318">arXiv:2309.17318</a>. The extended version of this paper can be found at <a href="/abs/2309.03138">arXiv:2309.03138</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 390, 2023, pp. 16-32
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>

</div>
<p class="mathjax">In this paper we introduce a novel quantifier elimination method for
conjunctions of linear real arithmetic constraints. Our algorithm is based on
the Fourier-Motzkin variable elimination procedure, but by case splitting we
are able to reduce the worst-case complexity from doubly to singly exponential.
The adaption of the procedure for SMT solving has strong correspondence to the
simplex algorithm, therefore we name it FMplex. Besides the theoretical
foundations, we provide an experimental evaluation in the context of SMT
solving.
</p>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00996" title="Abstract">arXiv:2310.00996</a> [<a href="/pdf/2310.00996" title="Download PDF">pdf</a>, <a href="/format/2310.00996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARN: A Comprehensive Framework and Dataset for Analogical Reasoning on  Narratives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sourati%2C+Z">Zhivar Sourati</a>, 
<a href="/search/cs?searchtype=author&query=Ilievski%2C+F">Filip Ilievski</a>, 
<a href="/search/cs?searchtype=author&query=Sommerauer%2C+P">Pia Sommerauer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Analogical reasoning is one of the prime abilities of humans and is linked to
creativity and scientific discoveries. This ability has been studied
extensively in natural language processing (NLP) as well as in cognitive
psychology by proposing various benchmarks and evaluation setups. Yet, a
substantial gap exists between evaluations of analogical reasoning in cognitive
psychology and NLP. Our aim is to bridge this by computationally adapting
theories related to analogical reasoning from cognitive psychology in the
context of narratives and developing an evaluation framework large in scale.
More concretely, we propose the task of matching narratives based on system
mappings and release the Analogical Reasoning on Narratives (ARN) dataset. To
create the dataset, we devise a framework inspired by cognitive psychology
theories about analogical reasoning to utilize narratives and their components
to form mappings of different abstractness levels. These mappings are then
leveraged to create pairs of analogies and disanalogies/distractors with more
than 1k triples of query narratives, analogies, and distractors. We cover four
categories of far/near analogies and far/near distractors that allow us to
study analogical reasoning in models from distinct perspectives. In this study,
we evaluate different large language models (LLMs) on this task. Our results
demonstrate that LLMs struggle to recognize higher-order mappings when they are
not accompanied by lower-order mappings (far analogies) and show better
performance when all mappings are present simultaneously (near analogies). We
observe that in all the settings, the analogical reasoning abilities of LLMs
can be easily impaired by near distractors that form lower-order mappings with
the query narratives.
</p>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00997" title="Abstract">arXiv:2310.00997</a> [<a href="/pdf/2310.00997" title="Download PDF">pdf</a>, <a href="/format/2310.00997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Handling of Past and Future with Phenesthe+
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pitsikalis%2C+M">Manolis Pitsikalis</a> (University of Liverpool), 
<a href="/search/cs?searchtype=author&query=Lisitsa%2C+A">Alexei Lisitsa</a> (University of Liverpool), 
<a href="/search/cs?searchtype=author&query=Totzke%2C+P">Patrick Totzke</a> (University of Liverpool)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings GandALF 2023, <a href="/abs/2309.17318">arXiv:2309.17318</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 390, 2023, pp. 33-49
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Writing temporal logic formulae for properties that combine instantaneous
events with overlapping temporal phenomena of some duration is difficult in
classical temporal logics. To address this issue, in previous work we
introduced a new temporal logic with intuitive temporal modalities specifically
tailored for the representation of both instantaneous and durative phenomena.
We also provided an implementation of a complex event processing system,
Phenesthe, based on this logic, that has been applied and tested on a real
maritime surveillance scenario.
<br />In this work, we extend our temporal logic with two extra modalities to
increase its expressive power for handling future formulae. We compare the
expressive power of different fragments of our logic with Linear Temporal Logic
and dyadic first-order logic. Furthermore, we define correctness criteria for
stream processors that use our language. Last but not least, we evaluate
empirically the performance of Phenesthe+, our extended implementation, and
show that the increased expressive power does not affect efficiency
significantly.
</p>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00998" title="Abstract">arXiv:2310.00998</a> [<a href="/pdf/2310.00998" title="Download PDF">pdf</a>, <a href="/format/2310.00998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modal Logic Characterizations of Forward, Reverse, and Forward-Reverse  Bisimilarities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernardo%2C+M">Marco Bernardo</a>, 
<a href="/search/cs?searchtype=author&query=Esposito%2C+A">Andrea Esposito</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings GandALF 2023, <a href="/abs/2309.17318">arXiv:2309.17318</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 390, 2023, pp. 67-81
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Reversible systems feature both forward computations and backward
computations, where the latter undo the effects of the former in a causally
consistent manner. The compositionality properties and equational
characterizations of strong and weak variants of forward-reverse bisimilarity
as well as of its two components, i.e., forward bisimilarity and reverse
bisimilarity, have been investigated on a minimal process calculus for
nondeterministic reversible systems that are sequential, so as to be neutral
with respect to interleaving vs. truly concurrent semantics of parallel
composition. In this paper we provide logical characterizations for the
considered bisimilarities based on forward and backward modalities, which
reveals that strong and weak reverse bisimilarities respectively correspond to
strong and weak reverse trace equivalences. Moreover, we establish a clear
connection between weak forward-reverse bisimilarity and branching
bisimilarity, so that the former inherits two further logical characterizations
from the latter over a specific class of processes.
</p>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00999" title="Abstract">arXiv:2310.00999</a> [<a href="/pdf/2310.00999" title="Download PDF">pdf</a>, <a href="/format/2310.00999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CGAAL: Distributed On-The-Fly ATL Model Checker with Heuristics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carlsen%2C+F+B+%C3%98">Falke B. &#xd8;. Carlsen</a>, 
<a href="/search/cs?searchtype=author&query=Frydenskov%2C+L+B+P">Lars Bo P. Frydenskov</a>, 
<a href="/search/cs?searchtype=author&query=Jensen%2C+N+%C3%98">Nicolaj &#xd8;. Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Rasmussen%2C+J">Jener Rasmussen</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%B8rensen%2C+M+M">Mathias M. S&#xf8;rensen</a>, 
<a href="/search/cs?searchtype=author&query=Weirs%C3%B8e%2C+A+G">Asger G. Weirs&#xf8;e</a>, 
<a href="/search/cs?searchtype=author&query=Jensen%2C+M+C">Mathias C. Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Larsen%2C+K+G">Kim G. Larsen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings GandALF 2023, <a href="/abs/2309.17318">arXiv:2309.17318</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 390, 2023, pp. 99-114
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We present CGAAL, our efficient on-the-fly model checker for alternating-time
temporal logic (ATL) on concurrent game structures (CGS). We present how our
tool encodes ATL as extended dependency graphs with negation edges and employs
the distributed on-the-fly algorithm by Dalsgaard et al. Our tool offers
multiple novel search strategies for the algorithm, including DHS which is
inspired by PageRank and uses the in-degree of configurations as a heuristic,
IHS which estimates instability of assignment values, and LPS which estimates
the distance to a state satisfying the constituent property using linear
programming. CGS are input using our modelling language LCGS, where composition
and synchronisation are easily described. We prove the correctness of our
encoding, and our experiments show that our tool CGAAL is often one to three
orders of magnitude faster than the popular tool PRISM-games on case studies
from PRISM's documentation and among case studies we have developed. In our
evaluation, we also compare and evaluate our search strategies, and find that
our custom search strategies are often significantly faster than the usual
breadth-first and depth-first search strategies.
</p>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01000" title="Abstract">arXiv:2310.01000</a> [<a href="/pdf/2310.01000" title="Download PDF">pdf</a>, <a href="/ps/2310.01000" title="Download PostScript">ps</a>, <a href="/format/2310.01000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> (Un)Decidability Bounds of the Synthesis Problem for Petri Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hannibal%2C+P">Paul Hannibal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings GandALF 2023, <a href="/abs/2309.17318">arXiv:2309.17318</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 390, 2023, pp. 115-131
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Petri games are a multi-player game model for the automatic synthesis of
distributed systems, where the players are represented as tokens on a Petri net
and are grouped into environment players and system players. As long as the
players move in independent parts of the net, they do not know of each other;
when they synchronize at a joint transition, each player gets informed of the
entire causal history of the other players.
<br />We show that the synthesis problem for two-player Petri games under a global
safety condition is NP-complete and it can be solved within a non-deterministic
exponential upper bound in the case of up to 4 players. Furthermore, we show
the undecidability of the synthesis problem for Petri games with at least 6
players under a local safety condition.
</p>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01001" title="Abstract">arXiv:2310.01001</a> [<a href="/pdf/2310.01001" title="Download PDF">pdf</a>, <a href="/format/2310.01001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Causality for Reachability and Safety based on Distance  Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parreaux%2C+J">Julie Parreaux</a> (Aix Marseille Univ, CNRS, LIS, Marseille, France), 
<a href="/search/cs?searchtype=author&query=Piribauer%2C+J">Jakob Piribauer</a> (Technische Universit&#xe4;t Dresden, Germany, Technische Universit&#xe4;t M&#xfc;nchen, Germany), 
<a href="/search/cs?searchtype=author&query=Baier%2C+C">Christel Baier</a> (Technische Universit&#xe4;t Dresden, Germany)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings GandALF 2023, <a href="/abs/2309.17318">arXiv:2309.17318</a>. An extended version can be found at <a href="/abs/2308.11385">arXiv:2308.11385v1</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 390, 2023, pp. 132-149
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Investigations of causality in operational systems aim at providing
human-understandable explanations of why a system behaves as it does. There is,
in particular, a demand to explain what went wrong on a given counterexample
execution that shows that a system does not satisfy a given specification. To
this end, this paper investigates a notion of counterfactual causality in
transition systems based on Stalnaker's and Lewis' semantics of counterfactuals
in terms of most similar possible worlds and introduces a novel corresponding
notion of counterfactual causality in two-player games. Using distance
functions between paths in transition systems, this notion defines whether
reaching a certain set of states is a cause for the violation of a reachability
or safety property. Similarly, using distance functions between memoryless
strategies in reachability and safety games, it is defined whether reaching a
set of states is a cause for the fact that a given strategy for the player
under investigation is losing. The contribution of the paper is two-fold: In
transition systems, it is shown that counterfactual causality can be checked in
polynomial time for three prominent distance functions between paths. In
two-player games, the introduced notion of counterfactual causality is shown to
be checkable in polynomial time for two natural distance functions between
memoryless strategies. Further, a notion of explanation that can be extracted
from a counterfactual cause and that pinpoints changes to be made to the given
strategy in order to transform it into a winning strategy is defined. For the
two distance functions under consideration, the problem to decide whether such
an explanation imposes only minimal necessary changes to the given strategy
with respect to the used distance function turns out to be coNP-complete and
not to be solvable in polynomial time if P is not equal to NP, respectively.
</p>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01003" title="Abstract">arXiv:2310.01003</a> [<a href="/pdf/2310.01003" title="Download PDF">pdf</a>, <a href="/format/2310.01003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conflict-Aware Active Automata Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+T">Tiago Ferreira</a> (University College London), 
<a href="/search/cs?searchtype=author&query=Henry%2C+L">L&#xe9;o Henry</a> (University College London), 
<a href="/search/cs?searchtype=author&query=da+Silva%2C+R+F">Raquel Fernandes da Silva</a> (University College London), 
<a href="/search/cs?searchtype=author&query=Silva%2C+A">Alexandra Silva</a> (Cornell University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings GandALF 2023, <a href="/abs/2309.17318">arXiv:2309.17318</a>; extended version at <a href="/abs/2308.14781">arXiv:2308.14781</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 390, 2023, pp. 150-167
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Active automata learning algorithms cannot easily handle conflict in the
observation data (different outputs observed for the same inputs). This
inherent inability to recover after a conflict impairs their effective
applicability in scenarios where noise is present or the system under learning
is mutating. We propose the Conflict-Aware Active Automata Learning (C3AL)
framework to enable handling conflicting information during the learning
process. The core idea is to consider the so-called observation tree as a
first-class citizen in the learning process. Though this idea is explored in
recent work, we take it to its full effect by enabling its use with any
existing learner and minimizing the number of tests performed on the system
under learning, specially in the face of conflicts. We evaluate C3AL in a large
set of benchmarks, covering over 30 different realistic targets, and over
18,000 different scenarios. The results of the evaluation show that C3AL is a
suitable alternative framework for closed-box learning that can better handle
noise and mutations.
</p>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01004" title="Abstract">arXiv:2310.01004</a> [<a href="/pdf/2310.01004" title="Download PDF">pdf</a>, <a href="/format/2310.01004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Recursive Arrival Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Webster%2C+T">Thomas Webster</a> (University of Edinburgh)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings GandALF 2023, <a href="/abs/2309.17318">arXiv:2309.17318</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 390, 2023, pp. 168-184
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">We study an extension of the Arrival problem, called Recursive Arrival,
inspired by Recursive State Machines, which allows for a family of switching
graphs that can call each other in a recursive way. We study the computational
complexity of deciding whether a Recursive Arrival instance terminates at a
given target vertex. We show this problem is contained in NP \cap coNP, and we
show that a search version of the problem lies in UEOPL, and hence in EOPL =
PLS \cap PPAD. Furthermore, we show P-hardness of the Recursive Arrival
decision problem. By contrast, the current best-known hardness result for
Arrival is PL-hardness.
</p>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01006" title="Abstract">arXiv:2310.01006</a> [<a href="/pdf/2310.01006" title="Download PDF">pdf</a>, <a href="/format/2310.01006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative Analysis of Technical and Legal Frameworks of Various  National Digial Identity Solutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naghmouchi%2C+M">Montassar Naghmouchi</a>, 
<a href="/search/cs?searchtype=author&query=Laurent%2C+M">Maryline Laurent</a>, 
<a href="/search/cs?searchtype=author&query=Levallois-Barth%2C+C">Claire Levallois-Barth</a>, 
<a href="/search/cs?searchtype=author&query=Kaaniche%2C+N">Nesrine Kaaniche</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 4 tables, 5 labled figures, 3 figures within table 1 as illustration, 51 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR); Computers and Society (cs.CY)

</div>
<p class="mathjax">National digital identity systems have become a key requirement for easy
access to online public services, specially during Covid-19. While many
countries have adopted a national digital identity system, many are still in
the process of establishing one. Through a comparative analysis of the
technological and legal dimensions of a few selected national digital identity
solutions currently being used in different countries, we highlight the
diversity of technologies and architectures and the key role of the legal
framework of a given digital identity solution. We also present several key
issues related to the implementation of these solutions, how to ensure the
State sovereignty over them, and how to strike the right balance between
private sector and public sector needs. This position paper aims to help policy
makers, software developers and concerned users understand the challenges of
designing, implementing and using a national digital identity management system
and establishing a legal framework for digital identity management, including
personal data protection measures. The authors of this paper have a favorable
position for self-sovereign identity management systems that are based on
Blockchain technology, and we believe they are the most suitable for national
digital identity systems.
</p>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01007" title="Abstract">arXiv:2310.01007</a> [<a href="/pdf/2310.01007" title="Download PDF">pdf</a>, <a href="/ps/2310.01007" title="Download PostScript">ps</a>, <a href="/format/2310.01007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Descriptive Complexity of Groups without Abelian Normal Subgroups  (Extended Abstract)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grochow%2C+J+A">Joshua A. Grochow</a> (University of Colorado Boulder, Departments of Computer Science and Mathematics), 
<a href="/search/cs?searchtype=author&query=Levet%2C+M">Michael Levet</a> (College of Charleston, Department of Computer Science)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings GandALF 2023, <a href="/abs/2309.17318">arXiv:2309.17318</a>. arXiv admin note: text overlap with <a href="/abs/2112.11487">arXiv:2112.11487</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 390, 2023, pp. 185-202
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">In this paper, we explore the descriptive complexity theory of finite groups
by examining the power of the second Ehrenfeucht-Fraisse bijective pebble game
in Hella's (Ann. Pure Appl. Log., 1989) hierarchy. This is a Spoiler-Duplicator
game in which Spoiler can place up to two pebbles each round. While it
trivially solves graph isomorphism, it may be nontrivial for finite groups, and
other ternary relational structures. We first provide a novel generalization of
Weisfeiler-Leman (WL) coloring, which we call 2-ary WL. We then show that the
2-ary WL is equivalent to the second Ehrenfeucht-Fraisse bijective pebble game
in Hella's hierarchy.
<br />Our main result is that, in the pebble game characterization, only O(1)
pebbles and O(1) rounds are sufficient to identify all groups without Abelian
normal subgroups (a class of groups for which isomorphism testing is known to
be in P; Babai, Codenotti, &amp; Qiao, ICALP 2012). In particular, we show that
within the first few rounds, Spoiler can force Duplicator to select an
isomorphism between two such groups at each subsequent round. By Hella's
results (ibid.), this is equivalent to saying that these groups are identified
by formulas in first-order logic with generalized 2-ary quantifiers, using only
O(1) variables and O(1) quantifier depth.
</p>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01008" title="Abstract">arXiv:2310.01008</a> [<a href="/pdf/2310.01008" title="Download PDF">pdf</a>, <a href="/format/2310.01008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Objective Improvement Approach to Solving Discounted Payoff Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dell%27Erba%2C+D">Daniele Dell&#x27;Erba</a> (University of Liverpool), 
<a href="/search/cs?searchtype=author&query=Dumas%2C+A">Arthur Dumas</a> (ENS Rennes), 
<a href="/search/cs?searchtype=author&query=Schewe%2C+S">Sven Schewe</a> (University of Liverpool)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings GandALF 2023, <a href="/abs/2309.17318">arXiv:2309.17318</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 390, 2023, pp. 203-219
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">While discounted payoff games and classic games that reduce to them, like
parity and mean-payoff games, are symmetric, their solutions are not. We have
taken a fresh view on the constraints that optimal solutions need to satisfy,
and devised a novel way to converge to them, which is entirely symmetric. It
also challenges the gospel that methods for solving payoff games are either
based on strategy improvement or on value iteration.
</p>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01010" title="Abstract">arXiv:2310.01010</a> [<a href="/pdf/2310.01010" title="Download PDF">pdf</a>, <a href="/format/2310.01010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategies Resilient to Delay: Games under Delayed Control vs. Delay  Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fr%C3%A4nzle%2C+M">Martin Fr&#xe4;nzle</a> (Carl von Ossietzky Universit&#xe4;t Oldenburg), 
<a href="/search/cs?searchtype=author&query=Winter%2C+S">Sarah Winter</a> (Universit&#xe9; libre de Bruxelles), 
<a href="/search/cs?searchtype=author&query=Zimmermann%2C+M">Martin Zimmermann</a> (Aalborg University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings GandALF 2023, <a href="/abs/2309.17318">arXiv:2309.17318</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 390, 2023, pp. 220-235
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Formal Languages and Automata Theory (cs.FL); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We compare games under delayed control and delay games, two types of infinite
games modelling asynchronicity in reactive synthesis. Our main result, the
interreducibility of the existence of sure winning strategies for the
protagonist, allows to transfer known complexity results and bounds on the
delay from delay games to games under delayed control, for which no such
results had been known. We furthermore analyze existence of randomized
strategies that win almost surely, where this correspondence between the two
types of games breaks down.
</p>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01011" title="Abstract">arXiv:2310.01011</a> [<a href="/pdf/2310.01011" title="Download PDF">pdf</a>, <a href="/format/2310.01011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Fixing Clever-Hans Predictors with Counterfactual Knowledge  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bender%2C+S">Sidney Bender</a>, 
<a href="/search/cs?searchtype=author&query=Anders%2C+C+J">Christopher J. Anders</a>, 
<a href="/search/cs?searchtype=author&query=Chormai%2C+P">Pattarawatt Chormai</a>, 
<a href="/search/cs?searchtype=author&query=Marxfeld%2C+H">Heike Marxfeld</a>, 
<a href="/search/cs?searchtype=author&query=Herrmann%2C+J">Jan Herrmann</a>, 
<a href="/search/cs?searchtype=author&query=Montavon%2C+G">Gr&#xe9;goire Montavon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This paper introduces a novel technique called counterfactual knowledge
distillation (CFKD) to detect and remove reliance on confounders in deep
learning models with the help of human expert feedback. Confounders are
spurious features that models tend to rely on, which can result in unexpected
errors in regulated or safety-critical domains. The paper highlights the
benefit of CFKD in such domains and shows some advantages of counterfactual
explanations over other types of explanations. We propose an experiment scheme
to quantitatively evaluate the success of CFKD and different teachers that can
give feedback to the model. We also introduce a new metric that is better
correlated with true test performance than validation accuracy. The paper
demonstrates the effectiveness of CFKD on synthetically augmented datasets and
on real-world histopathological datasets.
</p>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01012" title="Abstract">arXiv:2310.01012</a> [<a href="/pdf/2310.01012" title="Download PDF">pdf</a>, <a href="/format/2310.01012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Algorithms for the CCA Family: Unconstrained Objectives with  Unbiased Gradients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chapman%2C+J">James Chapman</a>, 
<a href="/search/cs?searchtype=author&query=Aguila%2C+A+L">Ana Lawry Aguila</a>, 
<a href="/search/cs?searchtype=author&query=Wells%2C+L">Lennie Wells</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">The Canonical Correlation Analysis (CCA) family of methods is foundational in
multi-view learning. Regularised linear CCA methods can be seen to generalise
Partial Least Squares (PLS) and unified with a Generalized Eigenvalue Problem
(GEP) framework. However, classical algorithms for these linear methods are
computationally infeasible for large-scale data. Extensions to Deep CCA show
great promise, but current training procedures are slow and complicated. First
we propose a novel unconstrained objective that characterizes the top subspace
of GEPs. Our core contribution is a family of fast algorithms for stochastic
PLS, stochastic CCA, and Deep CCA, simply obtained by applying stochastic
gradient descent (SGD) to the corresponding CCA objectives. These methods show
far faster convergence and recover higher correlations than the previous
state-of-the-art on all standard CCA and Deep CCA benchmarks. This speed allows
us to perform a first-of-its-kind PLS analysis of an extremely large biomedical
dataset from the UK Biobank, with over 33,000 individuals and 500,000 variants.
Finally, we not only match the performance of `CCA-family' Self-Supervised
Learning (SSL) methods on CIFAR-10 and CIFAR-100 with minimal hyper-parameter
tuning, but also establish the first solid theoretical links to classical CCA,
laying the groundwork for future insights.
</p>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01015" title="Abstract">arXiv:2310.01015</a> [<a href="/pdf/2310.01015" title="Download PDF">pdf</a>, <a href="/format/2310.01015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ETGraph: A Pioneering Dataset Bridging Ethereum and Twitter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zemin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shengliang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+B">Bingqiao Luo</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bingsheng He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While numerous public blockchain datasets are available, their utility is
constrained by a singular focus on blockchain data. This constraint limits the
incorporation of relevant social network data into blockchain analysis, thereby
diminishing the breadth and depth of insight that can be derived. To address
the above limitation, we introduce ETGraph, a novel dataset that authentically
links Ethereum and Twitter, marking the first and largest dataset of its kind.
ETGraph combines Ethereum transaction records (2 million nodes and 30 million
edges) and Twitter following data (1 million nodes and 3 million edges),
bonding 30,667 Ethereum addresses with verified Twitter accounts sourced from
OpenSea. Detailed statistical analysis on ETGraph highlights the structural
differences between Twitter-matched and non-Twitter-matched Ethereum addresses.
Extensive experiments, including Ethereum link prediction, wash-trading
Ethereum addresses detection, and Twitter-Ethereum matching link prediction,
emphasize the significant role of Twitter data in enhancing Ethereum analysis.
ETGraph is available at https://etgraph.deno.dev/.
</p>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01018" title="Abstract">arXiv:2310.01018</a> [<a href="/pdf/2310.01018" title="Download PDF">pdf</a>, <a href="/format/2310.01018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlling Vision-Language Models for Universal Image Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Ziwei Luo</a>, 
<a href="/search/cs?searchtype=author&query=Gustafsson%2C+F+K">Fredrik K. Gustafsson</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Sj%C3%B6lund%2C+J">Jens Sj&#xf6;lund</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6n%2C+T+B">Thomas B. Sch&#xf6;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://algolzw.github.io/daclip-uir/index.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Vision-language models such as CLIP have shown great impact on diverse
downstream tasks for zero-shot or label-free predictions. However, when it
comes to low-level vision such as image restoration their performance
deteriorates dramatically due to corrupted inputs. In this paper, we present a
degradation-aware vision-language model (DA-CLIP) to better transfer pretrained
vision-language models to low-level vision tasks as a universal framework for
image restoration. More specifically, DA-CLIP trains an additional controller
that adapts the fixed CLIP image encoder to predict high-quality feature
embeddings. By integrating the embedding into an image restoration network via
cross-attention, we are able to pilot the model to learn a high-fidelity image
reconstruction. The controller itself will also output a degradation feature
that matches the real corruptions of the input, yielding a natural classifier
for different degradation types. In addition, we construct a mixed degradation
dataset with synthetic captions for DA-CLIP training. Our approach advances
state-of-the-art performance on both degradation-specific and unified image
restoration tasks, showing a promising direction of prompting image restoration
with large-scale pretrained vision-language models. Our code is available at
https://github.com/Algolzw/daclip-uir.
</p>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01020" title="Abstract">arXiv:2310.01020</a> [<a href="/pdf/2310.01020" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Real-World Video Dataset for the Comparison of Defogging  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duminil%2C+A">Alexandra Duminil</a>, 
<a href="/search/cs?searchtype=author&query=Tarel%2C+J">Jean-Philippe Tarel</a>, 
<a href="/search/cs?searchtype=author&query=Br%C3%A9mond%2C+R">Roland Br&#xe9;mond</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Signal Processing and Artificial Intelligence (ASPAI'
  2022), Oct 2022, Corfu, Greece
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Video restoration for noise removal, deblurring or super-resolution is
attracting more and more attention in the fields of image processing and
computer vision. Works on video restoration with data-driven approaches for fog
removal are rare however, due to the lack of datasets containing videos in both
clear and foggy conditions which are required for deep learning and
benchmarking. A new dataset, called REVIDE, was recently proposed for just that
purpose. In this paper, we implement the same approach by proposing a new
REal-world VIdeo dataset for the comparison of Defogging Algorithms (VIREDA),
with various fog densities and ground truths without fog. This small database
can serve as a test base for defogging algorithms. A video defogging algorithm
is also mentioned (still under development), with the key idea of using
temporal redundancy to minimize artefacts and exposure variations between
frames. Inspired by the success of Transformers architecture in deep learning
for various applications, we select this kind of architecture in a neural
network to show the relevance of the proposed dataset.
</p>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01022" title="Abstract">arXiv:2310.01022</a> [<a href="/pdf/2310.01022" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subtractor-Based CNN Inference Accelerator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+V">Victor Gao</a>, 
<a href="/search/cs?searchtype=author&query=Hammad%2C+I">Issam Hammad</a>, 
<a href="/search/cs?searchtype=author&query=El-Sankary%2C+K">Kamal El-Sankary</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jason Gu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE Canadian Conference on Electrical and Computer
  Engineering (CCECE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents a novel method to boost the performance of CNN inference
accelerators by utilizing subtractors. The proposed CNN preprocessing
accelerator relies on sorting, grouping, and rounding the weights to create
combinations that allow for the replacement of one multiplication operation and
addition operation by a single subtraction operation when applying convolution
during inference. Given the high cost of multiplication in terms of power and
area, replacing it with subtraction allows for a performance boost by reducing
power and area. The proposed method allows for controlling the trade-off
between performance gains and accuracy loss through increasing or decreasing
the usage of subtractors. With a rounding size of 0.05 and by utilizing LeNet-5
with the MNIST dataset, the proposed design can achieve 32.03% power savings
and a 24.59% reduction in area at the cost of only 0.1% in terms of accuracy
loss.
</p>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01024" title="Abstract">arXiv:2310.01024</a> [<a href="/pdf/2310.01024" title="Download PDF">pdf</a>, <a href="/format/2310.01024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Source-Channel Coding System for 6G Communication: Design,  Prototype and Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+X">Xinchao Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S+L">Sean Longyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chou%2C+H">Hong-fu Chou</a>, 
<a href="/search/cs?searchtype=author&query=Mostaani%2C+A">Arsham Mostaani</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+T+X">Thang X. Vu</a>, 
<a href="/search/cs?searchtype=author&query=Chatzinotas%2C+S">Symeon Chatzinotas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 9 figures, Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">The goal of semantic communication is to surpass optimal Shannon's criterion
regarding a notable problem for future communication which lies in the
integration of collaborative efforts between the intelligence of the
transmission source and the joint design of source coding and channel coding.
The convergence of scholarly investigation and applicable products in the field
of semantic communication is facilitated by the utilization of flexible
structural hardware design, which is constrained by the computational
capabilities of edge devices. This characteristic represents a significant
benefit of joint source-channel coding (JSCC), as it enables the generation of
source alphabets with diverse lengths and achieves a code rate of unity.
Moreover, JSCC exhibits near-capacity performance while maintaining low
complexity. Therefore, we leverage not only quasi-cyclic (QC) characteristics
to propose a QC-LDPC code-based JSCC scheme but also Unequal Error Protection
(UEP) to ensure the recovery of semantic importance. In this study, the
feasibility for using a semantic encoder/decoder that is aware of UEP can be
explored based on the existing JSCC system. This approach is aimed at
protecting the significance of semantic task-oriented information.
Additionally, the deployment of a JSCC system can be facilitated by employing
Low-Density Parity-Check (LDPC) codes on a reconfigurable device. This is
achieved by reconstructing the LDPC codes as QC-LDPC codes. The QC-LDPC layered
decoding technique, which has been specifically optimized for hardware
parallelism and tailored for channel decoding applications, can be suitably
adapted to accommodate the JSCC system. The performance of the proposed system
is evaluated by conducting BER measurements using both floating-point and 6-bit
quantization.
</p>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01028" title="Abstract">arXiv:2310.01028</a> [<a href="/pdf/2310.01028" title="Download PDF">pdf</a>, <a href="/format/2310.01028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The PoW Landscape in the Aftermath of The Merge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kiffer%2C+L">Lucianna Kiffer</a>, 
<a href="/search/cs?searchtype=author&query=Skorik%2C+S">Sophia Skorik</a>, 
<a href="/search/cs?searchtype=author&query=Vonlanthen%2C+Y">Yann Vonlanthen</a>, 
<a href="/search/cs?searchtype=author&query=Wattenhofer%2C+R">Roger Wattenhofer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">On 15th September 2022, The Merge marked the Ethereum network's transition
from computation-hardness-based consensus (proof-of-work) to a committee-based
consensus mechanism (proof-of-stake). As a result, all the specialized hardware
and GPUs that were being used by miners ceased to be profitable in the main
Ethereum network. Miners were then left with the decision of how to re-purpose
their hardware. One such choice was to try and make a profit mining another
existing PoW system. In this study, we explore this choice by analyzing the
hashrate increase in the top PoW networks following the merge. Our findings
reveal that the peak increase in hashrate to other PoW networks following The
Merge represents an adoption of at least 41% of the hashrate that was present
in Ethereum, with 12% remaining more than 5 months later. Though we measure a
drastic decrease in profitability by almost an order of magnitude, the
continued presence of miners halts claims that power consumption was instantly
addressed by Ethereum's switch to PoS.
</p>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01029" title="Abstract">arXiv:2310.01029</a> [<a href="/pdf/2310.01029" title="Download PDF">pdf</a>, <a href="/format/2310.01029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incorporating Supervised Domain Generalization into Data Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Enomoto%2C+S">Shohei Enomoto</a>, 
<a href="/search/cs?searchtype=author&query=Busto%2C+M+R">Monikka Roslianna Busto</a>, 
<a href="/search/cs?searchtype=author&query=Eda%2C+T">Takeharu Eda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">With the increasing utilization of deep learning in outdoor settings, its
robustness needs to be enhanced to preserve accuracy in the face of
distribution shifts, such as compression artifacts. Data augmentation is a
widely used technique to improve robustness, thanks to its ease of use and
numerous benefits. However, it requires more training epochs, making it
difficult to train large models with limited computational resources. To
address this problem, we treat data augmentation as supervised domain
generalization~(SDG) and benefit from the SDG method, contrastive semantic
alignment~(CSA) loss, to improve the robustness and training efficiency of data
augmentation. The proposed method only adds loss during model training and can
be used as a plug-in for existing data augmentation methods. Experiments on the
CIFAR-100 and CUB datasets show that the proposed method improves the
robustness and training efficiency of typical data augmentations.
</p>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01030" title="Abstract">arXiv:2310.01030</a> [<a href="/pdf/2310.01030" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Robust Machine Learning Approach for Path Loss Prediction in 5G  Networks with Nested Cross Validation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yaz%C4%B1c%C4%B1%2C+I">Ibrahim Yaz&#x131;c&#x131;</a>, 
<a href="/search/cs?searchtype=author&query=Gures%2C+E">Emre Gures</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Wincom'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">The design and deployment of fifth-generation (5G) wireless networks pose
significant challenges due to the increasing number of wireless devices. Path
loss has a landmark importance in network performance optimization, and
accurate prediction of the path loss, which characterizes the attenuation of
signal power during transmission, is critical for effective network planning,
coverage estimation, and optimization. In this sense, we utilize machine
learning (ML) methods, which overcome conventional path loss prediction models
drawbacks, for path loss prediction in a 5G network system to facilitate more
accurate network planning, resource optimization, and performance improvement
in wireless communication systems. To this end, we utilize a novel approach,
nested cross validation scheme, with ML to prevent overfitting, thereby getting
better generalization error and stable results for ML deployment. First, we
acquire a publicly available dataset obtained through a comprehensive
measurement campaign conducted in an urban macro-cell scenario located in
Beijing, China. The dataset includes crucial information such as longitude,
latitude, elevation, altitude, clutter height, and distance, which are utilized
as essential features to predict the path loss in the 5G network system. We
deploy Support Vector Regression (SVR), CatBoost Regression (CBR), eXtreme
Gradient Boosting Regression (XGBR), Artificial Neural Network (ANN), and
Random Forest (RF) methods to predict the path loss, and compare the prediction
results in terms of Mean Absolute Error (MAE) and Mean Square Error (MSE). As
per obtained results, XGBR outperforms the rest of the methods. It outperforms
CBR with a slight performance differences by 0.4 % and 1 % in terms of MAE and
MSE metrics, respectively. On the other hand, it outperforms the rest of the
methods with clear performance differences.
</p>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01034" title="Abstract">arXiv:2310.01034</a> [<a href="/pdf/2310.01034" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Approach for Machine Learning-based Load Balancing in High-speed  Train System using Nested Cross Validation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yazici%2C+I">Ibrahim Yazici</a>, 
<a href="/search/cs?searchtype=author&query=Gures%2C+E">Emre Gures</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Wincom'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Fifth-generation (5G) mobile communication networks have recently emerged in
various fields, including highspeed trains. However, the dense deployment of 5G
millimeter wave (mmWave) base stations (BSs) and the high speed of moving
trains lead to frequent handovers (HOs), which can adversely affect the
Quality-of-Service (QoS) of mobile users. As a result, HO optimization and
resource allocation are essential considerations for managing mobility in
high-speed train systems. In this paper, we model system performance of a
high-speed train system with a novel machine learning (ML) approach that is
nested cross validation scheme that prevents information leakage from model
evaluation into the model parameter tuning, thereby avoiding overfitting and
resulting in better generalization error. To this end, we employ ML methods for
the high-speed train system scenario. Handover Margin (HOM) and Time-to-Trigger
(TTT) values are used as features, and several KPIs are used as outputs, and
several ML methods including Gradient Boosting Regression (GBR), Adaptive
Boosting (AdaBoost), CatBoost Regression (CBR), Artificial Neural Network
(ANN), Kernel Ridge Regression (KRR), Support Vector Regression (SVR), and
k-Nearest Neighbor Regression (KNNR) are employed for the problem. Finally,
performance comparisons of the cross validation schemes with the methods are
made in terms of mean absolute error (MAE) and mean square error (MSE) metrics
are made. As per obtained results, boosting methods, ABR, CBR, GBR, with nested
cross validation scheme superiorly outperforms conventional cross validation
scheme results with the same methods. On the other hand, SVR, KNRR, KRR, ANN
with the nested scheme produce promising results for prediction of some KPIs
with respect to their conventional scheme employment.
</p>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01035" title="Abstract">arXiv:2310.01035</a> [<a href="/pdf/2310.01035" title="Download PDF">pdf</a>, <a href="/format/2310.01035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learnable Cross-modal Knowledge Distillation for Multi-modal Learning  with Missing Modality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuanhong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Congbo Ma</a>, 
<a href="/search/cs?searchtype=author&query=Avery%2C+J">Jodie Avery</a>, 
<a href="/search/cs?searchtype=author&query=Hull%2C+L">Louise Hull</a>, 
<a href="/search/cs?searchtype=author&query=Carneiro%2C+G">Gustavo Carneiro</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Medical Image Computing and Computer-Assisted Intervention 2023
  (MICCAI 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The problem of missing modalities is both critical and non-trivial to be
handled in multi-modal models. It is common for multi-modal tasks that certain
modalities contribute more compared to other modalities, and if those important
modalities are missing, the model performance drops significantly. Such fact
remains unexplored by current multi-modal approaches that recover the
representation from missing modalities by feature reconstruction or blind
feature aggregation from other modalities, instead of extracting useful
information from the best performing modalities. In this paper, we propose a
Learnable Cross-modal Knowledge Distillation (LCKD) model to adaptively
identify important modalities and distil knowledge from them to help other
modalities from the cross-modal perspective for solving the missing modality
issue. Our approach introduces a teacher election procedure to select the most
``qualified'' teachers based on their single modality performance on certain
tasks. Then, cross-modal knowledge distillation is performed between teacher
and student modalities for each task to push the model parameters to a point
that is beneficial for all tasks. Hence, even if the teacher modalities for
certain tasks are missing during testing, the available student modalities can
accomplish the task well enough based on the learned knowledge from their
automatically elected teacher modalities. Experiments on the Brain Tumour
Segmentation Dataset 2018 (BraTS2018) shows that LCKD outperforms other methods
by a considerable margin, improving the state-of-the-art performance by 3.61%
for enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour in
terms of segmentation Dice score.
</p>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01038" title="Abstract">arXiv:2310.01038</a> [<a href="/pdf/2310.01038" title="Download PDF">pdf</a>, <a href="/format/2310.01038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dataset Condensation for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiahao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wenqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengcai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qijiong Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Rui He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+K">Ke Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Training recommendation models on large datasets often requires significant
time and computational resources. Consequently, an emergent imperative has
arisen to construct informative, smaller-scale datasets for efficiently
training. Dataset compression techniques explored in other domains show
potential possibility to address this problem, via sampling a subset or
synthesizing a small dataset. However, applying existing approaches to condense
recommendation datasets is impractical due to following challenges: (i)
sampling-based methods are inadequate in addressing the long-tailed
distribution problem; (ii) synthesizing-based methods are not applicable due to
discreteness of interactions and large size of recommendation datasets; (iii)
neither of them fail to address the specific issue in recommendation of false
negative items, where items with potential user interest are incorrectly
sampled as negatives owing to insufficient exposure.
<br />To bridge this gap, we investigate dataset condensation for recommendation,
where discrete interactions are continualized with probabilistic
re-parameterization. To avoid catastrophically expensive computations, we adopt
a one-step update strategy for inner model training and introducing policy
gradient estimation for outer dataset synthesis. To mitigate amplification of
long-tailed problem, we compensate long-tailed users in the condensed dataset.
Furthermore, we propose to utilize a proxy model to identify false negative
items. Theoretical analysis regarding the convergence property is provided.
Extensive experiments on multiple datasets demonstrate the efficacy of our
method. In particular, we reduce the dataset size by 75% while approximating
over 98% of the original performance on Dianping and over 90% on other
datasets.
</p>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01039" title="Abstract">arXiv:2310.01039</a> [<a href="/pdf/2310.01039" title="Download PDF">pdf</a>, <a href="/format/2310.01039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Software Reconfiguration in Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peldszus%2C+S">Sven Peldszus</a>, 
<a href="/search/cs?searchtype=author&query=Brugali%2C+D">Davide Brugali</a>, 
<a href="/search/cs?searchtype=author&query=Str%C3%BCber%2C+D">Daniel Str&#xfc;ber</a>, 
<a href="/search/cs?searchtype=author&query=Pelliccione%2C+P">Patrizio Pelliccione</a>, 
<a href="/search/cs?searchtype=author&query=Berger%2C+T">Thorsten Berger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Since it has often been claimed by academics that reconfiguration is
essential, many approaches to reconfiguration, especially of robotic systems,
have been developed. Accordingly, the literature on robotics is rich in
techniques for reconfiguring robotic systems. However, when talking to
researchers in the domain, there seems to be no common understanding of what
exactly reconfiguration is and how it relates to other concepts such as
adaptation. Beyond this academic perspective, robotics frameworks provide
mechanisms for dynamically loading and unloading parts of robotics
applications. While we have a fuzzy picture of the state-of-the-art in robotic
reconfiguration from an academic perspective, we lack a picture of the
state-of-practice from a practitioner perspective. To fill this gap, we survey
the literature on reconfiguration in robotic systems by identifying and
analyzing 98 relevant papers, review how four major robotics frameworks support
reconfiguration, and finally investigate the realization of reconfiguration in
48 robotics applications. When comparing the state-of-the-art with the
state-of-practice, we observed a significant discrepancy between them, in
particular, the scientific community focuses on complex structural
reconfiguration, while in practice only parameter reconfiguration is widely
used. Based on our observations, we discuss possible reasons for this
discrepancy and conclude with a takeaway message for academics and
practitioners interested in robotics.
</p>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01040" title="Abstract">arXiv:2310.01040</a> [<a href="/pdf/2310.01040" title="Download PDF">pdf</a>, <a href="/format/2310.01040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised motion segmentation in one go: Smooth long-term model over  a video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meunier%2C+E">Etienne Meunier</a>, 
<a href="/search/cs?searchtype=author&query=Bouthemy%2C+P">Patrick Bouthemy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Human beings have the ability to continuously analyze a video and immediately
extract the main motion components. Motion segmentation methods often proceed
frame by frame. We want to go beyond this classical paradigm, and perform the
motion segmentation over a video sequence in one go. It will be a prominent
added value for downstream computer vision tasks, and could provide a pretext
criterion for unsupervised video representation learning. In this perspective,
we propose a novel long-term spatio-temporal model operating in a totally
unsupervised way. It takes as input the volume of consecutive optical flow (OF)
fields, and delivers a volume of segments of coherent motion over the video.
More specifically, we have designed a transformer-based network, where we
leverage a mathematically well-founded framework, the Evidence Lower Bound
(ELBO), to infer the loss function. The loss function combines a flow
reconstruction term involving spatio-temporal parametric motion models
combining, in a novel way, polynomial (quadratic) motion models for the
$(x,y)$-spatial dimensions and B-splines for the time dimension of the video
sequence, and a regularization term enforcing temporal consistency on the
masks. We report experiments on four VOS benchmarks with convincing
quantitative results. We also highlight through visual results the key
contributions on temporal consistency brought by our method.
</p>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01041" title="Abstract">arXiv:2310.01041</a> [<a href="/pdf/2310.01041" title="Download PDF">pdf</a>, <a href="/format/2310.01041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Model Decoding as Direct Metrics Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Haozhe Ji</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+P">Pei Ke</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongning Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. 28 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite the remarkable advances in language modeling, current mainstream
decoding methods still struggle to generate texts that align with human texts
across different aspects. In particular, sampling-based methods produce
less-repetitive texts which are often disjunctive in discourse, while
search-based methods maintain topic coherence at the cost of increased
repetition. Overall, these methods fall short in achieving holistic alignment
across a broad range of aspects. In this work, we frame decoding from a
language model as an optimization problem with the goal of strictly matching
the expected performance with human texts measured by multiple metrics of
desired aspects simultaneously. The resulting decoding distribution enjoys an
analytical solution that scales the input language model distribution via a
sequence-level energy function defined by these metrics. And most importantly,
we prove that this induced distribution is guaranteed to improve the perplexity
on human texts, which suggests a better approximation to the underlying
distribution of human texts. To facilitate tractable sampling from this
globally normalized distribution, we adopt the Sampling-Importance-Resampling
technique. Experiments on various domains and model scales demonstrate the
superiority of our method in metrics alignment with human texts and human
evaluation over strong baselines.
</p>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01042" title="Abstract">arXiv:2310.01042</a> [<a href="/pdf/2310.01042" title="Download PDF">pdf</a>, <a href="/ps/2310.01042" title="Download PostScript">ps</a>, <a href="/format/2310.01042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constrained Flows in Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bessy%2C+S">St&#xe9;phane Bessy</a>, 
<a href="/search/cs?searchtype=author&query=Bang-Jensen%2C+J">J&#xf8;rgen Bang-Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Picasarri-Arrieta%2C+L">Lucas Picasarri-Arrieta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">The support of a flow $x$ in a network is the subdigraph induced by the arcs
$ij$ for which $x_{ij}&gt;0$. We discuss a number of results on flows in networks
where we put certain restrictions on structure of the support of the flow. Many
of these problems are NP-hard because they generalize linkage problems for
digraphs. For example deciding whether a network ${\cal N}=(D,s,t,c)$ has a
maximum flow $x$ such that the maximum out-degree of the support $D_x$ of $x$
is at most 2 is NP-complete as it contains the 2-linkage problem as a very
special case. Another problem which is NP-complete for the same reason is that
of deciding the maximum flow we can send from $s$ to $t$ along 2 paths (called
a maximum 2-path-flow) in ${\cal N}$. Baier et al. (2005) gave a polynomial
algorithm which finds a 2-path-flow $x$ whose value is at least $\frac{2}{3}$
of the value of a optimum 2-path-flow. This is best possible unless P=NP. They
also obtained a $\frac{2}{p}$-approximation for the maximum value of a
$p$-path-flow for every $p\geq 2$. In this paper we give an algorithm which
gets within a factor $\frac{1}{H(p)}$ of the optimum solution, where $H(p)$ is
the $p$'th harmonic number ($H(p) \sim \ln(p)$). This improves the
approximation bound due to Baier et al. when $p\geq 5$. We show that in the
case where the network is acyclic, we can find a maximum $p$-path-flow in
polynomial time for every $p$. We determine the complexity of a number of
related problems concerning the structure of flows. For the special case of
acyclic digraphs, some of the results we obtain are in some sense best
possible.
</p>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01045" title="Abstract">arXiv:2310.01045</a> [<a href="/pdf/2310.01045" title="Download PDF">pdf</a>, <a href="/format/2310.01045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tool-Augmented Reward Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+Y">Yekun Chai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuohuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+H">Hao Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hua Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Reward modeling (a.k.a., preference modeling) is instrumental for aligning
large language models with human preferences, particularly within the context
of reinforcement learning from human feedback (RLHF). While conventional reward
models (RMs) have exhibited remarkable scalability, they oft struggle with
fundamental functionality such as arithmetic computation, code execution, and
factual lookup. In this paper, we propose a tool-augmented preference modeling
approach, named \name, to address these limitations by empowering RMs with
access to external environments, including calculators and search engines. This
approach not only fosters synergy between tool utilization and reward grading
but also enhances interpretive capacity and scoring reliability. Our study
delves into the integration of external tools into RMs, enabling them to
interact with diverse external sources and construct task-specific tool
engagement and reasoning traces in an autoregressive manner. We validate our
approach across a wide range of domains, incorporating seven distinct external
tools. Our experimental results demonstrate a noteworthy overall improvement of
17.7% across eight tasks in preference ranking. Furthermore, our approach
outperforms Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation. In
human evaluations, RLHF trained with Themis attains an average win rate of 32%
when compared to baselines across four distinct tasks. Additionally, we provide
a comprehensive collection of tool-related RM datasets, incorporating data from
seven distinct tool APIs, totaling 15,000 instances. We anticipate that this
publicly available dataset will facilitate and inspire further research
advancements in the field.
</p>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01055" title="Abstract">arXiv:2310.01055</a> [<a href="/pdf/2310.01055" title="Download PDF">pdf</a>, <a href="/format/2310.01055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Crop and Weed Detection with Diverse Data Ensemble Learning in  Agriculture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asad%2C+M+H">Muhammad Hamza Asad</a>, 
<a href="/search/cs?searchtype=author&query=Anwar%2C+S">Saeed Anwar</a>, 
<a href="/search/cs?searchtype=author&query=Bais%2C+A">Abdul Bais</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Modern agriculture heavily relies on Site-Specific Farm Management practices,
necessitating accurate detection, localization, and quantification of crops and
weeds in the field, which can be achieved using deep learning techniques. In
this regard, crop and weed-specific binary segmentation models have shown
promise. However, uncontrolled field conditions limit their performance from
one field to the other. To improve semantic model generalization, existing
methods augment and synthesize agricultural data to account for uncontrolled
field conditions. However, given highly varied field conditions, these methods
have limitations. To overcome the challenges of model deterioration in such
conditions, we propose utilizing data specific to other crops and weeds for our
specific target problem. To achieve this, we propose a novel ensemble
framework. Our approach involves utilizing different crop and weed models
trained on diverse datasets and employing a teacher-student configuration. By
using homogeneous stacking of base models and a trainable meta-architecture to
combine their outputs, we achieve significant improvements for Canola crops and
Kochia weeds on unseen test data, surpassing the performance of single semantic
segmentation models. We identify the UNET meta-architecture as the most
effective in this context. Finally, through ablation studies, we demonstrate
and validate the effectiveness of our proposed model. We observe that including
base models trained on other target crops and weeds can help generalize the
model to capture varied field conditions. Lastly, we propose two novel datasets
with varied conditions for comparisons.
</p>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01057" title="Abstract">arXiv:2310.01057</a> [<a href="/pdf/2310.01057" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancements in Optimization: Adaptive Differential Evolution with  Diversification Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maitra%2C+S">Sarit Maitra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This study presents a population-based evolutionary optimization algorithm
(Adaptive Differential Evolution with Diversification Strategies or ADEDS). The
algorithm was initially developed using the sinusoidal objective function and
subsequently evaluated with a wide-ranging set of 22 benchmark functions,
including Rosenbrock, Rastrigin, Ackley, and DeVilliersGlasser02, among others.
This work employs single-objective optimization in a two-dimensional space and
runs ADEDS on each of these benchmark functions with multiple iterations. The
optimization algorithms used in supply chain analytics have a direct impact on
the efficiency and cost-effectiveness of supply chain operations. The findings
reveal the effectiveness of ADEDS in finding better solutions, which implies
its importance for improving supply chain efficiency, reducing costs, and
enhancing overall performance.
</p>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01061" title="Abstract">arXiv:2310.01061</a> [<a href="/pdf/2310.01061" title="Download PDF">pdf</a>, <a href="/format/2310.01061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reasoning on Graphs: Faithful and Interpretable Large Language Model  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Linhao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan-Fang Li</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated impressive reasoning abilities
in complex tasks. However, they lack up-to-date knowledge and experience
hallucinations during reasoning, which can lead to incorrect reasoning
processes and diminish their performance and trustworthiness. Knowledge graphs
(KGs), which capture vast amounts of facts in a structured format, offer a
reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM
reasoning methods only treat KGs as factual knowledge bases and overlook the
importance of their structural information for reasoning. In this paper, we
propose a novel method called reasoning on graphs (RoG) that synergizes LLMs
with KGs to enable faithful and interpretable reasoning. Specifically, we
present a planning-retrieval-reasoning framework, where RoG first generates
relation paths grounded by KGs as faithful plans. These plans are then used to
retrieve valid reasoning paths from the KGs for LLMs to conduct faithful
reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the
reasoning ability of LLMs through training but also allows seamless integration
with any arbitrary LLMs during inference. Extensive experiments on two
benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art
performance on KG reasoning tasks and generates faithful and interpretable
reasoning results.
</p>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01064" title="Abstract">arXiv:2310.01064</a> [<a href="/pdf/2310.01064" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Sensor Terrestrial SLAM for Real-Time, Large-Scale, and  GNSS-Interrupted Forest Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khaksar%2C+W">Weria Khaksar</a>, 
<a href="/search/cs?searchtype=author&query=Astrup%2C+R">Rasmus Astrup</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Forests, as critical components of our ecosystem, demand effective monitoring
and management. However, conducting real-time forest inventory in large-scale
and GNSS-interrupted forest environments has long been a formidable challenge.
In this paper, we present a novel solution that leverages robotics and
sensor-fusion technologies to overcome these challenges and enable real-time
forest inventory with higher accuracy and efficiency. The proposed solution
consists of a new SLAM algorithm to create an accurate 3D map of large-scale
forest stands with detailed estimation about the number of trees and the
corresponding DBH, solely with the consecutive scans of a 3D lidar and an imu.
This method utilized a hierarchical unsupervised clustering algorithm to detect
the trees and measure the DBH from the lidar point cloud. The algorithm can run
simultaneously as the data is being recorded or afterwards on the recorded
dataset. Furthermore, due to the proposed fast feature extraction and transform
estimation modules, the recorded data can be fed to the SLAM with higher
frequency than common SLAM algorithms. The performance of the proposed solution
was tested through filed data collection with hand-held sensor platform as well
as a mobile forestry robot. The accuracy of the results was also compared to
the state-of-the-art SLAM solutions.
</p>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01065" title="Abstract">arXiv:2310.01065</a> [<a href="/pdf/2310.01065" title="Download PDF">pdf</a>, <a href="/format/2310.01065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KGEx: Explaining Knowledge Graph Embeddings via Subgraph Sampling and  Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baltatzis%2C+V">Vasileios Baltatzis</a>, 
<a href="/search/cs?searchtype=author&query=Costabello%2C+L">Luca Costabello</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Despite being the go-to choice for link prediction on knowledge graphs,
research on interpretability of knowledge graph embeddings (KGE) has been
relatively unexplored. We present KGEx, a novel post-hoc method that explains
individual link predictions by drawing inspiration from surrogate models
research. Given a target triple to predict, KGEx trains surrogate KGE models
that we use to identify important training triples. To gauge the impact of a
training triple, we sample random portions of the target triple neighborhood
and we train multiple surrogate KGE models on each of them. To ensure
faithfulness, each surrogate is trained by distilling knowledge from the
original KGE model. We then assess how well surrogates predict the target
triple being explained, the intuition being that those leading to faithful
predictions have been trained on impactful neighborhood samples. Under this
assumption, we then harvest triples that appear frequently across impactful
neighborhoods. We conduct extensive experiments on two publicly available
datasets, to demonstrate that KGEx is capable of providing explanations
faithful to the black-box model.
</p>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01066" title="Abstract">arXiv:2310.01066</a> [<a href="/pdf/2310.01066" title="Download PDF">pdf</a>, <a href="/ps/2310.01066" title="Download PostScript">ps</a>, <a href="/format/2310.01066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding a reconfiguration sequence between longest increasing  subsequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aoike%2C+Y">Yuuki Aoike</a>, 
<a href="/search/cs?searchtype=author&query=Kiyomi%2C+M">Masashi Kiyomi</a>, 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+Y">Yasuaki Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Otachi%2C+Y">Yota Otachi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In this note, we consider the problem of finding a step-by-step
transformation between two longest increasing subsequences in a sequence,
namely Longest Increasing Subsequence Reconfiguration. We give a
polynomial-time algorithm for deciding whether there is a reconfiguration
sequence between two longest increasing subsequences in a sequence. This
implies that Independent Set Reconfiguration and Token Sliding are
polynomial-time solvable on permutation graphs, provided that the input two
independent sets are largest among all independent sets in the input graph. We
also consider a special case, where the underlying permutation graph of an
input sequence is bipartite. In this case, we give a polynomial-time algorithm
for finding a shortest reconfiguration sequence (if it exists).
</p>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01067" title="Abstract">arXiv:2310.01067</a> [<a href="/pdf/2310.01067" title="Download PDF">pdf</a>, <a href="/format/2310.01067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Roofline Extraction from True Orthophotos for LoD2 Building  Model Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Weixiao Gao</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+R">Ravi Peters</a>, 
<a href="/search/cs?searchtype=author&query=Stoter%2C+J">Jantien Stoter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper discusses the reconstruction of LoD2 building models from 2D and
3D data for large-scale urban environments. Traditional methods involve the use
of LiDAR point clouds, but due to high costs and long intervals associated with
acquiring such data for rapidly developing areas, researchers have started
exploring the use of point clouds generated from (oblique) aerial images.
However, using such point clouds for traditional plane detection-based methods
can result in significant errors and introduce noise into the reconstructed
building models. To address this, this paper presents a method for extracting
rooflines from true orthophotos using line detection for the reconstruction of
building models at the LoD2 level. The approach is able to extract relatively
complete rooflines without the need for pre-labeled training data or
pre-trained models. These lines can directly be used in the LoD2 building model
reconstruction process. The method is superior to existing plane
detection-based methods and state-of-the-art deep learning methods in terms of
the accuracy and completeness of the reconstructed building. Our source code is
available at https://github.com/tudelft3d/Roofline-extraction-from-orthophotos.
</p>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01069" title="Abstract">arXiv:2310.01069</a> [<a href="/pdf/2310.01069" title="Download PDF">pdf</a>, <a href="/ps/2310.01069" title="Download PostScript">ps</a>, <a href="/format/2310.01069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Abstract Normal Form Bisimulation for Call-by-Value PCF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koutavas%2C+V">Vasileios Koutavas</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yu-Yang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Tzevelekos%2C+N">Nikos Tzevelekos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We present the first fully abstract normal form bisimulation for
call-by-value PCF (PCF$_{\textsf{v}}$). Our model is based on a labelled
transition system (LTS) that combines elements from applicative bisimulation,
environmental bisimulation and game semantics. In order to obtain completeness
while avoiding the use of semantic quotiening, the LTS constructs traces
corresponding to interactions with possible functional contexts. The model
gives rise to a sound and complete technique for checking of PCF$_{\textsf{v}}$
program equivalence, which we implement in a bounded bisimulation checking
tool. We test our tool on known equivalences from the literature and new
examples.
</p>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01074" title="Abstract">arXiv:2310.01074</a> [<a href="/pdf/2310.01074" title="Download PDF">pdf</a>, <a href="/format/2310.01074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Back to the Future: Towards Explainable Temporal Reasoning with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chenhan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jimin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ananiadou%2C+S">Sophia Ananiadou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Temporal reasoning is a crucial NLP task, providing a nuanced understanding
of time-sensitive contexts within textual data. Although recent advancements in
LLMs have demonstrated their potential in temporal reasoning, the predominant
focus has been on tasks such as temporal expression and temporal relation
extraction. These tasks are primarily designed for the extraction of direct and
past temporal cues and to engage in simple reasoning processes. A significant
gap remains when considering complex reasoning tasks such as event forecasting,
which requires multi-step temporal reasoning on events and prediction on the
future timestamp. Another notable limitation of existing methods is their
incapability to provide an illustration of their reasoning process, hindering
explainability. In this paper, we introduce the first task of explainable
temporal reasoning, to predict an event's occurrence at a future timestamp
based on context which requires multiple reasoning over multiple events, and
subsequently provide a clear explanation for their prediction. Our task offers
a comprehensive evaluation of both the LLMs' complex temporal reasoning
ability, the future event prediction ability, and explainability-a critical
attribute for AI applications. To support this task, we present the first
multi-source instruction-tuning dataset of explainable temporal reasoning
(ExpTime) with 26k derived from the temporal knowledge graph datasets and their
temporal reasoning paths, using a novel knowledge-graph-instructed-generation
strategy. Based on the dataset, we propose the first open-source LLM series
TimeLlaMA based on the foundation LlaMA2, with the ability of instruction
following for explainable temporal reasoning. We compare the performance of our
method and a variety of LLMs, where our method achieves the state-of-the-art
performance of temporal prediction and explanation.
</p>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01077" title="Abstract">arXiv:2310.01077</a> [<a href="/pdf/2310.01077" title="Download PDF">pdf</a>, <a href="/format/2310.01077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Fulfilling the Exigent Need for Automating and Modernizing Logistics  Infrastructure in India: Enabling AI-based Integration, Digitalization, and  Smart Automation of Industrial Parks and Robotic Warehouses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shriyam%2C+S">Shaurya Shriyam</a>, 
<a href="/search/cs?searchtype=author&query=Palkar%2C+P">Prashant Palkar</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+A">Amber Srivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">To stay competitive, the Low- or Middle-Income Countries (LMICs) need to
embrace Industry 4.0 and Logistics 4.0. This requires government-level
interventions and policy-making to incentivize quality product solutions and
drive innovation in traditionally resistant economic sectors. In this position
paper, we support the establishment of Smart Industrial Parks (SIPs) with a
focus on enhancing operational efficiencies and bringing together MSMEs and
startups targeting niche clientele with innovative Industry 4.0 solutions. SIPs
along with the phased deployment of well-planned robotic automation
technologies shall enable bringing down India's untenable logistics costs.
Toward the successful execution of SIPs, we are required to implement the
efficient allocation of manufacturing resources and capabilities within SIPs.
Thus, we emphasize the importance of efficient resource utilization,
collaboration, and technology adoption in industrial parks to promote
industrial development and economic growth. We advocate the use of a
cloud-based cyber-physical system for real-time data access and analysis in
SIPs. Such centralized cloud-based monitoring of factory floors, warehouses,
and industrial units using IoT infrastructure shall improve decision-making,
efficiency, and safety. Digital Twins (DTs), which are cyber-replicas of
physical systems, could play a significant role in enabling simulation,
optimization, and real-time monitoring of smart manufacturing and distributed
manufacturing systems. However, there are several challenges involved in
implementing DTs in distributed manufacturing systems, such as defining data
schemas and collaboration protocols, ensuring interoperability, the need for
effective authentication technology, distributed machine learning models, and
scalability to manage multiple DTs.
</p>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01079" title="Abstract">arXiv:2310.01079</a> [<a href="/pdf/2310.01079" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Approach with Monte-Carlo Simulation and Hybrid Optimization  Approach for Inventory Management with Stochastic Demand
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maitra%2C+S">Sarit Maitra</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+V">Vivek Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Sukanya Kundu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Applications (stat.AP)

</div>
<p class="mathjax">This study addresses the difficulties associated with inventory management of
products with stochastic demand. The objective is to find the optimal
combination of order quantity and reorder point that maximizes profit while
considering ethical considerations in inventory management. The ethical
considerations are risk assessment, social responsibility, environmental
sustainability, and customer satisfaction. Monte Carlo simulation (MCS) is used
in this study to generate a distribution of demand and lead times for the
inventory items, which is then used to estimate the potential profit and risk
associated with different inventory policies. This work proposes a hybrid
optimization approach combining Gaussian process regression and conditioning
function to efficiently search the high-dimensional space of potential
continuous review (r, Q) and periodic review (p, Q) values to find the optimal
combination that maximizes profit while considering ethical considerations. The
findings show that both the (r, Q) and (p, Q) approaches can effectively manage
inventory with stochastic demand, but the (r, Q) approach performs better
(profits up by 12.73%) when demand is more volatile. The study adds
quantifiable risk assessment and sensitivity analysis to these considerations,
considering the variation in demand and expected output in profit percentage.
The results provide useful information for making ethical and responsible
choices in supply chain analytics, boosting efficiency and profits.
</p>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01080" title="Abstract">arXiv:2310.01080</a> [<a href="/pdf/2310.01080" title="Download PDF">pdf</a>, <a href="/format/2310.01080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rel2Graph: Automated Mapping From Relational Databases to a Unified  Property Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Ziyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=French%2C+T">Tim French</a>, 
<a href="/search/cs?searchtype=author&query=Stewart%2C+M">Michael Stewart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Although a few approaches are proposed to convert relational databases to
graphs, there is a genuine lack of systematic evaluation across a wider
spectrum of databases. Recognising the important issue of query mapping, this
paper proposes an approach Rel2Graph, an automatic knowledge graph construction
(KGC) approach from an arbitrary number of relational databases. Our approach
also supports the mapping of conjunctive SQL queries into pattern-based NoSQL
queries. We evaluate our proposed approach on two widely used relational
database-oriented datasets: Spider and KaggleDBQA benchmarks for semantic
parsing. We employ the execution accuracy (EA) metric to quantify the
proportion of results by executing the NoSQL queries on the property knowledge
graph we construct that aligns with the results of SQL queries performed on
relational databases. Consequently, the counterpart property knowledge graph of
benchmarks with high accuracy and integrity can be ensured. The code and data
will be publicly available. The code and data are available at
github\footnote{https://github.com/nlp-tlp/Rel2Graph}.
</p>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01081" title="Abstract">arXiv:2310.01081</a> [<a href="/pdf/2310.01081" title="Download PDF">pdf</a>, <a href="/format/2310.01081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unmasking Role-Play Attack Strategies in Exploiting Decentralized  Finance (DeFi) Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weilin Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Heying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+T">Taiyu Wong</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+P">Pengyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yufei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The rapid growth and adoption of decentralized finance (DeFi) systems have
been accompanied by various threats, notably those emerging from
vulnerabilities in their intricate design. In our work, we introduce and define
an attack strategy termed as Role-Play Attack, in which the attacker acts as
multiple roles concurrently to exploit the DeFi system and cause substantial
financial losses. We provide a formal definition of this strategy and
demonstrate its potential impacts by revealing the total loss of \$435.1M
caused by 14 historical attacks with applying this pattern. Besides, we
mathematically analyzed the attacks with top 2 losses and retrofitted the
corresponding attack pattern by concrete execution, indicating that this
strategy could increase the potential profit for original attacks by \$3.34M
(51.4%) and \$3.76M (12.0%), respectively.
</p>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01082" title="Abstract">arXiv:2310.01082</a> [<a href="/pdf/2310.01082" title="Download PDF">pdf</a>, <a href="/format/2310.01082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear attention is (maybe) all you need (to understand transformer  optimization)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahn%2C+K">Kwangjun Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xiang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Minhak Song</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+C">Chulhee Yun</a>, 
<a href="/search/cs?searchtype=author&query=Jadbabaie%2C+A">Ali Jadbabaie</a>, 
<a href="/search/cs?searchtype=author&query=Sra%2C+S">Suvrit Sra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Comments would be appreciated!!
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
<p class="mathjax">Transformer training is notoriously difficult, requiring a careful design of
optimizers and use of various heuristics. We make progress towards
understanding the subtleties of training transformers by carefully studying a
simple yet canonical linearized shallow transformer model. Specifically, we
train linear transformers to solve regression tasks, inspired by J. von Oswald
et al. (ICML 2023), and K. Ahn et al. (NeurIPS 2023). Most importantly, we
observe that our proposed linearized models can reproduce several prominent
aspects of transformer training dynamics. Consequently, the results obtained in
this paper suggest that a simple linearized transformer model could actually be
a valuable, realistic abstraction for understanding transformer optimization.
</p>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01084" title="Abstract">arXiv:2310.01084</a> [<a href="/pdf/2310.01084" title="Download PDF">pdf</a>, <a href="/format/2310.01084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-negative isomorphic neural networks for photonic neuromorphic  accelerators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kirtas%2C+M">Manos Kirtas</a>, 
<a href="/search/cs?searchtype=author&query=Passalis%2C+N">Nikolaos Passalis</a>, 
<a href="/search/cs?searchtype=author&query=Pleros%2C+N">Nikolaos Pleros</a>, 
<a href="/search/cs?searchtype=author&query=Tefas%2C+A">Anastasios Tefas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Neuromorphic photonic accelerators are becoming increasingly popular, since
they can significantly improve computation speed and energy efficiency, leading
to femtojoule per MAC efficiency. However, deploying existing DL models on such
platforms is not trivial, since a great range of photonic neural network
architectures relies on incoherent setups and power addition operational
schemes that cannot natively represent negative quantities. This results in
additional hardware complexity that increases cost and reduces energy
efficiency. To overcome this, we can train non-negative neural networks and
potentially exploit the full range of incoherent neuromorphic photonic
capabilities. However, existing approaches cannot achieve the same level of
accuracy as their regular counterparts, due to training difficulties, as also
recent evidence suggests. To this end, we introduce a methodology to obtain the
non-negative isomorphic equivalents of regular neural networks that meet
requirements of neuromorphic hardware, overcoming the aforementioned
limitations. Furthermore, we also introduce a sign-preserving optimization
approach that enables training of such isomorphic networks in a non-negative
manner.
</p>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01087" title="Abstract">arXiv:2310.01087</a> [<a href="/pdf/2310.01087" title="Download PDF">pdf</a>, <a href="/format/2310.01087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel DID Method Leveraging the IOTA Tangle and its Integration into  OpenSSL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Claudio%2C+A">Alessio Claudio</a>, 
<a href="/search/cs?searchtype=author&query=Vesco%2C+A">Andrea Vesco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">This paper presents, for the first time, a novel Decentralized IDentifier
(DID) Method called Over-The-Tangle and discusses its design and working
principles that leverage the IOTA Tangle as the Root-of-Trust for identity
data. The results of a long lasting experimental test campaign in real-world
settings suggests the adoption of a private gateway node synchronised with the
IOTA Tangle on the mainnet for efficient DID control. Moreover, the paper
promotes the integration of the DID technology into OpenSSL through the use of
Providers. A novel DID Operation and Provider is presented as a solution for
building DID Method agility in OpenSSL.
</p>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01088" title="Abstract">arXiv:2310.01088</a> [<a href="/pdf/2310.01088" title="Download PDF">pdf</a>, <a href="/format/2310.01088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards human-like spoken dialogue generation between AI agents from  written dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitsui%2C+K">Kentaro Mitsui</a>, 
<a href="/search/cs?searchtype=author&query=Hono%2C+Y">Yukiya Hono</a>, 
<a href="/search/cs?searchtype=author&query=Sawada%2C+K">Kei Sawada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 8 figures, 9 tables, audio samples: <a href="https://rinnakk.github.io/research/publications/CHATS/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The advent of large language models (LLMs) has made it possible to generate
natural written dialogues between two agents. However, generating human-like
spoken dialogues from these written dialogues remains challenging. Spoken
dialogues have several unique characteristics: they frequently include
backchannels and laughter, and the smoothness of turn-taking significantly
influences the fluidity of conversation. This study proposes CHATS - CHatty
Agents Text-to-Speech - a discrete token-based system designed to generate
spoken dialogues based on written dialogues. Our system can generate speech for
both the speaker side and the listener side simultaneously, using only the
transcription from the speaker side, which eliminates the need for
transcriptions of backchannels or laughter. Moreover, CHATS facilitates natural
turn-taking; it determines the appropriate duration of silence after each
utterance in the absence of overlap, and it initiates the generation of
overlapping speech based on the phoneme sequence of the next utterance in case
of overlap. Experimental evaluations indicate that CHATS outperforms the
text-to-speech baseline, producing spoken dialogues that are more interactive
and fluid while retaining clarity and intelligibility.
</p>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01089" title="Abstract">arXiv:2310.01089</a> [<a href="/pdf/2310.01089" title="Download PDF">pdf</a>, <a href="/format/2310.01089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphText: Graph Reasoning in Text Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jianan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+L">Le Zhuo</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yikang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+M">Meng Qu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bronstein%2C+M">Michael Bronstein</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhaocheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jian Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) have gained the ability to assimilate human
knowledge and facilitate natural language interactions with both humans and
other LLMs. However, despite their impressive achievements, LLMs have not made
significant advancements in the realm of graph machine learning. This
limitation arises because graphs encapsulate distinct relational data, making
it challenging to transform them into natural language that LLMs understand. In
this paper, we bridge this gap with a novel framework, GraphText, that
translates graphs into natural language. GraphText derives a graph-syntax tree
for each graph that encapsulates both the node attributes and inter-node
relationships. Traversal of the tree yields a graph text sequence, which is
then processed by an LLM to treat graph tasks as text generation tasks.
Notably, GraphText offers multiple advantages. It introduces training-free
graph reasoning: even without training on graph data, GraphText with ChatGPT
can achieve on par with, or even surpassing, the performance of
supervised-trained graph neural networks through in-context learning (ICL).
Furthermore, GraphText paves the way for interactive graph reasoning, allowing
both humans and LLMs to communicate with the model seamlessly using natural
language. These capabilities underscore the vast, yet-to-be-explored potential
of LLMs in the domain of graph machine learning.
</p>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01092" title="Abstract">arXiv:2310.01092</a> [<a href="/pdf/2310.01092" title="Download PDF">pdf</a>, <a href="/format/2310.01092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Cutting Edge Deep Learning Based Image Matching for  Reconstructing a Large Scene from Sparse Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=B%C3%B6kman%2C+G">Georg B&#xf6;kman</a>, 
<a href="/search/cs?searchtype=author&query=Edstedt%2C+J">Johan Edstedt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report for the top ranked solution to the AISG-SLA visual localization challenge at IJCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present the top ranked solution for the AISG-SLA Visual Localisation
Challenge benchmark (IJCAI 2023), where the task is to estimate relative motion
between images taken in sequence by a camera mounted on a car driving through
an urban scene.
<br />For matching images we use our recent deep learning based matcher RoMa.
Matching image pairs sequentially and estimating relative motion from point
correspondences sampled by RoMa already gives very competitive results -- third
rank on the challenge benchmark.
<br />To improve the estimations we extract keypoints in the images, match them
using RoMa, and perform structure from motion reconstruction using COLMAP. We
choose our recent DeDoDe keypoints for their high repeatability. Further, we
address time jumps in the image sequence by matching specific non-consecutive
image pairs based on image retrieval with DINOv2. These improvements yield a
solution beating all competitors.
<br />We further present a loose upper bound on the accuracy obtainable by the
image retrieval approach by also matching hand-picked non-consecutive pairs.
</p>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01095" title="Abstract">arXiv:2310.01095</a> [<a href="/pdf/2310.01095" title="Download PDF">pdf</a>, <a href="/format/2310.01095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoCUS: Learning Multiscale 3D-consistent Features from Posed Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kloepfer%2C+D+A">Dominik A. Kloepfer</a>, 
<a href="/search/cs?searchtype=author&query=Campbell%2C+D">Dylan Campbell</a>, 
<a href="/search/cs?searchtype=author&query=Henriques%2C+J+F">Jo&#xe3;o F. Henriques</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV) 2023, pages 16634-16644
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">An important challenge for autonomous agents such as robots is to maintain a
spatially and temporally consistent model of the world. It must be maintained
through occlusions, previously-unseen views, and long time horizons (e.g., loop
closure and re-identification). It is still an open question how to train such
a versatile neural representation without supervision. We start from the idea
that the training objective can be framed as a patch retrieval problem: given
an image patch in one view of a scene, we would like to retrieve (with high
precision and recall) all patches in other views that map to the same
real-world location. One drawback is that this objective does not promote
reusability of features: by being unique to a scene (achieving perfect
precision/recall), a representation will not be useful in the context of other
scenes. We find that it is possible to balance retrieval and reusability by
constructing the retrieval set carefully, leaving out patches that map to
far-away locations. Similarly, we can easily regulate the scale of the learned
features (e.g., points, objects, or rooms) by adjusting the spatial tolerance
for considering a retrieval to be positive. We optimize for (smooth) Average
Precision (AP), in a single unified ranking-based objective. This objective
also doubles as a criterion for choosing landmarks or keypoints, as patches
with high AP. We show results creating sparse, multi-scale, semantic spatial
maps composed of highly identifiable landmarks, with applications in landmark
retrieval, localization, semantic segmentation and instance segmentation.
</p>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01097" title="Abstract">arXiv:2310.01097</a> [<a href="/pdf/2310.01097" title="Download PDF">pdf</a>, <a href="/ps/2310.01097" title="Download PostScript">ps</a>, <a href="/format/2310.01097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Programming the Minimal Model Program: a proposal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lazi%C4%87%2C+V">Vladimir Lazi&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Algebraic Geometry (math.AG)

</div>
<p class="mathjax">The aim of this paper is to propose a strategy to implement the Minimal Model
Program in modern computer algebra systems.
</p>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01098" title="Abstract">arXiv:2310.01098</a> [<a href="/pdf/2310.01098" title="Download PDF">pdf</a>, <a href="/format/2310.01098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NP$^2$L: Negative Pseudo Partial Labels Extraction for Graph Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xinjie Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Danyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jitao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Junjie Liang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+F">Feiping Nie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">How to utilize the pseudo labels has always been a research hotspot in
machine learning. However, most methods use pseudo labels as supervised
training, and lack of valid assessing for their accuracy. Moreover,
applications of pseudo labels in graph neural networks (GNNs) oversee the
difference between graph learning and other machine learning tasks such as
message passing mechanism. Aiming to address the first issue, we found through
a large number of experiments that the pseudo labels are more accurate if they
are selected by not overlapping partial labels and defined as negative node
pairs relations. Therefore, considering the extraction based on pseudo and
partial labels, negative edges are constructed between two nodes by the
negative pseudo partial labels extraction (NP$^2$E) module. With that, a signed
graph are built containing highly accurate pseudo labels information from the
original graph, which effectively assists GNN in learning at the
message-passing level, provide one solution to the second issue. Empirical
results about link prediction and node classification tasks on several
benchmark datasets demonstrate the effectiveness of our method.
State-of-the-art performance is achieved on the both tasks.
</p>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01101" title="Abstract">arXiv:2310.01101</a> [<a href="/pdf/2310.01101" title="Download PDF">pdf</a>, <a href="/ps/2310.01101" title="Download PostScript">ps</a>, <a href="/format/2310.01101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed end-effector formation control for mixed fully- and  under-actuated manipulators with flexible joints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Peng%2C+Z">Zhiyu Peng</a>, 
<a href="/search/eess?searchtype=author&query=Jayawardhana%2C+B">Bayu Jayawardhana</a>, 
<a href="/search/eess?searchtype=author&query=Xin%2C+X">Xin Xin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO); Optimization and Control (math.OC)

</div>
<p class="mathjax">The presence of faulty or underactuated manipulators can disrupt the
end-effector formation keeping of a team of manipulators. Based on two-link
planar manipulators, we investigate this end-effector formation keeping problem
for mixed fully- and under-actuated manipulators with flexible joints. In this
case, the underactuated manipulators can comprise of active-passive (AP)
manipulators, passive-active (PA) manipulators, or a combination thereof. We
propose distributed control laws for the different types of manipulators to
achieve and maintain the desired formation shape of the end-effectors. It is
achieved by assigning virtual springs to the end-effectors for the
fully-actuated ones and to the virtual end-effectors for the under-actuated
ones. We study further the set of all desired and reachable shapes for the
networked manipulators' end-effectors. Finally, we validate our analysis via
numerical simulations.
</p>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01105" title="Abstract">arXiv:2310.01105</a> [<a href="/pdf/2310.01105" title="Download PDF">pdf</a>, <a href="/format/2310.01105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Guided Continuous Entropic Barycenter Estimation for General  Costs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kolesov%2C+A">Alexander Kolesov</a>, 
<a href="/search/cs?searchtype=author&query=Mokrov%2C+P">Petr Mokrov</a>, 
<a href="/search/cs?searchtype=author&query=Udovichenko%2C+I">Igor Udovichenko</a>, 
<a href="/search/cs?searchtype=author&query=Gazdieva%2C+M">Milena Gazdieva</a>, 
<a href="/search/cs?searchtype=author&query=Pammer%2C+G">Gudmund Pammer</a>, 
<a href="/search/cs?searchtype=author&query=Burnaev%2C+E">Evgeny Burnaev</a>, 
<a href="/search/cs?searchtype=author&query=Korotin%2C+A">Alexander Korotin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Optimal transport (OT) barycenters are a mathematically grounded way of
averaging probability distributions while capturing their geometric properties.
In short, the barycenter task is to take the average of a collection of
probability distributions w.r.t. given OT discrepancies. We propose a novel
algorithm for approximating the continuous Entropic OT (EOT) barycenter for
arbitrary OT cost functions. Our approach is built upon the dual reformulation
of the EOT problem based on weak OT, which has recently gained the attention of
the ML community. Beyond its novelty, our method enjoys several advantageous
properties: (i) we establish quality bounds for the recovered solution; (ii)
this approach seemlessly interconnects with the Energy-Based Models (EBMs)
learning procedure enabling the use of well-tuned algorithms for the problem of
interest; (iii) it provides an intuitive optimization scheme avoiding min-max,
reinforce and other intricate technical tricks. For validation, we consider
several low-dimensional scenarios and image-space setups, including
non-Euclidean cost functions. Furthermore, we investigate the practical task of
learning the barycenter on an image manifold generated by a pretrained
generative model, opening up new directions for real-world applications.
</p>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01107" title="Abstract">arXiv:2310.01107</a> [<a href="/pdf/2310.01107" title="Download PDF">pdf</a>, <a href="/format/2310.01107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+H">Hyeonho Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J+C">Jong Chul Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="http://ground-a-video.github.io">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Recent endeavors in video editing have showcased promising results in
single-attribute editing or style transfer tasks, either by training
text-to-video (T2V) models on text-video data or adopting training-free
methods. However, when confronted with the complexities of multi-attribute
editing scenarios, they exhibit shortcomings such as omitting or overlooking
intended attribute changes, modifying the wrong elements of the input video,
and failing to preserve regions of the input video that should remain intact.
To address this, here we present a novel grounding-guided video-to-video
translation framework called Ground-A-Video for multi-attribute video editing.
Ground-A-Video attains temporally consistent multi-attribute editing of input
videos in a training-free manner without aforementioned shortcomings. Central
to our method is the introduction of Cross-Frame Gated Attention which
incorporates groundings information into the latent representations in a
temporally consistent fashion, along with Modulated Cross-Attention and optical
flow guided inverted latents smoothing. Extensive experiments and applications
demonstrate that Ground-A-Video's zero-shot capacity outperforms other baseline
methods in terms of edit-accuracy and frame consistency. Further results and
codes are provided at our project page (<a href="http://ground-a-video.github.io">this http URL</a>).
</p>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01108" title="Abstract">arXiv:2310.01108</a> [<a href="/pdf/2310.01108" title="Download PDF">pdf</a>, <a href="/format/2310.01108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thou Shalt Not Reject: Analyzing Accept-Or-Pay Cookie Banners on the Web
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rasaii%2C+A">Ali Rasaii</a>, 
<a href="/search/cs?searchtype=author&query=Gosain%2C+D">Devashish Gosain</a>, 
<a href="/search/cs?searchtype=author&query=Gasser%2C+O">Oliver Gasser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and data available at <a href="https://bannerclick.github.io/">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 23rd ACM Internet Measurement Conference (IMC
  2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Privacy regulations have led to many websites showing cookie banners to their
users. Usually, cookie banners present the user with the option to "accept" or
"reject" cookies. Recently, a new form of paywall-like cookie banner has taken
hold on the Web, giving users the option to either accept cookies (and
consequently user tracking) or buy a paid subscription for a tracking-free
website experience. In this paper, we perform the first completely automated
analysis of cookiewalls, i.e., cookie banners acting as a paywall. We find
cookiewalls on 0.6% of all queried 45k websites. Moreover, cookiewalls are
deployed to a large degree on European websites, e.g., for Germany we see
cookiewalls on 8.5% of top 1k websites. Additionally, websites using
cookiewalls send 6.4 times more third-party cookies and 42 times more tracking
cookies to visitors, compared to regular cookie banner websites. We also
uncover two large Subscription Management Platforms used on hundreds of
websites, which provide website operators with easy-to-setup cookiewall
solutions. Finally, we publish tools, data, and code to foster reproducibility
and further studies.
</p>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01109" title="Abstract">arXiv:2310.01109</a> [<a href="/pdf/2310.01109" title="Download PDF">pdf</a>, <a href="/format/2310.01109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> R-divergence for Estimating Model-oriented Distribution Discrepancy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhilin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Longbing Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Real-life data are often non-IID due to complex distributions and
interactions, and the sensitivity to the distribution of samples can differ
among learning models. Accordingly, a key question for any supervised or
unsupervised model is whether the probability distributions of two given
datasets can be considered identical. To address this question, we introduce
R-divergence, designed to assess model-oriented distribution discrepancies. The
core insight is that two distributions are likely identical if their optimal
hypothesis yields the same expected risk for each distribution. To estimate the
distribution discrepancy between two datasets, R-divergence learns a minimum
hypothesis on the mixed data and then gauges the empirical risk difference
between them. We evaluate the test power across various unsupervised and
supervised tasks and find that R-divergence achieves state-of-the-art
performance. To demonstrate the practicality of R-divergence, we employ
R-divergence to train robust neural networks on samples with noisy labels.
</p>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01110" title="Abstract">arXiv:2310.01110</a> [<a href="/pdf/2310.01110" title="Download PDF">pdf</a>, <a href="/format/2310.01110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt-tuning latent diffusion models for inverse problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+H">Hyungjin Chung</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J+C">Jong Chul Ye</a>, 
<a href="/search/cs?searchtype=author&query=Milanfar%2C+P">Peyman Milanfar</a>, 
<a href="/search/cs?searchtype=author&query=Delbracio%2C+M">Mauricio Delbracio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">We propose a new method for solving imaging inverse problems using
text-to-image latent diffusion models as general priors. Existing methods using
latent diffusion models for inverse problems typically rely on simple null text
prompts, which can lead to suboptimal performance. To address this limitation,
we introduce a method for prompt tuning, which jointly optimizes the text
embedding on-the-fly while running the reverse diffusion process. This allows
us to generate images that are more faithful to the diffusion prior. In
addition, we propose a method to keep the evolution of latent variables within
the range space of the encoder, by projection. This helps to reduce image
artifacts, a major problem when using latent diffusion models instead of
pixel-based diffusion models. Our combined method, called P2L, outperforms both
image- and latent-diffusion model-based inverse problem solvers on a variety of
tasks, such as super-resolution, deblurring, and inpainting.
</p>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01113" title="Abstract">arXiv:2310.01113</a> [<a href="/pdf/2310.01113" title="Download PDF">pdf</a>, <a href="/format/2310.01113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperGraphDis: Leveraging Hypergraphs for Contextual and Social-Based  Disinformation Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salamanos%2C+N">Nikos Salamanos</a>, 
<a href="/search/cs?searchtype=author&query=Leonidou%2C+P">Pantelitsa Leonidou</a>, 
<a href="/search/cs?searchtype=author&query=Laoutaris%2C+N">Nikolaos Laoutaris</a>, 
<a href="/search/cs?searchtype=author&query=Sirivianos%2C+M">Michael Sirivianos</a>, 
<a href="/search/cs?searchtype=author&query=Aspri%2C+M">Maria Aspri</a>, 
<a href="/search/cs?searchtype=author&query=Paraschiv%2C+M">Marius Paraschiv</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">In light of the growing impact of disinformation on social, economic, and
political landscapes, accurate and efficient identification methods are
increasingly critical. This paper introduces HyperGraphDis, a novel approach
for detecting disinformation on Twitter that employs a hypergraph-based
representation to capture (i) the intricate social structure arising from
retweet cascades, (ii) relational features among users, and (iii) semantic and
topical nuances. Evaluated on four Twitter datasets -- focusing on the 2016
U.S. Presidential election and the COVID-19 pandemic -- HyperGraphDis
outperforms existing methods in both accuracy and computational efficiency,
underscoring its effectiveness and scalability for tackling the challenges
posed by disinformation dissemination. The HyperGraphDis displayed exceptional
performance in an evaluation using a COVID-19-related dataset, achieving an
impressive F1 score of approximately 92.5%. This result represents a notable
improvement of around 7% compared to other existing methods. Additionally,
significant enhancements in computation time were observed for both model
training and inference processes. In terms of model training, completion times
were noticeably accelerated, ranging from 1.6 to 16.5 times faster than
previous benchmarks. Similarly, during inference, computational times
demonstrated increased efficiency, ranging from 1.3 to 17.7 times faster than
alternative methods.
</p>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01119" title="Abstract">arXiv:2310.01119</a> [<a href="/pdf/2310.01119" title="Download PDF">pdf</a>, <a href="/format/2310.01119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Data Augmentation in Low-Resource Settings via Fine-Tuning of Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaddour%2C+J">Jean Kaddour</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The in-context learning ability of large language models (LLMs) enables them
to generalize to novel downstream tasks with relatively few labeled examples.
However, they require enormous computational resources to be deployed.
Alternatively, smaller models can solve specific tasks if fine-tuned with
enough labeled examples. These examples, however, are expensive to obtain. In
pursuit of the best of both worlds, we study the annotation and generation of
fine-tuning training data via fine-tuned teacher LLMs to improve the downstream
performance of much smaller models. In four text classification and two text
generation tasks, we find that both data generation and annotation dramatically
improve the respective downstream model's performance, occasionally
necessitating only a minor fraction of the original training dataset.
</p>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01129" title="Abstract">arXiv:2310.01129</a> [<a href="/pdf/2310.01129" title="Download PDF">pdf</a>, <a href="/format/2310.01129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strength in Diversity: Multi-Branch Representation Learning for Vehicle  Re-Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almeida%2C+E">Eurico Almeida</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+B">Bruno Silva</a>, 
<a href="/search/cs?searchtype=author&query=Batista%2C+J">Jorge Batista</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted in ITSC2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents an efficient and lightweight multi-branch deep
architecture to improve vehicle re-identification (V-ReID). While most V-ReID
work uses a combination of complex multi-branch architectures to extract robust
and diversified embeddings towards re-identification, we advocate that simple
and lightweight architectures can be designed to fulfill the Re-ID task without
compromising performance.
<br />We propose a combination of Grouped-convolution and Loss-Branch-Split
strategies to design a multi-branch architecture that improve feature diversity
and feature discriminability. We combine a ResNet50 global branch architecture
with a BotNet self-attention branch architecture, both designed within a
Loss-Branch-Split (LBS) strategy. We argue that specialized
loss-branch-splitting helps to improve re-identification tasks by generating
specialized re-identification features. A lightweight solution using grouped
convolution is also proposed to mimic the learning of loss-splitting into
multiple embeddings while significantly reducing the model size. In addition,
we designed an improved solution to leverage additional metadata, such as
camera ID and pose information, that uses 97% less parameters, further
improving re-identification performance.
<br />In comparison to state-of-the-art (SoTA) methods, our approach outperforms
competing solutions in Veri-776 by achieving 85.6% mAP and 97.7% CMC1 and
obtains competitive results in Veri-Wild with 88.1% mAP and 96.3% CMC1.
Overall, our work provides important insights into improving vehicle
re-identification and presents a strong basis for other retrieval tasks. Our
code is available at the
https://github.com/videturfortuna/vehicle_reid_itsc2023.
</p>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01132" title="Abstract">arXiv:2310.01132</a> [<a href="/pdf/2310.01132" title="Download PDF">pdf</a>, <a href="/format/2310.01132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Evaluation of Classroom Instructional Support with LLMs and  BoWs: Connecting Global Predictions to Specific Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Whitehill%2C+J">Jacob Whitehill</a>, 
<a href="/search/cs?searchtype=author&query=LoCasale-Crouch%2C+J">Jennifer LoCasale-Crouch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the aim to provide teachers with more specific, frequent, and actionable
feedback about their teaching, we explore how Large Language Models (LLMs) can
be used to estimate ``Instructional Support'' domain scores of the CLassroom
Assessment Scoring System (CLASS), a widely used observation protocol. We
design a machine learning architecture that uses either zero-shot prompting of
Meta's Llama2, and/or a classic Bag of Words (BoW) model, to classify
individual utterances of teachers' speech (transcribed automatically using
OpenAI's Whisper) for the presence of 11 behavioral indicators of Instructional
Support. Then, these utterance-level judgments are aggregated over an entire
15-min observation session to estimate a global CLASS score. Experiments on two
CLASS-coded datasets of toddler and pre-kindergarten classrooms indicate that
(1) automatic CLASS Instructional Support estimation accuracy using the
proposed method (Pearson $R$ up to $0.46$) approaches human inter-rater
reliability (up to $R=0.55$); (2) LLMs yield slightly greater accuracy than BoW
for this task; and (3) the best models often combined features extracted from
both LLM and BoW. Finally, (4) we illustrate how the model's outputs can be
visualized at the utterance level to provide teachers with explainable feedback
on which utterances were most positively or negatively correlated with specific
CLASS dimensions.
</p>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01134" title="Abstract">arXiv:2310.01134</a> [<a href="/pdf/2310.01134" title="Download PDF">pdf</a>, <a href="/ps/2310.01134" title="Download PostScript">ps</a>, <a href="/format/2310.01134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Existential Second-Order Logic Over Graphs: Parameterized Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bannach%2C+M">Max Bannach</a>, 
<a href="/search/cs?searchtype=author&query=Chudigiewitsch%2C+F">Florian Chudigiewitsch</a>, 
<a href="/search/cs?searchtype=author&query=Tantau%2C+T">Till Tantau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">By Fagin's Theorem, NP contains precisely those problems that can be
described by formulas starting with an existential second-order quantifier,
followed by only first-order quantifiers (ESO formulas). Subsequent research
refined this result, culminating in powerful theorems that characterize for
each possible sequence of first-order quantifiers how difficult the described
problem can be. We transfer this line of inquiry to the parameterized setting,
where the size of the set quantified by the second-order quantifier is the
parameter. Many natural parameterized problems can be described in this way
using simple sequences of first-order quantifiers: For the clique or vertex
cover problems, two universal first-order quantifiers suffice ("for all pairs
of vertices ... must hold"); for the dominating set problem, a universal
followed by an existential quantifier suffice ("for all vertices, there is a
vertex such that ..."); and so on. We present a complete characterization that
states for each possible sequence of first-order quantifiers how high the
parameterized complexity of the described problems can be. The uncovered
dividing line between quantifier sequences that lead to tractable versus
intractable problems is distinct from that known from the classical setting,
and it depends on whether the parameter is a lower bound on, an upper bound on,
or equal to the size of the quantified set.
</p>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01136" title="Abstract">arXiv:2310.01136</a> [<a href="/pdf/2310.01136" title="Download PDF">pdf</a>, <a href="/ps/2310.01136" title="Download PostScript">ps</a>, <a href="/format/2310.01136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deterministic Treasure Hunt and Rendezvous in Arbitrary Connected Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pattanayak%2C+D">Debasish Pattanayak</a>, 
<a href="/search/cs?searchtype=author&query=Pelc%2C+A">Andrzej Pelc</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Treasure hunt and rendezvous are fundamental tasks performed by mobile agents
in graphs. In treasure hunt, an agent has to find an inert target (called
treasure) situated at an unknown node of the graph. In rendezvous, two agents,
initially located at distinct nodes of the graph, traverse its edges in
synchronous rounds and have to meet at some node. We assume that the graph is
connected (otherwise none of these tasks is feasible) and consider
deterministic treasure hunt and rendezvous algorithms. The time of a treasure
hunt algorithm is the worst-case number of edge traversals performed by the
agent until the treasure is found. The time of a rendezvous algorithm is the
worst-case number of rounds since the wakeup of the earlier agent until the
meeting.
<br />To the best of our knowledge, all known treasure hunt and rendezvous
algorithms rely on the assumption that degrees of all nodes are finite, even
when the graph itself may be infinite. In the present paper we remove this
assumption for the first time, and consider both above tasks in arbitrary
connected graphs whose nodes can have either finite or countably infinite
degrees. Our main result is the first universal treasure hunt algorithm working
for arbitrary connected graphs. We prove that the time of this algorithm has
optimal order of magnitude among all possible treasure hunt algorithms working
for arbitrary connected graphs.
<br />As a consequence of this result we obtain the first universal rendezvous
algorithm working for arbitrary connected graphs. The time of this algorithm is
polynomial in a lower bound holding in many graphs, in particular in the tree
all of whose degrees are infinite.
</p>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01138" title="Abstract">arXiv:2310.01138</a> [<a href="/pdf/2310.01138" title="Download PDF">pdf</a>, <a href="/format/2310.01138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Target-Aware Contextual Political Bias Detection in News
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maab%2C+I">Iffat Maab</a>, 
<a href="/search/cs?searchtype=author&query=Marrese-Taylor%2C+E">Edison Marrese-Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Matsuo%2C+Y">Yutaka Matsuo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 figures, conference paper accepted in IJCNLP-AACL 2023 but will get published after Nov 4th Bali conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Media bias detection requires comprehensive integration of information
derived from multiple news sources. Sentence-level political bias detection in
news is no exception, and has proven to be a challenging task that requires an
understanding of bias in consideration of the context. Inspired by the fact
that humans exhibit varying degrees of writing styles, resulting in a diverse
range of statements with different local and global contexts, previous work in
media bias detection has proposed augmentation techniques to exploit this fact.
Despite their success, we observe that these techniques introduce noise by
over-generalizing bias context boundaries, which hinders performance. To
alleviate this issue, we propose techniques to more carefully search for
context using a bias-sensitive, target-aware approach for data augmentation.
Comprehensive experiments on the well-known BASIL dataset show that when
combined with pre-trained models such as BERT, our augmentation techniques lead
to state-of-the-art results. Our approach outperforms previous methods
significantly, obtaining an F1-score of 58.15 over state-of-the-art bias
detection task.
</p>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01139" title="Abstract">arXiv:2310.01139</a> [<a href="/pdf/2310.01139" title="Download PDF">pdf</a>, <a href="/ps/2310.01139" title="Download PostScript">ps</a>, <a href="/format/2310.01139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability and Generalization for Minibatch SGD and Local SGD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yunwen Lei</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingrui Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The increasing scale of data propels the popularity of leveraging parallelism
to speed up the optimization. Minibatch stochastic gradient descent (minibatch
SGD) and local SGD are two popular methods for parallel optimization. The
existing theoretical studies show a linear speedup of these methods with
respect to the number of machines, which, however, is measured by optimization
errors. As a comparison, the stability and generalization of these methods are
much less studied. In this paper, we pioneer the stability and generalization
analysis of minibatch and local SGD to understand their learnability. We
incorporate training errors into the stability analysis, which shows how small
training errors help generalization for overparameterized models. Our stability
bounds imply optimistic risk bounds which decay fast under a low noise
condition. We show both minibatch and local SGD achieve a linear speedup to
attain the optimal risk bounds.
</p>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01140" title="Abstract">arXiv:2310.01140</a> [<a href="/pdf/2310.01140" title="Download PDF">pdf</a>, <a href="/format/2310.01140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Processing of Tri-Plane Hybrid Neural Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cardace%2C+A">Adriano Cardace</a>, 
<a href="/search/cs?searchtype=author&query=Ramirez%2C+P+Z">Pierluigi Zama Ramirez</a>, 
<a href="/search/cs?searchtype=author&query=Ballerini%2C+F">Francesco Ballerini</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Allan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Salti%2C+S">Samuele Salti</a>, 
<a href="/search/cs?searchtype=author&query=Di+Stefano%2C+L">Luigi Di Stefano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Driven by the appealing properties of neural fields for storing and
communicating 3D data, the problem of directly processing them to address tasks
such as classification and part segmentation has emerged and has been
investigated in recent works. Early approaches employ neural fields
parameterized by shared networks trained on the whole dataset, achieving good
task performance but sacrificing reconstruction quality. To improve the latter,
later methods focus on individual neural fields parameterized as large
Multi-Layer Perceptrons (MLPs), which are, however, challenging to process due
to the high dimensionality of the weight space, intrinsic weight space
symmetries, and sensitivity to random initialization. Hence, results turn out
significantly inferior to those achieved by processing explicit
representations, e.g., point clouds or meshes. In the meantime, hybrid
representations, in particular based on tri-planes, have emerged as a more
effective and efficient alternative to realize neural fields, but their direct
processing has not been investigated yet. In this paper, we show that the
tri-plane discrete data structure encodes rich information, which can be
effectively processed by standard deep-learning machinery. We define an
extensive benchmark covering a diverse set of fields such as occupancy,
signed/unsigned distance, and, for the first time, radiance fields. While
processing a field with the same reconstruction quality, we achieve task
performance far superior to frameworks that process large MLPs and, for the
first time, almost on par with architectures handling explicit representations.
</p>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01142" title="Abstract">arXiv:2310.01142</a> [<a href="/pdf/2310.01142" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> [Re] CLRNet: Cross Layer Refinement Network for Lane Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=N%2C+V">Viswesh N</a>, 
<a href="/search/cs?searchtype=author&query=Jadhav%2C+K">Kaushal Jadhav</a>, 
<a href="/search/cs?searchtype=author&query=Amalanshu%2C+A">Avi Amalanshu</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+B">Bratin Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Waran%2C+S">Sabaris Waran</a>, 
<a href="/search/cs?searchtype=author&query=Sadhwani%2C+O">Om Sadhwani</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Apoorv Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Chakravarty%2C+D">Debashish Chakravarty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">The following work is a reproducibility report for CLRNet: Cross Layer
Refinement Network for Lane Detection. The basic code was made available by the
author. The paper proposes a novel Cross Layer Refinement Network to utilize
both high and low level features for lane detection. The authors assert that
the proposed technique sets the new state-of-the-art on three lane-detection
benchmarks
</p>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01143" title="Abstract">arXiv:2310.01143</a> [<a href="/pdf/2310.01143" title="Download PDF">pdf</a>, <a href="/format/2310.01143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preliminary Performance Evaluation of a Satellite-to-HAP Communication  Link
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grieco%2C+G">Giovanni Grieco</a>, 
<a href="/search/cs?searchtype=author&query=Iacovelli%2C+G">Giovanni Iacovelli</a>, 
<a href="/search/cs?searchtype=author&query=Sandri%2C+M">Mattia Sandri</a>, 
<a href="/search/cs?searchtype=author&query=Giordani%2C+M">Marco Giordani</a>, 
<a href="/search/cs?searchtype=author&query=Zorzi%2C+M">Michele Zorzi</a>, 
<a href="/search/cs?searchtype=author&query=Grieco%2C+L+A">Luigi Alfredo Grieco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The emergence of Fifth-Generation (5G) communication networks has brought
forth unprecedented connectivity with ultra-low latency, high data rates, and
pervasive coverage. However, meeting the increasing demands of applications for
seamless and high-quality communication, especially in rural areas, requires
exploring innovative solutions that expand 5G beyond traditional terrestrial
networks. Within the context of Non-Terrestrial Networks (NTNs), two promising
technologies with vast potential are High Altitude Platforms (HAPs) and
satellites. The combination of these two platforms is able to provide wide
coverage and reliable communication in remote and inaccessible areas, and/or
where terrestrial infrastructure is unavailable. This study evaluates the
performance of the communication link between a Geostationary Equatorial Orbit
(GEO) satellite and a HAP using the Internet of Drones Simulator (IoD-Sim),
implemented in ns-3 and incorporating the 3GPP TR 38.811 channel model. The
code base of IoD-Sim is extended to simulate HAPs, accounting for the Earths
curvature in various geographic coordinate systems, and considering realistic
mobility patterns. A simulation campaign is conducted to evaluate the
GEO-to-HAP communication link in terms of Signal-to-Noise Ratio (SNR) in two
different scenarios, considering the mobility of the HAP, and as a function of
the frequency and the distance.
</p>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01144" title="Abstract">arXiv:2310.01144</a> [<a href="/pdf/2310.01144" title="Download PDF">pdf</a>, <a href="/format/2310.01144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Map Equation Goes Neural
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bl%C3%B6cker%2C+C">Christopher Bl&#xf6;cker</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Chester Tan</a>, 
<a href="/search/cs?searchtype=author&query=Scholtes%2C+I">Ingo Scholtes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Community detection and graph clustering are essential for unsupervised data
exploration and understanding the high-level organisation of networked systems.
Recently, graph clustering has been highlighted as an under-explored primary
task for graph neural networks. While hierarchical graph pooling has been shown
to improve performance in graph and node classification tasks, it performs
poorly in identifying meaningful clusters. Community detection has a long
history in network science, but typically relies on optimising objective
functions with custom-tailored search algorithms, not leveraging recent
advances in deep learning, particularly from graph neural networks. In this
paper, we narrow this gap between the deep learning and network science
communities. We consider the map equation, an information-theoretic objective
function for community detection. Expressing it in a fully differentiable
tensor form that produces soft cluster assignments, we optimise the map
equation with deep learning through gradient descent. More specifically, the
reformulated map equation is a loss function compatible with any graph neural
network architecture, enabling flexible clustering and graph pooling that
clusters both graph structure and data features in an end-to-end way,
automatically finding an optimum number of clusters without explicit
regularisation. We evaluate our approach experimentally using different neural
network architectures for unsupervised clustering in synthetic and real data.
Our results show that our approach achieves competitive performance against
baselines, naturally detects overlapping communities, and avoids
over-partitioning sparse graphs.
</p>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01145" title="Abstract">arXiv:2310.01145</a> [<a href="/pdf/2310.01145" title="Download PDF">pdf</a>, <a href="/format/2310.01145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel-in-Time Probabilistic Numerical ODE Solvers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bosch%2C+N">Nathanael Bosch</a>, 
<a href="/search/math?searchtype=author&query=Corenflos%2C+A">Adrien Corenflos</a>, 
<a href="/search/math?searchtype=author&query=Yaghoobi%2C+F">Fatemeh Yaghoobi</a>, 
<a href="/search/math?searchtype=author&query=Tronarp%2C+F">Filip Tronarp</a>, 
<a href="/search/math?searchtype=author&query=Hennig%2C+P">Philipp Hennig</a>, 
<a href="/search/math?searchtype=author&query=S%C3%A4rkk%C3%A4%2C+S">Simo S&#xe4;rkk&#xe4;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Probabilistic numerical solvers for ordinary differential equations (ODEs)
treat the numerical simulation of dynamical systems as problems of Bayesian
state estimation. Aside from producing posterior distributions over ODE
solutions and thereby quantifying the numerical approximation error of the
method itself, one less-often noted advantage of this formalism is the
algorithmic flexibility gained by formulating numerical simulation in the
framework of Bayesian filtering and smoothing. In this paper, we leverage this
flexibility and build on the time-parallel formulation of iterated extended
Kalman smoothers to formulate a parallel-in-time probabilistic numerical ODE
solver. Instead of simulating the dynamical system sequentially in time, as
done by current probabilistic solvers, the proposed method processes all time
steps in parallel and thereby reduces the span cost from linear to logarithmic
in the number of time steps. We demonstrate the effectiveness of our approach
on a variety of ODEs and compare it to a range of both classic and
probabilistic numerical ODE solvers.
</p>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01146" title="Abstract">arXiv:2310.01146</a> [<a href="/pdf/2310.01146" title="Download PDF">pdf</a>, <a href="/format/2310.01146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NewsRecLib: A PyTorch-Lightning Library for Neural News Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iana%2C+A">Andreea Iana</a>, 
<a href="/search/cs?searchtype=author&query=Glava%C5%A1%2C+G">Goran Glava&#x161;</a>, 
<a href="/search/cs?searchtype=author&query=Paulheim%2C+H">Heiko Paulheim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">NewsRecLib is an open-source library based on Pytorch-Lightning and Hydra
developed for training and evaluating neural news recommendation models. The
foremost goals of NewsRecLib are to promote reproducible research and rigorous
experimental evaluation by (i) providing a unified and highly configurable
framework for exhaustive experimental studies and (ii) enabling a thorough
analysis of the performance contribution of different model architecture
components and training regimes. NewsRecLib is highly modular, allows
specifying experiments in a single configuration file, and includes extensive
logging facilities. Moreover, NewsRecLib provides out-of-the-box
implementations of several prominent neural models, training methods, standard
evaluation benchmarks, and evaluation metrics for news recommendation.
</p>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01147" title="Abstract">arXiv:2310.01147</a> [<a href="/pdf/2310.01147" title="Download PDF">pdf</a>, <a href="/format/2310.01147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Symbol Visibility through Displacement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%A4rtner%2C+B">Bernd G&#xe4;rtner</a>, 
<a href="/search/cs?searchtype=author&query=Kalani%2C+V">Vishwas Kalani</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+M+M">Meghana M. Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Meulemans%2C+W">Wouter Meulemans</a>, 
<a href="/search/cs?searchtype=author&query=Speckmann%2C+B">Bettina Speckmann</a>, 
<a href="/search/cs?searchtype=author&query=Stojakovi%C4%87%2C+M">Milo&#x161; Stojakovi&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">In information visualization, the position of symbols often encodes
associated data values. When visualizing data elements with both a numerical
and a categorical dimension, positioning in the categorical axis admits some
flexibility. This flexibility can be exploited to reduce symbol overlap, and
thereby increase legibility. In this paper we initialize the algorithmic study
of optimizing symbol legibility via a limited displacement of the symbols.
<br />Specifically, we consider unit square symbols that need to be placed at
specified $y$-coordinates. We optimize the drawing order of the symbols as well
as their $x$-displacement, constrained within a rectangular container, to
maximize the minimum visible perimeter over all squares. If the container has
width and height at most $2$, there is a point that stabs all squares. In this
case, we prove that a staircase layout is arbitrarily close to optimality and
can be computed in $O(n\log n)$ time. If the width is at most $2$, there is a
vertical line that stabs all squares, and in this case, we give a
2-approximation algorithm that runs in $O(n\log n)$ time. As a minimum visible
perimeter of $2$ is always trivially achievable, we measure this approximation
with respect to the visible perimeter exceeding $2$. We show that, despite its
simplicity, the algorithm gives asymptotically optimal results for certain
instances.
</p>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01148" title="Abstract">arXiv:2310.01148</a> [<a href="/pdf/2310.01148" title="Download PDF">pdf</a>, <a href="/format/2310.01148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cryptocurrency Portfolio Optimization by Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Q+M">Quoc Minh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+D+T">Dat Thanh Tran</a>, 
<a href="/search/cs?searchtype=author&query=Kanniainen%2C+J">Juho Kanniainen</a>, 
<a href="/search/cs?searchtype=author&query=Iosifidis%2C+A">Alexandros Iosifidis</a>, 
<a href="/search/cs?searchtype=author&query=Gabbouj%2C+M">Moncef Gabbouj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, accepted at SSCI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Many cryptocurrency brokers nowadays offer a variety of derivative assets
that allow traders to perform hedging or speculation. This paper proposes an
effective algorithm based on neural networks to take advantage of these
investment products. The proposed algorithm constructs a portfolio that
contains a pair of negatively correlated assets. A deep neural network, which
outputs the allocation weight of each asset at a time interval, is trained to
maximize the Sharpe ratio. A novel loss term is proposed to regulate the
network's bias towards a specific asset, thus enforcing the network to learn an
allocation strategy that is close to a minimum variance strategy. Extensive
experiments were conducted using data collected from Binance spanning 19 months
to evaluate the effectiveness of our approach. The backtest results show that
the proposed algorithm can produce neural networks that are able to make
profits in different market situations.
</p>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01149" title="Abstract">arXiv:2310.01149</a> [<a href="/pdf/2310.01149" title="Download PDF">pdf</a>, <a href="/format/2310.01149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On $b$-Matching and Fully-Dynamic Maximum $k$-Edge Coloring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=El-Hayek%2C+A">Antoine El-Hayek</a>, 
<a href="/search/cs?searchtype=author&query=Hanauer%2C+K">Kathrin Hanauer</a>, 
<a href="/search/cs?searchtype=author&query=Henzinger%2C+M">Monika Henzinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages + references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Given a graph $G$ that is modified by a sequence of edge insertions and
deletions, we study the Maximum $k$-Edge Coloring problem Having access to $k$
colors, how can we color as many edges of $G$ as possible such that no two
adjacent edges share the same color? While this problem is different from
simply maintaining a $b$-matching with $b=k$, the two problems are closely
related: a maximum $k$-matching always contains a $\frac{k+1}k$-approximate
maximum $k$-edge coloring. However, maximum $b$-matching can be solved
efficiently in the static setting, whereas the Maximum $k$-Edge Coloring
problem is NP-hard and even APX-hard for $k \ge 2$.
<br />We present new results on both problems: For $b$-matching, we show a new
integrality gap result and for the case where $b$ is a constant, we adapt
Wajc's matching sparsification scheme~[STOC20].
<br />Using these as basis, we give three new algorithms for the dynamic Maximum
$k$-Edge Coloring problem: Our MatchO algorithm builds on the dynamic
$(2+\epsilon)$-approximation algorithm of Bhattacharya, Gupta, and
Mohan~[ESA17] for $b$-matching and achieves a $(2+\epsilon)\frac{k+1}
k$-approximation in $O(poly(\log n, \epsilon^{-1}))$ update time against an
oblivious adversary. Our MatchA algorithm builds on the dynamic
$8$-approximation algorithm by Bhattacharya, Henzinger, and Italiano~[SODA15]
for fractional $b$-matching and achieves a
$(8+\epsilon)\frac{3k+3}{3k-1}$-approximation in $O(poly(\log n,
\epsilon^{-1}))$ update time against an adaptive adversary. Moreover, our
reductions use the dynamic $b$-matching algorithm as a black box, so any future
improvement in the approximation ratio for dynamic $b$-matching will
automatically translate into a better approximation ratio for our algorithms.
Finally, we present a greedy algorithm that runs in $O(\Delta+k)$ update time,
while guaranteeing a $2.16$~approximation factor.
</p>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01151" title="Abstract">arXiv:2310.01151</a> [<a href="/pdf/2310.01151" title="Download PDF">pdf</a>, <a href="/format/2310.01151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Functions by Teams of Deterministic Finite Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pattanayak%2C+D">Debasish Pattanayak</a>, 
<a href="/search/cs?searchtype=author&query=Pelc%2C+A">Andrzej Pelc</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">We consider the task of computing functions $f: \mathbb{N}^k\to \mathbb{N}$,
where $ \mathbb{N}$ is the set of natural numbers, by finite teams of agents
modelled as deterministic finite automata. The computation is carried out in a
distributed way, using the {\em discrete half-line}, which is the infinite
graph with one node of degree 1 (called the root) and infinitely many nodes of
degree 2. The node at distance $j$ from the root represents the integer $j$. We
say that a team $\mathcal{A}^f$ of automata computes a function $f$, if in the
beginning of the computation all automata from $\mathcal{A}^f$ are located at
the arguments $x_1,\dots,x_k$ of the function $f$, in groups $\mathcal{A}^f _j$
at $x_j$, and at the end, all automata of the team gather at $f(x_1,\dots,x_k)$
and transit to a special state $STOP$. At each step of the computation, an
automaton $a$ can ``see'' states of all automata colocated at the same node:
the set of these states forms an input of $a$.
<br />Our main result shows that, for every primitive recursive function, there
exists a finite team of automata that computes this function. We prove this by
showing that basic primitive recursive functions can be computed by teams of
automata, and that functions resulting from the operations of composition and
of primitive recursion can be computed by teams of automata, provided that the
ingredient functions of these operations can be computed by teams of automata.
We also observe that cooperation between automata is necessary: even some very
simple functions $f: \mathbb{N}\to \mathbb{N}$ cannot be computed by a single
automaton.
</p>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01152" title="Abstract">arXiv:2310.01152</a> [<a href="/pdf/2310.01152" title="Download PDF">pdf</a>, <a href="/format/2310.01152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model-Powered Smart Contract Vulnerability Detection: New  Perspectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Sihao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tiansheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=%C4%B0lhan%2C+F">Fatih &#x130;lhan</a>, 
<a href="/search/cs?searchtype=author&query=Tekin%2C+S+F">Selim Fukan Tekin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Ling Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE International Conference on Trust, Privacy and Security in
  Intelligent Systems, and Applications 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper provides a systematic analysis of the opportunities, challenges,
and potential solutions of harnessing LLMs to dig out vulnerabilities within
smart contracts based on our ongoing research. For the smart contract
vulnerability detection task, the key to achieving practical usability lies in
detecting as many true vulnerabilities as possible while minimizing the number
of false positives. However, our empirical study using LLM as a detection tool
reveals interesting yet contradictory findings: generating more answers with
higher randomness largely increases the likelihood of a correct answer being
generated while inevitably leading to a higher number of false positives,
resulting in exhaustive manual verification efforts. To mitigate this tension,
we propose an adversarial framework dubbed GPTLens that breaks the traditional
one-stage detection into two synergistic stages $-$ generation and
discrimination, for progressive detection and fine-tuning, wherein the LLM
plays dual roles, i.e., auditor and critic, respectively. The goal of auditor
is to identify multiple diverse vulnerabilities with intermediate reasoning,
while the goal of critic is to evaluate the accuracy of identified
vulnerabilities and to examine the integrity of the detection reasoning.
Experimental results and illustrative examples demonstrate that auditor and
critic work together harmoniously to yield significant improvements over the
traditional one-stage detection. GPTLens is intuitive, strategic, and entirely
LLM-driven without relying on specialist expertise in smart contracts,
showcasing its methodical generality and potential to detect a broad spectrum
of vulnerabilities. Our code is available at:
https://github.com/git-disl/GPTLens.
</p>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01154" title="Abstract">arXiv:2310.01154</a> [<a href="/pdf/2310.01154" title="Download PDF">pdf</a>, <a href="/format/2310.01154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modularity in Deep Learning: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haozhe Sun</a> (LISN, TAU, Inria), 
<a href="/search/cs?searchtype=author&query=Guyon%2C+I">Isabelle Guyon</a> (TAU, LISN, Inria)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Intelligent computing, Jun 2023, London, United Kingdom.
  pp.561-595
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Modularity is a general principle present in many fields. It offers
attractive advantages, including, among others, ease of conceptualization,
interpretability, scalability, module combinability, and module reusability.
The deep learning community has long sought to take inspiration from the
modularity principle, either implicitly or explicitly. This interest has been
increasing over recent years. We review the notion of modularity in deep
learning around three axes: data, task, and model, which characterize the life
cycle of deep learning. Data modularity refers to the observation or creation
of data groups for various purposes. Task modularity refers to the
decomposition of tasks into sub-tasks. Model modularity means that the
architecture of a neural network system can be decomposed into identifiable
modules. We describe different instantiations of the modularity principle, and
we contextualize their advantages in different deep learning sub-fields.
Finally, we conclude the paper with a discussion of the definition of
modularity and directions for future research.
</p>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01155" title="Abstract">arXiv:2310.01155</a> [<a href="/pdf/2310.01155" title="Download PDF">pdf</a>, <a href="/ps/2310.01155" title="Download PostScript">ps</a>, <a href="/format/2310.01155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EIP-4844 Economics and Rollup Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Crapis%2C+D">Davide Crapis</a>, 
<a href="/search/cs?searchtype=author&query=Felten%2C+E+W">Edward W. Felten</a>, 
<a href="/search/cs?searchtype=author&query=Mamageishvili%2C+A">Akaki Mamageishvili</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Theoretical Economics (econ.TH)

</div>
<p class="mathjax">We study the economics of the Ethereum improvement proposal 4844 and its
effect on rollups' data posting strategies. Rollups' cost consists of two
parts: data posting and delay. In the new proposal, the data posting cost
corresponds to a blob posting cost and is fixed in each block, no matter how
much of the blob is utilized by the rollup. The tradeoff is clear: the rollup
prefers to post a full blob, but if its transaction arrival rate is low,
filling up a blob space causes too large delay cost. The first result of the
paper shows that if a rollup transaction arrival rate is too low, it prefers to
use the regular blockspace market for data posting, as it offers a more
flexible cost structure. Second, we show that shared blob posting is not always
beneficial for participating rollups and change in the aggregate blob posting
cost in the equilibrium depends on the types of participating rollups. In the
end, we discuss blob cost-sharing rules from an axiomatic angle.
</p>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01156" title="Abstract">arXiv:2310.01156</a> [<a href="/pdf/2310.01156" title="Download PDF">pdf</a>, <a href="/format/2310.01156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Fiber Activation in Unipolar vs Bipolar Deep Brain Stimulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Frigge%2C+A+F">Anna Franziska Frigge</a>, 
<a href="/search/eess?searchtype=author&query=Medvedev%2C+A">Alexander Medvedev</a>, 
<a href="/search/eess?searchtype=author&query=Jiltsova%2C+E">Elena Jiltsova</a>, 
<a href="/search/eess?searchtype=author&query=Nyholm%2C+D">Dag Nyholm</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review (American Control Conference 2024), 7 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Deep Brain Stimulation (DBS) is an established and powerful treatment method
in various neurological disorders. It involves chronically delivering
electrical pulses to a certain stimulation target in the brain in order to
alleviate the symptoms of a disease. Traditionally, the effect of DBS on neural
tissue has been modeled based on the geometrical intersection of the static
Volume of Tissue Activated (VTA) and the stimulation target. Recent studies
suggest that the Dentato-Rubro-Thalamic Tract (DRTT) may serve as a potential
common underlying stimulation target for tremor control in Essential Tremor
(ET). However, clinical observations highlight that the therapeutic effect of
DBS, especially in ET, is strongly influenced by the dynamic DBS parameters
such as pulse width and frequency, as well as stimulation polarity. This study
introduces a computational model to elucidate the effect of the stimulation
signal shape on the DRTT under neural input. The simulation results suggest
that achieving a specific pulse amplitude threshold is necessary before
eliciting the therapeutic effect through adjustments in pulse widths and
frequencies becomes feasible. Longer pulse widths proved more likely to induce
firing, thus requiring a lower stimulation amplitude. Additionally, the
modulation effect of bipolar configurations on neural traffic was found to vary
significantly depending on the chosen stimulation polarity and the direction of
neural traffic. Further, bipolar configurations demonstrated the ability to
selectively influence firing patterns in different fiber tracts.
</p>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01157" title="Abstract">arXiv:2310.01157</a> [<a href="/pdf/2310.01157" title="Download PDF">pdf</a>, <a href="/format/2310.01157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RRR-Net: Reusing, Reducing, and Recycling a Deep Backbone Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haozhe Sun</a> (LISN, TAU, Inria), 
<a href="/search/cs?searchtype=author&query=Guyon%2C+I">Isabelle Guyon</a> (LISN, TAU, Inria), 
<a href="/search/cs?searchtype=author&query=Mohr%2C+F">Felix Mohr</a>, 
<a href="/search/cs?searchtype=author&query=Tabia%2C+H">Hedi Tabia</a> (IBISC)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 International Joint Conference on Neural Networks (IJCNN),
  Jun 2023, Gold Coast, Australia. pp.1-9
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">It has become mainstream in computer vision and other machine learning
domains to reuse backbone networks pre-trained on large datasets as
preprocessors. Typically, the last layer is replaced by a shallow learning
machine of sorts; the newly-added classification head and (optionally) deeper
layers are fine-tuned on a new task. Due to its strong performance and
simplicity, a common pre-trained backbone network is ResNet152.However,
ResNet152 is relatively large and induces inference latency. In many cases, a
compact and efficient backbone with similar performance would be preferable
over a larger, slower one. This paper investigates techniques to reuse a
pre-trained backbone with the objective of creating a smaller and faster model.
Starting from a large ResNet152 backbone pre-trained on ImageNet, we first
reduce it from 51 blocks to 5 blocks, reducing its number of parameters and
FLOPs by more than 6 times, without significant performance degradation. Then,
we split the model after 3 blocks into several branches, while preserving the
same number of parameters and FLOPs, to create an ensemble of sub-networks to
improve performance. Our experiments on a large benchmark of $40$ image
classification datasets from various domains suggest that our techniques match
the performance (if not better) of ``classical backbone fine-tuning'' while
achieving a smaller model size and faster inference speed.
</p>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01160" title="Abstract">arXiv:2310.01160</a> [<a href="/pdf/2310.01160" title="Download PDF">pdf</a>, <a href="/format/2310.01160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design, Modelling and Control of an Amphibious Quad-Rotor for Pipeline  Inspection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Durdevic%2C+P">Petar Durdevic</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shaobao Li</a>, 
<a href="/search/cs?searchtype=author&query=Ortiz-Arroyo%2C+D">Daniel Ortiz-Arroyo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Regular inspections are crucial to maintaining waste-water pipelines in good
condition. The challenge is that inside a pipeline the space is narrow and may
have a complex structure. The conventional methods that use pipe robots with
heavy cables are expensive, time-consuming, and difficult to operate. In this
work, we develop an amphibious system that combines a quad-copter with a
surface vehicle, creating a hybrid unmanned aerial floating vehicle (HUAFV).
Nonlinear dynamics of the HUAFV are modeled based on the dynamic models of both
operating modes. The model is validated through experiments and simulations. A
PI controller designed and tuned on the developed model is implemented onto a
prototype platform. Our experiments demonstrate the effectiveness of the new
HUAFV's modeling and design.
</p>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01162" title="Abstract">arXiv:2310.01162</a> [<a href="/pdf/2310.01162" title="Download PDF">pdf</a>, <a href="/format/2310.01162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DINE: Dimensional Interpretability of Node Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piaggesi%2C+S">Simone Piaggesi</a>, 
<a href="/search/cs?searchtype=author&query=Khosla%2C+M">Megha Khosla</a>, 
<a href="/search/cs?searchtype=author&query=Panisson%2C+A">Andr&#xe9; Panisson</a>, 
<a href="/search/cs?searchtype=author&query=Anand%2C+A">Avishek Anand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graphs are ubiquitous due to their flexibility in representing social and
technological systems as networks of interacting elements. Graph representation
learning methods, such as node embeddings, are powerful approaches to map nodes
into a latent vector space, allowing their use for various graph tasks. Despite
their success, only few studies have focused on explaining node embeddings
locally. Moreover, global explanations of node embeddings remain unexplored,
limiting interpretability and debugging potentials. We address this gap by
developing human-understandable explanations for dimensions in node embeddings.
Towards that, we first develop new metrics that measure the global
interpretability of embedding vectors based on the marginal contribution of the
embedding dimensions to predicting graph structure. We say that an embedding
dimension is more interpretable if it can faithfully map to an understandable
sub-structure in the input graph - like community structure. Having observed
that standard node embeddings have low interpretability, we then introduce DINE
(Dimension-based Interpretable Node Embedding), a novel approach that can
retrofit existing node embeddings by making them more interpretable without
sacrificing their task performance. We conduct extensive experiments on
synthetic and real-world graphs and show that we can simultaneously learn
highly interpretable node embeddings with effective performance in link
prediction.
</p>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01163" title="Abstract">arXiv:2310.01163</a> [<a href="/pdf/2310.01163" title="Download PDF">pdf</a>, <a href="/format/2310.01163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trust-Aware Motion Planning for Human-Robot Collaboration under  Distribution Temporal Logic Specifications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Pian Yu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+S">Shuyang Dong</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+S">Shili Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Lu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Kwiatkowska%2C+M">Marta Kwiatkowska</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Recent work has considered trust-aware decision making for human-robot
collaboration (HRC) with a focus on model learning. In this paper, we are
interested in enabling the HRC system to complete complex tasks specified using
temporal logic that involve human trust. Since human trust in robots is not
observable, we adopt the widely used partially observable Markov decision
process (POMDP) framework for modelling the interactions between humans and
robots. To specify the desired behaviour, we propose to use syntactically
co-safe linear distribution temporal logic (scLDTL), a logic that is defined
over predicates of states as well as belief states of partially observable
systems. The incorporation of belief predicates in scLDTL enhances its
expressiveness while simultaneously introducing added complexity. This also
presents a new challenge as the belief predicates must be evaluated over the
continuous (infinite) belief space. To address this challenge, we present an
algorithm for solving the optimal policy synthesis problem. First, we enhance
the belief MDP (derived by reformulating the POMDP) with a probabilistic
labelling function. Then a product belief MDP is constructed between the
probabilistically labelled belief MDP and the automaton translation of the
scLDTL formula. Finally, we show that the optimal policy can be obtained by
leveraging existing point-based value iteration algorithms with essential
modifications. Human subject experiments with 21 participants on a driving
simulator demonstrate the effectiveness of the proposed approach.
</p>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01164" title="Abstract">arXiv:2310.01164</a> [<a href="/pdf/2310.01164" title="Download PDF">pdf</a>, <a href="/format/2310.01164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segment Any Building
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The identification and segmentation of buildings in remote sensing imagery
has consistently been a important point of academic research. This work
highlights the effectiveness of using diverse datasets and advanced
representation learning models for the purpose of building segmentation in
remote sensing images. By fusing various datasets, we have broadened the scope
of our learning resources and achieved exemplary performance across several
datasets. Our innovative joint training process demonstrates the value of our
methodology in various critical areas such as urban planning, disaster
management, and environmental monitoring. Our approach, which involves
combining dataset fusion techniques and prompts from pre-trained models, sets a
new precedent for building segmentation tasks. The results of this study
provide a foundation for future exploration and indicate promising potential
for novel applications in building segmentation field.
</p>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01165" title="Abstract">arXiv:2310.01165</a> [<a href="/pdf/2310.01165" title="Download PDF">pdf</a>, <a href="/format/2310.01165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards guarantees for parameter isolation in continual learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lanzillotta%2C+G">Giulia Lanzillotta</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S+P">Sidak Pal Singh</a>, 
<a href="/search/cs?searchtype=author&query=Grewe%2C+B+F">Benjamin F. Grewe</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+T">Thomas Hofmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep learning has proved to be a successful paradigm for solving many
challenges in machine learning. However, deep neural networks fail when trained
sequentially on multiple tasks, a shortcoming known as catastrophic forgetting
in the continual learning literature. Despite a recent flourish of learning
algorithms successfully addressing this problem, we find that provable
guarantees against catastrophic forgetting are lacking. In this work, we study
the relationship between learning and forgetting by looking at the geometry of
neural networks' loss landscape. We offer a unifying perspective on a family of
continual learning algorithms, namely methods based on parameter isolation, and
we establish guarantees on catastrophic forgetting for some of them.
</p>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01166" title="Abstract">arXiv:2310.01166</a> [<a href="/pdf/2310.01166" title="Download PDF">pdf</a>, <a href="/format/2310.01166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gotcha! This Model Uses My Code! Evaluating Membership Leakage Risks in  Code Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhou Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhipeng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jieke Shi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dongsum Kim</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Donggyun Han</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+D">David Lo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Given large-scale source code datasets available in open-source projects and
advanced large language models, recent code models have been proposed to
address a series of critical software engineering tasks, such as program repair
and code completion. The training data of the code models come from various
sources, not only the publicly available source code, e.g., open-source
projects on GitHub but also the private data such as the confidential source
code from companies, which may contain sensitive information (for example, SSH
keys and personal information). As a result, the use of these code models may
raise new privacy concerns.
<br />In this paper, we focus on a critical yet not well-explored question on using
code models: what is the risk of membership information leakage in code models?
Membership information leakage refers to the risk that an attacker can infer
whether a given data point is included in (i.e., a member of) the training
data. To answer this question, we propose Gotcha, a novel membership inference
attack method specifically for code models. We investigate the membership
leakage risk of code models. Our results reveal a worrying fact that the risk
of membership leakage is high: although the previous attack methods are close
to random guessing, Gotcha can predict the data membership with a high true
positive rate of 0.95 and a low false positive rate of 0.10. We also show that
the attacker's knowledge of the victim model (e.g., the model architecture and
the pre-training data) impacts the success rate of attacks. Further analysis
demonstrates that changing the decoding strategy can mitigate the risk of
membership leakage. This study calls for more attention to understanding the
privacy of code models and developing more effective countermeasures against
such attacks.
</p>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01174" title="Abstract">arXiv:2310.01174</a> [<a href="/pdf/2310.01174" title="Download PDF">pdf</a>, <a href="/format/2310.01174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Light Schr&#xf6;dinger Bridge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Korotin%2C+A">Alexander Korotin</a>, 
<a href="/search/cs?searchtype=author&query=Gushchin%2C+N">Nikita Gushchin</a>, 
<a href="/search/cs?searchtype=author&query=Burnaev%2C+E">Evgeny Burnaev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Despite the recent advances in the field of computational Schrodinger Bridges
(SB), most existing SB solvers are still heavy-weighted and require complex
optimization of several neural networks. It turns out that there is no
principal solver which plays the role of simple-yet-effective baseline for SB
just like, e.g., $k$-means method in clustering, logistic regression in
classification or Sinkhorn algorithm in discrete optimal transport. We address
this issue and propose a novel fast and simple SB solver. Our development is a
smart combination of two ideas which recently appeared in the field: (a)
parameterization of the Schrodinger potentials with sum-exp quadratic functions
and (b) viewing the log-Schrodinger potentials as the energy functions. We show
that combined together these ideas yield a lightweight, simulation-free and
theoretically justified SB solver with a simple straightforward optimization
objective. As a result, it allows solving SB in moderate dimensions in a matter
of minutes on CPU without a painful hyperparameter selection. Our light solver
resembles the Gaussian mixture model which is widely used for density
estimation. Inspired by this similarity, we also prove an important theoretical
result showing that our light solver is a universal approximator of SBs. The
code for the LightSB solver can be found at
https://github.com/ngushchin/LightSB
</p>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01180" title="Abstract">arXiv:2310.01180</a> [<a href="/pdf/2310.01180" title="Download PDF">pdf</a>, <a href="/format/2310.01180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolutionary Neural Architecture Search for Transformer in Knowledge  Tracing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shangshang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiaoshan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Ye Tian</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xueming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Haiping Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingyi Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, accepted by NeurIPS 2023(poster)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Knowledge tracing (KT) aims to trace students' knowledge states by predicting
whether students answer correctly on exercises. Despite the excellent
performance of existing Transformer-based KT approaches, they are criticized
for the manually selected input features for fusion and the defect of single
global context modelling to directly capture students' forgetting behavior in
KT, when the related records are distant from the current record in terms of
time. To address the issues, this paper first considers adding convolution
operations to the Transformer to enhance its local context modelling ability
used for students' forgetting behavior, then proposes an evolutionary neural
architecture search approach to automate the input feature selection and
automatically determine where to apply which operation for achieving the
balancing of the local/global context modelling. In the search space, the
original global path containing the attention module in Transformer is replaced
with the sum of a global path and a local path that could contain different
convolutions, and the selection of input features is also considered. To search
the best architecture, we employ an effective evolutionary algorithm to explore
the search space and also suggest a search space reduction strategy to
accelerate the convergence of the algorithm. Experimental results on the two
largest and most challenging education datasets demonstrate the effectiveness
of the architecture found by the proposed approach.
</p>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01181" title="Abstract">arXiv:2310.01181</a> [<a href="/pdf/2310.01181" title="Download PDF">pdf</a>, <a href="/format/2310.01181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Isomorphic Networks for Assessing Reliability of the  Medium-Voltage Grid
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Nooten%2C+C+C">Charlotte Cambier van Nooten</a>, 
<a href="/search/cs?searchtype=author&query=van+de+Poll%2C+T">Tom van de Poll</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%BCllhase%2C+S">Sonja F&#xfc;llhase</a>, 
<a href="/search/cs?searchtype=author&query=Heres%2C+J">Jacco Heres</a>, 
<a href="/search/cs?searchtype=author&query=Heskes%2C+T">Tom Heskes</a>, 
<a href="/search/cs?searchtype=author&query=Shapovalova%2C+Y">Yuliya Shapovalova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Ensuring electricity grid reliability becomes increasingly challenging with
the shift towards renewable energy and declining conventional capacities.
Distribution System Operators (DSOs) aim to achieve grid reliability by
verifying the n-1 principle, ensuring continuous operation in case of component
failure. Electricity networks' complex graph-based data holds crucial
information for n-1 assessment: graph structure and data about stations/cables.
Unlike traditional machine learning methods, Graph Neural Networks (GNNs)
directly handle graph-structured data. This paper proposes using Graph
Isomorphic Networks (GINs) for n-1 assessments in medium voltage grids. The GIN
framework is designed to generalise to unseen grids and utilise graph structure
and data about stations/cables. The proposed GIN approach demonstrates faster
and more reliable grid assessments than a traditional mathematical optimisation
approach, reducing prediction times by approximately a factor of 1000. The
findings offer a promising approach to address computational challenges and
enhance the reliability and efficiency of energy grid assessments.
</p>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01188" title="Abstract">arXiv:2310.01188</a> [<a href="/pdf/2310.01188" title="Download PDF">pdf</a>, <a href="/format/2310.01188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying the Plausibility of Context Reliance in Neural Machine  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarti%2C+G">Gabriele Sarti</a>, 
<a href="/search/cs?searchtype=author&query=Chrupa%C5%82a%2C+G">Grzegorz Chrupa&#x142;a</a>, 
<a href="/search/cs?searchtype=author&query=Nissim%2C+M">Malvina Nissim</a>, 
<a href="/search/cs?searchtype=author&query=Bisazza%2C+A">Arianna Bisazza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, under review. 24 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Establishing whether language models can use contextual information in a
human-plausible way is important to ensure their safe adoption in real-world
settings. However, the questions of when and which parts of the context affect
model generations are typically tackled separately, and current plausibility
evaluations are practically limited to a handful of artificial benchmarks. To
address this, we introduce Plausibility Evaluation of Context Reliance
(PECoRe), an end-to-end interpretability framework designed to quantify context
usage in language models' generations. Our approach leverages model internals
to (i) contrastively identify context-sensitive target tokens in generated
texts and (ii) link them to contextual cues justifying their prediction. We use
PECoRe to quantify the plausibility of context-aware machine translation
models, comparing model rationales with human annotations across several
discourse-level phenomena. Finally, we apply our method to unannotated
generations to identify context-mediated predictions and highlight instances of
(im)plausible context usage in model translations.
</p>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01190" title="Abstract">arXiv:2310.01190</a> [<a href="/pdf/2310.01190" title="Download PDF">pdf</a>, <a href="/ps/2310.01190" title="Download PostScript">ps</a>, <a href="/format/2310.01190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-Theoretic B&#xe9;zier Curve Optimization over Safe Corridors for Safe  and Smooth Motion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zayou%2C+S">Soufyan Zayou</a>, 
<a href="/search/cs?searchtype=author&query=Arslan%2C+%C3%96">&#xd6;m&#xfc;r Arslan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures, 3 tables, extended version of a paper submitted for publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computational Geometry (cs.CG); Systems and Control (eess.SY)

</div>
<p class="mathjax">As a parametric motion representation, B\'ezier curves have significant
applications in polynomial trajectory optimization for safe and smooth motion
planning of various robotic systems, including flying drones, autonomous
vehicles, and robotic manipulators. An essential component of B\'ezier curve
optimization is the optimization objective, as it significantly influences the
resulting robot motion. Standard physical optimization objectives, such as
minimizing total velocity, acceleration, jerk, and snap, are known to yield
quadratic optimization of B\'ezier curve control points. In this paper, we
present a unifying graph-theoretic perspective for defining and understanding
B\'ezier curve optimization objectives using a consensus distance of B\'ezier
control points derived based on their interaction graph Laplacian. In addition
to demonstrating how standard physical optimization objectives define a
consensus distance between B\'ezier control points, we also introduce geometric
and statistical optimization objectives as alternative consensus distances,
constructed using finite differencing and differential variance. To compare
these optimization objectives, we apply B\'ezier curve optimization over convex
polygonal safe corridors that are automatically constructed around a
maximal-clearance minimal-length reference path. We provide an explicit
analytical formulation for quadratic optimization of B\'ezier curves using
B\'ezier matrix operations. We conclude that the norm and variance of the
finite differences of B\'ezier control points lead to simpler and more
intuitive interaction graphs and optimization objectives compared to B\'ezier
derivative norms, despite having similar robot motion profiles.
</p>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01194" title="Abstract">arXiv:2310.01194</a> [<a href="/pdf/2310.01194" title="Download PDF">pdf</a>, <a href="/ps/2310.01194" title="Download PostScript">ps</a>, <a href="/format/2310.01194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing Hyperexponential Functions over Monomial Extensions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shaoshi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hao Du</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yiman Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziming Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>

</div>
<p class="mathjax">We extend the shell and kernel reductions for hyperexponential functions over
the field of rational functions to a monomial extension. Both of the reductions
are incorporated into one algorithm. As an application, we present an additive
decomposition in rationally hyperexponential towers. The decomposition yields
an alternative algorithm for computing elementary integrals over such towers.
The alternative can find some elementary integrals that are unevaluated by the
integrators in the latest versions of Maple and Mathematica.
</p>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01195" title="Abstract">arXiv:2310.01195</a> [<a href="/pdf/2310.01195" title="Download PDF">pdf</a>, <a href="/ps/2310.01195" title="Download PostScript">ps</a>, <a href="/format/2310.01195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated K-means Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garst%2C+S">Swier Garst</a>, 
<a href="/search/cs?searchtype=author&query=Reinders%2C+M">Marcel Reinders</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated learning is a technique that enables the use of distributed
datasets for machine learning purposes without requiring data to be pooled,
thereby better preserving privacy and ownership of the data. While supervised
FL research has grown substantially over the last years, unsupervised FL
methods remain scarce. This work introduces an algorithm which implements
K-means clustering in a federated manner, addressing the challenges of varying
number of clusters between centers, as well as convergence on less separable
datasets.
</p>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01196" title="Abstract">arXiv:2310.01196</a> [<a href="/pdf/2310.01196" title="Download PDF">pdf</a>, <a href="/format/2310.01196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal transport for mesh adaptivity and shock capturing of  compressible flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Nguyen%2C+N+C">Ngoc Cuong Nguyen</a>, 
<a href="/search/math?searchtype=author&query=Van+Heyningen%2C+R+L">R. Loek Van Heyningen</a>, 
<a href="/search/math?searchtype=author&query=Vila-Perez%2C+J">Jordi Vila-Perez</a>, 
<a href="/search/math?searchtype=author&query=Peraire%2C+J">Jaime Peraire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 22 figures. arXiv admin note: text overlap with <a href="/abs/2305.00461">arXiv:2305.00461</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph); Analysis of PDEs (math.AP)

</div>
<p class="mathjax">We present an optimal transport approach for mesh adaptivity and shock
capturing of compressible flows. Shock capturing is based on a viscosity
regularization of the governing equations by introducing an artificial
viscosity field as solution of the Helmholtz equation. Mesh adaptation is based
on the optimal transport theory by formulating a mesh mapping as solution of
Monge-Ampere equation. The marriage of optimal transport and viscosity
regularization for compressible flows leads to a coupled system of the
compressible Euler/Navier-Stokes equations, the Helmholtz equation, and the
Monge-Ampere equation. We propose an iterative procedure to solve the coupled
system in a sequential fashion using homotopy continuation to minimize the
amount of artificial viscosity while enforcing positivity-preserving and
smoothness constraints on the numerical solution. We explore various mesh
monitor functions for computing r-adaptive meshes in order to reduce the amount
of artificial dissipation and improve the accuracy of the numerical solution.
The hybridizable discontinuous Galerkin method is used for the spatial
discretization of the governing equations to obtain high-order accurate
solutions. Extensive numerical results are presented to demonstrate the optimal
transport approach on transonic, supersonic, hypersonic flows in two
dimensions. The approach is found to yield accurate, sharp yet smooth solutions
within a few mesh adaptation iterations.
</p>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01200" title="Abstract">arXiv:2310.01200</a> [<a href="/pdf/2310.01200" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A worldwide overview on the information security posture of online  public services
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silva%2C+J+M">Jo&#xe3;o Marco Silva</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+D">Diogo Ribeiro</a>, 
<a href="/search/cs?searchtype=author&query=Ramos%2C+L+F">Luis Felipe Ramos</a>, 
<a href="/search/cs?searchtype=author&query=Fonte%2C+V">V&#xed;tor Fonte</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 57th Hawaii International Conference on System Sciences, 2024 (HICSS-57)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">The availability of public services through online platforms has improved the
coverage and efficiency of essential services provided to citizens worldwide.
These services also promote transparency and foster citizen participation in
government processes. However, the increased online presence also exposes
sensitive data exchanged between citizens and service providers to a wider
range of security threats. Therefore, ensuring the security and trustworthiness
of online services is crucial to Electronic Government (EGOV) initiatives'
success. Hence, this work assesses the security posture of online platforms
hosted in 3068 governmental domain names, across all UN Member States, in three
dimensions: support for secure communication protocols; the trustworthiness of
their digital certificate chains; and services' exposure to known
vulnerabilities. The results indicate that despite its rapid development, the
public sector still falls short in adopting international standards and best
security practices in services and infrastructure management. This reality
poses significant risks to citizens and services across all regions and income
levels.
</p>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01201" title="Abstract">arXiv:2310.01201</a> [<a href="/pdf/2310.01201" title="Download PDF">pdf</a>, <a href="/format/2310.01201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sebia%2C+H">Hana Sebia</a>, 
<a href="/search/cs?searchtype=author&query=Guyet%2C+T">Thomas Guyet</a>, 
<a href="/search/cs?searchtype=author&query=Audureau%2C+E">Etienne Audureau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Tensor decomposition has recently been gaining attention in the machine
learning community for the analysis of individual traces, such as Electronic
Health Records (EHR). However, this task becomes significantly more difficult
when the data follows complex temporal patterns. This paper introduces the
notion of a temporal phenotype as an arrangement of features over time and it
proposes SWoTTeD (Sliding Window for Temporal Tensor Decomposition), a novel
method to discover hidden temporal patterns. SWoTTeD integrates several
constraints and regularizations to enhance the interpretability of the
extracted phenotypes. We validate our proposal using both synthetic and
real-world datasets, and we present an original usecase using data from the
Greater Paris University Hospital. The results show that SWoTTeD achieves at
least as accurate reconstruction as recent state-of-the-art tensor
decomposition models, and extracts temporal phenotypes that are meaningful for
clinicians.
</p>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01206" title="Abstract">arXiv:2310.01206</a> [<a href="/pdf/2310.01206" title="Download PDF">pdf</a>, <a href="/format/2310.01206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> appjsonify: An Academic Paper PDF-to-JSON Conversion Toolkit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamaguchi%2C+A">Atsuki Yamaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Morishita%2C+T">Terufumi Morishita</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. PyPI: <a href="https://pypi.org/project/appjsonify/">this https URL</a> GitHub: <a href="https://pypi.org/project/appjsonify/.">this https URL</a> arXiv admin note: text overlap with <a href="/abs/2308.07336">arXiv:2308.07336</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present appjsonify, a Python-based PDF-to-JSON conversion toolkit for
academic papers. It parses a PDF file using several visual-based document
layout analysis models and rule-based text processing approaches. appjsonify is
a flexible tool that allows users to easily configure the processing pipeline
to handle a specific format of a paper they wish to process. We are publicly
releasing appjsonify as an easy-to-install toolkit available via PyPI and
GitHub.
</p>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01207" title="Abstract">arXiv:2310.01207</a> [<a href="/pdf/2310.01207" title="Download PDF">pdf</a>, <a href="/format/2310.01207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learn to Follow: Decentralized Lifelong Multi-agent Pathfinding via  Planning and Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Skrynnik%2C+A">Alexey Skrynnik</a>, 
<a href="/search/cs?searchtype=author&query=Andreychuk%2C+A">Anton Andreychuk</a>, 
<a href="/search/cs?searchtype=author&query=Nesterova%2C+M">Maria Nesterova</a>, 
<a href="/search/cs?searchtype=author&query=Yakovlev%2C+K">Konstantin Yakovlev</a>, 
<a href="/search/cs?searchtype=author&query=Panov%2C+A">Aleksandr Panov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Multi-agent Pathfinding (MAPF) problem generally asks to find a set of
conflict-free paths for a set of agents confined to a graph and is typically
solved in a centralized fashion.
<br />Conversely, in this work, we investigate the decentralized MAPF setting, when
the central controller that posses all the information on the agents' locations
and goals is absent and the agents have to sequientially decide the actions on
their own without having access to a full state of the environment. We focus on
the practically important lifelong variant of MAPF, which involves continuously
assigning new goals to the agents upon arrival to the previous ones. To address
this complex problem, we propose a method that integrates two complementary
approaches: planning with heuristic search and reinforcement learning through
policy optimization. Planning is utilized to construct and re-plan individual
paths. We enhance our planning algorithm with a dedicated technique tailored to
avoid congestion and increase the throughput of the system. We employ
reinforcement learning to discover the collision avoidance policies that
effectively guide the agents along the paths. The policy is implemented as a
neural network and is effectively trained without any reward-shaping or
external guidance.
<br />We evaluate our method on a wide range of setups comparing it to the
state-of-the-art solvers. The results show that our method consistently
outperforms the learnable competitors, showing higher throughput and better
ability to generalize to the maps that were unseen at the training stage.
Moreover our solver outperforms a rule-based one in terms of throughput and is
an order of magnitude faster than a state-of-the-art search-based solver.
</p>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01208" title="Abstract">arXiv:2310.01208</a> [<a href="/pdf/2310.01208" title="Download PDF">pdf</a>, <a href="/format/2310.01208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label Supervised LLaMA Finetuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xianming Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuzhang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Haoran Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fu-lee Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+X">Xiaoqin Zhong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The recent success of Large Language Models (LLMs) has gained significant
attention in both academia and industry. Substantial efforts have been made to
enhance the zero- and few-shot generalization capabilities of open-source LLMs
through finetuning. Currently, the prevailing approach is instruction-tuning,
which trains LLMs to complete real-world tasks by generating responses guided
by natural language instructions. It is worth noticing that such an approach
may underperform in sequence and token classification tasks. Unlike text
generation tasks, classification tasks have a limited label space, where
precise label prediction is more appreciated than generating diverse and
human-like responses. Prior research has unveiled that instruction-tuned LLMs
cannot outperform BERT, prompting us to explore the potential of leveraging
latent representations from LLMs for supervised label prediction. In this
paper, we introduce a label-supervised adaptation for LLMs, which aims to
finetuning the model with discriminant labels. We evaluate this approach with
Label Supervised LLaMA (LS-LLaMA), based on LLaMA-2-7B, a relatively
small-scale LLM, and can be finetuned on a single GeForce RTX4090 GPU. We
extract latent representations from the final LLaMA layer and project them into
the label space to compute the cross-entropy loss. The model is finetuned by
Low-Rank Adaptation (LoRA) to minimize this loss. Remarkably, without intricate
prompt engineering or external knowledge, LS-LLaMA substantially outperforms
LLMs ten times its size in scale and demonstrates consistent improvements
compared to robust baselines like BERT-Large and RoBERTa-Large in text
classification. Moreover, by removing the causal mask from decoders, LS-unLLaMA
achieves the state-of-the-art performance in named entity recognition (NER).
Our work will shed light on a novel approach to adapting LLMs for various
downstream tasks.
</p>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01209" title="Abstract">arXiv:2310.01209</a> [<a href="/pdf/2310.01209" title="Download PDF">pdf</a>, <a href="/format/2310.01209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-distilled Masked Attention guided masked image modeling with noise  Regularized Teacher (SMART) for medical image analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jue Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Veeraraghavan%2C+H">Harini Veeraraghavan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper is under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Hierarchical shifted window transformers (Swin) are a computationally
efficient and more accurate alternative to plain vision transformers. Masked
image modeling (MIM)-based pretraining is highly effective in increasing
models' transferability to a variety of downstream tasks. However, more
accurate and efficient attention guided MIM approaches are difficult to
implement with Swin due to it's lack of an explicit global attention. We thus
architecturally enhanced Swin with semantic class attention for self-supervised
attention guided co-distillation with MIM. We also introduced a noise injected
momentum teacher, implemented with patch dropout of teacher's inputs for
improved training regularization and accuracy. Our approach, called
\underline{s}elf-distilled \underline{m}asked \underline{a}ttention MIM with
noise \underline{r}egularized \underline{t}eacher (SMART) was pretrained with
\textbf{10,412} unlabeled 3D computed tomography (CT)s of multiple disease
sites and sourced from institutional and public datasets. We evaluated SMART
for multiple downstream tasks involving analysis of 3D CTs of lung cancer (LC)
patients for: (i) [Task I] predicting immunotherapy response in advanced stage
LC (n = 200 internal dataset), (ii) [Task II] predicting LC recurrence in early
stage LC before surgery (n = 156 public dataset), (iii) [Task III] LC
segmentation (n = 200 internal, 21 public dataset), and (iv) [Task IV]
unsupervised clustering of organs in the chest and abdomen (n = 1,743 public
dataset) \underline{without} finetuning. SMART predicted immunotherapy response
with an AUC of 0.916, LC recurrence with an AUC of 0.793, segmented LC with
Dice accuracy of 0.81, and clustered organs with an inter-class cluster
distance of 5.94, indicating capability of attention guided MIM for Swin in
medical image analysis.
</p>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01211" title="Abstract">arXiv:2310.01211</a> [<a href="/pdf/2310.01211" title="Download PDF">pdf</a>, <a href="/format/2310.01211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Bricks to Bridges: Product of Invariances to Enhance Latent Space  Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cannistraci%2C+I">Irene Cannistraci</a>, 
<a href="/search/cs?searchtype=author&query=Moschella%2C+L">Luca Moschella</a>, 
<a href="/search/cs?searchtype=author&query=Fumero%2C+M">Marco Fumero</a>, 
<a href="/search/cs?searchtype=author&query=Maiorca%2C+V">Valentino Maiorca</a>, 
<a href="/search/cs?searchtype=author&query=Rodol%C3%A0%2C+E">Emanuele Rodol&#xe0;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 10 figures and 28 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">It has been observed that representations learned by distinct neural networks
conceal structural similarities when the models are trained under similar
inductive biases. From a geometric perspective, identifying the classes of
transformations and the related invariances that connect these representations
is fundamental to unlocking applications, such as merging, stitching, and
reusing different neural modules. However, estimating task-specific
transformations a priori can be challenging and expensive due to several
factors (e.g., weights initialization, training hyperparameters, or data
modality). To this end, we introduce a versatile method to directly incorporate
a set of invariances into the representations, constructing a product space of
invariant components on top of the latent representations without requiring
prior knowledge about the optimal invariance to infuse. We validate our
solution on classification and reconstruction tasks, observing consistent
latent similarity and downstream performance improvements in a zero-shot
stitching setting. The experimental analysis comprises three modalities
(vision, text, and graphs), twelve pretrained foundational models, eight
benchmarks, and several architectures trained from scratch.
</p>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01212" title="Abstract">arXiv:2310.01212</a> [<a href="/pdf/2310.01212" title="Download PDF">pdf</a>, <a href="/ps/2310.01212" title="Download PostScript">ps</a>, <a href="/format/2310.01212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling predictable parallelism in single-GPU systems with persistent  CUDA threads
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Burgio%2C+P">Paolo Burgio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, 2 figures, accepted at the 27th Euromicro Conference on Real-Time Systems as Work-in-Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Graphics Processing Unit, or GPUs, have been successfully adopted both for
graphic computation in 3D applications, and for general purpose application
(GP-GPUs), thank to their tremendous performance-per-watt. Recently, there is a
big interest in adopting them also within automotive and avionic industrial
settings, imposing for the first time real-time constraints on the design of
such devices. Unfortunately, it is extremely hard to extract timing guarantees
from modern GPU designs, and current approaches rely on a model where the GPU
is treated as a unique monolithic execution device. Unlike state-of-the-art of
research, we try to open the box of modern GPU architectures, providing a clean
way to exploit intra-GPU predictable execution.
</p>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01215" title="Abstract">arXiv:2310.01215</a> [<a href="/pdf/2310.01215" title="Download PDF">pdf</a>, <a href="/format/2310.01215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence proof for first-order position-based dynamics: An efficient  scheme for inequality constrained ODEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Plunder%2C+S">Steffen Plunder</a>, 
<a href="/search/math?searchtype=author&query=Merino-Aceituno%2C+S">Sara Merino-Aceituno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">NVIDIA researchers have pioneered an explicit method, position-based dynamics
(PBD), for simulating systems with contact forces, gaining widespread use in
computer graphics and animation. While the method yields visually compelling
real-time simulations with surprising numerical stability, its scientific
validity has been questioned due to a lack of rigorous analysis.
<br />In this paper, we introduce a new mathematical convergence analysis
specifically tailored for PBD applied to first-order dynamics. Utilizing newly
derived bounds for projections onto uniformly prox-regular sets, our proof
extends classical compactness arguments. Our work paves the way for the
reliable application of PBD in various scientific and engineering fields,
including particle simulations with volume exclusion, agent-based models in
mathematical biology or inequality-constrained gradient-flow models.
</p>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01217" title="Abstract">arXiv:2310.01217</a> [<a href="/pdf/2310.01217" title="Download PDF">pdf</a>, <a href="/format/2310.01217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ScaLearn: Simple and Highly Parameter-Efficient Task Transfer by  Learning to Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frohmann%2C+M">Markus Frohmann</a>, 
<a href="/search/cs?searchtype=author&query=Holtermann%2C+C">Carolin Holtermann</a>, 
<a href="/search/cs?searchtype=author&query=Masoudian%2C+S">Shahed Masoudian</a>, 
<a href="/search/cs?searchtype=author&query=Lauscher%2C+A">Anne Lauscher</a>, 
<a href="/search/cs?searchtype=author&query=Rekabsaz%2C+N">Navid Rekabsaz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Multi-task learning (MTL) has shown considerable practical benefits,
particularly when using pre-trained language models (PLMs). While this is
commonly achieved by simultaneously learning $n$ tasks under a joint
optimization procedure, recent methods such as AdapterFusion structure the
problem into two distinct stages: (i) task learning, where knowledge specific
to a task is encapsulated within sets of parameters (\eg adapters), and (ii)
transfer, where this already learned knowledge is leveraged for a target task.
This separation of concerns provides numerous benefits, such as promoting
reusability, and addressing cases involving data privacy and societal concerns;
on the flip side, current two-stage MTL methods come with the cost of
introducing a substantial number of additional parameters. In this work, we
address this issue by leveraging the usefulness of linearly scaling the output
representations of source adapters for transfer learning. We introduce
ScaLearn, a simple and highly parameter-efficient two-stage MTL method that
capitalizes on the knowledge of the source tasks by learning a minimal set of
scaling parameters that enable effective knowledge transfer to a target task.
Our experiments on three benchmarks (GLUE, SuperGLUE, and HumSet) show that our
ScaLearn, in addition to facilitating the benefits of two-stage MTL,
consistently outperforms strong baselines with only a small number of transfer
parameters - roughly 0.35% of those of AdapterFusion. Remarkably, we observe
that ScaLearn maintains its strong abilities even when further reducing
parameters through uniform scaling and layer-sharing, achieving similarly
competitive results with only $8$ transfer parameters for each target task. Our
proposed approach thus demonstrates the power of simple scaling as a promise
for more efficient task transfer.
</p>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01218" title="Abstract">arXiv:2310.01218</a> [<a href="/pdf/2310.01218" title="Download PDF">pdf</a>, <a href="/format/2310.01218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making LLaMA SEE and Draw with SEED Tokenizer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yuying Ge</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Sijie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Ziyun Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yixiao Ge</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chen Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project released at: <a href="https://github.com/AILab-CVC/SEED.">this https URL</a> arXiv admin note: substantial text overlap with <a href="/abs/2307.08041">arXiv:2307.08041</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The great success of Large Language Models (LLMs) has expanded the potential
of multimodality, contributing to the gradual evolution of General Artificial
Intelligence (AGI). A true AGI agent should not only possess the capability to
perform predefined multi-tasks but also exhibit emergent abilities in an
open-world context. However, despite the considerable advancements made by
recent multimodal LLMs, they still fall short in effectively unifying
comprehension and generation tasks, let alone open-world emergent abilities. We
contend that the key to overcoming the present impasse lies in enabling text
and images to be represented and processed interchangeably within a unified
autoregressive Transformer. To this end, we introduce SEED, an elaborate image
tokenizer that empowers LLMs with the ability to SEE and Draw at the same time.
We identify two crucial design principles: (1) Image tokens should be
independent of 2D physical patch positions and instead be produced with a 1D
causal dependency, exhibiting intrinsic interdependence that aligns with the
left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens
should capture high-level semantics consistent with the degree of semantic
abstraction in words, and be optimized for both discriminativeness and
reconstruction during the tokenizer training phase. With SEED tokens, LLM is
able to perform scalable multimodal autoregression under its original training
recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by
large-scale pretraining and instruction tuning on the interleaved textual and
visual data, demonstrating impressive performance on a broad range of
multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has
exhibited compositional emergent abilities such as multi-turn in-context
multimodal generation, acting like your AI assistant.
</p>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01220" title="Abstract">arXiv:2310.01220</a> [<a href="/pdf/2310.01220" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The benefits and costs of explainable artificial intelligence in visual  quality control: Evidence from fault detection performance and eye movements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+R">Romy M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Reindel%2C+D+F">David F. Reindel</a>, 
<a href="/search/cs?searchtype=author&query=Stadtfeld%2C+Y+D">Yannick D. Stadtfeld</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Visual inspection tasks often require humans to cooperate with AI-based image
classifiers. To enhance this cooperation, explainable artificial intelligence
(XAI) can highlight those image areas that have contributed to an AI decision.
However, the literature on visual cueing suggests that such XAI support might
come with costs of its own. To better understand how the benefits and cost of
XAI depend on the accuracy of AI classifications and XAI highlights, we
conducted two experiments that simulated visual quality control in a chocolate
factory. Participants had to decide whether chocolate moulds contained faulty
bars or not, and were always informed whether the AI had classified the mould
as faulty or not. In half of the experiment, they saw additional XAI highlights
that justified this classification. While XAI speeded up performance, its
effects on error rates were highly dependent on (X)AI accuracy. XAI benefits
were observed when the system correctly detected and highlighted the fault, but
XAI costs were evident for misplaced highlights that marked an intact area
while the actual fault was located elsewhere. Eye movement analyses indicated
that participants spent less time searching the rest of the mould and thus
looked at the fault less often. However, we also observed large interindividual
differences. Taken together, the results suggest that despite its potentials,
XAI can discourage people from investing effort into their own information
analysis.
</p>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01224" title="Abstract">arXiv:2310.01224</a> [<a href="/pdf/2310.01224" title="Download PDF">pdf</a>, <a href="/format/2310.01224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Mobility Modeling with Graph: A Graph Transformer Model for  Next Point-of-Interest Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaohang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Suzumura%2C+T">Toyotaro Suzumura</a>, 
<a href="/search/cs?searchtype=author&query=Yong%2C+J">Jiawei Yong</a>, 
<a href="/search/cs?searchtype=author&query=Hanai%2C+M">Masatoshi Hanai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chuang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kanezashi%2C+H">Hiroki Kanezashi</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+R">Renhe Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Fukushima%2C+S">Shintaro Fukushima</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a full paper of SIGSPATIAL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Next Point-of-Interest (POI) recommendation plays a crucial role in urban
mobility applications. Recently, POI recommendation models based on Graph
Neural Networks (GNN) have been extensively studied and achieved, however, the
effective incorporation of both spatial and temporal information into such
GNN-based models remains challenging. Extracting distinct fine-grained features
unique to each piece of information is difficult since temporal information
often includes spatial information, as users tend to visit nearby POIs. To
address the challenge, we propose \textbf{\underline{Mob}}ility
\textbf{\underline{G}}raph \textbf{\underline{T}}ransformer (MobGT) that
enables us to fully leverage graphs to capture both the spatial and temporal
features in users' mobility patterns. MobGT combines individual spatial and
temporal graph encoders to capture unique features and global user-location
relations. Additionally, it incorporates a mobility encoder based on Graph
Transformer to extract higher-order information between POIs. To address the
long-tailed problem in spatial-temporal data, MobGT introduces a novel loss
function, Tail Loss. Experimental results demonstrate that MobGT outperforms
state-of-the-art models on various datasets and metrics, achieving 24\%
improvement on average. Our codes are available at
\url{https://github.com/Yukayo/MobGT}.
</p>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01228" title="Abstract">arXiv:2310.01228</a> [<a href="/pdf/2310.01228" title="Download PDF">pdf</a>, <a href="/format/2310.01228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstructing 3D Human Pose from RGB-D Data with Occlusions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dang%2C+B">Bowen Dang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bowen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">He Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">We propose a new method to reconstruct the 3D human body from RGB-D images
with occlusions. The foremost challenge is the incompleteness of the RGB-D data
due to occlusions between the body and the environment, leading to implausible
reconstructions that suffer from severe human-scene penetration. To reconstruct
a semantically and physically plausible human body, we propose to reduce the
solution space based on scene information and prior knowledge. Our key idea is
to constrain the solution space of the human body by considering the occluded
body parts and visible body parts separately: modeling all plausible poses
where the occluded body parts do not penetrate the scene, and constraining the
visible body parts using depth data. Specifically, the first component is
realized by a neural network that estimates the candidate region named the
"free zone", a region carved out of the open space within which it is safe to
search for poses of the invisible body parts without concern for penetration.
The second component constrains the visible body parts using the "truncated
shadow volume" of the scanned body point cloud. Furthermore, we propose to use
a volume matching strategy, which yields better performance than surface
matching, to match the human body with the confined region. We conducted
experiments on the PROX dataset, and the results demonstrate that our method
produces more accurate and plausible results compared with other methods.
</p>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01230" title="Abstract">arXiv:2310.01230</a> [<a href="/pdf/2310.01230" title="Download PDF">pdf</a>, <a href="/format/2310.01230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vehicle Fuel Consumption Virtual Sensing from GNSS and IMU Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cellina%2C+M">Marcello Cellina</a>, 
<a href="/search/cs?searchtype=author&query=Strada%2C+S">Silvia Strada</a>, 
<a href="/search/cs?searchtype=author&query=Savaresi%2C+S+M">Sergio Matteo Savaresi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper presents a vehicle-independent, non-intrusive, and light
monitoring system for accurately measuring fuel consumption in road vehicles
from longitudinal speed and acceleration derived continuously in time from GNSS
and IMU sensors mounted inside the vehicle. In parallel to boosting the
transition to zero-carbon cars, there is an increasing interest in low-cost
instruments for precise measurement of the environmental impact of the many
internal combustion engine vehicles still in circulation. The main contribution
of this work is the design and comparison of two innovative black-box
algorithms, one based on a reduced complexity physics modeling while the other
relying on a feedforward neural network for black-box fuel consumption
estimation using only velocity and acceleration measurements. Based on suitable
metrics, the developed algorithms outperform the state of the art best
approach, both in the instantaneous and in the integral fuel consumption
estimation, with errors smaller than 1\% with respect to the fuel flow ground
truth. The data used for model identification, testing, and experimental
validation is composed of GNSS velocity and IMU acceleration measurements
collected during several trips using a diesel fuel vehicle on different roads,
in different seasons, and with varying numbers of passengers. Compared to
built-in vehicle monitoring systems, this methodology is not customized, uses
off-the-shelf sensors, and is based on two simple algorithms that have been
validated offline and could be easily implemented in a real-time environment.
</p>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01232" title="Abstract">arXiv:2310.01232</a> [<a href="/pdf/2310.01232" title="Download PDF">pdf</a>, <a href="/format/2310.01232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modality-aware Transformer for Time series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Emami%2C+H">Hajar Emami</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+X">Xuan-Hong Dang</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+Y">Yousaf Shah</a>, 
<a href="/search/cs?searchtype=author&query=Zerfos%2C+P">Petros Zerfos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Time series forecasting presents a significant challenge, particularly when
its accuracy relies on external data sources rather than solely on historical
values. This issue is prevalent in the financial sector, where the future
behavior of time series is often intricately linked to information derived from
various textual reports and a multitude of economic indicators. In practice,
the key challenge lies in constructing a reliable time series forecasting model
capable of harnessing data from diverse sources and extracting valuable
insights to predict the target time series accurately. In this work, we tackle
this challenging problem and introduce a novel multimodal transformer-based
model named the Modality-aware Transformer. Our model excels in exploring the
power of both categorical text and numerical timeseries to forecast the target
time series effectively while providing insights through its neural attention
mechanism. To achieve this, we develop feature-level attention layers that
encourage the model to focus on the most relevant features within each data
modality. By incorporating the proposed feature-level attention, we develop a
novel Intra-modal multi-head attention (MHA), Inter-modal MHA and
Modality-target MHA in a way that both feature and temporal attentions are
incorporated in MHAs. This enables the MHAs to generate temporal attentions
with consideration of modality and feature importance which leads to more
informative embeddings. The proposed modality-aware structure enables the model
to effectively exploit information within each modality as well as foster
cross-modal understanding. Our extensive experiments on financial datasets
demonstrate that Modality-aware Transformer outperforms existing methods,
offering a novel and practical solution to the complex challenges of
multi-modality time series forecasting.
</p>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01235" title="Abstract">arXiv:2310.01235</a> [<a href="/pdf/2310.01235" title="Download PDF">pdf</a>, <a href="/format/2310.01235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COIN-LIO: Complementary Intensity-Augmented LiDAR Inertial Odometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pfreundschuh%2C+P">Patrick Pfreundschuh</a>, 
<a href="/search/cs?searchtype=author&query=Oleynikova%2C+H">Helen Oleynikova</a>, 
<a href="/search/cs?searchtype=author&query=Cadena%2C+C">Cesar Cadena</a>, 
<a href="/search/cs?searchtype=author&query=Siegwart%2C+R">Roland Siegwart</a>, 
<a href="/search/cs?searchtype=author&query=Andersson%2C+O">Olov Andersson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We present COIN-LIO, a LiDAR Inertial Odometry pipeline that tightly couples
information from LiDAR intensity with geometry-based point cloud registration.
The focus of our work is to improve the robustness of LiDAR-inertial odometry
in geometrically degenerate scenarios, like tunnels or flat fields. We project
LiDAR intensity returns into an intensity image, and propose an image
processing pipeline that produces filtered images with improved brightness
consistency within the image as well as across different scenes. To effectively
leverage intensity as an additional modality, we present a novel feature
selection scheme that detects uninformative directions in the point cloud
registration and explicitly selects patches with complementary image
information. Photometric error minimization in the image patches is then fused
with inertial measurements and point-to-plane registration in an iterated
Extended Kalman Filter. The proposed approach improves accuracy and robustness
on a public dataset. We additionally publish a new dataset, that captures five
real-world environments in challenging, geometrically degenerate scenes. By
using the additional photometric information, our approach shows drastically
improved robustness against geometric degeneracy in environments where all
compared baseline approaches fail.
</p>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01237" title="Abstract">arXiv:2310.01237</a> [<a href="/pdf/2310.01237" title="Download PDF">pdf</a>, <a href="/format/2310.01237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fitted finite element methods for singularly perturbed elliptic problems  of convection-diffusion type
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hegarty%2C+A+F">Alan F. Hegarty</a>, 
<a href="/search/math?searchtype=author&query=O%27Riordan%2C+E">Eugene O&#x27;Riordan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Fitted finite element methods are constructed for a singularly perturbed
convection-diffusion problem in two space dimensions. Exponential splines as
basis functions are combined with Shishkin meshes to obtain a stable
parameter-uniform numerical method. These schemes satisfy a discrete maximum
principle. In the classical case, the numerical approximations converge, in the
maximum pointwise norm, at a rate of second order and the approximations
converge at a rate of first order for all values of the singular perturbation
parameter.
</p>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01247" title="Abstract">arXiv:2310.01247</a> [<a href="/pdf/2310.01247" title="Download PDF">pdf</a>, <a href="/format/2310.01247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised Learning for Anomaly Detection in Computational  Workflows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Hongwei Jin</a>, 
<a href="/search/cs?searchtype=author&query=Raghavan%2C+K">Krishnan Raghavan</a>, 
<a href="/search/cs?searchtype=author&query=Papadimitriou%2C+G">George Papadimitriou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mandal%2C+A">Anirban Mandal</a>, 
<a href="/search/cs?searchtype=author&query=Deelman%2C+E">Ewa Deelman</a>, 
<a href="/search/cs?searchtype=author&query=Balaprakash%2C+P">Prasanna Balaprakash</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Anomaly detection is the task of identifying abnormal behavior of a system.
Anomaly detection in computational workflows is of special interest because of
its wide implications in various domains such as cybersecurity, finance, and
social networks. However, anomaly detection in computational workflows~(often
modeled as graphs) is a relatively unexplored problem and poses distinct
challenges. For instance, when anomaly detection is performed on graph data,
the complex interdependency of nodes and edges, the heterogeneity of node
attributes, and edge types must be accounted for. Although the use of graph
neural networks can help capture complex inter-dependencies, the scarcity of
labeled anomalous examples from workflow executions is still a significant
challenge. To address this problem, we introduce an autoencoder-driven
self-supervised learning~(SSL) approach that learns a summary statistic from
unlabeled workflow data and estimates the normal behavior of the computational
workflow in the latent space. In this approach, we combine generative and
contrastive learning objectives to detect outliers in the summary statistics.
We demonstrate that by estimating the distribution of normal behavior in the
latent space, we can outperform state-of-the-art anomaly detection methods on
our benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01248" title="Abstract">arXiv:2310.01248</a> [<a href="/pdf/2310.01248" title="Download PDF">pdf</a>, <a href="/format/2310.01248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Emotional Expression and Cohesion in Image-Based Playlist  Description and Music Topics: A Continuous Parameterization Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+Y">Yuelyu Ji</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yuheng Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruoyi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhongqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huiyun Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Text generation in image-based platforms, particularly for music-related
content, requires precise control over text styles and the incorporation of
emotional expression. However, existing approaches often need help to control
the proportion of external factors in generated text and rely on discrete
inputs, lacking continuous control conditions for desired text generation. This
study proposes Continuous Parameterization for Controlled Text Generation
(CPCTG) to overcome these limitations. Our approach leverages a Language Model
(LM) as a style learner, integrating Semantic Cohesion (SC) and Emotional
Expression Proportion (EEP) considerations. By enhancing the reward method and
manipulating the CPCTG level, our experiments on playlist description and music
topic generation tasks demonstrate significant improvements in ROUGE scores,
indicating enhanced relevance and coherence in the generated text.
</p>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01251" title="Abstract">arXiv:2310.01251</a> [<a href="/pdf/2310.01251" title="Download PDF">pdf</a>, <a href="/format/2310.01251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating 3D Brain Tumor Regions in MRI using Vector-Quantization  Generative Adversarial Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Meng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+M+W">Matthias W Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Tabori%2C+U">Uri Tabori</a>, 
<a href="/search/cs?searchtype=author&query=Hawkins%2C+C">Cynthia Hawkins</a>, 
<a href="/search/cs?searchtype=author&query=Ertl-Wagner%2C+B+B">Birgit B Ertl-Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Khalvati%2C+F">Farzad Khalvati</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, In Submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Medical image analysis has significantly benefited from advancements in deep
learning, particularly in the application of Generative Adversarial Networks
(GANs) for generating realistic and diverse images that can augment training
datasets. However, the effectiveness of such approaches is often limited by the
amount of available data in clinical settings. Additionally, the common
GAN-based approach is to generate entire image volumes, rather than solely the
region of interest (ROI). Research on deep learning-based brain tumor
classification using MRI has shown that it is easier to classify the tumor ROIs
compared to the entire image volumes. In this work, we present a novel
framework that uses vector-quantization GAN and a transformer incorporating
masked token modeling to generate high-resolution and diverse 3D brain tumor
ROIs that can be directly used as augmented data for the classification of
brain tumor ROI. We apply our method to two imbalanced datasets where we
augment the minority class: (1) the Multimodal Brain Tumor Segmentation
Challenge (BraTS) 2019 dataset to generate new low-grade glioma (LGG) ROIs to
balance with high-grade glioma (HGG) class; (2) the internal pediatric LGG
(pLGG) dataset tumor ROIs with BRAF V600E Mutation genetic marker to balance
with BRAF Fusion genetic marker class. We show that the proposed method
outperforms various baseline models in both qualitative and quantitative
measurements. The generated data was used to balance the data in the brain
tumor types classification task. Using the augmented data, our approach
surpasses baseline models by 6.4% in AUC on the BraTS 2019 dataset and 4.3% in
AUC on our internal pLGG dataset. The results indicate the generated tumor ROIs
can effectively address the imbalanced data problem. Our proposed method has
the potential to facilitate an accurate diagnosis of rare brain tumors using
MRI scans.
</p>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01252" title="Abstract">arXiv:2310.01252</a> [<a href="/pdf/2310.01252" title="Download PDF">pdf</a>, <a href="/format/2310.01252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-training Contextual Location Embeddings in Personal Trajectories via  Efficient Hierarchical Location Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chung Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taesan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Junui Hong</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+M">Minsung Choi</a>, 
<a href="/search/cs?searchtype=author&query=Choo%2C+J">Jaegul Choo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Pre-training the embedding of a location generated from human mobility data
has become a popular method for location based services. In practice, modeling
the location embedding is too expensive, due to the large number of locations
to be trained in situations with fine-grained resolution or extensive target
regions. Previous studies have handled less than ten thousand distinct
locations, which is insufficient in the real-world applications. To tackle this
problem, we propose a Geo-Tokenizer, designed to efficiently reduce the number
of locations to be trained by representing a location as a combination of
several grids at different scales. In the Geo-Tokenizer, a grid at a larger
scale shares the common set of grids at smaller scales, which is a key factor
in reducing the size of the location vocabulary. The sequences of locations
preprocessed with the Geo-Tokenizer are utilized by a causal location embedding
model to capture the temporal dependencies of locations. This model dynamically
calculates the embedding vector of a target location, which varies depending on
its trajectory. In addition, to efficiently pre-train the location embedding
model, we propose the Hierarchical Auto-regressive Location Model objective to
effectively train decomposed locations in the Geo-Tokenizer. We conducted
experiments on two real-world user trajectory datasets using our pre-trained
location model. The experimental results show that our model significantly
improves the performance of downstream tasks with fewer model parameters
compared to existing location embedding methods.
</p>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01254" title="Abstract">arXiv:2310.01254</a> [<a href="/pdf/2310.01254" title="Download PDF">pdf</a>, <a href="/format/2310.01254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Containment for Binary Guarded Monotone SNP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barsukov%2C+A">Alexey Barsukov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Databases (cs.DB); Logic (math.LO)

</div>
<p class="mathjax">Guarded Monotone Strict NP (GMSNP) extends the class of Monotone Monadic
Strict NP (MMSNP) by allowing existentially quantified relations of arities
greater than 1 but restricting them to always be guarded by input relations.
The containment problem is characterized for MMSNP by the existence of a
recoloring which is a mapping between the sets of second-order variables of the
two given logical sentences that satisfies some specific properties. This paper
extends this characterization to GMSNP problems, where the input signature
consists of unary and binary relation symbols.
</p>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01255" title="Abstract">arXiv:2310.01255</a> [<a href="/pdf/2310.01255" title="Download PDF">pdf</a>, <a href="/format/2310.01255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Dynamics-Chemistry Coupling Across Different Meshes in  LFRic-Atmosphere: Formulation and Idealised Tests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Brown%2C+A">Alex Brown</a>, 
<a href="/search/math?searchtype=author&query=Bendall%2C+T+M">Thomas M. Bendall</a>, 
<a href="/search/math?searchtype=author&query=Boutle%2C+I">Ian Boutle</a>, 
<a href="/search/math?searchtype=author&query=Melvin%2C+T">Thomas Melvin</a>, 
<a href="/search/math?searchtype=author&query=Shipway%2C+B">Ben Shipway</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">The main components of an atmospheric model for numerical weather prediction
are the dynamical core, which describes the resolved flow, and the physical
parametrisations, which capture the effects of unresolved processes.
Additionally, models used for air quality or climate applications may include a
component that represents the evolution of chemicals and aerosols within the
atmosphere. While traditionally all these components use the same mesh with the
same resolution, we present a formulation for the different components to use a
series of nested meshes, with different horizontal resolutions. This gives the
model greater flexibility in the allocation of computational resources, so that
resolution can be targeted to those parts which provide the greatest benefits
in accuracy.
<br />The formulation presented here concerns the methods for mapping fields
between meshes, and is designed for the compatible finite element
discretisation used by LFRic-Atmosphere, the Met Office's next-generation
atmosphere model. Key properties of the formulation include the consistent and
conservative transport of tracers on a mesh that is coarser than the dynamical
core, and the handling of moisture to ensure mass conservation without
generation of unphysical negative values. Having presented the formulation, it
is then demonstrated through a series of idealised test cases which show the
feasibility of this approach.
</p>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01259" title="Abstract">arXiv:2310.01259</a> [<a href="/pdf/2310.01259" title="Download PDF">pdf</a>, <a href="/format/2310.01259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster and Accurate Neural Networks with Semantic Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sayyed%2C+S">Sazzad Sayyed</a>, 
<a href="/search/cs?searchtype=author&query=Ashdown%2C+J">Jonathan Ashdown</a>, 
<a href="/search/cs?searchtype=author&query=Restuccia%2C+F">Francesco Restuccia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures, conference format
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep neural networks (DNN) usually come with a significant computational
burden. While approaches such as structured pruning and mobile-specific DNNs
have been proposed, they incur drastic accuracy loss. In this paper we leverage
the intrinsic redundancy in latent representations to reduce the computational
load with limited loss in performance. We show that semantically similar inputs
share many filters, especially in the earlier layers. Thus, semantically
similar classes can be clustered to create cluster-specific subgraphs. To this
end, we propose a new framework called Semantic Inference (SINF). In short,
SINF (i) identifies the semantic cluster the object belongs to using a small
additional classifier and (ii) executes the subgraph extracted from the base
DNN related to that semantic cluster for inference. To extract each
cluster-specific subgraph, we propose a new approach named Discriminative
Capability Score (DCS) that finds the subgraph with the capability to
discriminate among the members of a specific semantic cluster. DCS is
independent from SINF and can be applied to any DNN. We benchmark the
performance of DCS on the VGG16, VGG19, and ResNet50 DNNs trained on the
CIFAR100 dataset against 6 state-of-the-art pruning approaches. Our results
show that (i) SINF reduces the inference time of VGG19, VGG16, and ResNet50
respectively by up to 35%, 29% and 15% with only 0.17%, 3.75%, and 6.75%
accuracy loss (ii) DCS achieves respectively up to 3.65%, 4.25%, and 2.36%
better accuracy with VGG16, VGG19, and ResNet50 with respect to existing
discriminative scores (iii) when used as a pruning criterion, DCS achieves up
to 8.13% accuracy gain with 5.82% less parameters than the existing state of
the art work published at ICLR 2023 (iv) when considering per-cluster accuracy,
SINF performs on average 5.73%, 8.38% and 6.36% better than the base VGG16,
VGG19, and ResNet50.
</p>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01260" title="Abstract">arXiv:2310.01260</a> [<a href="/pdf/2310.01260" title="Download PDF">pdf</a>, <a href="/format/2310.01260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPELL: Semantic Prompt Evolution based on a LLM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y+B">Yujian Betterest Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kai Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Prompt engineering is a new paradigm for enhancing the performance of trained
neural network models. For optimizing text-style prompts, existing methods
usually individually operate small portions of a text step by step, which
either breaks the fluency or could not globally adjust a prompt. Since large
language models (LLMs) have powerful ability of generating coherent texts token
by token, can we utilize LLMs for improving prompts? Based on this motivation,
in this paper, considering a trained LLM as a text generator, we attempt to
design a black-box evolution algorithm for automatically optimizing texts,
namely SPELL (Semantic Prompt Evolution based on a LLM). The proposed method is
evaluated with different LLMs and evolution parameters in different text tasks.
Experimental results show that SPELL could rapidly improve the prompts indeed.
We further explore the evolution process and discuss on the limitations,
potential possibilities and future work.
</p>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01262" title="Abstract">arXiv:2310.01262</a> [<a href="/pdf/2310.01262" title="Download PDF">pdf</a>, <a href="/format/2310.01262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Exchangeable Conformal Risk Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farinhas%2C+A">Ant&#xf3;nio Farinhas</a>, 
<a href="/search/cs?searchtype=author&query=Zerva%2C+C">Chrysoula Zerva</a>, 
<a href="/search/cs?searchtype=author&query=Ulmer%2C+D">Dennis Ulmer</a>, 
<a href="/search/cs?searchtype=author&query=Martins%2C+A+F+T">Andr&#xe9; F. T. Martins</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Split conformal prediction has recently sparked great interest due to its
ability to provide formally guaranteed uncertainty sets or intervals for
predictions made by black-box neural models, ensuring a predefined probability
of containing the actual ground truth. While the original formulation assumes
data exchangeability, some extensions handle non-exchangeable data, which is
often the case in many real-world scenarios. In parallel, some progress has
been made in conformal methods that provide statistical guarantees for a
broader range of objectives, such as bounding the best F1-score or minimizing
the false negative rate in expectation. In this paper, we leverage and extend
these two lines of work by proposing non-exchangeable conformal risk control,
which allows controlling the expected value of any monotone loss function when
the data is not exchangeable. Our framework is flexible, makes very few
assumptions, and allows weighting the data based on its statistical similarity
with the test examples; a careful choice of weights may result on tighter
bounds, making our framework useful in the presence of change points, time
series, or other forms of distribution drift. Experiments with both synthetic
and real world data show the usefulness of our method.
</p>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01264" title="Abstract">arXiv:2310.01264</a> [<a href="/pdf/2310.01264" title="Download PDF">pdf</a>, <a href="/ps/2310.01264" title="Download PostScript">ps</a>, <a href="/format/2310.01264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cell-Free Bistatic Backscatter Communication: Channel Estimation,  Optimization, and Performance Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galappaththige%2C+D">Diluka Galappaththige</a>, 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+F">Fatemeh Rezaei</a>, 
<a href="/search/cs?searchtype=author&query=Tellambura%2C+C">Chintha Tellambura</a>, 
<a href="/search/cs?searchtype=author&query=Maaref%2C+A">Amine Maaref</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures, journal article
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This study introduces and investigates the integration of a cell-free
architecture with bistatic backscatter communication (BiBC), referred to as
cell-free BiBC or distributed access point (AP)-assisted BiBC, which can enable
potential applications in future (EH)-based Internet-of-Things (IoT) networks.
To that purpose, we first present a pilot-based channel estimation scheme for
estimating the direct, cascaded, forward channels of the proposed system setup.
We next utilize the channel estimates for designing the optimal beamforming
weights at the APs, reflection coefficients at the tags, and reception filters
at the reader to maximize the tag sum rate while meeting the tags' minimum
energy requirements. Because the proposed maximization problem is non-convex,
we propose a solution based on alternative optimization, fractional
programming, and Rayleigh quotient techniques. We also quantify the
computational complexity of the developed algorithms. Finally, we present
extensive numerical results to validate the proposed channel estimation scheme
and optimization framework, as well as the performance of the integration of
these two technologies. Compared to the random beamforming/combining benchmark,
our algorithm yields impressive gains. For example, it achieves $\sim$ 64.8\%
and $\sim$ 253.5\% gains in harvested power and tag sum rate, respectively, for
10 dBm with 36 APs and 3 tags.
</p>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01267" title="Abstract">arXiv:2310.01267</a> [<a href="/pdf/2310.01267" title="Download PDF">pdf</a>, <a href="/format/2310.01267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cooperative Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Finkelshtein%2C+B">Ben Finkelshtein</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xingyue Huang</a>, 
<a href="/search/cs?searchtype=author&query=Bronstein%2C+M">Michael Bronstein</a>, 
<a href="/search/cs?searchtype=author&query=Ceylan%2C+%C4%B0+%C4%B0">&#x130;smail &#x130;lkan Ceylan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph neural networks are popular architectures for graph machine learning,
based on iterative computation of node representations of an input graph
through a series of invariant transformations. A large class of graph neural
networks follow a standard message-passing paradigm: at every layer, each node
state is updated based on an aggregate of messages from its neighborhood. In
this work, we propose a novel framework for training graph neural networks,
where every node is viewed as a player that can choose to either 'listen',
'broadcast', 'listen and broadcast', or to 'isolate'. The standard message
propagation scheme can then be viewed as a special case of this framework where
every node 'listens and broadcasts' to all neighbors. Our approach offers a
more flexible and dynamic message-passing paradigm, where each node can
determine its own strategy based on their state, effectively exploring the
graph topology while learning. We provide a theoretical analysis of the new
message-passing scheme which is further supported by an extensive empirical
analysis on a synthetic dataset and on real-world datasets.
</p>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01271" title="Abstract">arXiv:2310.01271</a> [<a href="/pdf/2310.01271" title="Download PDF">pdf</a>, <a href="/format/2310.01271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEEC: A Legal Element Extraction Dataset with an Extensive  Domain-Specific Label System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zongyue%2C+X">Xue Zongyue</a>, 
<a href="/search/cs?searchtype=author&query=Huanghai%2C+L">Liu Huanghai</a>, 
<a href="/search/cs?searchtype=author&query=Yiran%2C+H">Hu Yiran</a>, 
<a href="/search/cs?searchtype=author&query=Kangle%2C+K">Kong Kangle</a>, 
<a href="/search/cs?searchtype=author&query=Chenlu%2C+W">Wang Chenlu</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+L">Liu Yun</a>, 
<a href="/search/cs?searchtype=author&query=Weixing%2C+S">Shen Weixing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">As a pivotal task in natural language processing, element extraction has
gained significance in the legal domain. Extracting legal elements from
judicial documents helps enhance interpretative and analytical capacities of
legal cases, and thereby facilitating a wide array of downstream applications
in various domains of law. Yet existing element extraction datasets are limited
by their restricted access to legal knowledge and insufficient coverage of
labels. To address this shortfall, we introduce a more comprehensive,
large-scale criminal element extraction dataset, comprising 15,831 judicial
documents and 159 labels. This dataset was constructed through two main steps:
First, designing the label system by our team of legal experts based on prior
legal research which identified critical factors driving and processes
generating sentencing outcomes in criminal cases; Second, employing the legal
knowledge to annotate judicial documents according to the label system and
annotation guideline. The Legal Element ExtraCtion dataset (LEEC) represents
the most extensive and domain-specific legal element extraction dataset for the
Chinese legal system. Leveraging the annotated data, we employed various SOTA
models that validates the applicability of LEEC for Document Event Extraction
(DEE) task. The LEEC dataset is available on https://github.com/THUlawtech/LEEC .
</p>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01272" title="Abstract">arXiv:2310.01272</a> [<a href="/pdf/2310.01272" title="Download PDF">pdf</a>, <a href="/format/2310.01272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified View on Neural Message Passing with Opinion Dynamics for  Social Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+O">Outongyi Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bingxin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xiang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weishu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lirong Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Social networks represent a common form of interconnected data frequently
depicted as graphs within the domain of deep learning-based inference. These
communities inherently form dynamic systems, achieving stability through
continuous internal communications and opinion exchanges among social actors
along their social ties. In contrast, neural message passing in deep learning
provides a clear and intuitive mathematical framework for understanding
information propagation and aggregation among connected nodes in graphs. Node
representations are dynamically updated by considering both the connectivity
and status of neighboring nodes. This research harmonizes concepts from
sociometry and neural message passing to analyze and infer the behavior of
dynamic systems. Drawing inspiration from opinion dynamics in sociology, we
propose ODNet, a novel message passing scheme incorporating bounded confidence,
to refine the influence weight of local nodes for message propagation. We
adjust the similarity cutoffs of bounded confidence and influence weights of
ODNet and define opinion exchange rules that align with the characteristics of
social network graphs. We show that ODNet enhances prediction performance
across various graph types and alleviates oversmoothing issues. Furthermore,
our approach surpasses conventional baselines in graph representation learning
and proves its practical significance in analyzing real-world co-occurrence
networks of metabolic genes. Remarkably, our method simplifies complex social
network graphs solely by leveraging knowledge of interaction frequencies among
entities within the system. It accurately identifies internal communities and
the roles of genes in different metabolic pathways, including opinion leaders,
bridge communicators, and isolators.
</p>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01273" title="Abstract">arXiv:2310.01273</a> [<a href="/pdf/2310.01273" title="Download PDF">pdf</a>, <a href="/format/2310.01273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning manipulation of steep granular slopes for fast Mini Rover  turning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kerimoglu%2C+D">Deniz Kerimoglu</a>, 
<a href="/search/cs?searchtype=author&query=Soto%2C+D">Daniel Soto</a>, 
<a href="/search/cs?searchtype=author&query=Hemsley%2C+M+L">Malone Lincoln Hemsley</a>, 
<a href="/search/cs?searchtype=author&query=Brunner%2C+J">Joseph Brunner</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+S">Sehoon Ha</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tingnan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Goldman%2C+D+I">Daniel I. Goldman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, conference paper submission for ICRA2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Future planetary exploration missions will require reaching challenging
regions such as craters and steep slopes. Such regions are ubiquitous and
present science-rich targets potentially containing information regarding the
planet's internal structure. Steep slopes consisting of low-cohesion regolith
are prone to flow downward under small disturbances, making it very challenging
for autonomous rovers to traverse. Moreover, the navigation trajectories of
rovers are heavily limited by the terrain topology and future systems will need
to maneuver on flowable surfaces without getting trapped, allowing them to
further expand their reach and increase mission efficiency.
<br />In this work, we used a laboratory-scale rover robot and performed
maneuvering experiments on a steep granular slope of poppy seeds to explore the
rover's turning capabilities. The rover is capable of lifting, sweeping, and
spinning its wheels, allowing it to execute leg-like gait patterns. The
high-dimensional actuation capabilities of the rover facilitate effective
manipulation of the underlying granular surface. We used Bayesian Optimization
(BO) to gain insight into successful turning gaits in high dimensional search
space and found strategies such as differential wheel spinning and pivoting
around a single sweeping wheel. We then used these insights to further
fine-tune the turning gait, enabling the rover to turn 90 degrees at just above
4 seconds with minimal slip. Combining gait optimization and human-tuning
approaches, we found that fast turning is empowered by creating anisotropic
torques with the sweeping wheel.
</p>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01278" title="Abstract">arXiv:2310.01278</a> [<a href="/pdf/2310.01278" title="Download PDF">pdf</a>, <a href="/format/2310.01278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open and Linked Data Model for Carbon Footprint Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruf%2C+B">Boris Ruf</a>, 
<a href="/search/cs?searchtype=author&query=Detyniecki%2C+M">Marcin Detyniecki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at International Conference on Renewable Energy and Conservation (ICREC) 2023 and accepted for publication at International Conference on Advanced Infocomm Technology (ICAIT) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Carbon footprint quantification is key to well-informed decision making over
carbon reduction potential, both for individuals and for companies. Many carbon
footprint case studies for products and services have been circulated recently.
Due to the complex relationships within each scenario, however, the underlying
assumptions often are difficult to understand. Also, re-using and adapting a
scenario to local or individual circumstances is not a straightforward task. To
overcome these challenges, we propose an open and linked data model for carbon
footprint scenarios which improves data quality and transparency by design. We
demonstrate the implementation of our idea with a web-based data interpreter
prototype.
</p>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01282" title="Abstract">arXiv:2310.01282</a> [<a href="/pdf/2310.01282" title="Download PDF">pdf</a>, <a href="/format/2310.01282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grasping AI: experiential exercises for designers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Murray-Rust%2C+D">Dave Murray-Rust</a>, 
<a href="/search/cs?searchtype=author&query=Lupetti%2C+M+L">Maria Luce Lupetti</a>, 
<a href="/search/cs?searchtype=author&query=Nicenboim%2C+I">Iohanna Nicenboim</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Hoog%2C+W">Wouter van der Hoog</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Artificial intelligence (AI) and machine learning (ML) are increasingly
integrated into the functioning of physical and digital products, creating
unprecedented opportunities for interaction and functionality. However, there
is a challenge for designers to ideate within this creative landscape,
balancing the possibilities of technology with human interactional concerns. We
investigate techniques for exploring and reflecting on the interactional
affordances, the unique relational possibilities, and the wider social
implications of AI systems. We introduced into an interaction design course
(n=100) nine 'AI exercises' that draw on more than human design, responsible
AI, and speculative enactment to create experiential engagements around AI
interaction design. We find that exercises around metaphors and enactments make
questions of training and learning, privacy and consent, autonomy and agency
more tangible, and thereby help students be more reflective and responsible on
how to design with AI and its complex properties in both their design process
and outcomes.
</p>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01283" title="Abstract">arXiv:2310.01283</a> [<a href="/pdf/2310.01283" title="Download PDF">pdf</a>, <a href="/format/2310.01283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The influence of coordinated behavior on toxicity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Loru%2C+E">Edoardo Loru</a>, 
<a href="/search/cs?searchtype=author&query=Cinelli%2C+M">Matteo Cinelli</a>, 
<a href="/search/cs?searchtype=author&query=Tesconi%2C+M">Maurizio Tesconi</a>, 
<a href="/search/cs?searchtype=author&query=Quattrociocchi%2C+W">Walter Quattrociocchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">In the intricate landscape of social media genuine content dissemination may
be altered by a number of threats. Coordinated Behavior (CB), defined as
orchestrated efforts by entities to deceive or mislead users about their
identity and intentions, emerges as a tactic to exploit or manipulate online
discourse. This study delves into the relationship between CB and toxic
conversation on Twitter. Using a dataset of 11 million tweets from 1 million
users preceding the 2019 UK General Elections, we show that users displaying CB
typically disseminate less harmful content, irrespective of political
affiliation. However, distinct toxicity patterns emerge among different CB
cohorts. Compared to their non-CB counterparts, CB participants show marginally
elevated toxicity levels only when considering their original posts. We further
show the effects of CB-driven toxic content on non-CB users, gauging its impact
based on political leanings. Our findings suggest a nuanced but statistically
significant influence of CB on digital discourse.
</p>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01286" title="Abstract">arXiv:2310.01286</a> [<a href="/pdf/2310.01286" title="Download PDF">pdf</a>, <a href="/ps/2310.01286" title="Download PostScript">ps</a>, <a href="/format/2310.01286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Dynamic Macroscopic Framework for Pricing of Ride-hailing Services  with an Optional Bus Lane Access for Pool Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fayed%2C+L">Lynn Fayed</a>, 
<a href="/search/eess?searchtype=author&query=Nilsson%2C+G">Gustav Nilsson</a>, 
<a href="/search/eess?searchtype=author&query=Geroliminis%2C+N">Nikolas Geroliminis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">On-demand trip sharing is an efficient solution to mitigate the negative
impact e-hailing has on congestion. It motivates platform operators to reduce
their fleet size, and serves the same demand level with a lower effective
distance traveled. Users nevertheless prefer to travel solo and for shorter
distances despite the fare discount they receive. By offering them the choice
to pool and travel in high occupancy dedicated bus lanes, we provide them with
a larger incentive to share their rides, yet this creates additional bus
delays. In this work, we develop dynamic feedback-based control schemes that
adjust the price gap between solo and pool trips to improve multi-modal delays.
First, we develop a modal- and space-dependent aggregate model for private
vehicles, ride-pooling, and buses, and we use this model to test different
control strategies. To minimize the error between the target and actual speeds
in the bus network, we design a PI controller and show that by adjusting pool
trip fares, we can, with little input data, minimize this error. We also put
forward a Model Predictive Control (MPC) framework to minimize the total
Passenger Hours Traveled (PHT) and Waiting Times (WT) for the different
travelers. Moreover, we show how the MPC framework can be utilized to impose a
minimum speed in dedicated bus lanes to ensure that the buses operate on
schedule. The results mark the possibility of improving the overall network
conditions by incentivizing or discouraging pooling in the vehicle or bus
network.
</p>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01287" title="Abstract">arXiv:2310.01287</a> [<a href="/pdf/2310.01287" title="Download PDF">pdf</a>, <a href="/format/2310.01287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenQuery: Supporting Expressive Visual Search with Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Son%2C+K">Kihoon Son</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+D">DaEun Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T+S">Tae Soo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Young-Ho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juho Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages and 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Designers rely on visual search to explore and develop ideas in early design
stages. However, designers can struggle to identify suitable text queries to
initiate a search or to discover images for similarity-based search that can
adequately express their intent. We propose GenQuery, a novel system that
integrates generative models into the visual search process. GenQuery can
automatically elaborate on users' queries and surface concrete search
directions when users only have abstract ideas. To support precise expression
of search intents, the system enables users to generatively modify images and
use these in similarity-based search. In a comparative user study (N=16),
designers felt that they could more accurately express their intents and find
more satisfactory outcomes with GenQuery compared to a tool without generative
features. Furthermore, the unpredictability of generations allowed participants
to uncover more diverse outcomes. By supporting both convergence and
divergence, GenQuery led to a more creative experience.
</p>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01288" title="Abstract">arXiv:2310.01288</a> [<a href="/pdf/2310.01288" title="Download PDF">pdf</a>, <a href="/format/2310.01288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline Tracking with Object Permanence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianzhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Caesar%2C+H">Holger Caesar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">To reduce the expensive labor cost for manual labeling autonomous driving
datasets, an alternative is to automatically label the datasets using an
offline perception system. However, objects might be temporally occluded. Such
occlusion scenarios in the datasets are common yet underexplored in offline
autolabeling. In this work, we propose an offline tracking model that focuses
on occluded object tracks. It leverages the concept of object permanence which
means objects continue to exist even if they are not observed anymore. The
model contains three parts: a standard online tracker, a re-identification
(Re-ID) module that associates tracklets before and after occlusion, and a
track completion module that completes the fragmented tracks. The Re-ID module
and the track completion module use the vectorized map as one of the inputs to
refine the tracking results with occlusion. The model can effectively recover
the occluded object trajectories. It achieves state-of-the-art performance in
3D multi-object tracking by improving over the original online tracking result
by 45% IDS and 2% AMOTA on the vehicle tracks.
</p>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01290" title="Abstract">arXiv:2310.01290</a> [<a href="/pdf/2310.01290" title="Download PDF">pdf</a>, <a href="/format/2310.01290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Crosswords: Geometric Reasoning over Structured Knowledge with  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wenxuan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shangbin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuhan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhaoxuan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Balachandran%2C+V">Vidhisha Balachandran</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianxing He</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) are widely adopted in knowledge-intensive tasks
and have achieved impressive performance thanks to their knowledge abilities.
While LLMs have demonstrated outstanding performance on atomic or linear
(multi-hop) QA tasks, whether they can reason in knowledge-rich scenarios with
interweaving constraints remains an underexplored problem. In this work, we
propose geometric reasoning over structured knowledge, where pieces of
knowledge are connected in a graph structure and models need to fill in the
missing information. Such geometric knowledge reasoning would require the
ability to handle structured knowledge, reason with uncertainty, verify facts,
and backtrack when an error occurs. We propose Knowledge Crosswords, a
multi-blank QA dataset where each problem consists of a natural language
question representing the geometric constraints of an incomplete entity
network, where LLMs are tasked with working out the missing entities while
meeting all factual constraints. Knowledge Crosswords contains 2,101 individual
problems, covering various knowledge domains and further divided into three
difficulty levels. We conduct extensive experiments to evaluate existing LLM
prompting approaches on the Knowledge Crosswords benchmark. We additionally
propose two new approaches, Staged Prompting and Verify-All, to augment LLMs'
ability to backtrack and verify structured constraints. Our results demonstrate
that while baseline approaches perform well on easier problems but struggle
with hard ones, our proposed Verify-All outperforms other methods by a large
margin and is more robust with hard problems. Further analysis reveals that
LLMs' ability of geometric reasoning over structured knowledge is still far
from robust or perfect, susceptible to confounders such as the order of
options, certain structural patterns, assumption of existence of correct
answer, and more.
</p>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01291" title="Abstract">arXiv:2310.01291</a> [<a href="/pdf/2310.01291" title="Download PDF">pdf</a>, <a href="/format/2310.01291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3DHR-Co: A Collaborative Test-time Refinement Framework for In-the-Wild  3D Human-Body Reconstruction Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lumentut%2C+J+S">Jonathan Samuel Lumentut</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K+M">Kyoung Mu Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The field of 3D human-body reconstruction (abbreviated as 3DHR) that utilizes
parametric pose and shape representations has witnessed significant
advancements in recent years. However, the application of 3DHR techniques to
handle real-world, diverse scenes, known as in-the-wild data, still faces
limitations. The primary challenge arises as curating accurate 3D human pose
ground truth (GT) for in-the-wild scenes is still difficult to obtain due to
various factors. Recent test-time refinement approaches on 3DHR leverage
initial 2D off-the-shelf human keypoints information to support the lack of 3D
supervision on in-the-wild data. However, we observed that additional 2D
supervision alone could cause the overfitting issue on common 3DHR backbones,
making the 3DHR test-time refinement task seem intractable. We answer this
challenge by proposing a strategy that complements 3DHR test-time refinement
work under a collaborative approach. Specifically, we initially apply a
pre-adaptation approach that works by collaborating various 3DHR models in a
single framework to directly improve their initial outputs. This approach is
then further combined with the test-time adaptation work under specific
settings that minimize the overfitting issue to further boost the 3DHR
performance. The whole framework is termed as 3DHR-Co, and on the experiment
sides, we showed that the proposed work can significantly enhance the scores of
common classic 3DHR backbones up to -34 mm pose error suppression, putting them
among the top list on the in-the-wild benchmark data. Such achievement shows
that our approach helps unveil the true potential of the common classic 3DHR
backbones. Based on these findings, we further investigate various settings on
the proposed framework to better elaborate the capability of our collaborative
approach in the 3DHR task.
</p>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01292" title="Abstract">arXiv:2310.01292</a> [<a href="/pdf/2310.01292" title="Download PDF">pdf</a>, <a href="/format/2310.01292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Remote Sensing Segmentation With Generative Adversarial  Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+L">Luyi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dayu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaofeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenxiao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Most deep learning methods that achieve high segmentation accuracy require
deep network architectures that are too heavy and complex to run on embedded
devices with limited storage and memory space. To address this issue, this
paper proposes an efficient Generative Adversarial Transfomer (GATrans) for
achieving high-precision semantic segmentation while maintaining an extremely
efficient size. The framework utilizes a Global Transformer Network (GTNet) as
the generator, efficiently extracting multi-level features through residual
connections. GTNet employs global transformer blocks with progressively linear
computational complexity to reassign global features based on a learnable
similarity function. To focus on object-level and pixel-level information, the
GATrans optimizes the objective function by combining structural similarity
losses. We validate the effectiveness of our approach through extensive
experiments on the Vaihingen dataset, achieving an average F1 score of 90.17%
and an overall accuracy of 91.92%.
</p>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01297" title="Abstract">arXiv:2310.01297</a> [<a href="/pdf/2310.01297" title="Download PDF">pdf</a>, <a href="/format/2310.01297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co-audit: tools to help humans double-check AI-generated content
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gordon%2C+A+D">Andrew D. Gordon</a>, 
<a href="/search/cs?searchtype=author&query=Negreanu%2C+C">Carina Negreanu</a>, 
<a href="/search/cs?searchtype=author&query=Cambronero%2C+J">Jos&#xe9; Cambronero</a>, 
<a href="/search/cs?searchtype=author&query=Chakravarthy%2C+R">Rasika Chakravarthy</a>, 
<a href="/search/cs?searchtype=author&query=Drosos%2C+I">Ian Drosos</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Hao Fang</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+B">Bhaskar Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Richardson%2C+H">Hannah Richardson</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Advait Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Simmons%2C+S">Stephanie Simmons</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+J">Jack Williams</a>, 
<a href="/search/cs?searchtype=author&query=Zorn%2C+B">Ben Zorn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Programming Languages (cs.PL)

</div>
<p class="mathjax">Users are increasingly being warned to check AI-generated content for
correctness. Still, as LLMs (and other generative models) generate more complex
output, such as summaries, tables, or code, it becomes harder for the user to
audit or evaluate the output for quality or correctness. Hence, we are seeing
the emergence of tool-assisted experiences to help the user double-check a
piece of AI-generated content. We refer to these as co-audit tools. Co-audit
tools complement prompt engineering techniques: one helps the user construct
the input prompt, while the other helps them check the output response. As a
specific example, this paper describes recent research on co-audit tools for
spreadsheet computations powered by generative models. We explain why co-audit
experiences are essential for any application of generative AI where quality is
important and errors are consequential (as is common in spreadsheet
computations). We propose a preliminary list of principles for co-audit, and
outline research challenges.
</p>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01299" title="Abstract">arXiv:2310.01299</a> [<a href="/pdf/2310.01299" title="Download PDF">pdf</a>, <a href="/format/2310.01299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Explanations in Medical Question-Answering by Expectation  Maximization Inference over Evidence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingxiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Sileo%2C+D">Damien Sileo</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+J">Jesse Davis</a>, 
<a href="/search/cs?searchtype=author&query=Moens%2C+M">Marie-Francine Moens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Medical Question Answering~(medical QA) systems play an essential role in
assisting healthcare workers in finding answers to their questions. However, it
is not sufficient to merely provide answers by medical QA systems because users
might want explanations, that is, more analytic statements in natural language
that describe the elements and context that support the answer. To do so, we
propose a novel approach for generating natural language explanations for
answers predicted by medical QA systems. As high-quality medical explanations
require additional medical knowledge, so that our system extract knowledge from
medical textbooks to enhance the quality of explanations during the explanation
generation process. Concretely, we designed an expectation-maximization
approach that makes inferences about the evidence found in these texts,
offering an efficient way to focus attention on lengthy evidence passages.
Experimental results, conducted on two datasets MQAE-diag and MQAE, demonstrate
the effectiveness of our framework for reasoning with textual evidence. Our
approach outperforms state-of-the-art models, achieving a significant
improvement of \textbf{6.86} and \textbf{9.43} percentage points on the Rouge-1
score; \textbf{8.23} and \textbf{7.82} percentage points on the Bleu-4 score on
the respective datasets.
</p>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01301" title="Abstract">arXiv:2310.01301</a> [<a href="/pdf/2310.01301" title="Download PDF">pdf</a>, <a href="/ps/2310.01301" title="Download PostScript">ps</a>, <a href="/format/2310.01301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Short Time Angular Impulse Response of Rayleigh Beams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goswami%2C+B">Bidhayak Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Jayaprakash%2C+K+R">K. R. Jayaprakash</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+A">Anindya Chatterjee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">In the dynamics of linear structures, the impulse response function is of
fundamental interest. In some cases one examines the short term response
wherein the disturbance is still local and the boundaries have not yet come
into play, and for such short-time analysis the geometrical extent of the
structure may be taken as unbounded. Here we examine the response of slender
beams to angular impulses. The Euler-Bernoulli model, which does not include
rotary inertia of cross sections, predicts an unphysical and unbounded initial
rotation at the point of application. A finite length Euler-Bernoulli beam,
when modelled using finite elements, predicts a mesh-dependent response that
shows fast large-amplitude oscillations setting in very quickly. The simplest
introduction of rotary inertia yields the Rayleigh beam model, which has more
reasonable behaviour including a finite wave speed at all frequencies. If a
Rayleigh beam is given an impulsive moment at a location away from its
boundaries, then the predicted behaviour has an instantaneous finite jump in
local slope or rotation, followed by smooth evolution of the slope for a finite
time interval until reflections arrive from the boundary, causing subsequent
slope discontinuities in time. We present a detailed study of the angular
impulse response of a simply supported Rayleigh beam, starting with dimensional
analysis, followed by modal expansion including all natural frequencies,
culminating with an asymptotic formula for the short-time response. The
asymptotic formula is obtained by breaking the series solution into two parts
to be treated independently term by term, and leads to a polynomial in time.
The polynomial matches the response from refined finite element (FE)
simulations.
</p>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01304" title="Abstract">arXiv:2310.01304</a> [<a href="/pdf/2310.01304" title="Download PDF">pdf</a>, <a href="/format/2310.01304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coupling public and private gradient provably helps optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruixuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+Z">Zhiqi Bu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+S">Sheng Zha</a>, 
<a href="/search/cs?searchtype=author&query=Karypis%2C+G">George Karypis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The success of large neural networks is crucially determined by the
availability of data. It has been observed that training only on a small amount
of public data, or privately on the abundant private data can lead to
undesirable degradation of accuracy. In this work, we leverage both private and
public data to improve the optimization, by coupling their gradients via a
weighted linear combination. We formulate an optimal solution for the optimal
weight in the convex setting to indicate that the weighting coefficient should
be hyperparameter-dependent. Then, we prove the acceleration in the convergence
of non-convex loss and the effects of hyper-parameters such as privacy budget,
number of iterations, batch size, and model size on the choice of the weighting
coefficient. We support our analysis with empirical experiments across language
and vision benchmarks, and provide a guideline for choosing the optimal weight
of the gradient coupling.
</p>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01307" title="Abstract">arXiv:2310.01307</a> [<a href="/pdf/2310.01307" title="Download PDF">pdf</a>, <a href="/format/2310.01307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Generalization of Training-based ChatGPT Detection Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Han Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jie Ren</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengfei He</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Shenglai Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yingqian Cui</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Amy Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">ChatGPT is one of the most popular language models which achieve amazing
performance on various natural language tasks. Consequently, there is also an
urgent need to detect the texts generated ChatGPT from human written. One of
the extensively studied methods trains classification models to distinguish
both. However, existing studies also demonstrate that the trained models may
suffer from distribution shifts (during test), i.e., they are ineffective to
predict the generated texts from unseen language tasks or topics. In this work,
we aim to have a comprehensive investigation on these methods' generalization
behaviors under distribution shift caused by a wide range of factors, including
prompts, text lengths, topics, and language tasks. To achieve this goal, we
first collect a new dataset with human and ChatGPT texts, and then we conduct
extensive studies on the collected dataset. Our studies unveil insightful
findings which provide guidance for developing future methodologies or data
collection strategies for ChatGPT detection.
</p>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01308" title="Abstract">arXiv:2310.01308</a> [<a href="/pdf/2310.01308" title="Download PDF">pdf</a>, <a href="/ps/2310.01308" title="Download PostScript">ps</a>, <a href="/format/2310.01308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Small in-plane oscillations of a slack catenary using assumed modes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goswami%2C+B">Bidhayak Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+I">Indrasis Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+A">Anindya Chatterjee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">In this paper we study a problem in oscillations wherein the assumed modes
method offers some analytical and theoretical peculiarities. Specifically, we
study small in-plane oscillations of a slack catenary, or a sagging
inextensible chain fixed at both endpoints. The horizontal and vertical
displacements cannot be approximated independently because of pointwise
inextensibility in the chain. Moreover, the potential energy is a linear
function of the generalized coordinates, and does not directly cause
oscillations. Using assumed modes for the vertical displacements only,
integrating from one endpoint to compute the required horizontal displacements,
treating the horizontal fixity at the distal end as an added scalar constraint,
and obtaining linearized equations, we construct an eigenvalue problem which
contains a Lagrange multiplier. For generic assumed modes, the Lagrange
multiplier is determined by enforcing equilibrium in the undeflected shape.
However, when the modes thus determined are reinserted in the assumed mode
expansion and the calculation done afresh, then the Lagrange multiplier is
indeterminate at first order. Upon retaining terms at the next order, the
distal end fixity constraint introduces quadratic terms into a Lagrangian
without constraints. Results from these two approaches match perfectly. Our
approach offers nontrivial insights into both oscillations and Lagrangian
mechanics. It is also potentially applicable to other problems with
inextensibility in one-dimensional slender members.
</p>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01309" title="Abstract">arXiv:2310.01309</a> [<a href="/pdf/2310.01309" title="Download PDF">pdf</a>, <a href="/format/2310.01309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimistic Online Caching for Batched Requests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Faticanti%2C+F">Francescomaria Faticanti</a>, 
<a href="/search/cs?searchtype=author&query=Neglia%2C+G">Giovanni Neglia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">In this paper we study online caching problems where predictions of future
requests, e.g., provided by a machine learning model, are available. Typical
online optimistic policies are based on the Follow-The-Regularized-Leader
algorithm and have higher computational cost than classic ones like LFU, LRU,
as each update of the cache state requires to solve a constrained optimization
problem. In this work we analysed the behaviour of two different optimistic
policies in a \textit{batched} case, i.e., when the cache is updated less
frequently in order to amortize the update cost over time or over multiple
requests. Experimental results show that such an optimistic batched approach
outperforms classical caching policies both on stationary and real traces
</p>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01310" title="Abstract">arXiv:2310.01310</a> [<a href="/pdf/2310.01310" title="Download PDF">pdf</a>, <a href="/format/2310.01310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameterized Complexity of Incomplete Connected Fair Division
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gahlawat%2C+H">Harmender Gahlawat</a>, 
<a href="/search/cs?searchtype=author&query=Zehavi%2C+M">Meirav Zehavi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A preliminary version of this paper will appear in the Proceedings of FSTTCS 23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">\textit{Fair division} of resources among competing agents is a fundamental
problem in computational social choice and economic game theory. It has been
intensively studied on various kinds of items (\textit{divisible} and
\textit{indivisible}) and under various notions of \textit{fairness}. We focus
on Connected Fair Division (\CFDO), the variant of fair division on graphs,
where the \textit{resources} are modeled as an \textit{item graph}. Here, each
agent has to be assigned a connected subgraph of the item graph, and each item
has to be assigned to some agent.
<br />We introduce a generalization of \CFDO, termed Incomplete Connected Fair
Division (\CFD), where exactly $p$ vertices of the item graph should be
assigned to the agents. This might be useful, in particular when the
allocations are intended to be ``economical'' as well as fair. We consider four
well-known notions of fairness: \PROP, \EF, \EFO, \EFX. First, we prove that
\EF-\CFD, \EFO-\CFD, and \EFX-\CFD are W[1]-hard parameterized by $p$ plus the
number of agents, even for graphs having constant \textit{vertex cover number}
($\mathsf{vcn}$). In contrast, we present a randomized \FPT algorithm for
\PROP-\CFD parameterized only by $p$. Additionally, we prove both positive and
negative results concerning the kernelization complexity of \CFD under all four
fairness notions, parameterized by $p$, $\mathsf{vcn}$, and the total number of
different valuations in the item graph ($\mathsf{val}$).
</p>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01320" title="Abstract">arXiv:2310.01320</a> [<a href="/pdf/2310.01320" title="Download PDF">pdf</a>, <a href="/format/2310.01320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Avalon&#x27;s Game of Thoughts: Battle Against Deception through Recursive  Contemplation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shenzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zilong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+S">Siyuan Qi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qisen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+A">Andrew Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaofei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shiji Song</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Gao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Recent breakthroughs in large language models (LLMs) have brought remarkable
success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is
that the information processed by LLMs is consistently honest, neglecting the
pervasive deceptive or misleading information in human society and AI-generated
content. This oversight makes LLMs susceptible to malicious manipulations,
potentially resulting in detrimental outcomes. This study utilizes the
intricate Avalon game as a testbed to explore LLMs' potential in deceptive
environments. Avalon, full of misinformation and requiring sophisticated logic,
manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans'
recursive thinking and perspective-taking in the Avalon game, we introduce a
novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to
identify and counteract deceptive information. ReCon combines formulation and
refinement contemplation processes; formulation contemplation produces initial
thoughts and speech, while refinement contemplation further polishes them.
Additionally, we incorporate first-order and second-order perspective
transitions into these processes respectively. Specifically, the first-order
allows an LLM agent to infer others' mental states, and the second-order
involves understanding how others perceive the agent's mental state. After
integrating ReCon with different LLMs, extensive experiment results from the
Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around
deceptive information without extra fine-tuning and data. Finally, we offer a
possible explanation for the efficacy of ReCon and explore the current
limitations of LLMs in terms of safety, reasoning, speaking style, and format,
potentially furnishing insights for subsequent research.
</p>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01321" title="Abstract">arXiv:2310.01321</a> [<a href="/pdf/2310.01321" title="Download PDF">pdf</a>, <a href="/format/2310.01321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Color and Texture Dual Pipeline Lightweight Style Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">ShiQi Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Style transfer methods typically generate a single stylized output of color
and texture coupling for reference styles, and color transfer schemes may
introduce distortion or artifacts when processing reference images with
duplicate textures. To solve the problem, we propose a Color and Texture Dual
Pipeline Lightweight Style Transfer CTDP method, which employs a dual pipeline
method to simultaneously output the results of color and texture transfer.
Furthermore, we designed a masked total variation loss to suppress artifacts
and small texture representations in color transfer results without affecting
the semantic part of the content. More importantly, we are able to add texture
structures with controllable intensity to color transfer results for the first
time. Finally, we conducted feature visualization analysis on the texture
generation mechanism of the framework and found that smoothing the input image
can almost completely eliminate this texture structure. In comparative
experiments, the color and texture transfer results generated by CTDP both
achieve state-of-the-art performance. Additionally, the weight of the color
transfer branch model size is as low as 20k, which is 100-1500 times smaller
than that of other state-of-the-art models.
</p>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01324" title="Abstract">arXiv:2310.01324</a> [<a href="/pdf/2310.01324" title="Download PDF">pdf</a>, <a href="/format/2310.01324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ZeroI2V: Zero-Cost Adaptation of Pre-trained Transformers from Image to  Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Limin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Adapting image models to video domain is becoming an efficient paradigm for
solving video recognition tasks. Due to the huge number of parameters and
effective transferability of image models, performing full fine-tuning is less
efficient and even unnecessary. Thus, recent research is shifting its focus
towards parameter-efficient image-to-video adaptation. However, these
adaptation strategies inevitably introduce extra computational cost to deal
with the domain gap and temporal modeling in videos. In this paper, our goal is
to present a zero-cost adaptation paradigm (ZeroI2V) to transfer the image
transformers to video recognition tasks (i.e., introduce zero extra cost to the
adapted models during inference). To achieve this goal, we present two core
designs. First, to capture the dynamics in videos and reduce the difficulty of
achieving image-to-video adaptation, we exploit the flexibility of
self-attention and introduce the spatial-temporal dual-headed attention (STDHA)
that efficiently endow the image transformers with temporal modeling capability
at zero extra parameters and computation. Second, to handle the domain gap
between images and videos, we propose a linear adaption strategy which utilizes
lightweight densely placed linear adapters to fully transfer the frozen image
models to video recognition. Due to its customized linear design, all newly
added adapters could be easily merged with the original modules through
structural reparameterization after training, thus achieving zero extra cost
during inference. Extensive experiments on four widely-used video recognition
benchmarks show that our ZeroI2V can match or even outperform previous
state-of-the-art methods while enjoying superior parameter and inference
efficiency.
</p>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01327" title="Abstract">arXiv:2310.01327</a> [<a href="/pdf/2310.01327" title="Download PDF">pdf</a>, <a href="/format/2310.01327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate  Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ashok%2C+A">Arjun Ashok</a>, 
<a href="/search/cs?searchtype=author&query=Marcotte%2C+%C3%89">&#xc9;tienne Marcotte</a>, 
<a href="/search/cs?searchtype=author&query=Zantedeschi%2C+V">Valentina Zantedeschi</a>, 
<a href="/search/cs?searchtype=author&query=Chapados%2C+N">Nicolas Chapados</a>, 
<a href="/search/cs?searchtype=author&query=Drouin%2C+A">Alexandre Drouin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce a new model for multivariate probabilistic time series
prediction, designed to flexibly address a range of tasks including
forecasting, interpolation, and their combinations. Building on copula theory,
we propose a simplified objective for the recently-introduced transformer-based
attentional copulas (TACTiS), wherein the number of distributional parameters
now scales linearly with the number of variables instead of factorially. The
new objective requires the introduction of a training curriculum, which goes
hand-in-hand with necessary changes to the original architecture. We show that
the resulting model has significantly better training dynamics and achieves
state-of-the-art performance across diverse real-world forecasting tasks, while
maintaining the flexibility of prior work, such as seamless handling of
unaligned and unevenly-sampled time series.
</p>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01329" title="Abstract">arXiv:2310.01329</a> [<a href="/pdf/2310.01329" title="Download PDF">pdf</a>, <a href="/format/2310.01329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BTR: Binary Token Representations for Efficient Retrieval Augmented  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qingqing Cao</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+S">Sewon Min</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Retrieval augmentation addresses many critical problems in large language
models such as hallucination, staleness, and privacy leaks. However, running
retrieval-augmented language models (LMs) is slow and difficult to scale due to
processing large amounts of retrieved text. We introduce binary token
representations (BTR), which use 1-bit vectors to precompute every token in
passages, significantly reducing computation during inference. Despite the
potential loss of accuracy, our new calibration techniques and training
objectives restore performance. Combined with offline and runtime compression,
this only requires 127GB of disk space for encoding 3 billion tokens in
Wikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTR
accelerates state-of-the-art inference by up to 4x and reduces storage by over
100x while maintaining over 95% task performance.
</p>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01330" title="Abstract">arXiv:2310.01330</a> [<a href="/pdf/2310.01330" title="Download PDF">pdf</a>, <a href="/format/2310.01330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards reporting bias in visual-language datasets: bimodal augmentation  by decoupling object-attribute association
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qiyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+M">Mengjie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yutong He</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ono%2C+J">Junya Ono</a>, 
<a href="/search/cs?searchtype=author&query=Wakaki%2C+H">Hiromi Wakaki</a>, 
<a href="/search/cs?searchtype=author&query=Mitsufuji%2C+Y">Yuki Mitsufuji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Reporting bias arises when people assume that some knowledge is universally
understood and hence, do not necessitate explicit elaboration. In this paper,
we focus on the wide existence of reporting bias in visual-language datasets,
embodied as the object-attribute association, which can subsequentially degrade
models trained on them. To mitigate this bias, we propose a bimodal
augmentation (BiAug) approach through object-attribute decoupling to flexibly
synthesize visual-language examples with a rich array of object-attribute
pairing and construct cross-modal hard negatives. We employ large language
models (LLMs) in conjunction with a grounding object detector to extract target
objects. Subsequently, the LLM generates a detailed attribute description for
each object and produces a corresponding hard negative counterpart. An
inpainting model is then used to create images based on these detailed object
descriptions. By doing so, the synthesized examples explicitly complement
omitted objects and attributes to learn, and the hard negative pairs steer the
model to distinguish object attributes. Our experiments demonstrated that BiAug
is superior in object-attribute understanding. In addition, BiAug also improves
the performance on zero-shot retrieval tasks on general benchmarks like MSCOCO
and Flickr30K. BiAug refines the way of collecting text-image datasets.
Mitigating the reporting bias helps models achieve a deeper understanding of
visual-language phenomena, expanding beyond mere frequent patterns to encompass
the richness and diversity of real-world scenarios.
</p>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01331" title="Abstract">arXiv:2310.01331</a> [<a href="/pdf/2310.01331" title="Download PDF">pdf</a>, <a href="/format/2310.01331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChoiceMates: Supporting Unfamiliar Online Decision-Making with  Multi-Agent Conversational Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jeongeon Park</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+B">Bryan Min</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaojuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juho Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Unfamiliar decisions -- decisions where people lack adequate domain knowledge
or expertise -- specifically increase the complexity and uncertainty of the
process of searching for, understanding, and making decisions with online
information. Through our formative study (n=14), we observed users' challenges
in accessing diverse perspectives, identifying relevant information, and
deciding the right moment to make the final decision. We present ChoiceMates, a
system that enables conversations with a dynamic set of LLM-powered agents for
a holistic domain understanding and efficient discovery and management of
information to make decisions. Agents, as opinionated personas, flexibly join
the conversation, not only providing responses but also conversing among
themselves to elicit each agent's preferences. Our between-subjects study
(n=36) comparing ChoiceMates to conventional web search and single-agent showed
that ChoiceMates was more helpful in discovering, diving deeper, and managing
information compared to Web with higher confidence. We also describe how
participants utilized multi-agent conversations in their decision-making
process.
</p>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01334" title="Abstract">arXiv:2310.01334</a> [<a href="/pdf/2310.01334" title="Download PDF">pdf</a>, <a href="/format/2310.01334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Merge, Then Compress: Demystify Efficient SMoE with Hints from Its  Routing Policy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pingzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yadav%2C+P">Prateek Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Sung%2C+Y">Yi-Lin Sung</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianlong Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures, 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up
the learning capacity of neural networks, however, they have issues like (a)
High Memory Usage, due to duplication of the network layers into multiple
copies as experts; and (b) Redundancy in Experts, as common learning-based
routing policies suffer from representational collapse. Therefore, vanilla SMoE
models are memory inefficient and non-scalable, especially for
resource-constrained downstream scenarios. In this paper, we ask: Can we craft
a compact SMoE model by consolidating expert information? What is the best
recipe to merge multiple experts into fewer but more knowledgeable experts? Our
pilot investigation reveals that conventional model merging methods fail to be
effective in such expert merging for SMoE. The potential reasons are: (1)
redundant information overshadows critical experts; (2) appropriate neuron
permutation for each expert is missing to bring all of them in alignment. To
address this, we propose M-SMoE, which leverages routing statistics to guide
expert merging. Specifically, it starts with neuron permutation alignment for
experts; then, dominant experts and their "group members" are formed; lastly,
every expert group is merged into a single expert by utilizing each expert's
activation frequency as their weight for merging, thus diminishing the impact
of insignificant experts. Moreover, we observed that our proposed merging
promotes a low dimensionality in the merged expert's weight space, naturally
paving the way for additional compression. Hence, our final method, MC-SMoE
(i.e., Merge, then Compress SMoE), further decomposes the merged experts into
low-rank and structural sparse alternatives. Extensive experiments across 8
benchmarks validate the effectiveness of MC-SMoE. For instance, our MC-SMoE
achieves up to 80% memory and a 20% FLOPs reduction, with virtually no loss in
performance.
</p>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01336" title="Abstract">arXiv:2310.01336</a> [<a href="/pdf/2310.01336" title="Download PDF">pdf</a>, <a href="/format/2310.01336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JugglePAC: A Pipelined Accumulation Circuit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Houraniah%2C+A">Ahmad Houraniah</a>, 
<a href="/search/cs?searchtype=author&query=Ugurdag%2C+H+F">H. Fatih Ugurdag</a>, 
<a href="/search/cs?searchtype=author&query=Aydin%2C+F">Furkan Aydin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Summing a set of numbers, namely, "Accumulation," is a subtask within many
computational tasks. If the numbers to sum arrive non-stop in back-to-back
clock cycles at high clock frequencies, summing them without allowing them to
pile up can be quite a challenge, that is, when the latency of addition (i.e.,
summing two numbers) is longer than one clock cycle, which is always the case
for floating-point numbers. This could also be the case for integer summations
with high clock frequencies. In the case of floating-point numbers, this is
handled by pipelining the adder, but that does not solve all problems. The
challenges include optimization of speed, area, and latency. As well as the
adaptability of the design to different application requirements, such as the
ability to handle variable-size subsequent data sets with no time gap in
between and with results produced in the input-order. All these factors make
designing an efficient floating-point accumulator a non-trivial problem.
Integer accumulation is a relatively simpler problem, where high frequencies
can be achieved by using carry-save tree adders. This can then be further
improved by efficient resource-sharing. In this paper, we present two fast and
area-efficient accumulation circuits, JugglePAC and INTAC. JugglePAC is
tailored for floating-point reduction operations (such as accumulation) and
offers significant advantages with respect to the literature in terms of speed,
area, and adaptability to various application requirements. INTAC is designed
for fast integer accumulation. Using carry-save adders and resource-sharing, it
can achieve very high clock frequencies while maintaining a low area
complexity.
</p>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01339" title="Abstract">arXiv:2310.01339</a> [<a href="/pdf/2310.01339" title="Download PDF">pdf</a>, <a href="/format/2310.01339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Dialogue Management: Quality Datasets vs Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Medina-Ram%C3%ADrez%2C+M+%C3%81">Miguel &#xc1;ngel Medina-Ram&#xed;rez</a>, 
<a href="/search/cs?searchtype=author&query=Guerra-Artal%2C+C">Cayetano Guerra-Artal</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Tejera%2C+M">Mario Hern&#xe1;ndez-Tejera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Task-oriented dialogue systems (TODS) have become crucial for users to
interact with machines and computers using natural language. One of its key
components is the dialogue manager, which guides the conversation towards a
good goal for the user by providing the best possible response. Previous works
have proposed rule-based systems (RBS), reinforcement learning (RL), and
supervised learning (SL) as solutions for the correct dialogue management; in
other words, select the best response given input by the user. However, this
work argues that the leading cause of DMs not achieving maximum performance
resides in the quality of the datasets rather than the models employed thus
far; this means that dataset errors, like mislabeling, originate a large
percentage of failures in dialogue management. We studied the main errors in
the most widely used datasets, Multiwoz 2.1 and SGD, to demonstrate this
hypothesis. To do this, we have designed a synthetic dialogue generator to
fully control the amount and type of errors introduced in the dataset. Using
this generator, we demonstrated that errors in the datasets contribute
proportionally to the performance of the models
</p>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01342" title="Abstract">arXiv:2310.01342</a> [<a href="/pdf/2310.01342" title="Download PDF">pdf</a>, <a href="/format/2310.01342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-field Integrated Sensing and Communication: Opportunities and  Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cong%2C+J">Jiayi Cong</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+C">Changsheng You</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiapeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Li Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+B">Beixiong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yi Gong</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Shi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work is submitted to IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">With the extremely large-scale array XL-array deployed in future wireless
systems, wireless communication and sensing are expected to operate in the
radiative near-field region, which needs to be characterized by the spherical
rather than planar wavefronts. Unlike most existing works that considered
far-field integrated sensing and communication (ISAC), we study in this article
the new near-field ISAC, which integrates both functions of sensing and
communication in the near-field region. To this end, we first discuss the
appealing advantages of near-field communication and sensing over their
far-field counterparts, respectively. Then, we introduce three approaches for
near-field ISAC, including joint near-field communication and sensing,
sensing-assisted near-field communication, and communication-assisted
near-field sensing. We discuss their individual research opportunities, new
design issues, as well as propose promising solutions. Finally, several
important directions in near-field ISAC are also highlighted to motivate future
work.
</p>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01351" title="Abstract">arXiv:2310.01351</a> [<a href="/pdf/2310.01351" title="Download PDF">pdf</a>, <a href="/format/2310.01351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Streaming Motion Forecasting for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+Z">Ziqi Pang</a>, 
<a href="/search/cs?searchtype=author&query=Ramanan%2C+D">Deva Ramanan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mengtian Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IROS 2023, 8 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Trajectory forecasting is a widely-studied problem for autonomous navigation.
However, existing benchmarks evaluate forecasting based on independent
snapshots of trajectories, which are not representative of real-world
applications that operate on a continuous stream of data. To bridge this gap,
we introduce a benchmark that continuously queries future trajectories on
streaming data and we refer to it as "streaming forecasting." Our benchmark
inherently captures the disappearance and re-appearance of agents, presenting
the emergent challenge of forecasting for occluded agents, which is a
safety-critical problem yet overlooked by snapshot-based benchmarks. Moreover,
forecasting in the context of continuous timestamps naturally asks for temporal
coherence between predictions from adjacent timestamps. Based on this
benchmark, we further provide solutions and analysis for streaming forecasting.
We propose a plug-and-play meta-algorithm called "Predictive Streamer" that can
adapt any snapshot-based forecaster into a streaming forecaster. Our algorithm
estimates the states of occluded agents by propagating their positions with
multi-modal trajectories, and leverages differentiable filters to ensure
temporal consistency. Both occlusion reasoning and temporal coherence
strategies significantly improve forecasting quality, resulting in 25% smaller
endpoint errors for occluded agents and 10-20% smaller fluctuations of
trajectories. Our work is intended to generate interest within the community by
highlighting the importance of addressing motion forecasting in its intrinsic
streaming setting. Code is available at
https://github.com/ziqipang/StreamingForecasting.
</p>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01352" title="Abstract">arXiv:2310.01352</a> [<a href="/pdf/2310.01352" title="Download PDF">pdf</a>, <a href="/format/2310.01352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RA-DIT: Retrieval-Augmented Dual Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+X+V">Xi Victoria Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xilun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingda Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weijia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Lomeli%2C+M">Maria Lomeli</a>, 
<a href="/search/cs?searchtype=author&query=James%2C+R">Rich James</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+P">Pedro Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Kahn%2C+J">Jacob Kahn</a>, 
<a href="/search/cs?searchtype=author&query=Szilvasy%2C+G">Gergely Szilvasy</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+M">Mike Lewis</a>, 
<a href="/search/cs?searchtype=author&query=Zettlemoyer%2C+L">Luke Zettlemoyer</a>, 
<a href="/search/cs?searchtype=author&query=Yih%2C+S">Scott Yih</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Retrieval-augmented language models (RALMs) improve performance by accessing
long-tail and up-to-date knowledge from external data stores, but are
challenging to build. Existing approaches require either expensive
retrieval-specific modifications to LM pre-training or use post-hoc integration
of the data store that leads to suboptimal performance. We introduce
Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning
methodology that provides a third option by retrofitting any LLM with retrieval
capabilities. Our approach operates in two distinct fine-tuning steps: (1) one
updates a pre-trained LM to better use retrieved information, while (2) the
other updates the retriever to return more relevant results, as preferred by
the LM. By fine-tuning over tasks that require both knowledge utilization and
contextual awareness, we demonstrate that each stage yields significant
performance improvements, and using both leads to additional gains. Our best
model, RA-DIT 65B, achieves state-of-the-art performance across a range of
knowledge-intensive zero- and few-shot learning benchmarks, significantly
outperforming existing in-context RALM approaches by up to +8.9% in 0-shot
setting and +1.4% in 5-shot setting on average.
</p>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01356" title="Abstract">arXiv:2310.01356</a> [<a href="/pdf/2310.01356" title="Download PDF">pdf</a>, <a href="/format/2310.01356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Less is More: Toward Zero-Shot Local Scene Graph Generation via  Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huijuan Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Humans inherently recognize objects via selective visual perception,
transform specific regions from the visual field into structured symbolic
knowledge, and reason their relationships among regions based on the allocation
of limited attention resources in line with humans' goals. While it is
intuitive for humans, contemporary perception systems falter in extracting
structural information due to the intricate cognitive abilities and commonsense
knowledge required. To fill this gap, we present a new task called Local Scene
Graph Generation. Distinct from the conventional scene graph generation task,
which encompasses generating all objects and relationships in an image, our
proposed task aims to abstract pertinent structural information with partial
objects and their relationships for boosting downstream tasks that demand
advanced comprehension and reasoning capabilities. Correspondingly, we
introduce zEro-shot Local scEne GrAph geNeraTion (ELEGANT), a framework
harnessing foundation models renowned for their powerful perception and
commonsense reasoning, where collaboration and information communication among
foundation models yield superior outcomes and realize zero-shot local scene
graph generation without requiring labeled supervision. Furthermore, we propose
a novel open-ended evaluation metric, Entity-level CLIPScorE (ECLIPSE),
surpassing previous closed-set evaluation metrics by transcending their limited
label space, offering a broader assessment. Experiment results show that our
approach markedly outperforms baselines in the open-ended evaluation setting,
and it also achieves a significant performance boost of up to 24.58% over prior
methods in the close-set setting, demonstrating the effectiveness and powerful
reasoning ability of our proposed framework.
</p>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01358" title="Abstract">arXiv:2310.01358</a> [<a href="/pdf/2310.01358" title="Download PDF">pdf</a>, <a href="/format/2310.01358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NEUCORE: Neural Concept Reasoning for Composed Image Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huijuan Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Composed image retrieval which combines a reference image and a text modifier
to identify the desired target image is a challenging task, and requires the
model to comprehend both vision and language modalities and their interactions.
Existing approaches focus on holistic multi-modal interaction modeling, and
ignore the composed and complimentary property between the reference image and
text modifier. In order to better utilize the complementarity of multi-modal
inputs for effective information fusion and retrieval, we move the multi-modal
understanding to fine-granularity at concept-level, and learn the multi-modal
concept alignment to identify the visual location in reference or target images
corresponding to text modifier. Toward the end, we propose a NEUral COncept
REasoning (NEUCORE) model which incorporates multi-modal concept alignment and
progressive multimodal fusion over aligned concepts. Specifically, considering
that text modifier may refer to semantic concepts not existing in the reference
image and requiring to be added into the target image, we learn the multi-modal
concept alignment between the text modifier and the concatenation of reference
and target images, under multiple-instance learning framework with image and
sentence level weak supervision. Furthermore, based on aligned concepts, to
form discriminative fusion features of the input modalities for accurate target
image retrieval, we propose a progressive fusion strategy with unified
execution architecture instantiated by the attended language semantic concepts.
Our proposed approach is evaluated on three datasets and achieves
state-of-the-art results.
</p>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01360" title="Abstract">arXiv:2310.01360</a> [<a href="/pdf/2310.01360" title="Download PDF">pdf</a>, <a href="/format/2310.01360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Scalable Visual Servoing Using Deep Reinforcement Learning and  Optimal Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asayesh%2C+S">Salar Asayesh</a>, 
<a href="/search/cs?searchtype=author&query=Darani%2C+H+S">Hossein Sheikhi Darani</a>, 
<a href="/search/cs?searchtype=author&query=chen%2C+M">Mo chen</a>, 
<a href="/search/cs?searchtype=author&query=Mehrandezh%2C+M">Mehran Mehrandezh</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+K">Kamal Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Classical pixel-based Visual Servoing (VS) approaches offer high accuracy but
suffer from a limited convergence area due to optimization nonlinearity. Modern
deep learning-based VS methods overcome traditional vision issues but lack
scalability, requiring training on limited scenes. This paper proposes a hybrid
VS strategy utilizing Deep Reinforcement Learning (DRL) and optimal control to
enhance both convergence area and scalability. The DRL component of our
approach separately handles representation and policy learning to enhance
scalability, generalizability, learning efficiency and ease domain adaptation.
Moreover, the optimal control part ensures high end-point accuracy. Our method
showcases remarkable achievements in terms of high convergence rates and
minimal end-positioning errors using a 7-DOF manipulator. Importantly, it
exhibits scalability across more than 1000 distinct scenes. Furthermore, we
demonstrate its capacity for generalization to previously unseen datasets.
Lastly, we illustrate the real-world applicability of our approach,
highlighting its adaptability through single-shot domain transfer learning in
environments with noise and occlusions. Real-robot experiments can be found at
\url{https://sites.google.com/view/vsls}.
</p>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01361" title="Abstract">arXiv:2310.01361</a> [<a href="/pdf/2310.01361" title="Download PDF">pdf</a>, <a href="/format/2310.01361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenSim: Generating Robotic Simulation Tasks via Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lirui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+Y">Yiyang Ling</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhecheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Shridhar%2C+M">Mohit Shridhar</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+C">Chen Bao</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yuzhe Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bailin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huazhe Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> See our project website (<a href="https://liruiw.github.io/gensim">this https URL</a>), demo (<a href="https://huggingface.co/spaces/Gen-Sim/Gen-Sim">this https URL</a>), and code (<a href="https://github.com/liruiw/GenSim">this https URL</a>) for visualizations and open-source models and datasets
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
<p class="mathjax">Collecting large amounts of real-world interaction data to train general
robotic policies is often prohibitively expensive, thus motivating the use of
simulation data. However, existing methods for data generation have generally
focused on scene-level diversity (e.g., object instances and poses) rather than
task-level diversity, due to the human effort required to come up with and
verify novel tasks. This has made it challenging for policies trained on
simulation data to demonstrate significant task-level generalization. In this
paper, we propose to automatically generate rich simulation environments and
expert demonstrations by exploiting a large language models' (LLM) grounding
and coding ability. Our approach, dubbed GenSim, has two modes: goal-directed
generation, wherein a target task is given to the LLM and the LLM proposes a
task curriculum to solve the target task, and exploratory generation, wherein
the LLM bootstraps from previous tasks and iteratively proposes novel tasks
that would be helpful in solving more complex tasks. We use GPT4 to expand the
existing benchmark by ten times to over 100 tasks, on which we conduct
supervised finetuning and evaluate several LLMs including finetuned GPTs and
Code Llama on code generation for robotic simulation tasks. Furthermore, we
observe that LLMs-generated simulation programs can enhance task-level
generalization significantly when used for multitask policy training. We
further find that with minimal sim-to-real adaptation, the multitask policies
pretrained on GPT4-generated simulation tasks exhibit stronger transfer to
unseen long-horizon tasks in the real world and outperform baselines by 25%.
See the project website (https://liruiw.github.io/gensim) for code, demos, and
videos.
</p>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01362" title="Abstract">arXiv:2310.01362</a> [<a href="/pdf/2310.01362" title="Download PDF">pdf</a>, <a href="/format/2310.01362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fleet Policy Learning via Weight Merging and An Application to Robotic  Tool-Use
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lirui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaiqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Allan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Simchowitz%2C+M">Max Simchowitz</a>, 
<a href="/search/cs?searchtype=author&query=Tedrake%2C+R">Russ Tedrake</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> See the code <a href="https://github.com/liruiw/Fleet-Tools">this https URL</a> for more details
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Fleets of robots ingest massive amounts of streaming data generated by
interacting with their environments, far more than those that can be stored or
transmitted with ease. At the same time, we hope that teams of robots can
co-acquire diverse skills through their experiences in varied settings. How can
we enable such fleet-level learning without having to transmit or centralize
fleet-scale data? In this paper, we investigate distributed learning of
policies as a potential solution. To efficiently merge policies in the
distributed setting, we propose fleet-merge, an instantiation of distributed
learning that accounts for the symmetries that can arise in learning policies
that are parameterized by recurrent neural networks. We show that fleet-merge
consolidates the behavior of policies trained on 50 tasks in the Meta-World
environment, with the merged policy achieving good performance on nearly all
training tasks at test time. Moreover, we introduce a novel robotic tool-use
benchmark, fleet-tools, for fleet policy learning in compositional and
contact-rich robot manipulation tasks, which might be of broader interest, and
validate the efficacy of fleet-merge on the benchmark.
</p>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01363" title="Abstract">arXiv:2310.01363</a> [<a href="/pdf/2310.01363" title="Download PDF">pdf</a>, <a href="/format/2310.01363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EAST: Environment Aware Safe Tracking using Planning and Control  Co-Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhichao Li</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+Y">Yinzhuang Yi</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Zhuolin Niu</a>, 
<a href="/search/cs?searchtype=author&query=Atanasov%2C+N">Nikolay Atanasov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper considers the problem of autonomous robot navigation in unknown
environments with moving obstacles. We propose a new method that systematically
puts planning, motion prediction and safety metric design together to achieve
environmental adaptive and safe navigation. This algorithm balances optimality
in travel distance and safety with respect to passing clearance. Robot adapts
progress speed adaptively according to the sensed environment, being fast in
wide open areas and slow down in narrow passages and taking necessary maneuvers
to avoid dangerous incoming obstacles. In our method, directional distance
measure, conic-shape motion prediction and custom costmap are integrated
properly to evaluate system risk accurately with respect to local geometry of
surrounding environments. Using such risk estimation, reference governor
technique and control barrier function are worked together to enable adaptive
and safe path tracking in dynamical environments. We validate our algorithm
extensively both in simulation and challenging real-world environments.
</p>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01365" title="Abstract">arXiv:2310.01365</a> [<a href="/pdf/2310.01365" title="Download PDF">pdf</a>, <a href="/format/2310.01365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Elephant Neural Networks: Born to Be a Continual Learner
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lan%2C+Q">Qingfeng Lan</a>, 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+A+R">A. Rupam Mahmood</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Catastrophic forgetting remains a significant challenge to continual learning
for decades. While recent works have proposed effective methods to mitigate
this problem, they mainly focus on the algorithmic side. Meanwhile, we do not
fully understand what architectural properties of neural networks lead to
catastrophic forgetting. This study aims to fill this gap by studying the role
of activation functions in the training dynamics of neural networks and their
impact on catastrophic forgetting. Our study reveals that, besides sparse
representations, the gradient sparsity of activation functions also plays an
important role in reducing forgetting. Based on this insight, we propose a new
class of activation functions, elephant activation functions, that can generate
both sparse representations and sparse gradients. We show that by simply
replacing classical activation functions with elephant activation functions, we
can significantly improve the resilience of neural networks to catastrophic
forgetting. Our method has broad applicability and benefits for continual
learning in regression, class incremental learning, and reinforcement learning
tasks. Specifically, we achieves excellent performance on Split MNIST dataset
in just one single pass, without using replay buffer, task boundary
information, or pre-training.
</p>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01366" title="Abstract">arXiv:2310.01366</a> [<a href="/pdf/2310.01366" title="Download PDF">pdf</a>, <a href="/format/2310.01366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Window-based Model Averaging Improves Generalization in Heterogeneous  Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Caldarola%2C+D">Debora Caldarola</a>, 
<a href="/search/cs?searchtype=author&query=Caputo%2C+B">Barbara Caputo</a>, 
<a href="/search/cs?searchtype=author&query=Ciccone%2C+M">Marco Ciccone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Computer Vision Workshop (ICCVW)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated Learning (FL) aims to learn a global model from distributed users
while protecting their privacy. However, when data are distributed
heterogeneously the learning process becomes noisy, unstable, and biased
towards the last seen clients' data, slowing down convergence. To address these
issues and improve the robustness and generalization capabilities of the global
model, we propose WIMA (Window-based Model Averaging). WIMA aggregates global
models from different rounds using a window-based approach, effectively
capturing knowledge from multiple users and reducing the bias from the last
ones. By adopting a windowed view on the rounds, WIMA can be applied from the
initial stages of training. Importantly, our method introduces no additional
communication or client-side computation overhead. Our experiments demonstrate
the robustness of WIMA against distribution shifts and bad client sampling,
resulting in smoother and more stable learning trends. Additionally, WIMA can
be easily integrated with state-of-the-art algorithms. We extensively evaluate
our approach on standard FL benchmarks, demonstrating its effectiveness.
</p>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01367" title="Abstract">arXiv:2310.01367</a> [<a href="/pdf/2310.01367" title="Download PDF">pdf</a>, <a href="/format/2310.01367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Ziv-Merhav theorem beyond Markovianity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barnfield%2C+N">Nicholas Barnfield</a>, 
<a href="/search/cs?searchtype=author&query=Grondin%2C+R">Rapha&#xeb;l Grondin</a>, 
<a href="/search/cs?searchtype=author&query=Pozzoli%2C+G">Gaia Pozzoli</a>, 
<a href="/search/cs?searchtype=author&query=Raqu%C3%A9pas%2C+R">Renaud Raqu&#xe9;pas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Dynamical Systems (math.DS); Probability (math.PR)

</div>
<p class="mathjax">We generalize to a broader class of decoupled measures a result of Ziv and
Merhav on universal estimation of the specific cross (or relative) entropy for
a pair of multi-level Markov measures. The result covers pairs of suitably
regular g-measures and pairs of equilibrium measures arising from the small
space of interactions in mathematical statistical mechanics.
</p>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01370" title="Abstract">arXiv:2310.01370</a> [<a href="/pdf/2310.01370" title="Download PDF">pdf</a>, <a href="/format/2310.01370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Type-Based Verification of Delegated Control in Hybrid~Systems (Full  Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamburjan%2C+E">Eduard Kamburjan</a>, 
<a href="/search/cs?searchtype=author&query=Lienhardt%2C+M">Michael Lienhardt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">We present a post-region-based verification system for distributed hybrid
systems modeled with Hybrid Active Objects. The post-region of a class method
is the region of the state space where a physical process must be proven safe
to ensure some object invariant. Prior systems computed the post-region locally
to a single object and could only verify systems where each object ensures its
own safety, or relied on specific, non-modular communication patterns. The
system presented here uses a type-and-effect system to structure the
interactions between objects and computes post-regions globally, but verifies
them locally. Furthermore, we are able to handle systems with delegated
control: the object and method that shape the post-region change over time. We
exemplify our approach with a model of a cloud-based hybrid system.
</p>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01373" title="Abstract">arXiv:2310.01373</a> [<a href="/pdf/2310.01373" title="Download PDF">pdf</a>, <a href="/format/2310.01373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weak Galerkin Finite Element Methods for Optimal Control Problems  Governed by Second Order Elliptic Partial Differential Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+C">Chunmei Wang</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+J">Junping Wang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+S">Shangyou Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 3 figures, 5 tables. arXiv admin note: substantial text overlap with <a href="/abs/1806.01583">arXiv:1806.01583</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper is concerned with the development of weak Galerkin (WG) finite
element method for optimal control problems governed by second order elliptic
partial differential equations (PDEs). It is advantageous to use discontinuous
finite elements over the traditional $C^1$ finite elements here. Optimal order
error estimates are established and confirmed by some numerical tests.
</p>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01376" title="Abstract">arXiv:2310.01376</a> [<a href="/pdf/2310.01376" title="Download PDF">pdf</a>, <a href="/format/2310.01376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Distribution-Agnostic Generalized Category Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jianhong Bai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hualiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruizhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+L">Lianrui Mu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaomeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J+T">Joey Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Haoji Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Data imbalance and open-ended distribution are two intrinsic characteristics
of the real visual world. Though encouraging progress has been made in tackling
each challenge separately, few works dedicated to combining them towards
real-world scenarios. While several previous works have focused on classifying
close-set samples and detecting open-set samples during testing, it's still
essential to be able to classify unknown subjects as human beings. In this
paper, we formally define a more realistic task as distribution-agnostic
generalized category discovery (DA-GCD): generating fine-grained predictions
for both close- and open-set classes in a long-tailed open-world setting. To
tackle the challenging problem, we propose a Self-Balanced Co-Advice
contrastive framework (BaCon), which consists of a contrastive-learning branch
and a pseudo-labeling branch, working collaboratively to provide interactive
supervision to resolve the DA-GCD task. In particular, the contrastive-learning
branch provides reliable distribution estimation to regularize the predictions
of the pseudo-labeling branch, which in turn guides contrastive learning
through self-balanced knowledge transfer and a proposed novel contrastive loss.
We compare BaCon with state-of-the-art methods from two closely related fields:
imbalanced semi-supervised learning and generalized category discovery. The
effectiveness of BaCon is demonstrated with superior performance over all
baselines and comprehensive analysis across various datasets. Our code is
publicly available.
</p>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01377" title="Abstract">arXiv:2310.01377</a> [<a href="/pdf/2310.01377" title="Download PDF">pdf</a>, <a href="/format/2310.01377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UltraFeedback: Boosting Language Models with High-quality Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+G">Ganqu Cui</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lifan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+N">Ning Ding</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+G">Guanming Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+Y">Yuan Ni</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+G">Guotong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Reinforcement learning from human feedback (RLHF) has become a pivot
technique in aligning large language models (LLMs) with human preferences. In
RLHF practice, preference data plays a crucial role in bridging human
proclivity and LLMs. However, the scarcity of diverse, naturalistic datasets of
human preferences on LLM outputs at scale poses a great challenge to RLHF as
well as feedback learning research within the open-source community. Current
preference datasets, either proprietary or limited in size and prompt variety,
result in limited RLHF adoption in open-source models and hinder further
exploration. In this study, we propose ULTRAFEEDBACK, a large-scale,
high-quality, and diversified preference dataset designed to overcome these
limitations and foster RLHF development. To create ULTRAFEEDBACK, we compile a
diverse array of instructions and models from multiple sources to produce
comparative data. We meticulously devise annotation instructions and employ
GPT-4 to offer detailed feedback in both numerical and textual forms.
ULTRAFEEDBACK establishes a reproducible and expandable preference data
construction pipeline, serving as a solid foundation for future RLHF and
feedback learning research. Utilizing ULTRAFEEDBACK, we train various models to
demonstrate its effectiveness, including the reward model UltraRM, chat
language model UltraLM-13B-PPO, and critique model UltraCM. Experimental
results indicate that our models outperform existing open-source models,
achieving top performance across multiple benchmarks. Our data and models are
available at https://github.com/thunlp/UltraFeedback.
</p>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01378" title="Abstract">arXiv:2310.01378</a> [<a href="/pdf/2310.01378" title="Download PDF">pdf</a>, <a href="/format/2310.01378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Grid Graph Reachability and Puzzle Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bofill%2C+M">Miquel Bofill</a>, 
<a href="/search/cs?searchtype=author&query=Borralleras%2C+C">Cristina Borralleras</a>, 
<a href="/search/cs?searchtype=author&query=Espasa%2C+J">Joan Espasa</a>, 
<a href="/search/cs?searchtype=author&query=Villaret%2C+M">Mateu Villaret</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Many puzzle video games, like Sokoban, involve moving some agent in a maze.
The reachable locations are usually apparent for a human player, and the
difficulty of the game is mainly related to performing actions on objects, such
as pushing (reachable) boxes. For this reason, the difficulty of a particular
level is often measured as the number of actions on objects, other than agent
walking, needed to find a solution. In this paper we study CP and SAT
approaches for solving these kind of problems. We review some reachability
encodings and propose a new one. We empirically show that the new encoding is
well-suited for solving puzzle problems in the planning as SAT paradigm,
especially when considering the execution of several actions in parallel.
</p>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01379" title="Abstract">arXiv:2310.01379</a> [<a href="/pdf/2310.01379" title="Download PDF">pdf</a>, <a href="/format/2310.01379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EXTRACTER: Efficient Texture Matching with Attention and Gradient  Enhancing for Large Scale Image Super Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reyes-Saldana%2C+E">Esteban Reyes-Saldana</a>, 
<a href="/search/cs?searchtype=author&query=Rivera%2C+M">Mariano Rivera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent Reference-Based image super-resolution (RefSR) has improved SOTA deep
methods introducing attention mechanisms to enhance low-resolution images by
transferring high-resolution textures from a reference high-resolution image.
The main idea is to search for matches between patches using LR and Reference
image pair in a feature space and merge them using deep architectures. However,
existing methods lack the accurate search of textures. They divide images into
as many patches as possible, resulting in inefficient memory usage, and cannot
manage large images. Herein, we propose a deep search with a more efficient
memory usage that reduces significantly the number of image patches and finds
the $k$ most relevant texture match for each low-resolution patch over the
high-resolution reference patches, resulting in an accurate texture match. We
enhance the Super Resolution result adding gradient density information using a
simple residual architecture showing competitive metrics results: PSNR and
SSMI.
</p>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01380" title="Abstract">arXiv:2310.01380</a> [<a href="/pdf/2310.01380" title="Download PDF">pdf</a>, <a href="/format/2310.01380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pessimistic Nonlinear Least-Squares Value Iteration for Offline  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di%2C+Q">Qiwei Di</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Heyang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiafan He</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Offline reinforcement learning (RL), where the agent aims to learn the
optimal policy based on the data collected by a behavior policy, has attracted
increasing attention in recent years. While offline RL with linear function
approximation has been extensively studied with optimal results achieved under
certain assumptions, many works shift their interest to offline RL with
non-linear function approximation. However, limited works on offline RL with
non-linear function approximation have instance-dependent regret guarantees. In
this paper, we propose an oracle-efficient algorithm, dubbed Pessimistic
Nonlinear Least-Square Value Iteration (PNLSVI), for offline RL with non-linear
function approximation. Our algorithmic design comprises three innovative
components: (1) a variance-based weighted regression scheme that can be applied
to a wide range of function classes, (2) a subroutine for variance estimation,
and (3) a planning phase that utilizes a pessimistic value iteration approach.
Our algorithm enjoys a regret bound that has a tight dependency on the function
class complexity and achieves minimax optimal instance-dependent regret when
specialized to linear function approximation. Our work extends the previous
instance-dependent results within simpler function classes, such as linear and
differentiable function to a more general framework.
</p>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01381" title="Abstract">arXiv:2310.01381</a> [<a href="/pdf/2310.01381" title="Download PDF">pdf</a>, <a href="/format/2310.01381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benita%2C+R">Roi Benita</a>, 
<a href="/search/cs?searchtype=author&query=Elad%2C+M">Michael Elad</a>, 
<a href="/search/cs?searchtype=author&query=Keshet%2C+J">Joseph Keshet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Diffusion models have recently been shown to be relevant for high-quality
speech generation. Most work has been focused on generating spectrograms, and
as such, they further require a subsequent model to convert the spectrogram to
a waveform (i.e., a vocoder). This work proposes a diffusion probabilistic
end-to-end model for generating a raw speech waveform. The proposed model is
autoregressive, generating overlapping frames sequentially, where each frame is
conditioned on a portion of the previously generated one. Hence, our model can
effectively synthesize an unlimited speech duration while preserving
high-fidelity synthesis and temporal coherence. We implemented the proposed
model for unconditional and conditional speech generation, where the latter can
be driven by an input sequence of phonemes, amplitudes, and pitch values.
Working on the waveform directly has some empirical advantages. Specifically,
it allows the creation of local acoustic behaviors, like vocal fry, which makes
the overall waveform sounds more natural. Furthermore, the proposed diffusion
model is stochastic and not deterministic; therefore, each inference generates
a slightly different waveform variation, enabling abundance of valid
realizations. Experiments show that the proposed model generates speech with
superior quality compared with other state-of-the-art neural speech generation
systems.
</p>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01382" title="Abstract">arXiv:2310.01382</a> [<a href="/pdf/2310.01382" title="Download PDF">pdf</a>, <a href="/format/2310.01382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compressing LLMs: The Truth is Rarely Pure and Never Simple
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaiswal%2C+A">Ajay Jaiswal</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+Z">Zhe Gan</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xianzhi Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bowen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yinfei Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite their remarkable achievements, modern Large Language Models (LLMs)
encounter exorbitant computational and memory footprints. Recently, several
works have shown significant success in training-free and data-free compression
(pruning and quantization) of LLMs achieving 50-60% sparsity and reducing the
bit-width down to 3 or 4 bits per weight, with negligible perplexity
degradation over the uncompressed baseline. As recent research efforts are
focused on developing increasingly sophisticated compression methods, our work
takes a step back, and re-evaluates the effectiveness of existing SoTA
compression methods, which rely on a fairly simple and widely questioned
metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive
Compressed LLM BenchmarK (LLM-KICK), a collection of carefully-curated tasks to
re-define the evaluation protocol for compressed LLMs, which have significant
alignment with their dense counterparts, and perplexity fail to capture subtle
change in their true capabilities. LLM-KICK unveils many favorable merits and
unfortunate plights of current SoTA compression methods: all pruning methods
suffer significant performance degradation, sometimes at trivial sparsity
ratios (e.g., 25-30%), and fail for N:M sparsity on knowledge-intensive tasks;
current quantization methods are more successful than pruning; yet, pruned LLMs
even at $\geq 50$% sparsity are robust in-context retrieval and summarization
systems; among others. LLM-KICK is designed to holistically access compressed
LLMs' ability for language understanding, reasoning, generation, in-context
retrieval, in-context summarization, etc. We hope our study can foster the
development of better LLM compression methods. All our related codes are planed
to be open-sourced.
</p>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01385" title="Abstract">arXiv:2310.01385</a> [<a href="/pdf/2310.01385" title="Download PDF">pdf</a>, <a href="/format/2310.01385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature-Driven Strategies for Trading Wind Power and Hydrogen
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Helgren%2C+E">Emil Helgren</a>, 
<a href="/search/eess?searchtype=author&query=Kazempour%2C+J">Jalal Kazempour</a>, 
<a href="/search/eess?searchtype=author&query=Mitridati%2C+L">Lesia Mitridati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper develops a feature-driven model for hybrid power plants, enabling
them to exploit available contextual information such as historical forecasts
of day-ahead prices and wind power, and make optimal wind power and hydrogen
trading decisions in the day-ahead stage. For that, we develop different
variations of feature-driven linear policies. In addition, we propose a
real-time adjustment strategy for hydrogen production. Our numerical results
show that the final profit obtained from our proposed feature-driven trading
mechanism in the day-ahead stage together with the real-time adjustment
strategy is very close to that in an ideal benchmark with perfect information.
</p>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01386" title="Abstract">arXiv:2310.01386</a> [<a href="/pdf/2310.01386" title="Download PDF">pdf</a>, <a href="/format/2310.01386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Who is ChatGPT? Benchmarking LLMs&#x27; Psychological Portrayal Using  PsychoBench
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jen-tse Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+E+J">Eric John Li</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+M+H">Man Ho Lam</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shujie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Youliang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+W">Wenxiang Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhaopeng Tu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+M+R">Michael R. Lyu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have recently showcased their remarkable
capacities, not only in natural language processing tasks but also across
diverse domains such as clinical medicine, legal consultation, and education.
LLMs become more than mere applications, evolving into assistants capable of
addressing diverse user requests. This narrows the distinction between human
beings and artificial intelligence agents, raising intriguing questions
regarding the potential manifestation of personalities, temperaments, and
emotions within LLMs. In this paper, we propose a framework, PsychoBench, for
evaluating diverse psychological aspects of LLMs. Comprising thirteen scales
commonly used in clinical psychology, PsychoBench further classifies these
scales into four distinct categories: personality traits, interpersonal
relationships, motivational tests, and emotional abilities. Our study examines
five popular models, namely \texttt{text-davinci-003}, ChatGPT, GPT-4,
LLaMA-2-7b, and LLaMA-2-13b. Additionally, we employ a jailbreak approach to
bypass the safety alignment protocols and test the intrinsic natures of LLMs.
We have made PsychoBench openly accessible via
\url{https://github.com/CUHK-ARISE/PsychoBench}.
</p>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01387" title="Abstract">arXiv:2310.01387</a> [<a href="/pdf/2310.01387" title="Download PDF">pdf</a>, <a href="/format/2310.01387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> It&#x27;s MBR All the Way Down: Modern Generation Techniques Through the Lens  of Minimum Bayes Risk
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bertsch%2C+A">Amanda Bertsch</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+A">Alex Xie</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>, 
<a href="/search/cs?searchtype=author&query=Gormley%2C+M+R">Matthew R. Gormley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a
machine learning system based not on the output with the highest probability,
but the output with the lowest risk (expected error) among multiple candidates.
It is a simple but powerful method: for an additional cost at inference time,
MBR provides reliable several-point improvements across metrics for a wide
variety of tasks without any additional data or training. Despite this, MBR is
not frequently applied in NLP works, and knowledge of the method itself is
limited. We first provide an introduction to the method and the recent
literature. We show that several recent methods that do not reference MBR can
be written as special cases of MBR; this reformulation provides additional
theoretical justification for the performance of these methods, explaining some
results that were previously only empirical. We provide theoretical and
empirical results about the effectiveness of various MBR variants and make
concrete recommendations for the application of MBR in NLP models, including
future directions in this area.
</p>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01390" title="Abstract">arXiv:2310.01390</a> [<a href="/pdf/2310.01390" title="Download PDF">pdf</a>, <a href="/format/2310.01390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of COVID-19 on Chronic Pain: Multidimensional Clustering  Reveals Deep Insights into Spinal Cord Stimulation Patients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berger%2C+S">Sara Berger</a>, 
<a href="/search/cs?searchtype=author&query=Agurto%2C+C">Carla Agurto</a>, 
<a href="/search/cs?searchtype=author&query=Cecchi%2C+G">Guillermo Cecchi</a>, 
<a href="/search/cs?searchtype=author&query=Eyigoz%2C+E">Elif Eyigoz</a>, 
<a href="/search/cs?searchtype=author&query=Hershey%2C+B">Brad Hershey</a>, 
<a href="/search/cs?searchtype=author&query=Lechleiter%2C+K">Kristen Lechleiter</a>, 
<a href="/search/cs?searchtype=author&query=NAVITAS%2FENVISION+studies+physician+author+group">NAVITAS/ENVISION studies physician author group</a>, 
<a href="/search/cs?searchtype=author&query=Huynh%2C+D">Dat Huynh</a>, 
<a href="/search/cs?searchtype=author&query=McDonald%2C+M">Matt McDonald</a>, 
<a href="/search/cs?searchtype=author&query=Rogers%2C+J+L">Jeffrey L Rogers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">The emergence of COVID-19 offered a unique opportunity to study chronic pain
patients as they responded to sudden changes in social environments, increased
community stress, and reduced access to care. We report findings from n=70
Spinal Cord Stimulation (SCS) patients before and during initial pandemic
stages resulting from advances in home monitoring and artificial intelligence
that produced novel insights despite pandemic-related disruptions. From a
multi-dimensional array of frequently monitored signals-including mobility,
sleep, voice, and psychological assessments-we found that while the overall
patient cohort appeared unaffected by the pandemic onset, patients had
significantly different individual experiences. Three distinct patient
responses (sub-cohorts) were revealed, those with: worsened pain, reduced
activities, or improved quality of life. Remarkably, none of the specific
measures by themselves were significantly affected; instead, it was their
synergy that exposed the effects elicited by the pandemic onset. Partial
correlations illustrating linked dimensions by sub-cohort during the pandemic
and those associations were different for each sub-cohort before COVID-19,
suggesting that daily at-home tele-monitoring of chronic conditions may reveal
novel patient types. This work highlights the opportunities afforded by
applying modern analytic techniques to more holistic and longitudinal patient
outcomes, which might aid clinicians in making more informed treatment
decisions in the future.
</p>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01393" title="Abstract">arXiv:2310.01393</a> [<a href="/pdf/2310.01393" title="Download PDF">pdf</a>, <a href="/format/2310.01393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DST-Det: Simple Dynamic Self-Training for Open-Vocabulary Object  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shilin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiangtai Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Size Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yining Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+G">Guangliang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+Y">Yunhai Tong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Loy%2C+C+C">Chen Change Loy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Open-vocabulary object detection (OVOD) aims to detect the objects beyond the
set of categories observed during training. This work presents a simple yet
effective strategy that leverages the zero-shot classification ability of
pre-trained vision-language models (VLM), such as CLIP, to classify proposals
for all possible novel classes directly. Unlike previous works that ignore
novel classes during training and rely solely on the region proposal network
(RPN) for novel object detection, our method selectively filters proposals
based on specific design criteria. The resulting sets of identified proposals
serve as pseudo-labels for novel classes during the training phase. It enables
our self-training strategy to improve the recall and accuracy of novel classes
in a self-training manner without requiring additional annotations or datasets.
We further propose a simple offline pseudo-label generation strategy to refine
the object detector. Empirical evaluations on three datasets, including LVIS,
V3Det, and COCO, demonstrate significant improvements over the baseline
performance without incurring additional parameters or computational costs
during inference. In particular, compared with previous F-VLM, our method
achieves a 1.7-2.0% improvement on LVIS dataset and 2.3-3.8% improvement on the
recent challenging V3Det dataset. Our method also boosts the strong baseline by
6% mAP on COCO. The code and models will be publicly available at
https://github.com/xushilin1/dst-det.
</p>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01395" title="Abstract">arXiv:2310.01395</a> [<a href="/pdf/2310.01395" title="Download PDF">pdf</a>, <a href="/format/2310.01395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Requirements&#x27; Characteristics: How do they Impact on Project Budget in a  Systems Engineering Context?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatzipetrou%2C+P">Panagiota Chatzipetrou</a>, 
<a href="/search/cs?searchtype=author&query=Unterkalmsteiner%2C+M">Michael Unterkalmsteiner</a>, 
<a href="/search/cs?searchtype=author&query=Gorschek%2C+T">Tony Gorschek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SEAA 2019: 260-267
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Background: Requirements engineering is of a principal importance when
starting a new project. However, the number of the requirements involved in a
single project can reach up to thousands. Controlling and assuring the quality
of natural language requirements (NLRs), in these quantities, is challenging.
Aims: In a field study, we investigated with the Swedish Transportation Agency
(STA) to what extent the characteristics of requirements had an influence on
change requests and budget changes in the project. Method: We choose the
following models to characterize system requirements formulated in natural
language: Concern-based Model of Requirements (CMR), Requirements Abstractions
Model (RAM) and Software-Hardware model (SHM). The classification of the NLRs
was conducted by the three authors. The robust statistical measure Fleiss'
Kappa was used to verify the reliability of the results. We used descriptive
statistics, contingency tables, results from the Chi-Square test of association
along with post hoc tests. Finally, a multivariate statistical technique,
Correspondence analysis was used in order to provide a means of displaying a
set of requirements in two-dimensional graphical form. Results: The results
showed that software requirements are associated with less budget cost than
hardware requirements. Moreover, software requirements tend to stay open for a
longer period indicating that they are "harder" to handle. Finally, the more
discussion or interaction on a change request can lower the actual estimated
change request cost. Conclusions: The results lead us to a need to further
investigate the reasons why the software requirements are treated differently
from the hardware requirements, interview the project managers, understand
better the way those requirements are formulated and propose effective ways of
Software management.
</p>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01396" title="Abstract">arXiv:2310.01396</a> [<a href="/pdf/2310.01396" title="Download PDF">pdf</a>, <a href="/format/2310.01396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Learning Based Scheme for Fair Timeliness in Sparse Gossip Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitra%2C+P">Purbesh Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Ulukus%2C+S">Sennur Ulukus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA); Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">We consider a gossip network, consisting of $n$ nodes, which tracks the
information at a source. The source updates its information with a Poisson
arrival process and also sends updates to the nodes in the network. The nodes
themselves can exchange information among themselves to become as timely as
possible. However, the network structure is sparse and irregular, i.e., not
every node is connected to every other node in the network, rather, the order
of connectivity is low, and varies across different nodes. This asymmetry of
the network implies that the nodes in the network do not perform equally in
terms of timelines. Due to the gossiping nature of the network, some nodes are
able to track the source very timely, whereas, some nodes fall behind versions
quite often. In this work, we investigate how the rate-constrained source
should distribute its update rate across the network to maintain fairness
regarding timeliness, i.e., the overall worst case performance of the network
can be minimized. Due to the continuous search space for optimum rate
allocation, we formulate this problem as a continuum-armed bandit problem and
employ Gaussian process based Bayesian optimization to meet a trade-off between
exploration and exploitation sequentially.
</p>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01400" title="Abstract">arXiv:2310.01400</a> [<a href="/pdf/2310.01400" title="Download PDF">pdf</a>, <a href="/format/2310.01400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequential Data Generation with Groupwise Diffusion Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sangyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+G">Gayoung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyunsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Uh%2C+Y">Youngjung Uh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present the Groupwise Diffusion Model (GDM), which divides data into
multiple groups and diffuses one group at one time interval in the forward
diffusion process. GDM generates data sequentially from one group at one time
interval, leading to several interesting properties. First, as an extension of
diffusion models, GDM generalizes certain forms of autoregressive models and
cascaded diffusion models. As a unified framework, GDM allows us to investigate
design choices that have been overlooked in previous works, such as
data-grouping strategy and order of generation. Furthermore, since one group of
the initial noise affects only a certain group of the generated data, latent
space now possesses group-wise interpretable meaning. We can further extend GDM
to the frequency domain where the forward process sequentially diffuses each
group of frequency components. Dividing the frequency bands of the data as
groups allows the latent variables to become a hierarchical representation
where individual groups encode data at different levels of abstraction. We
demonstrate several applications of such representation including
disentanglement of semantic attributes, image editing, and generating
variations.
</p>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01401" title="Abstract">arXiv:2310.01401</a> [<a href="/pdf/2310.01401" title="Download PDF">pdf</a>, <a href="/format/2310.01401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yiming Xie</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Huaizu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Gkioxari%2C+G">Georgia Gkioxari</a>, 
<a href="/search/cs?searchtype=author&query=Straub%2C+J">Julian Straub</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023. Project page: <a href="https://ymingxie.github.io/parq">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present PARQ - a multi-view 3D object detector with transformer and
pixel-aligned recurrent queries. Unlike previous works that use learnable
features or only encode 3D point positions as queries in the decoder, PARQ
leverages appearance-enhanced queries initialized from reference points in 3D
space and updates their 3D location with recurrent cross-attention operations.
Incorporating pixel-aligned features and cross attention enables the model to
encode the necessary 3D-to-2D correspondences and capture global contextual
information of the input images. PARQ outperforms prior best methods on the
ScanNet and ARKitScenes datasets, learns and detects faster, is more robust to
distribution shifts in reference points, can leverage additional input views
without retraining, and can adapt inference compute by changing the number of
recurrent iterations.
</p>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01403" title="Abstract">arXiv:2310.01403</a> [<a href="/pdf/2310.01403" title="Download PDF">pdf</a>, <a href="/format/2310.01403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Size Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lumin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Sheng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiangtai Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wentao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Loy%2C+C+C">Chen Change Loy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Open-vocabulary dense prediction tasks including object detection and image
segmentation have been advanced by the success of Contrastive Language-Image
Pre-training (CLIP). CLIP models, particularly those incorporating vision
transformers (ViTs), have exhibited remarkable generalization ability in
zero-shot image classification. However, when transferring the vision-language
alignment of CLIP from global image representation to local region
representation for the open-vocabulary dense prediction tasks, CLIP ViTs suffer
from the domain shift from full images to local image regions. In this paper,
we embark on an in-depth analysis of the region-language alignment in CLIP
models, which is essential for downstream open-vocabulary dense prediction
tasks. Subsequently, we propose an approach named CLIPSelf, which adapts the
image-level recognition ability of CLIP ViT to local image regions without
needing any region-text pairs. CLIPSelf empowers ViTs to distill itself by
aligning a region representation extracted from its dense feature map with the
image-level representation of the corresponding image crop. With the enhanced
CLIP ViTs, we achieve new state-of-the-art performance on open-vocabulary
object detection, semantic segmentation, and panoptic segmentation across
various benchmarks. Models and code will be available at
https://github.com/wusize/CLIPSelf.
</p>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01404" title="Abstract">arXiv:2310.01404</a> [<a href="/pdf/2310.01404" title="Download PDF">pdf</a>, <a href="/format/2310.01404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> H-InDex: Visual Reinforcement Learning with Hand-Informed  Representations for Dexterous Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ze%2C+Y">Yanjie Ze</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuyao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+R">Ruizhe Shi</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+J">Jiaxin Qin</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhecheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiashun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huazhe Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. Code and videos: <a href="https://yanjieze.com/H-InDex">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
<p class="mathjax">Human hands possess remarkable dexterity and have long served as a source of
inspiration for robotic manipulation. In this work, we propose a human
$\textbf{H}$and$\textbf{-In}$formed visual representation learning framework to
solve difficult $\textbf{Dex}$terous manipulation tasks ($\textbf{H-InDex}$)
with reinforcement learning. Our framework consists of three stages: (i)
pre-training representations with 3D human hand pose estimation, (ii) offline
adapting representations with self-supervised keypoint detection, and (iii)
reinforcement learning with exponential moving average BatchNorm. The last two
stages only modify $0.36\%$ parameters of the pre-trained representation in
total, ensuring the knowledge from pre-training is maintained to the full
extent. We empirically study 12 challenging dexterous manipulation tasks and
find that H-InDex largely surpasses strong baseline methods and the recent
visual foundation models for motor control. Code is available at
https://yanjieze.com/H-InDex .
</p>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01405" title="Abstract">arXiv:2310.01405</a> [<a href="/pdf/2310.01405" title="Download PDF">pdf</a>, <a href="/format/2310.01405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representation Engineering: A Top-Down Approach to AI Transparency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+A">Andy Zou</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+L">Long Phan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sarah Chen</a>, 
<a href="/search/cs?searchtype=author&query=Campbell%2C+J">James Campbell</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Phillip Guo</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+R">Richard Ren</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+A">Alexander Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+X">Xuwang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Mazeika%2C+M">Mantas Mazeika</a>, 
<a href="/search/cs?searchtype=author&query=Dombrowski%2C+A">Ann-Kathrin Dombrowski</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+S">Shashwat Goel</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Nathaniel Li</a>, 
<a href="/search/cs?searchtype=author&query=Byun%2C+M+J">Michael J. Byun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mallen%2C+A">Alex Mallen</a>, 
<a href="/search/cs?searchtype=author&query=Basart%2C+S">Steven Basart</a>, 
<a href="/search/cs?searchtype=author&query=Koyejo%2C+S">Sanmi Koyejo</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dawn Song</a>, 
<a href="/search/cs?searchtype=author&query=Fredrikson%2C+M">Matt Fredrikson</a>, 
<a href="/search/cs?searchtype=author&query=Kolter%2C+J+Z">J. Zico Kolter</a>, 
<a href="/search/cs?searchtype=author&query=Hendrycks%2C+D">Dan Hendrycks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/andyzoujm/representation-engineering">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY)

</div>
<p class="mathjax">In this paper, we identify and characterize the emerging area of
representation engineering (RepE), an approach to enhancing the transparency of
AI systems that draws on insights from cognitive neuroscience. RepE places
population-level representations, rather than neurons or circuits, at the
center of analysis, equipping us with novel methods for monitoring and
manipulating high-level cognitive phenomena in deep neural networks (DNNs). We
provide baselines and an initial analysis of RepE techniques, showing that they
offer simple yet effective solutions for improving our understanding and
control of large language models. We showcase how these methods can provide
traction on a wide range of safety-relevant problems, including honesty,
harmlessness, power-seeking, and more, demonstrating the promise of top-down
transparency research. We hope that this work catalyzes further exploration of
RepE and fosters advancements in the transparency and safety of AI systems.
</p>
</div>
</dd>
<dt><a name="item693">[693]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01406" title="Abstract">arXiv:2310.01406</a> [<a href="/pdf/2310.01406" title="Download PDF">pdf</a>, <a href="/format/2310.01406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HumanNorm: Learning Normal Diffusion Model for High-quality and  Realistic 3D Human Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+R">Ruizhi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongwen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Ying Feng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yebin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qing Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The project page of HumanNorm is <a href="https://humannorm.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent text-to-3D methods employing diffusion models have made significant
advancements in 3D human generation. However, these approaches face challenges
due to the limitations of the text-to-image diffusion model, which lacks an
understanding of 3D structures. Consequently, these methods struggle to achieve
high-quality human generation, resulting in smooth geometry and cartoon-like
appearances. In this paper, we observed that fine-tuning text-to-image
diffusion models with normal maps enables their adaptation into text-to-normal
diffusion models, which enhances the 2D perception of 3D geometry while
preserving the priors learned from large-scale datasets. Therefore, we propose
HumanNorm, a novel approach for high-quality and realistic 3D human generation
by learning the normal diffusion model including a normal-adapted diffusion
model and a normal-aligned diffusion model. The normal-adapted diffusion model
can generate high-fidelity normal maps corresponding to prompts with
view-dependent text. The normal-aligned diffusion model learns to generate
color images aligned with the normal maps, thereby transforming physical
geometry details into realistic appearance. Leveraging the proposed normal
diffusion model, we devise a progressive geometry generation strategy and
coarse-to-fine texture generation strategy to enhance the efficiency and
robustness of 3D human generation. Comprehensive experiments substantiate our
method's ability to generate 3D humans with intricate geometry and realistic
appearances, significantly outperforming existing text-to-3D methods in both
geometry and texture quality. The project page of HumanNorm is
https://humannorm.github.io/.
</p>
</div>
</dd>
<dt><a name="item694">[694]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01407" title="Abstract">arXiv:2310.01407</a> [<a href="/pdf/2310.01407" title="Download PDF">pdf</a>, <a href="/format/2310.01407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Diffusion Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mei%2C+K">Kangfu Mei</a>, 
<a href="/search/cs?searchtype=author&query=Delbracio%2C+M">Mauricio Delbracio</a>, 
<a href="/search/cs?searchtype=author&query=Talebi%2C+H">Hossein Talebi</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhengzhong Tu</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+V+M">Vishal M. Patel</a>, 
<a href="/search/cs?searchtype=author&query=Milanfar%2C+P">Peyman Milanfar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Generative diffusion models provide strong priors for text-to-image
generation and thereby serve as a foundation for conditional generation tasks
such as image editing, restoration, and super-resolution. However, one major
limitation of diffusion models is their slow sampling time. To address this
challenge, we present a novel conditional distillation method designed to
supplement the diffusion priors with the help of image conditions, allowing for
conditional sampling with very few steps. We directly distill the unconditional
pre-training in a single stage through joint-learning, largely simplifying the
previous two-stage procedures that involve both distillation and conditional
finetuning separately. Furthermore, our method enables a new
parameter-efficient distillation mechanism that distills each task with only a
small number of additional parameters combined with the shared frozen
unconditional backbone. Experiments across multiple tasks including
super-resolution, image editing, and depth-to-image generation demonstrate that
our method outperforms existing distillation techniques for the same sampling
time. Notably, our method is the first distillation strategy that can match the
performance of the much slower fine-tuned conditional diffusion models.
</p>
</div>
</dd>
<dt><a name="item695">[695]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01408" title="Abstract">arXiv:2310.01408</a> [<a href="/pdf/2310.01408" title="Download PDF">pdf</a>, <a href="/format/2310.01408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Animal Imitator: Agile Locomotion with Versatile Motion  Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Ruihan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuoqun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jianhan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chongyi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Q">Quan Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Further details and supportive media can be found at our project site: <a href="https://rchalyang.github.io/VIM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The agility of animals, particularly in complex activities such as running,
turning, jumping, and backflipping, stands as an exemplar for robotic system
design. Transferring this suite of behaviors to legged robotic systems
introduces essential inquiries: How can a robot be trained to learn multiple
locomotion behaviors simultaneously? How can the robot execute these tasks with
a smooth transition? And what strategies allow for the integrated application
of these skills? This paper introduces the Versatile Instructable Motion prior
(VIM) - a Reinforcement Learning framework designed to incorporate a range of
agile locomotion tasks suitable for advanced robotic applications. Our
framework enables legged robots to learn diverse agile low-level skills by
imitating animal motions and manually designed motions with Functionality
reward and Stylization reward. While the Functionality reward guides the
robot's ability to adopt varied skills, the Stylization reward ensures
performance alignment with reference motions. Our evaluations of the VIM
framework span both simulation environments and real-world deployment. To our
understanding, this is the first work that allows a robot to concurrently learn
diverse agile locomotion tasks using a singular controller. Further details and
supportive media can be found at our project site:
https://rchalyang.github.io/VIM .
</p>
</div>
</dd>
<dt><a name="item696">[696]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01410" title="Abstract">arXiv:2310.01410</a> [<a href="/pdf/2310.01410" title="Download PDF">pdf</a>, <a href="/format/2310.01410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEAP: Liberate Sparse-view 3D Modeling from Camera Poses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Hanwen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhenyu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yue Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qixing Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page <a href="https://hwjiang1510.github.io/LEAP/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Are camera poses necessary for multi-view 3D modeling? Existing approaches
predominantly assume access to accurate camera poses. While this assumption
might hold for dense views, accurately estimating camera poses for sparse views
is often elusive. Our analysis reveals that noisy estimated poses lead to
degraded performance for existing sparse-view 3D modeling methods. To address
this issue, we present LEAP, a novel pose-free approach, therefore challenging
the prevailing notion that camera poses are indispensable. LEAP discards
pose-based operations and learns geometric knowledge from data. LEAP is
equipped with a neural volume, which is shared across scenes and is
parameterized to encode geometry and texture priors. For each incoming scene,
we update the neural volume by aggregating 2D image features in a
feature-similarity-driven manner. The updated neural volume is decoded into the
radiance field, enabling novel view synthesis from any viewpoint. On both
object-centric and scene-level datasets, we show that LEAP significantly
outperforms prior methods when they employ predicted poses from
state-of-the-art pose estimators. Notably, LEAP performs on par with prior
approaches that use ground-truth poses while running $400\times$ faster than
PixelNeRF. We show LEAP generalizes to novel object categories and scenes, and
learns knowledge closely resembles epipolar geometry. Project page:
https://hwjiang1510.github.io/LEAP/
</p>
</div>
</dd>
<dt><a name="item697">[697]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01412" title="Abstract">arXiv:2310.01412</a> [<a href="/pdf/2310.01412" title="Download PDF">pdf</a>, <a href="/format/2310.01412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DriveGPT4: Interpretable End-to-end Autonomous Driving via Large  Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhenhua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yujia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K+K+Y">Kenneth K.Y. Wong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hengshuang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The project page is available at <a href="https://tonyxuqaq.github.io/projects/DriveGPT4/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">In the past decade, autonomous driving has experienced rapid development in
both academia and industry. However, its limited interpretability remains a
significant unsolved problem, severely hindering autonomous vehicle
commercialization and further development. Previous approaches utilizing small
language models have failed to address this issue due to their lack of
flexibility, generalization ability, and robustness. Recently, multimodal large
language models (LLMs) have gained considerable attention from the research
community for their capability to process and reason non-text data (e.g.,
images and videos) by text. In this paper, we present DriveGPT4, an
interpretable end-to-end autonomous driving system utilizing LLMs. DriveGPT4 is
capable of interpreting vehicle actions and providing corresponding reasoning,
as well as answering diverse questions posed by human users for enhanced
interaction. Additionally, DriveGPT4 predicts vehicle low-level control signals
in an end-to-end fashion. These capabilities stem from a customized visual
instruction tuning dataset specifically designed for autonomous driving. To the
best of our knowledge, DriveGPT4 is the first work focusing on interpretable
end-to-end autonomous driving. When evaluated on multiple tasks alongside
conventional methods and video understanding LLMs, DriveGPT4 demonstrates
superior qualitative and quantitative performance. Additionally, DriveGPT4 can
be generalized in a zero-shot fashion to accommodate more unseen scenarios. The
project page is available at https://tonyxuqaq.github.io/projects/DriveGPT4/ .
</p>
</div>
</dd>
<dt><a name="item698">[698]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01415" title="Abstract">arXiv:2310.01415</a> [<a href="/pdf/2310.01415" title="Download PDF">pdf</a>, <a href="/format/2310.01415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-Driver: Learning to Drive with GPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Jiageng Mao</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yuxi Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yue Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Robotics (cs.RO)

</div>
<p class="mathjax">We present a simple yet effective approach that can transform the OpenAI
GPT-3.5 model into a reliable motion planner for autonomous vehicles. Motion
planning is a core challenge in autonomous driving, aiming to plan a driving
trajectory that is safe and comfortable. Existing motion planners predominantly
leverage heuristic methods to forecast driving trajectories, yet these
approaches demonstrate insufficient generalization capabilities in the face of
novel and unseen driving scenarios. In this paper, we propose a novel approach
to motion planning that capitalizes on the strong reasoning capabilities and
generalization potential inherent to Large Language Models (LLMs). The
fundamental insight of our approach is the reformulation of motion planning as
a language modeling problem, a perspective not previously explored.
Specifically, we represent the planner inputs and outputs as language tokens,
and leverage the LLM to generate driving trajectories through a language
description of coordinate positions. Furthermore, we propose a novel
prompting-reasoning-finetuning strategy to stimulate the numerical reasoning
potential of the LLM. With this strategy, the LLM can describe highly precise
trajectory coordinates and also its internal decision-making process in natural
language. We evaluate our approach on the large-scale nuScenes dataset, and
extensive experiments substantiate the effectiveness, generalization ability,
and interpretability of our GPT-based motion planner. Code will be released
upon acceptance.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Tue,  3 Oct 23</h3>
<dl>
<dt><a name="item699">[699]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00018" title="Abstract">arXiv:2310.00018</a> (cross-list from quant-ph) [<a href="/pdf/2310.00018" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigation of factors regarding the effects of COVID-19 pandemic on  college students&#x27; depression by quantum annealer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Choi%2C+J">Junggu Choi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kim%2C+K">Kion Kim</a>, 
<a href="/search/quant-ph?searchtype=author&query=Park%2C+S">Soohyun Park</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hur%2C+J">Juyoen Hur</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yang%2C+H">Hyunjung Yang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kim%2C+Y">Younghoon Kim</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lee%2C+H">Hakbae Lee</a>, 
<a href="/search/quant-ph?searchtype=author&query=Han%2C+S">Sanghoon Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Diverse cases regarding the impact, with its related factors, of the COVID-19
pandemic on mental health have been reported in previous studies. College
student groups have been frequently selected as the target population in
previous studies because they are easily affected by pandemics. In this study,
multivariable datasets were collected from 751 college students based on the
complex relationships between various mental health factors. We utilized
quantum annealing (QA)-based feature selection algorithms that were executed by
commercial D-Wave quantum computers to determine the changes in the relative
importance of the associated factors before and after the pandemic.
Multivariable linear regression (MLR) and XGBoost models were also applied to
validate the QA-based algorithms. Based on the experimental results, we confirm
that QA-based algorithms have comparable capabilities in factor analysis
research to the MLR models that have been widely used in previous studies.
Furthermore, the performance of the QA-based algorithms was validated through
the important factor results from the algorithms. Pandemic-related factors
(e.g., confidence in the social system) and psychological factors (e.g.,
decision-making in uncertain situations) were more important in post-pandemic
conditions. We believe that our study will serve as a reference for researchers
studying similar topics.
</p>
</div>
</dd>
<dt><a name="item700">[700]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00027" title="Abstract">arXiv:2310.00027</a> (cross-list from stat.ML) [<a href="/pdf/2310.00027" title="Download PDF">pdf</a>, <a href="/ps/2310.00027" title="Download PostScript">ps</a>, <a href="/format/2310.00027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlabeled Out-Of-Domain Data Improves Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Saberi%2C+A+H">Amir Hossein Saberi</a>, 
<a href="/search/stat?searchtype=author&query=Najafi%2C+A">Amir Najafi</a>, 
<a href="/search/stat?searchtype=author&query=Heidari%2C+A">Alireza Heidari</a>, 
<a href="/search/stat?searchtype=author&query=Movasaghinia%2C+M+H">Mohammad Hosein Movasaghinia</a>, 
<a href="/search/stat?searchtype=author&query=Motahari%2C+A">Abolfazl Motahari</a>, 
<a href="/search/stat?searchtype=author&query=Khalaj%2C+B+H">Babak H. Khalaj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, no figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a novel framework for incorporating unlabeled data into
semi-supervised classification problems, where scenarios involving the
minimization of either i) adversarially robust or ii) non-robust loss functions
have been considered. Notably, we allow the unlabeled samples to deviate
slightly (in total variation sense) from the in-domain distribution. The core
idea behind our framework is to combine Distributionally Robust Optimization
(DRO) with self-supervised training. As a result, we also leverage efficient
polynomial-time algorithms for the training stage. From a theoretical
standpoint, we apply our framework on the classification problem of a mixture
of two Gaussians in $\mathbb{R}^d$, where in addition to the $m$ independent
and labeled samples from the true distribution, a set of $n$ (usually with
$n\gg m$) out of domain and unlabeled samples are gievn as well. Using only the
labeled data, it is known that the generalization error can be bounded by
$\propto\left(d/m\right)^{1/2}$. However, using our method on both isotropic
and non-isotropic Gaussian mixture models, one can derive a new set of
analytically explicit and non-asymptotic bounds which show substantial
improvement on the generalization error compared ERM. Our results underscore
two significant insights: 1) out-of-domain samples, even when unlabeled, can be
harnessed to narrow the generalization gap, provided that the true data
distribution adheres to a form of the "cluster assumption", and 2) the
semi-supervised learning paradigm can be regarded as a special case of our
framework when there are no distributional shifts. We validate our claims
through experiments conducted on a variety of synthetic and real-world
datasets.
</p>
</div>
</dd>
<dt><a name="item701">[701]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00049" title="Abstract">arXiv:2310.00049</a> (cross-list from hep-ph) [<a href="/pdf/2310.00049" title="Download PDF">pdf</a>, <a href="/format/2310.00049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EPiC-ly Fast Particle Cloud Generation with Flow-Matching and Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ph?searchtype=author&query=Buhmann%2C+E">Erik Buhmann</a>, 
<a href="/search/hep-ph?searchtype=author&query=Ewen%2C+C">Cedric Ewen</a>, 
<a href="/search/hep-ph?searchtype=author&query=Faroughy%2C+D+A">Darius A. Faroughy</a>, 
<a href="/search/hep-ph?searchtype=author&query=Golling%2C+T">Tobias Golling</a>, 
<a href="/search/hep-ph?searchtype=author&query=Kasieczka%2C+G">Gregor Kasieczka</a>, 
<a href="/search/hep-ph?searchtype=author&query=Leigh%2C+M">Matthew Leigh</a>, 
<a href="/search/hep-ph?searchtype=author&query=Qu%C3%A9tant%2C+G">Guillaume Qu&#xe9;tant</a>, 
<a href="/search/hep-ph?searchtype=author&query=Raine%2C+J+A">John Andrew Raine</a>, 
<a href="/search/hep-ph?searchtype=author&query=Sengupta%2C+D">Debajyoti Sengupta</a>, 
<a href="/search/hep-ph?searchtype=author&query=Shih%2C+D">David Shih</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Phenomenology (hep-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Jets at the LHC, typically consisting of a large number of highly correlated
particles, are a fascinating laboratory for deep generative modeling. In this
paper, we present two novel methods that generate LHC jets as point clouds
efficiently and accurately. We introduce \epcjedi, which combines
score-matching diffusion models with the Equivariant Point Cloud (EPiC)
architecture based on the deep sets framework. This model offers a much faster
alternative to previous transformer-based diffusion models without reducing the
quality of the generated jets. In addition, we introduce \epcfm, the first
permutation equivariant continuous normalizing flow (CNF) for particle cloud
generation. This model is trained with {\it flow-matching}, a scalable and
easy-to-train objective based on optimal transport that directly regresses the
vector fields connecting the Gaussian noise prior to the data distribution. Our
experiments demonstrate that \epcjedi and \epcfm both achieve state-of-the-art
performance on the top-quark JetNet datasets whilst maintaining fast generation
speed. Most notably, we find that the \epcfm model consistently outperforms all
the other generative models considered here across every metric. Finally, we
also introduce two new particle cloud performance metrics: the first based on
the Kullback-Leibler divergence between feature distributions, the second is
the negative log-posterior of a multi-model ParticleNet classifier.
</p>
</div>
</dd>
<dt><a name="item702">[702]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00052" title="Abstract">arXiv:2310.00052</a> (cross-list from astro-ph.IM) [<a href="/pdf/2310.00052" title="Download PDF">pdf</a>, <a href="/format/2310.00052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI ensemble for signal detection of higher order gravitational wave  modes of quasi-circular, spinning, non-precessing binary black hole mergers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Tian%2C+M">Minyang Tian</a>, 
<a href="/search/astro-ph?searchtype=author&query=Huerta%2C+E+A">E. A. Huerta</a>, 
<a href="/search/astro-ph?searchtype=author&query=Zheng%2C+H">Huihuo Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 2 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Artificial Intelligence (cs.AI); General Relativity and Quantum Cosmology (gr-qc)

</div>
<p class="mathjax">We introduce spatiotemporal-graph models that concurrently process data from
the twin advanced LIGO detectors and the advanced Virgo detector. We trained
these AI classifiers with 2.4 million \texttt{IMRPhenomXPHM} waveforms that
describe quasi-circular, spinning, non-precessing binary black hole mergers
with component masses $m_{\{1,2\}}\in[3M_\odot, 50 M_\odot]$, and individual
spins $s^z_{\{1,2\}}\in[-0.9, 0.9]$; and which include the $(\ell, |m|) = \{(2,
2), (2, 1), (3, 3), (3, 2), (4, 4)\}$ modes, and mode mixing effects in the
$\ell = 3, |m| = 2$ harmonics. We trained these AI classifiers within 22 hours
using distributed training over 96 NVIDIA V100 GPUs in the Summit
supercomputer. We then used transfer learning to create AI predictors that
estimate the total mass of potential binary black holes identified by all AI
classifiers in the ensemble. We used this ensemble, 3 AI classifiers and 2
predictors, to process a year-long test set in which we injected 300,000
signals. This year-long test set was processed within 5.19 minutes using 1024
NVIDIA A100 GPUs in the Polaris supercomputer (for AI inference) and 128 CPU
nodes in the ThetaKNL supercomputer (for post-processing of noise triggers),
housed at the Argonne Leadership Supercomputing Facility. These studies
indicate that our AI ensemble provides state-of-the-art signal detection
accuracy, and reports 2 misclassifications for every year of searched data.
This is the first AI ensemble designed to search for and find higher order
gravitational wave mode signals.
</p>
</div>
</dd>
<dt><a name="item703">[703]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00060" title="Abstract">arXiv:2310.00060</a> (cross-list from q-bio.TO) [<a href="/pdf/2310.00060" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Patient-specific computational forecasting of prostate cancer growth  during active surveillance using an imaging-informed biomechanistic model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Lorenzo%2C+G">Guillermo Lorenzo</a>, 
<a href="/search/q-bio?searchtype=author&query=Heiselman%2C+J+S">Jon S. Heiselman</a>, 
<a href="/search/q-bio?searchtype=author&query=Liss%2C+M+A">Michael A. Liss</a>, 
<a href="/search/q-bio?searchtype=author&query=Miga%2C+M+I">Michael I. Miga</a>, 
<a href="/search/q-bio?searchtype=author&query=Gomez%2C+H">Hector Gomez</a>, 
<a href="/search/q-bio?searchtype=author&query=Yankeelov%2C+T+E">Thomas E. Yankeelov</a>, 
<a href="/search/q-bio?searchtype=author&query=Reali%2C+A">Alessandro Reali</a>, 
<a href="/search/q-bio?searchtype=author&query=Hughes%2C+T+J+R">Thomas J. R. Hughes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Tissues and Organs (q-bio.TO)</span>; Computational Engineering, Finance, and Science (cs.CE); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Active surveillance (AS) is a suitable management option for newly-diagnosed
prostate cancer (PCa), which usually presents low to intermediate clinical
risk. Patients enrolled in AS have their tumor closely monitored via
longitudinal multiparametric magnetic resonance imaging (mpMRI), serum
prostate-specific antigen tests, and biopsies. Hence, the patient is prescribed
treatment when these tests identify progression to higher-risk PCa. However,
current AS protocols rely on detecting tumor progression through direct
observation according to standardized monitoring strategies. This approach
limits the design of patient-specific AS plans and may lead to the late
detection and treatment of tumor progression. Here, we propose to address these
issues by leveraging personalized computational predictions of PCa growth. Our
forecasts are obtained with a spatiotemporal biomechanistic model informed by
patient-specific longitudinal mpMRI data. Our results show that our predictive
technology can represent and forecast the global tumor burden for individual
patients, achieving concordance correlation coefficients ranging from 0.93 to
0.99 across our cohort (n=7). Additionally, we identify a model-based biomarker
of higher-risk PCa: the mean proliferation activity of the tumor (p=0.041).
Using logistic regression, we construct a PCa risk classifier based on this
biomarker that achieves an area under the receiver operating characteristic
curve of 0.83. We further show that coupling our tumor forecasts with this PCa
risk classifier enables the early identification of PCa progression to
higher-risk disease by more than one year. Thus, we posit that our predictive
technology constitutes a promising clinical decision-making tool to design
personalized AS plans for PCa patients.
</p>
</div>
</dd>
<dt><a name="item704">[704]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00064" title="Abstract">arXiv:2310.00064</a> (cross-list from math.CO) [<a href="/pdf/2310.00064" title="Download PDF">pdf</a>, <a href="/format/2310.00064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Phases of Unique Sink Orientations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Borzechowski%2C+M">Michaela Borzechowski</a>, 
<a href="/search/math?searchtype=author&query=Weber%2C+S">Simon Weber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">A unique sink orientation (USO) is an orientation of the $n$-dimensional
hypercube graph such that every non-empty face contains a unique sink. Schurr
showed that given any $n$-dimensional USO and any dimension $i$, the set of
edges $E_i$ in that dimension can be decomposed into equivalence classes
(so-called phases), such that flipping the orientation of a subset $S$ of $E_i$
yields another USO if and only if $S$ is a union of a set of these phases. In
this paper we prove various results on the structure of phases. Using these
results, we show that all phases can be computed in $O(3^n)$ time,
significantly improving upon the previously known $O(4^n)$ trivial algorithm.
Furthermore, we show that given a boolean circuit of size $poly(n)$ succinctly
encoding an $n$-dimensional (acyclic) USO, it is PSPACE-complete to determine
whether two given edges are in the same phase. The problem is thus equally
difficult as determining whether the hypercube orientation encoded by a given
circuit is an acyclic USO [G\"artner and Thomas, STACS'15].
</p>
</div>
</dd>
<dt><a name="item705">[705]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00066" title="Abstract">arXiv:2310.00066</a> (cross-list from cond-mat.dis-nn) [<a href="/pdf/2310.00066" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal credit assignment for one-shot learning utilizing a phase  transition material
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Galloni%2C+A+R">Alessandro R. Galloni</a>, 
<a href="/search/cond-mat?searchtype=author&query=Yuan%2C+Y">Yifan Yuan</a>, 
<a href="/search/cond-mat?searchtype=author&query=Zhu%2C+M">Minning Zhu</a>, 
<a href="/search/cond-mat?searchtype=author&query=Yu%2C+H">Haoming Yu</a>, 
<a href="/search/cond-mat?searchtype=author&query=Bisht%2C+R+S">Ravindra S. Bisht</a>, 
<a href="/search/cond-mat?searchtype=author&query=Wu%2C+C+M">Chung-Tse Michael Wu</a>, 
<a href="/search/cond-mat?searchtype=author&query=Grienberger%2C+C">Christine Grienberger</a>, 
<a href="/search/cond-mat?searchtype=author&query=Ramanathan%2C+S">Shriram Ramanathan</a>, 
<a href="/search/cond-mat?searchtype=author&query=Milstein%2C+A+D">Aaron D. Milstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 5 figures, 6 supplementary figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Disordered Systems and Neural Networks (cond-mat.dis-nn)</span>; Materials Science (cond-mat.mtrl-sci); Hardware Architecture (cs.AR); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Design of hardware based on biological principles of neuronal computation and
plasticity in the brain is a leading approach to realizing energy- and
sample-efficient artificial intelligence and learning machines. An important
factor in selection of the hardware building blocks is the identification of
candidate materials with physical properties suitable to emulate the large
dynamic ranges and varied timescales of neuronal signaling. Previous work has
shown that the all-or-none spiking behavior of neurons can be mimicked by
threshold switches utilizing phase transitions. Here we demonstrate that
devices based on a prototypical metal-insulator-transition material, vanadium
dioxide (VO2), can be dynamically controlled to access a continuum of
intermediate resistance states. Furthermore, the timescale of their intrinsic
relaxation can be configured to match a range of biologically-relevant
timescales from milliseconds to seconds. We exploit these device properties to
emulate three aspects of neuronal analog computation: fast (~1 ms) spiking in a
neuronal soma compartment, slow (~100 ms) spiking in a dendritic compartment,
and ultraslow (~1 s) biochemical signaling involved in temporal credit
assignment for a recently discovered biological mechanism of one-shot learning.
Simulations show that an artificial neural network using properties of VO2
devices to control an agent navigating a spatial environment can learn an
efficient path to a reward in up to 4 fold fewer trials than standard methods.
The phase relaxations described in our study may be engineered in a variety of
materials, and can be controlled by thermal, electrical, or optical stimuli,
suggesting further opportunities to emulate biological learning.
</p>
</div>
</dd>
<dt><a name="item706">[706]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00071" title="Abstract">arXiv:2310.00071</a> (cross-list from astro-ph.IM) [<a href="/pdf/2310.00071" title="Download PDF">pdf</a>, <a href="/format/2310.00071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ShOpt.jl: A Julia Package for Empirical Point Spread Function  Characterization of JWST NIRCam Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Berman%2C+E">Edward Berman</a>, 
<a href="/search/astro-ph?searchtype=author&query=McCleary%2C+J">Jacqueline McCleary</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures; submitted to the Journal of Open Source Software
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">As astronomical data grows in volume and complexity, the scalability of
analysis software becomes increasingly important. At the same time,
astrophysics analysis software relies heavily on open-source contributions, so
languages and tools that prioritize both performance and readability are
especially valuable. Julia, with its just-in-time compiler and high level
syntax, offers a compelling alternative to traditional languages like Python or
C.
<br />In this paper, we outline ShOpt.jl, a new software package for point spread
function (PSF) characterization written in Julia. ShOpt.jl features a number of
performance optimizations, such as multithreading, the use of preconditioners,
and the implementation of the memory-limited Broyden-Fletcher-Goldfarb-Shanno
algorithm, as well as the flexibility to choose between principal component
analysis, an autoencoder, and analytic profiles for PSF characterization. As
observatories like the James Webb Space Telescope bring astrophysics into a new
era of wide-field, high-resolution imaging, the challenges of PSF modeling
become more pronounced. Tools like ShOpt.jl provide the community with a
scalable, efficient, and accurate solution to these challenges, while also
demonstrating the potential of Julia as a language that meets the demands of
modern astrophysical research.
</p>
</div>
</dd>
<dt><a name="item707">[707]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00102" title="Abstract">arXiv:2310.00102</a> (cross-list from math.AC) [<a href="/pdf/2310.00102" title="Download PDF">pdf</a>, <a href="/ps/2310.00102" title="Download PostScript">ps</a>, <a href="/format/2310.00102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Geometry of Minimum Distance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Pawlina%2C+J">John Pawlina</a>, 
<a href="/search/math?searchtype=author&query=Tohaneanu%2C+S">Stefan Tohaneanu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 4 pictures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Commutative Algebra (math.AC)</span>; Information Theory (cs.IT); Algebraic Geometry (math.AG)

</div>
<p class="mathjax">Let \({\mathbb K}\) be any field, let \(X\subset {\mathbb P}^{k-1}\) be a set
of \(n\) distinct \({\mathbb K}\)-rational points, and let \(a\geq 1\) be an
integer. In this paper we find lower bounds for the minimum distance \(d(X)_a\)
of the evaluation code of order \(a\) associated to \(X\). The first results
use \(\alpha(X)\), the \(\alpha\)-invariant of the defining ideal of \(X\), and
the bounds are true for any set \(X\). In another result we use \(s(X)\), the
minimum socle degree, to find a lower bound for the case when \(X\) is in
general linear position. In both situations we improve and generalize known
results.
</p>
</div>
</dd>
<dt><a name="item708">[708]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00110" title="Abstract">arXiv:2310.00110</a> (cross-list from stat.ML) [<a href="/pdf/2310.00110" title="Download PDF">pdf</a>, <a href="/format/2310.00110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient and Uncertainty Enhanced Sequential Sampling for Global Fit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=L%C3%A4mmle%2C+S">Sven L&#xe4;mmle</a>, 
<a href="/search/stat?searchtype=author&query=Bogoclu%2C+C">Can Bogoclu</a>, 
<a href="/search/stat?searchtype=author&query=Cremanns%2C+K">Kevin Cremanns</a>, 
<a href="/search/stat?searchtype=author&query=Roos%2C+D">Dirk Roos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Surrogate models based on machine learning methods have become an important
part of modern engineering to replace costly computer simulations. The data
used for creating a surrogate model are essential for the model accuracy and
often restricted due to cost and time constraints. Adaptive sampling strategies
have been shown to reduce the number of samples needed to create an accurate
model. This paper proposes a new sampling strategy for global fit called
Gradient and Uncertainty Enhanced Sequential Sampling (GUESS). The acquisition
function uses two terms: the predictive posterior uncertainty of the surrogate
model for exploration of unseen regions and a weighted approximation of the
second and higher-order Taylor expansion values for exploitation. Although
various sampling strategies have been proposed so far, the selection of a
suitable method is not trivial. Therefore, we compared our proposed strategy to
9 adaptive sampling strategies for global surrogate modeling, based on 26
different 1 to 8-dimensional deterministic benchmarks functions. Results show
that GUESS achieved on average the highest sample efficiency compared to other
surrogate-based strategies on the tested examples. An ablation study
considering the behavior of GUESS in higher dimensions and the importance of
surrogate choice is also presented.
</p>
</div>
</dd>
<dt><a name="item709">[709]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00121" title="Abstract">arXiv:2310.00121</a> (cross-list from quant-ph) [<a href="/pdf/2310.00121" title="Download PDF">pdf</a>, <a href="/format/2310.00121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Construction of a Circuit for the Simulation of a Hamiltonian with a  Tridiagonal Matrix Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Arseniev%2C+B">Boris Arseniev</a>, 
<a href="/search/quant-ph?searchtype=author&query=Guskov%2C+D">Dmitry Guskov</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sengupta%2C+R">Richik Sengupta</a>, 
<a href="/search/quant-ph?searchtype=author&query=Biamonte%2C+J">Jacob Biamonte</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zacharov%2C+I">Igor Zacharov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">The simulation of quantum systems is an area where quantum computers are
promised to achieve an exponential speedup over classical simulations.
State-of-the-art quantum algorithms for Hamiltonian simulation achieve this by
reducing the amount of oracle queries. Unfortunately, these predicted speedups
may be limited by a sub-optimal oracle implementation, thus limiting their use
in practical applications. In this paper we present a construction of a circuit
for simulation of Hamiltonians with a tridiagonal matrix representation. We
claim efficiency by estimating the resulting gate complexity. This is done by
determining all Pauli strings present in the decomposition of an arbitrary
tridiagonal matrix and dividing them into commuting sets. The union of these
sets has a cardinality exponentially smaller than that of the set of all Pauli
strings. Furthermore, the number of commuting sets grows logarithmically with
the size of the matrix. Additionally, our method for computing the
decomposition coefficients requires exponentially fewer multiplications
compared to the direct approach. Finally, we exemplify our method in the case
of the Hamiltonian of the one-dimensional wave equation and numerically show
the dependency of the number of gates on the number of qubits.
</p>
</div>
</dd>
<dt><a name="item710">[710]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00174" title="Abstract">arXiv:2310.00174</a> (cross-list from q-bio.BM) [<a href="/pdf/2310.00174" title="Download PDF">pdf</a>, <a href="/format/2310.00174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ADMET property prediction through combinations of molecular fingerprints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Notwell%2C+J+H">James H. Notwell</a>, 
<a href="/search/q-bio?searchtype=author&query=Wood%2C+M+W">Michael W. Wood</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">While investigating methods to predict small molecule potencies, we found
random forests or support vector machines paired with extended-connectivity
fingerprints (ECFP) consistently outperformed recently developed methods. A
detailed investigation into regression algorithms and molecular fingerprints
revealed gradient-boosted decision trees, particularly CatBoost, in conjunction
with a combination of ECFP, Avalon, and ErG fingerprints, as well as 200
molecular properties, to be most effective. Incorporating a graph neural
network fingerprint further enhanced performance. We successfully validated our
model across 22 Therapeutics Data Commons ADMET benchmarks. Our findings
underscore the significance of richer molecular representations for accurate
property prediction.
</p>
</div>
</dd>
<dt><a name="item711">[711]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00216" title="Abstract">arXiv:2310.00216</a> (cross-list from eess.SP) [<a href="/pdf/2310.00216" title="Download PDF">pdf</a>, <a href="/format/2310.00216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel U-Net Architecture for Denoising of Real-world Noise Corrupted  Phonocardiogram Signal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mukherjee%2C+A">Ayan Mukherjee</a>, 
<a href="/search/eess?searchtype=author&query=Banerjee%2C+R">Rohan Banerjee</a>, 
<a href="/search/eess?searchtype=author&query=Ghose%2C+A">Avik Ghose</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The bio-acoustic information contained within heart sound signals are
utilized by physicians world-wide for auscultation purpose. However, the heart
sounds are inherently susceptible to noise contamination. Various sources of
noises like lung sound, coughing, sneezing, and other background noises are
involved in such contamination. Such corruption of the heart sound signal often
leads to inconclusive or false diagnosis. To address this issue, we have
proposed a novel U-Net based deep neural network architecture for denoising of
phonocardiogram (PCG) signal in this paper. For the design, development and
validation of the proposed architecture, a novel approach of synthesizing
real-world noise corrupted PCG signals have been proposed. For the purpose, an
open-access real-world noise sample dataset and an open-access PCG dataset has
been utilized. The performance of the proposed denoising methodology has been
evaluated on the synthesized noisy PCG dataset. The performance of the proposed
algorithm has been compared with existing state-of-the-art (SoA) denoising
algorithms qualitatively and quantitatively. The proposed denoising technique
has shown improvement in performance as comparison to the SoAs.
</p>
</div>
</dd>
<dt><a name="item712">[712]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00220" title="Abstract">arXiv:2310.00220</a> (cross-list from physics.plasm-ph) [<a href="/pdf/2310.00220" title="Download PDF">pdf</a>, <a href="/format/2310.00220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimization of Tritium Breeding Ratio in a DT and DD Submersion Tokamak  Fusion Reactor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Goel%2C+V">Vikram Goel</a>, 
<a href="/search/physics?searchtype=author&query=Aslam%2C+S">Soha Aslam</a>, 
<a href="/search/physics?searchtype=author&query=Dua%2C+S">Sejal Dua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Plasma Physics (physics.plasm-ph)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">The mass of stars is enough to confine a plasma to fuse light atoms, but this
is not possible to engineer on Earth. Fortunately, nuclear engineering can rely
on the magnetic confinement of a plasma using superconducting coils so long as
the Tritium Breeding Ratio (TBR) is optimized. This paper will investigate some
of the materials which can increase the rate at which Tritium is produced
within the breeding blanket layer of Submersion Tokamak reactors, a design that
uses magnetic confinement of a plasma in the shape of a torus to execute
nuclear fusion. Using the Paramak Python module to model several geometries and
OpenMC to run a simulation, it can be observed how neutron multipliers,
enrichment, and the neutron energy spectrum affect TBR. This experiment will
mainly observe different material choices that have been considered and their
TBR based on their cross sections, dose rate, thermal properties and safety. By
altering the neutron energy spectrum to account for DD and DT plasma, the
difference in these compounds' Tritium breeding efficacy is noted. Neutron
energy spectra are an important factor in optimising the TBR levels as the
neutrons generated by the fusion reactions in the plasma interact with the
breeder material in the blanket and produce tritium through the reaction with
Lithium. Since Tritium is a rare isotope of hydrogen that is used as fuel in
fusion reactions and has a short half-life, it is essential to produce tritium
within the fusion reactor itself. Without the tritium breeding capability, it
would not be feasible to generate energy via fusion. A TBR greater than unity
indicates that the reactor can generate more tritium than it consumes, ensuring
self-sufficiency in the tritium inventory. Since Tritium is the most reliable
and efficient fuel for these reactors, optimising the TBR is of paramount
importance in the long road to commercialization of nuclear fusion.
</p>
</div>
</dd>
<dt><a name="item713">[713]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00238" title="Abstract">arXiv:2310.00238</a> (cross-list from math.OC) [<a href="/pdf/2310.00238" title="Download PDF">pdf</a>, <a href="/format/2310.00238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feasibility-Guaranteed Safety Critical Control with Applications to  Heterogeneous Platoons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+S">Shuo Liu</a>, 
<a href="/search/math?searchtype=author&query=Xiao%2C+W">Wei Xiao</a>, 
<a href="/search/math?searchtype=author&query=Belta%2C+C+A">Calin A. Belta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures. arXiv admin note: text overlap with <a href="/abs/2304.00372">arXiv:2304.00372</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper studies safety and feasibility guarantees for systems with tight
control bounds. It has been shown that stabilizing an affine control system
while optimizing a quadratic cost and satisfying state and control constraints
can be mapped to a sequence of Quadratic Programs (QPs) using Control Barrier
Functions (CBF) and Control Lyapunov Functions (CLF). One of the main
challenges in this method is that the QP could easily become infeasible under
safety constraints of high relative degree, especially under tight control
bounds. Recent work focused on deriving sufficient conditions for guaranteeing
feasibility. The existing results are case-dependent. In this paper, we
consider the general case and define a feasibility constraint, and propose a
new type of CBF to enforce the feasibility constraint. Our method guarantees
the feasibility of the above mentioned QPs, while satisfying safety
requirements. We demonstrate the advantages of using the proposed method on a
heterogeneous Adaptive Cruise Control (ACC) platoon with tight control bounds,
and compare our method to existing CBF-CLF approaches. The results show that
our proposed approach can generate gradually transitioned control with
guaranteed feasibility and safety.
</p>
</div>
</dd>
<dt><a name="item714">[714]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00257" title="Abstract">arXiv:2310.00257</a> (cross-list from math.OC) [<a href="/pdf/2310.00257" title="Download PDF">pdf</a>, <a href="/format/2310.00257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Lov&#xe1;sz Theta Function for Recovering Planted Clique Covers and  Graph Colorings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hou%2C+J">Jiaxin Hou</a>, 
<a href="/search/math?searchtype=author&query=Soh%2C+Y+S">Yong Sheng Soh</a>, 
<a href="/search/math?searchtype=author&query=Varvitsiotis%2C+A">Antonios Varvitsiotis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Data Structures and Algorithms (cs.DS); Information Theory (cs.IT); Combinatorics (math.CO)

</div>
<p class="mathjax">The problems of computing graph colorings and clique covers are central
challenges in combinatorial optimization. Both of these are known to be
NP-hard, and thus computationally intractable in the worst-case instance. A
prominent approach for computing approximate solutions to these problems is the
celebrated Lov\'asz theta function $\vartheta(G)$, which is specified as the
solution of a semidefinite program (SDP), and hence tractable to compute. In
this work, we move beyond the worst-case analysis and set out to understand
whether the Lov\'asz theta function recovers clique covers for random instances
that have a latent clique cover structure, possibly obscured by noise. We
answer this question in the affirmative and show that for graphs generated from
the planted clique model we introduce in this work, the SDP formulation of
$\vartheta(G)$ has a unique solution that reveals the underlying clique-cover
structure with high-probability. The main technical step is an intermediate
result where we prove a deterministic condition of recovery based on an
appropriate notion of sparsity.
</p>
</div>
</dd>
<dt><a name="item715">[715]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00260" title="Abstract">arXiv:2310.00260</a> (cross-list from math.OC) [<a href="/pdf/2310.00260" title="Download PDF">pdf</a>, <a href="/ps/2310.00260" title="Download PostScript">ps</a>, <a href="/format/2310.00260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Sinkhorn&#x27;s Algorithm and Choice Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Qu%2C+Z">Zhaonan Qu</a>, 
<a href="/search/math?searchtype=author&query=Galichon%2C+A">Alfred Galichon</a>, 
<a href="/search/math?searchtype=author&query=Ugander%2C+J">Johan Ugander</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Econometrics (econ.EM)

</div>
<p class="mathjax">For a broad class of choice and ranking models based on Luce's choice axiom,
including the Bradley--Terry--Luce and Plackett--Luce models, we show that the
associated maximum likelihood estimation problems are equivalent to a classic
matrix balancing problem with target row and column sums. This perspective
opens doors between two seemingly unrelated research areas, and allows us to
unify existing algorithms in the choice modeling literature as special
instances or analogs of Sinkhorn's celebrated algorithm for matrix balancing.
We draw inspirations from these connections and resolve important open problems
on the study of Sinkhorn's algorithm. We first prove the global linear
convergence of Sinkhorn's algorithm for non-negative matrices whenever finite
solutions to the matrix balancing problem exist. We characterize this global
rate of convergence in terms of the algebraic connectivity of the bipartite
graph constructed from data. Next, we also derive the sharp asymptotic rate of
linear convergence, which generalizes a classic result of Knight (2008), but
with a more explicit analysis that exploits an intrinsic orthogonality
structure. To our knowledge, these are the first quantitative linear
convergence results for Sinkhorn's algorithm for general non-negative matrices
and positive marginals. The connections we establish in this paper between
matrix balancing and choice modeling could help motivate further transmission
of ideas and interesting results in both directions.
</p>
</div>
</dd>
<dt><a name="item716">[716]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00267" title="Abstract">arXiv:2310.00267</a> (cross-list from physics.soc-ph) [<a href="/pdf/2310.00267" title="Download PDF">pdf</a>, <a href="/format/2310.00267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Physics of Preference: Unravelling Imprecision of Human Preferences  through Magnetisation Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Maksymov%2C+I+S">Ivan S. Maksymov</a>, 
<a href="/search/physics?searchtype=author&query=Pogrebna%2C+G">Ganna Pogrebna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Other Condensed Matter (cond-mat.other); Artificial Intelligence (cs.AI); Quantum Physics (quant-ph)

</div>
<p class="mathjax">Paradoxical decision-making behaviours such as preference reversal often
arise from imprecise or noisy human preferences. By harnessing the physical
principle of magnetisation reversal in ferromagnetic nanostructures driven by
electric current, we developed a model that closely reflects human
decision-making dynamics. Tested against a spectrum of psychological data, our
model adeptly captures the complexities inherent in individual choices. This
blend of physics and psychology paves the way for fresh perspectives on
understanding human decision-making processes.
</p>
</div>
</dd>
<dt><a name="item717">[717]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00285" title="Abstract">arXiv:2310.00285</a> (cross-list from quant-ph) [<a href="/pdf/2310.00285" title="Download PDF">pdf</a>, <a href="/format/2310.00285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Local Measurements in Many-body Quantum Metrology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Liu%2C+J">Jia-Xuan Liu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yang%2C+J">Jing Yang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Shi%2C+H">Hai-Long Shi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yu%2C+S">Sixia Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6+13 pages, 3 figures. Comments are welcome!
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Quantum Gases (cond-mat.quant-gas); Information Theory (cs.IT); Optimization and Control (math.OC); Atomic Physics (physics.atom-ph)

</div>
<p class="mathjax">Quantum measurements are key to quantum metrology. Constrained by
experimental capabilities, collective measurements on a large number of copies
of metrological probes can pose significant challenges. Therefore, the locality
in quantum measurements must be considered. In this work, we propose a method
dubbed as the "iterative matrix partition" approach to elucidate the underlying
structures of optimal local measurements, with and without classical
communications, that saturate the quantum Cram\'er-Rao Bound (qCRB).
Furthermore, we find that while exact saturation is possible for all two-qubit
pure states, it is generically restrictive for multi-qubit pure states.
However, we demonstrate that the qCRB can be universally saturated in an
approximate manner through adaptive coherent controls, as long as the initial
state is separable and the Hamiltonian allows for interaction. Our results
bridge the gap between theoretical proposals and experiments in many-body
metrology and can find immediate applications in noisy intermediate-scale
quantum devices.
</p>
</div>
</dd>
<dt><a name="item718">[718]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00289" title="Abstract">arXiv:2310.00289</a> (cross-list from eess.IV) [<a href="/pdf/2310.00289" title="Download PDF">pdf</a>, <a href="/format/2310.00289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pubic Symphysis-Fetal Head Segmentation Using Full Transformer with  Bi-level Routing Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cai%2C+P">Pengzhou Cai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this paper, we propose a method, named BRAU-Net, to solve the pubic
symphysis-fetal head segmentation task. The method adopts a U-Net-like pure
Transformer architecture with bi-level routing attention and skip connections,
which effectively learns local-global semantic information. The proposed
BRAU-Net was evaluated on transperineal Ultrasound images dataset from the
pubic symphysis-fetal head segmentation and angle of progression (FH-PS-AOP)
challenge. The results demonstrate that the proposed BRAU-Net achieves
comparable a final score. The codes will be available at
https://github.com/Caipengzhou/BRAU-Net.
</p>
</div>
</dd>
<dt><a name="item719">[719]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00319" title="Abstract">arXiv:2310.00319</a> (cross-list from eess.SP) [<a href="/pdf/2310.00319" title="Download PDF">pdf</a>, <a href="/format/2310.00319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-Variant Overlap-Add in Partitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jaeger%2C+H">Hagen Jaeger</a>, 
<a href="/search/eess?searchtype=author&query=Simmer%2C+U">Uwe Simmer</a>, 
<a href="/search/eess?searchtype=author&query=Bitzer%2C+J">J&#xf6;rg Bitzer</a>, 
<a href="/search/eess?searchtype=author&query=Blau%2C+M">Matthias Blau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Virtual and augmented realities are increasingly popular tools in many
domains such as architecture, production, training and education,
(psycho)therapy, gaming, and others. For a convincing rendering of sound in
virtual and augmented environments, audio signals must be convolved in
real-time with impulse responses that change from one moment in time to
another. Key requirements for the implementation of such time-variant real-time
convolution algorithms are short latencies, moderate computational cost and
memory footprint, and no perceptible switching artifacts. In this engineering
report, we introduce a partitioned convolution algorithm that is able to
quickly switch between impulse responses without introducing perceptible
artifacts, while maintaining a constant computational load and low memory
usage. Implementations in several popular programming languages are freely
available via GitHub.
</p>
</div>
</dd>
<dt><a name="item720">[720]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00327" title="Abstract">arXiv:2310.00327</a> (cross-list from stat.ML) [<a href="/pdf/2310.00327" title="Download PDF">pdf</a>, <a href="/format/2310.00327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memorization with neural nets: going beyond the worst case
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dirksen%2C+S">Sjoerd Dirksen</a>, 
<a href="/search/stat?searchtype=author&query=Finke%2C+P">Patrick Finke</a>, 
<a href="/search/stat?searchtype=author&query=Genzel%2C+M">Martin Genzel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">In practice, deep neural networks are often able to easily interpolate their
training data. To understand this phenomenon, many works have aimed to quantify
the memorization capacity of a neural network architecture: the largest number
of points such that the architecture can interpolate any placement of these
points with any assignment of labels. For real-world data, however, one
intuitively expects the presence of a benign structure so that interpolation
already occurs at a smaller network size than suggested by memorization
capacity. In this paper, we investigate interpolation by adopting an
instance-specific viewpoint. We introduce a simple randomized algorithm that,
given a fixed finite dataset with two classes, with high probability constructs
an interpolating three-layer neural network in polynomial time. The required
number of parameters is linked to geometric properties of the two classes and
their mutual arrangement. As a result, we obtain guarantees that are
independent of the number of samples and hence move beyond worst-case
memorization capacity bounds. We illustrate the effectiveness of the algorithm
in non-pathological situations with extensive numerical experiments and link
the insights back to the theoretical results.
</p>
</div>
</dd>
<dt><a name="item721">[721]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00397" title="Abstract">arXiv:2310.00397</a> (cross-list from math.OC) [<a href="/pdf/2310.00397" title="Download PDF">pdf</a>, <a href="/format/2310.00397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A First-Order Method with Expansive Projection for Optimal Powered  Descent Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Choi%2C+J">Jiwoo Choi</a>, 
<a href="/search/math?searchtype=author&query=Kim%2C+J">Jong-Han Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper introduces a first-order method for solving optimal powered
descent guidance (PDG) problems, that directly handles the nonconvex
constraints associated with the maximum and minimum thrust bounds with varying
mass and the pointing angle constraints on thrust vectors. This issue has been
conventionally circumvented via lossless convexification (LCvx), which lifts a
nonconvex feasible set to a higher-dimensional convex set, and via linear
approximation of another nonconvex feasible set defined by exponential
functions. However, this approach sometimes results in an infeasible solution
when the solution obtained from the higher-dimensional space is projected back
to the original space, especially when the problem involves a nonoptimal time
of flight. Additionally, the Taylor series approximation introduces an
approximation error that grows with both flight time and deviation from the
reference trajectory. In this paper, we introduce a first-order approach that
makes use of orthogonal projections onto nonconvex sets, allowing expansive
projection (ExProj). We show that 1) this approach produces a feasible solution
with better performance even for the nonoptimal time of flight cases for which
conventional techniques fail and 2) the proposed method compensates for the
linearization error that arises from Taylor series approximation. We claim that
the proposed approach offers more flexibility in generating feasible
trajectories for a wide variety of planetary soft landing problems.
</p>
</div>
</dd>
<dt><a name="item722">[722]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00398" title="Abstract">arXiv:2310.00398</a> (cross-list from math.OC) [<a href="/pdf/2310.00398" title="Download PDF">pdf</a>, <a href="/format/2310.00398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Impact Angle Guidance via First-Order Optimization Under  Nonconvex Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Park%2C+G">Gyubin Park</a>, 
<a href="/search/math?searchtype=author&query=Jeong%2C+D+H">Da Hoon Jeong</a>, 
<a href="/search/math?searchtype=author&query=Kim%2C+J">Jong-Han Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Most optimal guidance problems can be formulated as nonconvex optimization
problems, which can be solved indirectly by relaxation, convexification, or
linearization. Although these methods are guaranteed to converge to the global
optimum of the modified problems, the obtained solution may not guarantee
global optimality or even the feasibility of the original nonconvex problems.
In this paper, we propose a computational optimal guidance approach that
directly handles the nonconvex constraints encountered in formulating guidance
problems. The proposed computational guidance approach alternately solves the
least squares problem and projects the solution onto nonconvex feasible sets,
which rapidly converge to feasible suboptimal solutions or, sometimes, to
globally optimal solutions. The proposed algorithm is verified via a series of
numerical simulations on impact angle guidance problems, and it is demonstrated
that the proposed algorithm provides superior guidance performance compared to
conventional techniques.
</p>
</div>
</dd>
<dt><a name="item723">[723]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00417" title="Abstract">arXiv:2310.00417</a> (cross-list from stat.AP) [<a href="/pdf/2310.00417" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Teaching at the Intersection of Social Justice, Ethics, and the ASA  Ethical Guidelines for Statistical Practice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Tractenberg%2C+R+E">Rochelle E Tractenberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages; 3 tables; 2 figures. To appear (verbatim) in Proceedings of 2023 JSM, Toronto Canada
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Case studies are typically used to teach 'ethics', but when the content of a
course is focused on formulae and proofs, a case analysis and the knowledge,
skills, and abilities they require can be distracting. Moreover, case analyses
are typically focused narrowly on research issues: obtaining consent, dealing
with research team members, and/or research policy violations. Not all students
in quantitative courses plan to become researchers, and ethical practice of
mathematics, statistics, data science, and computing is an essential topic
regardless of the learner's career plans. While it is incorrect to treat
'social justice' as a proxy for 'ethical practice', the topic of 'social
justice' may be more interesting to both students and instructors. This paper
offers concrete recommendations for integrating social justice content into
quantitative courses in ways that limit the burden of new knowledge, skills,
and abilities but also support reproducible and actionable assessments. Five
tools can be utilized to integrate social justice into a course in a way that
also meets calls to integrate 'ethics'; minimizes the burden on instructors to
create and grade new materials and assignments; minimizes the burden on
learners to develop the skill set to complete a case analysis; and maximizes
the likelihood that the ethics content will be embedded in the learners'
cognitive representation of the knowledge being taught in the quantitative
course. These tools are: a. Curriculum Development Guidelines b. 7-task
Statistics and Data Science Pipeline c. ASA Ethical Guidelines for Statistical
Practice d. Stakeholder Analysis e. 6-step Ethical Reasoning paradigm This
paper discusses how to use these tools in quantitative courses. The tools and
frameworks offer structure, and facilitate ensuring that changes made to any
course are evaluable and generate actionable assessments for learners.
</p>
</div>
</dd>
<dt><a name="item724">[724]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00419" title="Abstract">arXiv:2310.00419</a> (cross-list from math.OC) [<a href="/pdf/2310.00419" title="Download PDF">pdf</a>, <a href="/format/2310.00419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Convergence of Pre-Conditioned PI Consensus Algorithm under  Restricted Strong Convexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chakrabarti%2C+K">Kushal Chakrabarti</a>, 
<a href="/search/math?searchtype=author&query=Baranwal%2C+M">Mayank Baranwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper considers solving distributed convex optimization problems in
peer-to-peer multi-agent networks. The network is assumed to be synchronous and
connected. By using the proportional-integral (PI) control strategy, various
algorithms with fixed stepsize have been developed. The earliest among them is
the PI consensus algorithm. Using Lyapunov theory, we guarantee exponential
convergence of the PI consensus algorithm for restricted strongly convex
functions with rate-matching discretization, without requiring convexity of
individual local cost functions, for the first time. In order to accelerate the
PI consensus algorithm, we incorporate local pre-conditioning in the form of
constant positive definite matrices and numerically validate its efficiency
compared to the prominent distributed convex optimization algorithms. Unlike
classical pre-conditioning, where only the gradients are multiplied by a
pre-conditioner, the proposed pre-conditioning modifies both the gradients and
the consensus terms, thereby controlling the effect of the communication graph
between the agents on the PI consensus algorithm.
</p>
</div>
</dd>
<dt><a name="item725">[725]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00420" title="Abstract">arXiv:2310.00420</a> (cross-list from eess.SP) [<a href="/pdf/2310.00420" title="Download PDF">pdf</a>, <a href="/format/2310.00420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Algorithm for Clustered Multi-Task Compressive Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lin%2C+A">Alexander Lin</a>, 
<a href="/search/eess?searchtype=author&query=Ba%2C+D">Demba Ba</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper considers clustered multi-task compressive sensing, a hierarchical
model that solves multiple compressive sensing tasks by finding clusters of
tasks that leverage shared information to mutually improve signal
reconstruction. The existing inference algorithm for this model is
computationally expensive and does not scale well in high dimensions. The main
bottleneck involves repeated matrix inversion and log-determinant computation
for multiple large covariance matrices. We propose a new algorithm that
substantially accelerates model inference by avoiding the need to explicitly
compute these covariance matrices. Our approach combines Monte Carlo sampling
with iterative linear solvers. Our experiments reveal that compared to the
existing baseline, our algorithm can be up to thousands of times faster and an
order of magnitude more memory-efficient.
</p>
</div>
</dd>
<dt><a name="item726">[726]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00443" title="Abstract">arXiv:2310.00443</a> (cross-list from stat.ML) [<a href="/pdf/2310.00443" title="Download PDF">pdf</a>, <a href="/ps/2310.00443" title="Download PostScript">ps</a>, <a href="/format/2310.00443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The objective function equality property of infoGAN for two-layer  network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hasan%2C+M">Mahmud Hasan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Information Maximizing Generative Adversarial Network (infoGAN) can be
understood as a minimax problem involving two networks: discriminators and
generators with mutual information functions. The infoGAN incorporates various
components, including latent variables, mutual information, and objective
function. This research demonstrates that the two objective functions in
infoGAN become equivalent as the discriminator and generator sample size
approaches infinity. This equivalence is established by considering the
disparity between the empirical and population versions of the objective
function. The bound on this difference is determined by the Rademacher
complexity of the discriminator and generator function class. Furthermore, the
utilization of a two-layer network for both the discriminator and generator,
featuring Lipschitz and non-decreasing activation functions, validates this
equality
</p>
</div>
</dd>
<dt><a name="item727">[727]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00444" title="Abstract">arXiv:2310.00444</a> (cross-list from quant-ph) [<a href="/pdf/2310.00444" title="Download PDF">pdf</a>, <a href="/format/2310.00444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FragQC: An Efficient Quantum Error Reduction Technique using Quantum  Circuit Fragmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Basu%2C+S">Saikat Basu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Das%2C+A">Arnav Das</a>, 
<a href="/search/quant-ph?searchtype=author&query=Saha%2C+A">Amit Saha</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chakrabarti%2C+A">Amlan Chakrabarti</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sur-Kolay%2C+S">Susmita Sur-Kolay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Quantum computers must meet extremely stringent qualitative and quantitative
requirements on their qubits in order to solve real-life problems. Quantum
circuit fragmentation techniques divide a large quantum circuit into a number
of sub-circuits that can be executed on the smaller noisy quantum hardware
available. However, the process of quantum circuit fragmentation involves
finding an ideal cut that has exponential time complexity, and also classical
post-processing required to reconstruct the output. In this paper, we represent
a quantum circuit using a weighted graph and propose a novel classical graph
partitioning algorithm for selecting an efficient fragmentation that reduces
the entanglement between the sub-circuits along with balancing the estimated
error in each sub-circuit. We also demonstrate a comparative study over
different classical and quantum approaches of graph partitioning for finding
such a cut. We present {\it FragQC}, a software tool that cuts a quantum
circuit into sub-circuits when its error probability exceeds a certain
threshold. With this proposed approach, we achieve an increase of fidelity by
14.83\% compared to direct execution without cutting the circuit, and 8.45\%
over the state-of-the-art ILP-based method, for the benchmark circuits.
</p>
</div>
</dd>
<dt><a name="item728">[728]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00475" title="Abstract">arXiv:2310.00475</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2310.00475" title="Download PDF">pdf</a>, <a href="/format/2310.00475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Design of inorganic compounds using deep diffusion language  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Dong%2C+R">Rongzhi Dong</a>, 
<a href="/search/cond-mat?searchtype=author&query=Fu%2C+N">Nihang Fu</a>, 
<a href="/search/cond-mat?searchtype=author&query=Siriwardane%2C+d+M+D">dirisuriya M. D. Siriwardane</a>, 
<a href="/search/cond-mat?searchtype=author&query=Hu%2C+J">Jianjun Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Due to the vast chemical space, discovering materials with a specific
function is challenging. Chemical formulas are obligated to conform to a set of
exacting criteria such as charge neutrality, balanced electronegativity,
synthesizability, and mechanical stability. In response to this formidable
task, we introduce a deep learning-based generative model for material
composition and structure design by learning and exploiting explicit and
implicit chemical knowledge. Our pipeline first uses deep diffusion language
models as the generator of compositions and then applies a template-based
crystal structure prediction algorithm to predict their corresponding
structures, which is then followed by structure relaxation using a universal
graph neural network-based potential. The density functional theory (DFT)
calculations of the formation energies and energy-above-the-hull analysis are
used to validate new structures generated through our pipeline. Based on the
DFT calculation results, six new materials, including Ti2HfO5, TaNbP, YMoN2,
TaReO4, HfTiO2, and HfMnO2, with formation energy less than zero have been
found. Remarkably, among these, four materials, namely Ti2$HfO5, TaNbP, YMoN2,
and TaReO4, exhibit an e-above-hull energy of less than 0.3 eV. These findings
have proved the effectiveness of our approach.
</p>
</div>
</dd>
<dt><a name="item729">[729]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00509" title="Abstract">arXiv:2310.00509</a> (cross-list from math.OC) [<a href="/pdf/2310.00509" title="Download PDF">pdf</a>, <a href="/format/2310.00509" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smoothing Mixed Traffic with Robust Data-driven Predictive Control for  Connected and Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shang%2C+X">Xu Shang</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+J">Jiawei Wang</a>, 
<a href="/search/math?searchtype=author&query=Zheng%2C+Y">Yang Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The recently developed DeeP-LCC (Data-EnablEd Predictive Leading Cruise
Control) method has shown promising performance for data-driven predictive
control of Connected and Autonomous Vehicles (CAVs) in mixed traffic. However,
its simplistic zero assumption of the future velocity errors for the head
vehicle may pose safety concerns and limit its performance of smoothing traffic
flow. In this paper, we propose a robust DeeP-LCC method to control CAVs in
mixed traffic with enhanced safety performance. In particular, we first present
a robust formulation that enforces a safety constraint for a range of potential
velocity error trajectories, and then estimate all potential velocity errors
based on the past data from the head vehicle. We also provide efficient
computational approaches to solve the robust optimization for online predictive
control. Nonlinear traffic simulations show that our robust DeeP-LCC can
provide better traffic efficiency and stronger safety performance while
requiring less offline data.
</p>
</div>
</dd>
<dt><a name="item730">[730]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00514" title="Abstract">arXiv:2310.00514</a> (cross-list from math.LO) [<a href="/pdf/2310.00514" title="Download PDF">pdf</a>, <a href="/ps/2310.00514" title="Download PostScript">ps</a>, <a href="/format/2310.00514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The CSP Dichotomy, the Axiom of Choice, and Cyclic Polymorphisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=K%C3%A1tay%2C+T">Tam&#xe1;s K&#xe1;tay</a>, 
<a href="/search/math?searchtype=author&query=T%C3%B3th%2C+L+M">L&#xe1;szl&#xf3; M&#xe1;rton T&#xf3;th</a>, 
<a href="/search/math?searchtype=author&query=Vidny%C3%A1nszky%2C+Z">Zolt&#xe1;n Vidny&#xe1;nszky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">We study Constraint Satisfaction Problems (CSPs) in an infinite context. We
show that the dichotomy between easy and hard problems -- established already
in the finite case -- presents itself as the strength of the corresponding De
Bruijin-Erd\H{o}s-type compactness theorem over ZF. More precisely, if
$\mathcal{D}$ is a structure, let $K_\mathcal{D}$ stand for the following
statement: for every structure $\mathcal{X}$ if every finite substructure of
$\mathcal{X}$ admits a solution to $\mathcal{D}$, then so does $\mathcal{X}$.
We prove that if $\mathcal{D}$ admits no cyclic polymorphism, and thus it is
NP-complete by the CSP Dichotomy Theorem, then $K_\mathcal{D}$ is equivalent to
the Boolean Prime Ideal Theorem (BPI) over ZF. Conversely, we also show that if
$\mathcal{D}$ admits a cyclic polymorphism, and thus it is in P, then
$K_\mathcal{D}$ is strictly weaker than BPI.
</p>
</div>
</dd>
<dt><a name="item731">[731]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00518" title="Abstract">arXiv:2310.00518</a> (cross-list from quant-ph) [<a href="/pdf/2310.00518" title="Download PDF">pdf</a>, <a href="/format/2310.00518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Informative Latent Representation for Quantum State Tomography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Ma%2C+H">Hailan Ma</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sun%2C+Z">Zhenhong Sun</a>, 
<a href="/search/quant-ph?searchtype=author&query=Dong%2C+D">Daoyi Dong</a>, 
<a href="/search/quant-ph?searchtype=author&query=Gong%2C+D">Dong Gong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Quantum state tomography (QST) is the process of reconstructing the complete
state of a quantum system (mathematically described as a density matrix)
through a series of different measurements. These measurements are performed on
a number of identical copies of the quantum system, with outcomes gathered as
frequencies. QST aims to recover the density matrix and the corresponding
properties of the quantum state from the measured frequencies. Although an
informationally complete set of measurements can specify quantum state
accurately in an ideal scenario with a large number of identical copies, both
measurements and identical copies are restricted and imperfect in practical
scenarios, making QST highly ill-posed. The conventional QST methods usually
assume adequate or accurate measured frequencies or rely on manually designed
regularizers to handle the ill-posed reconstruction problem, suffering from
limited applications in realistic scenarios. Recent advances in deep neural
networks (DNNs) led to the emergence of deep learning (DL) in QST. However,
existing DL-based QST approaches often employ generic DNN models that are not
optimized for imperfect conditions of QST. In this paper, we propose a
transformer-based autoencoder architecture tailored for QST with imperfect
measurement data. Our method leverages a transformer-based encoder to extract
an informative latent representation (ILR) from imperfect measurement data and
employs a decoder to predict the quantum states based on the ILR. We anticipate
that the high-dimensional ILR will capture more comprehensive information about
quantum states. To achieve this, we conduct pre-training of the encoder using a
pretext task that involves reconstructing high-quality frequencies from
measured frequencies. Extensive simulations and experiments demonstrate the
remarkable ability of the ILR in dealing with imperfect measurement data in
QST.
</p>
</div>
</dd>
<dt><a name="item732">[732]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00536" title="Abstract">arXiv:2310.00536</a> (cross-list from math.AT) [<a href="/pdf/2310.00536" title="Download PDF">pdf</a>, <a href="/format/2310.00536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing the alpha complex using dual active set methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Carlsson%2C+E">Erik Carlsson</a>, 
<a href="/search/math?searchtype=author&query=Carlsson%2C+J">John Carlsson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Topology (math.AT)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">The alpha complex is a fundamental data structure from computational
geometry, which encodes the topological type of a union of balls $B(x; r)
\subset \mathbb{R}^m$ for $x\in S$, including a weighted version that allows
for varying radii. It consists of the collection of "simplices" $\sigma =
\{x_0, ..., x_k \} \subset S$, which correspond to nomempty $(k + 1)$-fold
intersections of cells in a radius-restricted version of the Voronoi diagram.
Existing algorithms for computing the alpha complex require that the points
reside in low dimension because they begin by computing the entire Delaunay
complex, which rapidly becomes intractable, even when the alpha complex is of a
reasonable size. This paper presents a method for computing the alpha complex
without computing the full Delaunay triangulation by applying Lagrangian
duality, specifically an algorithm based on dual quadratic programming that
seeks to rule simplices out rather than ruling them in.
</p>
</div>
</dd>
<dt><a name="item733">[733]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00539" title="Abstract">arXiv:2310.00539</a> (cross-list from stat.ML) [<a href="/pdf/2310.00539" title="Download PDF">pdf</a>, <a href="/format/2310.00539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thompson Exploration with Best Challenger Rule in Best Arm  Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lee%2C+J">Jongyeong Lee</a>, 
<a href="/search/stat?searchtype=author&query=Honda%2C+J">Junya Honda</a>, 
<a href="/search/stat?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> TBA ACML2023, 49pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper studies the fixed-confidence best arm identification (BAI) problem
in the bandit framework in the canonical single-parameter exponential models.
For this problem, many policies have been proposed, but most of them require
solving an optimization problem at every round and/or are forced to explore an
arm at least a certain number of times except those restricted to the Gaussian
model. To address these limitations, we propose a novel policy that combines
Thompson sampling with a computationally efficient approach known as the best
challenger rule. While Thompson sampling was originally considered for
maximizing the cumulative reward, we demonstrate that it can be used to
naturally explore arms in BAI without forcing it. We show that our policy is
asymptotically optimal for any two-armed bandit problems and achieves near
optimality for general $K$-armed bandit problems for $K\geq 3$. Nevertheless,
in numerical experiments, our policy shows competitive performance compared to
asymptotically optimal policies in terms of sample complexity while requiring
less computation cost. In addition, we highlight the advantages of our policy
by comparing it to the concept of $\beta$-optimality, a relaxed notion of
asymptotic optimality commonly considered in the analysis of a class of
policies including the proposed one.
</p>
</div>
</dd>
<dt><a name="item734">[734]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00541" title="Abstract">arXiv:2310.00541</a> (cross-list from stat.ML) [<a href="/pdf/2310.00541" title="Download PDF">pdf</a>, <a href="/format/2310.00541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Nonparametric Hypothesis Testing to Understand Variability in  Training Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Banerjee%2C+S">Sinjini Banerjee</a>, 
<a href="/search/stat?searchtype=author&query=Cannon%2C+R">Reilly Cannon</a>, 
<a href="/search/stat?searchtype=author&query=Marrinan%2C+T">Tim Marrinan</a>, 
<a href="/search/stat?searchtype=author&query=Chiang%2C+T">Tony Chiang</a>, 
<a href="/search/stat?searchtype=author&query=Sarwate%2C+A+D">Anand D. Sarwate</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Training a deep neural network (DNN) often involves stochastic optimization,
which means each run will produce a different model. Several works suggest this
variability is negligible when models have the same performance, which in the
case of classification is test accuracy. However, models with similar test
accuracy may not be computing the same function. We propose a new measure of
closeness between classification models based on the output of the network
before thresholding. Our measure is based on a robust hypothesis-testing
framework and can be adapted to other quantities derived from trained models.
</p>
</div>
</dd>
<dt><a name="item735">[735]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00545" title="Abstract">arXiv:2310.00545</a> (cross-list from eess.SP) [<a href="/pdf/2310.00545" title="Download PDF">pdf</a>, <a href="/format/2310.00545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Neural Representations and the Algebra of Complex Wavelets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Roddenberry%2C+T+M">T. Mitchell Roddenberry</a>, 
<a href="/search/eess?searchtype=author&query=Saragadam%2C+V">Vishwanath Saragadam</a>, 
<a href="/search/eess?searchtype=author&query=de+Hoop%2C+M+V">Maarten V. de Hoop</a>, 
<a href="/search/eess?searchtype=author&query=Baraniuk%2C+R+G">Richard G. Baraniuk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures. 2 appendix pages, 1 appendix figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Implicit neural representations (INRs) have arisen as useful methods for
representing signals on Euclidean domains. By parameterizing an image as a
multilayer perceptron (MLP) on Euclidean space, INRs effectively represent
signals in a way that couples spatial and spectral features of the signal that
is not obvious in the usual discrete representation, paving the way for
continuous signal processing and machine learning approaches that were not
previously possible. Although INRs using sinusoidal activation functions have
been studied in terms of Fourier theory, recent works have shown the advantage
of using wavelets instead of sinusoids as activation functions, due to their
ability to simultaneously localize in both frequency and space. In this work,
we approach such INRs and demonstrate how they resolve high-frequency features
of signals from coarse approximations done in the first layer of the MLP. This
leads to multiple prescriptions for the design of INR architectures, including
the use of complex wavelets, decoupling of low and band-pass approximations,
and initialization schemes based on the singularities of the desired signal.
</p>
</div>
</dd>
<dt><a name="item736">[736]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00549" title="Abstract">arXiv:2310.00549</a> (cross-list from math.OC) [<a href="/pdf/2310.00549" title="Download PDF">pdf</a>, <a href="/format/2310.00549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convex Restriction of Feasible Sets for AC Radial Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+L">Ling Zhang</a>, 
<a href="/search/math?searchtype=author&query=Tabas%2C+D">Daniel Tabas</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+B">Baosen Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Many problems in power systems involve optimizing a certain objective
function subject to power flow equations and engineering constraints. A
long-standing challenge in solving them is the nonconvexity of their feasible
sets. In this paper, we propose an analytical method to construct the convex
restriction of the feasible set for AC power flows in radial networks. The
construction relies on simple geometrical ideas and is explicit, in the sense
that it does not involve solving other complicated optimization problems. We
also show that the construct restrictions are in some sense maximal, that is,
the best possible ones. Optimization problems constrained to these sets are not
only simpler to solve but also offer feasibility guarantee for the solutions to
the original OPF problem. Furthermore, we present an iterative algorithm to
improve on the solution quality by successively constructing a sequence of
convex restricted sets and solving the optimization on them. The numerical
experiments on the IEEE 123-bus distribution network show that our method finds
good feasible solutions within just a few iterations and works well with
various objective functions, even in situations where traditional methods fail
to return a solution.
</p>
</div>
</dd>
<dt><a name="item737">[737]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00561" title="Abstract">arXiv:2310.00561</a> (cross-list from stat.CO) [<a href="/pdf/2310.00561" title="Download PDF">pdf</a>, <a href="/format/2310.00561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CausalGPS: An R Package for Causal Inference With Continuous Exposures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Khoshnevis%2C+N">Naeem Khoshnevis</a>, 
<a href="/search/stat?searchtype=author&query=Wu%2C+X">Xiao Wu</a>, 
<a href="/search/stat?searchtype=author&query=Braun%2C+D">Danielle Braun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Mathematical Software (cs.MS); Econometrics (econ.EM)

</div>
<p class="mathjax">Quantifying the causal effects of continuous exposures on outcomes of
interest is critical for social, economic, health, and medical research.
However, most existing software packages focus on binary exposures. We develop
the CausalGPS R package that implements a collection of algorithms to provide
algorithmic solutions for causal inference with continuous exposures. CausalGPS
implements a causal inference workflow, with algorithms based on generalized
propensity scores (GPS) as the core, extending propensity scores (the
probability of a unit being exposed given pre-exposure covariates) from binary
to continuous exposures. As the first step, the package implements efficient
and flexible estimations of the GPS, allowing multiple user-specified modeling
options. As the second step, the package provides two ways to adjust for
confounding: weighting and matching, generating weighted and matched data sets,
respectively. Lastly, the package provides built-in functions to fit flexible
parametric, semi-parametric, or non-parametric regression models on the
weighted or matched data to estimate the exposure-response function relating
the outcome with the exposures. The computationally intensive tasks are
implemented in C++, and efficient shared-memory parallelization is achieved by
OpenMP API. This paper outlines the main components of the CausalGPS R package
and demonstrates its application to assess the effect of long-term exposure to
PM2.5 on educational attainment using zip code-level data from the contiguous
United States from 2000-2016.
</p>
</div>
</dd>
<dt><a name="item738">[738]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00562" title="Abstract">arXiv:2310.00562</a> (cross-list from stat.ML) [<a href="/pdf/2310.00562" title="Download PDF">pdf</a>, <a href="/format/2310.00562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discrete Choice Multi-Armed Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Melo%2C+E">Emerson Melo</a>, 
<a href="/search/stat?searchtype=author&query=M%C3%BCller%2C+D">David M&#xfc;ller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper establishes a connection between a category of discrete choice
models and the realms of online learning and multiarmed bandit algorithms. Our
contributions can be summarized in two key aspects. Firstly, we furnish
sublinear regret bounds for a comprehensive family of algorithms, encompassing
the Exp3 algorithm as a particular case. Secondly, we introduce a novel family
of adversarial multiarmed bandit algorithms, drawing inspiration from the
generalized nested logit models initially introduced by \citet{wen:2001}. These
algorithms offer users the flexibility to fine-tune the model extensively, as
they can be implemented efficiently due to their closed-form sampling
distribution probabilities. To demonstrate the practical implementation of our
algorithms, we present numerical experiments, focusing on the stochastic bandit
case.
</p>
</div>
</dd>
<dt><a name="item739">[739]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00585" title="Abstract">arXiv:2310.00585</a> (cross-list from quant-ph) [<a href="/pdf/2310.00585" title="Download PDF">pdf</a>, <a href="/format/2310.00585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum generative adversarial learning in photonics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+Y">Yizhi Wang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Xue%2C+S">Shichuan Xue</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+Y">Yaxuan Wang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Liu%2C+Y">Yong Liu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ding%2C+J">Jiangfang Ding</a>, 
<a href="/search/quant-ph?searchtype=author&query=Shi%2C+W">Weixu Shi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+D">Dongyang Wang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Liu%2C+Y">Yingwen Liu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Fu%2C+X">Xiang Fu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Huang%2C+G">Guangyao Huang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Huang%2C+A">Anqi Huang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Deng%2C+M">Mingtang Deng</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wu%2C+J">Junjie Wu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Optics Letters Vol. 48, Issue 20, pp. 5197-5200 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Machine Learning (cs.LG); Optics (physics.optics)

</div>
<p class="mathjax">Quantum Generative Adversarial Networks (QGANs), an intersection of quantum
computing and machine learning, have attracted widespread attention due to
their potential advantages over classical analogs. However, in the current era
of Noisy Intermediate-Scale Quantum (NISQ) computing, it is essential to
investigate whether QGANs can perform learning tasks on near-term quantum
devices usually affected by noise and even defects. In this Letter, using a
programmable silicon quantum photonic chip, we experimentally demonstrate the
QGAN model in photonics for the first time, and investigate the effects of
noise and defects on its performance. Our results show that QGANs can generate
high-quality quantum data with a fidelity higher than 90\%, even under
conditions where up to half of the generator's phase shifters are damaged, or
all of the generator and discriminator's phase shifters are subjected to phase
noise up to 0.04$\pi$. Our work sheds light on the feasibility of implementing
QGANs on NISQ-era quantum hardware.
</p>
</div>
</dd>
<dt><a name="item740">[740]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00587" title="Abstract">arXiv:2310.00587</a> (cross-list from eess.AS) [<a href="/pdf/2310.00587" title="Download PDF">pdf</a>, <a href="/format/2310.00587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mechatronic Generation of Datasets for Acoustics Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lu%2C+A">Austin Lu</a>, 
<a href="/search/eess?searchtype=author&query=Moore%2C+E">Ethaniel Moore</a>, 
<a href="/search/eess?searchtype=author&query=Nallanthighall%2C+A">Arya Nallanthighall</a>, 
<a href="/search/eess?searchtype=author&query=Sarkar%2C+K">Kanad Sarkar</a>, 
<a href="/search/eess?searchtype=author&query=Mittal%2C+M">Manan Mittal</a>, 
<a href="/search/eess?searchtype=author&query=Corey%2C+R+M">Ryan M. Corey</a>, 
<a href="/search/eess?searchtype=author&query=Smaragdis%2C+P">Paris Smaragdis</a>, 
<a href="/search/eess?searchtype=author&query=Singer%2C+A">Andrew Singer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures, IWAENC 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">We address the challenge of making spatial audio datasets by proposing a
shared mechanized recording space that can run custom acoustic experiments: a
Mechatronic Acoustic Research System (MARS). To accommodate a wide variety of
experiments, we implement an extensible architecture for wireless multi-robot
coordination which enables synchronized robot motion for dynamic scenes with
moving speakers and microphones. Using a virtual control interface, we can
remotely design automated experiments to collect large-scale audio data. This
data is shown to be similar across repeated runs, demonstrating the reliability
of MARS. We discuss the potential for MARS to make audio data collection
accessible for researchers without dedicated acoustic research spaces.
</p>
</div>
</dd>
<dt><a name="item741">[741]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00589" title="Abstract">arXiv:2310.00589</a> (cross-list from math.OC) [<a href="/pdf/2310.00589" title="Download PDF">pdf</a>, <a href="/format/2310.00589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structural Controllability of Drift-free Bilinear Systems on  $\mathbb{SE(n)}$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dilip%2C+A+S+A">A. Sanand Amita Dilip</a>, 
<a href="/search/math?searchtype=author&query=Athalye%2C+C+D">Chirayu D. Athalye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">We obtain graph theoretic necessary and sufficient conditions for the
structural controllability of drift free bilinear systems on the special
Euclidean group leveraging results from the existing literature.
<br />The graph theoretic conditions allow us to check the structural
controllability in polynomial time.
<br />We use these conditions to find the sparsest structures for structural
controllability.
<br />These conditions can also be used to check the structural accessibility of
bilinear systems with drift.
<br />Equivalent conditions using the permutation group are also obtained.
<br />We consider the problem of link failures within a given structure and obtain
equivalent conditions for structural controllability which are polynomial time
checkable.
<br />We show that the problem of finding sparsest structures under $k$ link
failures is NP-hard for $k&gt;0$.
<br />We also discuss the case of structural controllability under probabilistic
link failures.
</p>
</div>
</dd>
<dt><a name="item742">[742]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00592" title="Abstract">arXiv:2310.00592</a> (cross-list from quant-ph) [<a href="/pdf/2310.00592" title="Download PDF">pdf</a>, <a href="/format/2310.00592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nearest neighbor synthesis of CNOT circuits on general quantum  architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+X">Xinyu Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhu%2C+M">Mingqiang Zhu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Cheng%2C+X">Xueyun Cheng</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhu%2C+P">Pengcheng Zhu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Guan%2C+Z">Zhijin Guan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">In recent years, quantum computing has entered the Noisy Intermediate-Scale
Quantum (NISQ). However, NISQ devices have inherent limitations in terms of
connectivity and hardware noise, necessitating the transformation of quantum
logic circuits for correct execution on NISQ chips. The synthesis of CNOT
circuits considering physical constraints can transform quantum algorithms into
low-level quantum circuits, which can be directly executed on physical chips.
In the current trend, quantum chip architectures without Hamiltonian paths are
gradually replacing architectures with Hamiltonian paths due to their
scalability and low-noise characteristics. To this end, this paper addresses
the nearest neighbor synthesis of CNOT circuits in the architecture with and
without Hamiltonian paths, aiming to enhance the fidelity of the circuits after
execution. Firstly, a key-qubit priority mapping model for the general
architecture with and without Hamiltonian paths is proposed. Secondly, the
initial mapping is further improved by using tabu search to reduce the number
of CNOT gates after circuit synthesis and enhance its fidelity. Finally, the
noise-aware CNOT circuit nearest neighbor synthesis algorithm for the general
architecture is proposed based on the key-qubit priority mapping model.
Experimental results show that the proposed method can enhance the fidelity of
the CNOT circuit by about 64.7% on a real quantum computing device, achieving a
significant optimization effect. Furthermore, the method can be extended to
other circuits, thereby improving the overall performance of quantum computing
on NISQ devices.
</p>
</div>
</dd>
<dt><a name="item743">[743]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00602" title="Abstract">arXiv:2310.00602</a> (cross-list from eess.AS) [<a href="/pdf/2310.00602" title="Download PDF">pdf</a>, <a href="/ps/2310.00602" title="Download PostScript">ps</a>, <a href="/format/2310.00602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wavelet Scattering Transform for Improving Generalization in  Low-Resourced Spoken Language Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dey%2C+S">Spandan Dey</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+P">Premjeet Singh</a>, 
<a href="/search/eess?searchtype=author&query=Saha%2C+G">Goutam Saha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in INTERSPEECH 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Commonly used features in spoken language identification (LID), such as
mel-spectrogram or MFCC, lose high-frequency information due to windowing. The
loss further increases for longer temporal contexts. To improve generalization
of the low-resourced LID systems, we investigate an alternate feature
representation, wavelet scattering transform (WST), that compensates for the
shortcomings. To our knowledge, WST is not explored earlier in LID tasks. We
first optimize WST features for multiple South Asian LID corpora. We show that
LID requires low octave resolution and frequency-scattering is not useful.
Further, cross-corpora evaluations show that the optimal WST hyper-parameters
depend on both train and test corpora. Hence, we develop fused ECAPA-TDNN based
LID systems with different sets of WST hyper-parameters to improve
generalization for unknown data. Compared to MFCC, EER is reduced upto 14.05%
and 6.40% for same-corpora and blind VoxLingua107 evaluations, respectively.
</p>
</div>
</dd>
<dt><a name="item744">[744]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00639" title="Abstract">arXiv:2310.00639</a> (cross-list from eess.IV) [<a href="/pdf/2310.00639" title="Download PDF">pdf</a>, <a href="/format/2310.00639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segmentation-based Assessment of Tumor-Vessel Involvement for Surgical  Resectability Prediction of Pancreatic Ductal Adenocarcinoma
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Viviers%2C+C">Christiaan Viviers</a>, 
<a href="/search/eess?searchtype=author&query=Ramaekers%2C+M">Mark Ramaekers</a>, 
<a href="/search/eess?searchtype=author&query=Valiuddin%2C+A">Amaan Valiuddin</a>, 
<a href="/search/eess?searchtype=author&query=Hellstr%C3%B6m%2C+T">Terese Hellstr&#xf6;m</a>, 
<a href="/search/eess?searchtype=author&query=Tasios%2C+N">Nick Tasios</a>, 
<a href="/search/eess?searchtype=author&query=van+der+Ven%2C+J">John van der Ven</a>, 
<a href="/search/eess?searchtype=author&query=Jacobs%2C+I">Igor Jacobs</a>, 
<a href="/search/eess?searchtype=author&query=Ewals%2C+L">Lotte Ewals</a>, 
<a href="/search/eess?searchtype=author&query=Nederend%2C+J">Joost Nederend</a>, 
<a href="/search/eess?searchtype=author&query=de+With%2C+P">Peter de With</a>, 
<a href="/search/eess?searchtype=author&query=Luyer%2C+M">Misha Luyer</a>, 
<a href="/search/eess?searchtype=author&query=van+der+Sommen%2C+F">Fons van der Sommen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV CVAMD 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Pancreatic ductal adenocarcinoma (PDAC) is a highly aggressive cancer with
limited treatment options. This research proposes a workflow and deep
learning-based segmentation models to automatically assess tumor-vessel
involvement, a key factor in determining tumor resectability. Correct
assessment of resectability is vital to determine treatment options. The
proposed workflow involves processing CT scans to segment the tumor and
vascular structures, analyzing spatial relationships and the extent of vascular
involvement, which follows a similar way of working as expert radiologists in
PDAC assessment. Three segmentation architectures (nnU-Net, 3D U-Net, and
Probabilistic 3D U-Net) achieve a high accuracy in segmenting veins, arteries,
and the tumor. The segmentations enable automated detection of tumor
involvement with high accuracy (0.88 sensitivity and 0.86 specificity) and
automated computation of the degree of tumor-vessel contact. Additionally, due
to significant inter-observer variability in these important structures, we
present the uncertainty captured by each of the models to further increase
insights into the predicted involvement. This result provides clinicians with a
clear indication of tumor-vessel involvement and may be used to facilitate more
informed decision-making for surgical interventions. The proposed method offers
a valuable tool for improving patient outcomes, personalized treatment
strategies and survival rates in pancreatic cancer.
</p>
</div>
</dd>
<dt><a name="item745">[745]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00644" title="Abstract">arXiv:2310.00644</a> (cross-list from quant-ph) [<a href="/pdf/2310.00644" title="Download PDF">pdf</a>, <a href="/format/2310.00644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Hardness of $\sf{S|LWE\rangle}$ with Gaussian and Other  Amplitudes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+Y">Yilei Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hu%2C+Z">Zihan Hu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Liu%2C+Q">Qipeng Liu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Luo%2C+H">Han Luo</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tu%2C+Y">Yaxin Tu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The learning with errors problem (LWE) is one of the most important building
blocks for post-quantum cryptography. To better understand the quantum hardness
of LWE, it is crucial to explore quantum variants of LWE, show quantum
algorithms for those variants, or prove they are as hard as standard LWE.
<br />To this end, Chen, Liu, and Zhandry [Eurocrypt 2022] define the
$\sf{S|LWE\rangle}$ problem, which encodes the error of LWE samples into
quantum amplitudes. They then show efficient quantum algorithms for
$\sf{S|LWE\rangle}$ with a few interesting amplitudes. However, the hardness of
the most interesting amplitude, Gaussian, was not addressed by Chen et al., or
only known for some restricted settings (for example, when the number of
$\sf{S|LWE\rangle}$ samples is very small, it is well known that
$\sf{S|LWE\rangle}$ is as hard as standard LWE).
<br />In this paper, we show new hardness and algorithms for $\sf{S|LWE\rangle}$
with Gaussian and other amplitudes. Our main results are
<br />1. There exist quantum reductions from standard LWE or worst-case GapSVP to
$\sf{S|LWE\rangle}$ with Gaussian amplitude with unknown phase, and arbitrarily
many $\sf{S|LWE\rangle}$ samples.
<br />2. There is a $2^{\widetilde{O}(\sqrt{n})}$-time algorithm for
$\sf{S|LWE\rangle}$ with Gaussian amplitude with known phase, given
$2^{\widetilde{O}(\sqrt{n})}$ many quantum samples. The algorithm is modified
from Kuperberg's sieve, and in fact works for more general amplitudes as long
as the amplitudes and phases are completely known.
<br />One way of interpreting our result is: to show a sub-exponential time quantum
algorithm for standard LWE, all we need is to handle phases in
$\sf{S|LWE\rangle}$ amplitudes better, either in the algorithm or the
reduction.
</p>
</div>
</dd>
<dt><a name="item746">[746]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00681" title="Abstract">arXiv:2310.00681</a> (cross-list from q-bio.BM) [<a href="/pdf/2310.00681" title="Download PDF">pdf</a>, <a href="/format/2310.00681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PharmacoNet: Accelerating Structure-based Virtual Screening by  Pharmacophore Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Seo%2C+S">Seonghwan Seo</a>, 
<a href="/search/q-bio?searchtype=author&query=Kim%2C+W+Y">Woo Youn Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">As the size of accessible compound libraries expands to over 10 billion, the
need for more efficient structure-based virtual screening methods is emerging.
Different pre-screening methods have been developed to rapidly screen the
library while retaining the hit candidates, but the structure-based methods
applicable to general proteins are still lacking: the challenge is to predict
the binding pose between proteins and ligands and perform scoring in an
extremely short time. We introduce PharmacoNet, a deep learning framework that
identifies optimal 3D pharmacophore arrangements which a ligand should have for
stable binding from binding sites. By coarse-grained graph matching between
ligands and generated pharmacophore arrangements, we solve the expensive
binding pose sampling and scoring procedures of existing methods in a single
step. PharmacoNet is significantly faster than state-of-the-art structure-based
approaches, yet reasonably accurate even with a simple scoring function.
Furthermore, we show the promising result that PharmacoNet effectively retains
hit candidates even under the rigorous pre-screening threshold. Overall, our
study uncovers the hitherto untapped potential of a pharmacophore modeling
approach in deep learning-based drug discovery.
</p>
</div>
</dd>
<dt><a name="item747">[747]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00747" title="Abstract">arXiv:2310.00747</a> (cross-list from q-fin.PM) [<a href="/pdf/2310.00747" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NoxTrader: LSTM-Based Stock Return Momentum Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Liu%2C+H">Hsiang-Hui Liu</a>, 
<a href="/search/q-fin?searchtype=author&query=Shu%2C+H">Han-Jay Shu</a>, 
<a href="/search/q-fin?searchtype=author&query=Chiu%2C+W">Wei-Ning Chiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Portfolio Management (q-fin.PM)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce NoxTrader, which is designed for portfolio construction and
trading execution, aims at generating profitable outcomes. The primary focus of
NoxTrader is on stock market trading with an emphasis on cultivating moderate
to long-term profits. The underlying learning process of NoxTrader hinges on
the assimilation of insights gleaned from historical trading data, primarily
hinging on time-series analysis due to the inherent nature of the employed
dataset. We delineate the sequential progression encompassing data acquisition,
feature engineering, predictive modeling, parameter configuration,
establishment of a rigorous backtesting framework, and ultimately position
NoxTrader as a testament to the prospective viability of algorithmic trading
models within real-world trading scenarios.
</p>
</div>
</dd>
<dt><a name="item748">[748]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00775" title="Abstract">arXiv:2310.00775</a> (cross-list from math.OC) [<a href="/pdf/2310.00775" title="Download PDF">pdf</a>, <a href="/format/2310.00775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-market Optimal Energy Storage Arbitrage with Capacity Blocking for  Emergency Services
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hashmi%2C+M+U">Md Umar Hashmi</a>, 
<a href="/search/math?searchtype=author&query=Hardy%2C+S">Stephen Hardy</a>, 
<a href="/search/math?searchtype=author&query=Van+Hertem%2C+D">Dirk Van Hertem</a>, 
<a href="/search/math?searchtype=author&query=Nagarajan%2C+H">Harsha Nagarajan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The future power system is increasingly interconnected via both AC and DC
interconnectors. These interconnectors establish links between previously
decoupled energy markets. In this paper, we propose an optimal multi-market
energy storage arbitrage model that includes emergency service provisions for
system operator(s). The model considers battery ramping and capacity
constraints and utilizes operating envelopes calculated based on interconnector
capacity, efficiency, dynamic energy injection and offshore wind generation in
the day-ahead market. The arbitrage model considers two separate electricity
prices for buying and selling of electricity in the two regions, connected via
an interconnector. Using disjunctive linearization of nonlinear terms, we
exactly reformulate the inter-regional energy arbitrage optimization as a mixed
integer linear programming problem. We propose two capacity limit selection
models for storage owners providing emergency services. The numerical analyses
focus on two interconnections linking Belgium and the UK. The results are
assessed based on revenue, operational cycles, payback period, shelf life and
computation times.
</p>
</div>
</dd>
<dt><a name="item749">[749]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00817" title="Abstract">arXiv:2310.00817</a> (cross-list from stat.ML) [<a href="/pdf/2310.00817" title="Download PDF">pdf</a>, <a href="/format/2310.00817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Make Adherence-Aware Advice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+G">Guanting Chen</a>, 
<a href="/search/stat?searchtype=author&query=Li%2C+X">Xiaocheng Li</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+C">Chunlin Sun</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+H">Hanzhao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">As artificial intelligence (AI) systems play an increasingly prominent role
in human decision-making, challenges surface in the realm of human-AI
interactions. One challenge arises from the suboptimal AI policies due to the
inadequate consideration of humans disregarding AI recommendations, as well as
the need for AI to provide advice selectively when it is most pertinent. This
paper presents a sequential decision-making model that (i) takes into account
the human's adherence level (the probability that the human follows/rejects
machine advice) and (ii) incorporates a defer option so that the machine can
temporarily refrain from making advice. We provide learning algorithms that
learn the optimal advice policy and make advice only at critical time stamps.
Compared to problem-agnostic reinforcement learning algorithms, our specialized
learning algorithms not only enjoy better theoretical convergence properties
but also show strong empirical performance.
</p>
</div>
</dd>
<dt><a name="item750">[750]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00894" title="Abstract">arXiv:2310.00894</a> (cross-list from eess.IV) [<a href="/pdf/2310.00894" title="Download PDF">pdf</a>, <a href="/format/2310.00894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JPEG Information Regularized Deep Image Prior for Denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Takagi%2C+T">Tsukasa Takagi</a>, 
<a href="/search/eess?searchtype=author&query=Ishizaki%2C+S">Shinya Ishizaki</a>, 
<a href="/search/eess?searchtype=author&query=Maeda%2C+S">Shin-ichi Maeda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE International Conference on Image Processing (ICIP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Image denoising is a representative image restoration task in computer
vision. Recent progress of image denoising from only noisy images has attracted
much attention. Deep image prior (DIP) demonstrated successful image denoising
from only a noisy image by inductive bias of convolutional neural network
architectures without any pre-training. The major challenge of DIP based image
denoising is that DIP would completely recover the original noisy image unless
applying early stopping. For early stopping without a ground-truth clean image,
we propose to monitor JPEG file size of the recovered image during optimization
as a proxy metric of noise levels in the recovered image. Our experiments show
that the compressed image file size works as an effective metric for early
stopping.
</p>
</div>
</dd>
<dt><a name="item751">[751]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00903" title="Abstract">arXiv:2310.00903</a> (cross-list from math.CA) [<a href="/pdf/2310.00903" title="Download PDF">pdf</a>, <a href="/ps/2310.00903" title="Download PostScript">ps</a>, <a href="/format/2310.00903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symmetric Solutions to Symmetric Partial Difference Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shankar%2C+S">Shiva Shankar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Classical Analysis and ODEs (math.CA)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper studies systems of linear difference equations on the lattice
$\Z^n$ that are invariant under a finite group of symmetries, and shows that
there exist solutions to such systems that are also invariant under this group
of symmetries.
</p>
</div>
</dd>
<dt><a name="item752">[752]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00919" title="Abstract">arXiv:2310.00919</a> (cross-list from eess.IV) [<a href="/pdf/2310.00919" title="Download PDF">pdf</a>, <a href="/format/2310.00919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BAAF: A Benchmark Attention Adaptive Framework for Medical Ultrasound  Image Segmentation Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+G">Gongping Chen</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+L">Lei Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Yin%2C+X">Xiaotao Yin</a>, 
<a href="/search/eess?searchtype=author&query=Cui%2C+L">Liang Cui</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+J">Jianxun Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Dai%2C+Y">Yu Dai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">The AI-based assisted diagnosis programs have been widely investigated on
medical ultrasound images. Complex scenario of ultrasound image, in which the
coupled interference of internal and external factors is severe, brings a
unique challenge for localize the object region automatically and precisely in
ultrasound images. In this study, we seek to propose a more general and robust
Benchmark Attention Adaptive Framework (BAAF) to assist doctors segment or
diagnose lesions and tissues in ultrasound images more quickly and accurately.
Different from existing attention schemes, the BAAF consists of a parallel
hybrid attention module (PHAM) and an adaptive calibration mechanism (ACM).
Specifically, BAAF first coarsely calibrates the input features from the
channel and spatial dimensions, and then adaptively selects more robust lesion
or tissue characterizations from the coarse-calibrated feature maps. The design
of BAAF further optimizes the "what" and "where" focus and selection problems
in CNNs and seeks to improve the segmentation accuracy of lesions or tissues in
medical ultrasound images. The method is evaluated on four medical ultrasound
segmentation tasks, and the adequate experimental results demonstrate the
remarkable performance improvement over existing state-of-the-art methods. In
addition, the comparison with existing attention mechanisms also demonstrates
the superiority of BAAF. This work provides the possibility for automated
medical ultrasound assisted diagnosis and reduces reliance on human accuracy
and precision.
</p>
</div>
</dd>
<dt><a name="item753">[753]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01032" title="Abstract">arXiv:2310.01032</a> (cross-list from stat.ML) [<a href="/pdf/2310.01032" title="Download PDF">pdf</a>, <a href="/ps/2310.01032" title="Download PostScript">ps</a>, <a href="/format/2310.01032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Fisher-Rao geometry of CES distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bouchard%2C+F">Florent Bouchard</a>, 
<a href="/search/stat?searchtype=author&query=Breloy%2C+A">Arnaud Breloy</a>, 
<a href="/search/stat?searchtype=author&query=Collas%2C+A">Antoine Collas</a>, 
<a href="/search/stat?searchtype=author&query=Renaux%2C+A">Alexandre Renaux</a>, 
<a href="/search/stat?searchtype=author&query=Ginolhac%2C+G">Guillaume Ginolhac</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
<p class="mathjax">When dealing with a parametric statistical model, a Riemannian manifold can
naturally appear by endowing the parameter space with the Fisher information
metric. The geometry induced on the parameters by this metric is then referred
to as the Fisher-Rao information geometry. Interestingly, this yields a point
of view that allows for leveragingmany tools from differential geometry. After
a brief introduction about these concepts, we will present some practical uses
of these geometric tools in the framework of elliptical distributions. This
second part of the exposition is divided into three main axes: Riemannian
optimization for covariance matrix estimation, Intrinsic Cram\'er-Rao bounds,
and classification using Riemannian distances.
</p>
</div>
</dd>
<dt><a name="item754">[754]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01037" title="Abstract">arXiv:2310.01037</a> (cross-list from physics.geo-ph) [<a href="/pdf/2310.01037" title="Download PDF">pdf</a>, <a href="/format/2310.01037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seismogram Transformer: A generic deep learning backbone network for  multiple earthquake monitoring tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Li%2C+S">Sen Li</a>, 
<a href="/search/physics?searchtype=author&query=Yang%2C+X">Xu Yang</a>, 
<a href="/search/physics?searchtype=author&query=Cao%2C+A">Anye Cao</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+C">Changbin Wang</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+Y">Yaoqi Liu</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+Y">Yapeng Liu</a>, 
<a href="/search/physics?searchtype=author&query=Niu%2C+Q">Qiang Niu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Seismic records, known as seismograms, are crucial records of ground motion
resulting from seismic events, constituting the backbone of earthquake research
and monitoring. The latest advancements in deep learning have significantly
facilitated various seismic signal processing tasks. This paper introduces a
novel backbone neural network model designed for various seismic monitoring
tasks, named Seismogram Transformer (SeisT). Thanks to its efficient network
architecture, SeisT matches or even outperforms the state-of-the-art models in
earthquake detection, seismic phase picking, first-motion polarity
classification, magnitude estimation, and azimuth estimation tasks,
particularly in terms of out-of-distribution generalization performance. SeisT
consists of multiple network layers composed of different foundational blocks,
which help the model understand multi-level feature representations of
seismograms from low-level to high-level complex features, effectively
extracting features such as frequency, phase, and time-frequency relationships
from input seismograms. Three different-sized models were customized based on
these diverse foundational modules. Through extensive experiments and
performance evaluations, this study showcases the capabilities and potential of
SeisT in advancing seismic signal processing and earthquake research.
</p>
</div>
</dd>
<dt><a name="item755">[755]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01046" title="Abstract">arXiv:2310.01046</a> (cross-list from physics.soc-ph) [<a href="/pdf/2310.01046" title="Download PDF">pdf</a>, <a href="/format/2310.01046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Epistemic integration and social segregation of AI in neuroscience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Fontaine%2C+S">Sylvain Fontaine</a>, 
<a href="/search/physics?searchtype=author&query=Gargiulo%2C+F">Floriana Gargiulo</a>, 
<a href="/search/physics?searchtype=author&query=Dubois%2C+M">Michel Dubois</a>, 
<a href="/search/physics?searchtype=author&query=Tubaro%2C+P">Paola Tubaro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">In recent years, Artificial Intelligence (AI) shows a spectacular ability of
insertion inside a variety of disciplines which use it for scientific
advancements and which sometimes improve it for their conceptual and
methodological needs. According to the transverse science framework originally
conceived by Shinn and Joerges, AI can be seen as an instrument which is
progressively acquiring an universal character through its diffusion across
science. In this paper we address empirically one aspect of this diffusion,
namely the penetration of AI into a specific field of research. Taking
neuroscience as a case study, we conduct a scientometric analysis of the
development of AI in this field We especially study the temporal egocentric
citation network around the articles included in this literature, their
represented journals and their authors linked together by a temporal
collaboration network. We find that AI is driving the constitution of a
particular disciplinary ecosystem in neuroscience which is distinct from other
subfields when regarding the references, and which is gathering atypical
scientific profiles who are coming from neuroscience or outside it. Moreover we
observe that this AI community in neuroscience is socially confined in a
specific zone of the neuroscience collaboration network, which is also keeping
to publish in a small set of dedicated journals that are mostly active in AI
research. According to these results, the diffusion of AI in a discipline such
as neuroscience didn't really challenge its disciplinary orientations but
rather induced the constitution of a dedicated socio-cognitive workforce inside
this field.
</p>
</div>
</dd>
<dt><a name="item756">[756]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01063" title="Abstract">arXiv:2310.01063</a> (cross-list from q-fin.RM) [<a href="/pdf/2310.01063" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Deep Learning and GARCH Models for Financial Volatility and  Risk Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Micha%C5%84k%C3%B3w%2C+J">Jakub Micha&#x144;k&#xf3;w</a>, 
<a href="/search/q-fin?searchtype=author&query=Kwiatkowski%2C+%C5%81">&#x141;ukasz Kwiatkowski</a>, 
<a href="/search/q-fin?searchtype=author&query=Morajda%2C+J">Janusz Morajda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Risk Management (q-fin.RM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Computational Finance (q-fin.CP); General Finance (q-fin.GN)

</div>
<p class="mathjax">In this paper, we develop a hybrid approach to forecasting the volatility and
risk of financial instruments by combining common econometric GARCH time series
models with deep learning neural networks. For the latter, we employ Gated
Recurrent Unit (GRU) networks, whereas four different specifications are used
as the GARCH component: standard GARCH, EGARCH, GJR-GARCH and APARCH. Models
are tested using daily logarithmic returns on the S&amp;P 500 index as well as gold
price Bitcoin prices, with the three assets representing quite distinct
volatility dynamics. As the main volatility estimator, also underlying the
target function of our hybrid models, we use the price-range-based Garman-Klass
estimator, modified to incorporate the opening and closing prices. Volatility
forecasts resulting from the hybrid models are employed to evaluate the assets'
risk using the Value-at-Risk (VaR) and Expected Shortfall (ES) at two different
tolerance levels of 5% and 1%. Gains from combining the GARCH and GRU
approaches are discussed in the contexts of both the volatility and risk
forecasts. In general, it can be concluded that the hybrid solutions produce
more accurate point volatility forecasts, although it does not necessarily
translate into superior VaR and ES forecasts.
</p>
</div>
</dd>
<dt><a name="item757">[757]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01099" title="Abstract">arXiv:2310.01099</a> (cross-list from eess.IV) [<a href="/pdf/2310.01099" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyMNet: a Multimodal Deep Learning System for Hypertension  Classification using Fundus Photographs and Cardiometabolic Risk Factors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Baharoon%2C+M">Mohammed Baharoon</a>, 
<a href="/search/eess?searchtype=author&query=Almatar%2C+H">Hessa Almatar</a>, 
<a href="/search/eess?searchtype=author&query=Alduhayan%2C+R">Reema Alduhayan</a>, 
<a href="/search/eess?searchtype=author&query=Aldebasi%2C+T">Tariq Aldebasi</a>, 
<a href="/search/eess?searchtype=author&query=Alahmadi%2C+B">Badr Alahmadi</a>, 
<a href="/search/eess?searchtype=author&query=Bokhari%2C+Y">Yahya Bokhari</a>, 
<a href="/search/eess?searchtype=author&query=Alawad%2C+M">Mohammed Alawad</a>, 
<a href="/search/eess?searchtype=author&query=Almazroa%2C+A">Ahmed Almazroa</a>, 
<a href="/search/eess?searchtype=author&query=Aljouie%2C+A">Abdulrhman Aljouie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, deep learning has shown promise in predicting hypertension
(HTN) from fundus images. However, most prior research has primarily focused on
analyzing a single type of data, which may not capture the full complexity of
HTN risk. To address this limitation, this study introduces a multimodal deep
learning (MMDL) system, dubbed HyMNet, which combines fundus images and
cardiometabolic risk factors, specifically age and gender, to improve
hypertension detection capabilities. Our MMDL system uses the DenseNet-201
architecture, pre-trained on ImageNet, for the fundus imaging path and a fully
connected neural network for the age and gender path. The two paths are jointly
trained by concatenating 64 features output from each path that are then fed
into a fusion network. The system was trained on 1,143 retinal images from 626
individuals collected from the Saudi Ministry of National Guard Health Affairs.
The results show that the multimodal model that integrates fundus images along
with age and gender achieved an AUC of 0.791 [CI: 0.735, 0.848], which
outperforms the unimodal model trained solely on fundus photographs that
yielded an AUC of 0.766 [CI: 0.705, 0.828] for hypertension detection.
</p>
</div>
</dd>
<dt><a name="item758">[758]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01103" title="Abstract">arXiv:2310.01103</a> (cross-list from physics.comp-ph) [<a href="/pdf/2310.01103" title="Download PDF">pdf</a>, <a href="/format/2310.01103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An efficient computational model of the in-flow capturing of magnetic  nanoparticles by a cylindrical magnet for cancer nanomedicine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Wirthl%2C+B">Barbara Wirthl</a>, 
<a href="/search/physics?searchtype=author&query=Wirthl%2C+V">Vitaly Wirthl</a>, 
<a href="/search/physics?searchtype=author&query=Wall%2C+W+A">Wolfgang A. Wall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Magnetic nanoparticles have emerged as a promising approach to improving
cancer treatment. However, many novel nanoparticle designs fail in clinical
trials due to a lack of understanding of how to overcome the in vivo transport
barriers. To address this shortcoming, we develop a novel computational model
aimed at the study of magnetic nanoparticles in vitro and in vivo. In this
paper, we present an important building block for this overall goal, namely an
efficient computational model of the in-flow capture of magnetic nanoparticles
by a cylindrical permanent magnet in an idealised test setup. We use a
continuum approach based on the Smoluchowski advection-diffusion equation,
combined with a simple approach to consider the capture at an impenetrable
boundary, and derive an analytical expression for the magnetic force of a
cylindrical magnet of finite length on the nanoparticles. This provides a
simple and numerically efficient way to study different magnet configurations
and their influence on the nanoparticle distribution in three dimensions. Such
an in silico model can increase insight into the underlying physics, help to
design novel prototypes and serve as a precursor to more complex systems in
vivo and in silico.
</p>
</div>
</dd>
<dt><a name="item759">[759]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01117" title="Abstract">arXiv:2310.01117</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2310.01117" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting emergence of crystals from amorphous matter with deep  learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Aykol%2C+M">Muratahan Aykol</a>, 
<a href="/search/cond-mat?searchtype=author&query=Merchant%2C+A">Amil Merchant</a>, 
<a href="/search/cond-mat?searchtype=author&query=Batzner%2C+S">Simon Batzner</a>, 
<a href="/search/cond-mat?searchtype=author&query=Wei%2C+J+N">Jennifer N. Wei</a>, 
<a href="/search/cond-mat?searchtype=author&query=Cubuk%2C+E+D">Ekin Dogus Cubuk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 main figures, 4 supplementary figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Crystallization of the amorphous phases into metastable crystals plays a
fundamental role in the formation of new matter, from geological to biological
processes in nature to synthesis and development of new materials in the
laboratory. Predicting the outcome of such phase transitions reliably would
enable new research directions in these areas, but has remained beyond reach
with molecular modeling or ab-initio methods. Here, we show that
crystallization products of amorphous phases can be predicted in any inorganic
chemistry by sampling the crystallization pathways of their local structural
motifs at the atomistic level using universal deep learning potentials. We show
that this approach identifies the crystal structures of polymorphs that
initially nucleate from amorphous precursors with high accuracy across a
diverse set of material systems, including polymorphic oxides, nitrides,
carbides, fluorides, chlorides, chalcogenides, and metal alloys. Our results
demonstrate that Ostwald's rule of stages can be exploited mechanistically at
the molecular level to predictably access new metastable crystals from the
amorphous phase in material synthesis.
</p>
</div>
</dd>
<dt><a name="item760">[760]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01128" title="Abstract">arXiv:2310.01128</a> (cross-list from eess.AS) [<a href="/pdf/2310.01128" title="Download PDF">pdf</a>, <a href="/format/2310.01128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangling Voice and Content with Self-Supervision for Speaker  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+T">Tianchi Liu</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+K+A">Kong Aik Lee</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Q">Qiongqiong Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Haizhou Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 (main track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">For speaker recognition, it is difficult to extract an accurate speaker
representation from speech because of its mixture of speaker traits and
content. This paper proposes a disentanglement framework that simultaneously
models speaker traits and content variability in speech. It is realized with
the use of three Gaussian inference layers, each consisting of a learnable
transition model that extracts distinct speech components. Notably, a
strengthened transition model is specifically designed to model complex speech
dynamics. We also propose a self-supervision method to dynamically disentangle
content without the use of labels other than speaker identities. The efficacy
of the proposed framework is validated via experiments conducted on the
VoxCeleb and SITW datasets with 9.56% and 8.24% average reductions in EER and
minDCF, respectively. Since neither additional model training nor data is
specifically needed, it is easily applicable in practical use.
</p>
</div>
</dd>
<dt><a name="item761">[761]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01130" title="Abstract">arXiv:2310.01130</a> (cross-list from eess.IV) [<a href="/pdf/2310.01130" title="Download PDF">pdf</a>, <a href="/format/2310.01130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CommIN: Semantic Image Communications as an Inverse Problem with  INN-Guided Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+J">Jiakang Chen</a>, 
<a href="/search/eess?searchtype=author&query=You%2C+D">Di You</a>, 
<a href="/search/eess?searchtype=author&query=G%C3%BCnd%C3%BCz%2C+D">Deniz G&#xfc;nd&#xfc;z</a>, 
<a href="/search/eess?searchtype=author&query=Dragotti%2C+P+L">Pier Luigi Dragotti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Joint source-channel coding schemes based on deep neural networks (DeepJSCC)
have recently achieved remarkable performance for wireless image transmission.
However, these methods usually focus only on the distortion of the
reconstructed signal at the receiver side with respect to the source at the
transmitter side, rather than the perceptual quality of the reconstruction
which carries more semantic information. As a result, severe perceptual
distortion can be introduced under extreme conditions such as low bandwidth and
low signal-to-noise ratio. In this work, we propose CommIN, which views the
recovery of high-quality source images from degraded reconstructions as an
inverse problem. To address this, CommIN combines Invertible Neural Networks
(INN) with diffusion models, aiming for superior perceptual quality. Through
experiments, we show that our CommIN significantly improves the perceptual
quality compared to DeepJSCC under extreme conditions and outperforms other
inverse problem approaches used in DeepJSCC.
</p>
</div>
</dd>
<dt><a name="item762">[762]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01159" title="Abstract">arXiv:2310.01159</a> (cross-list from eess.IV) [<a href="/pdf/2310.01159" title="Download PDF">pdf</a>, <a href="/format/2310.01159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterative Semi-Supervised Learning for Abdominal Organs and Tumor  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhuang%2C+J">Jiaxin Zhuang</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+L">Luyang Luo</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Z">Zhixuan Chen</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+L">Linshan Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2309.05405">arXiv:2309.05405</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep-learning (DL) based methods are playing an important role in the task of
abdominal organs and tumors segmentation in CT scans. However, the large
requirements of annotated datasets heavily limit its development. The FLARE23
challenge provides a large-scale dataset with both partially and fully
annotated data, which also focuses on both segmentation accuracy and
computational efficiency. In this study, we propose to use the strategy of
Semi-Supervised Learning (SSL) and iterative pseudo labeling to address
FLARE23. Initially, a deep model (nn-UNet) trained on datasets with complete
organ annotations (about 220 scans) generates pseudo labels for the whole
dataset. These pseudo labels are then employed to train a more powerful
segmentation model. Employing the FLARE23 dataset, our approach achieves an
average DSC score of 89.63% for organs and 46.07% for tumors on online
validation leaderboard. For organ segmentation, We obtain 0.9007\% DSC and
0.9493\% NSD. For tumor segmentation, we obtain 0.3785% DSC and 0.2842% NSD.
Our code is available at https://github.com/USTguy/Flare23.
</p>
</div>
</dd>
<dt><a name="item763">[763]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01169" title="Abstract">arXiv:2310.01169</a> (cross-list from cond-mat.dis-nn) [<a href="/pdf/2310.01169" title="Download PDF">pdf</a>, <a href="/format/2310.01169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fitting an ellipsoid to random points: predictions using the replica  method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Maillard%2C+A">Antoine Maillard</a>, 
<a href="/search/cond-mat?searchtype=author&query=Kunisky%2C+D">Dmitriy Kunisky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Disordered Systems and Neural Networks (cond-mat.dis-nn)</span>; Statistical Mechanics (cond-mat.stat-mech); Data Structures and Algorithms (cs.DS); Probability (math.PR); Statistics Theory (math.ST)

</div>
<p class="mathjax">We consider the problem of fitting a centered ellipsoid to $n$ standard
Gaussian random vectors in $\mathbb{R}^d$, as $n, d \to \infty$ with $n/d^2 \to
\alpha &gt; 0$. It has been conjectured that this problem is, with high
probability, satisfiable (SAT; that is, there exists an ellipsoid passing
through all $n$ points) for $\alpha &lt; 1/4$, and unsatisfiable (UNSAT) for
$\alpha &gt; 1/4$. In this work we give a precise analytical argument, based on
the non-rigorous replica method of statistical physics, that indeed predicts a
SAT/UNSAT transition at $\alpha = 1/4$, as well as the shape of a typical
fitting ellipsoid in the SAT phase (i.e., the lengths of its principal axes).
Besides the replica method, our main tool is the dilute limit of extensive-rank
"HCIZ integrals" of random matrix theory. We further study different explicit
algorithmic constructions of the matrix characterizing the ellipsoid. In
particular, we show that a procedure based on minimizing its nuclear norm
yields a solution in the whole SAT phase. Finally, we characterize the
SAT/UNSAT transition for ellipsoid fitting of a large class of
rotationally-invariant random vectors. Our work suggests mathematically
rigorous ways to analyze fitting ellipsoids to random vectors, which is the
topic of a companion work.
</p>
</div>
</dd>
<dt><a name="item764">[764]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01176" title="Abstract">arXiv:2310.01176</a> (cross-list from eess.IV) [<a href="/pdf/2310.01176" title="Download PDF">pdf</a>, <a href="/format/2310.01176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-adversarial local distribution regularization for semi-supervised  medical image segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nguyen-Duc%2C+T">Thanh Nguyen-Duc</a>, 
<a href="/search/eess?searchtype=author&query=Le%2C+T">Trung Le</a>, 
<a href="/search/eess?searchtype=author&query=Bammer%2C+R">Roland Bammer</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+H">He Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Cai%2C+J">Jianfei Cai</a>, 
<a href="/search/eess?searchtype=author&query=Phung%2C+D">Dinh Phung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MICCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Medical semi-supervised segmentation is a technique where a model is trained
to segment objects of interest in medical images with limited annotated data.
Existing semi-supervised segmentation methods are usually based on the
smoothness assumption. This assumption implies that the model output
distributions of two similar data samples are encouraged to be invariant. In
other words, the smoothness assumption states that similar samples (e.g.,
adding small perturbations to an image) should have similar outputs. In this
paper, we introduce a novel cross-adversarial local distribution (Cross-ALD)
regularization to further enhance the smoothness assumption for semi-supervised
medical image segmentation task. We conducted comprehensive experiments that
the Cross-ALD archives state-of-the-art performance against many recent methods
on the public LA and ACDC datasets.
</p>
</div>
</dd>
<dt><a name="item765">[765]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01189" title="Abstract">arXiv:2310.01189</a> (cross-list from stat.ML) [<a href="/pdf/2310.01189" title="Download PDF">pdf</a>, <a href="/format/2310.01189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> If there is no underfitting, there is no Cold Posterior Effect
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+Y">Yijie Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Wu%2C+Y">Yi-Shan Wu</a>, 
<a href="/search/stat?searchtype=author&query=Ortega%2C+L+A">Luis A. Ortega</a>, 
<a href="/search/stat?searchtype=author&query=Masegosa%2C+A+R">Andr&#xe9;s R. Masegosa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures, ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The cold posterior effect (CPE) (Wenzel et al., 2020) in Bayesian deep
learning shows that, for posteriors with a temperature $T&lt;1$, the resulting
posterior predictive could have better performances than the Bayesian posterior
($T=1$). As the Bayesian posterior is known to be optimal under perfect model
specification, many recent works have studied the presence of CPE as a model
misspecification problem, arising from the prior and/or from the likelihood
function. In this work, we provide a more nuanced understanding of the CPE as
we show that misspecification leads to CPE only when the resulting Bayesian
posterior underfits. In fact, we theoretically show that if there is no
underfitting, there is no CPE.
</p>
</div>
</dd>
<dt><a name="item766">[766]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01202" title="Abstract">arXiv:2310.01202</a> (cross-list from stat.ML) [<a href="/pdf/2310.01202" title="Download PDF">pdf</a>, <a href="/format/2310.01202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Uncertainty Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chaudhuri%2C+K">Kamalika Chaudhuri</a>, 
<a href="/search/stat?searchtype=author&query=Lopez-Paz%2C+D">David Lopez-Paz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">To build robust, fair, and safe AI systems, we would like our classifiers to
say ``I don't know'' when facing test examples that are difficult or fall
outside of the training classes.The ubiquitous strategy to predict under
uncertainty is the simplistic \emph{reject-or-classify} rule: abstain from
prediction if epistemic uncertainty is high, classify otherwise.Unfortunately,
this recipe does not allow different sources of uncertainty to communicate with
each other, produces miscalibrated predictions, and it does not allow to
correct for misspecifications in our uncertainty estimates. To address these
three issues, we introduce \emph{unified uncertainty calibration (U2C)}, a
holistic framework to combine aleatoric and epistemic uncertainties. U2C
enables a clean learning-theoretical analysis of uncertainty estimation, and
outperforms reject-or-classify across a variety of ImageNet benchmarks.
</p>
</div>
</dd>
<dt><a name="item767">[767]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01210" title="Abstract">arXiv:2310.01210</a> (cross-list from eess.IV) [<a href="/pdf/2310.01210" title="Download PDF">pdf</a>, <a href="/format/2310.01210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Cardiac Segmentation using Graph Convolutional Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Van+De+Vyver%2C+G">Gilles Van De Vyver</a>, 
<a href="/search/eess?searchtype=author&query=Thomas%2C+S">Sarina Thomas</a>, 
<a href="/search/eess?searchtype=author&query=Ben-Yosef%2C+G">Guy Ben-Yosef</a>, 
<a href="/search/eess?searchtype=author&query=Olaisen%2C+S+H">Sindre Hellum Olaisen</a>, 
<a href="/search/eess?searchtype=author&query=Dalen%2C+H">H&#xe5;vard Dalen</a>, 
<a href="/search/eess?searchtype=author&query=L%C3%B8vstakken%2C+L">Lasse L&#xf8;vstakken</a>, 
<a href="/search/eess?searchtype=author&query=Smistad%2C+E">Erik Smistad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Fully automatic cardiac segmentation can be a fast and reproducible method to
extract clinical measurements from an echocardiography examination. The U-Net
architecture is the current state-of-the-art deep learning architecture for
medical segmentation and can segment cardiac structures in real-time with
average errors comparable to inter-observer variability. However, this
architecture still generates large outliers that are often anatomically
incorrect. This work uses the concept of graph convolutional neural networks
that predict the contour points of the structures of interest instead of
labeling each pixel. We propose a graph architecture that uses two
convolutional rings based on cardiac anatomy and show that this eliminates
anatomical incorrect multi-structure segmentations on the publicly available
CAMUS dataset. Additionally, this work contributes with an ablation study on
the graph convolutional architecture and an evaluation of clinical measurements
on the clinical HUNT4 dataset. Finally, we propose to use the inter-model
agreement of the U-Net and the graph network as a predictor of both the input
and segmentation quality. We show this predictor can detect out-of-distribution
and unsuitable input images in real-time. Source code is available online:
https://github.com/gillesvntnu/GCN_multistructure
</p>
</div>
</dd>
<dt><a name="item768">[768]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01213" title="Abstract">arXiv:2310.01213</a> (cross-list from math.CO) [<a href="/pdf/2310.01213" title="Download PDF">pdf</a>, <a href="/format/2310.01213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure and growth of \mathbb{R}-bonacci words
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dovgal%2C+S">Sergey Dovgal</a>, 
<a href="/search/math?searchtype=author&query=Kirgizov%2C+S">Sergey Kirgizov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 7 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">A Sturmian word of slope $q$ is the cutting sequence of a half-line $y=qx$.
We establish a bijection between sequences of certain prefixes of the Sturmian
word of slope $q$, and the $q$-decreasing words, which are binary words whose
maximal factors of the form $0^a1^b$ satisfy $q \cdot a &gt; b$ whenever $a&gt;0$. We
also show that the number of $q$-decreasing words of length $n$ grows as
$\Phi(q)^{n(1 + o(1))}$, where $\Phi(1)$ is the golden ratio, $\Phi(2)$ is
equal to the tribonacci constant, and that the function $\Phi(q)$ is strictly
increasing, discontinuous at every rational point, and exhibits a nice fractal
structure related to the Stern--Brocot tree and Minkowski's question mark
function.
</p>
</div>
</dd>
<dt><a name="item769">[769]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01221" title="Abstract">arXiv:2310.01221</a> (cross-list from math.AP) [<a href="/pdf/2310.01221" title="Download PDF">pdf</a>, <a href="/ps/2310.01221" title="Download PostScript">ps</a>, <a href="/format/2310.01221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlocal diffusion model with maximum principle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shi%2C+Z">Zuoqiang Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this paper, we propose nonlocal diffusion models with Dirichlet boundary.
These nonlocal diffusion models preserve the maximum principle and also have
corresponding variational form. With these good properties, It is relatively
easy to prove the well-posedness and the vanishing nonlocality convergence.
Furthermore, by specifically designed weight function, we can get a nonlocal
diffusion model with second order convergence which is optimal for nonlocal
diffusion models.
</p>
</div>
</dd>
<dt><a name="item770">[770]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01225" title="Abstract">arXiv:2310.01225</a> (cross-list from stat.ML) [<a href="/pdf/2310.01225" title="Download PDF">pdf</a>, <a href="/format/2310.01225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A path-norm toolkit for modern networks: consequences, promises and  challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Gonon%2C+A">Antoine Gonon</a>, 
<a href="/search/stat?searchtype=author&query=Brisebarre%2C+N">Nicolas Brisebarre</a>, 
<a href="/search/stat?searchtype=author&query=Riccietti%2C+E">Elisa Riccietti</a>, 
<a href="/search/stat?searchtype=author&query=Gribonval%2C+R">R&#xe9;mi Gribonval</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">This work introduces the first toolkit around path-norms that is fully able
to encompass general DAG ReLU networks with biases, skip connections and max
pooling. This toolkit notably allows us to establish generalization bounds for
real modern neural networks that are not only the most widely applicable
path-norm based ones, but also recover or beat the sharpest known bounds of
this type. These extended path-norms further enjoy the usual benefits of
path-norms: ease of computation, invariance under the symmetries of the
network, and improved sharpness on feedforward networks compared to the product
of operators' norms, another complexity measure most commonly used.
<br />The versatility of the toolkit and its ease of implementation allow us to
challenge the concrete promises of path-norm-based generalization bounds, by
numerically evaluating the sharpest known bounds for ResNets on ImageNet.
</p>
</div>
</dd>
<dt><a name="item771">[771]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01227" title="Abstract">arXiv:2310.01227</a> (cross-list from astro-ph.EP) [<a href="/pdf/2310.01227" title="Download PDF">pdf</a>, <a href="/format/2310.01227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstructing Atmospheric Parameters of Exoplanets Using Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Giobergia%2C+F">Flavio Giobergia</a>, 
<a href="/search/astro-ph?searchtype=author&query=Koudounas%2C+A">Alkis Koudounas</a>, 
<a href="/search/astro-ph?searchtype=author&query=Baralis%2C+E">Elena Baralis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages + references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Earth and Planetary Astrophysics (astro-ph.EP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Exploring exoplanets has transformed our understanding of the universe by
revealing many planetary systems that defy our current understanding. To study
their atmospheres, spectroscopic observations are used to infer essential
atmospheric properties that are not directly measurable. Estimating atmospheric
parameters that best fit the observed spectrum within a specified atmospheric
model is a complex problem that is difficult to model. In this paper, we
present a multi-target probabilistic regression approach that combines deep
learning and inverse modeling techniques within a multimodal architecture to
extract atmospheric parameters from exoplanets. Our methodology overcomes
computational limitations and outperforms previous approaches, enabling
efficient analysis of exoplanetary atmospheres. This research contributes to
advancements in the field of exoplanet research and offers valuable insights
for future studies.
</p>
</div>
</dd>
<dt><a name="item772">[772]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01236" title="Abstract">arXiv:2310.01236</a> (cross-list from stat.ML) [<a href="/pdf/2310.01236" title="Download PDF">pdf</a>, <a href="/format/2310.01236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mirror Diffusion Models for Constrained and Watermarked Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liu%2C+G">Guan-Horng Liu</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+T">Tianrong Chen</a>, 
<a href="/search/stat?searchtype=author&query=Theodorou%2C+E+A">Evangelos A. Theodorou</a>, 
<a href="/search/stat?searchtype=author&query=Tao%2C+M">Molei Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to NeurIPS on 5/18 but did not arxiv per NeurIPS policy, accepted on 9/22
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Modern successes of diffusion models in learning complex, high-dimensional
data distributions are attributed, in part, to their capability to construct
diffusion processes with analytic transition kernels and score functions. The
tractability results in a simulation-free framework with stable regression
losses, from which reversed, generative processes can be learned at scale.
However, when data is confined to a constrained set as opposed to a standard
Euclidean space, these desirable characteristics appear to be lost based on
prior attempts. In this work, we propose Mirror Diffusion Models (MDM), a new
class of diffusion models that generate data on convex constrained sets without
losing any tractability. This is achieved by learning diffusion processes in a
dual space constructed from a mirror map, which, crucially, is a standard
Euclidean space. We derive efficient computation of mirror maps for popular
constrained sets, such as simplices and $\ell_2$-balls, showing significantly
improved performance of MDM over existing methods. For safety and privacy
purposes, we also explore constrained sets as a new mechanism to embed
invisible but quantitative information (i.e., watermarks) in generated data,
for which MDM serves as a compelling approach. Our work brings new algorithmic
opportunities for learning tractable diffusion on complex domains.
</p>
</div>
</dd>
<dt><a name="item773">[773]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01256" title="Abstract">arXiv:2310.01256</a> (cross-list from math.AP) [<a href="/pdf/2310.01256" title="Download PDF">pdf</a>, <a href="/ps/2310.01256" title="Download PostScript">ps</a>, <a href="/format/2310.01256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Gevrey class implicit mapping theorem with application to UQ of  semilinear elliptic PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Harbrecht%2C+H">Helmut Harbrecht</a>, 
<a href="/search/math?searchtype=author&query=Schmidlin%2C+M">Marc Schmidlin</a>, 
<a href="/search/math?searchtype=author&query=Schwab%2C+C">Christoph Schwab</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">This article is concerned with a regularity analysis of parametric operator
equations with a perspective on uncertainty quantification. We study the
regularity of mappings between Banach spaces near branches of isolated
solutions that are implicitly defined by a residual equation. Under $s$-Gevrey
assumptions on on the residual equation, we establish $s$-Gevrey bounds on the
Fr\'echet derivatives of the local data-to-solution mapping. This abstract
framework is illustrated in a proof of regularity bounds for a semilinear
elliptic partial differential equation with parametric and random field input.
</p>
</div>
</dd>
<dt><a name="item774">[774]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01258" title="Abstract">arXiv:2310.01258</a> (cross-list from eess.IV) [<a href="/pdf/2310.01258" title="Download PDF">pdf</a>, <a href="/format/2310.01258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MobileNVC: Real-time 1080p Neural Video Compression on a Mobile Device
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=van+Rozendaal%2C+T">Ties van Rozendaal</a>, 
<a href="/search/eess?searchtype=author&query=Singhal%2C+T">Tushar Singhal</a>, 
<a href="/search/eess?searchtype=author&query=Le%2C+H">Hoang Le</a>, 
<a href="/search/eess?searchtype=author&query=Sautiere%2C+G">Guillaume Sautiere</a>, 
<a href="/search/eess?searchtype=author&query=Said%2C+A">Amir Said</a>, 
<a href="/search/eess?searchtype=author&query=Buska%2C+K">Krishna Buska</a>, 
<a href="/search/eess?searchtype=author&query=Raha%2C+A">Anjuman Raha</a>, 
<a href="/search/eess?searchtype=author&query=Kalatzis%2C+D">Dimitris Kalatzis</a>, 
<a href="/search/eess?searchtype=author&query=Mehta%2C+H">Hitarth Mehta</a>, 
<a href="/search/eess?searchtype=author&query=Mayer%2C+F">Frank Mayer</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+L">Liang Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Nagel%2C+M">Markus Nagel</a>, 
<a href="/search/eess?searchtype=author&query=Wiggers%2C+A">Auke Wiggers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Neural video codecs have recently become competitive with standard codecs
such as HEVC in the low-delay setting. However, most neural codecs are large
floating-point networks that use pixel-dense warping operations for temporal
modeling, making them too computationally expensive for deployment on mobile
devices. Recent work has demonstrated that running a neural decoder in real
time on mobile is feasible, but shows this only for 720p RGB video, while the
YUV420 format is more commonly used in production. This work presents the first
neural video codec that decodes 1080p YUV420 video in real time on a mobile
device. Our codec relies on two major contributions. First, we design an
efficient codec that uses a block-based motion compensation algorithm available
on the warping core of the mobile accelerator, and we show how to quantize this
model to integer precision. Second, we implement a fast decoder pipeline that
concurrently runs neural network components on the neural signal processor,
parallel entropy coding on the mobile GPU, and warping on the warping core. Our
codec outperforms the previous on-device codec by a large margin with up to 48
% BD-rate savings, while reducing the MAC count on the receiver side by 10x. We
perform a careful ablation to demonstrate the effect of the introduced motion
compensation scheme, and ablate the effect of model quantization.
</p>
</div>
</dd>
<dt><a name="item775">[775]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01285" title="Abstract">arXiv:2310.01285</a> (cross-list from q-fin.CP) [<a href="/pdf/2310.01285" title="Download PDF">pdf</a>, <a href="/format/2310.01285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated regime detection in multidimensional time series data using  sliced Wasserstein k-means clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Luan%2C+Q">Qinmeng Luan</a>, 
<a href="/search/q-fin?searchtype=author&query=Hamp%2C+J">James Hamp</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Finance (q-fin.CP)</span>; Machine Learning (cs.LG); Mathematical Finance (q-fin.MF); Machine Learning (stat.ML)

</div>
<p class="mathjax">Recent work has proposed Wasserstein k-means (Wk-means) clustering as a
powerful method to identify regimes in time series data, and one-dimensional
asset returns in particular. In this paper, we begin by studying in detail the
behaviour of the Wasserstein k-means clustering algorithm applied to synthetic
one-dimensional time series data. We study the dynamics of the algorithm and
investigate how varying different hyperparameters impacts the performance of
the clustering algorithm for different random initialisations. We compute
simple metrics that we find are useful in identifying high-quality clusterings.
Then, we extend the technique of Wasserstein k-means clustering to
multidimensional time series data by approximating the multidimensional
Wasserstein distance as a sliced Wasserstein distance, resulting in a method we
call `sliced Wasserstein k-means (sWk-means) clustering'. We apply the
sWk-means clustering method to the problem of automated regime detection in
multidimensional time series data, using synthetic data to demonstrate the
validity of the approach. Finally, we show that the sWk-means method is
effective in identifying distinct market regimes in real multidimensional
financial time series, using publicly available foreign exchange spot rate data
as a case study. We conclude with remarks about some limitations of our
approach and potential complementary or alternative approaches.
</p>
</div>
</dd>
<dt><a name="item776">[776]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01326" title="Abstract">arXiv:2310.01326</a> (cross-list from stat.ML) [<a href="/pdf/2310.01326" title="Download PDF">pdf</a>, <a href="/format/2310.01326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Estimator for Linear Regression with Shuffled Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+H">Hang Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Li%2C+P">Ping Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper considers the task of linear regression with shuffled labels,
i.e., $\mathbf Y = \mathbf \Pi \mathbf X \mathbf B + \mathbf W$, where $\mathbf
Y \in \mathbb R^{n\times m}, \mathbf Pi \in \mathbb R^{n\times n}, \mathbf X\in
\mathbb R^{n\times p}, \mathbf B \in \mathbb R^{p\times m}$, and $\mathbf W\in
\mathbb R^{n\times m}$, respectively, represent the sensing results, (unknown
or missing) corresponding information, sensing matrix, signal of interest, and
additive sensing noise. Given the observation $\mathbf Y$ and sensing matrix
$\mathbf X$, we propose a one-step estimator to reconstruct $(\mathbf \Pi,
\mathbf B)$. From the computational perspective, our estimator's complexity is
$O(n^3 + np^2m)$, which is no greater than the maximum complexity of a linear
assignment algorithm (e.g., $O(n^3)$) and a least square algorithm (e.g.,
$O(np^2 m)$). From the statistical perspective, we divide the minimum $snr$
requirement into four regimes, e.g., unknown, hard, medium, and easy regimes;
and present sufficient conditions for the correct permutation recovery under
each regime: $(i)$ $snr \geq \Omega(1)$ in the easy regime; $(ii)$ $snr \geq
\Omega(\log n)$ in the medium regime; and $(iii)$ $snr \geq \Omega((\log
n)^{c_0}\cdot n^{{c_1}/{srank(\mathbf B)}})$ in the hard regime ($c_0, c_1$ are
some positive constants and $srank(\mathbf B)$ denotes the stable rank of
$\mathbf B$). In the end, we also provide numerical experiments to confirm the
above claims.
</p>
</div>
</dd>
<dt><a name="item777">[777]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01347" title="Abstract">arXiv:2310.01347</a> (cross-list from quant-ph) [<a href="/pdf/2310.01347" title="Download PDF">pdf</a>, <a href="/ps/2310.01347" title="Download PostScript">ps</a>, <a href="/format/2310.01347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hamiltonians whose low-energy states require $&#x3a9;(n)$ T gates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Coble%2C+N+J">Nolan J. Coble</a>, 
<a href="/search/quant-ph?searchtype=author&query=Coudron%2C+M">Matthew Coudron</a>, 
<a href="/search/quant-ph?searchtype=author&query=Nelson%2C+J">Jon Nelson</a>, 
<a href="/search/quant-ph?searchtype=author&query=Nezhadi%2C+S+S">Seyed Sajjad Nezhadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">The recent resolution of the NLTS Conjecture [ABN22] establishes a
prerequisite to the Quantum PCP (QPCP) Conjecture through a novel use of
newly-constructed QLDPC codes [LZ22]. Even with NLTS now solved, there remain
many independent and unresolved prerequisites to the QPCP Conjecture, such as
the NLSS Conjecture of [GL22]. In this work we focus on a specific and natural
prerequisite to both NLSS and the QPCP Conjecture, namely, the existence of
local Hamiltonians whose low-energy states all require $\omega(\log n)$ T gates
to prepare. In fact, we prove a stronger result which is not necessarily
implied by either conjecture: we construct local Hamiltonians whose low-energy
states require $\Omega(n)$ T gates. Following a previous work [CCNN23], we
further show that our procedure can be applied to the NLTS Hamiltonians of
[ABN22] to yield local Hamiltonians whose low-energy states require both
${\Omega}(\log n)$-depth and $\Omega(n)$ T gates to prepare. Our results
utilize a connection between T-count and stabilizer groups, which was recently
applied in the context of learning low T-count states [GIKL23a, GIKL23b,
GIKL23c].
</p>
</div>
</dd>
<dt><a name="item778">[778]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01348" title="Abstract">arXiv:2310.01348</a> (cross-list from math.OC) [<a href="/pdf/2310.01348" title="Download PDF">pdf</a>, <a href="/format/2310.01348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairly Extreme: Minimizing Outages Equitably
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sundar%2C+K">Kaarthik Sundar</a>, 
<a href="/search/math?searchtype=author&query=Deka%2C+D">Deepjyoti Deka</a>, 
<a href="/search/math?searchtype=author&query=Bent%2C+R">Russell Bent</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper focuses on the problem of minimizing the outages due to extreme
events on the power grid equitably among all customers of the grid. The paper
presents two ways of incorporating fairness into the existing formulations that
seek to minimize the total outage in the power grid. The first method is
motivated by existing literature on incorporating fairness in optimization
problems and this is done by modifying the problem's objective function. The
second method introduces a novel notion of fairness, termed
$\varepsilon$-fairness, that can be incorporated into existing problem
formulations through a single second-order cone constraint. Both these methods
are very general and can be used to incorporate fairness in existing planning
and operational optimization problems in the power grid and beyond. Extensive
computational case studies that examine the effectiveness of both these methods
to characterize fairness is presented followed by conclusions and ways forward.
</p>
</div>
</dd>
<dt><a name="item779">[779]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01350" title="Abstract">arXiv:2310.01350</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2310.01350" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A peridynamic-informed deep learning model for brittle damage prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Eghbalpoor%2C+R">Roozbeh Eghbalpoor</a>, 
<a href="/search/cond-mat?searchtype=author&query=Sheidaei%2C+A">Azadeh Sheidaei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this study, a novel approach that combines the principles of peridynamic
(PD) theory with PINN is presented to predict quasi-static damage and crack
propagation in brittle materials. To achieve high prediction accuracy and
convergence rate, the linearized PD governing equation is enforced in the
PINN's residual-based loss function. The proposed PD-INN is able to learn and
capture intricate displacement patterns associated with different geometrical
parameters, such as pre-crack position and length. Several enhancements like
cyclical annealing schedule and deformation gradient aware optimization
technique are proposed to ensure the model would not get stuck in its trivial
solution. The model's performance assessment is conducted by monitoring the
behavior of loss function throughout the training process. The PD-INN
predictions are also validated through several benchmark cases with the results
obtained from high-fidelity techniques such as PD direct numerical method and
Extended-Finite Element Method. Our results show the ability of the nonlocal
PD-INN to predict damage and crack propagation accurately and efficiently.
</p>
</div>
</dd>
<dt><a name="item780">[780]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01353" title="Abstract">arXiv:2310.01353</a> (cross-list from eess.AS) [<a href="/pdf/2310.01353" title="Download PDF">pdf</a>, <a href="/format/2310.01353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Up Music Information Retrieval Training with Semi-Supervised  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hung%2C+Y">Yun-Ning Hung</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Ju-Chiang Wang</a>, 
<a href="/search/eess?searchtype=author&query=Won%2C+M">Minz Won</a>, 
<a href="/search/eess?searchtype=author&query=Le%2C+D">Duc Le</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">In the era of data-driven Music Information Retrieval (MIR), the scarcity of
labeled data has been one of the major concerns to the success of an MIR task.
In this work, we leverage the semi-supervised teacher-student training approach
to improve MIR tasks. For training, we scale up the unlabeled music data to
240k hours, which is much larger than any public MIR datasets. We iteratively
create and refine the pseudo-labels in the noisy teacher-student training
process. Knowledge expansion is also explored to iteratively scale up the model
sizes from as small as less than 3M to almost 100M parameters. We study the
performance correlation between data size and model size in the experiments. By
scaling up both model size and training data, our models achieve
state-of-the-art results on several MIR tasks compared to models that are
either trained in a supervised manner or based on a self-supervised pretrained
model. To our knowledge, this is the first attempt to study the effects of
scaling up both model and training data for a variety of MIR tasks.
</p>
</div>
</dd>
<dt><a name="item781">[781]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01413" title="Abstract">arXiv:2310.01413</a> (cross-list from eess.IV) [<a href="/pdf/2310.01413" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A multi-institutional pediatric dataset of clinical radiology MRIs by  the Children&#x27;s Brain Tumor Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Familiar%2C+A+M">Ariana M. Familiar</a>, 
<a href="/search/eess?searchtype=author&query=Kazerooni%2C+A+F">Anahita Fathi Kazerooni</a>, 
<a href="/search/eess?searchtype=author&query=Anderson%2C+H">Hannah Anderson</a>, 
<a href="/search/eess?searchtype=author&query=Lubneuski%2C+A">Aliaksandr Lubneuski</a>, 
<a href="/search/eess?searchtype=author&query=Viswanathan%2C+K">Karthik Viswanathan</a>, 
<a href="/search/eess?searchtype=author&query=Breslow%2C+R">Rocky Breslow</a>, 
<a href="/search/eess?searchtype=author&query=Khalili%2C+N">Nastaran Khalili</a>, 
<a href="/search/eess?searchtype=author&query=Bagheri%2C+S">Sina Bagheri</a>, 
<a href="/search/eess?searchtype=author&query=Haldar%2C+D">Debanjan Haldar</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+M+C">Meen Chul Kim</a>, 
<a href="/search/eess?searchtype=author&query=Arif%2C+S">Sherjeel Arif</a>, 
<a href="/search/eess?searchtype=author&query=Madhogarhia%2C+R">Rachel Madhogarhia</a>, 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+T+Q">Thinh Q. Nguyen</a>, 
<a href="/search/eess?searchtype=author&query=Frenkel%2C+E+A">Elizabeth A. Frenkel</a>, 
<a href="/search/eess?searchtype=author&query=Helili%2C+Z">Zeinab Helili</a>, 
<a href="/search/eess?searchtype=author&query=Harrison%2C+J">Jessica Harrison</a>, 
<a href="/search/eess?searchtype=author&query=Farahani%2C+K">Keyvan Farahani</a>, 
<a href="/search/eess?searchtype=author&query=Linguraru%2C+M+G">Marius George Linguraru</a>, 
<a href="/search/eess?searchtype=author&query=Bagci%2C+U">Ulas Bagci</a>, 
<a href="/search/eess?searchtype=author&query=Velichko%2C+Y">Yury Velichko</a>, 
<a href="/search/eess?searchtype=author&query=Stevens%2C+J">Jeffrey Stevens</a>, 
<a href="/search/eess?searchtype=author&query=Leary%2C+S">Sarah Leary</a>, 
<a href="/search/eess?searchtype=author&query=Lober%2C+R+M">Robert M. Lober</a>, 
<a href="/search/eess?searchtype=author&query=Campion%2C+S">Stephani Campion</a>, 
<a href="/search/eess?searchtype=author&query=Smith%2C+A+A">Amy A. Smith</a>, 
<a href="/search/eess?searchtype=author&query=Morinigo%2C+D">Denise Morinigo</a>, 
<a href="/search/eess?searchtype=author&query=Rood%2C+B">Brian Rood</a>, 
<a href="/search/eess?searchtype=author&query=Diamond%2C+K">Kimberly Diamond</a>, 
<a href="/search/eess?searchtype=author&query=Pollack%2C+I+F">Ian F. Pollack</a>, 
<a href="/search/eess?searchtype=author&query=Williams%2C+M">Melissa Williams</a>, 
<a href="/search/eess?searchtype=author&query=Vossough%2C+A">Arastoo Vossough</a>, 
<a href="/search/eess?searchtype=author&query=Ware%2C+J+B">Jeffrey B. Ware</a>, 
<a href="/search/eess?searchtype=author&query=Mueller%2C+S">Sabine Mueller</a>, 
<a href="/search/eess?searchtype=author&query=Storm%2C+P+B">Phillip B. Storm</a>, 
<a href="/search/eess?searchtype=author&query=Heath%2C+A+P">Allison P. Heath</a>, 
<a href="/search/eess?searchtype=author&query=Waanders%2C+A+J">Angela J. Waanders</a>, 
<a href="/search/eess?searchtype=author&query=Lilly%2C+J+V">Jena V. Lilly</a>, 
<a href="/search/eess?searchtype=author&query=Mason%2C+J+L">Jennifer L. Mason</a>, 
<a href="/search/eess?searchtype=author&query=Resnick%2C+A+C">Adam C. Resnick</a>, 
<a href="/search/eess?searchtype=author&query=Nabavizadeh%2C+A">Ali Nabavizadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Pediatric brain and spinal cancers remain the leading cause of cancer-related
death in children. Advancements in clinical decision-support in pediatric
neuro-oncology utilizing the wealth of radiology imaging data collected through
standard care, however, has significantly lagged other domains. Such data is
ripe for use with predictive analytics such as artificial intelligence (AI)
methods, which require large datasets. To address this unmet need, we provide a
multi-institutional, large-scale pediatric dataset of 23,101 multi-parametric
MRI exams acquired through routine care for 1,526 brain tumor patients, as part
of the Children's Brain Tumor Network. This includes longitudinal MRIs across
various cancer diagnoses, with associated patient-level clinical information,
digital pathology slides, as well as tissue genotype and omics data. To
facilitate downstream analysis, treatment-na\"ive images for 370 subjects were
processed and released through the NCI Childhood Cancer Data Initiative via the
Cancer Data Service. Through ongoing efforts to continuously build these
imaging repositories, our aim is to accelerate discovery and translational AI
models with real-world data, to ultimately empower precision medicine for
children.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Tue,  3 Oct 23</h3>
<dl>
<dt><a name="item782">[782]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1803.00422" title="Abstract">arXiv:1803.00422</a> (replaced) [<a href="/pdf/1803.00422" title="Download PDF">pdf</a>, <a href="/format/1803.00422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Multivariate Regression Modeling For Selecting Biomarkers  Under Data Protection Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Z%C3%B6ller%2C+D">Daniela Z&#xf6;ller</a>, 
<a href="/search/stat?searchtype=author&query=Binder%2C+H">Harald Binder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item783">[783]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1805.10766" title="Abstract">arXiv:1805.10766</a> (replaced) [<a href="/pdf/1805.10766" title="Download PDF">pdf</a>, <a href="/format/1805.10766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving the Resolution of CNN Feature Maps Efficiently with  Multisampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sadigh%2C+S">Shayan Sadigh</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+P">Pradeep Sen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item784">[784]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1906.09948" title="Abstract">arXiv:1906.09948</a> (replaced) [<a href="/pdf/1906.09948" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The dual-process approach to human sociality: Meta-analytic evidence for  a theory of internalized heuristics for self-preservation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Capraro%2C+V">Valerio Capraro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Computer Science and Game Theory (cs.GT); Populations and Evolution (q-bio.PE)

</div>
</div>
</dd>
<dt><a name="item785">[785]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2001.02444" title="Abstract">arXiv:2001.02444</a> (replaced) [<a href="/pdf/2001.02444" title="Download PDF">pdf</a>, <a href="/format/2001.02444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Encode and Classify Test Executions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsimpourlas%2C+F">Foivos Tsimpourlas</a>, 
<a href="/search/cs?searchtype=author&query=Rajan%2C+A">Ajitha Rajan</a>, 
<a href="/search/cs?searchtype=author&query=Allamanis%2C+M">Miltiadis Allamanis</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of the 36th Annual ACM Symposium on Applied
  Computing 2021 Mar 22 (pp. 1521-1531)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item786">[786]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2002.09285" title="Abstract">arXiv:2002.09285</a> (replaced) [<a href="/pdf/2002.09285" title="Download PDF">pdf</a>, <a href="/format/2002.09285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Convolutional Neural Network into graph space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martineau%2C+C">Chlo&#xe9; Martineau</a>, 
<a href="/search/cs?searchtype=author&query=Raveaux%2C+R">Romain Raveaux</a>, 
<a href="/search/cs?searchtype=author&query=Conte%2C+D">Donatello Conte</a>, 
<a href="/search/cs?searchtype=author&query=Venturini%2C+G">Gilles Venturini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/1611.08402">arXiv:1611.08402</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item787">[787]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2004.08249" title="Abstract">arXiv:2004.08249</a> (replaced) [<a href="/pdf/2004.08249" title="Download PDF">pdf</a>, <a href="/format/2004.08249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Difficulty of Training Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaodong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item788">[788]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2004.08672" title="Abstract">arXiv:2004.08672</a> (replaced) [<a href="/pdf/2004.08672" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> iCORPP: Interleaved Commonsense Reasoning and Probabilistic Planning on  Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Khandelwal%2C+P">Piyush Khandelwal</a>, 
<a href="/search/cs?searchtype=author&query=Stone%2C+P">Peter Stone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item789">[789]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.10189" title="Abstract">arXiv:2006.10189</a> (replaced) [<a href="/pdf/2006.10189" title="Download PDF">pdf</a>, <a href="/format/2006.10189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting minimum description length complexity in overparameterized  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dwivedi%2C+R">Raaz Dwivedi</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+C">Chandan Singh</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wainwright%2C+M+J">Martin J. Wainwright</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item790">[790]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.15729" title="Abstract">arXiv:2010.15729</a> (replaced) [<a href="/pdf/2010.15729" title="Download PDF">pdf</a>, <a href="/format/2010.15729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fundamental limitations to key distillation from Gaussian states with  Gaussian operations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Lami%2C+L">Ludovico Lami</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mi%C5%A1ta%2C+L">Ladislav Mi&#x161;ta, Jr.</a>, 
<a href="/search/quant-ph?searchtype=author&query=Adesso%2C+G">Gerardo Adesso</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 4 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Phys. Rev. Research 5, 033153 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT); Mathematical Physics (math-ph)

</div>
</div>
</dd>
<dt><a name="item791">[791]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.04991" title="Abstract">arXiv:2011.04991</a> (replaced) [<a href="/pdf/2011.04991" title="Download PDF">pdf</a>, <a href="/ps/2011.04991" title="Download PostScript">ps</a>, <a href="/format/2011.04991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weak Galerkin Method for Electrical Impedance Tomography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liang%2C+Y">Ying Liang</a>, 
<a href="/search/math?searchtype=author&query=Zou%2C+J">Jun Zou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item792">[792]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.00038" title="Abstract">arXiv:2012.00038</a> (replaced) [<a href="/pdf/2012.00038" title="Download PDF">pdf</a>, <a href="/format/2012.00038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equitable [[2,10],[6,6]]-partitions of the 12-cube
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Krotov%2C+D+S">Denis S. Krotov</a> (Sobolev Institute of Mathematics, Novosibirsk, Russia)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2: Section 4 and related data have been added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item793">[793]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.00851" title="Abstract">arXiv:2102.00851</a> (replaced) [<a href="/pdf/2102.00851" title="Download PDF">pdf</a>, <a href="/format/2102.00851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rich Prosody Diversity Modelling with Phone-level Mixture Density  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chenpeng Du</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Kai Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Interspeech 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>

</div>
</div>
</dd>
<dt><a name="item794">[794]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.07401" title="Abstract">arXiv:2103.07401</a> (replaced) [<a href="/pdf/2103.07401" title="Download PDF">pdf</a>, <a href="/format/2103.07401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bears with Hats and Independence Polynomials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bla%C5%BEej%2C+V">V&#xe1;clav Bla&#x17e;ej</a>, 
<a href="/search/math?searchtype=author&query=Dvo%C5%99%C3%A1k%2C+P">Pavel Dvo&#x159;&#xe1;k</a>, 
<a href="/search/math?searchtype=author&query=Opler%2C+M">Michal Opler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item795">[795]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.03652" title="Abstract">arXiv:2104.03652</a> (replaced) [<a href="/pdf/2104.03652" title="Download PDF">pdf</a>, <a href="/format/2104.03652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interval constraint programming for globally solving catalog-based  categorical optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Vanaret%2C+C">Charlie Vanaret</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Combinatorics (math.CO); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item796">[796]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.06955" title="Abstract">arXiv:2105.06955</a> (replaced) [<a href="/pdf/2105.06955" title="Download PDF">pdf</a>, <a href="/format/2105.06955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the enumeration of plane bipolar posets and transversal structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fusy%2C+%C3%89">&#xc9;ric Fusy</a>, 
<a href="/search/math?searchtype=author&query=Narmanli%2C+E">Erkan Narmanli</a>, 
<a href="/search/math?searchtype=author&query=Schaeffer%2C+G">Gilles Schaeffer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item797">[797]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.14362" title="Abstract">arXiv:2105.14362</a> (replaced) [<a href="/pdf/2105.14362" title="Download PDF">pdf</a>, <a href="/format/2105.14362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dash Sylvereye: A WebGL-powered Library for Dashboard-driven  Visualization of Large Street Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garcia-Robledo%2C+A">Alberto Garcia-Robledo</a>, 
<a href="/search/cs?searchtype=author&query=Zangiabady%2C+M">Mahboobeh Zangiabady</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Re-submitted to IEEE Access on Aug. 11, 2023. The interpretation of the results in Section V has been corrected, as a more in-depth analysis unveiled that the prior results are attributed to the software (CPU) acceleration capabilities of Dash Sylvereye. Additionally, the manuscript now features a performance comparison with Kepler.gl and city-roads
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item798">[798]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02626" title="Abstract">arXiv:2106.02626</a> (replaced) [<a href="/pdf/2106.02626" title="Download PDF">pdf</a>, <a href="/format/2106.02626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamics of specialization in neural modules under resource constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=B%C3%A9na%2C+G">Gabriel B&#xe9;na</a>, 
<a href="/search/q-bio?searchtype=author&query=Goodman%2C+D+F+M">Dan F. M. Goodman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item799">[799]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.14869" title="Abstract">arXiv:2106.14869</a> (replaced) [<a href="/pdf/2106.14869" title="Download PDF">pdf</a>, <a href="/format/2106.14869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Isomorphism Testing Parameterized by Genus and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neuen%2C+D">Daniel Neuen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 5 figure, full version of a paper accepted at ESA 2021; second version improves the presentation of the results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item800">[800]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.06178" title="Abstract">arXiv:2107.06178</a> (replaced) [<a href="/pdf/2107.06178" title="Download PDF">pdf</a>, <a href="/format/2107.06178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Ecological Robustness-Oriented Approach for Power System Network  Expansion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+H">Hao Huang</a>, 
<a href="/search/eess?searchtype=author&query=Mao%2C+Z">Zeyu Mao</a>, 
<a href="/search/eess?searchtype=author&query=Panyam%2C+V">Varuneswara Panyam</a>, 
<a href="/search/eess?searchtype=author&query=Layton%2C+A">Astrid Layton</a>, 
<a href="/search/eess?searchtype=author&query=Davis%2C+K">Katherine Davis</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Power Systems 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item801">[801]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.00340" title="Abstract">arXiv:2108.00340</a> (replaced) [<a href="/pdf/2108.00340" title="Download PDF">pdf</a>, <a href="/format/2108.00340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstruction guided Meta-learning for Few Shot Open Set Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nag%2C+S">Sayak Nag</a>, 
<a href="/search/cs?searchtype=author&query=Raychaudhuri%2C+D+S">Dripta S. Raychaudhuri</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+S">Sujoy Paul</a>, 
<a href="/search/cs?searchtype=author&query=Roy-Chowdhury%2C+A+K">Amit K. Roy-Chowdhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE Transactions in Pattern Analysis and Machine Intelligence (TPAMI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item802">[802]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.08346" title="Abstract">arXiv:2109.08346</a> (replaced) [<a href="/pdf/2109.08346" title="Download PDF">pdf</a>, <a href="/format/2109.08346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comfetch: Federated Learning of Large Networks on Constrained Clients  via Sketching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rabbani%2C+T">Tahseen Rabbani</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+B">Brandon Feng</a>, 
<a href="/search/cs?searchtype=author&query=Bornstein%2C+M">Marco Bornstein</a>, 
<a href="/search/cs?searchtype=author&query=Sang%2C+K+R">Kyle Rui Sang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Rajkumar%2C+A">Arjun Rajkumar</a>, 
<a href="/search/cs?searchtype=author&query=Varshney%2C+A">Amitabh Varshney</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item803">[803]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.09658" title="Abstract">arXiv:2109.09658</a> (replaced) [<a href="/pdf/2109.09658" title="Download PDF">pdf</a>, <a href="/format/2109.09658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FUTURE-AI: Guiding Principles and Consensus Recommendations for  Trustworthy Artificial Intelligence in Medical Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lekadir%2C+K">Karim Lekadir</a>, 
<a href="/search/cs?searchtype=author&query=Osuala%2C+R">Richard Osuala</a>, 
<a href="/search/cs?searchtype=author&query=Gallin%2C+C">Catherine Gallin</a>, 
<a href="/search/cs?searchtype=author&query=Lazrak%2C+N">Noussair Lazrak</a>, 
<a href="/search/cs?searchtype=author&query=Kushibar%2C+K">Kaisar Kushibar</a>, 
<a href="/search/cs?searchtype=author&query=Tsakou%2C+G">Gianna Tsakou</a>, 
<a href="/search/cs?searchtype=author&query=Auss%C3%B3%2C+S">Susanna Auss&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Alberich%2C+L+C">Leonor Cerd&#xe1; Alberich</a>, 
<a href="/search/cs?searchtype=author&query=Marias%2C+K">Kostas Marias</a>, 
<a href="/search/cs?searchtype=author&query=Tsiknakis%2C+M">Manolis Tsiknakis</a>, 
<a href="/search/cs?searchtype=author&query=Colantonio%2C+S">Sara Colantonio</a>, 
<a href="/search/cs?searchtype=author&query=Papanikolaou%2C+N">Nickolas Papanikolaou</a>, 
<a href="/search/cs?searchtype=author&query=Salahuddin%2C+Z">Zohaib Salahuddin</a>, 
<a href="/search/cs?searchtype=author&query=Woodruff%2C+H+C">Henry C Woodruff</a>, 
<a href="/search/cs?searchtype=author&query=Lambin%2C+P">Philippe Lambin</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%AD-Bonmat%C3%AD%2C+L">Luis Mart&#xed;-Bonmat&#xed;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Please refer to <a href="/abs/2309.12325">arXiv:2309.12325</a> for the latest FUTURE-AI framework for healthcare
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item804">[804]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.12701" title="Abstract">arXiv:2109.12701</a> (replaced) [<a href="/pdf/2109.12701" title="Download PDF">pdf</a>, <a href="/format/2109.12701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Plus Low Rank Matrix Decomposition: A Discrete Optimization  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bertsimas%2C+D">Dimitris Bertsimas</a>, 
<a href="/search/stat?searchtype=author&query=Cory-Wright%2C+R">Ryan Cory-Wright</a>, 
<a href="/search/stat?searchtype=author&query=Johnson%2C+N+A+G">Nicholas A. G. Johnson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item805">[805]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.13337" title="Abstract">arXiv:2109.13337</a> (replaced) [<a href="/pdf/2109.13337" title="Download PDF">pdf</a>, <a href="/format/2109.13337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DEBOSH: Deep Bayesian Shape Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Durasov%2C+N">Nikita Durasov</a>, 
<a href="/search/cs?searchtype=author&query=Lukoyanov%2C+A">Artem Lukoyanov</a>, 
<a href="/search/cs?searchtype=author&query=Donier%2C+J">Jonathan Donier</a>, 
<a href="/search/cs?searchtype=author&query=Fua%2C+P">Pascal Fua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item806">[806]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.05351" title="Abstract">arXiv:2110.05351</a> (replaced) [<a href="/pdf/2110.05351" title="Download PDF">pdf</a>, <a href="/format/2110.05351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse recovery of elliptic solvers from matrix-vector products
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sch%C3%A4fer%2C+F">Florian Sch&#xe4;fer</a>, 
<a href="/search/math?searchtype=author&query=Owhadi%2C+H">Houman Owhadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in SISC. This version updates the link of the code repository and corrects some minor typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item807">[807]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.09680" title="Abstract">arXiv:2110.09680</a> (replaced) [<a href="/pdf/2110.09680" title="Download PDF">pdf</a>, <a href="/format/2110.09680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilevel Stochastic Optimization for Imputation in Massive Medical  Data Records
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+W">Wenrui Li</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+X">Xiaoyu Wang</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+Y">Yuetian Sun</a>, 
<a href="/search/stat?searchtype=author&query=Milanovic%2C+S">Snezana Milanovic</a>, 
<a href="/search/stat?searchtype=author&query=Kon%2C+M">Mark Kon</a>, 
<a href="/search/stat?searchtype=author&query=Castrillon-Candas%2C+J+E">Julio Enrique Castrillon-Candas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item808">[808]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.03898" title="Abstract">arXiv:2111.03898</a> (replaced) [<a href="/pdf/2111.03898" title="Download PDF">pdf</a>, <a href="/format/2111.03898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rapid mixing for the hardcore Glauber dynamics and other Markov chains  in bounded-treewidth graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eppstein%2C+D">David Eppstein</a>, 
<a href="/search/cs?searchtype=author&query=Frishberg%2C+D">Daniel Frishberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item809">[809]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.02240" title="Abstract">arXiv:2112.02240</a> (replaced) [<a href="/pdf/2112.02240" title="Download PDF">pdf</a>, <a href="/format/2112.02240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tracking Patches for Open Source Software Vulnerabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Congying Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bihuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chenhao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaifeng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xin Peng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item810">[810]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.12590" title="Abstract">arXiv:2112.12590</a> (replaced) [<a href="/pdf/2112.12590" title="Download PDF">pdf</a>, <a href="/format/2112.12590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploration of Overlap Volumes for Radiotherapy Plan Evaluation with the  Aim of Healthy Tissue Sparing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schlachter%2C+M">Matthias Schlachter</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+S">Samuel Peters</a>, 
<a href="/search/cs?searchtype=author&query=Camenisch%2C+D">Daniel Camenisch</a>, 
<a href="/search/cs?searchtype=author&query=Putora%2C+P+M">Paul Martin Putora</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%BChler%2C+K">Katja B&#xfc;hler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item811">[811]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.08582" title="Abstract">arXiv:2201.08582</a> (replaced) [<a href="/pdf/2201.08582" title="Download PDF">pdf</a>, <a href="/format/2201.08582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SegTransVAE: Hybrid CNN -- Transformer with Regularization for medical  image segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pham%2C+Q">Quan-Dung Pham</a> (1), 
<a href="/search/eess?searchtype=author&query=Nguyen-Truong%2C+H">Hai Nguyen-Truong</a> (1, 2 and 3), 
<a href="/search/eess?searchtype=author&query=Phuong%2C+N+N">Nam Nguyen Phuong</a> (1), 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+K+N+A">Khoa N. A. Nguyen</a> (1, 2 and 3) ((1) VinBrain JSC., Vietnam, (2) University of Science, Ho Chi Minh City, Vietnam, (3) Vietnam National University, Ho Chi Minh City, Vietnam)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2022 IEEE 19th International Symposium on Biomedical Imaging
  (ISBI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item812">[812]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.01125" title="Abstract">arXiv:2202.01125</a> (replaced) [<a href="/pdf/2202.01125" title="Download PDF">pdf</a>, <a href="/format/2202.01125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GLISp-r: A preference-based optimization algorithm with convergence  guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Previtali%2C+D">Davide Previtali</a>, 
<a href="/search/math?searchtype=author&query=Mazzoleni%2C+M">Mirko Mazzoleni</a>, 
<a href="/search/math?searchtype=author&query=Ferramosca%2C+A">Antonio Ferramosca</a>, 
<a href="/search/math?searchtype=author&query=Previdi%2C+F">Fabio Previdi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Journal version available at: <a href="https://doi.org/10.1007/s10589-023-00491-2">this https URL</a> 28 pages, 7 figures and 1 table
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Comput Optim Appl 86, 383-420 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item813">[813]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.05135" title="Abstract">arXiv:2202.05135</a> (replaced) [<a href="/pdf/2202.05135" title="Download PDF">pdf</a>, <a href="/format/2202.05135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Group-Agent Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kaiyue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xiao-Jun Zeng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item814">[814]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.12924" title="Abstract">arXiv:2202.12924</a> (replaced) [<a href="/pdf/2202.12924" title="Download PDF">pdf</a>, <a href="/format/2202.12924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAFQA: A classical simulation bootstrap for variational quantum  algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Ravi%2C+G+S">Gokul Subramanian Ravi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Gokhale%2C+P">Pranav Gokhale</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ding%2C+Y">Yi Ding</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kirby%2C+W+M">William M. Kirby</a>, 
<a href="/search/quant-ph?searchtype=author&query=Smith%2C+K+N">Kaitlin N. Smith</a>, 
<a href="/search/quant-ph?searchtype=author&query=Baker%2C+J+M">Jonathan M. Baker</a>, 
<a href="/search/quant-ph?searchtype=author&query=Love%2C+P+J">Peter J. Love</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hoffmann%2C+H">Henry Hoffmann</a>, 
<a href="/search/quant-ph?searchtype=author&query=Brown%2C+K+R">Kenneth R. Brown</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chong%2C+F+T">Frederic T. Chong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appears at the 28th Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2023). Previous title - CAFQA: Clifford Ansatz For Quantum Accuracy. Paper revised to ASPLOS requirements, added additional improvements to the CAFQA framework / evaluation. Added preliminary exploration on CAFQA with T gates
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item815">[815]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.01302" title="Abstract">arXiv:2203.01302</a> (replaced) [<a href="/pdf/2203.01302" title="Download PDF">pdf</a>, <a href="/format/2203.01302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolving Curricula with Regret-Based Environment Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parker-Holder%2C+J">Jack Parker-Holder</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Minqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Dennis%2C+M">Michael Dennis</a>, 
<a href="/search/cs?searchtype=author&query=Samvelyan%2C+M">Mikayel Samvelyan</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+J">Jakob Foerster</a>, 
<a href="/search/cs?searchtype=author&query=Grefenstette%2C+E">Edward Grefenstette</a>, 
<a href="/search/cs?searchtype=author&query=Rockt%C3%A4schel%2C+T">Tim Rockt&#xe4;schel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item816">[816]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.02693" title="Abstract">arXiv:2203.02693</a> (replaced) [<a href="/pdf/2203.02693" title="Download PDF">pdf</a>, <a href="/ps/2203.02693" title="Download PostScript">ps</a>, <a href="/format/2203.02693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximation Guarantees for the Non-Dominated Sorting Genetic Algorithm  II (NSGA-II)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Weijie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Doerr%2C+B">Benjamin Doerr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of our GECCO 2022 paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item817">[817]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.12114" title="Abstract">arXiv:2203.12114</a> (replaced) [<a href="/pdf/2203.12114" title="Download PDF">pdf</a>, <a href="/format/2203.12114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Optical Control Environment for Benchmarking Reinforcement Learning  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abuduweili%2C+A">Abulikemu Abuduweili</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changliu Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Transactions on Machine Learning Research (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item818">[818]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.03569" title="Abstract">arXiv:2204.03569</a> (replaced) [<a href="/pdf/2204.03569" title="Download PDF">pdf</a>, <a href="/format/2204.03569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Output-sensitive ERM-based techniques for data-driven algorithm design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balcan%2C+M">Maria-Florina Balcan</a>, 
<a href="/search/cs?searchtype=author&query=Seiler%2C+C">Christopher Seiler</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+D">Dravyansh Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item819">[819]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.06350" title="Abstract">arXiv:2204.06350</a> (replaced) [<a href="/pdf/2204.06350" title="Download PDF">pdf</a>, <a href="/format/2204.06350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LDPC codes: comparing cluster graphs to factor graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toit%2C+J+d">J du Toit</a>, 
<a href="/search/cs?searchtype=author&query=Preez%2C+J+d">J du Preez</a>, 
<a href="/search/cs?searchtype=author&query=Wolhuter%2C+R">R Wolhuter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item820">[820]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.07037" title="Abstract">arXiv:2204.07037</a> (replaced) [<a href="/pdf/2204.07037" title="Download PDF">pdf</a>, <a href="/format/2204.07037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LDPC codes: tracking non-stationary channel noise using sequential  variational Bayesian estimates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Toit%2C+J+d">J du Toit</a>, 
<a href="/search/eess?searchtype=author&query=Preez%2C+J+d">J du Preez</a>, 
<a href="/search/eess?searchtype=author&query=Wolhuter%2C+R">R Wolhuter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures. arXiv admin note: text overlap with <a href="/abs/2204.06350">arXiv:2204.06350</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item821">[821]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.07928" title="Abstract">arXiv:2204.07928</a> (replaced) [<a href="/pdf/2204.07928" title="Download PDF">pdf</a>, <a href="/ps/2204.07928" title="Download PostScript">ps</a>, <a href="/format/2204.07928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimally Reconfiguring List and Correspondence Colourings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cambie%2C+S">Stijn Cambie</a>, 
<a href="/search/math?searchtype=author&query=van+Batenburg%2C+W+C">Wouter Cames van Batenburg</a>, 
<a href="/search/math?searchtype=author&query=Cranston%2C+D+W">Daniel W. Cranston</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 3 figures; to appear in European J. Combinatorics
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> European J. Combinatorics, Volume 115, January 2024, 103798
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item822">[822]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.09202" title="Abstract">arXiv:2204.09202</a> (replaced) [<a href="/pdf/2204.09202" title="Download PDF">pdf</a>, <a href="/format/2204.09202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Best Cost-Sharing Rule Design for Selfish Bin Packing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Changjun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guochuan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 19th Conference on Web and InterNet Economics (WINE 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item823">[823]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.11171" title="Abstract">arXiv:2205.11171</a> (replaced) [<a href="/pdf/2205.11171" title="Download PDF">pdf</a>, <a href="/format/2205.11171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Energy Resources Cybersecurity Outlook: Vulnerabilities,  Attacks, Impacts, and Mitigations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zografopoulos%2C+I">Ioannis Zografopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Hatziargyriou%2C+N+D">Nikos D. Hatziargyriou</a>, 
<a href="/search/cs?searchtype=author&query=Konstantinou%2C+C">Charalambos Konstantinou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Systems Journal (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item824">[824]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.11521" title="Abstract">arXiv:2205.11521</a> (replaced) [<a href="/pdf/2205.11521" title="Download PDF">pdf</a>, <a href="/format/2205.11521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Hours to Seconds: Towards 100x Faster Quantitative Phase Imaging  via Differentiable Microscopy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Haputhanthri%2C+U">Udith Haputhanthri</a>, 
<a href="/search/eess?searchtype=author&query=Herath%2C+K">Kithmini Herath</a>, 
<a href="/search/eess?searchtype=author&query=Hettiarachchi%2C+R">Ramith Hettiarachchi</a>, 
<a href="/search/eess?searchtype=author&query=Kariyawasam%2C+H">Hasindu Kariyawasam</a>, 
<a href="/search/eess?searchtype=author&query=Ahmad%2C+A">Azeem Ahmad</a>, 
<a href="/search/eess?searchtype=author&query=Ahluwalia%2C+B+S">Balpreet S. Ahluwalia</a>, 
<a href="/search/eess?searchtype=author&query=Edussooriya%2C+C+U+S">Chamira U. S. Edussooriya</a>, 
<a href="/search/eess?searchtype=author&query=Wadduwage%2C+D+N">Dushan N. Wadduwage</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Computational Physics (physics.comp-ph); Optics (physics.optics)

</div>
</div>
</dd>
<dt><a name="item825">[825]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.12841" title="Abstract">arXiv:2205.12841</a> (replaced) [<a href="/pdf/2205.12841" title="Download PDF">pdf</a>, <a href="/format/2205.12841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Marginal Post Processing of Bayesian Inference Products with Normalizing  Flows and Kernel Density Estimators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Bevins%2C+H+T+J">Harry T. J. Bevins</a>, 
<a href="/search/astro-ph?searchtype=author&query=Handley%2C+W+J">William J. Handley</a>, 
<a href="/search/astro-ph?searchtype=author&query=Lemos%2C+P">Pablo Lemos</a>, 
<a href="/search/astro-ph?searchtype=author&query=Sims%2C+P+H">Peter H. Sims</a>, 
<a href="/search/astro-ph?searchtype=author&query=de+Lera+Acedo%2C+E">Eloy de Lera Acedo</a>, 
<a href="/search/astro-ph?searchtype=author&query=Fialkov%2C+A">Anastasia Fialkov</a>, 
<a href="/search/astro-ph?searchtype=author&query=Alsing%2C+J">Justin Alsing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for MNRAS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Cosmology and Nongalactic Astrophysics (astro-ph.CO); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item826">[826]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.02340" title="Abstract">arXiv:2206.02340</a> (replaced) [<a href="/pdf/2206.02340" title="Download PDF">pdf</a>, <a href="/format/2206.02340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimising the Expected Posterior Entropy Yields Optimal Summary  Statistics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hoffmann%2C+T">Till Hoffmann</a>, 
<a href="/search/stat?searchtype=author&query=Onnela%2C+J">Jukka-Pekka Onnela</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 6 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item827">[827]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.03601" title="Abstract">arXiv:2206.03601</a> (replaced) [<a href="/pdf/2206.03601" title="Download PDF">pdf</a>, <a href="/format/2206.03601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoupled Self-supervised Learning for Non-Homophilous Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Teng Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhimeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zeyang Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item828">[828]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.04661" title="Abstract">arXiv:2206.04661</a> (replaced) [<a href="/pdf/2206.04661" title="Download PDF">pdf</a>, <a href="/format/2206.04661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distillation Decision Tree
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lu%2C+X">Xuetao Lu</a>, 
<a href="/search/stat?searchtype=author&query=Lee%2C+J+J">J. Jack Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item829">[829]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.07562" title="Abstract">arXiv:2206.07562</a> (replaced) [<a href="/pdf/2206.07562" title="Download PDF">pdf</a>, <a href="/format/2206.07562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning with Uncertainty via Distilled Predictive  Distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhatt%2C+S">Shrey Bhatt</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Aishwarya Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Rai%2C+P">Piyush Rai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ACML 2023; 21 pages(14 pages of main content, 2 pages of references, and 5 pages of supplementary content)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item830">[830]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.09380" title="Abstract">arXiv:2206.09380</a> (replaced) [<a href="/pdf/2206.09380" title="Download PDF">pdf</a>, <a href="/format/2206.09380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supervision Adaptation Balancing In-distribution Generalization and  Out-of-distribution Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhilin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Longbing Cao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+K">Kun-Yu Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item831">[831]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.11102" title="Abstract">arXiv:2206.11102</a> (replaced) [<a href="/pdf/2206.11102" title="Download PDF">pdf</a>, <a href="/format/2206.11102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 40 Years of Designing Code Comprehension Experiments: A Systematic  Mapping Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wyrich%2C+M">Marvin Wyrich</a>, 
<a href="/search/cs?searchtype=author&query=Bogner%2C+J">Justus Bogner</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+S">Stefan Wagner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at ACM Computing Surveys
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item832">[832]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.14867" title="Abstract">arXiv:2206.14867</a> (replaced) [<a href="/pdf/2206.14867" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-stressed Bi-stable Hair Clip Mechanism for Faster Swimming Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zechen Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+W">Wenxiong Hao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+P">Pengfei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item833">[833]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.01033" title="Abstract">arXiv:2207.01033</a> (replaced) [<a href="/pdf/2207.01033" title="Download PDF">pdf</a>, <a href="/format/2207.01033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Force Controller for Contact-Rich Robotic Systems using an  Unscented Kalman Filter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schperberg%2C+A">Alexander Schperberg</a>, 
<a href="/search/cs?searchtype=author&query=Shirai%2C+Y">Yuki Shirai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xuan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+Y">Yusuke Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+D">Dennis Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE RAS International Conference on Humanoid Robots 2023, December 12-14 in Austin, Texas, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item834">[834]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.08348" title="Abstract">arXiv:2207.08348</a> (replaced) [<a href="/e-print/2207.08348" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Swimming Robots Based on Elastic Instability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zechen Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+W">Wenxiong Hao</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yufeng Su</a>, 
<a href="/search/cs?searchtype=author&query=Lipson%2C+H">Hod Lipson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Wrong title and contents
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item835">[835]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.11530" title="Abstract">arXiv:2207.11530</a> (replaced) [<a href="/pdf/2207.11530" title="Download PDF">pdf</a>, <a href="/format/2207.11530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kellect: a Kernel-Based Efficient and Lossless Event Log Collector for  Windows Security
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tieming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Q">Qijie Song</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xuebo Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tiantian Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhiling Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+M">Mingqi Lv</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item836">[836]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.13426" title="Abstract">arXiv:2207.13426</a> (replaced) [<a href="/pdf/2207.13426" title="Download PDF">pdf</a>, <a href="/format/2207.13426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards quantitative super-resolution microscopy: Molecular maps with  statistical guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Proksch%2C+K">Katharina Proksch</a>, 
<a href="/search/stat?searchtype=author&query=Werner%2C+F">Frank Werner</a>, 
<a href="/search/stat?searchtype=author&query=Keller-Findeisen%2C+J">Jan Keller-Findeisen</a>, 
<a href="/search/stat?searchtype=author&query=Ta%2C+H">Haisen Ta</a>, 
<a href="/search/stat?searchtype=author&query=Munk%2C+A">Axel Munk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item837">[837]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.07978" title="Abstract">arXiv:2208.07978</a> (replaced) [<a href="/pdf/2208.07978" title="Download PDF">pdf</a>, <a href="/format/2208.07978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Heterogeneous Federated Learning with Knowledge Extraction and  Multi-Model Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D+P">Duy Phuong Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Sixing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Mu%C3%B1oz%2C+J+P">J. Pablo Mu&#xf1;oz</a>, 
<a href="/search/cs?searchtype=author&query=Jannesari%2C+A">Ali Jannesari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accept at the 4th workshop on Artificial Intelligence and Machine Learning for Scientific Applications (AI4S), SC 23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item838">[838]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.02935" title="Abstract">arXiv:2209.02935</a> (replaced) [<a href="/pdf/2209.02935" title="Download PDF">pdf</a>, <a href="/format/2209.02935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Normalised clustering accuracy: An asymmetric external cluster validity  measure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gagolewski%2C+M">Marek Gagolewski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item839">[839]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.02965" title="Abstract">arXiv:2209.02965</a> (replaced) [<a href="/pdf/2209.02965" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Risk of Bias in Chest Radiography Deep Learning Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Glocker%2C+B">Ben Glocker</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+C">Charles Jones</a>, 
<a href="/search/cs?searchtype=author&query=Roschewitz%2C+M">Melanie Roschewitz</a>, 
<a href="/search/cs?searchtype=author&query=Winzeck%2C+S">Stefan Winzeck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Radiology: Artificial Intelligence (2023). Code available under <a href="https://github.com/biomedia-mira/cxr-foundation-bias/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item840">[840]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.04851" title="Abstract">arXiv:2209.04851</a> (replaced) [<a href="/pdf/2209.04851" title="Download PDF">pdf</a>, <a href="/format/2209.04851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenMixup: A Comprehensive Mixup Benchmark for Visual Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zedong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Weiyang Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint V2. The source code is available at <a href="https://github.com/Westlake-AI/openmixup">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item841">[841]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.05013" title="Abstract">arXiv:2209.05013</a> (replaced) [<a href="/pdf/2209.05013" title="Download PDF">pdf</a>, <a href="/format/2209.05013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning A Locally Unified 3D Point Cloud for View Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+M">Meng You</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Mantang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+X">Xianqiang Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Junhui Hou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to TIP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item842">[842]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.10241" title="Abstract">arXiv:2209.10241</a> (replaced) [<a href="/pdf/2209.10241" title="Download PDF">pdf</a>, <a href="/format/2209.10241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact and sampling methods for mining higher-order motifs in large  hypergraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lotito%2C+Q+F">Quintino Francesco Lotito</a>, 
<a href="/search/cs?searchtype=author&query=Musciotto%2C+F">Federico Musciotto</a>, 
<a href="/search/cs?searchtype=author&query=Battiston%2C+F">Federico Battiston</a>, 
<a href="/search/cs?searchtype=author&query=Montresor%2C+A">Alberto Montresor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In press at Springer Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Data Structures and Algorithms (cs.DS); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item843">[843]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.12280" title="Abstract">arXiv:2209.12280</a> (replaced) [<a href="/pdf/2209.12280" title="Download PDF">pdf</a>, <a href="/format/2209.12280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating the dynamic noise landscape of variational quantum algorithms  with QISMET
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Ravi%2C+G+S">Gokul Subramanian Ravi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Smith%2C+K+N">Kaitlin N. Smith</a>, 
<a href="/search/quant-ph?searchtype=author&query=Baker%2C+J+M">Jonathan M. Baker</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kannan%2C+T">Tejas Kannan</a>, 
<a href="/search/quant-ph?searchtype=author&query=Earnest%2C+N">Nathan Earnest</a>, 
<a href="/search/quant-ph?searchtype=author&query=Javadi-Abhari%2C+A">Ali Javadi-Abhari</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hoffmann%2C+H">Henry Hoffmann</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chong%2C+F+T">Frederic T. Chong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appears at the 28th Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Hardware Architecture (cs.AR); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item844">[844]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.12332" title="Abstract">arXiv:2209.12332</a> (replaced) [<a href="/pdf/2209.12332" title="Download PDF">pdf</a>, <a href="/format/2209.12332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Optimal Linear Contraction Order of Tree Tensor Networks, and  Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Stoian%2C+M">Mihail Stoian</a>, 
<a href="/search/quant-ph?searchtype=author&query=Milbradt%2C+R">Richard Milbradt</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mendl%2C+C+B">Christian B. Mendl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submission-ready version: improved presentation, clean notation, suitable title, algorithm pseudo-codes, and experiments; results remain the same
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Databases (cs.DB); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item845">[845]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.13091" title="Abstract">arXiv:2209.13091</a> (replaced) [<a href="/pdf/2209.13091" title="Download PDF">pdf</a>, <a href="/format/2209.13091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WaterNeRF: Neural Radiance Fields for Underwater Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sethuraman%2C+A+V">Advaith Venkatramanan Sethuraman</a>, 
<a href="/search/cs?searchtype=author&query=Ramanagopal%2C+M+S">Manikandasriram Srinivasan Ramanagopal</a>, 
<a href="/search/cs?searchtype=author&query=Skinner%2C+K+A">Katherine A. Skinner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item846">[846]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.13482" title="Abstract">arXiv:2209.13482</a> (replaced) [<a href="/pdf/2209.13482" title="Download PDF">pdf</a>, <a href="/format/2209.13482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Swarm Equatorial Plasma Bubbles via Machine Learning and  Shapley Values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Reddy%2C+S+A">S. A. Reddy</a>, 
<a href="/search/physics?searchtype=author&query=Forsyth%2C+C">C. Forsyth</a>, 
<a href="/search/physics?searchtype=author&query=Aruliah%2C+A">A. Aruliah</a>, 
<a href="/search/physics?searchtype=author&query=Smith%2C+A">A. Smith</a>, 
<a href="/search/physics?searchtype=author&query=Bortnik%2C+J">J. Bortnik</a>, 
<a href="/search/physics?searchtype=author&query=Aa%2C+E">E. Aa</a>, 
<a href="/search/physics?searchtype=author&query=Kataria%2C+D+O">D. O. Kataria</a>, 
<a href="/search/physics?searchtype=author&query=Lewis%2C+G">G. Lewis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 Pages, 9 Figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Geophysical Research: Space Physics (2023):
  e2022JA031183
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Space Physics (physics.space-ph)</span>; Earth and Planetary Astrophysics (astro-ph.EP); Machine Learning (cs.LG); Atmospheric and Oceanic Physics (physics.ao-ph); Plasma Physics (physics.plasm-ph)

</div>
</div>
</dd>
<dt><a name="item847">[847]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.14937" title="Abstract">arXiv:2209.14937</a> (replaced) [<a href="/pdf/2209.14937" title="Download PDF">pdf</a>, <a href="/format/2209.14937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NAG-GS: Semi-Implicit, Accelerated and Robust Stochastic Optimizer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Leplat%2C+V">Valentin Leplat</a>, 
<a href="/search/math?searchtype=author&query=Merkulov%2C+D">Daniil Merkulov</a>, 
<a href="/search/math?searchtype=author&query=Katrutsa%2C+A">Aleksandr Katrutsa</a>, 
<a href="/search/math?searchtype=author&query=Bershatsky%2C+D">Daniel Bershatsky</a>, 
<a href="/search/math?searchtype=author&query=Tsymboi%2C+O">Olga Tsymboi</a>, 
<a href="/search/math?searchtype=author&query=Oseledets%2C+I">Ivan Oseledets</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We study Nesterov acceleration for the Stochastic Differential Equation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item848">[848]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.15140" title="Abstract">arXiv:2209.15140</a> (replaced) [<a href="/pdf/2209.15140" title="Download PDF">pdf</a>, <a href="/format/2209.15140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Proprioceptive Slip-Velocity-Aware State Estimation for Mobile  Robots via Invariant Kalman Filtering and Disturbance Observer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xihang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+S">Sangli Teng</a>, 
<a href="/search/cs?searchtype=author&query=Chakhachiro%2C+T">Theodor Chakhachiro</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+W">Wenzhe Tong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tingjun Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tzu-Yuan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Koehler%2C+S">Sarah Koehler</a>, 
<a href="/search/cs?searchtype=author&query=Ahumada%2C+M">Manuel Ahumada</a>, 
<a href="/search/cs?searchtype=author&query=Walls%2C+J+M">Jeffrey M. Walls</a>, 
<a href="/search/cs?searchtype=author&query=Ghaffari%2C+M">Maani Ghaffari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The work will be presented in IROS2023. github repository at <a href="https://github.com/UMich-CURLY/slip_detection_DOB.">this https URL</a> arXiv admin note: text overlap with <a href="/abs/1805.10410">arXiv:1805.10410</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item849">[849]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.15179" title="Abstract">arXiv:2209.15179</a> (replaced) [<a href="/pdf/2209.15179" title="Download PDF">pdf</a>, <a href="/format/2209.15179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physical Adversarial Attack meets Computer Vision: A Decade Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hui Wei</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xuemei Jia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhixiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hanxun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhubo Li</a>, 
<a href="/search/cs?searchtype=author&query=Satoh%2C+S">Shin&#x27;ichi Satoh</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zheng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages. Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item850">[850]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01672" title="Abstract">arXiv:2210.01672</a> (replaced) [<a href="/pdf/2210.01672" title="Download PDF">pdf</a>, <a href="/format/2210.01672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bringing robotics taxonomies to continuous domains via GPLVM on  hyperbolic manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaquier%2C+N">No&#xe9;mie Jaquier</a>, 
<a href="/search/cs?searchtype=author&query=Rozo%2C+L">Leonel Rozo</a>, 
<a href="/search/cs?searchtype=author&query=Gonz%C3%A1lez-Duque%2C+M">Miguel Gonz&#xe1;lez-Duque</a>, 
<a href="/search/cs?searchtype=author&query=Borovitskiy%2C+V">Viacheslav Borovitskiy</a>, 
<a href="/search/cs?searchtype=author&query=Asfour%2C+T">Tamim Asfour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item851">[851]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.02685" title="Abstract">arXiv:2210.02685</a> (replaced) [<a href="/pdf/2210.02685" title="Download PDF">pdf</a>, <a href="/format/2210.02685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Real2Sim2Real Method for Robust Object Grasping with Neural Surface  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Luobin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+R">Runlin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Vuong%2C+Q">Quan Vuong</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yuzhe Qin</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hao Su</a>, 
<a href="/search/cs?searchtype=author&query=Christensen%2C+H">Henrik Christensen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Video presentation available at <a href="https://youtu.be/TkvAKLsxkSc">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item852">[852]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.07513" title="Abstract">arXiv:2210.07513</a> (replaced) [<a href="/pdf/2210.07513" title="Download PDF">pdf</a>, <a href="/ps/2210.07513" title="Download PostScript">ps</a>, <a href="/format/2210.07513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous-in-time Limit for Bayesian Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhu%2C+Y">Yuhua Zhu</a>, 
<a href="/search/math?searchtype=author&query=Izzo%2C+Z">Zachary Izzo</a>, 
<a href="/search/math?searchtype=author&query=Ying%2C+L">Lexing Ying</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item853">[853]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.09997" title="Abstract">arXiv:2210.09997</a> (replaced) [<a href="/pdf/2210.09997" title="Download PDF">pdf</a>, <a href="/format/2210.09997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bag All You Need: Learning a Generalizable Bagging Strategy for  Heterogeneous Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bahety%2C+A">Arpit Bahety</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Shreeya Jain</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+H">Huy Ha</a>, 
<a href="/search/cs?searchtype=author&query=Hager%2C+N">Nathalie Hager</a>, 
<a href="/search/cs?searchtype=author&query=Burchfiel%2C+B">Benjamin Burchfiel</a>, 
<a href="/search/cs?searchtype=author&query=Cousineau%2C+E">Eric Cousineau</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Siyuan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shuran Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, project website: <a href="https://bag-all-you-need.cs.columbia.edu/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item854">[854]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.16656" title="Abstract">arXiv:2210.16656</a> (replaced) [<a href="/pdf/2210.16656" title="Download PDF">pdf</a>, <a href="/format/2210.16656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auxo: Efficient Federated Learning via Scalable Client Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiachen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+F">Fan Lai</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yinwei Dai</a>, 
<a href="/search/cs?searchtype=author&query=Akella%2C+A">Aditya Akella</a>, 
<a href="/search/cs?searchtype=author&query=Madhyastha%2C+H">Harsha Madhyastha</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+M">Mosharaf Chowdhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item855">[855]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.17237" title="Abstract">arXiv:2210.17237</a> (replaced) [<a href="/pdf/2210.17237" title="Download PDF">pdf</a>, <a href="/format/2210.17237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Multimodal Functional Graphical Model Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Tsai%2C+K">Katherine Tsai</a>, 
<a href="/search/stat?searchtype=author&query=Zhao%2C+B">Boxin Zhao</a>, 
<a href="/search/stat?searchtype=author&query=Koyejo%2C+S">Sanmi Koyejo</a>, 
<a href="/search/stat?searchtype=author&query=Kolar%2C+M">Mladen Kolar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item856">[856]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.06774" title="Abstract">arXiv:2211.06774</a> (replaced) [<a href="/pdf/2211.06774" title="Download PDF">pdf</a>, <a href="/format/2211.06774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-Scale Bidirectional Training for Zero-Shot Image Captioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taehoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Marsden%2C+M">Mark Marsden</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+P">Pyunghwan Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sangyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sihaeng Lee</a>, 
<a href="/search/cs?searchtype=author&query=Sala%2C+A">Alessandra Sala</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S+H">Seung Hwan Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Arxiv Preprint. Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item857">[857]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.07384" title="Abstract">arXiv:2211.07384</a> (replaced) [<a href="/pdf/2211.07384" title="Download PDF">pdf</a>, <a href="/format/2211.07384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language models are good pathologists: using attention-based sequence  reduction and text-pretrained transformers for efficient WSI classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pisula%2C+J+I">Juan I. Pisula</a>, 
<a href="/search/cs?searchtype=author&query=Bozek%2C+K">Katarzyna Bozek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item858">[858]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.08229" title="Abstract">arXiv:2211.08229</a> (replaced) [<a href="/pdf/2211.08229" title="Download PDF">pdf</a>, <a href="/format/2211.08229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinghuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongbin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinyuan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+N+Z">Neil Zhenqiang Gong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item859">[859]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.08939" title="Abstract">arXiv:2211.08939</a> (replaced) [<a href="/pdf/2211.08939" title="Download PDF">pdf</a>, <a href="/format/2211.08939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmented Physics-Informed Neural Networks (APINNs): A gating  network-based soft domain decomposition methodology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zheyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jagtap%2C+A+D">Ameya D. Jagtap</a>, 
<a href="/search/cs?searchtype=author&query=Karniadakis%2C+G+E">George Em Karniadakis</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Engineering Applications of Artificial Intelligence (EAAI)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Engineering Applications of Artificial Intelligence, Volume 126,
  Part B, November 2023, 107183
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Dynamical Systems (math.DS); Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item860">[860]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09110" title="Abstract">arXiv:2211.09110</a> (replaced) [<a href="/pdf/2211.09110" title="Download PDF">pdf</a>, <a href="/format/2211.09110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Holistic Evaluation of Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>, 
<a href="/search/cs?searchtype=author&query=Bommasani%2C+R">Rishi Bommasani</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+T">Tony Lee</a>, 
<a href="/search/cs?searchtype=author&query=Tsipras%2C+D">Dimitris Tsipras</a>, 
<a href="/search/cs?searchtype=author&query=Soylu%2C+D">Dilara Soylu</a>, 
<a href="/search/cs?searchtype=author&query=Yasunaga%2C+M">Michihiro Yasunaga</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+D">Deepak Narayanan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuhuai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Ananya Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Newman%2C+B">Benjamin Newman</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+B">Binhang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Bobby Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Ce Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cosgrove%2C+C">Christian Cosgrove</a>, 
<a href="/search/cs?searchtype=author&query=Manning%2C+C+D">Christopher D. Manning</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%A9%2C+C">Christopher R&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Acosta-Navas%2C+D">Diana Acosta-Navas</a>, 
<a href="/search/cs?searchtype=author&query=Hudson%2C+D+A">Drew A. Hudson</a>, 
<a href="/search/cs?searchtype=author&query=Zelikman%2C+E">Eric Zelikman</a>, 
<a href="/search/cs?searchtype=author&query=Durmus%2C+E">Esin Durmus</a>, 
<a href="/search/cs?searchtype=author&query=Ladhak%2C+F">Faisal Ladhak</a>, 
<a href="/search/cs?searchtype=author&query=Rong%2C+F">Frieda Rong</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Hongyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Huaxiu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Santhanam%2C+K">Keshav Santhanam</a>, 
<a href="/search/cs?searchtype=author&query=Orr%2C+L">Laurel Orr</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lucia Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yuksekgonul%2C+M">Mert Yuksekgonul</a>, 
<a href="/search/cs?searchtype=author&query=Suzgun%2C+M">Mirac Suzgun</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N">Nathan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Guha%2C+N">Neel Guha</a>, 
<a href="/search/cs?searchtype=author&query=Chatterji%2C+N">Niladri Chatterji</a>, 
<a href="/search/cs?searchtype=author&query=Khattab%2C+O">Omar Khattab</a>, 
<a href="/search/cs?searchtype=author&query=Henderson%2C+P">Peter Henderson</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+R">Ryan Chi</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S+M">Sang Michael Xie</a>, 
<a href="/search/cs?searchtype=author&query=Santurkar%2C+S">Shibani Santurkar</a>, 
<a href="/search/cs?searchtype=author&query=Ganguli%2C+S">Surya Ganguli</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+T">Tatsunori Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Icard%2C+T">Thomas Icard</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhary%2C+V">Vishrav Chaudhary</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">William Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuechen Li</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+Y">Yifan Mai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuhui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Koreeda%2C+Y">Yuta Koreeda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Authored by the Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered Artificial Intelligence (HAI). Project page: <a href="https://crfm.stanford.edu/helm/v1.0">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Published in Transactions on Machine Learning Research (TMLR),
  2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item861">[861]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11242" title="Abstract">arXiv:2211.11242</a> (replaced) [<a href="/pdf/2211.11242" title="Download PDF">pdf</a>, <a href="/format/2211.11242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> L-MAE: Masked Autoencoders are Semantic Segmentation Datasets Augmenter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jiaru Jia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingzhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jiake Xie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Feixiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+A">Aiqing Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item862">[862]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11296" title="Abstract">arXiv:2211.11296</a> (replaced) [<a href="/pdf/2211.11296" title="Download PDF">pdf</a>, <a href="/format/2211.11296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SeeABLE: Soft Discrepancies and Bounded Contrastive Learning for  Exposing Deepfakes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Larue%2C+N">Nicolas Larue</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+N">Ngoc-Son Vu</a>, 
<a href="/search/cs?searchtype=author&query=Struc%2C+V">Vitomir Struc</a>, 
<a href="/search/cs?searchtype=author&query=Peer%2C+P">Peter Peer</a>, 
<a href="/search/cs?searchtype=author&query=Christophides%2C+V">Vassilis Christophides</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item863">[863]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.12158" title="Abstract">arXiv:2211.12158</a> (replaced) [<a href="/pdf/2211.12158" title="Download PDF">pdf</a>, <a href="/ps/2211.12158" title="Download PostScript">ps</a>, <a href="/format/2211.12158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Post&#x27;s correspondence problem for hyperbolic and virtually nilpotent  groups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ciobanu%2C+L">Laura Ciobanu</a>, 
<a href="/search/math?searchtype=author&query=Levine%2C+A">Alex Levine</a>, 
<a href="/search/math?searchtype=author&query=Logan%2C+A+D">Alan D. Logan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, v2. Final version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Group Theory (math.GR)</span>; Computational Complexity (cs.CC); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item864">[864]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13579" title="Abstract">arXiv:2211.13579</a> (replaced) [<a href="/pdf/2211.13579" title="Download PDF">pdf</a>, <a href="/format/2211.13579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge-Aware Federated Active Learning with Non-IID Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yu-Tong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Ye Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Baosheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingya Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 12 figures, ICCV23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item865">[865]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13723" title="Abstract">arXiv:2211.13723</a> (replaced) [<a href="/pdf/2211.13723" title="Download PDF">pdf</a>, <a href="/format/2211.13723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Multi-task Learning via Seeking Task-based Flat Regions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Phan%2C+H">Hoang Phan</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+L">Lam Tran</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+N+N">Ngoc N. Tran</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+N">Nhat Ho</a>, 
<a href="/search/cs?searchtype=author&query=Phung%2C+D">Dinh Phung</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Trung Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 17 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item866">[866]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.15255" title="Abstract">arXiv:2211.15255</a> (replaced) [<a href="/pdf/2211.15255" title="Download PDF">pdf</a>, <a href="/format/2211.15255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARISE: Graph Anomaly Detection on Attributed Networks via Substructure  Awareness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jingcan Duan</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+B">Bin Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Haifang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinwang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures, accepted by IEEE TNNLS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item867">[867]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.15445" title="Abstract">arXiv:2211.15445</a> (replaced) [<a href="/pdf/2211.15445" title="Download PDF">pdf</a>, <a href="/format/2211.15445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $\texttt{Davos}$: a Python &quot;smuggler&quot; for constructing lightweight  reproducible notebooks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fitzpatrick%2C+P+C">Paxton C. Fitzpatrick</a>, 
<a href="/search/cs?searchtype=author&query=Manning%2C+J+R">Jeremy R. Manning</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Computer Science (cs.OH)</span>

</div>
</div>
</dd>
<dt><a name="item868">[868]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.15498" title="Abstract">arXiv:2211.15498</a> (replaced) [<a href="/pdf/2211.15498" title="Download PDF">pdf</a>, <a href="/format/2211.15498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-informed neural networks with unknown measurement noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Pilar%2C+P">Philipp Pilar</a>, 
<a href="/search/stat?searchtype=author&query=Wahlstr%C3%B6m%2C+N">Niklas Wahlstr&#xf6;m</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item869">[869]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.00344" title="Abstract">arXiv:2212.00344</a> (replaced) [<a href="/pdf/2212.00344" title="Download PDF">pdf</a>, <a href="/ps/2212.00344" title="Download PostScript">ps</a>, <a href="/format/2212.00344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Heuristics for Robust Spatial Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chughtai%2C+A+H">Aamir Hussain Chughtai</a>, 
<a href="/search/cs?searchtype=author&query=Tahir%2C+M">Muhammad Tahir</a>, 
<a href="/search/cs?searchtype=author&query=Uppal%2C+M">Momin Uppal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item870">[870]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.02494" title="Abstract">arXiv:2212.02494</a> (replaced) [<a href="/pdf/2212.02494" title="Download PDF">pdf</a>, <a href="/ps/2212.02494" title="Download PostScript">ps</a>, <a href="/format/2212.02494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivalence of eval-readback and eval-apply big-step evaluators by  structuring the lambda-calculus&#x27;s strategy space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nogueira%2C+P">Pablo Nogueira</a> (1), 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-P%C3%A9rez%2C+%C3%81">&#xc1;lvaro Garc&#xed;a-P&#xe9;rez</a> (2) ((1) UDIT, University of Design and Technology, Madrid, Spain, (2) Universit&#xe9; Paris-Saclay, CEA, List, Palaiseau, France)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 74 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item871">[871]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.03553" title="Abstract">arXiv:2212.03553</a> (replaced) [<a href="/pdf/2212.03553" title="Download PDF">pdf</a>, <a href="/format/2212.03553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expressiveness of SHACL Features and Extensions for Full Equality and  Disjointness Tests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bogaerts%2C+B">Bart Bogaerts</a>, 
<a href="/search/cs?searchtype=author&query=Jakubowski%2C+M">Maxime Jakubowski</a>, 
<a href="/search/cs?searchtype=author&query=Van+den+Bussche%2C+J">Jan Van den Bussche</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item872">[872]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.04726" title="Abstract">arXiv:2212.04726</a> (replaced) [<a href="/pdf/2212.04726" title="Download PDF">pdf</a>, <a href="/format/2212.04726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking the Barrier $2^k$ for Subset Feedback Vertex Set in Chordal  Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+T">Tian Bai</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+M">Mingyu Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 9 figures. Full version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item873">[873]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.05799" title="Abstract">arXiv:2212.05799</a> (replaced) [<a href="/pdf/2212.05799" title="Download PDF">pdf</a>, <a href="/ps/2212.05799" title="Download PostScript">ps</a>, <a href="/format/2212.05799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy dissipation and hysteresis cycles in pre-sliding transients of  kinetic friction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ruderman%2C+M">Michael Ruderman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item874">[874]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.08108" title="Abstract">arXiv:2212.08108</a> (replaced) [<a href="/pdf/2212.08108" title="Download PDF">pdf</a>, <a href="/format/2212.08108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Steenhoek%2C+B">Benjamin Steenhoek</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hongyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+W">Wei Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICSE 2024 (Early Cycle). Camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item875">[875]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09178" title="Abstract">arXiv:2212.09178</a> (replaced) [<a href="/e-print/2212.09178" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Support Vector Regression: Risk Quadrangle Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Malandii%2C+A">Anton Malandii</a>, 
<a href="/search/stat?searchtype=author&query=Uryasev%2C+S">Stan Uryasev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Incomplete result
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item876">[876]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00190" title="Abstract">arXiv:2301.00190</a> (replaced) [<a href="/e-print/2301.00190" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tracking Passengers and Baggage Items using Multiple Overhead Cameras at  Security Checkpoints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Siddique%2C+A">Abubakar Siddique</a>, 
<a href="/search/cs?searchtype=author&query=Medeiros%2C+H">Henry Medeiros</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Need to replace already published arxiv version of this work. This work will be the latest version of the previously published <a href="/abs/2007.07924">arXiv:2007.07924</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Systems, Man, and Cybernetics: Systems, Early
  Access, 14 December 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item877">[877]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.01913" title="Abstract">arXiv:2301.01913</a> (replaced) [<a href="/pdf/2301.01913" title="Download PDF">pdf</a>, <a href="/format/2301.01913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning a Generic Value-Selection Heuristic Inside a Constraint  Programming Solver
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marty%2C+T">Tom Marty</a>, 
<a href="/search/cs?searchtype=author&query=Fran%C3%A7ois%2C+T">Tristan Fran&#xe7;ois</a>, 
<a href="/search/cs?searchtype=author&query=Tessier%2C+P">Pierre Tessier</a>, 
<a href="/search/cs?searchtype=author&query=Gauthier%2C+L">Louis Gauthier</a>, 
<a href="/search/cs?searchtype=author&query=Rousseau%2C+L">Louis-Martin Rousseau</a>, 
<a href="/search/cs?searchtype=author&query=Cappart%2C+Q">Quentin Cappart</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item878">[878]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02507" title="Abstract">arXiv:2301.02507</a> (replaced) [<a href="/pdf/2301.02507" title="Download PDF">pdf</a>, <a href="/ps/2301.02507" title="Download PostScript">ps</a>, <a href="/format/2301.02507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perturbation results for distance-edge-monitoring numbers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chenxu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Klasing%2C+R">Ralf Klasing</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Changxiang He</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yaping Mao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Data Structures and Algorithms (cs.DS); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item879">[879]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02615" title="Abstract">arXiv:2301.02615</a> (replaced) [<a href="/pdf/2301.02615" title="Download PDF">pdf</a>, <a href="/format/2301.02615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Silent Killer: A Stealthy, Clean-Label, Black-Box Backdoor Attack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lederer%2C+T">Tzvi Lederer</a>, 
<a href="/search/cs?searchtype=author&query=Maimon%2C+G">Gallil Maimon</a>, 
<a href="/search/cs?searchtype=author&query=Rokach%2C+L">Lior Rokach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item880">[880]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.03150" title="Abstract">arXiv:2301.03150</a> (replaced) [<a href="/pdf/2301.03150" title="Download PDF">pdf</a>, <a href="/format/2301.03150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MOTOR: A Time-To-Event Foundation Model For Structured Medical Records
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Steinberg%2C+E">Ethan Steinberg</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yizhe Xu</a>, 
<a href="/search/cs?searchtype=author&query=Fries%2C+J">Jason Fries</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Nigam Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item881">[881]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.03213" title="Abstract">arXiv:2301.03213</a> (replaced) [<a href="/pdf/2301.03213" title="Download PDF">pdf</a>, <a href="/format/2301.03213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+K">Kevin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Feiszli%2C+M">Matt Feiszli</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiyao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item882">[882]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.03417" title="Abstract">arXiv:2301.03417</a> (replaced) [<a href="/pdf/2301.03417" title="Download PDF">pdf</a>, <a href="/ps/2301.03417" title="Download PostScript">ps</a>, <a href="/format/2301.03417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digraph redicolouring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bousquet%2C+N">Nicolas Bousquet</a> (1), 
<a href="/search/cs?searchtype=author&query=Havet%2C+F">Fr&#xe9;d&#xe9;ric Havet</a> (2), 
<a href="/search/cs?searchtype=author&query=Nisse%2C+N">Nicolas Nisse</a> (2), 
<a href="/search/cs?searchtype=author&query=Picasarri-Arrieta%2C+L">Lucas Picasarri-Arrieta</a> (2), 
<a href="/search/cs?searchtype=author&query=Reinald%2C+A">Amadeus Reinald</a> (2 and 3) ((1) LIRIS, CNRS, Universit&#xe9; Claude Bernard Lyon 1, Lyon, France, (2) CNRS, Universit&#xe9; C&#xf4;te d&#x27;Azur, I3S, Inria, Sophia-Antipolis, France, (3) LIRMM, CNRS, Universit&#xe9; de Montpellier, Montpellier, France)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item883">[883]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.05599" title="Abstract">arXiv:2301.05599</a> (replaced) [<a href="/pdf/2301.05599" title="Download PDF">pdf</a>, <a href="/format/2301.05599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Short-length SSVEP data extension by a novel generative adversarial  networks based framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Pan%2C+Y">Yudong Pan</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+N">Ning Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+Y">Yangsong Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Xu%2C+P">Peng Xu</a>, 
<a href="/search/q-bio?searchtype=author&query=Yao%2C+D">Dezhong Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item884">[884]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.08940" title="Abstract">arXiv:2301.08940</a> (replaced) [<a href="/pdf/2301.08940" title="Download PDF">pdf</a>, <a href="/format/2301.08940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quasi-optimal Reinforcement Learning with Continuous Actions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+Y">Yuhan Li</a>, 
<a href="/search/stat?searchtype=author&query=Zhou%2C+W">Wenzhuo Zhou</a>, 
<a href="/search/stat?searchtype=author&query=Zhu%2C+R">Ruoqing Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item885">[885]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09043" title="Abstract">arXiv:2301.09043</a> (replaced) [<a href="/pdf/2301.09043" title="Download PDF">pdf</a>, <a href="/format/2301.09043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeScore: Evaluating Code Generation by Learning Code Execution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yihong Dong</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jiazheng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xue Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Ge Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuo Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhi Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item886">[886]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09272" title="Abstract">arXiv:2301.09272</a> (replaced) [<a href="/pdf/2301.09272" title="Download PDF">pdf</a>, <a href="/format/2301.09272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Hardness of Approximation for Geometric Bin Packing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ray%2C+A">Arka Ray</a>, 
<a href="/search/cs?searchtype=author&query=Sandeep%2C+S">Sai Sandeep</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages; bug fixes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC); Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item887">[887]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09688" title="Abstract">arXiv:2301.09688</a> (replaced) [<a href="/pdf/2301.09688" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rapid grasping of fabric using bionic soft grippers with elastic  instability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zechen Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zihan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Li Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yufeng Su</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yitong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lipson%2C+H">Hod Lipson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item888">[888]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.10959" title="Abstract">arXiv:2301.10959</a> (replaced) [<a href="/pdf/2301.10959" title="Download PDF">pdf</a>, <a href="/format/2301.10959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tracking Control of Optical Beam Transceivers using Mean Field Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=N%27Doye%2C+I">Ibrahima N&#x27;Doye</a>, 
<a href="/search/eess?searchtype=author&query=Laleg-Kirati%2C+T">Taous-Meriem Laleg-Kirati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item889">[889]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11147" title="Abstract">arXiv:2301.11147</a> (replaced) [<a href="/pdf/2301.11147" title="Download PDF">pdf</a>, <a href="/format/2301.11147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Train Hard, Fight Easy: Robust Meta Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Greenberg%2C+I">Ido Greenberg</a>, 
<a href="/search/cs?searchtype=author&query=Mannor%2C+S">Shie Mannor</a>, 
<a href="/search/cs?searchtype=author&query=Chechik%2C+G">Gal Chechik</a>, 
<a href="/search/cs?searchtype=author&query=Meirom%2C+E">Eli Meirom</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item890">[890]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11259" title="Abstract">arXiv:2301.11259</a> (replaced) [<a href="/pdf/2301.11259" title="Download PDF">pdf</a>, <a href="/format/2301.11259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain-Agnostic Molecular Generation with Self-feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yin Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lingbing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xiaohui Fan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item891">[891]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11443" title="Abstract">arXiv:2301.11443</a> (replaced) [<a href="/pdf/2301.11443" title="Download PDF">pdf</a>, <a href="/format/2301.11443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Limitless stability for Graph Convolutional Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koke%2C+C">Christian Koke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Functional Analysis (math.FA)

</div>
</div>
</dd>
<dt><a name="item892">[892]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12537" title="Abstract">arXiv:2301.12537</a> (replaced) [<a href="/pdf/2301.12537" title="Download PDF">pdf</a>, <a href="/format/2301.12537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Asymptotic State-Space Identification of Closed-Loop Stochastic  Linear Systems using Instrumental Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Szentp%C3%A9teri%2C+S">Szabolcs Szentp&#xe9;teri</a>, 
<a href="/search/eess?searchtype=author&query=Cs%C3%A1ji%2C+B+C">Bal&#xe1;zs Csan&#xe1;d Cs&#xe1;ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 tables, 3 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Systems &amp; Control Letters, Elsevier, Volume 178, 2023, 105565
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Dynamical Systems (math.DS); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item893">[893]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12584" title="Abstract">arXiv:2301.12584</a> (replaced) [<a href="/pdf/2301.12584" title="Download PDF">pdf</a>, <a href="/format/2301.12584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Exact Leverage Score Sampling from Khatri-Rao Products with  Applications to Tensor Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bharadwaj%2C+V">Vivek Bharadwaj</a>, 
<a href="/search/math?searchtype=author&query=Malik%2C+O+A">Osman Asif Malik</a>, 
<a href="/search/math?searchtype=author&query=Murray%2C+R">Riley Murray</a>, 
<a href="/search/math?searchtype=author&query=Grigori%2C+L">Laura Grigori</a>, 
<a href="/search/math?searchtype=author&query=Buluc%2C+A">Aydin Buluc</a>, 
<a href="/search/math?searchtype=author&query=Demmel%2C+J">James Demmel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at the 37th Conference on Neural Information Processing Systems (Neurips'23). 27 pages, 10 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item894">[894]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00456" title="Abstract">arXiv:2302.00456</a> (replaced) [<a href="/pdf/2302.00456" title="Download PDF">pdf</a>, <a href="/format/2302.00456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Feed-Forward Blocks in Transformers through the Lens of  Attention Map
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+G">Goro Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Kuribayashi%2C+T">Tatsuki Kuribayashi</a>, 
<a href="/search/cs?searchtype=author&query=Yokoi%2C+S">Sho Yokoi</a>, 
<a href="/search/cs?searchtype=author&query=Inui%2C+K">Kentaro Inui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 25 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item895">[895]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00713" title="Abstract">arXiv:2302.00713</a> (replaced) [<a href="/pdf/2302.00713" title="Download PDF">pdf</a>, <a href="/format/2302.00713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Weisfeiler-Lehman Distance: Reinterpretation and Connection with  GNNs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Samantha Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+S">Sunhyuk Lim</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%A9moli%2C+F">Facundo M&#xe9;moli</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zhengchao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yusu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item896">[896]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03686" title="Abstract">arXiv:2302.03686</a> (replaced) [<a href="/pdf/2302.03686" title="Download PDF">pdf</a>, <a href="/format/2302.03686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long Horizon Temperature Scaling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shih%2C+A">Andy Shih</a>, 
<a href="/search/cs?searchtype=author&query=Sadigh%2C+D">Dorsa Sadigh</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item897">[897]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04903" title="Abstract">arXiv:2302.04903</a> (replaced) [<a href="/pdf/2302.04903" title="Download PDF">pdf</a>, <a href="/format/2302.04903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaptSim: Task-Driven Simulation Adaptation for Sim-to-Real Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+A+Z">Allen Z. Ren</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Hongkai Dai</a>, 
<a href="/search/cs?searchtype=author&query=Burchfiel%2C+B">Benjamin Burchfiel</a>, 
<a href="/search/cs?searchtype=author&query=Majumdar%2C+A">Anirudha Majumdar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference on Robot Learning (CoRL), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item898">[898]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05361" title="Abstract">arXiv:2302.05361</a> (replaced) [<a href="/pdf/2302.05361" title="Download PDF">pdf</a>, <a href="/format/2302.05361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Inpainting for Single-Image Shadow Removal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoguang Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Abdelfattah%2C+R">Rabab Abdelfattah</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Di Lin</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Wei Feng</a>, 
<a href="/search/cs?searchtype=author&query=Tsang%2C+I">Ivor Tsang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Song Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item899">[899]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06857" title="Abstract">arXiv:2302.06857</a> (replaced) [<a href="/pdf/2302.06857" title="Download PDF">pdf</a>, <a href="/format/2302.06857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Make Your Brief Stroke Real and Stereoscopic: 3D-Aware Simplified Sketch  to Portrait Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yasheng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qianyi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaisiyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tianshu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+C">Chen-Chieh Liao</a>, 
<a href="/search/cs?searchtype=author&query=Miyafuji%2C+S">Shio Miyafuji</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Koike%2C+H">Hideki Koike</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page on <a href="https://hangz-nju-cuhk.github.io/projects/SSSP">this https URL</a>, Video Url: <a href="https://youtu.be/GiOKbvr2U_E">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item900">[900]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07477" title="Abstract">arXiv:2302.07477</a> (replaced) [<a href="/pdf/2302.07477" title="Download PDF">pdf</a>, <a href="/ps/2302.07477" title="Download PostScript">ps</a>, <a href="/format/2302.07477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Sample Complexity of Reinforcement Learning for Mixing  Discounted Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shengbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Blanchet%2C+J">Jose Blanchet</a>, 
<a href="/search/cs?searchtype=author&query=Glynn%2C+P">Peter Glynn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item901">[901]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07778" title="Abstract">arXiv:2302.07778</a> (replaced) [<a href="/pdf/2302.07778" title="Download PDF">pdf</a>, <a href="/format/2302.07778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring the Instability of Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yupei Du</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Dong Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 26 Figures, accepted to ACL 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item902">[902]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08705" title="Abstract">arXiv:2302.08705</a> (replaced) [<a href="/pdf/2302.08705" title="Download PDF">pdf</a>, <a href="/format/2302.08705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OTFS -- Predictability in the Delay-Doppler Domain and its Value to  Communication and Radar Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mohammed%2C+S+K">Saif Khan Mohammed</a>, 
<a href="/search/eess?searchtype=author&query=Hadani%2C+R">Ronny Hadani</a>, 
<a href="/search/eess?searchtype=author&query=Chockalingam%2C+A">Ananthanarayanan Chockalingam</a>, 
<a href="/search/eess?searchtype=author&query=Calderbank%2C+R">Robert Calderbank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article has been accepted for publication in IEEE BITS the Information Theory Magazine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item903">[903]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09693" title="Abstract">arXiv:2302.09693</a> (replaced) [<a href="/pdf/2302.09693" title="Download PDF">pdf</a>, <a href="/format/2302.09693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> mSAM: Micro-Batch-Averaged Sharpness-Aware Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Behdin%2C+K">Kayhan Behdin</a>, 
<a href="/search/stat?searchtype=author&query=Song%2C+Q">Qingquan Song</a>, 
<a href="/search/stat?searchtype=author&query=Gupta%2C+A">Aman Gupta</a>, 
<a href="/search/stat?searchtype=author&query=Keerthi%2C+S">Sathiya Keerthi</a>, 
<a href="/search/stat?searchtype=author&query=Acharya%2C+A">Ayan Acharya</a>, 
<a href="/search/stat?searchtype=author&query=Ocejo%2C+B">Borja Ocejo</a>, 
<a href="/search/stat?searchtype=author&query=Dexter%2C+G">Gregory Dexter</a>, 
<a href="/search/stat?searchtype=author&query=Khanna%2C+R">Rajiv Khanna</a>, 
<a href="/search/stat?searchtype=author&query=Durfee%2C+D">David Durfee</a>, 
<a href="/search/stat?searchtype=author&query=Mazumder%2C+R">Rahul Mazumder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2212.04343">arXiv:2212.04343</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item904">[904]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11464" title="Abstract">arXiv:2302.11464</a> (replaced) [<a href="/e-print/2302.11464" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debiased Mapping for Full-Reference Image Quality Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Baoliang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanwei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lingyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiqi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Basis Angle Consistency in Sec.3.2 will be revised
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item905">[905]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.12533" title="Abstract">arXiv:2302.12533</a> (replaced) [<a href="/e-print/2302.12533" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HUST bearing: a practical dataset for ball bearing fault diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thuan%2C+N+D">Nguyen Duc Thuan</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+H+S">Hoang Si Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We are considering some issues in the paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item906">[906]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13186" title="Abstract">arXiv:2302.13186</a> (replaced) [<a href="/pdf/2302.13186" title="Download PDF">pdf</a>, <a href="/ps/2302.13186" title="Download PostScript">ps</a>, <a href="/format/2302.13186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Construction numbers: How to build a graph?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kainen%2C+P+C">Paul C. Kainen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages; Problem 12401 - 06 in the June 2023 issue of the American Math Monthly is to find this construction number for $K_n$. Solutions are due by {\bf October 31, 2023}
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item907">[907]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13293" title="Abstract">arXiv:2302.13293</a> (replaced) [<a href="/e-print/2302.13293" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PDIWS: Thermal Imaging Dataset for Person Detection in Intrusion Warning  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thuan%2C+N+D">Nguyen Duc Thuan</a>, 
<a href="/search/cs?searchtype=author&query=Anh%2C+L+H">Le Hai Anh</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+H+S">Hoang Si Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We are considering some issues in the paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item908">[908]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14838" title="Abstract">arXiv:2302.14838</a> (replaced) [<a href="/pdf/2302.14838" title="Download PDF">pdf</a>, <a href="/format/2302.14838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EvoPrompting: Language Models for Code-Level Neural Architecture Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Angelica Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dohan%2C+D+M">David M. Dohan</a>, 
<a href="/search/cs?searchtype=author&query=So%2C+D+R">David R. So</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be presented at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item909">[909]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00315" title="Abstract">arXiv:2303.00315</a> (replaced) [<a href="/pdf/2303.00315" title="Download PDF">pdf</a>, <a href="/format/2303.00315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Explorative Key-term Selection Strategies for Conversational  Contextual Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiyong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xutong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuai Li</a>, 
<a href="/search/cs?searchtype=author&query=Lui%2C+J+C+S">John C.S. Lui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item910">[910]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01346" title="Abstract">arXiv:2303.01346</a> (replaced) [<a href="/pdf/2303.01346" title="Download PDF">pdf</a>, <a href="/format/2303.01346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co-learning Planning and Control Policies Constrained by Differentiable  Logic Specifications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zikang Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Lawson%2C+D">Daniel Lawson</a>, 
<a href="/search/cs?searchtype=author&query=Eappen%2C+J">Joe Eappen</a>, 
<a href="/search/cs?searchtype=author&query=Qureshi%2C+A+H">Ahmed H. Qureshi</a>, 
<a href="/search/cs?searchtype=author&query=Jagannathan%2C+S">Suresh Jagannathan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item911">[911]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03124" title="Abstract">arXiv:2303.03124</a> (replaced) [<a href="/pdf/2303.03124" title="Download PDF">pdf</a>, <a href="/format/2303.03124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IFAN: An Explainability-Focused Interaction Framework for Humans and NLP  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mosca%2C+E">Edoardo Mosca</a>, 
<a href="/search/cs?searchtype=author&query=Dementieva%2C+D">Daryna Dementieva</a>, 
<a href="/search/cs?searchtype=author&query=Ajdari%2C+T+E">Tohid Ebrahim Ajdari</a>, 
<a href="/search/cs?searchtype=author&query=Kummeth%2C+M">Maximilian Kummeth</a>, 
<a href="/search/cs?searchtype=author&query=Gringauz%2C+K">Kirill Gringauz</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yutong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Groh%2C+G">Georg Groh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AACL 2023 Demonstration systems Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item912">[912]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04281" title="Abstract">arXiv:2303.04281</a> (replaced) [<a href="/pdf/2303.04281" title="Download PDF">pdf</a>, <a href="/format/2303.04281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Extended Model for Ecological Robustness to Capture Power System  Resilience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+H">Hao Huang</a>, 
<a href="/search/eess?searchtype=author&query=Davis%2C+K+R">Katherine R. Davis</a>, 
<a href="/search/eess?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted By IEEE PES General Meeting 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE Power &amp; Energy Society General Meeting (PESGM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item913">[913]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05341" title="Abstract">arXiv:2303.05341</a> (replaced) [<a href="/pdf/2303.05341" title="Download PDF">pdf</a>, <a href="/format/2303.05341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Penalized Deep Partially Linear Cox Models with Application to CT Scans  of Lung Cancer Patients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sun%2C+Y">Yuming Sun</a>, 
<a href="/search/stat?searchtype=author&query=Kang%2C+J">Jian Kang</a>, 
<a href="/search/stat?searchtype=author&query=Haridas%2C+C">Chinmay Haridas</a>, 
<a href="/search/stat?searchtype=author&query=Mayne%2C+N+R">Nicholas R. Mayne</a>, 
<a href="/search/stat?searchtype=author&query=Potter%2C+A+L">Alexandra L. Potter</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+C+J">Chi-Fu Jeffrey Yang</a>, 
<a href="/search/stat?searchtype=author&query=Christiani%2C+D+C">David C. Christiani</a>, 
<a href="/search/stat?searchtype=author&query=Li%2C+Y">Yi Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item914">[914]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06511" title="Abstract">arXiv:2303.06511</a> (replaced) [<a href="/pdf/2303.06511" title="Download PDF">pdf</a>, <a href="/format/2303.06511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Need for Speed: Fast Correspondence-Free Lidar-Inertial Odometry Using  Doppler Velocity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+D+J">David J. Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Burnett%2C+K">Keenan Burnett</a>, 
<a href="/search/cs?searchtype=author&query=Laconte%2C+J">Johann Laconte</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Vhavle%2C+H">Heethesh Vhavle</a>, 
<a href="/search/cs?searchtype=author&query=Kammel%2C+S">Soeren Kammel</a>, 
<a href="/search/cs?searchtype=author&query=Reuther%2C+J">James Reuther</a>, 
<a href="/search/cs?searchtype=author&query=Barfoot%2C+T+D">Timothy D. Barfoot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted and presented at IROS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item915">[915]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06842" title="Abstract">arXiv:2303.06842</a> (replaced) [<a href="/pdf/2303.06842" title="Download PDF">pdf</a>, <a href="/format/2303.06842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Relationships: A New Perspective to Enhance Scene Graph  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+B">Bowen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+C+J">Camillo J. Taylor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item916">[916]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08040" title="Abstract">arXiv:2303.08040</a> (replaced) [<a href="/pdf/2303.08040" title="Download PDF">pdf</a>, <a href="/format/2303.08040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Demographic Parity: Redefining Equal Treatment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mougan%2C+C">Carlos Mougan</a>, 
<a href="/search/cs?searchtype=author&query=State%2C+L">Laura State</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+A">Antonio Ferrara</a>, 
<a href="/search/cs?searchtype=author&query=Ruggieri%2C+S">Salvatore Ruggieri</a>, 
<a href="/search/cs?searchtype=author&query=Staab%2C+S">Steffen Staab</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item917">[917]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08605" title="Abstract">arXiv:2303.08605</a> (replaced) [<a href="/pdf/2303.08605" title="Download PDF">pdf</a>, <a href="/format/2303.08605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RICO: Regularizing the Unobservable for Indoor Compositional  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zizhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+X">Xiaoyang Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yuanyuan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengmeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Y">Yiyi Liao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item918">[918]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09656" title="Abstract">arXiv:2303.09656</a> (replaced) [<a href="/pdf/2303.09656" title="Download PDF">pdf</a>, <a href="/format/2303.09656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revealing complexities when adult readers engage in the credibility  evaluation of social media posts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuutila%2C+M">Miikka Kuutila</a>, 
<a href="/search/cs?searchtype=author&query=Kiili%2C+C">Carita Kiili</a>, 
<a href="/search/cs?searchtype=author&query=Kupiainen%2C+R">Reijo Kupiainen</a>, 
<a href="/search/cs?searchtype=author&query=Huusko%2C+E">Eetu Huusko</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Hosio%2C+S">Simo Hosio</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%A4ntyl%C3%A4%2C+M">Mika M&#xe4;ntyl&#xe4;</a>, 
<a href="/search/cs?searchtype=author&query=Coiro%2C+J">Julie Coiro</a>, 
<a href="/search/cs?searchtype=author&query=Kiili%2C+K">Kristian Kiili</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 5 figures including the appendix. Submitted to a journal for peer review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item919">[919]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09792" title="Abstract">arXiv:2303.09792</a> (replaced) [<a href="/pdf/2303.09792" title="Download PDF">pdf</a>, <a href="/format/2303.09792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Sparse Visual Prompt for Domain Adaptive Dense Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Senqiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiarui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qizhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+M">Mingjie Pan</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+Y">Yulu Gan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zehui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shanghang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item920">[920]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09874" title="Abstract">arXiv:2303.09874</a> (replaced) [<a href="/pdf/2303.09874" title="Download PDF">pdf</a>, <a href="/format/2303.09874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangling the Link Between Image Statistics and Human Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hepburn%2C+A">Alexander Hepburn</a>, 
<a href="/search/cs?searchtype=author&query=Laparra%2C+V">Valero Laparra</a>, 
<a href="/search/cs?searchtype=author&query=Santos-Rodriguez%2C+R">Ra&#xfa;l Santos-Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Malo%2C+J">Jes&#xfa;s Malo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item921">[921]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09892" title="Abstract">arXiv:2303.09892</a> (replaced) [<a href="/pdf/2303.09892" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memotion 3: Dataset on Sentiment and Emotion Analysis of Codemixed  Hindi-English Memes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Shreyash Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Suryavardan%2C+S">S Suryavardan</a>, 
<a href="/search/cs?searchtype=author&query=Patwa%2C+P">Parth Patwa</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+M">Megha Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Rani%2C+A">Anku Rani</a>, 
<a href="/search/cs?searchtype=author&query=Reganti%2C+A">Aishwarya Reganti</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Amitava Das</a>, 
<a href="/search/cs?searchtype=author&query=Sheth%2C+A">Amit Sheth</a>, 
<a href="/search/cs?searchtype=author&query=Chinnakotla%2C+M">Manoj Chinnakotla</a>, 
<a href="/search/cs?searchtype=author&query=Ekbal%2C+A">Asif Ekbal</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Srijan Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Defactify2 @AAAI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item922">[922]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10310" title="Abstract">arXiv:2303.10310</a> (replaced) [<a href="/pdf/2303.10310" title="Download PDF">pdf</a>, <a href="/format/2303.10310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain-knowledge Inspired Pseudo Supervision (DIPS) for Unsupervised  Image-to-Image Translation Models to Support Cross-Domain Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-Hindawi%2C+F">Firas Al-Hindawi</a>, 
<a href="/search/cs?searchtype=author&query=Siddiquee%2C+M+M+R">Md Mahfuzur Rahman Siddiquee</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Teresa Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Han Hu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Ying Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2212.09107">arXiv:2212.09107</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item923">[923]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12206" title="Abstract">arXiv:2303.12206</a> (replaced) [<a href="/pdf/2303.12206" title="Download PDF">pdf</a>, <a href="/format/2303.12206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Policy Optimization for Personalized Interventions in Behavioral Health
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baek%2C+J">Jackie Baek</a>, 
<a href="/search/cs?searchtype=author&query=Boutilier%2C+J+J">Justin J. Boutilier</a>, 
<a href="/search/cs?searchtype=author&query=Farias%2C+V+F">Vivek F. Farias</a>, 
<a href="/search/cs?searchtype=author&query=Jonasson%2C+J+O">Jonas Oddur Jonasson</a>, 
<a href="/search/cs?searchtype=author&query=Yoeli%2C+E">Erez Yoeli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item924">[924]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12706" title="Abstract">arXiv:2303.12706</a> (replaced) [<a href="/pdf/2303.12706" title="Download PDF">pdf</a>, <a href="/format/2303.12706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-modal Variational Autoencoders for normative modelling across  multiple imaging modalities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aguila%2C+A+L">Ana Lawry Aguila</a>, 
<a href="/search/cs?searchtype=author&query=Chapman%2C+J">James Chapman</a>, 
<a href="/search/cs?searchtype=author&query=Altmann%2C+A">Andre Altmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item925">[925]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12981" title="Abstract">arXiv:2303.12981</a> (replaced) [<a href="/pdf/2303.12981" title="Download PDF">pdf</a>, <a href="/format/2303.12981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connected Superlevel Set in (Deep) Reinforcement Learning and its  Application to Minimax Theorems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Sihan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Doan%2C+T+T">Thinh T. Doan</a>, 
<a href="/search/cs?searchtype=author&query=Romberg%2C+J">Justin Romberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item926">[926]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13006" title="Abstract">arXiv:2303.13006</a> (replaced) [<a href="/pdf/2303.13006" title="Download PDF">pdf</a>, <a href="/format/2303.13006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controllable Inversion of Black-Box Face Recognition Models via  Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kansy%2C+M">Manuel Kansy</a>, 
<a href="/search/cs?searchtype=author&query=Ra%C3%ABl%2C+A">Anton Ra&#xeb;l</a>, 
<a href="/search/cs?searchtype=author&query=Mignone%2C+G">Graziana Mignone</a>, 
<a href="/search/cs?searchtype=author&query=Naruniec%2C+J">Jacek Naruniec</a>, 
<a href="/search/cs?searchtype=author&query=Schroers%2C+C">Christopher Schroers</a>, 
<a href="/search/cs?searchtype=author&query=Gross%2C+M">Markus Gross</a>, 
<a href="/search/cs?searchtype=author&query=Weber%2C+R+M">Romann M. Weber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages main paper + 23 pages supplementary material. Moderate revisions from v1 (different template, added user study, wording). Presented at AMFG workshop at ICCV 2023. Project page: <a href="https://studios.disneyresearch.com/2023/10/02/controllable-inversion-of-black-box-face-recognition-models-via-diffusion/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item927">[927]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13227" title="Abstract">arXiv:2303.13227</a> (replaced) [<a href="/pdf/2303.13227" title="Download PDF">pdf</a>, <a href="/format/2303.13227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confidence-Aware and Self-Supervised Image Anomaly Localisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+J+P">Johanna P. M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Baugh%2C+M">Matthew Baugh</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jeremy Tan</a>, 
<a href="/search/cs?searchtype=author&query=Dombrowski%2C+M">Mischa Dombrowski</a>, 
<a href="/search/cs?searchtype=author&query=Kainz%2C+B">Bernhard Kainz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for MICCAI UNSURE Workshop 2023 (Spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item928">[928]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15553" title="Abstract">arXiv:2303.15553</a> (replaced) [<a href="/pdf/2303.15553" title="Download PDF">pdf</a>, <a href="/format/2303.15553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoViT: Memorizing Vision Transformers for Medical Image Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yiqing Shen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Pengfei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jingpu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qianqi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+N">Nhat Le</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jinyuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shanshan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Unberath%2C+M">Mathias Unberath</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item929">[929]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15564" title="Abstract">arXiv:2303.15564</a> (replaced) [<a href="/pdf/2303.15564" title="Download PDF">pdf</a>, <a href="/format/2303.15564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mask and Restore: Blind Backdoor Defense at Test Time with Masked  Autoencoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+L">Lu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+H">Haibin Ling</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item930">[930]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15688" title="Abstract">arXiv:2303.15688</a> (replaced) [<a href="/pdf/2303.15688" title="Download PDF">pdf</a>, <a href="/format/2303.15688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Deep Learning of Robust, Adaptive Policies using Tube  MPC-Guided Data Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Tagliabue%2C+A">Andrea Tagliabue</a>, 
<a href="/search/cs?searchtype=author&query=How%2C+J+P">Jonathan P. How</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures. IROS23 final version, with page numbers added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item931">[931]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16454" title="Abstract">arXiv:2303.16454</a> (replaced) [<a href="/pdf/2303.16454" title="Download PDF">pdf</a>, <a href="/format/2303.16454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conductivity Imaging from Internal Measurements with Mixed Least-Squares  Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jin%2C+B">Bangti Jin</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+X">Xiyao Li</a>, 
<a href="/search/math?searchtype=author&query=Quan%2C+Q">Qimeng Quan</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Z">Zhi Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages. 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item932">[932]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16721" title="Abstract">arXiv:2303.16721</a> (replaced) [<a href="/pdf/2303.16721" title="Download PDF">pdf</a>, <a href="/ps/2303.16721" title="Download PostScript">ps</a>, <a href="/format/2303.16721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance-guaranteed regularization in maximum likelihood method:  Gauge symmetry in Kullback -- Leibler divergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ichiki%2C+A">Akihisa Ichiki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item933">[933]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17550" title="Abstract">arXiv:2303.17550</a> (replaced) [<a href="/pdf/2303.17550" title="Download PDF">pdf</a>, <a href="/format/2303.17550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with  Diffusion Autoencoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chenpeng Du</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qi Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianyu He</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Kai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Sheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACM Multimedia 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item934">[934]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17584" title="Abstract">arXiv:2303.17584</a> (replaced) [<a href="/pdf/2303.17584" title="Download PDF">pdf</a>, <a href="/format/2303.17584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consensus controller with safety guarantee: an application to the  kinematic bicycle model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Niu%2C+K">Kaicheng Niu</a>, 
<a href="/search/eess?searchtype=author&query=Abdallah%2C+C">Chaouki Abdallah</a>, 
<a href="/search/eess?searchtype=author&query=Hayajneh%2C+M">Mohammad Hayajneh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item935">[935]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17778" title="Abstract">arXiv:2303.17778</a> (replaced) [<a href="/pdf/2303.17778" title="Download PDF">pdf</a>, <a href="/format/2303.17778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CrossLoc3D: Aerial-Ground Cross-Source 3D Place Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guan%2C+T">Tianrui Guan</a>, 
<a href="/search/cs?searchtype=author&query=Muthuselvam%2C+A">Aswath Muthuselvam</a>, 
<a href="/search/cs?searchtype=author&query=Hoover%2C+M">Montana Hoover</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xijun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jing Liang</a>, 
<a href="/search/cs?searchtype=author&query=Sathyamoorthy%2C+A+J">Adarsh Jagan Sathyamoorthy</a>, 
<a href="/search/cs?searchtype=author&query=Conover%2C+D">Damon Conover</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item936">[936]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.18154" title="Abstract">arXiv:2303.18154</a> (replaced) [<a href="/pdf/2303.18154" title="Download PDF">pdf</a>, <a href="/ps/2303.18154" title="Download PostScript">ps</a>, <a href="/format/2303.18154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Data-Driven Computation of Polytopic Robust Control Invariant  Sets and State-Feedback Controllers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mejari%2C+M">Manas Mejari</a>, 
<a href="/search/eess?searchtype=author&query=Gupta%2C+A">Ankit Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures, accepted for publication, to appear at the 62nd IEEE Conference on Decision and Control (CDC 2023), Singapore
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item937">[937]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00002" title="Abstract">arXiv:2304.00002</a> (replaced) [<a href="/pdf/2304.00002" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Interpretable Benchmarks: Contextual Learning through Cognitive  and Multimodal Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=DiSanto%2C+N">Nick DiSanto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item938">[938]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00186" title="Abstract">arXiv:2304.00186</a> (replaced) [<a href="/pdf/2304.00186" title="Download PDF">pdf</a>, <a href="/format/2304.00186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subject-driven Text-to-Image Generation via Apprenticeship Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hexiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yandong Li</a>, 
<a href="/search/cs?searchtype=author&query=Ruiz%2C+N">Nataniel Ruiz</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xuhui Jia</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+M">Ming-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+W+W">William W. Cohen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023. Model Service to be appear as Google Vertex AI - Instant Tuning (<a href="https://cloud.google.com/vertex-ai/docs/generative-ai/image/fine-tune-model">this https URL</a>). The link to demo video: <a href="https://www.youtube.com/watch?v=Q2xQ91D_dhM">this https URL</a>&amp;t=2071s&amp;ab_channel=GoogleCloud
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item939">[939]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01230" title="Abstract">arXiv:2304.01230</a> (replaced) [<a href="/pdf/2304.01230" title="Download PDF">pdf</a>, <a href="/format/2304.01230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEENN: Towards Temporal Spiking Early-Exit Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Geller%2C+T">Tamar Geller</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Youngeun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Panda%2C+P">Priyadarshini Panda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item940">[940]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02419" title="Abstract">arXiv:2304.02419</a> (replaced) [<a href="/pdf/2304.02419" title="Download PDF">pdf</a>, <a href="/format/2304.02419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+K">Kehong Gong</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+D">Dongze Lian</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+H">Heng Chang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+C">Chuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zihang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+X">Xinxin Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+M+B">Michael Bi Mi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinchao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item941">[941]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02858" title="Abstract">arXiv:2304.02858</a> (replaced) [<a href="/pdf/2304.02858" title="Download PDF">pdf</a>, <a href="/format/2304.02858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A review of ensemble learning and data augmentation models for class  imbalanced problems: combination, implementation and evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+A+A">Azal Ahmad Khan</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhari%2C+O">Omkar Chaudhari</a>, 
<a href="/search/cs?searchtype=author&query=Chandra%2C+R">Rohitash Chandra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item942">[942]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03897" title="Abstract">arXiv:2304.03897</a> (replaced) [<a href="/pdf/2304.03897" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factify 2: A Multimodal Fake News and Satire News Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suryavardan%2C+S">S Suryavardan</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Shreyash Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Patwa%2C+P">Parth Patwa</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+M">Megha Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Rani%2C+A">Anku Rani</a>, 
<a href="/search/cs?searchtype=author&query=Reganti%2C+A">Aishwarya Reganti</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Amitava Das</a>, 
<a href="/search/cs?searchtype=author&query=Sheth%2C+A">Amit Sheth</a>, 
<a href="/search/cs?searchtype=author&query=Chinnakotla%2C+M">Manoj Chinnakotla</a>, 
<a href="/search/cs?searchtype=author&query=Ekbal%2C+A">Asif Ekbal</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Srijan Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Defactify2 @AAAI2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item943">[943]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04546" title="Abstract">arXiv:2304.04546</a> (replaced) [<a href="/pdf/2304.04546" title="Download PDF">pdf</a>, <a href="/format/2304.04546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kinship Representation Learning with Face Componential Relation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+W">Weng-Tai Su</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Min-Hung Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chien-Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+S">Shang-Hong Lai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T+P">Trista Pei-Chun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023 Workshop (Analysis and Modeling of Faces and Gestures)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item944">[944]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05673" title="Abstract">arXiv:2304.05673</a> (replaced) [<a href="/pdf/2304.05673" title="Download PDF">pdf</a>, <a href="/format/2304.05673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Precise localization of corneal reflections in eye images using deep  learning trained on synthetic data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Byrne%2C+S+A">Sean Anthony Byrne</a>, 
<a href="/search/cs?searchtype=author&query=Nystr%C3%B6m%2C+M">Marcus Nystr&#xf6;m</a>, 
<a href="/search/cs?searchtype=author&query=Maquiling%2C+V">Virmarie Maquiling</a>, 
<a href="/search/cs?searchtype=author&query=Kasneci%2C+E">Enkelejda Kasneci</a>, 
<a href="/search/cs?searchtype=author&query=Niehorster%2C+D+C">Diederick C. Niehorster</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item945">[945]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06022" title="Abstract">arXiv:2304.06022</a> (replaced) [<a href="/pdf/2304.06022" title="Download PDF">pdf</a>, <a href="/format/2304.06022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAM Struggles in Concealed Scenes -- Empirical Study on &quot;Segment  Anything&quot;
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+G">Ge-Peng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+D">Deng-Ping Fan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Peng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Ming-Ming Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bowen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by SCIENCE CHINA Information Sciences, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item946">[946]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06094" title="Abstract">arXiv:2304.06094</a> (replaced) [<a href="/pdf/2304.06094" title="Download PDF">pdf</a>, <a href="/format/2304.06094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-guided Entropic Neural Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mokrov%2C+P">Petr Mokrov</a>, 
<a href="/search/cs?searchtype=author&query=Korotin%2C+A">Alexander Korotin</a>, 
<a href="/search/cs?searchtype=author&query=Kolesov%2C+A">Alexander Kolesov</a>, 
<a href="/search/cs?searchtype=author&query=Gushchin%2C+N">Nikita Gushchin</a>, 
<a href="/search/cs?searchtype=author&query=Burnaev%2C+E">Evgeny Burnaev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item947">[947]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06141" title="Abstract">arXiv:2304.06141</a> (replaced) [<a href="/pdf/2304.06141" title="Download PDF">pdf</a>, <a href="/ps/2304.06141" title="Download PostScript">ps</a>, <a href="/format/2304.06141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Analysis for Near-Field MIMO: Discrete and Continuous  Aperture Antennas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Ziyi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiaqi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuanli Wu</a>, 
<a href="/search/cs?searchtype=author&query=Nallanathan%2C+A">Arumugam Nallanathan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures. This is the long version of the paper which published in IEEE Wireless Communications Letters with the same title
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item948">[948]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06686" title="Abstract">arXiv:2304.06686</a> (replaced) [<a href="/pdf/2304.06686" title="Download PDF">pdf</a>, <a href="/format/2304.06686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OKRidge: Scalable Optimal k-Sparse Ridge Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiachang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Rosen%2C+S">Sam Rosen</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+C">Chudi Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Rudin%2C+C">Cynthia Rudin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023, pre camera ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item949">[949]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07665" title="Abstract">arXiv:2304.07665</a> (replaced) [<a href="/pdf/2304.07665" title="Download PDF">pdf</a>, <a href="/format/2304.07665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression  with Bayesian Hierarchical Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Islam%2C+U+J">Upala Junaida Islam</a>, 
<a href="/search/cs?searchtype=author&query=Paynabar%2C+K">Kamran Paynabar</a>, 
<a href="/search/cs?searchtype=author&query=Runger%2C+G">George Runger</a>, 
<a href="/search/cs?searchtype=author&query=Iquebal%2C+A+S">Ashif Sikandar Iquebal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 10 figures, 0 table, submitted to IISE Transaction
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item950">[950]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07866" title="Abstract">arXiv:2304.07866</a> (replaced) [<a href="/pdf/2304.07866" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Flexible Modified Impedance Network Converter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Besati%2C+S">Shirin Besati</a>, 
<a href="/search/eess?searchtype=author&query=Essakiappan%2C+S">Somasundaram Essakiappan</a>, 
<a href="/search/eess?searchtype=author&query=Manjrekar%2C+M">Madhav Manjrekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be presented in ITEC2023, Detroit, MI, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item951">[951]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09718" title="Abstract">arXiv:2304.09718</a> (replaced) [<a href="/pdf/2304.09718" title="Download PDF">pdf</a>, <a href="/format/2304.09718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample-efficient Model-based Reinforcement Learning for Quantum Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Khalid%2C+I">Irtaza Khalid</a>, 
<a href="/search/quant-ph?searchtype=author&query=Weidner%2C+C+A">Carrie A. Weidner</a>, 
<a href="/search/quant-ph?searchtype=author&query=Jonckheere%2C+E+A">Edmond A. Jonckheere</a>, 
<a href="/search/quant-ph?searchtype=author&query=Shermer%2C+S+G">Sophie G. Shermer</a>, 
<a href="/search/quant-ph?searchtype=author&query=Langbein%2C+F+C">Frank C. Langbein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14+10 pages, 6+6 figures, revised version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Phys. Rev. Research 5, 043002 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item952">[952]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10592" title="Abstract">arXiv:2304.10592</a> (replaced) [<a href="/pdf/2304.10592" title="Download PDF">pdf</a>, <a href="/format/2304.10592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Deyao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xiaoqian Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Elhoseiny%2C+M">Mohamed Elhoseiny</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Website: <a href="https://minigpt-4.github.io/">this https URL</a>; Code, Pretrained Model, and Dataset: <a href="https://github.com/Vision-CAIR/MiniGPT-4">this https URL</a>; Deyao Zhu and Jun Chen contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item953">[953]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10864" title="Abstract">arXiv:2304.10864</a> (replaced) [<a href="/pdf/2304.10864" title="Download PDF">pdf</a>, <a href="/format/2304.10864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FreMIM: Fourier Transform Meets Masked Image Modeling for Medical Image  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J">Jianbo Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yuanxiu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shanshan Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiangyun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item954">[954]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11160" title="Abstract">arXiv:2304.11160</a> (replaced) [<a href="/pdf/2304.11160" title="Download PDF">pdf</a>, <a href="/format/2304.11160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Isotonic Mechanism for Exponential Family Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yan%2C+Y">Yuling Yan</a>, 
<a href="/search/math?searchtype=author&query=Su%2C+W+J">Weijie J. Su</a>, 
<a href="/search/math?searchtype=author&query=Fan%2C+J">Jianqing Fan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); Theoretical Economics (econ.TH); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item955">[955]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11328" title="Abstract">arXiv:2304.11328</a> (replaced) [<a href="/pdf/2304.11328" title="Download PDF">pdf</a>, <a href="/ps/2304.11328" title="Download PostScript">ps</a>, <a href="/format/2304.11328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Accelerating Diffusion-Based Sampling Process via Improved  Integration Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guoqiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kenta%2C+N">Niwa Kenta</a>, 
<a href="/search/cs?searchtype=author&query=Kleijn%2C+W+B">W. Bastiaan Kleijn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item956">[956]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12317" title="Abstract">arXiv:2304.12317</a> (replaced) [<a href="/pdf/2304.12317" title="Download PDF">pdf</a>, <a href="/format/2304.12317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Total-Recon: Deformable Scene Reconstruction for Embodied View Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chonghyuk Song</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Gengshan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+K">Kangle Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun-Yan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ramanan%2C+D">Deva Ramanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023 camera-ready version. Project page with code, models, and data: <a href="https://andrewsonga.github.io/totalrecon">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item957">[957]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14065" title="Abstract">arXiv:2304.14065</a> (replaced) [<a href="/pdf/2304.14065" title="Download PDF">pdf</a>, <a href="/format/2304.14065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lightweight, Pre-trained Transformers for Remote Sensing Timeseries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tseng%2C+G">Gabriel Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Cartuyvels%2C+R">Ruben Cartuyvels</a>, 
<a href="/search/cs?searchtype=author&query=Zvonkov%2C+I">Ivan Zvonkov</a>, 
<a href="/search/cs?searchtype=author&query=Purohit%2C+M">Mirali Purohit</a>, 
<a href="/search/cs?searchtype=author&query=Rolnick%2C+D">David Rolnick</a>, 
<a href="/search/cs?searchtype=author&query=Kerner%2C+H">Hannah Kerner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item958">[958]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01639" title="Abstract">arXiv:2305.01639</a> (replaced) [<a href="/pdf/2305.01639" title="Download PDF">pdf</a>, <a href="/format/2305.01639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving In-Context Learning for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Panda%2C+A">Ashwinee Panda</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J+T">Jiachen T. Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+P">Prateek Mittal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item959">[959]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02154" title="Abstract">arXiv:2305.02154</a> (replaced) [<a href="/pdf/2305.02154" title="Download PDF">pdf</a>, <a href="/format/2305.02154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral bound for random Schreier graphs of the general linear group
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Caillat-Grenier%2C+G">Geoffroy Caillat-Grenier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item960">[960]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02614" title="Abstract">arXiv:2305.02614</a> (replaced) [<a href="/pdf/2305.02614" title="Download PDF">pdf</a>, <a href="/format/2305.02614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-dimensional Bayesian Optimization via Semi-supervised Learning with  Optimized Unlabeled Data Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yuxuan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item961">[961]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03731" title="Abstract">arXiv:2305.03731</a> (replaced) [<a href="/pdf/2305.03731" title="Download PDF">pdf</a>, <a href="/format/2305.03731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Working Memory Capacity of ChatGPT: An Empirical Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+D">Dongyu Gong</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xingchen Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dingmin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 9 figures, and 2 tables in the main text
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item962">[962]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03815" title="Abstract">arXiv:2305.03815</a> (replaced) [<a href="/pdf/2305.03815" title="Download PDF">pdf</a>, <a href="/format/2305.03815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Persistent Homology Meets Object Unity: Object Recognition in Clutter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Samani%2C+E+U">Ekta U. Samani</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+A+G">Ashis G. Banerjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conditionally accepted for publication in the IEEE Transactions on Robotics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item963">[963]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04391" title="Abstract">arXiv:2305.04391</a> (replaced) [<a href="/pdf/2305.04391" title="Download PDF">pdf</a>, <a href="/format/2305.04391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Variational Perspective on Solving Inverse Problems with Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mardani%2C+M">Morteza Mardani</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiaming Song</a>, 
<a href="/search/cs?searchtype=author&query=Kautz%2C+J">Jan Kautz</a>, 
<a href="/search/cs?searchtype=author&query=Vahdat%2C+A">Arash Vahdat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item964">[964]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05485" title="Abstract">arXiv:2305.05485</a> (replaced) [<a href="/pdf/2305.05485" title="Download PDF">pdf</a>, <a href="/format/2305.05485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resilient Temporal Logic Planning in the Presence of Robot Failures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalluraya%2C+S">Samarth Kalluraya</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+G+J">George J. Pappas</a>, 
<a href="/search/cs?searchtype=author&query=Kantaros%2C+Y">Yiannis Kantaros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item965">[965]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05559" title="Abstract">arXiv:2305.05559</a> (replaced) [<a href="/pdf/2305.05559" title="Download PDF">pdf</a>, <a href="/format/2305.05559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Stream Semantic Registers: A Lightweight ISA Extension  Accelerating General Sparse Linear Algebra
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scheffler%2C+P">Paul Scheffler</a>, 
<a href="/search/cs?searchtype=author&query=Zaruba%2C+F">Florian Zaruba</a>, 
<a href="/search/cs?searchtype=author&query=Schuiki%2C+F">Fabian Schuiki</a>, 
<a href="/search/cs?searchtype=author&query=Hoefler%2C+T">Torsten Hoefler</a>, 
<a href="/search/cs?searchtype=author&query=Benini%2C+L">Luca Benini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 8 figures. Accepted for publication in IEEE TPDS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item966">[966]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06568" title="Abstract">arXiv:2305.06568</a> (replaced) [<a href="/pdf/2305.06568" title="Download PDF">pdf</a>, <a href="/format/2305.06568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convolutional Neural Networks Rarely Learn Shape for Semantic  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mazurowski%2C+M+A">Maciej A. Mazurowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Pattern Recognition
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item967">[967]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08024" title="Abstract">arXiv:2305.08024</a> (replaced) [<a href="/pdf/2305.08024" title="Download PDF">pdf</a>, <a href="/format/2305.08024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grasping Extreme Aerodynamics on a Low-Dimensional Manifold
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Fukami%2C+K">Kai Fukami</a>, 
<a href="/search/physics?searchtype=author&query=Taira%2C+K">Kunihiko Taira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item968">[968]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09770" title="Abstract">arXiv:2305.09770</a> (replaced) [<a href="/pdf/2305.09770" title="Download PDF">pdf</a>, <a href="/format/2305.09770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to  Support Human-AI Scientific Writing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Hua Shen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chieh-Yang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tongshuang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T+%27">Ting-Hao &#x27;Kenneth&#x27; Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in CSCW 2023 Demo. ConvXAI system code: <a href="https://github.com/huashen218/convxai.git">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item969">[969]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10616" title="Abstract">arXiv:2305.10616</a> (replaced) [<a href="/pdf/2305.10616" title="Download PDF">pdf</a>, <a href="/format/2305.10616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation Metrics for DNNs Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghobrial%2C+A">Abanoub Ghobrial</a>, 
<a href="/search/cs?searchtype=author&query=Budgett%2C+S">Samuel Budgett</a>, 
<a href="/search/cs?searchtype=author&query=Balemans%2C+D">Dieter Balemans</a>, 
<a href="/search/cs?searchtype=author&query=Asgari%2C+H">Hamid Asgari</a>, 
<a href="/search/cs?searchtype=author&query=Reiter%2C+P">Phil Reiter</a>, 
<a href="/search/cs?searchtype=author&query=Eder%2C+K">Kerstin Eder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item970">[970]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10790" title="Abstract">arXiv:2305.10790</a> (replaced) [<a href="/pdf/2305.10790" title="Download PDF">pdf</a>, <a href="/format/2305.10790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Listen, Think, and Understand
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gong%2C+Y">Yuan Gong</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+H">Hongyin Luo</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+A+H">Alexander H. Liu</a>, 
<a href="/search/eess?searchtype=author&query=Karlinsky%2C+L">Leonid Karlinsky</a>, 
<a href="/search/eess?searchtype=author&query=Glass%2C+J">James Glass</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, work in progress. Interactive demo at <a href="https://huggingface.co/spaces/yuangongfdu/ltu">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item971">[971]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10865" title="Abstract">arXiv:2305.10865</a> (replaced) [<a href="/pdf/2305.10865" title="Download PDF">pdf</a>, <a href="/format/2305.10865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantically Aligned Task Decomposition in Multi-Agent Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+D">Dan Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baoxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiangfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+B">Bo Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+H">Hongyuan Zha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 54 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item972">[972]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10924" title="Abstract">arXiv:2305.10924</a> (replaced) [<a href="/pdf/2305.10924" title="Download PDF">pdf</a>, <a href="/format/2305.10924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structural Pruning for Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+G">Gongfan Fang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xinyin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinchao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item973">[973]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11499" title="Abstract">arXiv:2305.11499</a> (replaced) [<a href="/pdf/2305.11499" title="Download PDF">pdf</a>, <a href="/format/2305.11499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by  Reversing Chain-of-Thought
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+T">Tianci Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenhailong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chi Han</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Pengfei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item974">[974]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11738" title="Abstract">arXiv:2305.11738</a> (replaced) [<a href="/pdf/2305.11738" title="Download PDF">pdf</a>, <a href="/format/2305.11738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CRITIC: Large Language Models Can Self-Correct with Tool-Interactive  Critiquing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gou%2C+Z">Zhibin Gou</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zhihong Shao</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yeyun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yelong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+N">Nan Duan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> add LLaMA-2 7B to 70B results; add more mathematical program synthesis datasets
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item975">[975]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11854" title="Abstract">arXiv:2305.11854</a> (replaced) [<a href="/pdf/2305.11854" title="Download PDF">pdf</a>, <a href="/format/2305.11854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Web Navigation with Instruction-Finetuned Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Furuta%2C+H">Hiroki Furuta</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kuang-Huei Lee</a>, 
<a href="/search/cs?searchtype=author&query=Nachum%2C+O">Ofir Nachum</a>, 
<a href="/search/cs?searchtype=author&query=Matsuo%2C+Y">Yutaka Matsuo</a>, 
<a href="/search/cs?searchtype=author&query=Faust%2C+A">Aleksandra Faust</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+S+S">Shixiang Shane Gu</a>, 
<a href="/search/cs?searchtype=author&query=Gur%2C+I">Izzeddin Gur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website: <a href="https://sites.google.com/view/mm-webnav/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item976">[976]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12286" title="Abstract">arXiv:2305.12286</a> (replaced) [<a href="/pdf/2305.12286" title="Download PDF">pdf</a>, <a href="/format/2305.12286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Earth Satellite Orbit Determination Using Deep Convolutional  Networks with Satellite Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khorana%2C+R">Rohit Khorana</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item977">[977]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12718" title="Abstract">arXiv:2305.12718</a> (replaced) [<a href="/pdf/2305.12718" title="Download PDF">pdf</a>, <a href="/format/2305.12718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HighLight: Efficient and Flexible DNN Acceleration with Hierarchical  Structured Sparsity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y+N">Yannan Nellie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+P">Po-An Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Muralidharan%2C+S">Saurav Muralidharan</a>, 
<a href="/search/cs?searchtype=author&query=Parashar%2C+A">Angshuman Parashar</a>, 
<a href="/search/cs?searchtype=author&query=Sze%2C+V">Vivienne Sze</a>, 
<a href="/search/cs?searchtype=author&query=Emer%2C+J+S">Joel S. Emer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to MICRO23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item978">[978]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12788" title="Abstract">arXiv:2305.12788</a> (replaced) [<a href="/pdf/2305.12788" title="Download PDF">pdf</a>, <a href="/format/2305.12788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge  Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+P">Pengcheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Cao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Cross%2C+A">Adam Cross</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jimeng Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item979">[979]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13301" title="Abstract">arXiv:2305.13301</a> (replaced) [<a href="/pdf/2305.13301" title="Download PDF">pdf</a>, <a href="/format/2305.13301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Diffusion Models with Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Black%2C+K">Kevin Black</a>, 
<a href="/search/cs?searchtype=author&query=Janner%2C+M">Michael Janner</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yilun Du</a>, 
<a href="/search/cs?searchtype=author&query=Kostrikov%2C+I">Ilya Kostrikov</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item980">[980]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13330" title="Abstract">arXiv:2305.13330</a> (replaced) [<a href="/pdf/2305.13330" title="Download PDF">pdf</a>, <a href="/format/2305.13330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised ASR via Cross-Lingual Pseudo-Labeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Likhomanenko%2C+T">Tatiana Likhomanenko</a>, 
<a href="/search/eess?searchtype=author&query=Lugosch%2C+L">Loren Lugosch</a>, 
<a href="/search/eess?searchtype=author&query=Collobert%2C+R">Ronan Collobert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item981">[981]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13495" title="Abstract">arXiv:2305.13495</a> (replaced) [<a href="/pdf/2305.13495" title="Download PDF">pdf</a>, <a href="/format/2305.13495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Type-to-Track: Retrieve Any Object via Prompt-based Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+P">Pha Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Quach%2C+K+G">Kha Gia Quach</a>, 
<a href="/search/cs?searchtype=author&query=Kitani%2C+K">Kris Kitani</a>, 
<a href="/search/cs?searchtype=author&query=Luu%2C+K">Khoa Luu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023. Project page: <a href="https://uark-cviu.github.io/Type-to-Track/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item982">[982]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14062" title="Abstract">arXiv:2305.14062</a> (replaced) [<a href="/pdf/2305.14062" title="Download PDF">pdf</a>, <a href="/format/2305.14062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Amplitude-Independent Machine Learning for PPG through Visibility Graphs  and Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Miao%2C+Y">Yuyang Miao</a>, 
<a href="/search/eess?searchtype=author&query=Davies%2C+H+J">Harry J. Davies</a>, 
<a href="/search/eess?searchtype=author&query=Mandic%2C+D+P">Danilo P. Mandic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item983">[983]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14279" title="Abstract">arXiv:2305.14279</a> (replaced) [<a href="/pdf/2305.14279" title="Download PDF">pdf</a>, <a href="/format/2305.14279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Angelica Chen</a>, 
<a href="/search/cs?searchtype=author&query=Phang%2C+J">Jason Phang</a>, 
<a href="/search/cs?searchtype=author&query=Parrish%2C+A">Alicia Parrish</a>, 
<a href="/search/cs?searchtype=author&query=Padmakumar%2C+V">Vishakh Padmakumar</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Bowman%2C+S+R">Samuel R. Bowman</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+K">Kyunghyun Cho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added GPT-4 results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item984">[984]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14428" title="Abstract">arXiv:2305.14428</a> (replaced) [<a href="/pdf/2305.14428" title="Download PDF">pdf</a>, <a href="/format/2305.14428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Language-Informed Distribution for Compositional Zero-Shot  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+W">Wentao Bao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lichang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Y">Yu Kong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item985">[985]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14567" title="Abstract">arXiv:2305.14567</a> (replaced) [<a href="/pdf/2305.14567" title="Download PDF">pdf</a>, <a href="/format/2305.14567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory Efficient Neural Processes via Constant Memory Attention Block
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Leo Feng</a>, 
<a href="/search/cs?searchtype=author&query=Tung%2C+F">Frederick Tung</a>, 
<a href="/search/cs?searchtype=author&query=Hajimirsadeghi%2C+H">Hossein Hajimirsadeghi</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+M+O">Mohamed Osama Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item986">[986]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14816" title="Abstract">arXiv:2305.14816</a> (replaced) [<a href="/pdf/2305.14816" title="Download PDF">pdf</a>, <a href="/ps/2305.14816" title="Download PostScript">ps</a>, <a href="/format/2305.14816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provable Offline Preference-Based Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wenhao Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Uehara%2C+M">Masatoshi Uehara</a>, 
<a href="/search/cs?searchtype=author&query=Kallus%2C+N">Nathan Kallus</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+D">Jason D. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wen Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contribute equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item987">[987]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15016" title="Abstract">arXiv:2305.15016</a> (replaced) [<a href="/pdf/2305.15016" title="Download PDF">pdf</a>, <a href="/format/2305.15016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Unsupervised Method for Estimating Class Separability of Datasets  with Application to LLMs Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghalyan%2C+N">Najah Ghalyan</a>, 
<a href="/search/cs?searchtype=author&query=Gourgoulias%2C+K">Kostis Gourgoulias</a>, 
<a href="/search/cs?searchtype=author&query=Satsangi%2C+Y">Yash Satsangi</a>, 
<a href="/search/cs?searchtype=author&query=Moran%2C+S">Sean Moran</a>, 
<a href="/search/cs?searchtype=author&query=Labonne%2C+M">Maxime Labonne</a>, 
<a href="/search/cs?searchtype=author&query=Sabelja%2C+J">Joseph Sabelja</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item988">[988]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15090" title="Abstract">arXiv:2305.15090</a> (replaced) [<a href="/pdf/2305.15090" title="Download PDF">pdf</a>, <a href="/format/2305.15090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STAR: Improving Low-Resource Information Extraction by Structure-to-Text  Data Generation with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+M+D">Mingyu Derek Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kung%2C+P">Po-Nien Kung</a>, 
<a href="/search/cs?searchtype=author&query=Brantingham%2C+P+J">P. Jeffrey Brantingham</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item989">[989]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15352" title="Abstract">arXiv:2305.15352</a> (replaced) [<a href="/pdf/2305.15352" title="Download PDF">pdf</a>, <a href="/format/2305.15352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Rates for Bandit Nonstochastic Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y+J">Y. Jennifer Sun</a>, 
<a href="/search/cs?searchtype=author&query=Newman%2C+S">Stephen Newman</a>, 
<a href="/search/cs?searchtype=author&query=Hazan%2C+E">Elad Hazan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item990">[990]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15523" title="Abstract">arXiv:2305.15523</a> (replaced) [<a href="/pdf/2305.15523" title="Download PDF">pdf</a>, <a href="/format/2305.15523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task-aware Distributed Source Coding under Dynamic Bandwidth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Po-han Li</a>, 
<a href="/search/cs?searchtype=author&query=Ankireddy%2C+S+K">Sravan Kumar Ankireddy</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Ruihan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Mahjoub%2C+H+N">Hossein Nourkhiz Mahjoub</a>, 
<a href="/search/cs?searchtype=author&query=Moradi-Pari%2C+E">Ehsan Moradi-Pari</a>, 
<a href="/search/cs?searchtype=author&query=Topcu%2C+U">Ufuk Topcu</a>, 
<a href="/search/cs?searchtype=author&query=Chinchali%2C+S">Sandeep Chinchali</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyeji Kim</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item991">[991]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15583" title="Abstract">arXiv:2305.15583</a> (replaced) [<a href="/pdf/2305.15583" title="Download PDF">pdf</a>, <a href="/format/2305.15583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alleviating Exposure Bias in Diffusion Models through Sampling with  Shifted Time Steps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingxiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+T">Tingyu Qu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+R">Ruicong Yao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Moens%2C+M">Marie-Francine Moens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item992">[992]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15611" title="Abstract">arXiv:2305.15611</a> (replaced) [<a href="/pdf/2305.15611" title="Download PDF">pdf</a>, <a href="/format/2305.15611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Size Generalization of Graph Neural Networks on Biological Data:  Insights and Practices from the Spectral Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yujun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gaotang Li</a>, 
<a href="/search/cs?searchtype=author&query=koutra%2C+D">Danai koutra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, including appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item993">[993]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15630" title="Abstract">arXiv:2305.15630</a> (replaced) [<a href="/pdf/2305.15630" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multicast and Unicast Superposition Transmission in MIMO OFDMA Systems  with Statistical CSIT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y+J+D">Yong Jin Daniel Kim</a>, 
<a href="/search/cs?searchtype=author&query=Vargas%2C+D">David Vargas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 10 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item994">[994]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15700" title="Abstract">arXiv:2305.15700</a> (replaced) [<a href="/pdf/2305.15700" title="Download PDF">pdf</a>, <a href="/format/2305.15700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness Continual Learning Approach to Semantic Scene Understanding in  Open-World Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Truong%2C+T">Thanh-Dat Truong</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H">Hoang-Quan Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>, 
<a href="/search/cs?searchtype=author&query=Luu%2C+K">Khoa Luu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item995">[995]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15798" title="Abstract">arXiv:2305.15798</a> (replaced) [<a href="/pdf/2305.15798" title="Download PDF">pdf</a>, <a href="/format/2305.15798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Architectural Compression of Text-to-Image Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Bo-Kyeong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H">Hyoung-Kyu Song</a>, 
<a href="/search/cs?searchtype=author&query=Castells%2C+T">Thibault Castells</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Shinkook Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated results: mobile inference, different training data volumes, and pruning sensitivity analysis; Short version: accepted to ICML Workshop on ES-FoMo (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item996">[996]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15852" title="Abstract">arXiv:2305.15852</a> (replaced) [<a href="/pdf/2305.15852" title="Download PDF">pdf</a>, <a href="/format/2305.15852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-contradictory Hallucinations of Large Language Models: Evaluation,  Detection and Mitigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCndler%2C+N">Niels M&#xfc;ndler</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jingxuan He</a>, 
<a href="/search/cs?searchtype=author&query=Jenko%2C+S">Slobodan Jenko</a>, 
<a href="/search/cs?searchtype=author&query=Vechev%2C+M">Martin Vechev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item997">[997]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16183" title="Abstract">arXiv:2305.16183</a> (replaced) [<a href="/pdf/2305.16183" title="Download PDF">pdf</a>, <a href="/format/2305.16183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Passive learning of active causal strategies in agents and language  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lampinen%2C+A+K">Andrew Kyle Lampinen</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+S+C+Y">Stephanie C Y Chan</a>, 
<a href="/search/cs?searchtype=author&query=Dasgupta%2C+I">Ishita Dasgupta</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+A+J">Andrew J Nam</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J+X">Jane X Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Advances in Neural Information Processing Systems (NeurIPS 2023). 10 pages main text
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item998">[998]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16209" title="Abstract">arXiv:2305.16209</a> (replaced) [<a href="/pdf/2305.16209" title="Download PDF">pdf</a>, <a href="/format/2305.16209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> C-MCTS: Safe Planning with Monte Carlo Tree Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parthasarathy%2C+D">Dinesh Parthasarathy</a>, 
<a href="/search/cs?searchtype=author&query=Kontes%2C+G">Georgios Kontes</a>, 
<a href="/search/cs?searchtype=author&query=Plinge%2C+A">Axel Plinge</a>, 
<a href="/search/cs?searchtype=author&query=Mutschler%2C+C">Christopher Mutschler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item999">[999]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16309" title="Abstract">arXiv:2305.16309</a> (replaced) [<a href="/pdf/2305.16309" title="Download PDF">pdf</a>, <a href="/format/2305.16309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Imitating Task and Motion Planning with Visuomotor Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dalal%2C+M">Murtaza Dalal</a>, 
<a href="/search/cs?searchtype=author&query=Mandlekar%2C+A">Ajay Mandlekar</a>, 
<a href="/search/cs?searchtype=author&query=Garrett%2C+C">Caelan Garrett</a>, 
<a href="/search/cs?searchtype=author&query=Handa%2C+A">Ankur Handa</a>, 
<a href="/search/cs?searchtype=author&query=Salakhutdinov%2C+R">Ruslan Salakhutdinov</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+D">Dieter Fox</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference on Robot Learning (CoRL) 2023. 8 pages, 5 figures, 2 tables; 11 pages appendix (10 additional figures)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1000">[1000]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16610" title="Abstract">arXiv:2305.16610</a> (replaced) [<a href="/pdf/2305.16610" title="Download PDF">pdf</a>, <a href="/format/2305.16610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Slingshot Perturbation to Learning in Monotone Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abe%2C+K">Kenshi Abe</a>, 
<a href="/search/cs?searchtype=author&query=Ariu%2C+K">Kaito Ariu</a>, 
<a href="/search/cs?searchtype=author&query=Sakamoto%2C+M">Mitsuki Sakamoto</a>, 
<a href="/search/cs?searchtype=author&query=Iwasaki%2C+A">Atsushi Iwasaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1001">[1001]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16694" title="Abstract">arXiv:2305.16694</a> (replaced) [<a href="/pdf/2305.16694" title="Download PDF">pdf</a>, <a href="/ps/2305.16694" title="Download PostScript">ps</a>, <a href="/format/2305.16694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reputation-based Persuasion Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arieli%2C+I">Itai Arieli</a>, 
<a href="/search/cs?searchtype=author&query=Madmon%2C+O">Omer Madmon</a>, 
<a href="/search/cs?searchtype=author&query=Tennenholtz%2C+M">Moshe Tennenholtz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item1002">[1002]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17102" title="Abstract">arXiv:2305.17102</a> (replaced) [<a href="/pdf/2305.17102" title="Download PDF">pdf</a>, <a href="/format/2305.17102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GeoVLN: Learning Geometry-Enhanced Visual Representation with Slot  Attention for Vision-and-Language Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huo%2C+J">Jingyang Huo</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+B">Boyan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haitao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yanwei Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CVPR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1003">[1003]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17343" title="Abstract">arXiv:2305.17343</a> (replaced) [<a href="/pdf/2305.17343" title="Download PDF">pdf</a>, <a href="/format/2305.17343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modality-Independent Teachers Meet Weakly-Supervised Audio-Visual Event  Parser
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yung-Hsuan Lai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yen-Chun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y+F">Yu-Chiang Frank Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1004">[1004]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17555" title="Abstract">arXiv:2305.17555</a> (replaced) [<a href="/pdf/2305.17555" title="Download PDF">pdf</a>, <a href="/format/2305.17555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffeomorphic Deformation via Sliced Wasserstein Distance Optimization  for Cortical Surface Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Tung Le</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Khai Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shanlin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kun Han</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+N">Nhat Ho</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xiaohui Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Update experimental results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1005">[1005]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17884" title="Abstract">arXiv:2305.17884</a> (replaced) [<a href="/pdf/2305.17884" title="Download PDF">pdf</a>, <a href="/format/2305.17884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Monte Carlo and Tensor-network Methods for Partial  Differential Equations via Sketching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+Y">Yian Chen</a>, 
<a href="/search/math?searchtype=author&query=Khoo%2C+Y">Yuehaw Khoo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1006">[1006]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18111" title="Abstract">arXiv:2305.18111</a> (replaced) [<a href="/pdf/2305.18111" title="Download PDF">pdf</a>, <a href="/format/2305.18111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The minimax risk in testing the histogram of discrete distributions for  uniformity under missing ball alternatives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kipnis%2C+A">Alon Kipnis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at the 59th annual Allerton conference on communication, control, and computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1007">[1007]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18270" title="Abstract">arXiv:2305.18270</a> (replaced) [<a href="/pdf/2305.18270" title="Download PDF">pdf</a>, <a href="/format/2305.18270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Two-Layer Neural Networks Learn, One (Giant) Step at a Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dandi%2C+Y">Yatin Dandi</a>, 
<a href="/search/stat?searchtype=author&query=Krzakala%2C+F">Florent Krzakala</a>, 
<a href="/search/stat?searchtype=author&query=Loureiro%2C+B">Bruno Loureiro</a>, 
<a href="/search/stat?searchtype=author&query=Pesce%2C+L">Luca Pesce</a>, 
<a href="/search/stat?searchtype=author&query=Stephan%2C+L">Ludovic Stephan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1008">[1008]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18436" title="Abstract">arXiv:2305.18436</a> (replaced) [<a href="/pdf/2305.18436" title="Download PDF">pdf</a>, <a href="/format/2305.18436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistically Optimal K-means Clustering via Nonnegative Low-rank  Semidefinite Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhuang%2C+Y">Yubo Zhuang</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+X">Xiaohui Chen</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+Y">Yun Yang</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+R+Y">Richard Y. Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1009">[1009]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18505" title="Abstract">arXiv:2305.18505</a> (replaced) [<a href="/pdf/2305.18505" title="Download PDF">pdf</a>, <a href="/ps/2305.18505" title="Download PostScript">ps</a>, <a href="/format/2305.18505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provable Reward-Agnostic Preference-Based Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wenhao Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Uehara%2C+M">Masatoshi Uehara</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+D">Jason D. Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1010">[1010]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19243" title="Abstract">arXiv:2305.19243</a> (replaced) [<a href="/pdf/2305.19243" title="Download PDF">pdf</a>, <a href="/format/2305.19243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking Tuning-free Generalization: Minimizing the PAC-Bayes Bound  with Trainable Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+X">Xitong Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Ghosh%2C+A">Avrajit Ghosh</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+G">Guangliang Liu</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+R">Rongrong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 15 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1011">[1011]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19358" title="Abstract">arXiv:2305.19358</a> (replaced) [<a href="/pdf/2305.19358" title="Download PDF">pdf</a>, <a href="/format/2305.19358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable Anisotropic Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rudman%2C+W">William Rudman</a>, 
<a href="/search/cs?searchtype=author&query=Eickhoff%2C+C">Carsten Eickhoff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is publicly available at: <a href="https://github.com/bcbi-edu/p_eickhoff_isoscore.git.">this https URL</a> Additionally, you can use IsoScore* using pip install IsoScore
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1012">[1012]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19947" title="Abstract">arXiv:2305.19947</a> (replaced) [<a href="/pdf/2305.19947" title="Download PDF">pdf</a>, <a href="/format/2305.19947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Geometric Perspective on Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Defang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhenyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+J">Jian-Ping Mei</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chunhua Shen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Can Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1013">[1013]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00561" title="Abstract">arXiv:2306.00561</a> (replaced) [<a href="/pdf/2306.00561" title="Download PDF">pdf</a>, <a href="/format/2306.00561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Masked Autoencoders with Multi-Window Local-Global Attention Are Better  Audio Learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yadav%2C+S">Sarthak Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Theodoridis%2C+S">Sergios Theodoridis</a>, 
<a href="/search/cs?searchtype=author&query=Hansen%2C+L+K">Lars Kai Hansen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zheng-Hua Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1014">[1014]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00788" title="Abstract">arXiv:2306.00788</a> (replaced) [<a href="/pdf/2306.00788" title="Download PDF">pdf</a>, <a href="/format/2306.00788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Augmentation-based Self-Supervised Representation Learning  via RKHS Approximation and Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+R">Runtian Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bingbin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Risteski%2C+A">Andrej Risteski</a>, 
<a href="/search/cs?searchtype=author&query=Kolter%2C+Z">Zico Kolter</a>, 
<a href="/search/cs?searchtype=author&query=Ravikumar%2C+P">Pradeep Ravikumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1015">[1015]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00952" title="Abstract">arXiv:2306.00952</a> (replaced) [<a href="/pdf/2306.00952" title="Download PDF">pdf</a>, <a href="/format/2306.00952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-Learning Framework for End-to-End Imposter Identification in Unseen  Speaker Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chaubey%2C+A">Ashutosh Chaubey</a>, 
<a href="/search/eess?searchtype=author&query=Sinha%2C+S">Sparsh Sinha</a>, 
<a href="/search/eess?searchtype=author&query=Ghose%2C+S">Susmita Ghose</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item1016">[1016]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01569" title="Abstract">arXiv:2306.01569</a> (replaced) [<a href="/pdf/2306.01569" title="Download PDF">pdf</a>, <a href="/format/2306.01569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FPIM: Field-Programmable Ising Machines for Solving SAT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jagielski%2C+T">Thomas Jagielski</a> (1), 
<a href="/search/cs?searchtype=author&query=Manohar%2C+R">Rajit Manohar</a> (1), 
<a href="/search/cs?searchtype=author&query=Roychowdhury%2C+J">Jaijeet Roychowdhury</a> (2) ((1) Yale University, (2) University of California, Berkeley.)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>

</div>
</div>
</dd>
<dt><a name="item1017">[1017]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01631" title="Abstract">arXiv:2306.01631</a> (replaced) [<a href="/pdf/2306.01631" title="Download PDF">pdf</a>, <a href="/format/2306.01631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bi-level Contrastive Learning for Knowledge-Enhanced Molecule  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+P">Pengcheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Cao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Tianfan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jimeng Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item1018">[1018]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01731" title="Abstract">arXiv:2306.01731</a> (replaced) [<a href="/pdf/2306.01731" title="Download PDF">pdf</a>, <a href="/format/2306.01731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAGAR: Taming Reward Misalignment in Inverse Reinforcement  Learning-Based Imitation Learning with Protagonist Antagonist Guided  Adversarial Reward
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Weichao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenchao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1019">[1019]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02210" title="Abstract">arXiv:2306.02210</a> (replaced) [<a href="/pdf/2306.02210" title="Download PDF">pdf</a>, <a href="/format/2306.02210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-FL: Generative Pre-trained Model-Assisted Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Tiantian Feng</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+S">Samiul Alam</a>, 
<a href="/search/cs?searchtype=author&query=Dimitriadis%2C+D">Dimitrios Dimitriadis</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+S+S">Shrikanth S. Narayanan</a>, 
<a href="/search/cs?searchtype=author&query=Avestimehr%2C+S">Salman Avestimehr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1020">[1020]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03242" title="Abstract">arXiv:2306.03242</a> (replaced) [<a href="/pdf/2306.03242" title="Download PDF">pdf</a>, <a href="/format/2306.03242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Systems Architecture for Quantum Random Access Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Xu%2C+S">Shifan Xu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hann%2C+C+T">Connor T. Hann</a>, 
<a href="/search/quant-ph?searchtype=author&query=Foxman%2C+B">Ben Foxman</a>, 
<a href="/search/quant-ph?searchtype=author&query=Girvin%2C+S+M">Steven M. Girvin</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ding%2C+Y">Yongshan Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item1021">[1021]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04344" title="Abstract">arXiv:2306.04344</a> (replaced) [<a href="/pdf/2306.04344" title="Download PDF">pdf</a>, <a href="/format/2306.04344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViDA: Homeostatic Visual Domain Adapter for Continual Test Time  Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Senqiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+P">Peidong Jia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Renrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+M">Ming Lu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yandong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+W">Wei Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shanghang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neurips2023 final Rating: Weak Accept; Weak Accept; Borderline accept; Borderline accept
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1022">[1022]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04987" title="Abstract">arXiv:2306.04987</a> (replaced) [<a href="/pdf/2306.04987" title="Download PDF">pdf</a>, <a href="/format/2306.04987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convolutional Recurrent Neural Network with Attention for 3D Speech  Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yin%2C+H">Han Yin</a>, 
<a href="/search/eess?searchtype=author&query=Bai%2C+J">Jisheng Bai</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+M">Mou Wang</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+S">Siwei Huang</a>, 
<a href="/search/eess?searchtype=author&query=Jia%2C+Y">Yafei Jia</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+J">Jianfeng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages,5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item1023">[1023]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05032" title="Abstract">arXiv:2306.05032</a> (replaced) [<a href="/pdf/2306.05032" title="Download PDF">pdf</a>, <a href="/format/2306.05032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Log-based Anomaly Detection based on EVT Theory with feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Junjie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+Y">Yintong Huo</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhihan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiazhen Gu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuangbin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Cong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Minzhi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+M+R">Michael R. Lyu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1024">[1024]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05079" title="Abstract">arXiv:2306.05079</a> (replaced) [<a href="/pdf/2306.05079" title="Download PDF">pdf</a>, <a href="/format/2306.05079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Robustness of AI Offensive Code Generators via Data  Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Improta%2C+C">Cristina Improta</a>, 
<a href="/search/cs?searchtype=author&query=Liguori%2C+P">Pietro Liguori</a>, 
<a href="/search/cs?searchtype=author&query=Natella%2C+R">Roberto Natella</a>, 
<a href="/search/cs?searchtype=author&query=Cukic%2C+B">Bojan Cukic</a>, 
<a href="/search/cs?searchtype=author&query=Cotroneo%2C+D">Domenico Cotroneo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item1025">[1025]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06040" title="Abstract">arXiv:2306.06040</a> (replaced) [<a href="/pdf/2306.06040" title="Download PDF">pdf</a>, <a href="/format/2306.06040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstructing Human Expressiveness in Piano Performances with a  Transformer Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jingjing Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wiggins%2C+G">Geraint Wiggins</a>, 
<a href="/search/cs?searchtype=author&query=Fazekas%2C+G">Gyorgy Fazekas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures, accepted by CMMR2023, the 16th International Symposium on Computer Music Multidisciplinary Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1026">[1026]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06093" title="Abstract">arXiv:2306.06093</a> (replaced) [<a href="/pdf/2306.06093" title="Download PDF">pdf</a>, <a href="/format/2306.06093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sen%2C+B">Bipasha Sen</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+G">Gaurav Singh</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Aditya Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Agaram%2C+R">Rohith Agaram</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+K+M">K Madhava Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Sridhar%2C+S">Srinath Sridhar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1027">[1027]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06479" title="Abstract">arXiv:2306.06479</a> (replaced) [<a href="/pdf/2306.06479" title="Download PDF">pdf</a>, <a href="/format/2306.06479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning a Neuron by a Shallow ReLU Network: Dynamics and Implicit Bias  for Correlated Inputs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chistikov%2C+D">Dmitry Chistikov</a>, 
<a href="/search/cs?searchtype=author&query=Englert%2C+M">Matthias Englert</a>, 
<a href="/search/cs?searchtype=author&query=Lazic%2C+R">Ranko Lazic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1028">[1028]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06528" title="Abstract">arXiv:2306.06528</a> (replaced) [<a href="/pdf/2306.06528" title="Download PDF">pdf</a>, <a href="/format/2306.06528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Push: Concurrent Probabilistic Programming for Bayesian Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Daniel Huang</a>, 
<a href="/search/cs?searchtype=author&query=Cama%C3%B1o%2C+C">Chris Cama&#xf1;o</a>, 
<a href="/search/cs?searchtype=author&query=Tsegaye%2C+J">Jonathan Tsegaye</a>, 
<a href="/search/cs?searchtype=author&query=Gale%2C+J+A">Jonathan Austin Gale</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item1029">[1029]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06599" title="Abstract">arXiv:2306.06599</a> (replaced) [<a href="/pdf/2306.06599" title="Download PDF">pdf</a>, <a href="/format/2306.06599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Imbalanced Regression: Fair Uncertainty Quantification via  Probabilistic Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1030">[1030]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06842" title="Abstract">arXiv:2306.06842</a> (replaced) [<a href="/pdf/2306.06842" title="Download PDF">pdf</a>, <a href="/format/2306.06842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AerialFormer: Multi-resolution Transformer for Aerial Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamazaki%2C+K">Kashu Yamazaki</a>, 
<a href="/search/cs?searchtype=author&query=Hanyu%2C+T">Taisei Hanyu</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+M">Minh Tran</a>, 
<a href="/search/cs?searchtype=author&query=de+Luis%2C+A">Adrian de Luis</a>, 
<a href="/search/cs?searchtype=author&query=McCann%2C+R">Roy McCann</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+H">Haitao Liao</a>, 
<a href="/search/cs?searchtype=author&query=Rainwater%2C+C">Chase Rainwater</a>, 
<a href="/search/cs?searchtype=author&query=Adkins%2C+M">Meredith Adkins</a>, 
<a href="/search/cs?searchtype=author&query=Cothren%2C+J">Jackson Cothren</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+N">Ngan Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1031">[1031]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07570" title="Abstract">arXiv:2306.07570</a> (replaced) [<a href="/pdf/2306.07570" title="Download PDF">pdf</a>, <a href="/format/2306.07570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-intrusive reduced order models for partitioned fluid-structure  interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tiba%2C+A">Azzeddine Tiba</a> (M2N), 
<a href="/search/cs?searchtype=author&query=Dairay%2C+T">Thibault Dairay</a> (M.F.P. Michelin), 
<a href="/search/cs?searchtype=author&query=de+Vuyst%2C+F">Florian de Vuyst</a> (LMAC), 
<a href="/search/cs?searchtype=author&query=Mortazavi%2C+I">Iraj Mortazavi</a> (M2N), 
<a href="/search/cs?searchtype=author&query=Ramirez%2C+J+B">Juan-Pedro Berro Ramirez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item1032">[1032]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07856" title="Abstract">arXiv:2306.07856</a> (replaced) [<a href="/pdf/2306.07856" title="Download PDF">pdf</a>, <a href="/format/2306.07856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DreamDecompiler: Bayesian Program Learning by Decompiling Amortised  Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Palmarini%2C+A+B">Alessandro B. Palmarini</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+C+G">Christopher G. Lucas</a>, 
<a href="/search/cs?searchtype=author&query=Siddharth%2C+N">N. Siddharth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item1033">[1033]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07879" title="Abstract">arXiv:2306.07879</a> (replaced) [<a href="/pdf/2306.07879" title="Download PDF">pdf</a>, <a href="/format/2306.07879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking pose estimation in crowds: overcoming the detection  information-bottleneck and ambiguity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Stoffl%2C+L">Lucas Stoffl</a>, 
<a href="/search/cs?searchtype=author&query=Mathis%2C+M+W">Mackenzie Weygandt Mathis</a>, 
<a href="/search/cs?searchtype=author&query=Mathis%2C+A">Alexander Mathis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at ICCV 2023; Code at <a href="https://github.com/amathislab/BUCTD">this https URL</a> Video at <a href="https://www.youtube.com/watch?v=BHZnA-CZeZY">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item1034">[1034]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08018" title="Abstract">arXiv:2306.08018</a> (replaced) [<a href="/pdf/2306.08018" title="Download PDF">pdf</a>, <a href="/format/2306.08018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Fang%2C+Y">Yin Fang</a>, 
<a href="/search/q-bio?searchtype=author&query=Liang%2C+X">Xiaozhuan Liang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+K">Kangwei Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Huang%2C+R">Rui Huang</a>, 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/q-bio?searchtype=author&query=Fan%2C+X">Xiaohui Fan</a>, 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project homepage: <a href="https://github.com/zjunlp/Mol-Instructions">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1035">[1035]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08103" title="Abstract">arXiv:2306.08103</a> (replaced) [<a href="/pdf/2306.08103" title="Download PDF">pdf</a>, <a href="/format/2306.08103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adding 3D Geometry Control to Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wufei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qihao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiahao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xiaoding Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Angtian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zihao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guofeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+B">Beijia Lu</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+R">Ruxiao Duan</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yongrui Qi</a>, 
<a href="/search/cs?searchtype=author&query=Kortylewski%2C+A">Adam Kortylewski</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yaoyao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuille%2C+A">Alan Yuille</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1036">[1036]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08243" title="Abstract">arXiv:2306.08243</a> (replaced) [<a href="/pdf/2306.08243" title="Download PDF">pdf</a>, <a href="/format/2306.08243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MMASD: A Multimodal Dataset for Autism Intervention Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jicheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Chheang%2C+V">Vuthea Chheang</a>, 
<a href="/search/cs?searchtype=author&query=Kullu%2C+P">Pinar Kullu</a>, 
<a href="/search/cs?searchtype=author&query=Brignac%2C+E">Eli Brignac</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Barner%2C+K+E">Kenneth E. Barner</a>, 
<a href="/search/cs?searchtype=author&query=Bhat%2C+A">Anjana Bhat</a>, 
<a href="/search/cs?searchtype=author&query=Barmaki%2C+R+L">Roghayeh Leila Barmaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1037">[1037]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08553" title="Abstract">arXiv:2306.08553</a> (replaced) [<a href="/pdf/2306.08553" title="Download PDF">pdf</a>, <a href="/format/2306.08553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Noise Stability Optimization for Flat Minima with Tight Rates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ju%2C+H">Haotian Ju</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H+R">Hongyang R. Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1038">[1038]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08889" title="Abstract">arXiv:2306.08889</a> (replaced) [<a href="/pdf/2306.08889" title="Download PDF">pdf</a>, <a href="/format/2306.08889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revealing the Illusion of Joint Multimodal Understanding in VideoQA  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rawal%2C+I+S">Ishaan Singh Rawal</a>, 
<a href="/search/cs?searchtype=author&query=Jaiswal%2C+S">Shantanu Jaiswal</a>, 
<a href="/search/cs?searchtype=author&query=Fernando%2C+B">Basura Fernando</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Cheston Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1039">[1039]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09001" title="Abstract">arXiv:2306.09001</a> (replaced) [<a href="/pdf/2306.09001" title="Download PDF">pdf</a>, <a href="/format/2306.09001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SSCBench: Monocular 3D Semantic Scene Completion Benchmark in Street  Views
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiming Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sihang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinhao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+M">Moonjun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kenan Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N">Nuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zijun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+T">Tao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fisher Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhiding Yu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chen Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1040">[1040]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09251" title="Abstract">arXiv:2306.09251</a> (replaced) [<a href="/pdf/2306.09251" title="Download PDF">pdf</a>, <a href="/ps/2306.09251" title="Download PostScript">ps</a>, <a href="/format/2306.09251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+G">Gen Li</a>, 
<a href="/search/stat?searchtype=author&query=Wei%2C+Y">Yuting Wei</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+Y">Yuxin Chen</a>, 
<a href="/search/stat?searchtype=author&query=Chi%2C+Y">Yuejie Chi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Score estimation errors are included in the convergence theory in the new version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item1041">[1041]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10060" title="Abstract">arXiv:2306.10060</a> (replaced) [<a href="/pdf/2306.10060" title="Download PDF">pdf</a>, <a href="/format/2306.10060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MUBen: Benchmarking the Uncertainty of Molecular Representation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Li%2C+Y">Yinghao Li</a>, 
<a href="/search/physics?searchtype=author&query=Kong%2C+L">Lingkai Kong</a>, 
<a href="/search/physics?searchtype=author&query=Du%2C+Y">Yuanqi Du</a>, 
<a href="/search/physics?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/physics?searchtype=author&query=Zhuang%2C+Y">Yuchen Zhuang</a>, 
<a href="/search/physics?searchtype=author&query=Mu%2C+W">Wenhao Mu</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1042">[1042]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10080" title="Abstract">arXiv:2306.10080</a> (replaced) [<a href="/pdf/2306.10080" title="Download PDF">pdf</a>, <a href="/ps/2306.10080" title="Download PostScript">ps</a>, <a href="/format/2306.10080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Driven Near Real-time Locational Marginal Pricing Method: A  Feasibility and Robustness Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jami%2C+N+V+S+J">Naga Venkata Sai Jitin Jami</a>, 
<a href="/search/cs?searchtype=author&query=Kardo%C5%A1%2C+J">Juraj Kardo&#x161;</a>, 
<a href="/search/cs?searchtype=author&query=Schenk%2C+O">Olaf Schenk</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6stler%2C+H">Harald K&#xf6;stler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1043">[1043]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10856" title="Abstract">arXiv:2306.10856</a> (replaced) [<a href="/pdf/2306.10856" title="Download PDF">pdf</a>, <a href="/format/2306.10856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Effects of Permanent Faults in GPU&#x27;s Parallelism  Management and Control Units
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guerrero-Balaguera%2C+J">Juan-David Guerrero-Balaguera</a> (DAUIN), 
<a href="/search/cs?searchtype=author&query=Condia%2C+J+E+R">Josie E. Rodriguez Condia</a> (DAUIN), 
<a href="/search/cs?searchtype=author&query=Santos%2C+F+F+d">Fernando F. dos Santos</a> (TARAN), 
<a href="/search/cs?searchtype=author&query=Sonza%2C+M">Matteo Sonza</a> (DAUIN), 
<a href="/search/cs?searchtype=author&query=Rech%2C+P">Paolo Rech</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ACM/IEEE International Conference for High Performance Computing,
  Networking, Storage, and Analysis, ACM; IEEE, Nov 2023, Denver, United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item1044">[1044]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11180" title="Abstract">arXiv:2306.11180</a> (replaced) [<a href="/pdf/2306.11180" title="Download PDF">pdf</a>, <a href="/format/2306.11180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyperbolic Active Learning for Semantic Segmentation under Domain Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Franco%2C+L">Luca Franco</a>, 
<a href="/search/cs?searchtype=author&query=Mandica%2C+P">Paolo Mandica</a>, 
<a href="/search/cs?searchtype=author&query=Kallidromitis%2C+K">Konstantinos Kallidromitis</a>, 
<a href="/search/cs?searchtype=author&query=Guillory%2C+D">Devin Guillory</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yu-Teng Li</a>, 
<a href="/search/cs?searchtype=author&query=Darrell%2C+T">Trevor Darrell</a>, 
<a href="/search/cs?searchtype=author&query=Galasso%2C+F">Fabio Galasso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1045">[1045]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11295" title="Abstract">arXiv:2306.11295</a> (replaced) [<a href="/pdf/2306.11295" title="Download PDF">pdf</a>, <a href="/ps/2306.11295" title="Download PostScript">ps</a>, <a href="/format/2306.11295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bounds on the genus for 2-cell embeddings of prefix-reversal graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Blanco%2C+S+A">Sa&#xfa;l A. Blanco</a>, 
<a href="/search/math?searchtype=author&query=Buehrle%2C+C">Charles Buehrle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fixed some typos and improved some exposition. 21 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item1046">[1046]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11644" title="Abstract">arXiv:2306.11644</a> (replaced) [<a href="/pdf/2306.11644" title="Download PDF">pdf</a>, <a href="/format/2306.11644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Textbooks Are All You Need
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gunasekar%2C+S">Suriya Gunasekar</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Aneja%2C+J">Jyoti Aneja</a>, 
<a href="/search/cs?searchtype=author&query=Mendes%2C+C+C+T">Caio C&#xe9;sar Teodoro Mendes</a>, 
<a href="/search/cs?searchtype=author&query=Del+Giorno%2C+A">Allie Del Giorno</a>, 
<a href="/search/cs?searchtype=author&query=Gopi%2C+S">Sivakanth Gopi</a>, 
<a href="/search/cs?searchtype=author&query=Javaheripi%2C+M">Mojan Javaheripi</a>, 
<a href="/search/cs?searchtype=author&query=Kauffmann%2C+P">Piero Kauffmann</a>, 
<a href="/search/cs?searchtype=author&query=de+Rosa%2C+G">Gustavo de Rosa</a>, 
<a href="/search/cs?searchtype=author&query=Saarikivi%2C+O">Olli Saarikivi</a>, 
<a href="/search/cs?searchtype=author&query=Salim%2C+A">Adil Salim</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+S">Shital Shah</a>, 
<a href="/search/cs?searchtype=author&query=Behl%2C+H+S">Harkirat Singh Behl</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bubeck%2C+S">S&#xe9;bastien Bubeck</a>, 
<a href="/search/cs?searchtype=author&query=Eldan%2C+R">Ronen Eldan</a>, 
<a href="/search/cs?searchtype=author&query=Kalai%2C+A+T">Adam Tauman Kalai</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y+T">Yin Tat Lee</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanzhi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages; changed color scheme of plot. fixed minor typos and added couple clarifications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1047">[1047]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11886" title="Abstract">arXiv:2306.11886</a> (replaced) [<a href="/pdf/2306.11886" title="Download PDF">pdf</a>, <a href="/format/2306.11886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jesse Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pertsch%2C+K">Karl Pertsch</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiahui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+J+J">Joseph J. Lim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1048">[1048]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13050" title="Abstract">arXiv:2306.13050</a> (replaced) [<a href="/pdf/2306.13050" title="Download PDF">pdf</a>, <a href="/format/2306.13050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data augmentation and refinement for recommender system: A  semi-supervised approach using maximum margin matrix factorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaikh%2C+S">Shamal Shaikh</a>, 
<a href="/search/cs?searchtype=author&query=Kagita%2C+V+R">Venkateswara Rao Kagita</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V">Vikas Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Pujari%2C+A+K">Arun K Pujari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1049">[1049]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13575" title="Abstract">arXiv:2306.13575</a> (replaced) [<a href="/pdf/2306.13575" title="Download PDF">pdf</a>, <a href="/format/2306.13575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling MLPs: A Tale of Inductive Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bachmann%2C+G">Gregor Bachmann</a>, 
<a href="/search/cs?searchtype=author&query=Anagnostidis%2C+S">Sotiris Anagnostidis</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+T">Thomas Hofmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1050">[1050]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13956" title="Abstract">arXiv:2306.13956</a> (replaced) [<a href="/pdf/2306.13956" title="Download PDF">pdf</a>, <a href="/format/2306.13956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pointwise-in-Time Explanation for Linear Temporal Logic Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brindise%2C+N">Noel Brindise</a>, 
<a href="/search/cs?searchtype=author&query=Langbort%2C+C">Cedric Langbort</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> See related publication in Conference on Decision and Control (CDC) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1051">[1051]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14975" title="Abstract">arXiv:2306.14975</a> (replaced) [<a href="/pdf/2306.14975" title="Download PDF">pdf</a>, <a href="/format/2306.14975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Underlying Scaling Laws and Universal Statistical Structure of  Complex Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levi%2C+N">Noam Levi</a>, 
<a href="/search/cs?searchtype=author&query=Oz%2C+Y">Yaron Oz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); High Energy Physics - Theory (hep-th); Probability (math.PR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1052">[1052]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15110" title="Abstract">arXiv:2306.15110</a> (replaced) [<a href="/pdf/2306.15110" title="Download PDF">pdf</a>, <a href="/format/2306.15110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Transfer Learning for Intelligent Vehicle Perception: a Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huiming Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhigang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hongkai Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated some figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1053">[1053]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15203" title="Abstract">arXiv:2306.15203</a> (replaced) [<a href="/pdf/2306.15203" title="Download PDF">pdf</a>, <a href="/format/2306.15203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Polychromatic Neural Representation for CT Metal Artifact  Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+Q">Qing Wu</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+L">Lixuan Chen</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+C">Ce Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wei%2C+H">Hongjiang Wei</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+S+K">S. Kevin Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+J">Jingyi Yu</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yuyao Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1054">[1054]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16605" title="Abstract">arXiv:2306.16605</a> (replaced) [<a href="/pdf/2306.16605" title="Download PDF">pdf</a>, <a href="/format/2306.16605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KITE: Keypoint-Conditioned Policies for Semantic Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sundaresan%2C+P">Priya Sundaresan</a>, 
<a href="/search/cs?searchtype=author&query=Belkhale%2C+S">Suneel Belkhale</a>, 
<a href="/search/cs?searchtype=author&query=Sadigh%2C+D">Dorsa Sadigh</a>, 
<a href="/search/cs?searchtype=author&query=Bohg%2C+J">Jeannette Bohg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1055">[1055]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00301" title="Abstract">arXiv:2307.00301</a> (replaced) [<a href="/pdf/2307.00301" title="Download PDF">pdf</a>, <a href="/ps/2307.00301" title="Download PostScript">ps</a>, <a href="/format/2307.00301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Words for the Graphs with Permutation-Representation Number at most  Three
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mozhui%2C+K">Khyodeno Mozhui</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+K+V">K. V. Krishna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item1056">[1056]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00329" title="Abstract">arXiv:2307.00329</a> (replaced) [<a href="/pdf/2307.00329" title="Download PDF">pdf</a>, <a href="/format/2307.00329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DoReMi: Grounding Language Model by Detecting and Recovering from  Plan-Execution Misalignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yanjiang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yen-Jen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+L">Lihan Zha</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zheyuan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianyu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1057">[1057]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00433" title="Abstract">arXiv:2307.00433</a> (replaced) [<a href="/pdf/2307.00433" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Traffic Control with Smart Speed Bumps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mokhtari%2C+M">Melvin Mokhtari</a>, 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+A">Amirreza Hosseini</a>, 
<a href="/search/cs?searchtype=author&query=Habibi%2C+A">Alireza Habibi</a>, 
<a href="/search/cs?searchtype=author&query=Karshenas%2C+A">Adel Karshenas</a>, 
<a href="/search/cs?searchtype=author&query=Amoomahdi%2C+A">Ali Amoomahdi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1058">[1058]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00997" title="Abstract">arXiv:2307.00997</a> (replaced) [<a href="/pdf/2307.00997" title="Download PDF">pdf</a>, <a href="/format/2307.00997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RefSAM: Efficiently Adapting Segmenting Anything Model for Referring  Video Object Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yonglin Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+X">Xiao Teng</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+L">Long Lan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The code and models will be made publicly at <a href="https://github.com/LancasterLi/RefSAM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1059">[1059]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01597" title="Abstract">arXiv:2307.01597</a> (replaced) [<a href="/pdf/2307.01597" title="Download PDF">pdf</a>, <a href="/format/2307.01597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking the Potential of Deep Learning in Peak-Hour Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jingyuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Heling Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuantao Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be published in CIKM'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1060">[1060]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02037" title="Abstract">arXiv:2307.02037</a> (replaced) [<a href="/pdf/2307.02037" title="Download PDF">pdf</a>, <a href="/format/2307.02037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reverse Diffusion Monte Carlo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Huang%2C+X">Xunpeng Huang</a>, 
<a href="/search/stat?searchtype=author&query=Dong%2C+H">Hanze Dong</a>, 
<a href="/search/stat?searchtype=author&query=Hao%2C+Y">Yifan Hao</a>, 
<a href="/search/stat?searchtype=author&query=Ma%2C+Y">Yian Ma</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1061">[1061]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02231" title="Abstract">arXiv:2307.02231</a> (replaced) [<a href="/pdf/2307.02231" title="Download PDF">pdf</a>, <a href="/format/2307.02231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tit-for-Token: Understanding Fairness when Forwarding Data by  Incentivized Peers in Decentralized Storage Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lakhani%2C+V+H">Vahid Heidaripour Lakhani</a>, 
<a href="/search/cs?searchtype=author&query=Babaei%2C+A">Arman Babaei</a>, 
<a href="/search/cs?searchtype=author&query=Jehl%2C+L">Leander Jehl</a>, 
<a href="/search/cs?searchtype=author&query=Ishmaev%2C+G">Georgy Ishmaev</a>, 
<a href="/search/cs?searchtype=author&query=Estrada-Gali%C3%B1anes%2C+V">Vero Estrada-Gali&#xf1;anes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1062">[1062]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03354" title="Abstract">arXiv:2307.03354</a> (replaced) [<a href="/pdf/2307.03354" title="Download PDF">pdf</a>, <a href="/format/2307.03354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Token-Level Serialized Output Training for Joint Streaming ASR and ST  Leveraging Textual Alignments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papi%2C+S">Sara Papi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peidong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junkun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jian Xue</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Gaur%2C+Y">Yashesh Gaur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1063">[1063]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03533" title="Abstract">arXiv:2307.03533</a> (replaced) [<a href="/pdf/2307.03533" title="Download PDF">pdf</a>, <a href="/ps/2307.03533" title="Download PostScript">ps</a>, <a href="/format/2307.03533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The CHiME-7 UDASE task: Unsupervised domain adaptation for  conversational speech enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leglaive%2C+S">Simon Leglaive</a>, 
<a href="/search/cs?searchtype=author&query=Borne%2C+L">L&#xe9;onie Borne</a>, 
<a href="/search/cs?searchtype=author&query=Tzinis%2C+E">Efthymios Tzinis</a>, 
<a href="/search/cs?searchtype=author&query=Sadeghi%2C+M">Mostafa Sadeghi</a>, 
<a href="/search/cs?searchtype=author&query=Fraticelli%2C+M">Matthieu Fraticelli</a>, 
<a href="/search/cs?searchtype=author&query=Wisdom%2C+S">Scott Wisdom</a>, 
<a href="/search/cs?searchtype=author&query=Pariente%2C+M">Manuel Pariente</a>, 
<a href="/search/cs?searchtype=author&query=Pressnitzer%2C+D">Daniel Pressnitzer</a>, 
<a href="/search/cs?searchtype=author&query=Hershey%2C+J+R">John R. Hershey</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 7th International Workshop on Speech Processing in Everyday
  Environments (CHiME), Dublin, Ireland, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1064">[1064]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03723" title="Abstract">arXiv:2307.03723</a> (replaced) [<a href="/pdf/2307.03723" title="Download PDF">pdf</a>, <a href="/format/2307.03723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Steel Surface Roughness Parameter Calculations Using Lasers and Machine  Learning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Milne%2C+A">Alex Milne</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xianghua Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1065">[1065]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03917" title="Abstract">arXiv:2307.03917</a> (replaced) [<a href="/pdf/2307.03917" title="Download PDF">pdf</a>, <a href="/format/2307.03917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On decoder-only architecture for speech-to-text and large language model  integration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/eess?searchtype=author&query=Gaur%2C+Y">Yashesh Gaur</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+L">Long Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+Y">Yimeng Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+T">Tianrui Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jinyu Li</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+S">Shujie Liu</a>, 
<a href="/search/eess?searchtype=author&query=Ren%2C+B">Bo Ren</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+L">Linquan Liu</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Y">Yu Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item1066">[1066]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03980" title="Abstract">arXiv:2307.03980</a> (replaced) [<a href="/e-print/2307.03980" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building and Road Segmentation Using EffUNet and Transfer Learning  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gangurde%2C+S">Sahil Gangurde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The transformer network analysis was not included in the current paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item1067">[1067]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04081" title="Abstract">arXiv:2307.04081</a> (replaced) [<a href="/pdf/2307.04081" title="Download PDF">pdf</a>, <a href="/format/2307.04081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Score-based Conditional Generation with Fewer Labeled Data by  Self-calibrating Classifier Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+P+K">Paul Kuo-Ming Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Si-An Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hsuan-Tien Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1068">[1068]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04536" title="Abstract">arXiv:2307.04536</a> (replaced) [<a href="/pdf/2307.04536" title="Download PDF">pdf</a>, <a href="/format/2307.04536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DADO -- Low-Cost Query Strategies for Deep Active Design Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Decke%2C+J">Jens Decke</a>, 
<a href="/search/cs?searchtype=author&query=Gruhl%2C+C">Christian Gruhl</a>, 
<a href="/search/cs?searchtype=author&query=Rauch%2C+L">Lukas Rauch</a>, 
<a href="/search/cs?searchtype=author&query=Sick%2C+B">Bernhard Sick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item1069">[1069]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05831" title="Abstract">arXiv:2307.05831</a> (replaced) [<a href="/pdf/2307.05831" title="Download PDF">pdf</a>, <a href="/format/2307.05831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memorization Through the Lens of Curvature of Loss Function Around  Samples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garg%2C+I">Isha Garg</a>, 
<a href="/search/cs?searchtype=author&query=Ravikumar%2C+D">Deepak Ravikumar</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+K">Kaushik Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1070">[1070]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06930" title="Abstract">arXiv:2307.06930</a> (replaced) [<a href="/pdf/2307.06930" title="Download PDF">pdf</a>, <a href="/format/2307.06930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geigle%2C+G">Gregor Geigle</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+A">Abhay Jain</a>, 
<a href="/search/cs?searchtype=author&query=Timofte%2C+R">Radu Timofte</a>, 
<a href="/search/cs?searchtype=author&query=Glava%C5%A1%2C+G">Goran Glava&#x161;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1071">[1071]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07321" title="Abstract">arXiv:2307.07321</a> (replaced) [<a href="/pdf/2307.07321" title="Download PDF">pdf</a>, <a href="/format/2307.07321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NS4AR: A new, focused on sampling areas sampling method in graphical  recommendation Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiangqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Aishan%2C+D">Dilinuer Aishan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> None
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item1072">[1072]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07697" title="Abstract">arXiv:2307.07697</a> (replaced) [<a href="/pdf/2307.07697" title="Download PDF">pdf</a>, <a href="/format/2307.07697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Think-on-Graph: Deep and Responsible Reasoning of Large Language Model  on Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiashuo Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chengjin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+L">Lumingyuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Saizhuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yeyun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+L+M">Lionel M. Ni</a>, 
<a href="/search/cs?searchtype=author&query=Shum%2C+H">Heung-Yeung Shum</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jian Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 13 figures, 20 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1073">[1073]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08114" title="Abstract">arXiv:2307.08114</a> (replaced) [<a href="/pdf/2307.08114" title="Download PDF">pdf</a>, <a href="/format/2307.08114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tangent Model Composition for Ensembling and Continual Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T+Y">Tian Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Soatto%2C+S">Stefano Soatto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at International Conference on Computer Vision (ICCV) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1074">[1074]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08442" title="Abstract">arXiv:2307.08442</a> (replaced) [<a href="/pdf/2307.08442" title="Download PDF">pdf</a>, <a href="/ps/2307.08442" title="Download PostScript">ps</a>, <a href="/format/2307.08442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Algorithms for Energy Games in Special Cases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Forster%2C+S">Sebastian Forster</a> (University of Salzburg), 
<a href="/search/cs?searchtype=author&query=Skarlatos%2C+A">Antonis Skarlatos</a> (University of Salzburg), 
<a href="/search/cs?searchtype=author&query=de+Vos%2C+T">Tijn de Vos</a> (University of Salzburg)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings GandALF 2023, <a href="/abs/2309.17318">arXiv:2309.17318</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 390, 2023, pp. 236-252
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item1075">[1075]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08701" title="Abstract">arXiv:2307.08701</a> (replaced) [<a href="/pdf/2307.08701" title="Download PDF">pdf</a>, <a href="/format/2307.08701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AlpaGasus: Training A Better Alpaca with Fewer Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lichang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shiyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gunaratna%2C+K">Kalpa Gunaratna</a>, 
<a href="/search/cs?searchtype=author&query=Yadav%2C+V">Vikas Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zheng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+V">Vijay Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Hongxia Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 Pages; 29 Figures; 15 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1076">[1076]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09882" title="Abstract">arXiv:2307.09882</a> (replaced) [<a href="/pdf/2307.09882" title="Download PDF">pdf</a>, <a href="/format/2307.09882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Likelihood Estimation With One-Way Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ben-Dov%2C+O">Omri Ben-Dov</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+P+S">Pravir Singh Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Abrevaya%2C+V">Victoria Abrevaya</a>, 
<a href="/search/cs?searchtype=author&query=Black%2C+M+J">Michael J. Black</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+P">Partha Ghosh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1077">[1077]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09912" title="Abstract">arXiv:2307.09912</a> (replaced) [<a href="/pdf/2307.09912" title="Download PDF">pdf</a>, <a href="/format/2307.09912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning invariant representations of time-homogeneous stochastic  dynamical systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kostic%2C+V+R">Vladimir R. Kostic</a>, 
<a href="/search/cs?searchtype=author&query=Novelli%2C+P">Pietro Novelli</a>, 
<a href="/search/cs?searchtype=author&query=Grazzi%2C+R">Riccardo Grazzi</a>, 
<a href="/search/cs?searchtype=author&query=Lounici%2C+K">Karim Lounici</a>, 
<a href="/search/cs?searchtype=author&query=Pontil%2C+M">Massimiliano Pontil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1078">[1078]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10085" title="Abstract">arXiv:2307.10085</a> (replaced) [<a href="/pdf/2307.10085" title="Download PDF">pdf</a>, <a href="/format/2307.10085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Decision Making Framework for Recommended Maintenance of Road Segments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haoyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yan Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 8 figures, 4 tables, and 2 algorithms
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1079">[1079]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10173" title="Abstract">arXiv:2307.10173</a> (replaced) [<a href="/pdf/2307.10173" title="Download PDF">pdf</a>, <a href="/format/2307.10173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity  Human-centric Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruixiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+W">Wanqi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+S">Siming Fan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Keyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Honglin He</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Huiwen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhongang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhengming Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhengyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+D">Daxuan Ren</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Loy%2C+C+C">Chen Change Loy</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wayne Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+B">Bo Dai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+K">Kwan-Yee Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted by ICCV2023. Project page: <a href="https://dna-rendering.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1080">[1080]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10829" title="Abstract">arXiv:2307.10829</a> (replaced) [<a href="/pdf/2307.10829" title="Download PDF">pdf</a>, <a href="/format/2307.10829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact Diffusion Inversion via Bi-directional Integration Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guoqiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+J+P">J. P. Lewis</a>, 
<a href="/search/cs?searchtype=author&query=Kleijn%2C+W+B">W. Bastiaan Kleijn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2304.11328">arXiv:2304.11328</a>. Our code is available at <a href="https://github.com/guoqiang-zhang-x/BDIA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1081">[1081]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10922" title="Abstract">arXiv:2307.10922</a> (replaced) [<a href="/pdf/2307.10922" title="Download PDF">pdf</a>, <a href="/format/2307.10922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-based Action Concept Spaces Improve Video Self-Supervised  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ranasinghe%2C+K">Kanchana Ranasinghe</a>, 
<a href="/search/cs?searchtype=author&query=Ryoo%2C+M">Michael Ryoo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1082">[1082]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11342" title="Abstract">arXiv:2307.11342</a> (replaced) [<a href="/pdf/2307.11342" title="Download PDF">pdf</a>, <a href="/format/2307.11342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tuning Pre-trained Model via Moment Probing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Mingze Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qilong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhenyi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+P">Pengfei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qinghua Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingbo Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV 2023; Project Page: <a href="https://github.com/mingzeG/Moment-Probing">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1083">[1083]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11394" title="Abstract">arXiv:2307.11394</a> (replaced) [<a href="/pdf/2307.11394" title="Download PDF">pdf</a>, <a href="/format/2307.11394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MeetEval: A Toolkit for Computation of Word Error Rates for Meeting  Transcription Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=von+Neumann%2C+T">Thilo von Neumann</a>, 
<a href="/search/cs?searchtype=author&query=Boeddeker%2C+C">Christoph Boeddeker</a>, 
<a href="/search/cs?searchtype=author&query=Delcroix%2C+M">Marc Delcroix</a>, 
<a href="/search/cs?searchtype=author&query=Haeb-Umbach%2C+R">Reinhold Haeb-Umbach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at the CHiME7 workshop 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1084">[1084]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12082" title="Abstract">arXiv:2307.12082</a> (replaced) [<a href="/pdf/2307.12082" title="Download PDF">pdf</a>, <a href="/format/2307.12082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Quantitative Analysis of Open Source Software Code Quality: Insights  from Metric Distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Siyuan Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mianmian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yekai Guo</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuejiang He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bichao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yong Xia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been revised according to the feedback received from the reviews during IEEE QRS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item1085">[1085]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12856" title="Abstract">arXiv:2307.12856</a> (replaced) [<a href="/pdf/2307.12856" title="Download PDF">pdf</a>, <a href="/format/2307.12856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Real-World WebAgent with Planning, Long Context Understanding, and  Program Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gur%2C+I">Izzeddin Gur</a>, 
<a href="/search/cs?searchtype=author&query=Furuta%2C+H">Hiroki Furuta</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+A">Austin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Safdari%2C+M">Mustafa Safdari</a>, 
<a href="/search/cs?searchtype=author&query=Matsuo%2C+Y">Yutaka Matsuo</a>, 
<a href="/search/cs?searchtype=author&query=Eck%2C+D">Douglas Eck</a>, 
<a href="/search/cs?searchtype=author&query=Faust%2C+A">Aleksandra Faust</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1086">[1086]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14535" title="Abstract">arXiv:2307.14535</a> (replaced) [<a href="/pdf/2307.14535" title="Download PDF">pdf</a>, <a href="/format/2307.14535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ha%2C+H">Huy Ha</a>, 
<a href="/search/cs?searchtype=author&query=Florence%2C+P">Pete Florence</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shuran Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 9 figures, videos and code links on website <a href="https://www.cs.columbia.edu/~huy/scalingup/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1087">[1087]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15546" title="Abstract">arXiv:2307.15546</a> (replaced) [<a href="/pdf/2307.15546" title="Download PDF">pdf</a>, <a href="/format/2307.15546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Trade-off Between Efficiency and Precision of Neural Abstraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Edwards%2C+A">Alec Edwards</a>, 
<a href="/search/cs?searchtype=author&query=Giacobbe%2C+M">Mirco Giacobbe</a>, 
<a href="/search/cs?searchtype=author&query=Abate%2C+A">Alessandro Abate</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appeared at QEST 2023. Added codebase link; corrected Eq. 11
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1088">[1088]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15996" title="Abstract">arXiv:2307.15996</a> (replaced) [<a href="/pdf/2307.15996" title="Download PDF">pdf</a>, <a href="/format/2307.15996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locked Polyomino Tilings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tucker-Foltz%2C+J">Jamie Tucker-Foltz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item1089">[1089]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16189" title="Abstract">arXiv:2307.16189</a> (replaced) [<a href="/pdf/2307.16189" title="Download PDF">pdf</a>, <a href="/format/2307.16189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trustworthy Optimization: A Novel Approach to Counter Numerical  Instability in 16-bit Neural Network Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yun%2C+J">Juyoung Yun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1090">[1090]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16708" title="Abstract">arXiv:2307.16708</a> (replaced) [<a href="/pdf/2307.16708" title="Download PDF">pdf</a>, <a href="/format/2307.16708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Meets Adaptive Filtering: A Stein&#x27;s Unbiased Risk  Estimator Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Esmaeilbeig%2C+Z">Zahra Esmaeilbeig</a>, 
<a href="/search/eess?searchtype=author&query=Soltanalian%2C+M">Mojtaba Soltanalian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2011.07458">arXiv:2011.07458</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1091">[1091]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00180" title="Abstract">arXiv:2308.00180</a> (replaced) [<a href="/pdf/2308.00180" title="Download PDF">pdf</a>, <a href="/format/2308.00180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> General Anomaly Detection of Underwater Gliders Validated by Large-scale  Deployment Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Ruochu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lembke%2C+C">Chad Lembke</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fumin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Edwards%2C+C">Catherine Edwards</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE/MTS OCEANS Gulf Coast 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1092">[1092]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01544" title="Abstract">arXiv:2308.01544</a> (replaced) [<a href="/pdf/2308.01544" title="Download PDF">pdf</a>, <a href="/format/2308.01544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Neurons in Pretrained Text-Only Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schwettmann%2C+S">Sarah Schwettmann</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+N">Neil Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+S">Samuel Klein</a>, 
<a href="/search/cs?searchtype=author&query=Bau%2C+D">David Bau</a>, 
<a href="/search/cs?searchtype=author&query=Torralba%2C+A">Antonio Torralba</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Oral presentation at ICCV CLVL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1093">[1093]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02599" title="Abstract">arXiv:2308.02599</a> (replaced) [<a href="/pdf/2308.02599" title="Download PDF">pdf</a>, <a href="/format/2308.02599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Branched Latent Neural Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salvador%2C+M">Matteo Salvador</a>, 
<a href="/search/cs?searchtype=author&query=Marsden%2C+A+L">Alison Lesley Marsden</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item1094">[1094]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03308" title="Abstract">arXiv:2308.03308</a> (replaced) [<a href="/pdf/2308.03308" title="Download PDF">pdf</a>, <a href="/format/2308.03308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synchronized CTL over One-Counter Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almagor%2C+S">Shaull Almagor</a>, 
<a href="/search/cs?searchtype=author&query=Assa%2C+D">Daniel Assa</a>, 
<a href="/search/cs?searchtype=author&query=Boker%2C+U">Udi Boker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item1095">[1095]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03901" title="Abstract">arXiv:2308.03901</a> (replaced) [<a href="/pdf/2308.03901" title="Download PDF">pdf</a>, <a href="/format/2308.03901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLIPS: Federated Learning using Intelligent Participant Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhope%2C+R+A">Rahul Atul Bhope</a>, 
<a href="/search/cs?searchtype=author&query=Jayaram%2C+K+R">K. R. Jayaram</a>, 
<a href="/search/cs?searchtype=author&query=Venkatasubramanian%2C+N">Nalini Venkatasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+A">Ashish Verma</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+G">Gegi Thomas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item1096">[1096]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03985" title="Abstract">arXiv:2308.03985</a> (replaced) [<a href="/pdf/2308.03985" title="Download PDF">pdf</a>, <a href="/format/2308.03985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fourier neural operator for real-time simulation of 3D dynamic urban  microclimate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Wenhui Peng</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+S">Shaoxiang Qin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Senwen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianchun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L+L">Liangzhu Leon Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Fluid Dynamics (physics.flu-dyn)

</div>
</div>
</dd>
<dt><a name="item1097">[1097]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04152" title="Abstract">arXiv:2308.04152</a> (replaced) [<a href="/pdf/2308.04152" title="Download PDF">pdf</a>, <a href="/format/2308.04152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative  Instructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juncheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+K">Kaihang Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Z">Zhiqi Ge</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Minghe Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanwang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+W">Wei Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenqiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Siliang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yueting Zhuang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1098">[1098]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04670" title="Abstract">arXiv:2308.04670</a> (replaced) [<a href="/pdf/2308.04670" title="Download PDF">pdf</a>, <a href="/format/2308.04670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TRTM: Template-based Reconstruction and Target-oriented Manipulation of  Crumpled Cloths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gen Li</a>, 
<a href="/search/cs?searchtype=author&query=Zamora%2C+M">Miguel Zamora</a>, 
<a href="/search/cs?searchtype=author&query=Coros%2C+S">Stelian Coros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1099">[1099]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04847" title="Abstract">arXiv:2308.04847</a> (replaced) [<a href="/pdf/2308.04847" title="Download PDF">pdf</a>, <a href="/ps/2308.04847" title="Download PostScript">ps</a>, <a href="/format/2308.04847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vehicle State Estimation through Modular Factor Graph-based Fusion of  Multiple Sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dahal%2C+P">Pragyan Dahal</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+J">Jai Prakash</a>, 
<a href="/search/cs?searchtype=author&query=Arrigoni%2C+S">Stefano Arrigoni</a>, 
<a href="/search/cs?searchtype=author&query=Braghin%2C+F">Francesco Braghin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1100">[1100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05320" title="Abstract">arXiv:2308.05320</a> (replaced) [<a href="/pdf/2308.05320" title="Download PDF">pdf</a>, <a href="/format/2308.05320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Transferable and Stealthy Adversarial Patch via  Attention-guided Adversarial Inpainting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+M">Mingxing Duan</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xuelong Dai</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+B">Bin Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICLR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1101">[1101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06810" title="Abstract">arXiv:2308.06810</a> (replaced) [<a href="/pdf/2308.06810" title="Download PDF">pdf</a>, <a href="/format/2308.06810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ground Manipulator Primitive Tasks to Executable Actions using Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yue Cao</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C+S+G">C.S. George Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI Fall Symposium on Unifying Representations for Robot Application Development, Arlington, VA, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1102">[1102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07545" title="Abstract">arXiv:2308.07545</a> (replaced) [<a href="/pdf/2308.07545" title="Download PDF">pdf</a>, <a href="/format/2308.07545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-Language Dataset Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xindi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Byron Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhiwei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Russakovsky%2C+O">Olga Russakovsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1103">[1103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07939" title="Abstract">arXiv:2308.07939</a> (replaced) [<a href="/pdf/2308.07939" title="Download PDF">pdf</a>, <a href="/format/2308.07939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ada-QPacknet -- adaptive pruning with bit width reduction as an  efficient continual learning method without forgetting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pietro%C5%84%2C+M">Marcin Pietro&#x144;</a>, 
<a href="/search/cs?searchtype=author&query=%C5%BBurek%2C+D">Dominik &#x17b;urek</a>, 
<a href="/search/cs?searchtype=author&query=Faber%2C+K">Kamil Faber</a>, 
<a href="/search/cs?searchtype=author&query=Corizzo%2C+R">Roberto Corizzo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted at ECAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1104">[1104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08493" title="Abstract">arXiv:2308.08493</a> (replaced) [<a href="/pdf/2308.08493" title="Download PDF">pdf</a>, <a href="/ps/2308.08493" title="Download PostScript">ps</a>, <a href="/format/2308.08493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time Travel in LLMs: Tracing Data Contamination in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golchin%2C+S">Shahriar Golchin</a>, 
<a href="/search/cs?searchtype=author&query=Surdeanu%2C+M">Mihai Surdeanu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2 preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1105">[1105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10047" title="Abstract">arXiv:2308.10047</a> (replaced) [<a href="/pdf/2308.10047" title="Download PDF">pdf</a>, <a href="/format/2308.10047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Probabilistic Causal Discovery, Inference &amp; Explanations for  Autonomous Drones in Mine Surveying Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cannizzaro%2C+R">Ricardo Cannizzaro</a>, 
<a href="/search/cs?searchtype=author&query=Howard%2C+R">Rhys Howard</a>, 
<a href="/search/cs?searchtype=author&query=Lewinska%2C+P">Paulina Lewinska</a>, 
<a href="/search/cs?searchtype=author&query=Kunze%2C+L">Lars Kunze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 Pages, 3 Figures, 1 Algorithm, To be published in the Proceedings of the "Causality for Robotics: Answering the Question of Why" workshop at the 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Final submission version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1106">[1106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10249" title="Abstract">arXiv:2308.10249</a> (replaced) [<a href="/pdf/2308.10249" title="Download PDF">pdf</a>, <a href="/format/2308.10249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Formally Verified Security Monitor for VM-based Confidential  Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ozga%2C+W">Wojciech Ozga</a>, 
<a href="/search/cs?searchtype=author&query=Hunt%2C+G+D+H">Guerney D. H. Hunt</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+M+V">Michael V. Le</a>, 
<a href="/search/cs?searchtype=author&query=Palmer%2C+E+R">Elaine R. Palmer</a>, 
<a href="/search/cs?searchtype=author&query=Shinnar%2C+A">Avraham Shinnar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item1107">[1107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10819" title="Abstract">arXiv:2308.10819</a> (replaced) [<a href="/pdf/2308.10819" title="Download PDF">pdf</a>, <a href="/format/2308.10819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Instruction-Following Robustness of Large Language Models  to Prompt Injection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zekun Li</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Baolin Peng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengcheng He</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xifeng Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The data and code can be found at <a href="https://github.com/Leezekun/Adv-Instruct-Eval">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1108">[1108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11471" title="Abstract">arXiv:2308.11471</a> (replaced) [<a href="/pdf/2308.11471" title="Download PDF">pdf</a>, <a href="/format/2308.11471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence  (DOVESEI)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bong%2C+H+M">Haechan Mark Bong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rongge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=de+Azambuja%2C+R">Ricardo de Azambuja</a>, 
<a href="/search/cs?searchtype=author&query=Beltrame%2C+G">Giovanni Beltrame</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IROS 2023 The Last-Mile Robotics Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1109">[1109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11819" title="Abstract">arXiv:2308.11819</a> (replaced) [<a href="/pdf/2308.11819" title="Download PDF">pdf</a>, <a href="/format/2308.11819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Counterfactual Fair Model for Longitudinal Electronic Health Records  via Deconfounder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaohan Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Philip Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1110">[1110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12030" title="Abstract">arXiv:2308.12030</a> (replaced) [<a href="/pdf/2308.12030" title="Download PDF">pdf</a>, <a href="/format/2308.12030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt-Based Length Controlled Generation with Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jie%2C+R">Renlong Jie</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+X">Xiaojun Meng</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1111">[1111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13328" title="Abstract">arXiv:2308.13328</a> (replaced) [<a href="/pdf/2308.13328" title="Download PDF">pdf</a>, <a href="/format/2308.13328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compressor-Based Classification for Atrial Fibrillation Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Markov%2C+N">Nikita Markov</a>, 
<a href="/search/eess?searchtype=author&query=Ushenin%2C+K">Konstantin Ushenin</a>, 
<a href="/search/eess?searchtype=author&query=Bozhko%2C+Y">Yakov Bozhko</a>, 
<a href="/search/eess?searchtype=author&query=Solovyova%2C+O">Olga Solovyova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is sent for review at the IEEE conference, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1112">[1112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13498" title="Abstract">arXiv:2308.13498</a> (replaced) [<a href="/pdf/2308.13498" title="Download PDF">pdf</a>, <a href="/format/2308.13498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty  Estimation with Pairwise-Distance Estimators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berry%2C+L">Lucas Berry</a>, 
<a href="/search/cs?searchtype=author&query=Meger%2C+D">David Meger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1113">[1113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13561" title="Abstract">arXiv:2308.13561</a> (replaced) [<a href="/pdf/2308.13561" title="Download PDF">pdf</a>, <a href="/format/2308.13561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Project Aria: A New Tool for Egocentric Multi-Modal AI Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Engel%2C+J">Jakob Engel</a>, 
<a href="/search/cs?searchtype=author&query=Somasundaram%2C+K">Kiran Somasundaram</a>, 
<a href="/search/cs?searchtype=author&query=Goesele%2C+M">Michael Goesele</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+A">Albert Sun</a>, 
<a href="/search/cs?searchtype=author&query=Gamino%2C+A">Alexander Gamino</a>, 
<a href="/search/cs?searchtype=author&query=Turner%2C+A">Andrew Turner</a>, 
<a href="/search/cs?searchtype=author&query=Talattof%2C+A">Arjang Talattof</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+A">Arnie Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Souti%2C+B">Bilal Souti</a>, 
<a href="/search/cs?searchtype=author&query=Meredith%2C+B">Brighid Meredith</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+C">Cheng Peng</a>, 
<a href="/search/cs?searchtype=author&query=Sweeney%2C+C">Chris Sweeney</a>, 
<a href="/search/cs?searchtype=author&query=Wilson%2C+C">Cole Wilson</a>, 
<a href="/search/cs?searchtype=author&query=Barnes%2C+D">Dan Barnes</a>, 
<a href="/search/cs?searchtype=author&query=DeTone%2C+D">Daniel DeTone</a>, 
<a href="/search/cs?searchtype=author&query=Caruso%2C+D">David Caruso</a>, 
<a href="/search/cs?searchtype=author&query=Valleroy%2C+D">Derek Valleroy</a>, 
<a href="/search/cs?searchtype=author&query=Ginjupalli%2C+D">Dinesh Ginjupalli</a>, 
<a href="/search/cs?searchtype=author&query=Frost%2C+D">Duncan Frost</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+E">Edward Miller</a>, 
<a href="/search/cs?searchtype=author&query=Mueggler%2C+E">Elias Mueggler</a>, 
<a href="/search/cs?searchtype=author&query=Oleinik%2C+E">Evgeniy Oleinik</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Somasundaram%2C+G">Guruprasad Somasundaram</a>, 
<a href="/search/cs?searchtype=author&query=Solaira%2C+G">Gustavo Solaira</a>, 
<a href="/search/cs?searchtype=author&query=Lanaras%2C+H">Harry Lanaras</a>, 
<a href="/search/cs?searchtype=author&query=Howard-Jenkins%2C+H">Henry Howard-Jenkins</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Huixuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H+J">Hyo Jin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Rivera%2C+J">Jaime Rivera</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Ji Luo</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Jing Dong</a>, 
<a href="/search/cs?searchtype=author&query=Straub%2C+J">Julian Straub</a>, 
<a href="/search/cs?searchtype=author&query=Bailey%2C+K">Kevin Bailey</a>, 
<a href="/search/cs?searchtype=author&query=Eckenhoff%2C+K">Kevin Eckenhoff</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lingni Ma</a>, 
<a href="/search/cs?searchtype=author&query=Pesqueira%2C+L">Luis Pesqueira</a>, 
<a href="/search/cs?searchtype=author&query=Schwesinger%2C+M">Mark Schwesinger</a>, 
<a href="/search/cs?searchtype=author&query=Monge%2C+M">Maurizio Monge</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+N">Nan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Charron%2C+N">Nick Charron</a>, 
<a href="/search/cs?searchtype=author&query=Raina%2C+N">Nikhil Raina</a>, 
<a href="/search/cs?searchtype=author&query=Parkhi%2C+O">Omkar Parkhi</a>, 
<a href="/search/cs?searchtype=author&query=Borschowa%2C+P">Peter Borschowa</a>, 
<a href="/search/cs?searchtype=author&query=Moulon%2C+P">Pierre Moulon</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+P">Prince Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Mur-Artal%2C+R">Raul Mur-Artal</a>, 
<a href="/search/cs?searchtype=author&query=Pennington%2C+R">Robbie Pennington</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+S">Sachin Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Miglani%2C+S">Sagar Miglani</a>, 
<a href="/search/cs?searchtype=author&query=Gondi%2C+S">Santosh Gondi</a>, 
<a href="/search/cs?searchtype=author&query=Solanki%2C+S">Saransh Solanki</a>, 
<a href="/search/cs?searchtype=author&query=Diener%2C+S">Sean Diener</a>,  et al. (21 additional authors not shown)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1114">[1114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13728" title="Abstract">arXiv:2308.13728</a> (replaced) [<a href="/pdf/2308.13728" title="Download PDF">pdf</a>, <a href="/ps/2308.13728" title="Download PostScript">ps</a>, <a href="/format/2308.13728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Indicator functions, v-numbers and Gorenstein rings in the theory of  projective Reed-Muller-type codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gonz%C3%A1lez-Sarabia%2C+M">Manuel Gonz&#xe1;lez-Sarabia</a>, 
<a href="/search/math?searchtype=author&query=Mu%C3%B1oz-George%2C+H">Humberto Mu&#xf1;oz-George</a>, 
<a href="/search/math?searchtype=author&query=Ordaz%2C+J+A">Jorge A. Ordaz</a>, 
<a href="/search/math?searchtype=author&query=S%C3%A1enz-de-Cabez%C3%B3n%2C+E">Eduardo S&#xe1;enz-de-Cabez&#xf3;n</a>, 
<a href="/search/math?searchtype=author&query=Villarreal%2C+R+H">Rafael H. Villarreal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Commutative Algebra (math.AC)</span>; Information Theory (cs.IT); Algebraic Geometry (math.AG)

</div>
</div>
</dd>
<dt><a name="item1115">[1115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13868" title="Abstract">arXiv:2308.13868</a> (replaced) [<a href="/pdf/2308.13868" title="Download PDF">pdf</a>, <a href="/ps/2308.13868" title="Download PostScript">ps</a>, <a href="/format/2308.13868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Graph-Theoretic Model for a Generic Three Jug Puzzle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hegde%2C+S+M">Suresh Manjanath Hegde</a>, 
<a href="/search/math?searchtype=author&query=Kulamarva%2C+S">Shashanka Kulamarva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">History and Overview (math.HO)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item1116">[1116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14491" title="Abstract">arXiv:2308.14491</a> (replaced) [<a href="/pdf/2308.14491" title="Download PDF">pdf</a>, <a href="/ps/2308.14491" title="Download PostScript">ps</a>, <a href="/format/2308.14491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closeness of Some Line Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dangalchev%2C+C">Chavdar Dangalchev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
</div>
</dd>
<dt><a name="item1117">[1117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14945" title="Abstract">arXiv:2308.14945</a> (replaced) [<a href="/pdf/2308.14945" title="Download PDF">pdf</a>, <a href="/format/2308.14945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Noise-Free Sampling Algorithms via Regularized Wasserstein Proximals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Tan%2C+H+Y">Hong Ye Tan</a>, 
<a href="/search/stat?searchtype=author&query=Osher%2C+S">Stanley Osher</a>, 
<a href="/search/stat?searchtype=author&query=Li%2C+W">Wuchen Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item1118">[1118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15276" title="Abstract">arXiv:2308.15276</a> (replaced) [<a href="/pdf/2308.15276" title="Download PDF">pdf</a>, <a href="/format/2308.15276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models in Fault Localisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yonghao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+M">Jie M. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Papadakis%2C+M">Mike Papadakis</a>, 
<a href="/search/cs?searchtype=author&query=Harman%2C+M">Mark Harman</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item1119">[1119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16458" title="Abstract">arXiv:2308.16458</a> (replaced) [<a href="/pdf/2308.16458" title="Download PDF">pdf</a>, <a href="/format/2308.16458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual  Pragmatic Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+B">Bill Qian</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+R">Rick Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiakang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gerstein%2C+M">Mark Gerstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1120">[1120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16463" title="Abstract">arXiv:2308.16463</a> (replaced) [<a href="/pdf/2308.16463" title="Download PDF">pdf</a>, <a href="/format/2308.16463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparkles: Unlocking Chats Across Multiple Images for Multimodal  Instruction-Following Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yupan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zaiqiao Meng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fangyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yixuan Su</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+N">Nigel Collier</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yutong Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Reduced main content to 9 pages; typos corrected
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1121">[1121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16512" title="Abstract">arXiv:2308.16512</a> (replaced) [<a href="/pdf/2308.16512" title="Download PDF">pdf</a>, <a href="/format/2308.16512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MVDream: Multi-view Diffusion for 3D Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yichun Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jianglong Ye</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+M">Mai Long</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kejie Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiao Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our project page is <a href="https://MV-Dream.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1122">[1122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00079" title="Abstract">arXiv:2309.00079</a> (replaced) [<a href="/pdf/2309.00079" title="Download PDF">pdf</a>, <a href="/format/2309.00079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Implicit Bias of Adam
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cattaneo%2C+M+D">Matias D. Cattaneo</a>, 
<a href="/search/cs?searchtype=author&query=Klusowski%2C+J+M">Jason M. Klusowski</a>, 
<a href="/search/cs?searchtype=author&query=Shigida%2C+B">Boris Shigida</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1123">[1123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01026" title="Abstract">arXiv:2309.01026</a> (replaced) [<a href="/pdf/2309.01026" title="Download PDF">pdf</a>, <a href="/format/2309.01026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Recommendations with Pre-Trained Large Language Models for  Multimodal Nudging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harrison%2C+R+M">Rachel M. Harrison</a>, 
<a href="/search/cs?searchtype=author&query=Dereventsov%2C+A">Anton Dereventsov</a>, 
<a href="/search/cs?searchtype=author&query=Bibin%2C+A">Anton Bibin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1124">[1124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01759" title="Abstract">arXiv:2309.01759</a> (replaced) [<a href="/pdf/2309.01759" title="Download PDF">pdf</a>, <a href="/ps/2309.01759" title="Download PostScript">ps</a>, <a href="/format/2309.01759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extremal Growth of Multiple Toeplitz Operators and Implications for  Numerical Stability of Approximation Schemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rastogi%2C+Y">Yash Rastogi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Functional Analysis (math.FA)

</div>
</div>
</dd>
<dt><a name="item1125">[1125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03038" title="Abstract">arXiv:2309.03038</a> (replaced) [<a href="/pdf/2309.03038" title="Download PDF">pdf</a>, <a href="/format/2309.03038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cellular Wireless Networks in the Upper Mid-Band
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+S">Seongjoon Kang</a>, 
<a href="/search/cs?searchtype=author&query=Mezzavilla%2C+M">Marco Mezzavilla</a>, 
<a href="/search/cs?searchtype=author&query=Rangan%2C+S">Sundeep Rangan</a>, 
<a href="/search/cs?searchtype=author&query=Madanayake%2C+A">Arjuna Madanayake</a>, 
<a href="/search/cs?searchtype=author&query=Venkatakrishnan%2C+S+B">Satheesh Bojja Venkatakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Hellbourg%2C+G">Gregory Hellbourg</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+M">Monisha Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+H">Hamed Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Dhananjay%2C+A">Aditya Dhananjay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1126">[1126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03049" title="Abstract">arXiv:2309.03049</a> (replaced) [<a href="/pdf/2309.03049" title="Download PDF">pdf</a>, <a href="/format/2309.03049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Growth: Real-time CNN Layer Expansion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yunjie Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunhao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/YunjieZhu/Extensible-Convolutional-Layer-git-version">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1127">[1127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03160" title="Abstract">arXiv:2309.03160</a> (replaced) [<a href="/pdf/2309.03160" title="Download PDF">pdf</a>, <a href="/format/2309.03160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ResFields: Residual Neural Fields for Spatiotemporal Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mihajlovic%2C+M">Marko Mihajlovic</a>, 
<a href="/search/cs?searchtype=author&query=Prokudin%2C+S">Sergey Prokudin</a>, 
<a href="/search/cs?searchtype=author&query=Pollefeys%2C+M">Marc Pollefeys</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Siyu Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page and code at <a href="https://markomih.github.io/ResFields/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1128">[1128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05494" title="Abstract">arXiv:2309.05494</a> (replaced) [<a href="/pdf/2309.05494" title="Download PDF">pdf</a>, <a href="/format/2309.05494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CrisisTransformers: Pre-trained language models and sentence encoders  for crisis-related social media texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lamsal%2C+R">Rabindra Lamsal</a>, 
<a href="/search/cs?searchtype=author&query=Read%2C+M+R">Maria Rodriguez Read</a>, 
<a href="/search/cs?searchtype=author&query=Karunasekera%2C+S">Shanika Karunasekera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1129">[1129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05653" title="Abstract">arXiv:2309.05653</a> (replaced) [<a href="/pdf/2309.05653" title="Download PDF">pdf</a>, <a href="/format/2309.05653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAmmoTH: Building Math Generalist Models through Hybrid Instruction  Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiang Yue</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+X">Xingwei Qu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress; Xiang Yue and Wenhu Chen contributed equally to this paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1130">[1130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06034" title="Abstract">arXiv:2309.06034</a> (replaced) [<a href="/pdf/2309.06034" title="Download PDF">pdf</a>, <a href="/format/2309.06034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Normality Learning-based Graph Anomaly Detection via Multi-Scale  Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jingcan Duan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jingtao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Hu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Haifang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinwang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures, accepted by ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1131">[1131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06687" title="Abstract">arXiv:2309.06687</a> (replaced) [<a href="/pdf/2309.06687" title="Download PDF">pdf</a>, <a href="/format/2309.06687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Refined Large Language Model as Automated Reward Function Designer  for Deep Reinforcement Learning in Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiayang Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhehua Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiawei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+C">Chunrong Fang</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+Z">Zhan Shu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lei Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1132">[1132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07431" title="Abstract">arXiv:2309.07431</a> (replaced) [<a href="/pdf/2309.07431" title="Download PDF">pdf</a>, <a href="/format/2309.07431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asynchronous Spatial Allocation Protocol for Trajectory Planning of  Heterogeneous Multi-Agent Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuda Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Haoze Dong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhongkui Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1133">[1133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07915" title="Abstract">arXiv:2309.07915</a> (replaced) [<a href="/pdf/2309.07915" title="Download PDF">pdf</a>, <a href="/format/2309.07915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MMICL: Empowering Vision-language Model with Multi-Modal In-Context  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haozhe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zefan Cai</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+S">Shuzheng Si</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaojian Ma</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+K">Kaikai An</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zixuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Wenjuan Han</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+B">Baobao Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code, dataset, checkpoints, and demos are available at <a href="https://github.com/PKUnlp-icler/MIC">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1134">[1134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07936" title="Abstract">arXiv:2309.07936</a> (replaced) [<a href="/pdf/2309.07936" title="Download PDF">pdf</a>, <a href="/format/2309.07936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Landscape-Sketch-Step: An AI/ML-Based Metaheuristic for Surrogate  Optimization Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Monteiro%2C+R">Rafael Monteiro</a>, 
<a href="/search/cs?searchtype=author&query=Sau%2C+K">Kartik Sau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Git-hub on <a href="https://github.com/rafael-a-monteiro-math/landscape_sketch_and_step/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item1135">[1135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08374" title="Abstract">arXiv:2309.08374</a> (replaced) [<a href="/pdf/2309.08374" title="Download PDF">pdf</a>, <a href="/format/2309.08374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the limitations of self-supervised learning for tabular  anomaly detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mai%2C+K+T">Kimberly T. Mai</a>, 
<a href="/search/cs?searchtype=author&query=Davies%2C+T">Toby Davies</a>, 
<a href="/search/cs?searchtype=author&query=Griffin%2C+L+D">Lewis D. Griffin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1136">[1136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08375" title="Abstract">arXiv:2309.08375</a> (replaced) [<a href="/pdf/2309.08375" title="Download PDF">pdf</a>, <a href="/format/2309.08375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Fair Classifier Generalization through Adaptive Priority  Reweighing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhihao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yiran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Mengnan Du</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jindong Gu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+X">Xinmei Tian</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+F">Fengxiang He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1137">[1137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08448" title="Abstract">arXiv:2309.08448</a> (replaced) [<a href="/pdf/2309.08448" title="Download PDF">pdf</a>, <a href="/format/2309.08448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing the Evaluation of Traditional Chinese Language Models: Towards  a Comprehensive Benchmark Suite
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chan-Jan Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chang-Le Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+F">Feng-Ting Liao</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+P">Po-Chun Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yi-Chang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shiu%2C+D">Da-shan Shiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1138">[1138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08887" title="Abstract">arXiv:2309.08887</a> (replaced) [<a href="/pdf/2309.08887" title="Download PDF">pdf</a>, <a href="/format/2309.08887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRaCE: Optimizing Grasps to Satisfy Ranked Criteria in Complex Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taunyazov%2C+T">Tasbolat Taunyazov</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+K">Kelvin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Soh%2C+H">Harold Soh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1139">[1139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09075" title="Abstract">arXiv:2309.09075</a> (replaced) [<a href="/pdf/2309.09075" title="Download PDF">pdf</a>, <a href="/ps/2309.09075" title="Download PostScript">ps</a>, <a href="/format/2309.09075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Music Generation based on Generative Adversarial Networks with  Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Ziyi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Ruoxue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenghan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaoxuan Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> co-author want to withdraw
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Multimedia (cs.MM); Audio and Speech Processing (eess.AS); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1140">[1140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09277" title="Abstract">arXiv:2309.09277</a> (replaced) [<a href="/pdf/2309.09277" title="Download PDF">pdf</a>, <a href="/format/2309.09277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness for All: Investigating Harms to Within-Group Individuals in  Producer Fairness Re-ranking Optimization -- A Reproducibility Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pellegrini%2C+G">Giovanni Pellegrini</a>, 
<a href="/search/cs?searchtype=author&query=Faraco%2C+V+M">Vittorio Maria Faraco</a>, 
<a href="/search/cs?searchtype=author&query=Deldjoo%2C+Y">Yashar Deldjoo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item1141">[1141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09454" title="Abstract">arXiv:2309.09454</a> (replaced) [<a href="/pdf/2309.09454" title="Download PDF">pdf</a>, <a href="/ps/2309.09454" title="Download PostScript">ps</a>, <a href="/format/2309.09454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymptotically Efficient Online Learning for Censored Regression Models  Under Non-I.I.D Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lantian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lei Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1142">[1142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09472" title="Abstract">arXiv:2309.09472</a> (replaced) [<a href="/pdf/2309.09472" title="Download PDF">pdf</a>, <a href="/format/2309.09472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstructing Existing Levels through Level Inpainting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+J+J">Johor Jara Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Guzdial%2C+M">Mathew Guzdial</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, Artificial Intelligence and Interactive Digital Entertainment
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1143">[1143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09476" title="Abstract">arXiv:2309.09476</a> (replaced) [<a href="/pdf/2309.09476" title="Download PDF">pdf</a>, <a href="/format/2309.09476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated  Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+J+J">Johor Jara Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Cooper%2C+S">Seth Cooper</a>, 
<a href="/search/cs?searchtype=author&query=Guzdial%2C+M">Mathew Guzdial</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, Artificial Intelligence and Interactive Digital Entertainment
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1144">[1144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10007" title="Abstract">arXiv:2309.10007</a> (replaced) [<a href="/pdf/2309.10007" title="Download PDF">pdf</a>, <a href="/format/2309.10007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive  Autonomous Vehicles using AutoDRIVE Ecosystem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Samak%2C+T+V">Tanmay Vilas Samak</a>, 
<a href="/search/cs?searchtype=author&query=Samak%2C+C+V">Chinmay Vilas Samak</a>, 
<a href="/search/cs?searchtype=author&query=Krovi%2C+V">Venkat Krovi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as Multi-Agent Dynamic Games (MAD-Games) Workshop Paper at IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item1145">[1145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10569" title="Abstract">arXiv:2309.10569</a> (replaced) [<a href="/pdf/2309.10569" title="Download PDF">pdf</a>, <a href="/format/2309.10569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task Graph offloading via Deep Reinforcement Learning in Mobile Edge  Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiagang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+Y">Yun Mi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1146">[1146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10609" title="Abstract">arXiv:2309.10609</a> (replaced) [<a href="/pdf/2309.10609" title="Download PDF">pdf</a>, <a href="/ps/2309.10609" title="Download PostScript">ps</a>, <a href="/format/2309.10609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Game Connectivity and Adaptive Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Johnston%2C+T">Tom Johnston</a>, 
<a href="/search/econ?searchtype=author&query=Savery%2C+M">Michael Savery</a>, 
<a href="/search/econ?searchtype=author&query=Scott%2C+A">Alex Scott</a>, 
<a href="/search/econ?searchtype=author&query=Tarbush%2C+B">Bassel Tarbush</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages; v2: small corrections
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item1147">[1147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10694" title="Abstract">arXiv:2309.10694</a> (replaced) [<a href="/pdf/2309.10694" title="Download PDF">pdf</a>, <a href="/format/2309.10694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;With Great Power Comes Great Responsibility!&quot;: Student and Instructor  Perspectives on the influence of LLMs on Undergraduate Engineering Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joshi%2C+I">Ishika Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Budhiraja%2C+R">Ritvik Budhiraja</a>, 
<a href="/search/cs?searchtype=author&query=Tanna%2C+P+D">Pranav Deepak Tanna</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+L">Lovenya Jain</a>, 
<a href="/search/cs?searchtype=author&query=Deshpande%2C+M">Mihika Deshpande</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+A">Arjun Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Rallapalli%2C+S">Srinivas Rallapalli</a>, 
<a href="/search/cs?searchtype=author&query=Akolekar%2C+H+D">Harshal D Akolekar</a>, 
<a href="/search/cs?searchtype=author&query=Challa%2C+J+S">Jagat Sesh Challa</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+D">Dhruv Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item1148">[1148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10706" title="Abstract">arXiv:2309.10706</a> (replaced) [<a href="/pdf/2309.10706" title="Download PDF">pdf</a>, <a href="/format/2309.10706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model  Pre-trained from Scratch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juntao Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zecheng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yuyang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pinzheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Pei Guo</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+W">Wangjie You</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+D">Dan Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenliang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+G">Guohong Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qiaoming Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guodong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1149">[1149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11054" title="Abstract">arXiv:2309.11054</a> (replaced) [<a href="/pdf/2309.11054" title="Download PDF">pdf</a>, <a href="/format/2309.11054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design of Chain-of-Thought in Math Problem Solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jie%2C+Z">Zhanming Jie</a>, 
<a href="/search/cs?searchtype=author&query=Luong%2C+T+Q">Trung Quoc Luong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinbo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiaoran Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item1150">[1150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11083" title="Abstract">arXiv:2309.11083</a> (replaced) [<a href="/pdf/2309.11083" title="Download PDF">pdf</a>, <a href="/format/2309.11083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ElasticNotebook: Enabling Live Migration for Computational Notebooks  (Technical Report)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhaoheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Gor%2C+P">Pranav Gor</a>, 
<a href="/search/cs?searchtype=author&query=Prabhu%2C+R">Rahul Prabhu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hui Yu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yuzhou Mao</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+Y">Yongjoo Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to VLDB 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item1151">[1151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11721" title="Abstract">arXiv:2309.11721</a> (replaced) [<a href="/e-print/2309.11721" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defining and Preventing Asymmetric Mempool DoS in Ethereum with saferAd
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wanning Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yibo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yuzhe Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In page 5 section 5, the evaluation results lack representativeness. More evaluations are required on various workloads to prove the conclusion. Historical versions of the paper are inconsistent
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1152">[1152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11998" title="Abstract">arXiv:2309.11998</a> (replaced) [<a href="/pdf/2309.11998" title="Download PDF">pdf</a>, <a href="/format/2309.11998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lianmin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+W">Wei-Lin Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Y">Ying Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianle Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+S">Siyuan Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhanghao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yonghao Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuohan Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E+P">Eric. P Xing</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+J+E">Joseph E. Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Stoica%2C+I">Ion Stoica</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1153">[1153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12341" title="Abstract">arXiv:2309.12341</a> (replaced) [<a href="/pdf/2309.12341" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task allocation planning based on HTN for national economic mobilization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Peng Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, in Chinese language. 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computational Engineering, Finance, and Science (cs.CE); Robotics (cs.RO); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1154">[1154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12474" title="Abstract">arXiv:2309.12474</a> (replaced) [<a href="/pdf/2309.12474" title="Download PDF">pdf</a>, <a href="/format/2309.12474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAVME: Efficient Safety Validation for Autonomous Systems Using  Meta-Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schlichting%2C+M+R">Marc R. Schlichting</a>, 
<a href="/search/cs?searchtype=author&query=Boord%2C+N+V">Nina V. Boord</a>, 
<a href="/search/cs?searchtype=author&query=Corso%2C+A+L">Anthony L. Corso</a>, 
<a href="/search/cs?searchtype=author&query=Kochenderfer%2C+M+J">Mykel J. Kochenderfer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for ITSC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Emerging Technologies (cs.ET); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1155">[1155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12488" title="Abstract">arXiv:2309.12488</a> (replaced) [<a href="/pdf/2309.12488" title="Download PDF">pdf</a>, <a href="/format/2309.12488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sharpness-Aware Minimization and the Edge of Stability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+P+M">Philip M. Long</a>, 
<a href="/search/cs?searchtype=author&query=Bartlett%2C+P+L">Peter L. Bartlett</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1156">[1156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12491" title="Abstract">arXiv:2309.12491</a> (replaced) [<a href="/pdf/2309.12491" title="Download PDF">pdf</a>, <a href="/format/2309.12491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Impact of Training Data Distribution and Subword  Tokenization on Gender Bias in Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iluz%2C+B">Bar Iluz</a>, 
<a href="/search/cs?searchtype=author&query=Limisiewicz%2C+T">Tomasz Limisiewicz</a>, 
<a href="/search/cs?searchtype=author&query=Stanovsky%2C+G">Gabriel Stanovsky</a>, 
<a href="/search/cs?searchtype=author&query=Mare%C4%8Dek%2C+D">David Mare&#x10d;ek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1157">[1157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12515" title="Abstract">arXiv:2309.12515</a> (replaced) [<a href="/pdf/2309.12515" title="Download PDF">pdf</a>, <a href="/ps/2309.12515" title="Download PostScript">ps</a>, <a href="/format/2309.12515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Diamond Machine For Strong Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Accattoli%2C+B">Beniamino Accattoli</a>, 
<a href="/search/cs?searchtype=author&query=Barenbaum%2C+P">Pablo Barenbaum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item1158">[1158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12668" title="Abstract">arXiv:2309.12668</a> (replaced) [<a href="/pdf/2309.12668" title="Download PDF">pdf</a>, <a href="/format/2309.12668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UWA360CAM: A 360$^{\circ}$ 24/7 Real-Time Streaming Camera System for  Underwater Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pham%2C+Q">Quan-Dung Pham</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yipeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+T">Tan-Sang Ha</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K+H+L">K.H. Long Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+B">Binh-Son Hua</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+S">Sai-Kit Yeung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1159">[1159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12916" title="Abstract">arXiv:2309.12916</a> (replaced) [<a href="/pdf/2309.12916" title="Download PDF">pdf</a>, <a href="/format/2309.12916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meso-scale size effects of material heterogeneities on crack propagation  in brittle solids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Li%2C+L">Liuchi Li</a>, 
<a href="/search/cond-mat?searchtype=author&query=Rao%2C+J">Jack Rao</a>, 
<a href="/search/cond-mat?searchtype=author&query=Hufnagel%2C+T">Todd Hufnagel</a>, 
<a href="/search/cond-mat?searchtype=author&query=Ramesh%2C+K">KT Ramesh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item1160">[1160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13061" title="Abstract">arXiv:2309.13061</a> (replaced) [<a href="/pdf/2309.13061" title="Download PDF">pdf</a>, <a href="/format/2309.13061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applying BioBERT to Extract Germline Gene-Disease Associations for  Building a Knowledge Graph from the Biomedical Literature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+A+D+D">Armando D. Diaz Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+S">Songhui Yue</a>, 
<a href="/search/cs?searchtype=author&query=Hayes%2C+S+T">Sean T. Hayes</a>, 
<a href="/search/cs?searchtype=author&query=Hughes%2C+K+S">Kevin S. Hughes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 7th International Conference on Information System and Data
  Mining (ICISDM2023-ACM), Atlanta, USA, May 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1161">[1161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13196" title="Abstract">arXiv:2309.13196</a> (replaced) [<a href="/pdf/2309.13196" title="Download PDF">pdf</a>, <a href="/format/2309.13196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ClusterFormer: Clustering As A Universal Visual Learner
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+J+C">James C. Liang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yiming Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+T">Tong Geng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenguan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dongfang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1162">[1162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13278" title="Abstract">arXiv:2309.13278</a> (replaced) [<a href="/pdf/2309.13278" title="Download PDF">pdf</a>, <a href="/format/2309.13278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributional Shift-Aware Off-Policy Interval Estimation: A Unified  Error Quantification Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhou%2C+W">Wenzhuo Zhou</a>, 
<a href="/search/stat?searchtype=author&query=Li%2C+Y">Yuhan Li</a>, 
<a href="/search/stat?searchtype=author&query=Zhu%2C+R">Ruoqing Zhu</a>, 
<a href="/search/stat?searchtype=author&query=Qu%2C+A">Annie Qu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1163">[1163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13396" title="Abstract">arXiv:2309.13396</a> (replaced) [<a href="/pdf/2309.13396" title="Download PDF">pdf</a>, <a href="/format/2309.13396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EquiCity Game: A mathematical serious game for participatory design of  spatial configurations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nourian%2C+P">Pirouz Nourian</a>, 
<a href="/search/cs?searchtype=author&query=Azadi%2C+S">Shervin Azadi</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+N">Nan Bai</a>, 
<a href="/search/cs?searchtype=author&query=de+Andrade%2C+B">Bruno de Andrade</a>, 
<a href="/search/cs?searchtype=author&query=Zaid%2C+N+A">Nour Abu Zaid</a>, 
<a href="/search/cs?searchtype=author&query=Rezvani%2C+S">Samaneh Rezvani</a>, 
<a href="/search/cs?searchtype=author&query=Roders%2C+A+P">Ana Pereira Roders</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages (the paper), 15 pages (supplemental materials), references missing in the supplemental document
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1164">[1164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13414" title="Abstract">arXiv:2309.13414</a> (replaced) [<a href="/pdf/2309.13414" title="Download PDF">pdf</a>, <a href="/format/2309.13414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> State-space Models with Layer-wise Nonlinearity are Universal  Approximators with Exponential Decaying Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shida Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+B">Beichen Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 6 figures,
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item1165">[1165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13459" title="Abstract">arXiv:2309.13459</a> (replaced) [<a href="/pdf/2309.13459" title="Download PDF">pdf</a>, <a href="/format/2309.13459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Model-Agnostic Graph Neural Network for Integrating Local and Global  Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhou%2C+W">Wenzhuo Zhou</a>, 
<a href="/search/stat?searchtype=author&query=Qu%2C+A">Annie Qu</a>, 
<a href="/search/stat?searchtype=author&query=Cooper%2C+K+W">Keiland W. Cooper</a>, 
<a href="/search/stat?searchtype=author&query=Fortin%2C+N">Norbert Fortin</a>, 
<a href="/search/stat?searchtype=author&query=Shahbaba%2C+B">Babak Shahbaba</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1166">[1166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13549" title="Abstract">arXiv:2309.13549</a> (replaced) [<a href="/pdf/2309.13549" title="Download PDF">pdf</a>, <a href="/format/2309.13549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Robot 3D Perception in Urban Environments: The UT Campus  Object Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Arthur Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Eranki%2C+C">Chaitanya Eranki</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Christina Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Ji-Hwan Park</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+R">Raymond Hong</a>, 
<a href="/search/cs?searchtype=author&query=Kalyani%2C+P">Pranav Kalyani</a>, 
<a href="/search/cs?searchtype=author&query=Kalyanaraman%2C+L">Lochana Kalyanaraman</a>, 
<a href="/search/cs?searchtype=author&query=Gamare%2C+A">Arsh Gamare</a>, 
<a href="/search/cs?searchtype=author&query=Bagad%2C+A">Arnav Bagad</a>, 
<a href="/search/cs?searchtype=author&query=Esteva%2C+M">Maria Esteva</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+J">Joydeep Biswas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 18 figures, 12 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1167">[1167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13702" title="Abstract">arXiv:2309.13702</a> (replaced) [<a href="/pdf/2309.13702" title="Download PDF">pdf</a>, <a href="/ps/2309.13702" title="Download PostScript">ps</a>, <a href="/format/2309.13702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skill Check: Some Considerations on the Evaluation of Gamemastering  Models for Role-playing Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%B3ngora%2C+S">Santiago G&#xf3;ngora</a>, 
<a href="/search/cs?searchtype=author&query=Chiruzzo%2C+L">Luis Chiruzzo</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%A9ndez%2C+G">Gonzalo M&#xe9;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Gerv%C3%A1s%2C+P">Pablo Gerv&#xe1;s</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages. Accepted at GALA 2023 (Games and Learning Alliance 12th International Conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1168">[1168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13766" title="Abstract">arXiv:2309.13766</a> (replaced) [<a href="/pdf/2309.13766" title="Download PDF">pdf</a>, <a href="/ps/2309.13766" title="Download PostScript">ps</a>, <a href="/format/2309.13766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reserve Matching with Thresholds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Evren%2C+S">Suat Evren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 figures, 4 algorithms/mechanisms, 25 pages without appendix and references, 40 pages in total
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item1169">[1169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13837" title="Abstract">arXiv:2309.13837</a> (replaced) [<a href="/pdf/2309.13837" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Backorder Prediction in Inventory Management: Classification Techniques  and Cost Considerations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maitra%2C+S">Sarit Maitra</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Sukanya Kundu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, IEEE (ICSEC 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item1170">[1170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13876" title="Abstract">arXiv:2309.13876</a> (replaced) [<a href="/pdf/2309.13876" title="Download PDF">pdf</a>, <a href="/format/2309.13876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reproducing Whisper-Style Training Using an Open-Source Toolkit and  Publicly Available Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yifan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Jinchuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Brian Yan</a>, 
<a href="/search/cs?searchtype=author&query=Berrebbi%2C+D">Dan Berrebbi</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xuankai Chang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinjian Li</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiatong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Siddhant Arora</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">William Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Roshan Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wangyou Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sudo%2C+Y">Yui Sudo</a>, 
<a href="/search/cs?searchtype=author&query=Shakeel%2C+M">Muhammad Shakeel</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+J">Jee-weon Jung</a>, 
<a href="/search/cs?searchtype=author&query=Maiti%2C+S">Soumi Maiti</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1171">[1171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14327" title="Abstract">arXiv:2309.14327</a> (replaced) [<a href="/pdf/2309.14327" title="Download PDF">pdf</a>, <a href="/format/2309.14327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via  Multi-Modal Causal Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zhewei Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaoxia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Conglong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Minjia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+H">Heyang Qin</a>, 
<a href="/search/cs?searchtype=author&query=Ruwase%2C+O">Olatunji Ruwase</a>, 
<a href="/search/cs?searchtype=author&query=Awan%2C+A+A">Ammar Ahmad Awan</a>, 
<a href="/search/cs?searchtype=author&query=Rajbhandari%2C+S">Samyam Rajbhandari</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuxiong He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1172">[1172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14331" title="Abstract">arXiv:2309.14331</a> (replaced) [<a href="/pdf/2309.14331" title="Download PDF">pdf</a>, <a href="/format/2309.14331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LinGCN: Structural Linearized Graph Convolutional Network for  Homomorphically Encrypted Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hongwu Peng</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+R">Ran Ran</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yukui Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiahui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaoyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Thorat%2C+K">Kiran Thorat</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+T">Tong Geng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenghong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaolin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+W">Wujie Wen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+C">Caiwen Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 accepted publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item1173">[1173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14405" title="Abstract">arXiv:2309.14405</a> (replaced) [<a href="/pdf/2309.14405" title="Download PDF">pdf</a>, <a href="/format/2309.14405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Audio and Speech Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yuan Gong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A+H">Alexander H. Liu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Hongyin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Karlinsky%2C+L">Leonid Karlinsky</a>, 
<a href="/search/cs?searchtype=author&query=Glass%2C+J">James Glass</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ASRU 2023. Interactive demo at <a href="https://huggingface.co/spaces/yuangongfdu/ltu-2">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1174">[1174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14584" title="Abstract">arXiv:2309.14584</a> (replaced) [<a href="/pdf/2309.14584" title="Download PDF">pdf</a>, <a href="/format/2309.14584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Sparse Fast Chebyshev Transform for High-Dimensional Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jones%2C+D">Dalton Jones</a>, 
<a href="/search/math?searchtype=author&query=Letourneau%2C+P">Pierre-David Letourneau</a>, 
<a href="/search/math?searchtype=author&query=Morse%2C+M+J">Matthew J. Morse</a>, 
<a href="/search/math?searchtype=author&query=Langston%2C+M+H">M. Harper Langston</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Software (cs.MS); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1175">[1175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14681" title="Abstract">arXiv:2309.14681</a> (replaced) [<a href="/pdf/2309.14681" title="Download PDF">pdf</a>, <a href="/format/2309.14681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Human-generated Demonstrations Necessary for In-context Learning?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Rui Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoyin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1176">[1176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14709" title="Abstract">arXiv:2309.14709</a> (replaced) [<a href="/pdf/2309.14709" title="Download PDF">pdf</a>, <a href="/format/2309.14709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bootstrap Diffusion Model Curve Estimation for High Resolution Low-Light  Image Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiancheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yifan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shifeng Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1177">[1177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14922" title="Abstract">arXiv:2309.14922</a> (replaced) [<a href="/pdf/2309.14922" title="Download PDF">pdf</a>, <a href="/format/2309.14922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segment-Level Vectorized Beam Search Based on Partially Autoregressive  Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Someki%2C+M">Masao Someki</a>, 
<a href="/search/eess?searchtype=author&query=Eng%2C+N">Nicholas Eng</a>, 
<a href="/search/eess?searchtype=author&query=Higuchi%2C+Y">Yosuke Higuchi</a>, 
<a href="/search/eess?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item1178">[1178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15048" title="Abstract">arXiv:2309.15048</a> (replaced) [<a href="/pdf/2309.15048" title="Download PDF">pdf</a>, <a href="/format/2309.15048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Class Incremental Learning via Likelihood Ratio Based Task Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haowei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yijia Shao</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+W">Weinan Qian</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+N">Ningxin Pan</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yiduo Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bing Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1179">[1179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15079" title="Abstract">arXiv:2309.15079</a> (replaced) [<a href="/pdf/2309.15079" title="Download PDF">pdf</a>, <a href="/format/2309.15079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards High Efficient Long-horizon Planning with Expert-guided  Motion-encoding Tree Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+E">Erli Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaole Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cen%2C+G">Guangdu Cen</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+Z">Ziqi Zha</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+S">Senmao Qi</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+M+Q+-">Max Q.-H. Meng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1180">[1180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15104" title="Abstract">arXiv:2309.15104</a> (replaced) [<a href="/pdf/2309.15104" title="Download PDF">pdf</a>, <a href="/ps/2309.15104" title="Download PostScript">ps</a>, <a href="/format/2309.15104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient, traceable, and numerical error-free implementation of the MMS  voting rule
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%C3%A1nchez-Fern%C3%A1ndez%2C+L">Luis S&#xe1;nchez-Fern&#xe1;ndez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item1181">[1181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15111" title="Abstract">arXiv:2309.15111</a> (replaced) [<a href="/pdf/2309.15111" title="Download PDF">pdf</a>, <a href="/ps/2309.15111" title="Download PostScript">ps</a>, <a href="/format/2309.15111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SGD Finds then Tunes Features in Two-Layer Neural Networks with  near-Optimal Sample Complexity: A Case Study in the XOR problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Glasgow%2C+M">Margalit Glasgow</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1182">[1182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15188" title="Abstract">arXiv:2309.15188</a> (replaced) [<a href="/pdf/2309.15188" title="Download PDF">pdf</a>, <a href="/format/2309.15188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ICML 2023 Topological Deep Learning Challenge : Design and Results
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papillon%2C+M">Mathilde Papillon</a>, 
<a href="/search/cs?searchtype=author&query=Hajij%2C+M">Mustafa Hajij</a>, 
<a href="/search/cs?searchtype=author&query=Frantzen%2C+F">Florian Frantzen</a>, 
<a href="/search/cs?searchtype=author&query=Hoppe%2C+J">Josef Hoppe</a>, 
<a href="/search/cs?searchtype=author&query=Jenne%2C+H">Helen Jenne</a>, 
<a href="/search/cs?searchtype=author&query=Mathe%2C+J">Johan Mathe</a>, 
<a href="/search/cs?searchtype=author&query=Myers%2C+A">Audun Myers</a>, 
<a href="/search/cs?searchtype=author&query=Papamarkou%2C+T">Theodore Papamarkou</a>, 
<a href="/search/cs?searchtype=author&query=Schaub%2C+M+T">Michael T. Schaub</a>, 
<a href="/search/cs?searchtype=author&query=Zamzmi%2C+G">Ghada Zamzmi</a>, 
<a href="/search/cs?searchtype=author&query=Birdal%2C+T">Tolga Birdal</a>, 
<a href="/search/cs?searchtype=author&query=Dey%2C+T">Tamal Dey</a>, 
<a href="/search/cs?searchtype=author&query=Doster%2C+T">Tim Doster</a>, 
<a href="/search/cs?searchtype=author&query=Emerson%2C+T">Tegan Emerson</a>, 
<a href="/search/cs?searchtype=author&query=Gopalakrishnan%2C+G">Gurusankar Gopalakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Govil%2C+D">Devendra Govil</a>, 
<a href="/search/cs?searchtype=author&query=Grande%2C+V">Vincent Grande</a>, 
<a href="/search/cs?searchtype=author&query=Guzm%C3%A1n-S%C3%A1enz%2C+A">Aldo Guzm&#xe1;n-S&#xe1;enz</a>, 
<a href="/search/cs?searchtype=author&query=Kvinge%2C+H">Henry Kvinge</a>, 
<a href="/search/cs?searchtype=author&query=Livesay%2C+N">Neal Livesay</a>, 
<a href="/search/cs?searchtype=author&query=Meisner%2C+J">Jan Meisner</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Soham Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Samaga%2C+S+N">Shreyas N. Samaga</a>, 
<a href="/search/cs?searchtype=author&query=Ramamurthy%2C+K+N">Karthikeyan Natesan Ramamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Karri%2C+M+R">Maneel Reddy Karri</a>, 
<a href="/search/cs?searchtype=author&query=Rosen%2C+P">Paul Rosen</a>, 
<a href="/search/cs?searchtype=author&query=Sanborn%2C+S">Sophia Sanborn</a>, 
<a href="/search/cs?searchtype=author&query=Scholkemper%2C+M">Michael Scholkemper</a>, 
<a href="/search/cs?searchtype=author&query=Walters%2C+R">Robin Walters</a>, 
<a href="/search/cs?searchtype=author&query=Agerberg%2C+J">Jens Agerberg</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6kman%2C+G">Georg B&#xf6;kman</a>, 
<a href="/search/cs?searchtype=author&query=Barikbin%2C+S">Sadrodin Barikbin</a>, 
<a href="/search/cs?searchtype=author&query=Battiloro%2C+C">Claudio Battiloro</a>, 
<a href="/search/cs?searchtype=author&query=Bazhenov%2C+G">Gleb Bazhenov</a>, 
<a href="/search/cs?searchtype=author&query=Bernardez%2C+G">Guillermo Bernardez</a>, 
<a href="/search/cs?searchtype=author&query=Brent%2C+A">Aiden Brent</a>, 
<a href="/search/cs?searchtype=author&query=Escalera%2C+S">Sergio Escalera</a>, 
<a href="/search/cs?searchtype=author&query=Fiorellino%2C+S">Simone Fiorellino</a>, 
<a href="/search/cs?searchtype=author&query=Gavrilev%2C+D">Dmitrii Gavrilev</a>, 
<a href="/search/cs?searchtype=author&query=Hassanin%2C+M">Mohammed Hassanin</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A4usner%2C+P">Paul H&#xe4;usner</a>, 
<a href="/search/cs?searchtype=author&query=Gardaa%2C+O+H">Odin Hoff Gardaa</a>, 
<a href="/search/cs?searchtype=author&query=Khamis%2C+A">Abdelwahed Khamis</a>, 
<a href="/search/cs?searchtype=author&query=Lecha%2C+M">Manuel Lecha</a>, 
<a href="/search/cs?searchtype=author&query=Magai%2C+G">German Magai</a>, 
<a href="/search/cs?searchtype=author&query=Malygina%2C+T">Tatiana Malygina</a>, 
<a href="/search/cs?searchtype=author&query=Melnyk%2C+P">Pavlo Melnyk</a>,  et al. (18 additional authors not shown)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1183">[1183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15523" title="Abstract">arXiv:2309.15523</a> (replaced) [<a href="/pdf/2309.15523" title="Download PDF">pdf</a>, <a href="/format/2309.15523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Facade Parsing with Vision Transformers and Line Integration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bowen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ran Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunqin Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Nakashima%2C+Y">Yuta Nakashima</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1184">[1184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15641" title="Abstract">arXiv:2309.15641</a> (replaced) [<a href="/pdf/2309.15641" title="Download PDF">pdf</a>, <a href="/format/2309.15641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Exact Subgraph Matching via GNN-based Path Dominance Embedding  (Technical Report)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yutong Ye</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+X">Xiang Lian</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingsong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item1185">[1185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15786" title="Abstract">arXiv:2309.15786</a> (replaced) [<a href="/pdf/2309.15786" title="Download PDF">pdf</a>, <a href="/format/2309.15786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-based design of temporal analysis of products (TAP) reactors: A  simulated case study in oxidative propane dehydrogenation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yonge%2C+A+C">Adam C. Yonge</a>, 
<a href="/search/cs?searchtype=author&query=Gusm%C3%A3o%2C+G+S">Gabriel S. Gusm&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Fushimi%2C+R">Rebecca Fushimi</a>, 
<a href="/search/cs?searchtype=author&query=Medford%2C+A+J">A.J. Medford</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item1186">[1186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15806" title="Abstract">arXiv:2309.15806</a> (replaced) [<a href="/pdf/2309.15806" title="Download PDF">pdf</a>, <a href="/format/2309.15806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lyra: Orchestrating Dual Correction in Automated Theorem Proving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chuanyang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiankai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+H">Huajian Xin</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jianhao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yu Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tech Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1187">[1187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16157" title="Abstract">arXiv:2309.16157</a> (replaced) [<a href="/pdf/2309.16157" title="Download PDF">pdf</a>, <a href="/format/2309.16157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling Methods for Inner Product Sketching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daliri%2C+M">Majid Daliri</a>, 
<a href="/search/cs?searchtype=author&query=Freire%2C+J">Juliana Freire</a>, 
<a href="/search/cs?searchtype=author&query=Musco%2C+C">Christopher Musco</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+A">A&#xe9;cio Santos</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoxiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item1188">[1188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16188" title="Abstract">arXiv:2309.16188</a> (replaced) [<a href="/pdf/2309.16188" title="Download PDF">pdf</a>, <a href="/format/2309.16188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stackelberg Batch Policy Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhou%2C+W">Wenzhuo Zhou</a>, 
<a href="/search/stat?searchtype=author&query=Qu%2C+A">Annie Qu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1189">[1189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16203" title="Abstract">arXiv:2309.16203</a> (replaced) [<a href="/pdf/2309.16203" title="Download PDF">pdf</a>, <a href="/format/2309.16203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Cloud Strikes Back: Investigating the Decentralization of IPFS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balduf%2C+L">Leonhard Balduf</a>, 
<a href="/search/cs?searchtype=author&query=Korczy%C5%84ski%2C+M">Maciej Korczy&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Ascigil%2C+O">Onur Ascigil</a>, 
<a href="/search/cs?searchtype=author&query=Keizer%2C+N+V">Navin V. Keizer</a>, 
<a href="/search/cs?searchtype=author&query=Pavlou%2C+G">George Pavlou</a>, 
<a href="/search/cs?searchtype=author&query=Scheuermann%2C+B">Bj&#xf6;rn Scheuermann</a>, 
<a href="/search/cs?searchtype=author&query=Kr%C3%B3l%2C+M">Micha&#x142; Kr&#xf3;l</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be presented at IMC'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item1190">[1190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16263" title="Abstract">arXiv:2309.16263</a> (replaced) [<a href="/pdf/2309.16263" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cooperation Dynamics in Multi-Agent Systems: Exploring Game-Theoretic  Scenarios with Mean-Field Equilibria
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sathi%2C+V">Vaigarai Sathi</a>, 
<a href="/search/cs?searchtype=author&query=Shaik%2C+S">Sabahat Shaik</a>, 
<a href="/search/cs?searchtype=author&query=Nidamanuri%2C+J">Jaswanth Nidamanuri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for MADGames: Multi-Agent Dynamic Games Workshop at IROS 2023, see details at <a href="https://iros2023-madgames.f1tenth.org/proceedings.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1191">[1191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16298" title="Abstract">arXiv:2309.16298</a> (replaced) [<a href="/pdf/2309.16298" title="Download PDF">pdf</a>, <a href="/format/2309.16298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> At Which Training Stage Does Code Data Help LLMs Reasoning?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yingwei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuanliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Changjian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shanshan Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1192">[1192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16304" title="Abstract">arXiv:2309.16304</a> (replaced) [<a href="/pdf/2309.16304" title="Download PDF">pdf</a>, <a href="/format/2309.16304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single-Shot Lossy Compression for Joint Inference and Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%9Clger%2C+O+K">O&#x11f;uzhan Kubilay &#xdc;lger</a>, 
<a href="/search/cs?searchtype=author&query=Erkip%2C+E">Elza Erkip</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item1193">[1193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16595" title="Abstract">arXiv:2309.16595</a> (replaced) [<a href="/pdf/2309.16595" title="Download PDF">pdf</a>, <a href="/format/2309.16595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LLMs Effectively Leverage Graph Structural Information: When and Why
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingjian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Q">Qiaozhu Mei</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiaqi Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1194">[1194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16671" title="Abstract">arXiv:2309.16671</a> (replaced) [<a href="/pdf/2309.16671" title="Download PDF">pdf</a>, <a href="/format/2309.16671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demystifying CLIP Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Saining Xie</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+X+E">Xiaoqing Ellen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+P">Po-Yao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Howes%2C+R">Russell Howes</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+V">Vasu Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shang-Wen Li</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+G">Gargi Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Zettlemoyer%2C+L">Luke Zettlemoyer</a>, 
<a href="/search/cs?searchtype=author&query=Feichtenhofer%2C+C">Christoph Feichtenhofer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages. arXiv admin note: text overlap with <a href="/abs/2103.00020">arXiv:2103.00020</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1195">[1195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16737" title="Abstract">arXiv:2309.16737</a> (replaced) [<a href="/pdf/2309.16737" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Small Teams Propel Fresh Ideas in Science and Technology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yiling Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lingfei Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
</div>
</dd>
<dt><a name="item1196">[1196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16805" title="Abstract">arXiv:2309.16805</a> (replaced) [<a href="/pdf/2309.16805" title="Download PDF">pdf</a>, <a href="/ps/2309.16805" title="Download PostScript">ps</a>, <a href="/format/2309.16805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cascaded Nonlinear Control Design for Highly Underactuated Balance  Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+F">Feng Han</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+J">Jingang Yi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1197">[1197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17036" title="Abstract">arXiv:2309.17036</a> (replaced) [<a href="/pdf/2309.17036" title="Download PDF">pdf</a>, <a href="/format/2309.17036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniQuadric: A SLAM Backend for Unknown Rigid Object 3D Tracking and  Light-Weight Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linghao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yanmin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yu Deng</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+R">Rui Tian</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xinggang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Tiefeng Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1198">[1198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17053" title="Abstract">arXiv:2309.17053</a> (replaced) [<a href="/pdf/2309.17053" title="Download PDF">pdf</a>, <a href="/ps/2309.17053" title="Download PostScript">ps</a>, <a href="/format/2309.17053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lanzinger%2C+M">Matthias Lanzinger</a>, 
<a href="/search/cs?searchtype=author&query=Barcel%C3%B3%2C+P">Pablo Barcel&#xf3;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1199">[1199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17097" title="Abstract">arXiv:2309.17097</a> (replaced) [<a href="/pdf/2309.17097" title="Download PDF">pdf</a>, <a href="/format/2309.17097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Collaborative Learning Methods Cost-Effectiveness for  Prostate Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Innocenti%2C+L">Lucia Innocenti</a>, 
<a href="/search/cs?searchtype=author&query=Antonelli%2C+M">Michela Antonelli</a>, 
<a href="/search/cs?searchtype=author&query=Cremonesi%2C+F">Francesco Cremonesi</a>, 
<a href="/search/cs?searchtype=author&query=Sarhan%2C+K">Kenaan Sarhan</a>, 
<a href="/search/cs?searchtype=author&query=Granados%2C+A">Alejandro Granados</a>, 
<a href="/search/cs?searchtype=author&query=Goh%2C+V">Vicky Goh</a>, 
<a href="/search/cs?searchtype=author&query=Ourselin%2C+S">Sebastien Ourselin</a>, 
<a href="/search/cs?searchtype=author&query=Lorenzi%2C+M">Marco Lorenzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1200">[1200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17157" title="Abstract">arXiv:2309.17157</a> (replaced) [<a href="/pdf/2309.17157" title="Download PDF">pdf</a>, <a href="/format/2309.17157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LatticeGen: A Cooperative Framework which Hides Generated Text in a  Lattice for Privacy-Aware Generation on Cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengke Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianxing He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianle Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+L">Lu Mi</a>, 
<a href="/search/cs?searchtype=author&query=Mireshghallah%2C+F">Fatemehsadat Mireshghallah</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Binyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1201">[1201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17204" title="Abstract">arXiv:2309.17204</a> (replaced) [<a href="/pdf/2309.17204" title="Download PDF">pdf</a>, <a href="/format/2309.17204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bandwidth Parameterized by Cluster Vertex Deletion Number
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gima%2C+T">Tatsuya Gima</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+E+J">Eun Jung Kim</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6hler%2C+N">Noleen K&#xf6;hler</a>, 
<a href="/search/cs?searchtype=author&query=Melissinos%2C+N">Nikolaos Melissinos</a>, 
<a href="/search/cs?searchtype=author&query=Vasilakis%2C+M">Manolis Vasilakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An extended abstract of this article was presented at IPEC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item1202">[1202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17260" title="Abstract">arXiv:2309.17260</a> (replaced) [<a href="/pdf/2309.17260" title="Download PDF">pdf</a>, <a href="/format/2309.17260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PlaceNav: Topological Navigation through Place Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suomela%2C+L">Lauri Suomela</a>, 
<a href="/search/cs?searchtype=author&query=Kalliola%2C+J">Jussi Kalliola</a>, 
<a href="/search/cs?searchtype=author&query=Edelman%2C+H">Harry Edelman</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A4m%C3%A4r%C3%A4inen%2C+J">Joni-Kristian K&#xe4;m&#xe4;r&#xe4;inen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1203">[1203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17425" title="Abstract">arXiv:2309.17425</a> (replaced) [<a href="/pdf/2309.17425" title="Download PDF">pdf</a>, <a href="/format/2309.17425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Filtering Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+A">Alex Fang</a>, 
<a href="/search/cs?searchtype=author&query=Jose%2C+A+M">Albin Madappally Jose</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+A">Amit Jain</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+L">Ludwig Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Toshev%2C+A">Alexander Toshev</a>, 
<a href="/search/cs?searchtype=author&query=Shankar%2C+V">Vaishaal Shankar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1204">[1204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17437" title="Abstract">arXiv:2309.17437</a> (replaced) [<a href="/pdf/2309.17437" title="Download PDF">pdf</a>, <a href="/format/2309.17437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Decentralized Flocking Controllers with Spatio-Temporal Graph  Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siji Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanshen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peihan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Lifeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chang-Tien Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1205">[1205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17444" title="Abstract">arXiv:2309.17444</a> (replaced) [<a href="/pdf/2309.17444" title="Download PDF">pdf</a>, <a href="/format/2309.17444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-grounded Video Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lian%2C+L">Long Lian</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Baifeng Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yala%2C+A">Adam Yala</a>, 
<a href="/search/cs?searchtype=author&query=Darrell%2C+T">Trevor Darrell</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://llm-grounded-video-diffusion.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1206">[1206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17446" title="Abstract">arXiv:2309.17446</a> (replaced) [<a href="/pdf/2309.17446" title="Download PDF">pdf</a>, <a href="/format/2309.17446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> L2CEval: Evaluating Language-to-Code Generation Capabilities of Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+A">Ansong Ni</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+P">Pengcheng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yilun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Riddell%2C+M">Martin Riddell</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Troy Feng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+R">Rui Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+S">Stephen Yin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Ye Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yavuz%2C+S">Semih Yavuz</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yingbo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Radev%2C+D">Dragomir Radev</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Website: <a href="https://l2c-eval.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Programming Languages (cs.PL); Software Engineering (cs.SE)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item699">Cross-lists</a></li>
<li><a href="#item782">Replacements</a></li>
</ul>
<small>[ total of 1206 entries:  <b>1-1206</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
