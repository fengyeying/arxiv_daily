<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Fri  6 Oct 23  to  Mon  9 Oct 23, announced Tue, 10 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item771">Cross-lists</a></li>
<li><a href="#item859">Replacements</a></li>
</ul>
<small>[ total of 1366 entries:  <b>1-1366</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Tue, 10 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04421" title="Abstract">arXiv:2310.04421</a> [<a href="/pdf/2310.04421" title="Download PDF">pdf</a>, <a href="/format/2310.04421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kinetic equations and level-set approach for simulating solid-state  microstructure evolutions at the mesoscopic scale: state of the art,  limitations, and prospects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernacki%2C+M">Marc Bernacki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Materials Science (cond-mat.mtrl-sci)

</div>
<p class="mathjax">For over three decades, the front-capturing level-set method has demonstrated
its prowess for the simulation, at the mesoscopic scale, of numerous mechanisms
in the context of microstructure evolution occurring during complex
thermomechanical paths. This review delves into the foundations of this
numerical framework, charting its evolution concerning polycrystalline
materials, examining its recent advancements, scrutinizing its current
shortcomings, and exploring future possibilities. Special attention will be
given to the context of hot metal forming processes. In this context, this
article also aims to reintroduce, as simply as possible, the kinetic equations
related to the grain boundary migration.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04422" title="Abstract">arXiv:2310.04422</a> [<a href="/pdf/2310.04422" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Qualitative and quantitative evaluation of a methodology for the Digital  Twin creation of brownfield production systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Braun%2C+D">Dominik Braun</a>, 
<a href="/search/cs?searchtype=author&query=Jazdi%2C+N">Nasser Jazdi</a>, 
<a href="/search/cs?searchtype=author&query=Schloegl%2C+W">Wolfgang Schloegl</a>, 
<a href="/search/cs?searchtype=author&query=Weyrich%2C+M">Michael Weyrich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, 7 pages, 8 figures, Accepted at IEEE ETFA 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The Digital Twin is a well-known concept of industry 4.0 and is the cyber
part of a cyber-physical production system providing several benefits such as
virtual commissioning or predictive maintenance. The existing production
systems are lacking a Digital Twin which has to be created manually in a
time-consuming and error-prone process. Therefore, methods to create digital
models of existing production systems and their relations between them were
developed. This paper presents the implementation of the methodology for the
creation of multi-disciplinary relations and a quantitative and qualitative
evaluation of the benefits of the methodology.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04424" title="Abstract">arXiv:2310.04424</a> [<a href="/pdf/2310.04424" title="Download PDF">pdf</a>, <a href="/format/2310.04424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability Analysis of Non-Linear Classifiers using Gene Regulatory  Neural Network for Biological AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ratwatte%2C+A">Adrian Ratwatte</a>, 
<a href="/search/cs?searchtype=author&query=Somathilaka%2C+S">Samitha Somathilaka</a>, 
<a href="/search/cs?searchtype=author&query=Balasubramaniam%2C+S">Sasitharan Balasubramaniam</a>, 
<a href="/search/cs?searchtype=author&query=Gilad%2C+A+A">Assaf A. Gilad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Molecular Networks (q-bio.MN)

</div>
<p class="mathjax">The Gene Regulatory Network (GRN) of biological cells governs a number of key
functionalities that enables them to adapt and survive through different
environmental conditions. Close observation of the GRN shows that the structure
and operational principles resembles an Artificial Neural Network (ANN), which
can pave the way for the development of Biological Artificial Intelligence. In
particular, a gene's transcription and translation process resembles a
sigmoidal-like property based on transcription factor inputs. In this paper, we
develop a mathematical model of gene-perceptron using a dual-layered
transcription-translation chemical reaction model, enabling us to transform a
GRN into a Gene Regulatory Neural Network (GRNN). We perform stability analysis
for each gene-perceptron within the fully-connected GRNN sub network to
determine temporal as well as stable concentration outputs that will result in
reliable computing performance. We focus on a non-linear classifier application
for the GRNN, where we analyzed generic multi-layer GRNNs as well as E.Coli
GRNN that is derived from trans-omic experimental data. Our analysis found that
varying the parameters of the chemical reactions can allow us shift the
boundaries of the classification region, laying the platform for programmable
GRNNs that suit diverse application requirements.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04425" title="Abstract">arXiv:2310.04425</a> [<a href="/pdf/2310.04425" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Red Teaming Generative AI/NLP, the BB84 quantum cryptography protocol  and the NIST-approved Quantum-Resistant Cryptographic Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Radanliev%2C+P">Petar Radanliev</a>, 
<a href="/search/cs?searchtype=author&query=De+Roure%2C+D">David De Roure</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+O">Omar Santos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Cryptography and Security (cs.CR); Emerging Technologies (cs.ET); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the contemporary digital age, Quantum Computing and Artificial
Intelligence (AI) convergence is reshaping the cyber landscape, introducing
unprecedented opportunities and potential vulnerabilities.This research,
conducted over five years, delves into the cybersecurity implications of this
convergence, with a particular focus on AI/Natural Language Processing (NLP)
models and quantum cryptographic protocols, notably the BB84 method and
specific NIST-approved algorithms. Utilising Python and C++ as primary
computational tools, the study employs a "red teaming" approach, simulating
potential cyber-attacks to assess the robustness of quantum security measures.
Preliminary research over 12 months laid the groundwork, which this study seeks
to expand upon, aiming to translate theoretical insights into actionable,
real-world cybersecurity solutions. Located at the University of Oxford's
technology precinct, the research benefits from state-of-the-art infrastructure
and a rich collaborative environment. The study's overarching goal is to ensure
that as the digital world transitions to quantum-enhanced operations, it
remains resilient against AI-driven cyber threats. The research aims to foster
a safer, quantum-ready digital future through iterative testing, feedback
integration, and continuous improvement. The findings are intended for broad
dissemination, ensuring that the knowledge benefits academia and the global
community, emphasising the responsible and secure harnessing of quantum
technology.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04426" title="Abstract">arXiv:2310.04426</a> [<a href="/pdf/2310.04426" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Research Funding in the Middle East and North Africa: Analyses of  Acknowledgments in Scientific Publications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=El-Ouahi%2C+J">Jamal El-Ouahi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 4 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">This study uses a mixed-methods approach to analyze the funding
acknowledgments found in 2.3 million scientific publications published between
2008 and 2021 by authors affiliated with research institutions located in
Middle Eastern and North African (MENA) countries. The aim is to identify the
major funders and their contribution to national scientific publications but
also to better understand the funding mechanism in relation to collaboration
and publication. Publication data from the Web of Science is examined to
provide key insights about funding activities. Saudi Arabia and Qatar lead the
region with about half of their publications of funding sources but also
because most countries in MENA show strong linkages with foreign agencies which
are mainly due to a high level of international collaborations. The distinction
between domestic and international publications reveals some differences in
terms of funding structures. For instance, Turkey and Iran are dominated by one
or two major funders whereas Saudi Arabia is an example of countries with
multiple funders. Iran and Kuwait are examples of countries where research is
mainly funded by domestic agencies. The government and academic sectors mainly
fund scientific research in MENA whereas the industry sector plays little or no
role in terms of research funding. Lastly, the qualitative analyses provide
more context into the complex funding mechanism. The findings of this study
contribute to a better understanding of the funding structure in MENA countries
and provide insights to funders and research managers to evaluate the funding
landscape.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04427" title="Abstract">arXiv:2310.04427</a> [<a href="/pdf/2310.04427" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI in the Construction Industry: Opportunities &amp; Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghimire%2C+P">Prashnna Ghimire</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kyungki Kim</a>, 
<a href="/search/cs?searchtype=author&query=Acharya%2C+M">Manoj Acharya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the last decade, despite rapid advancements in artificial intelligence
(AI) transforming many industry practices, construction largely lags in
adoption. Recently, the emergence and rapid adoption of advanced large language
models (LLM) like OpenAI's GPT, Google's PaLM, and Meta's Llama have shown
great potential and sparked considerable global interest. However, the current
surge lacks a study investigating the opportunities and challenges of
implementing Generative AI (GenAI) in the construction sector, creating a
critical knowledge gap for researchers and practitioners. This underlines the
necessity to explore the prospects and complexities of GenAI integration.
Bridging this gap is fundamental to optimizing GenAI's early-stage adoption
within the construction sector. Given GenAI's unprecedented capabilities to
generate human-like content based on learning from existing content, we reflect
on two guiding questions: What will the future bring for GenAI in the
construction industry? What are the potential opportunities and challenges in
implementing GenAI in the construction industry? This study delves into
reflected perception in literature, analyzes the industry perception using
programming-based word cloud and frequency analysis, and integrates authors'
opinions to answer these questions. This paper recommends a conceptual GenAI
implementation framework, provides practical recommendations, summarizes future
research questions, and builds foundational literature to foster subsequent
research expansion in GenAI within the construction and its allied architecture
&amp; engineering domains.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04429" title="Abstract">arXiv:2310.04429</a> [<a href="/pdf/2310.04429" title="Download PDF">pdf</a>, <a href="/format/2310.04429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NetDiffus: Network Traffic Generation by Diffusion Models through  Time-Series Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sivaroopan%2C+N">Nirhoshan Sivaroopan</a>, 
<a href="/search/cs?searchtype=author&query=Bandara%2C+D">Dumindu Bandara</a>, 
<a href="/search/cs?searchtype=author&query=Madarasingha%2C+C">Chamara Madarasingha</a>, 
<a href="/search/cs?searchtype=author&query=Jourjon%2C+G">Guilluame Jourjon</a>, 
<a href="/search/cs?searchtype=author&query=Jayasumana%2C+A">Anura Jayasumana</a>, 
<a href="/search/cs?searchtype=author&query=Thilakarathna%2C+K">Kanchana Thilakarathna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages and 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Network data analytics are now at the core of almost every networking
solution. Nonetheless, limited access to networking data has been an enduring
challenge due to many reasons including complexity of modern networks,
commercial sensitivity, privacy and regulatory constraints. In this work, we
explore how to leverage recent advancements in Diffusion Models (DM) to
generate synthetic network traffic data. We develop an end-to-end framework -
NetDiffus that first converts one-dimensional time-series network traffic into
two-dimensional images, and then synthesizes representative images for the
original data. We demonstrate that NetDiffus outperforms the state-of-the-art
traffic generation methods based on Generative Adversarial Networks (GANs) by
providing 66.4% increase in fidelity of the generated data and 18.1% increase
in downstream machine learning tasks. We evaluate NetDiffus on seven diverse
traffic traces and show that utilizing synthetic data significantly improves
traffic fingerprinting, anomaly detection and traffic classification.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04431" title="Abstract">arXiv:2310.04431</a> [<a href="/pdf/2310.04431" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can neural networks count digit frequency?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khandelwal%2C+P">Padmaksh Khandelwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and Datasets: <a href="https://github.com/PadmakshKhandelwal/Can-neural-networks-count">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this research, we aim to compare the performance of different classical
machine learning models and neural networks in identifying the frequency of
occurrence of each digit in a given number. It has various applications in
machine learning and computer vision, e.g. for obtaining the frequency of a
target object in a visual scene. We considered this problem as a hybrid of
classification and regression tasks. We carefully create our own datasets to
observe systematic differences between different methods. We evaluate each of
the methods using different metrics across multiple datasets.The metrics of
performance used were the root mean squared error and mean absolute error for
regression evaluation, and accuracy for classification performance evaluation.
We observe that decision trees and random forests overfit to the dataset, due
to their inherent bias, and are not able to generalize well. We also observe
that the neural networks significantly outperform the classical machine
learning models in terms of both the regression and classification metrics for
both the 6-digit and 10-digit number datasets. Dataset and code are available
on github.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04432" title="Abstract">arXiv:2310.04432</a> [<a href="/pdf/2310.04432" title="Download PDF">pdf</a>, <a href="/format/2310.04432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training-free Linear Image Inversion via Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pokle%2C+A">Ashwini Pokle</a>, 
<a href="/search/cs?searchtype=author&query=Muckley%2C+M+J">Matthew J. Muckley</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R+T+Q">Ricky T. Q. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Karrer%2C+B">Brian Karrer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Training-free linear inversion involves the use of a pretrained generative
model and -- through appropriate modifications to the generation process --
solving inverse problems without any finetuning of the generative model. While
recent prior methods have explored the use of diffusion models, they still
require the manual tuning of many hyperparameters for different inverse
problems. In this work, we propose a training-free method for image inversion
using pretrained flow models, leveraging the simplicity and efficiency of Flow
Matching models, using theoretically-justified weighting schemes and thereby
significantly reducing the amount of manual tuning. In particular, we draw
inspiration from two main sources: adopting prior gradient correction methods
to the flow regime, and a solver scheme based on conditional Optimal Transport
paths. As pretrained diffusion models are widely accessible, we also show how
to practically adapt diffusion models for our method. Empirically, our approach
requires no problem-specific tuning across an extensive suite of noisy linear
image inversion problems on high-dimensional datasets, ImageNet-64/128 and
AFHQ-256, and we observe that our flow-based method for image inversion
significantly improves upon closely-related diffusion-based linear inversion
methods.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04436" title="Abstract">arXiv:2310.04436</a> [<a href="/pdf/2310.04436" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Control of an Inverted Pendulum by a Reinforcement  Learning-based LQR Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yildiran%2C+U">Ugur Yildiran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Inverted pendulums constitute one of the popular systems for benchmarking
control algorithms. Several methods have been proposed for the control of this
system, the majority of which rely on the availability of a mathematical model.
However, deriving a mathematical model using physical parameters or system
identification techniques requires manual effort. Moreover, the designed
controllers may perform poorly if system parameters change. To mitigate these
problems, recently, some studies used Reinforcement Learning (RL) based
approaches for the control of inverted pendulum systems. Unfortunately, these
methods suffer from slow convergence and local minimum problems. Moreover, they
may require hyperparameter tuning which complicates the design process
significantly. To alleviate these problems, the present study proposes an
LQR-based RL method for adaptive balancing control of an inverted pendulum. As
shown by numerical experiments, the algorithm stabilizes the system very fast
without requiring a mathematical model or extensive hyperparameter tuning. In
addition, it can adapt to parametric changes online.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04437" title="Abstract">arXiv:2310.04437</a> [<a href="/pdf/2310.04437" title="Download PDF">pdf</a>, <a href="/format/2310.04437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extended superposition theorem under power grid topological changes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Marot%2C+A">Antoine Marot</a>, 
<a href="/search/eess?searchtype=author&query=Donnot%2C+B">Benjamin Donnot</a>, 
<a href="/search/eess?searchtype=author&query=Henka%2C+N">Noureddine Henka</a>, 
<a href="/search/eess?searchtype=author&query=Tazi%2C+S">Sami Tazi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Standard superposition theorem has been the basis in the last decades for
many power system problems decomposition involving changes in nodal injections,
from productions and loads. Its application scope has however been limited to
fixed grid topology and breaks as soon as a topology change happens in the
grid. For instance, it cannot be applied to compute N-2 power flows simply from
N-1 security analysis. Topological changes also become a flexibility used more
and more frequently for congestion management. Studying the effect of
combinatorial topological changes is hence of interest, but so far very
computation intensive. In this paper, we propose an extension of the
superposition theorem to varying grid topologies. We demonstrate it under the
DC approximation for all topological changes, namely line disconnection and
reconnection, bus splitting and merging. We finally apply it to two use cases
related to the above mentioned, effectively extending its scope of application.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04438" title="Abstract">arXiv:2310.04438</a> [<a href="/pdf/2310.04438" title="Download PDF">pdf</a>, <a href="/ps/2310.04438" title="Download PostScript">ps</a>, <a href="/format/2310.04438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Brief History of Prompt: Leveraging Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muktadir%2C+G+M">Golam Md Muktadir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents a comprehensive exploration of the evolution of prompt
engineering and generation in the field of natural language processing (NLP).
Starting from the early language models and information retrieval systems, we
trace the key developments that have shaped prompt engineering over the years.
The introduction of attention mechanisms in 2015 revolutionized language
understanding, leading to advancements in controllability and
context-awareness. Subsequent breakthroughs in reinforcement learning
techniques further enhanced prompt engineering, addressing issues like exposure
bias and biases in generated text. We examine the significant contributions in
2018 and 2019, focusing on fine-tuning strategies, control codes, and
template-based generation. The paper also discusses the growing importance of
fairness, human-AI collaboration, and low-resource adaptation. In 2020 and
2021, contextual prompting and transfer learning gained prominence, while 2022
and 2023 witnessed the emergence of advanced techniques like unsupervised
pre-training and novel reward shaping. Throughout the paper, we reference
specific research studies that exemplify the impact of various developments on
prompt engineering. The journey of prompt engineering continues, with ethical
considerations being paramount for the responsible and inclusive future of AI
systems.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04440" title="Abstract">arXiv:2310.04440</a> [<a href="/pdf/2310.04440" title="Download PDF">pdf</a>, <a href="/format/2310.04440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Facilitating Battery Swapping Services for Freight Trucks with  Spatial-Temporal Demand Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+L">Linyu Liu</a>, 
<a href="/search/eess?searchtype=author&query=Dai%2C+Z">Zhen Dai</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+S">Shiji Song</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xiaocheng Li</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+G">Guanting Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Electrifying heavy-duty trucks offers a substantial opportunity to curtail
carbon emissions, advancing toward a carbon-neutral future. However, the
inherent challenges of limited battery energy and the sheer weight of
heavy-duty trucks lead to reduced mileage and prolonged charging durations.
Consequently, battery-swapping services emerge as an attractive solution for
these trucks. This paper employs a two-fold approach to investigate the
potential and enhance the efficacy of such services. Firstly, spatial-temporal
demand prediction models are adopted to predict the traffic patterns for the
upcoming hours. Subsequently, the prediction guides an optimization module for
efficient battery allocation and deployment. Analyzing the heavy-duty truck
data on a highway network spanning over 2,500 miles, our model and analysis
underscore the value of prediction/machine learning in facilitating future
decision-makings. In particular, we find that the initial phase of implementing
battery-swapping services favors mobile battery-swapping stations, but as the
system matures, fixed-location stations are preferred.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04443" title="Abstract">arXiv:2310.04443</a> [<a href="/pdf/2310.04443" title="Download PDF">pdf</a>, <a href="/format/2310.04443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human Mobility Question Answering (Vision Paper)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+H">Hao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Salim%2C+F+D">Flora D. Salim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Question answering (QA) systems have attracted much attention from the
artificial intelligence community as they can learn to answer questions based
on the given knowledge source (e.g., images in visual question answering).
However, the research into question answering systems with human mobility data
remains unexplored. Mining human mobility data is crucial for various
applications such as smart city planning, pandemic management, and personalised
recommendation system. In this paper, we aim to tackle this gap and introduce a
novel task, that is, human mobility question answering (MobQA). The aim of the
task is to let the intelligent system learn from mobility data and answer
related questions. This task presents a new paradigm change in mobility
prediction research and further facilitates the research of human mobility
recommendation systems. To better support this novel research topic, this
vision paper also proposes an initial design of the dataset and a potential
deep learning model framework for the introduced MobQA task. We hope that this
paper will provide novel insights and open new directions in human mobility
research and question answering research.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04444" title="Abstract">arXiv:2310.04444</a> [<a href="/pdf/2310.04444" title="Download PDF">pdf</a>, <a href="/format/2310.04444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What&#x27;s the Magic Word? A Control Theory of LLM Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhargava%2C+A">Aman Bhargava</a>, 
<a href="/search/cs?searchtype=author&query=Witkowski%2C+C">Cameron Witkowski</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+M">Manav Shah</a>, 
<a href="/search/cs?searchtype=author&query=Thomson%2C+M">Matt Thomson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 8 figures. Under review for ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Prompt engineering is effective and important in the deployment of LLMs but
is poorly understood mathematically. Here, we formalize prompt engineering as
an optimal control problem on LLMs -- where the prompt is considered a control
variable for modulating the output distribution of the LLM. Within this
framework, we ask a simple question: given a sequence of tokens, does there
always exist a prompt we can prepend that will steer the LLM toward accurately
predicting the final token? We call such an optimal prompt the magic word since
prepending the prompt causes the LLM to output the correct answer. If magic
words exist, can we find them? If so, what are their properties? We offer
analytic analysis on the controllability of the self-attention head where we
prove a bound on controllability as a function of the singular values of its
weight matrices. We take inspiration from control theory to propose a metric
called $k-\epsilon$ controllability to characterize LLM steerability. We
compute the $k-\epsilon$ controllability of a panel of large language models,
including Falcon-7b, Llama-7b, and Falcon-40b on 5000 WikiText causal language
modeling tasks. Remarkably, we find that magic words of 10 tokens or less exist
for over 97% of WikiText instances surveyed for each model.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04445" title="Abstract">arXiv:2310.04445</a> [<a href="/pdf/2310.04445" title="Download PDF">pdf</a>, <a href="/format/2310.04445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoFT: Local Proxy Fine-tuning For Improving Transferability Of  Adversarial Attacks Against Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+M+A">Muhammad Ahmed Shah</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Roshan Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Dhamyal%2C+H">Hira Dhamyal</a>, 
<a href="/search/cs?searchtype=author&query=Olivier%2C+R">Raphael Olivier</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+A">Ankit Shah</a>, 
<a href="/search/cs?searchtype=author&query=Alharthi%2C+D">Dareen Alharthi</a>, 
<a href="/search/cs?searchtype=author&query=Bukhari%2C+H+T">Hazim T Bukhari</a>, 
<a href="/search/cs?searchtype=author&query=Baali%2C+M">Massa Baali</a>, 
<a href="/search/cs?searchtype=author&query=Deshmukh%2C+S">Soham Deshmukh</a>, 
<a href="/search/cs?searchtype=author&query=Kuhlmann%2C+M">Michael Kuhlmann</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Rita Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">It has been shown that Large Language Model (LLM) alignments can be
circumvented by appending specially crafted attack suffixes with harmful
queries to elicit harmful responses. To conduct attacks against private target
models whose characterization is unknown, public models can be used as proxies
to fashion the attack, with successful attacks being transferred from public
proxies to private target models. The success rate of attack depends on how
closely the proxy model approximates the private model. We hypothesize that for
attacks to be transferrable, it is sufficient if the proxy can approximate the
target model in the neighborhood of the harmful query. Therefore, in this
paper, we propose \emph{Local Fine-Tuning (LoFT)}, \textit{i.e.}, fine-tuning
proxy models on similar queries that lie in the lexico-semantic neighborhood of
harmful queries to decrease the divergence between the proxy and target models.
First, we demonstrate three approaches to prompt private target models to
obtain similar queries given harmful queries. Next, we obtain data for local
fine-tuning by eliciting responses from target models for the generated similar
queries. Then, we optimize attack suffixes to generate attack prompts and
evaluate the impact of our local fine-tuning on the attack's success rate.
Experiments show that local fine-tuning of proxy models improves attack
transferability and increases attack success rate by $39\%$, $7\%$, and $0.5\%$
(absolute) on target models ChatGPT, GPT-4, and Claude respectively.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04447" title="Abstract">arXiv:2310.04447</a> [<a href="/pdf/2310.04447" title="Download PDF">pdf</a>, <a href="/format/2310.04447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Conflict Detection in IoT-based Smart Homes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Bing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chaki%2C+D">Dipankar Chaki</a>, 
<a href="/search/cs?searchtype=author&query=Bouguettaya%2C+A">Athman Bouguettaya</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+K">Kwok-Yan Lam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 4 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">As the adoption of IoT-based smart homes continues to grow, the importance of
addressing potential conflicts becomes increasingly vital for ensuring seamless
functionality and user satisfaction. In this survey, we introduce a novel
conflict taxonomy, complete with formal definitions of each conflict type that
may arise within the smart home environment. We design an advanced conflict
model to effectively categorize these conflicts, setting the stage for our
in-depth review of recent research in the field. By employing our proposed
model, we systematically classify conflicts and present a comprehensive
overview of cutting-edge conflict detection approaches. This extensive analysis
allows us to highlight similarities, clarify significant differences, and
uncover prevailing trends in conflict detection techniques. In conclusion, we
shed light on open issues and suggest promising avenues for future research to
foster accelerated development and deployment of IoT-based smart homes,
ultimately enhancing their overall performance and user experience.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04448" title="Abstract">arXiv:2310.04448</a> [<a href="/pdf/2310.04448" title="Download PDF">pdf</a>, <a href="/format/2310.04448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fault Trees, Decision Trees, And Binary Decision Diagrams: A Systematic  Comparison
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jimenez-Roa%2C+L+A">L.A. Jimenez-Roa</a>, 
<a href="/search/cs?searchtype=author&query=Heskes%2C+T">T. Heskes</a>, 
<a href="/search/cs?searchtype=author&query=Stoelinga%2C+M">M. Stoelinga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Computer Science (cs.OH)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">In reliability engineering, we need to understand system dependencies,
cause-effect relations, identify critical components, and analyze how they
trigger failures. Three prominent graph models commonly used for these purposes
are fault trees (FTs), decision trees (DTs), and binary decision diagrams
(BDDs). These models are popular because they are easy to interpret, serve as a
communication tool between stakeholders of various backgrounds, and support
decision-making processes. Moreover, these models help to understand real-world
problems by computing reliability metrics, minimum cut sets, logic rules, and
displaying dependencies. Nevertheless, it is unclear how these graph models
compare. Thus, the goal of this paper is to understand the similarities and
differences through a systematic comparison based on their (i) purpose and
application, (ii) structural representation, (iii) analysis methods, (iv)
construction, and (v) benefits &amp; limitations. Furthermore, we use a running
example based on a Container Seal Design to showcase the models in practice.
Our results show that, given that FTs, DTs and BDDs have different purposes and
application domains, they adopt different structural representations and
analysis methodologies that entail a variety of benefits and limitations, the
latter can be addressed via conversion methods or extensions. Specific remarks
are that BDDs can be considered as a compact representation of binary DTs,
since the former allows sub-node sharing, which makes BDDs more efficient at
representing logical rules than binary DTs. It is possible to obtain cut sets
from BDDs and DTs and construct a FT using the (con/dis)junctive normal form,
although this may result in a sub-optimal FT structure.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04450" title="Abstract">arXiv:2310.04450</a> [<a href="/pdf/2310.04450" title="Download PDF">pdf</a>, <a href="/format/2310.04450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Large Language Models&#x27; Perception of Emotion Using  Appraisal Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yongsatianchot%2C+N">Nutchanon Yongsatianchot</a>, 
<a href="/search/cs?searchtype=author&query=Torshizi%2C+P+G">Parisa Ghanad Torshizi</a>, 
<a href="/search/cs?searchtype=author&query=Marsella%2C+S">Stacy Marsella</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 11th International Conference on Affective Computing and
  Intelligent Interaction Workshop and Demo (ACIIW) 2023 1-8
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLM) like ChatGPT have significantly advanced in
recent years and are now being used by the general public. As more people
interact with these systems, improving our understanding of these black box
models is crucial, especially regarding their understanding of human
psychological aspects. In this work, we investigate their emotion perception
through the lens of appraisal and coping theory using the Stress and Coping
Process Questionaire (SCPQ). SCPQ is a validated clinical instrument consisting
of multiple stories that evolve over time and differ in key appraisal variables
such as controllability and changeability. We applied SCPQ to three recent LLMs
from OpenAI, davinci-003, ChatGPT, and GPT-4 and compared the results with
predictions from the appraisal theory and human data. The results show that
LLMs' responses are similar to humans in terms of dynamics of appraisal and
coping, but their responses did not differ along key appraisal dimensions as
predicted by the theory and data. The magnitude of their responses is also
quite different from humans in several variables. We also found that GPTs can
be quite sensitive to instruction and how questions are asked. This work adds
to the growing literature evaluating the psychological aspects of LLMs and
helps enrich our understanding of the current models.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04451" title="Abstract">arXiv:2310.04451</a> [<a href="/pdf/2310.04451" title="Download PDF">pdf</a>, <a href="/format/2310.04451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaogeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Nan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaowei Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print, code is available at <a href="https://github.com/SheltonLiu-N/AutoDAN">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The aligned Large Language Models (LLMs) are powerful language understanding
and decision-making tools that are created through extensive alignment with
human feedback. However, these large models remain susceptible to jailbreak
attacks, where adversaries manipulate prompts to elicit malicious outputs that
should not be given by aligned LLMs. Investigating jailbreak prompts can lead
us to delve into the limitations of LLMs and further guide us to secure them.
Unfortunately, existing jailbreak techniques suffer from either (1) scalability
issues, where attacks heavily rely on manual crafting of prompts, or (2)
stealthiness problems, as attacks depend on token-based algorithms to generate
prompts that are often semantically meaningless, making them susceptible to
detection through basic perplexity testing. In light of these challenges, we
intend to answer this question: Can we develop an approach that can
automatically generate stealthy jailbreak prompts? In this paper, we introduce
AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can
automatically generate stealthy jailbreak prompts by the carefully designed
hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN
not only automates the process while preserving semantic meaningfulness, but
also demonstrates superior attack strength in cross-model transferability, and
cross-sample universality compared with the baseline. Moreover, we also compare
AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass
them effectively.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04452" title="Abstract">arXiv:2310.04452</a> [<a href="/pdf/2310.04452" title="Download PDF">pdf</a>, <a href="/format/2310.04452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Short text classification with machine learning in the social sciences:  The case of climate change on Twitter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shyrokykh%2C+K">Karina Shyrokykh</a>, 
<a href="/search/cs?searchtype=author&query=Girnyk%2C+M">Maksym Girnyk</a>, 
<a href="/search/cs?searchtype=author&query=Dellmuth%2C+L">Lisa Dellmuth</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> PLoS ONE 18(9): e0290762 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">To analyse large numbers of texts, social science researchers are
increasingly confronting the challenge of text classification. When manual
labeling is not possible and researchers have to find automatized ways to
classify texts, computer science provides a useful toolbox of machine-learning
methods whose performance remains understudied in the social sciences. In this
article, we compare the performance of the most widely used text classifiers by
applying them to a typical research scenario in social science research: a
relatively small labeled dataset with infrequent occurrence of categories of
interest, which is a part of a large unlabeled dataset. As an example case, we
look at Twitter communication regarding climate change, a topic of increasing
scholarly interest in interdisciplinary social science research. Using a novel
dataset including 5,750 tweets from various international organizations
regarding the highly ambiguous concept of climate change, we evaluate the
performance of methods in automatically classifying tweets based on whether
they are about climate change or not. In this context, we highlight two main
findings. First, supervised machine-learning methods perform better than
state-of-the-art lexicons, in particular as class balance increases. Second,
traditional machine-learning methods, such as logistic regression and random
forest, perform similarly to sophisticated deep-learning methods, whilst
requiring much less training time and computational resources. The results have
important implications for the analysis of short texts in social science
research.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04453" title="Abstract">arXiv:2310.04453</a> [<a href="/pdf/2310.04453" title="Download PDF">pdf</a>, <a href="/format/2310.04453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COVID-19 South African Vaccine Hesitancy Models Show Boost in  Performance Upon Fine-Tuning on M-pox Tweets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perikli%2C+N">Nicholas Perikli</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S">Srimoy Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Ogbuokiri%2C+B">Blessing Ogbuokiri</a>, 
<a href="/search/cs?searchtype=author&query=Nia%2C+Z+M">Zahra Movahedi Nia</a>, 
<a href="/search/cs?searchtype=author&query=Lieberman%2C+B">Benjamin Lieberman</a>, 
<a href="/search/cs?searchtype=author&query=Tripathi%2C+N">Nidhi Tripathi</a>, 
<a href="/search/cs?searchtype=author&query=Dahbi%2C+S">Salah-Eddine Dahbi</a>, 
<a href="/search/cs?searchtype=author&query=Stevenson%2C+F">Finn Stevenson</a>, 
<a href="/search/cs?searchtype=author&query=Bragazzi%2C+N">Nicola Bragazzi</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+J">Jude Kong</a>, 
<a href="/search/cs?searchtype=author&query=Mellado%2C+B">Bruce Mellado</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Very large numbers of M-pox cases have, since the start of May 2022, been
reported in non-endemic countries leading many to fear that the M-pox Outbreak
would rapidly transition into another pandemic, while the COVID-19 pandemic
ravages on. Given the similarities of M-pox with COVID-19, we chose to test the
performance of COVID-19 models trained on South African twitter data on a
hand-labelled M-pox dataset before and after fine-tuning. More than 20k
M-pox-related tweets from South Africa were hand-labelled as being either
positive, negative or neutral. After fine-tuning these COVID-19 models on the
M-pox dataset, the F1-scores increased by more than 8% falling just short of
70%, but still outperforming state-of-the-art models and well-known
classification algorithms. An LDA-based topic modelling procedure was used to
compare the miss-classified M-pox tweets of the original COVID-19 RoBERTa model
with its fine-tuned version, and from this analysis, we were able to draw
conclusions on how to build more sophisticated models.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04454" title="Abstract">arXiv:2310.04454</a> [<a href="/pdf/2310.04454" title="Download PDF">pdf</a>, <a href="/format/2310.04454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spherical Position Encoding for Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Unlu%2C+E">Eren Unlu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Position encoding is the primary mechanism which induces notion of sequential
order for input tokens in transformer architectures. Even though this
formulation in the original transformer paper has yielded plausible performance
for general purpose language understanding and generation, several new
frameworks such as Rotary Position Embedding (RoPE) are proposed for further
enhancement. In this paper, we introduce the notion of "geotokens" which are
input elements for transformer architectures, each representing an information
related to a geological location. Unlike the natural language the sequential
position is not important for the model but the geographical coordinates are.
In order to induce the concept of relative position for such a setting and
maintain the proportion between the physical distance and distance on embedding
space, we formulate a position encoding mechanism based on RoPE architecture
which is adjusted for spherical coordinates.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04455" title="Abstract">arXiv:2310.04455</a> [<a href="/pdf/2310.04455" title="Download PDF">pdf</a>, <a href="/format/2310.04455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inclusive Data Representation in Federated Learning: A Novel Approach  Integrating Textual and Visual Prompt
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zihao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhenpeng Shi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wenbo Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Federated Learning (FL) is often impeded by communication overhead issues.
Prompt tuning, as a potential solution, has been introduced to only adjust a
few trainable parameters rather than the whole model. However, current
single-modality prompt tuning approaches fail to comprehensively portray local
clients' data. To overcome this limitation, we present Twin Prompt Federated
learning (TPFL), a pioneering solution that integrates both visual and textual
modalities, ensuring a more holistic representation of local clients' data
characteristics. Furthermore, in order to tackle the data heterogeneity issues,
we introduce the Augmented TPFL (ATPFL) employing the contrastive learning to
TPFL, which not only enhances the global knowledge acquisition of client models
but also fosters the development of robust, compact models. The effectiveness
of TPFL and ATPFL is substantiated by our extensive evaluations, consistently
showing superior performance compared to all baselines.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04456" title="Abstract">arXiv:2310.04456</a> [<a href="/pdf/2310.04456" title="Download PDF">pdf</a>, <a href="/format/2310.04456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Prompt Transformer with Hybrid Contrastive Learning for  Emotion Recognition in Conversation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+S">Shihao Zou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xianying Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xudong Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Emotion Recognition in Conversation (ERC) plays an important role in driving
the development of human-machine interaction. Emotions can exist in multiple
modalities, and multimodal ERC mainly faces two problems: (1) the noise problem
in the cross-modal information fusion process, and (2) the prediction problem
of less sample emotion labels that are semantically similar but different
categories. To address these issues and fully utilize the features of each
modality, we adopted the following strategies: first, deep emotion cues
extraction was performed on modalities with strong representation ability, and
feature filters were designed as multimodal prompt information for modalities
with weak representation ability. Then, we designed a Multimodal Prompt
Transformer (MPT) to perform cross-modal information fusion. MPT embeds
multimodal fusion information into each attention layer of the Transformer,
allowing prompt information to participate in encoding textual features and
being fused with multi-level textual information to obtain better multimodal
fusion features. Finally, we used the Hybrid Contrastive Learning (HCL)
strategy to optimize the model's ability to handle labels with few samples.
This strategy uses unsupervised contrastive learning to improve the
representation ability of multimodal fusion and supervised contrastive learning
to mine the information of labels with few samples. Experimental results show
that our proposed model outperforms state-of-the-art models in ERC on two
benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04459" title="Abstract">arXiv:2310.04459</a> [<a href="/pdf/2310.04459" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extended Kalman Filter State Estimation for Autonomous Competition  Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kou%2C+E">Ethan Kou</a>, 
<a href="/search/eess?searchtype=author&query=Haggenmiller%2C+A">Acshi Haggenmiller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Autonomous mobile robot competitions judge based on a robot's ability to
quickly and accurately navigate the game field. This means accurate
localization is crucial for creating an autonomous competition robot. Two
common localization methods are odometry and computer vision landmark
detection. Odometry provides frequent velocity measurements, while landmark
detection provides infrequent position measurements. The state can also be
predicted with a physics model. These three types of localization can be
"fused" to create a more accurate state estimate using an Extended Kalman
Filter (EKF). The EKF is a nonlinear full-state estimator that approximates the
state estimate with the lowest covariance error when given the sensor
measurements, the model prediction, and their variances. In this paper, we
demonstrate the effectiveness of the EKF by implementing it on a 4-wheel
mecanum-drive robot simulation. The position and velocity accuracy of fusing
together various combinations of these three data sources are compared. We also
discuss the assumptions and limitations of an EKF.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04460" title="Abstract">arXiv:2310.04460</a> [<a href="/pdf/2310.04460" title="Download PDF">pdf</a>, <a href="/format/2310.04460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tuning In to Neural Encoding: Linking Human Brain and Artificial  Supervised Representations of Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingyuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaohan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Moens%2C+M">Marie-Francine Moens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ECAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">To understand the algorithm that supports the human brain's language
representation, previous research has attempted to predict neural responses to
linguistic stimuli using embeddings generated by artificial neural networks
(ANNs), a process known as neural encoding. However, most of these studies have
focused on probing neural representations of Germanic languages, such as
English, with unsupervised ANNs. In this paper, we propose to bridge the gap
between human brain and supervised ANN representations of the Chinese language.
Specifically, we investigate how task tuning influences a pretained Transformer
for neural encoding and which tasks lead to the best encoding performances. We
generate supervised representations on eight Natural Language Understanding
(NLU) tasks using prompt-tuning, a technique that is seldom explored in neural
encoding for language. We demonstrate that prompt-tuning yields representations
that better predict neural responses to Chinese stimuli than traditional
fine-tuning on four tasks. Furthermore, we discover that tasks that require a
fine-grained processing of concepts and entities lead to representations that
are most predictive of brain activation patterns. Additionally, we reveal that
the proportion of tuned parameters highly influences the neural encoding
performance of fine-tuned models. Overall, our experimental findings could help
us better understand the relationship between supervised artificial and brain
language representations.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04461" title="Abstract">arXiv:2310.04461</a> [<a href="/pdf/2310.04461" title="Download PDF">pdf</a>, <a href="/format/2310.04461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-based automated active learning for discovery of hidden dynamic  processes: A use case in light microscopy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Friederich%2C+N">Nils Friederich</a>, 
<a href="/search/cs?searchtype=author&query=Sitcheu%2C+A+Y">Angelo Yamachui Sitcheu</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+O">Oliver Neumann</a>, 
<a href="/search/cs?searchtype=author&query=Ero%C4%9Flu-Kay%C4%B1k%C3%A7%C4%B1%2C+S">S&#xfc;heyla Ero&#x11f;lu-Kay&#x131;k&#xe7;&#x131;</a>, 
<a href="/search/cs?searchtype=author&query=Prizak%2C+R">Roshan Prizak</a>, 
<a href="/search/cs?searchtype=author&query=Hilbert%2C+L">Lennart Hilbert</a>, 
<a href="/search/cs?searchtype=author&query=Mikut%2C+R">Ralf Mikut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings - 33. Workshop Computational Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the biomedical environment, experiments assessing dynamic processes are
primarily performed by a human acquisition supervisor. Contemporary
implementations of such experiments frequently aim to acquire a maximum number
of relevant events from sometimes several hundred parallel, non-synchronous
processes. Since in some high-throughput experiments, only one or a few
instances of a given process can be observed simultaneously, a strategy for
planning and executing an efficient acquisition paradigm is essential. To
address this problem, we present two new methods in this paper. The first
method, Encoded Dynamic Process (EDP), is Artificial Intelligence (AI)-based
and represents dynamic processes so as to allow prediction of pseudo-time
values from single still images. Second, with Experiment Automation Pipeline
for Dynamic Processes (EAPDP), we present a Machine Learning Operations
(MLOps)-based pipeline that uses the extracted knowledge from EDP to
efficiently schedule acquisition in biomedical experiments for dynamic
processes in practice. In a first experiment, we show that the pre-trained
State-Of-The- Art (SOTA) object segmentation method Contour Proposal Networks
(CPN) works reliably as a module of EAPDP to extract the relevant object for
EDP from the acquired three-dimensional image stack.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04465" title="Abstract">arXiv:2310.04465</a> [<a href="/pdf/2310.04465" title="Download PDF">pdf</a>, <a href="/format/2310.04465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How toxic is antisemitism? Potentials and limitations of automated  toxicity scoring for antisemitic online content
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mihaljevi%C4%87%2C+H">Helena Mihaljevi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Steffen%2C+E">Elisabeth Steffen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In: Proceedings of the 2nd Workshop on Computational Linguistics for Political Text Analysis (CPSS-2022), Potsdam, Germany, Sep 12, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">The Perspective API, a popular text toxicity assessment service by Google and
Jigsaw, has found wide adoption in several application areas, notably content
moderation, monitoring, and social media research. We examine its potentials
and limitations for the detection of antisemitic online content that, by
definition, falls under the toxicity umbrella term. Using a manually annotated
German-language dataset comprising around 3,600 posts from Telegram and
Twitter, we explore as how toxic antisemitic texts are rated and how the
toxicity scores differ regarding different subforms of antisemitism and the
stance expressed in the texts. We show that, on a basic level, Perspective API
recognizes antisemitic content as toxic, but shows critical weaknesses with
respect to non-explicit forms of antisemitism and texts taking a critical
stance towards it. Furthermore, using simple text manipulations, we demonstrate
that the use of widespread antisemitic codes can substantially reduce API
scores, making it rather easy to bypass content moderation based on the
service's results.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04467" title="Abstract">arXiv:2310.04467</a> [<a href="/pdf/2310.04467" title="Download PDF">pdf</a>, <a href="/format/2310.04467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design Principles for Lifelong Learning AI Accelerators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kudithipudi%2C+D">Dhireesha Kudithipudi</a>, 
<a href="/search/cs?searchtype=author&query=Daram%2C+A">Anurag Daram</a>, 
<a href="/search/cs?searchtype=author&query=Zyarah%2C+A+M">Abdullah M. Zyarah</a>, 
<a href="/search/cs?searchtype=author&query=Zohora%2C+F+T">Fatima Tuz Zohora</a>, 
<a href="/search/cs?searchtype=author&query=Aimone%2C+J+B">James B. Aimone</a>, 
<a href="/search/cs?searchtype=author&query=Yanguas-Gil%2C+A">Angel Yanguas-Gil</a>, 
<a href="/search/cs?searchtype=author&query=Soures%2C+N">Nicholas Soures</a>, 
<a href="/search/cs?searchtype=author&query=Neftci%2C+E">Emre Neftci</a>, 
<a href="/search/cs?searchtype=author&query=Mattina%2C+M">Matthew Mattina</a>, 
<a href="/search/cs?searchtype=author&query=Lomonaco%2C+V">Vincenzo Lomonaco</a>, 
<a href="/search/cs?searchtype=author&query=Thiem%2C+C+D">Clare D. Thiem</a>, 
<a href="/search/cs?searchtype=author&query=Epstein%2C+B">Benjamin Epstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">Lifelong learning - an agent's ability to learn throughout its lifetime - is
a hallmark of biological learning systems and a central challenge for
artificial intelligence (AI). The development of lifelong learning algorithms
could lead to a range of novel AI applications, but this will also require the
development of appropriate hardware accelerators, particularly if the models
are to be deployed on edge platforms, which have strict size, weight, and power
constraints. Here, we explore the design of lifelong learning AI accelerators
that are intended for deployment in untethered environments. We identify key
desirable capabilities for lifelong learning accelerators and highlight metrics
to evaluate such accelerators. We then discuss current edge AI accelerators and
explore the future design of lifelong learning accelerators, considering the
role that different emerging technologies could play.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04468" title="Abstract">arXiv:2310.04468</a> [<a href="/pdf/2310.04468" title="Download PDF">pdf</a>, <a href="/format/2310.04468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Validating transformers for redaction of text from electronic health  records in real-world healthcare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kraljevic%2C+Z">Zeljko Kraljevic</a>, 
<a href="/search/cs?searchtype=author&query=Shek%2C+A">Anthony Shek</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+J+A">Joshua Au Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Sheldon%2C+E+J">Ewart Jonathan Sheldon</a>, 
<a href="/search/cs?searchtype=author&query=Al-Agil%2C+M">Mohammad Al-Agil</a>, 
<a href="/search/cs?searchtype=author&query=Shuaib%2C+H">Haris Shuaib</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xi Bai</a>, 
<a href="/search/cs?searchtype=author&query=Noor%2C+K">Kawsar Noor</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+A+D">Anoop D. Shah</a>, 
<a href="/search/cs?searchtype=author&query=Dobson%2C+R">Richard Dobson</a>, 
<a href="/search/cs?searchtype=author&query=Teo%2C+J">James Teo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Protecting patient privacy in healthcare records is a top priority, and
redaction is a commonly used method for obscuring directly identifiable
information in text. Rule-based methods have been widely used, but their
precision is often low causing over-redaction of text and frequently not being
adaptable enough for non-standardised or unconventional structures of personal
health information. Deep learning techniques have emerged as a promising
solution, but implementing them in real-world environments poses challenges due
to the differences in patient record structure and language across different
departments, hospitals, and countries.
<br />In this study, we present AnonCAT, a transformer-based model and a blueprint
on how deidentification models can be deployed in real-world healthcare.
AnonCAT was trained through a process involving manually annotated redactions
of real-world documents from three UK hospitals with different electronic
health record systems and 3116 documents. The model achieved high performance
in all three hospitals with a Recall of 0.99, 0.99 and 0.96.
<br />Our findings demonstrate the potential of deep learning techniques for
improving the efficiency and accuracy of redaction in global healthcare data
and highlight the importance of building workflows which not just use these
models but are also able to continually fine-tune and audit the performance of
these algorithms to ensure continuing effectiveness in real-world settings.
This approach provides a blueprint for the real-world use of de-identifying
algorithms through fine-tuning and localisation, the code together with
tutorials is available on GitHub (https://github.com/CogStack/MedCAT).
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04469" title="Abstract">arXiv:2310.04469</a> [<a href="/pdf/2310.04469" title="Download PDF">pdf</a>, <a href="/format/2310.04469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taming Binarized Neural Networks and Mixed-Integer Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aspman%2C+J">Johannes Aspman</a>, 
<a href="/search/cs?searchtype=author&query=Korpas%2C+G">Georgios Korpas</a>, 
<a href="/search/cs?searchtype=author&query=Marecek%2C+J">Jakub Marecek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
<p class="mathjax">There has been a great deal of recent interest in binarized neural networks,
especially because of their explainability. At the same time, automatic
differentiation algorithms such as backpropagation fail for binarized neural
networks, which limits their applicability. By reformulating the problem of
training binarized neural networks as a subadditive dual of a mixed-integer
program, we show that binarized neural networks admit a tame representation.
This, in turn, makes it possible to use the framework of Bolte et al. for
implicit differentiation, which offers the possibility for practical
implementation of backpropagation in the context of binarized neural networks.
This approach could also be used for a broader class of mixed-integer programs,
beyond the training of binarized neural networks, as encountered in symbolic
approaches to AI and beyond.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04470" title="Abstract">arXiv:2310.04470</a> [<a href="/pdf/2310.04470" title="Download PDF">pdf</a>, <a href="/format/2310.04470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Multi-Marginal Optimal Transport for Network Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhichen Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Boxin Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Si Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yinglong Xia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhining Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+H">Hanghang Tong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Finding node correspondence across networks, namely multi-network alignment,
is an essential prerequisite for joint learning on multiple networks. Despite
great success in aligning networks in pairs, the literature on multi-network
alignment is sparse due to the exponentially growing solution space and lack of
high-order discrepancy measures. To fill this gap, we propose a hierarchical
multi-marginal optimal transport framework named HOT for multi-network
alignment. To handle the large solution space, multiple networks are decomposed
into smaller aligned clusters via the fused Gromov-Wasserstein (FGW)
barycenter. To depict high-order relationships across multiple networks, the
FGW distance is generalized to the multi-marginal setting, based on which
networks can be aligned jointly. A fast proximal point method is further
developed with guaranteed convergence to a local optimum. Extensive experiments
and analysis show that our proposed HOT achieves significant improvements over
the state-of-the-art in both effectiveness and scalability.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04471" title="Abstract">arXiv:2310.04471</a> [<a href="/pdf/2310.04471" title="Download PDF">pdf</a>, <a href="/format/2310.04471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Neighborhood Search Heuristics for the Colorful Bin Packing Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=da+Silva%2C+R+F+F">Renan F. F. da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Borges%2C+Y+G+F">Yulle G. F. Borges</a>, 
<a href="/search/cs?searchtype=author&query=Schouery%2C+R+C+S">Rafael C. S. Schouery</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">The Colorful Bin Packing Problem (CBPP) is a generalization of the Bin
Packing Problem (BPP). The CBPP consists of packing a set of items, each with a
weight and a color, in bins of limited capacity, minimizing the number of used
bins and satisfying the constraint that two items of the same color cannot be
packed side by side in the same bin. In this article, we proposed an adaptation
of BPP heuristics and new heuristics for the CBPP. Moreover, we propose a set
of fast neighborhood search algorithms for CBPP. These neighborhoods are
applied in a meta-heuristic approach based on the Variable Neighborhood Search
(VNS) and a matheuristic approach that mixes linear programming with the
meta-heuristics VNS and Greedy Randomized Adaptive Search (GRASP). The results
indicate that our matheuristic is superior to VNS and that both approaches can
find near-optimal solutions for a large number instances, even for instances
with many items.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04472" title="Abstract">arXiv:2310.04472</a> [<a href="/pdf/2310.04472" title="Download PDF">pdf</a>, <a href="/format/2310.04472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Slogan Generation with Noise Perturbation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jongeun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">MinChung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taehwan Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in CIKM 2023 short paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Slogans play a crucial role in building the brand's identity of the firm. A
slogan is expected to reflect firm's vision and brand's value propositions in
memorable and likeable ways. Automating the generation of slogans with such
characteristics is challenging. Previous studies developted and tested slogan
generation with syntactic control and summarization models which are not
capable of generating distinctive slogans. We introduce a a novel apporach that
leverages pre-trained transformer T5 model with noise perturbation on newly
proposed 1:N matching pair dataset. This approach serves as a contributing
fator in generting distinctive and coherent slogans. Turthermore, the proposed
approach incorporates descriptions about the firm and brand into the generation
of slogans. We evaluate generated slogans based on ROUGE1, ROUGEL and Cosine
Similarity metrics and also assess them with human subjects in terms of
slogan's distinctiveness, coherence, and fluency. The results demonstrate that
our approach yields better performance than baseline models and other
transformer-based models.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04474" title="Abstract">arXiv:2310.04474</a> [<a href="/pdf/2310.04474" title="Download PDF">pdf</a>, <a href="/format/2310.04474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yinger Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+H">Hui Cai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yicheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Rui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jing Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL)

</div>
<p class="mathjax">While enabling large language models to implement function calling (known as
APIs) can greatly enhance the performance of LLMs, function calling is still a
challenging task due to the complicated relations between different APIs,
especially in a context-learning setting without fine-tuning. This paper
proposes a simple yet controllable target-driven approach called Reverse Chain
to empower LLMs with capabilities to use external APIs with only prompts. Given
that most open-source LLMs have limited tool-use or tool-plan capabilities,
LLMs in Reverse Chain are only employed to implement simple tasks, e.g., API
selection and argument completion, and a generic rule is employed to implement
a controllable multiple functions calling. In this generic rule, after
selecting a final API to handle a given task via LLMs, we first ask LLMs to
fill the required arguments from user query and context. Some missing arguments
could be further completed by letting LLMs select another API based on API
description before asking user. This process continues until a given task is
completed. Extensive numerical experiments indicate an impressive capability of
Reverse Chain on implementing multiple function calling. Interestingly enough,
the experiments also reveal that tool-use capabilities of the existing LLMs,
e.g., ChatGPT, can be greatly improved via Reverse Chain.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04475" title="Abstract">arXiv:2310.04475</a> [<a href="/pdf/2310.04475" title="Download PDF">pdf</a>, <a href="/format/2310.04475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demystifying Embedding Spaces using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tennenholtz%2C+G">Guy Tennenholtz</a>, 
<a href="/search/cs?searchtype=author&query=Chow%2C+Y">Yinlam Chow</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chih-Wei Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+J">Jihwan Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Shani%2C+L">Lior Shani</a>, 
<a href="/search/cs?searchtype=author&query=Tulepbergenov%2C+A">Azamat Tulepbergenov</a>, 
<a href="/search/cs?searchtype=author&query=Ramachandran%2C+D">Deepak Ramachandran</a>, 
<a href="/search/cs?searchtype=author&query=Mladenov%2C+M">Martin Mladenov</a>, 
<a href="/search/cs?searchtype=author&query=Boutilier%2C+C">Craig Boutilier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Embeddings have become a pivotal means to represent complex, multi-faceted
information about entities, concepts, and relationships in a condensed and
useful format. Nevertheless, they often preclude direct interpretation. While
downstream tasks make use of these compressed representations, meaningful
interpretation usually requires visualization using dimensionality reduction or
specialized machine learning interpretability methods. This paper addresses the
challenge of making such embeddings more interpretable and broadly useful, by
employing Large Language Models (LLMs) to directly interact with embeddings --
transforming abstract vectors into understandable narratives. By injecting
embeddings into LLMs, we enable querying and exploration of complex embedding
data. We demonstrate our approach on a variety of diverse tasks, including:
enhancing concept activation vectors (CAVs), communicating novel embedded
entities, and decoding user preferences in recommender systems. Our work
couples the immense information potential of embeddings with the interpretative
power of LLMs.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04477" title="Abstract">arXiv:2310.04477</a> [<a href="/pdf/2310.04477" title="Download PDF">pdf</a>, <a href="/format/2310.04477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Higher-Order DeepTrails: Unified Approach to *Trails
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koopmann%2C+T">Tobias Koopmann</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+J">Jan Pfister</a>, 
<a href="/search/cs?searchtype=author&query=Markus%2C+A">Andr&#xe9; Markus</a>, 
<a href="/search/cs?searchtype=author&query=Carolus%2C+A">Astrid Carolus</a>, 
<a href="/search/cs?searchtype=author&query=Wienrich%2C+C">Carolin Wienrich</a>, 
<a href="/search/cs?searchtype=author&query=Hotho%2C+A">Andreas Hotho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Analyzing, understanding, and describing human behavior is advantageous in
different settings, such as web browsing or traffic navigation. Understanding
human behavior naturally helps to improve and optimize the underlying
infrastructure or user interfaces. Typically, human navigation is represented
by sequences of transitions between states. Previous work suggests to use
hypotheses, representing different intuitions about the navigation to analyze
these transitions. To mathematically grasp this setting, first-order Markov
chains are used to capture the behavior, consequently allowing to apply
different kinds of graph comparisons, but comes with the inherent drawback of
losing information about higher-order dependencies within the sequences. To
this end, we propose to analyze entire sequences using autoregressive language
models, as they are traditionally used to model higher-order dependencies in
sequences. We show that our approach can be easily adapted to model different
settings introduced in previous work, namely HypTrails, MixedTrails and even
SubTrails, while at the same time bringing unique advantages: 1. Modeling
higher-order dependencies between state transitions, while 2. being able to
identify short comings in proposed hypotheses, and 3. naturally introducing a
unified approach to model all settings. To show the expressiveness of our
approach, we evaluate our approach on different synthetic datasets and conclude
with an exemplary analysis of a real-world dataset, examining the behavior of
users who interact with voice assistants.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04478" title="Abstract">arXiv:2310.04478</a> [<a href="/pdf/2310.04478" title="Download PDF">pdf</a>, <a href="/format/2310.04478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Full-scale modal testing of a Hawk T1A aircraft for benchmarking  vibration-based methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Haywood-Alexander%2C+M">Marcus Haywood-Alexander</a>, 
<a href="/search/eess?searchtype=author&query=Mills%2C+R+S">Robin S. Mills</a>, 
<a href="/search/eess?searchtype=author&query=Champneys%2C+M+D">Max D. Champneys</a>, 
<a href="/search/eess?searchtype=author&query=Jones%2C+M+R">Matthew R. Jones</a>, 
<a href="/search/eess?searchtype=author&query=Bonney%2C+M+S">Matthew S. Bonney</a>, 
<a href="/search/eess?searchtype=author&query=Wagg%2C+D">David Wagg</a>, 
<a href="/search/eess?searchtype=author&query=Rogers%2C+T+J">Timothy J. Rogers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Research developments for structural dynamics in the fields of design, system
identification and structural health monitoring (SHM) have dramatically
expanded the bounds of what can be learned from measured vibration data.
However, significant challenges remain in the tasks of identification,
prediction and evaluation of full-scale structures. A significant aid in the
roadmap to the application of cutting-edge methods to the demands of in-service
engineering structures, is the development of comprehensive benchmark datasets.
With the aim of developing a useful and worthwhile benchmark dataset for
structural dynamics, an extensive testing campaign is presented here. This
recent campaign was performed on a decommissioned BAE system Hawk T1A aircraft
at the Laboratory for Verification and Validation (LVV) in Sheffield. The aim
of this paper is to present the dataset, providing details on the structure,
experimental design, and data acquired. The collected data is made freely and
openly available with the intention that it serve as a benchmark dataset for
challenges in full-scale structural dynamics. Here, the details pertaining to
two test phases (frequency and time domain) are presented. So as to ensure that
the presented dataset is able to function as a benchmark, some baseline-level
results are additionally presented for the tasks of identification and
prediction, using standard approaches. It is envisaged that advanced
methodologies will demonstrate superiority by favourable comparison with the
results presented here. Finally, some dataset-specific challenges are
described, with a view to form a hierarchy of tasks and frame discussion over
their relative difficulty.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04479" title="Abstract">arXiv:2310.04479</a> [<a href="/pdf/2310.04479" title="Download PDF">pdf</a>, <a href="/format/2310.04479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Data Geometry to Mitigate CSM in Steganalysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abecidan%2C+R">Rony Abecidan</a> (CRIStAL, CNRS), 
<a href="/search/cs?searchtype=author&query=Itier%2C+V">Vincent Itier</a> (IMT Nord Europe, CRIStAL), 
<a href="/search/cs?searchtype=author&query=Boulanger%2C+J">J&#xe9;r&#xe9;mie Boulanger</a> (CRIStAL), 
<a href="/search/cs?searchtype=author&query=Bas%2C+P">Patrick Bas</a> (CRIStAL, CNRS), 
<a href="/search/cs?searchtype=author&query=Pevn%C3%BD%2C+T">Tom&#xe1;&#x161; Pevn&#xfd;</a> (CTU)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE International Workshop on Information Forensics and Security
  (WIFS 2023), Dec 2023, Nuremberg, Germany
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM); Signal Processing (eess.SP)

</div>
<p class="mathjax">In operational scenarios, steganographers use sets of covers from various
sensors and processing pipelines that differ significantly from those used by
researchers to train steganalysis models. This leads to an inevitable
performance gap when dealing with out-of-distribution covers, commonly referred
to as Cover Source Mismatch (CSM). In this study, we consider the scenario
where test images are processed using the same pipeline. However, knowledge
regarding both the labels and the balance between cover and stego is missing.
Our objective is to identify a training dataset that allows for maximum
generalization to our target. By exploring a grid of processing pipelines
fostering CSM, we discovered a geometrical metric based on the chordal distance
between subspaces spanned by DCTr features, that exhibits high correlation with
operational regret while being not affected by the cover-stego balance. Our
contribution lies in the development of a strategy that enables the selection
or derivation of customized training datasets, enhancing the overall
generalization performance for a given target. Experimental validation
highlights that our geometry-based optimization strategy outperforms
traditional atomistic methods given reasonable assumptions. Additional
resources are available at
github.com/RonyAbecidan/LeveragingGeometrytoMitigateCSM.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04480" title="Abstract">arXiv:2310.04480</a> [<a href="/pdf/2310.04480" title="Download PDF">pdf</a>, <a href="/format/2310.04480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auto-survey Challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rachmat%2C+B+K">Benedictus Kent Rachmat</a> (TAU, LISN), 
<a href="/search/cs?searchtype=author&query=Khuong%2C+T+G+H">Thanh Gia Hieu Khuong</a> (LISN, TAU)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Junior Conference on Data Science and Engineering 2023, Sep 2023, Orsay, France
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present a novel platform for evaluating the capability of Large Language
Models (LLMs) to autonomously compose and critique survey papers spanning a
vast array of disciplines including sciences, humanities, education, and law.
Within this framework, AI systems undertake a simulated peer-review mechanism
akin to traditional scholarly journals, with human organizers serving in an
editorial oversight capacity. Within this framework, we organized a competition
for the AutoML conference 2023. Entrants are tasked with presenting stand-alone
models adept at authoring articles from designated prompts and subsequently
appraising them. Assessment criteria include clarity, reference
appropriateness, accountability, and the substantive value of the content. This
paper presents the design of the competition, including the implementation
baseline submissions and methods of evaluation.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04482" title="Abstract">arXiv:2310.04482</a> [<a href="/pdf/2310.04482" title="Download PDF">pdf</a>, <a href="/format/2310.04482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EMOFM: Ensemble MLP mOdel with Feature-based Mixers for Click-Through  Rate Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y+B">Yujian Betterest Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kai Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Track one of CTI competition is on click-through rate (CTR) prediction. The
dataset contains millions of records and each field-wise feature in a record
consists of hashed integers for privacy. For this task, the keys of
network-based methods might be type-wise feature extraction and information
fusion across different fields. Multi-layer perceptrons (MLPs) are able to
extract field feature, but could not efficiently fuse features. Motivated by
the natural fusion characteristic of cross attention and the efficiency of
transformer-based structures, we propose simple plug-in mixers for
field/type-wise feature fusion, and thus construct an field&amp;type-wise ensemble
model, namely EMOFM (Ensemble MLP mOdel with Feature-based Mixers). In the
experiments, the proposed model is evaluated on the dataset, the optimization
process is visualized and ablation studies are explored. It is shown that EMOFM
outperforms compared baselines. In the end, we discuss on future work. WARNING:
The comparison might not be fair enough since the proposed method is designed
for this data in particular while compared methods are not. For example, EMOFM
especially takes different types of interactions into consideration while
others do not. Anyway, we do hope that the ideas inside our method could help
other developers/learners/researchers/thinkers and so on.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04483" title="Abstract">arXiv:2310.04483</a> [<a href="/pdf/2310.04483" title="Download PDF">pdf</a>, <a href="/format/2310.04483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bi-objective Perspective on Controllable Language Models: Reward  Dropout Improves Off-policy Control Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Changhun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+C">Chiehyeon Lim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 14 figures, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We study the theoretical aspects of CLMs (Controllable Language Models) from
a bi-objective optimization perspective. Specifically, we consider the CLMs as
an off-policy RL problem that requires simultaneously maximizing the reward and
likelihood objectives. Our main contribution consists of three parts. First, we
establish the theoretical foundations of CLM by presenting reward upper bound
and Pareto improvement/optimality conditions. Second, we analyze conditions
that improve and violate Pareto optimality itself, respectively. Finally, we
propose Reward Dropout, a simple yet powerful method to guarantee policy
improvement based on a Pareto improvement condition. Our theoretical outcomes
are supported by not only deductive proofs but also empirical results. The
performance of Reward Dropout was evaluated on five CLM benchmark datasets, and
it turns out that the Reward Dropout significantly improves the performance of
CLMs.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04484" title="Abstract">arXiv:2310.04484</a> [<a href="/pdf/2310.04484" title="Download PDF">pdf</a>, <a href="/format/2310.04484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ada-Instruct: Adapting Instruction Generators for Complex Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+W">Wanyun Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianle Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Generating diverse and sophisticated instructions for downstream tasks by
Large Language Models (LLMs) is pivotal for advancing the effect. Current
approaches leverage closed-source LLMs, employing in-context prompting for
instruction generation. However, in this paper, we found that in-context
prompting cannot generate complex instructions with length $\ge 100$ for tasks
like code completion.
<br />To solve this problem, we introduce Ada-Instruct, an adaptive instruction
generator developed by fine-tuning open-source LLMs. Our pivotal finding
illustrates that fine-tuning open-source LLMs with a mere ten samples generates
long instructions that maintain distributional consistency for complex
reasoning tasks. We empirically validated Ada-Instruct's efficacy across
different applications, including code completion, mathematical reasoning, and
commonsense reasoning. The results underscore Ada-Instruct's superiority,
evidencing its improvements over its base models, current self-instruct
methods, and other state-of-the-art models.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04486" title="Abstract">arXiv:2310.04486</a> [<a href="/pdf/2310.04486" title="Download PDF">pdf</a>, <a href="/format/2310.04486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> T-Rep: Representation Learning for Time Series using Time-Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fraikin%2C+A">Archibald Fraikin</a>, 
<a href="/search/cs?searchtype=author&query=Bennetot%2C+A">Adrien Bennetot</a>, 
<a href="/search/cs?searchtype=author&query=Allassonni%C3%A8re%2C+S">St&#xe9;phanie Allassonni&#xe8;re</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Multivariate time series present challenges to standard machine learning
techniques, as they are often unlabeled, high dimensional, noisy, and contain
missing data. To address this, we propose T-Rep, a self-supervised method to
learn time series representations at a timestep granularity. T-Rep learns
vector embeddings of time alongside its feature extractor, to extract temporal
features such as trend, periodicity, or distribution shifts from the signal.
These time-embeddings are leveraged in pretext tasks, to incorporate smooth and
fine-grained temporal dependencies in the representations, as well as reinforce
robustness to missing data. We evaluate T-Rep on downstream classification,
forecasting, and anomaly detection tasks. It is compared to existing
self-supervised algorithms for time series, which it outperforms in all three
tasks. We test T-Rep in missing data regimes, where it proves more resilient
than its counterparts. Finally, we provide latent space visualisation
experiments, highlighting the interpretability of the learned representations.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04490" title="Abstract">arXiv:2310.04490</a> [<a href="/pdf/2310.04490" title="Download PDF">pdf</a>, <a href="/format/2310.04490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Diffusion From An Action Principle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Premkumar%2C+A">Akhil Premkumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages + references, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Generative diffusion models synthesize new samples by reversing a diffusive
process that converts a given data set to generic noise. This is accomplished
by training a neural network to match the gradient of the log of the
probability distribution of a given data set, also called the score. By casting
reverse diffusion as an optimal control problem, we show that score matching
can be derived from an action principle, like the ones commonly used in
physics. We use this insight to demonstrate the connection between different
classes of diffusion models.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04496" title="Abstract">arXiv:2310.04496</a> [<a href="/pdf/2310.04496" title="Download PDF">pdf</a>, <a href="/format/2310.04496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> URLOST: Unsupervised Representation Learning without Stationarity or  Topology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yun%2C+Z">Zeyu Yun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Juexiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Olshausen%2C+B">Bruno Olshausen</a>, 
<a href="/search/cs?searchtype=author&query=LeCun%2C+Y">Yann LeCun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yubei Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Unsupervised representation learning has seen tremendous progress but is
constrained by its reliance on data modality-specific stationarity and
topology, a limitation not found in biological intelligence systems. For
instance, human vision processes visual signals derived from irregular and
non-stationary sampling lattices yet accurately perceives the geometry of the
world. We introduce a novel framework that learns from high-dimensional data
lacking stationarity and topology. Our model combines a learnable
self-organizing layer, density adjusted spectral clustering, and masked
autoencoders. We evaluate its effectiveness on simulated biological vision
data, neural recordings from the primary visual cortex, and gene expression
datasets. Compared to state-of-the-art unsupervised learning methods like
SimCLR and MAE, our model excels at learning meaningful representations across
diverse modalities without depending on stationarity or topology. It also
outperforms other methods not dependent on these factors, setting a new
benchmark in the field. This work represents a step toward unsupervised
learning methods that can generalize across diverse high-dimensional data
modalities.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04513" title="Abstract">arXiv:2310.04513</a> [<a href="/pdf/2310.04513" title="Download PDF">pdf</a>, <a href="/ps/2310.04513" title="Download PostScript">ps</a>, <a href="/format/2310.04513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Data Security: Practices from Cybersecurity and Challenges  of Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roy%2C+P">Padmaksha Roy</a>, 
<a href="/search/cs?searchtype=author&query=Chadrasekaran%2C+J">Jaganmohan Chadrasekaran</a>, 
<a href="/search/cs?searchtype=author&query=Lanus%2C+E">Erin Lanus</a>, 
<a href="/search/cs?searchtype=author&query=Freeman%2C+L">Laura Freeman</a>, 
<a href="/search/cs?searchtype=author&query=Werner%2C+J">Jeremy Werner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Machine learning (ML) is increasingly being deployed in critical systems. The
data dependence of ML makes securing data used to train and test ML-enabled
systems of utmost importance. While the field of cybersecurity has
well-established practices for securing information, ML-enabled systems create
new attack vectors. Furthermore, data science and cybersecurity domains adhere
to their own set of skills and terminologies. This survey aims to present
background information for experts in both domains in topics such as
cryptography, access control, zero trust architectures, homomorphic encryption,
differential privacy for machine learning, and federated learning to establish
shared foundations and promote advancements in data security.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04515" title="Abstract">arXiv:2310.04515</a> [<a href="/pdf/2310.04515" title="Download PDF">pdf</a>, <a href="/format/2310.04515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Utilizing Free Clients in Federated Learning for Focused Model  Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ravi%2C+A+N">Aditya Narayan Ravi</a>, 
<a href="/search/cs?searchtype=author&query=Shomorony%2C+I">Ilan Shomorony</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Federated Learning (FL) is a distributed machine learning approach to learn
models on decentralized heterogeneous data, without the need for clients to
share their data. Many existing FL approaches assume that all clients have
equal importance and construct a global objective based on all clients. We
consider a version of FL we call Prioritized FL, where the goal is to learn a
weighted mean objective of a subset of clients, designated as priority clients.
An important question arises: How do we choose and incentivize well aligned non
priority clients to participate in the federation, while discarding misaligned
clients? We present FedALIGN (Federated Adaptive Learning with Inclusion of
Global Needs) to address this challenge. The algorithm employs a matching
strategy that chooses non priority clients based on how similar the models loss
is on their data compared to the global data, thereby ensuring the use of non
priority client gradients only when it is beneficial for priority clients. This
approach ensures mutual benefits as non priority clients are motivated to join
when the model performs satisfactorily on their data, and priority clients can
utilize their updates and computational resources when their goals align. We
present a convergence analysis that quantifies the trade off between client
selection and speed of convergence. Our algorithm shows faster convergence and
higher test accuracy than baselines for various synthetic and benchmark
datasets.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04516" title="Abstract">arXiv:2310.04516</a> [<a href="/pdf/2310.04516" title="Download PDF">pdf</a>, <a href="/format/2310.04516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vulnerability Analysis of Nonlinear Control Systems to Stealthy False  Data Injection Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Khazraei%2C+A">Amir Khazraei</a>, 
<a href="/search/eess?searchtype=author&query=Pajic%2C+M">Miroslav Pajic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2204.03217">arXiv:2204.03217</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this work, we focus on analyzing vulnerability of nonlinear dynamical
control systems to stealthy false data injection attacks on sensors. We start
by defining the stealthiness notion in the most general form where an attack is
considered stealthy if it would be undetected by any intrusion detector, i.e.,
any intrusion detector could not do better than a random guess. Depending on
the level of attacker's knowledge about the plant model, controller, and the
system states, two different attack models are considered. For each attack
model, we derive the conditions for which the system will be vulnerable to
stealthy impactful attacks, in addition to finding a methodology for designing
such sequence of false data injection attacks. When the attacker has complete
knowledge about the system, we show that if the closed loop system is
incrementally exponentially stable while the open loop plant is incrementally
unstable, then the system is vulnerable to stealthy yet impactful attacks on
sensors. However, in the second attack model, with less knowledge about the
system, additional conditions need to be satisfied and the level of
stealthiness depends on the accuracy of attacker's knowledge about the system.
We also consider the impact of stealthy attacks on state estimation, and show
that if the closed loop control system including the estimator is incrementally
stable, then the state estimation in the presence of attack converges to the
attack free estimates. Finally, we illustrate our results on numerical case
studies.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04517" title="Abstract">arXiv:2310.04517</a> [<a href="/pdf/2310.04517" title="Download PDF">pdf</a>, <a href="/format/2310.04517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Randomization for Sim2real Transfer of Automatically Generated  Grasping Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huber%2C+J">Johann Huber</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A9l%C3%A9non%2C+F">Fran&#xe7;ois H&#xe9;l&#xe9;non</a>, 
<a href="/search/cs?searchtype=author&query=Watrelot%2C+H">Hippolyte Watrelot</a>, 
<a href="/search/cs?searchtype=author&query=Amar%2C+F+B">Faiz Ben Amar</a>, 
<a href="/search/cs?searchtype=author&query=Doncieux%2C+S">St&#xe9;phane Doncieux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures, draft version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Robotic grasping refers to making a robotic system pick an object by applying
forces and torques on its surface. Many recent studies use data-driven
approaches to address grasping, but the sparse reward nature of this task made
the learning process challenging to bootstrap. To avoid constraining the
operational space, an increasing number of works propose grasping datasets to
learn from. But most of them are limited to simulations. The present paper
investigates how automatically generated grasps can be exploited in the real
world. More than 7000 reach-and-grasp trajectories have been generated with
Quality-Diversity (QD) methods on 3 different arms and grippers, including
parallel fingers and a dexterous hand, and tested in the real world. Conducted
analysis on the collected measure shows correlations between several Domain
Randomization-based quality criteria and sim-to-real transferability. Key
challenges regarding the reality gap for grasping have been identified,
stressing matters on which researchers on grasping should focus in the future.
A QD approach has finally been proposed for making grasps more robust to domain
randomization, resulting in a transfer ratio of 84% on the Franka Research 3
arm.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04519" title="Abstract">arXiv:2310.04519</a> [<a href="/pdf/2310.04519" title="Download PDF">pdf</a>, <a href="/format/2310.04519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPADE: Sparsity-Guided Debugging for Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moakhar%2C+A+S">Arshia Soltani Moakhar</a>, 
<a href="/search/cs?searchtype=author&query=Iofinova%2C+E">Eugenia Iofinova</a>, 
<a href="/search/cs?searchtype=author&query=Alistarh%2C+D">Dan Alistarh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint. 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Interpretability, broadly defined as mechanisms for understanding why and how
machine learning models reach their decisions, is one of the key open goals at
the intersection of deep learning theory and practice. Towards this goal,
multiple tools have been proposed to aid a human examiner in reasoning about a
network's behavior in general or on a set of instances. However, the outputs of
these tools-such as input saliency maps or neuron visualizations-are frequently
difficult for a human to interpret, or even misleading, due, in particular, to
the fact that neurons can be multifaceted, i.e., a single neuron can be
associated with multiple distinct feature combinations. In this paper, we
present a new general approach to address this problem, called SPADE, which,
given a trained model and a target sample, uses sample-targeted pruning to
provide a "trace" of the network's execution on the sample, reducing the
network to the connections that are most relevant to the specific prediction.
We demonstrate that preprocessing with SPADE significantly increases both the
accuracy of image saliency maps across several interpretability methods and the
usefulness of neuron visualizations, aiding humans in reasoning about network
behavior. Our findings show that sample-specific pruning of connections can
disentangle multifaceted neurons, leading to consistently improved
interpretability.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04521" title="Abstract">arXiv:2310.04521</a> [<a href="/pdf/2310.04521" title="Download PDF">pdf</a>, <a href="/format/2310.04521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie  Algebras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tzu-Yuan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Minghan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ghaffari%2C+M">Maani Ghaffari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper proposes an adjoint-equivariant neural network that takes Lie
algebra data as input. Various types of equivariant neural networks have been
proposed in the literature, which treat the input data as elements in a vector
space carrying certain types of transformations. In comparison, we aim to
process inputs that are transformations between vector spaces. The change of
basis on transformation is described by conjugations, inducing the
adjoint-equivariance relationship that our model is designed to capture.
Leveraging the invariance property of the Killing form, the proposed network is
a general framework that works for arbitrary semisimple Lie algebras. Our
network possesses a simple structure that can be viewed as a Lie algebraic
generalization of a multi-layer perceptron (MLP). This work extends the
application of equivariant feature learning. As an example, we showcase its
value in homography modeling using sl(3) Lie algebra.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04528" title="Abstract">arXiv:2310.04528</a> [<a href="/pdf/2310.04528" title="Download PDF">pdf</a>, <a href="/format/2310.04528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DPGOMI: Differentially Private Data Publishing with Gaussian Optimized  Model Inversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dongjie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+S+S">Sen-ching S. Cheung</a>, 
<a href="/search/cs?searchtype=author&query=Chuah%2C+C">Chen-Nee Chuah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 2023 IEEE International Conference on Image Processing (ICIP) workshop on privacy attacks in computer vision, Kuala Lumpur, Malaysia, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">High-dimensional data are widely used in the era of deep learning with
numerous applications. However, certain data which has sensitive information
are not allowed to be shared without privacy protection. In this paper, we
propose a novel differentially private data releasing method called
Differentially Private Data Publishing with Gaussian Optimized Model Inversion
(DPGOMI) to address this issue. Our approach involves mapping private data to
the latent space using a public generator, followed by a lower-dimensional
DP-GAN with better convergence properties. We evaluate the performance of
DPGOMI on standard datasets CIFAR10 and SVHN. Our results show that DPGOMI
outperforms the standard DP-GAN method in terms of Inception Score, Fr\'echet
Inception Distance, and classification performance, while providing the same
level of privacy. Our proposed approach offers a promising solution for
protecting sensitive data in GAN training while maintaining high-quality
results.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04529" title="Abstract">arXiv:2310.04529</a> [<a href="/pdf/2310.04529" title="Download PDF">pdf</a>, <a href="/format/2310.04529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Envisioning Narrative Intelligence: A Creative Visual Storytelling  Anthology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Halperin%2C+B+A">Brett A. Halperin</a>, 
<a href="/search/cs?searchtype=author&query=Lukin%2C+S+M">Stephanie M. Lukin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 11 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 2023 CHI Conference on Human Factors in
  Computing Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we collect an anthology of 100 visual stories from authors who
participated in our systematic creative process of improvised story-building
based on image sequences. Following close reading and thematic analysis of our
anthology, we present five themes that characterize the variations found in
this creative visual storytelling process: (1) Narrating What is in Vision vs.
Envisioning; (2) Dynamically Characterizing Entities/Objects; (3) Sensing
Experiential Information About the Scenery; (4) Modulating the Mood; (5)
Encoding Narrative Biases. In understanding the varied ways that people derive
stories from images, we offer considerations for collecting story-driven
training data to inform automatic story generation. In correspondence with each
theme, we envision narrative intelligence criteria for computational visual
storytelling as: creative, reliable, expressive, grounded, and responsible.
From these criteria, we discuss how to foreground creative expression, account
for biases, and operate in the bounds of visual storyworlds.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04535" title="Abstract">arXiv:2310.04535</a> [<a href="/pdf/2310.04535" title="Download PDF">pdf</a>, <a href="/format/2310.04535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM4DV: Using Large Language Models for Hardware Test Stimuli Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zixi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chadwick%2C+G">Greg Chadwick</a>, 
<a href="/search/cs?searchtype=author&query=McNally%2C+H">Hugo McNally</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yiren Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Mullins%2C+R">Robert Mullins</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Test stimuli generation has been a crucial but labor-intensive task in
hardware design verification. In this paper, we revolutionize this process by
harnessing the power of large language models (LLMs) and present a novel
benchmarking framework, LLM4DV. This framework introduces a prompt template for
interactively eliciting test stimuli from the LLM, along with four innovative
prompting improvements to support the pipeline execution and further enhance
its performance. We compare LLM4DV to traditional constrained-random testing
(CRT), using three self-designed design-under-test (DUT) modules. Experiments
demonstrate that LLM4DV excels in efficiently handling straightforward DUT
scenarios, leveraging its ability to employ basic mathematical reasoning and
pre-trained knowledge. While it exhibits reduced efficiency in complex task
settings, it still outperforms CRT in relative terms. The proposed framework
and the DUT modules used in our experiments will be open-sourced upon
publication.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04538" title="Abstract">arXiv:2310.04538</a> [<a href="/pdf/2310.04538" title="Download PDF">pdf</a>, <a href="/format/2310.04538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> mCLARI: a shape-morphing insect-scale robot capable of omnidirectional  terrain-adaptive locomotion in laterally confined spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kabutz%2C+H">Heiko Kabutz</a>, 
<a href="/search/cs?searchtype=author&query=Hedrick%2C+A">Alexander Hedrick</a>, 
<a href="/search/cs?searchtype=author&query=McDonnell%2C+P">Parker McDonnell</a>, 
<a href="/search/cs?searchtype=author&query=Jayaram%2C+K">Kaushik Jayaram</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Soft compliant microrobots have the potential to deliver significant societal
impact when deployed in applications such as search and rescue. In this
research we present mCLARI, a body compliant quadrupedal microrobot of 20mm
neutral body length and 0.97g, improving on its larger predecessor, CLARI. This
robot has four independently actuated leg modules with 2 degrees of freedom,
each driven by piezoelectric actuators. The legs are interconnected in a closed
kinematic chain via passive body joints, enabling passive body compliance for
shape adaptation to external constraints. Despite scaling its larger
predecessor down to 60 percent in length and 38 percent in mass, mCLARI
maintains 80 percent of the actuation power to achieve high agility.
Additionally, we demonstrate the new capability of passively shape-morphing
mCLARI - omnidirectional laterally confined locomotion - and experimentally
quantify its running performance achieving a new unconstrained top speed of 3
bodylengths/s (60 mms-1). Leveraging passive body compliance, mCLARI can
navigate through narrow spaces with a body compression ratio of up to 1.5x the
neutral body shape.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04539" title="Abstract">arXiv:2310.04539</a> [<a href="/pdf/2310.04539" title="Download PDF">pdf</a>, <a href="/format/2310.04539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Less Certain Adversarial Examples Improves Robust  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Minxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Backes%2C+M">Michael Backes</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent studies have shown that deep neural networks are vulnerable to
adversarial examples. Numerous defenses have been proposed to improve model
robustness, among which adversarial training is most successful. In this work,
we revisit the robust overfitting phenomenon. In particular, we argue that
overconfident models produced during adversarial training could be a potential
cause, supported by the empirical observation that the predicted labels of
adversarial examples generated by models with better robust generalization
ability tend to have significantly more even distributions. Based on the
proposed definition of adversarial certainty, we incorporate an extragradient
step in the adversarial training framework to search for models that can
generate adversarially perturbed inputs with lower certainty, further improving
robust generalization. Our approach is general and can be easily combined with
other variants of adversarial training methods. Extensive experiments on image
benchmarks demonstrate that our method effectively alleviates robust
overfitting and is able to produce models with consistently improved
robustness.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04540" title="Abstract">arXiv:2310.04540</a> [<a href="/pdf/2310.04540" title="Download PDF">pdf</a>, <a href="/format/2310.04540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-decadal Sea Level Prediction using Neural Networks and Spectral  Clustering on Climate Model Large Ensembles and Satellite Altimeter Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sinha%2C+S">Saumya Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Fasullo%2C+J">John Fasullo</a>, 
<a href="/search/cs?searchtype=author&query=Nerem%2C+R+S">R. Steven Nerem</a>, 
<a href="/search/cs?searchtype=author&query=Monteleoni%2C+C">Claire Monteleoni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Sea surface height observations provided by satellite altimetry since 1993
show a rising rate (3.4 mm/year) for global mean sea level. While on average,
sea level has risen 10 cm over the last 30 years, there is considerable
regional variation in the sea level change. Through this work, we predict sea
level trends 30 years into the future at a 2-degree spatial resolution and
investigate the future patterns of the sea level change. We show the potential
of machine learning (ML) in this challenging application of long-term sea level
forecasting over the global ocean. Our approach incorporates sea level data
from both altimeter observations and climate model simulations. We develop a
supervised learning framework using fully connected neural networks (FCNNs)
that can predict the sea level trend based on climate model projections.
Alongside this, our method provides uncertainty estimates associated with the
ML prediction. We also show the effectiveness of partitioning our spatial
dataset and learning a dedicated ML model for each segmented region. We compare
two partitioning strategies: one achieved using domain knowledge, and the other
employing spectral clustering. Our results demonstrate that segmenting the
spatial dataset with spectral clustering improves the ML predictions.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04541" title="Abstract">arXiv:2310.04541</a> [<a href="/pdf/2310.04541" title="Download PDF">pdf</a>, <a href="/format/2310.04541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iris Liveness Detection Competition (LivDet-Iris) -- The 2023 Edition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tinsley%2C+P">Patrick Tinsley</a>, 
<a href="/search/cs?searchtype=author&query=Purnapatra%2C+S">Sandip Purnapatra</a>, 
<a href="/search/cs?searchtype=author&query=Mitcheff%2C+M">Mahsa Mitcheff</a>, 
<a href="/search/cs?searchtype=author&query=Boyd%2C+A">Aidan Boyd</a>, 
<a href="/search/cs?searchtype=author&query=Crum%2C+C">Colton Crum</a>, 
<a href="/search/cs?searchtype=author&query=Bowyer%2C+K">Kevin Bowyer</a>, 
<a href="/search/cs?searchtype=author&query=Flynn%2C+P">Patrick Flynn</a>, 
<a href="/search/cs?searchtype=author&query=Schuckers%2C+S">Stephanie Schuckers</a>, 
<a href="/search/cs?searchtype=author&query=Czajka%2C+A">Adam Czajka</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Meiling Fang</a>, 
<a href="/search/cs?searchtype=author&query=Damer%2C+N">Naser Damer</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xingyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Caiyong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xianyun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Z">Zhaohua Chang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+G">Guangzhe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Tapia%2C+J">Juan Tapia</a>, 
<a href="/search/cs?searchtype=author&query=Busch%2C+C">Christoph Busch</a>, 
<a href="/search/cs?searchtype=author&query=Aravena%2C+C">Carlos Aravena</a>, 
<a href="/search/cs?searchtype=author&query=Schulz%2C+D">Daniel Schulz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, IJCB 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper describes the results of the 2023 edition of the ''LivDet'' series
of iris presentation attack detection (PAD) competitions. New elements in this
fifth competition include (1) GAN-generated iris images as a category of
presentation attack instruments (PAI), and (2) an evaluation of human accuracy
at detecting PAI as a reference benchmark. Clarkson University and the
University of Notre Dame contributed image datasets for the competition,
composed of samples representing seven different PAI categories, as well as
baseline PAD algorithms. Fraunhofer IGD, Beijing University of Civil
Engineering and Architecture, and Hochschule Darmstadt contributed results for
a total of eight PAD algorithms to the competition. Accuracy results are
analyzed by different PAI types, and compared to human accuracy. Overall, the
Fraunhofer IGD algorithm, using an attention-based pixel-wise binary
supervision network, showed the best-weighted accuracy results (average
classification error rate of 37.31%), while the Beijing University of Civil
Engineering and Architecture's algorithm won when equal weights for each PAI
were given (average classification rate of 22.15%). These results suggest that
iris PAD is still a challenging problem.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04546" title="Abstract">arXiv:2310.04546</a> [<a href="/pdf/2310.04546" title="Download PDF">pdf</a>, <a href="/format/2310.04546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving Financial Anomaly Detection via Federated Learning &amp;  Multi-Party Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Sunpreet Arora</a>, 
<a href="/search/cs?searchtype=author&query=Beams%2C+A">Andrew Beams</a>, 
<a href="/search/cs?searchtype=author&query=Chatzigiannis%2C+P">Panagiotis Chatzigiannis</a>, 
<a href="/search/cs?searchtype=author&query=Meiser%2C+S">Sebastian Meiser</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+K">Karan Patel</a>, 
<a href="/search/cs?searchtype=author&query=Raghuraman%2C+S">Srinivasan Raghuraman</a>, 
<a href="/search/cs?searchtype=author&query=Rindal%2C+P">Peter Rindal</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+H">Harshal Shah</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuhang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zamani%2C+M">Mahdi Zamani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">One of the main goals of financial institutions (FIs) today is combating
fraud and financial crime. To this end, FIs use sophisticated machine-learning
models trained using data collected from their customers. The output of machine
learning models may be manually reviewed for critical use cases, e.g.,
determining the likelihood of a transaction being anomalous and the subsequent
course of action. While advanced machine learning models greatly aid an FI in
anomaly detection, model performance could be significantly improved using
additional customer data from other FIs. In practice, however, an FI may not
have appropriate consent from customers to share their data with other FIs.
Additionally, data privacy regulations may prohibit FIs from sharing clients'
sensitive data in certain geographies. Combining customer data to jointly train
highly accurate anomaly detection models is therefore challenging for FIs in
operational settings.
<br />In this paper, we describe a privacy-preserving framework that allows FIs to
jointly train highly accurate anomaly detection models. The framework combines
the concept of federated learning with efficient multi-party computation and
noisy aggregates inspired by differential privacy. The presented framework was
submitted as a winning entry to the financial crime detection track of the
US/UK PETs Challenge. The challenge considered an architecture where banks hold
customer data and execute transactions through a central network. We show that
our solution enables the network to train a highly accurate anomaly detection
model while preserving privacy of customer data. Experimental results
demonstrate that use of additional customer data using the proposed approach
results in improvement of our anomaly detection model's AUPRC from 0.6 to 0.7.
We discuss how our framework, can be generalized to other similar scenarios.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04548" title="Abstract">arXiv:2310.04548</a> [<a href="/pdf/2310.04548" title="Download PDF">pdf</a>, <a href="/ps/2310.04548" title="Download PostScript">ps</a>, <a href="/format/2310.04548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Submodular Norms with Applications To Online Facility Location and  Stochastic Probing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patton%2C+K">Kalen Patton</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+M">Matteo Russo</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+S">Sahil Singla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preliminary version appeared in APPROX 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Optimization problems often involve vector norms, which has led to extensive
research on developing algorithms that can handle objectives beyond the
$\ell_p$ norms. Our work introduces the concept of submodular norms, which are
a versatile type of norms that possess marginal properties similar to
submodular set functions. We show that submodular norms can accurately
represent or approximate well-known classes of norms, such as $\ell_p$ norms,
ordered norms, and symmetric norms. Furthermore, we establish that submodular
norms can be applied to optimization problems such as online facility location,
stochastic probing, and generalized load balancing. This allows us to develop a
logarithmic-competitive algorithm for online facility location with symmetric
norms, to prove a logarithmic adaptivity gap for stochastic probing with
symmetric norms, and to give an alternative poly-logarithmic approximation
algorithm for generalized load balancing with outer $\ell_1$ norm and inner
symmetric norms.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04550" title="Abstract">arXiv:2310.04550</a> [<a href="/pdf/2310.04550" title="Download PDF">pdf</a>, <a href="/format/2310.04550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Module-wise Adaptive Distillation for Multimodality Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+C">Chen Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jiahui Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+M">Matthew Brown</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yin Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tuo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+B">Boqing Gong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Pre-trained multimodal foundation models have demonstrated remarkable
generalizability but pose challenges for deployment due to their large sizes.
One effective approach to reducing their sizes is layerwise distillation,
wherein small student models are trained to match the hidden representations of
large teacher models at each layer. Motivated by our observation that certain
architecture components, referred to as modules, contribute more significantly
to the student's performance than others, we propose to track the contributions
of individual modules by recording the loss decrement after distillation each
module and choose the module with a greater contribution to distill more
frequently. Such an approach can be naturally formulated as a multi-armed
bandit (MAB) problem, where modules and loss decrements are considered as arms
and rewards, respectively. We then develop a modified-Thompson sampling
algorithm named OPTIMA to address the nonstationarity of module contributions
resulting from model updating. Specifically, we leverage the observed
contributions in recent history to estimate the changing contribution of each
module and select modules based on these estimations to maximize the cumulative
contribution. We evaluate the effectiveness of OPTIMA through distillation
experiments on various multimodal understanding and image captioning tasks,
using the CoCa-Large model (Yu et al., 2022) as the teacher model.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04551" title="Abstract">arXiv:2310.04551</a> [<a href="/pdf/2310.04551" title="Download PDF">pdf</a>, <a href="/format/2310.04551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MeSa: Masked, Geometric, and Supervised Pre-training for Monocular Depth  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+O">Muhammad Osama Khan</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Junbang Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chun-Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+Y">Yu Lou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Pre-training has been an important ingredient in developing strong monocular
depth estimation models in recent years. For instance, self-supervised learning
(SSL) is particularly effective by alleviating the need for large datasets with
dense ground-truth depth maps. However, despite these improvements, our study
reveals that the later layers of the SOTA SSL method are actually suboptimal.
By examining the layer-wise representations, we demonstrate significant changes
in these later layers during fine-tuning, indicating the ineffectiveness of
their pre-trained features for depth estimation. To address these limitations,
we propose MeSa, a comprehensive framework that leverages the complementary
strengths of masked, geometric, and supervised pre-training. Hence, MeSa
benefits from not only general-purpose representations learnt via masked pre
training but also specialized depth-specific features acquired via geometric
and supervised pre-training. Our CKA layer-wise analysis confirms that our
pre-training strategy indeed produces improved representations for the later
layers, overcoming the drawbacks of the SOTA SSL method. Furthermore, via
experiments on the NYUv2 and IBims-1 datasets, we demonstrate that these
enhanced representations translate to performance improvements in both the
in-distribution and out-of-distribution settings. We also investigate the
influence of the pre-training dataset and demonstrate the efficacy of
pre-training on LSUN, which yields significantly better pre-trained
representations. Overall, our approach surpasses the masked pre-training SSL
method by a substantial margin of 17.1% on the RMSE. Moreover, even without
utilizing any recently proposed techniques, MeSa also outperforms the most
recent methods and establishes a new state-of-the-art for monocular depth
estimation on the challenging NYUv2 dataset.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04557" title="Abstract">arXiv:2310.04557</a> [<a href="/pdf/2310.04557" title="Download PDF">pdf</a>, <a href="/format/2310.04557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Information in Text Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zining Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Rudzicz%2C+F">Frank Rudzicz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Text-based explanation is a particularly promising approach in explainable
AI, but the evaluation of text explanations is method-dependent. We argue that
placing the explanations on an information-theoretic framework could unify the
evaluations of two popular text explanation methods: rationale and natural
language explanations (NLE). This framework considers the post-hoc text
pipeline as a series of communication channels, which we refer to as
``explanation channels''. We quantify the information flow through these
channels, thereby facilitating the assessment of explanation characteristics.
We set up tools for quantifying two information scores: relevance and
informativeness. We illustrate what our proposed information scores measure by
comparing them against some traditional evaluation metrics. Our
information-theoretic scores reveal some unique observations about the
underlying mechanisms of two representative text explanations. For example, the
NLEs trade-off slightly between transmitting the input-related information and
the target-related information, whereas the rationales do not exhibit such a
trade-off mechanism. Our work contributes to the ongoing efforts in
establishing rigorous and standardized evaluation criteria in the rapidly
evolving field of explainable AI.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04558" title="Abstract">arXiv:2310.04558</a> [<a href="/pdf/2310.04558" title="Download PDF">pdf</a>, <a href="/format/2310.04558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VTON-IT: Virtual Try-On using Image Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adhikari%2C+S">Santosh Adhikari</a>, 
<a href="/search/cs?searchtype=author&query=Bhusal%2C+B">Bishnu Bhusal</a>, 
<a href="/search/cs?searchtype=author&query=Ghimire%2C+P">Prashant Ghimire</a>, 
<a href="/search/cs?searchtype=author&query=Shrestha%2C+A">Anil Shrestha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Virtual Try-On (trying clothes virtually) is a promising application of the
Generative Adversarial Network (GAN). However, it is an arduous task to
transfer the desired clothing item onto the corresponding regions of a human
body because of varying body size, pose, and occlusions like hair and
overlapped clothes. In this paper, we try to produce photo-realistic translated
images through semantic segmentation and a generative adversarial
architecture-based image translation network. We present a novel image-based
Virtual Try-On application VTON-IT that takes an RGB image, segments desired
body part, and overlays target cloth over the segmented body region. Most
state-of-the-art GAN-based Virtual Try-On applications produce unaligned
pixelated synthesis images on real-life test images. However, our approach
generates high-resolution natural images with detailed textures on such variant
images.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04560" title="Abstract">arXiv:2310.04560</a> [<a href="/pdf/2310.04560" title="Download PDF">pdf</a>, <a href="/format/2310.04560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Talk like a Graph: Encoding Graphs for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fatemi%2C+B">Bahare Fatemi</a>, 
<a href="/search/cs?searchtype=author&query=Halcrow%2C+J">Jonathan Halcrow</a>, 
<a href="/search/cs?searchtype=author&query=Perozzi%2C+B">Bryan Perozzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graphs are a powerful tool for representing and analyzing complex
relationships in real-world applications such as social networks, recommender
systems, and computational finance. Reasoning on graphs is essential for
drawing inferences about the relationships between entities in a complex
system, and to identify hidden patterns and trends. Despite the remarkable
progress in automated reasoning with natural text, reasoning on graphs with
large language models (LLMs) remains an understudied problem. In this work, we
perform the first comprehensive study of encoding graph-structured data as text
for consumption by LLMs. We show that LLM performance on graph reasoning tasks
varies on three fundamental levels: (1) the graph encoding method, (2) the
nature of the graph task itself, and (3) interestingly, the very structure of
the graph considered. These novel results provide valuable insight on
strategies for encoding graphs as text. Using these insights we illustrate how
the correct choice of encoders can boost performance on graph reasoning tasks
inside LLMs by 4.8% to 61.8%, depending on the task.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04561" title="Abstract">arXiv:2310.04561</a> [<a href="/pdf/2310.04561" title="Download PDF">pdf</a>, <a href="/format/2310.04561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DragD3D: Vertex-based Editing for Realistic Mesh Deformations using 2D  Diffusion Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tianhao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Belilovsky%2C+E">Eugene Belilovsky</a>, 
<a href="/search/cs?searchtype=author&query=Mudur%2C+S">Sudhir Mudur</a>, 
<a href="/search/cs?searchtype=author&query=Popa%2C+T">Tiberiu Popa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 9 figures, project page: <a href="https://tianhaoxie.github.io/project/DragD3D/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Direct mesh editing and deformation are key components in the geometric
modeling and animation pipeline. Direct mesh editing methods are typically
framed as optimization problems combining user-specified vertex constraints
with a regularizer that determines the position of the rest of the vertices.
The choice of the regularizer is key to the realism and authenticity of the
final result. Physics and geometry-based regularizers are not aware of the
global context and semantics of the object, and the more recent deep learning
priors are limited to a specific class of 3D object deformations. In this work,
our main contribution is a local mesh editing method called DragD3D for global
context-aware realistic deformation through direct manipulation of a few
vertices. DragD3D is not restricted to any class of objects. It achieves this
by combining the classic geometric ARAP (as rigid as possible) regularizer with
2D priors obtained from a large-scale diffusion model. Specifically, we render
the objects from multiple viewpoints through a differentiable renderer and use
the recently introduced DDS loss which scores the faithfulness of the rendered
image to one from a diffusion model. DragD3D combines the approximate gradients
of the DDS with gradients from the ARAP loss to modify the mesh vertices via
neural Jacobian field, while also satisfying vertex constraints. We show that
our deformations are realistic and aware of the global context of the objects,
and provide better results than just using geometric regularizers.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04562" title="Abstract">arXiv:2310.04562</a> [<a href="/pdf/2310.04562" title="Download PDF">pdf</a>, <a href="/format/2310.04562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Foundation Models for Knowledge Graph Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galkin%2C+M">Mikhail Galkin</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xinyu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Mostafa%2C+H">Hesham Mostafa</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jian Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhaocheng Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Foundation models in language and vision have the ability to run inference on
any textual and visual inputs thanks to the transferable representations such
as a vocabulary of tokens in language. Knowledge graphs (KGs) have different
entity and relation vocabularies that generally do not overlap. The key
challenge of designing foundation models on KGs is to learn such transferable
representations that enable inference on any graph with arbitrary entity and
relation vocabularies. In this work, we make a step towards such foundation
models and present ULTRA, an approach for learning universal and transferable
graph representations. ULTRA builds relational representations as a function
conditioned on their interactions. Such a conditioning strategy allows a
pre-trained ULTRA model to inductively generalize to any unseen KG with any
relation vocabulary and to be fine-tuned on any graph. Conducting link
prediction experiments on 57 different KGs, we find that the zero-shot
inductive inference performance of a single pre-trained ULTRA model on unseen
graphs of various sizes is often on par or better than strong baselines trained
on specific graphs. Fine-tuning further boosts the performance.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04564" title="Abstract">arXiv:2310.04564</a> [<a href="/pdf/2310.04564" title="Download PDF">pdf</a>, <a href="/format/2310.04564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReLU Strikes Back: Exploiting Activation Sparsity in Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mirzadeh%2C+I">Iman Mirzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Alizadeh%2C+K">Keivan Alizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+S">Sachin Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Del+Mundo%2C+C+C">Carlo C Del Mundo</a>, 
<a href="/search/cs?searchtype=author&query=Tuzel%2C+O">Oncel Tuzel</a>, 
<a href="/search/cs?searchtype=author&query=Samei%2C+G">Golnoosh Samei</a>, 
<a href="/search/cs?searchtype=author&query=Rastegari%2C+M">Mohammad Rastegari</a>, 
<a href="/search/cs?searchtype=author&query=Farajtabar%2C+M">Mehrdad Farajtabar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) with billions of parameters have drastically
transformed AI applications. However, their demanding computation during
inference has raised significant challenges for deployment on
resource-constrained devices. Despite recent trends favoring alternative
activation functions such as GELU or SiLU, known for increased computation,
this study strongly advocates for reinstating ReLU activation in LLMs. We
demonstrate that using the ReLU activation function has a negligible impact on
convergence and performance while significantly reducing computation and weight
transfer. This reduction is particularly valuable during the memory-bound
inference step, where efficiency is paramount. Exploring sparsity patterns in
ReLU-based LLMs, we unveil the reutilization of activated neurons for
generating new tokens and leveraging these insights, we propose practical
strategies to substantially reduce LLM inference computation up to three times,
using ReLU activations with minimal performance trade-offs.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04565" title="Abstract">arXiv:2310.04565</a> [<a href="/pdf/2310.04565" title="Download PDF">pdf</a>, <a href="/format/2310.04565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Binary Quantification and Dataset Shift: An Experimental Investigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gonz%C3%A1lez%2C+P">Pablo Gonz&#xe1;lez</a>, 
<a href="/search/cs?searchtype=author&query=Moreo%2C+A">Alejandro Moreo</a>, 
<a href="/search/cs?searchtype=author&query=Sebastiani%2C+F">Fabrizio Sebastiani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Quantification is the supervised learning task that consists of training
predictors of the class prevalence values of sets of unlabelled data, and is of
special interest when the labelled data on which the predictor has been trained
and the unlabelled data are not IID, i.e., suffer from dataset shift. To date,
quantification methods have mostly been tested only on a special case of
dataset shift, i.e., prior probability shift; the relationship between
quantification and other types of dataset shift remains, by and large,
unexplored. In this work we carry out an experimental analysis of how current
quantification algorithms behave under different types of dataset shift, in
order to identify limitations of current approaches and hopefully pave the way
for the development of more broadly applicable methods. We do this by proposing
a fine-grained taxonomy of types of dataset shift, by establishing protocols
for the generation of datasets affected by these types of shift, and by testing
existing quantification methods on the datasets thus generated. One finding
that results from this investigation is that many existing quantification
methods that had been found robust to prior probability shift are not
necessarily robust to other types of dataset shift. A second finding is that no
existing quantification method seems to be robust enough to dealing with all
the types of dataset shift we simulate in our experiments. The code needed to
reproduce all our experiments is publicly available at
https://github.com/pglez82/quant_datasetshift.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04566" title="Abstract">arXiv:2310.04566</a> [<a href="/pdf/2310.04566" title="Download PDF">pdf</a>, <a href="/format/2310.04566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knolling bot: A Transformer-based Approach to Organizing a Messy Table
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yuhang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhizhuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruibo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wyder%2C+P">Philippe Wyder</a>, 
<a href="/search/cs?searchtype=author&query=Lipson%2C+H">Hod Lipson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this study, we propose an approach to equip domestic robots with the
ability to perform simple household tidying tasks. We focus specifically on
'knolling,' an activity related to organizing scattered items into neat and
space-efficient arrangements. Unlike the uniformity of industrial environments,
household settings present unique challenges due to their diverse array of
items and the subjectivity of tidiness. Here, we draw inspiration from natural
language processing (NLP) and utilize a transformer-based approach that
predicts the next position of an item in a sequence of neatly positioned items.
We integrate the knolling model with a visual perception model and a physical
robot arm to demonstrate a machine that declutters and organizes a dozen
freeform items of various shapes and sizes.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04570" title="Abstract">arXiv:2310.04570</a> [<a href="/pdf/2310.04570" title="Download PDF">pdf</a>, <a href="/format/2310.04570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-Based Neural Surrogate for Link-Level Path Loss Prediction  from Variable-Sized Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hehn%2C+T+M">Thomas M. Hehn</a>, 
<a href="/search/cs?searchtype=author&query=Orekondy%2C+T">Tribhuvanesh Orekondy</a>, 
<a href="/search/cs?searchtype=author&query=Shental%2C+O">Ori Shental</a>, 
<a href="/search/cs?searchtype=author&query=Behboodi%2C+A">Arash Behboodi</a>, 
<a href="/search/cs?searchtype=author&query=Bucheli%2C+J">Juan Bucheli</a>, 
<a href="/search/cs?searchtype=author&query=Doshi%2C+A">Akash Doshi</a>, 
<a href="/search/cs?searchtype=author&query=Namgoong%2C+J">June Namgoong</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+T">Taesang Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Sampath%2C+A">Ashwin Sampath</a>, 
<a href="/search/cs?searchtype=author&query=Soriaga%2C+J+B">Joseph B. Soriaga</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE GLOBECOM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Estimating path loss for a transmitter-receiver location is key to many
use-cases including network planning and handover. Machine learning has become
a popular tool to predict wireless channel properties based on map data. In
this work, we present a transformer-based neural network architecture that
enables predicting link-level properties from maps of various dimensions and
from sparse measurements. The map contains information about buildings and
foliage. The transformer model attends to the regions that are relevant for
path loss prediction and, therefore, scales efficiently to maps of different
size. Further, our approach works with continuous transmitter and receiver
coordinates without relying on discretization. In experiments, we show that the
proposed model is able to efficiently learn dominant path losses from sparse
training data and generalizes well when tested on novel maps.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04572" title="Abstract">arXiv:2310.04572</a> [<a href="/pdf/2310.04572" title="Download PDF">pdf</a>, <a href="/format/2310.04572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LIVE: Lidar Informed Visual Search for Multiple Objects with Multiple  Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R">Ryan Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minkyu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+J+T">Juliana T Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Morgenstein%2C+K">Kyle Morgenstein</a>, 
<a href="/search/cs?searchtype=author&query=Sentis%2C+L">Luis Sentis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages + references; 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper introduces LIVE: Lidar Informed Visual Search focused on the
problem of multi-robot (MR) planning and execution for robust visual detection
of multiple objects. We perform extensive real-world experiments with a
two-robot team in an indoor apartment setting. LIVE acts as a perception module
that detects unmapped obstacles, or Short Term Features (STFs), in Lidar
observations. STFs are filtered, resulting in regions to be visually inspected
by modifying plans online. Lidar Coverage Path Planning (CPP) is employed for
generating highly efficient global plans for heterogeneous robot teams.
Finally, we present a data model and a demonstration dataset, which can be
found by visiting our project website
https://sites.google.com/view/live-iros2023/home.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04573" title="Abstract">arXiv:2310.04573</a> [<a href="/pdf/2310.04573" title="Download PDF">pdf</a>, <a href="/format/2310.04573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can pruning make Large Language Models more efficient?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gholami%2C+S">Sia Gholami</a>, 
<a href="/search/cs?searchtype=author&query=Omar%2C+M">Marwan Omar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Transformer models have revolutionized natural language processing with their
unparalleled ability to grasp complex contextual relationships. However, the
vast number of parameters in these models has raised concerns regarding
computational efficiency, environmental impact, and deployability on
resource-limited platforms. To address these challenges, this paper
investigates the application of weight pruning-a strategic reduction of model
parameters based on their significance-as an optimization strategy for
Transformer architectures. Through extensive experimentation, we explore
various pruning methodologies, highlighting their impact on model performance,
size, and computational demands. Our findings suggest that with judicious
selection of pruning hyperparameters, significant reductions in model size are
attainable without considerable compromise on performance. Moreover, when
coupled with post-pruning fine-tuning strategies, some pruned models even
exhibit enhanced generalization capabilities. This work seeks to bridge the gap
between model efficiency and performance, paving the way for more scalable and
environmentally responsible deep learning applications.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04575" title="Abstract">arXiv:2310.04575</a> [<a href="/pdf/2310.04575" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time Optical Network Sensing Control Plane Enabled by a Novel Sub  us Response Time Fibre Sensing Control Device
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Szczerban%2C+M">Mijail Szczerban</a>, 
<a href="/search/eess?searchtype=author&query=Mazur%2C+M">Mikael Mazur</a>, 
<a href="/search/eess?searchtype=author&query=Dallachiesa%2C+L">Lauren Dallachiesa</a>, 
<a href="/search/eess?searchtype=author&query=Mardoyan%2C+H">Ha&#xef;k Mardoyan</a>, 
<a href="/search/eess?searchtype=author&query=Bidkar%2C+S">Sarvesh Bidkar</a>, 
<a href="/search/eess?searchtype=author&query=Ryf%2C+R">Roland Ryf</a>, 
<a href="/search/eess?searchtype=author&query=Simsarian%2C+J">Jesse Simsarian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optics (physics.optics)

</div>
<p class="mathjax">We propose and implement a novel fibre sensing control device and associated
sensing control plane that effectively controls backscatter and
polarization-based fibre sensing. We experimentally demonstrate in a fibre
network that this device and associated control plane can achieve sub-us
response time.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04579" title="Abstract">arXiv:2310.04579</a> [<a href="/pdf/2310.04579" title="Download PDF">pdf</a>, <a href="/format/2310.04579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Confirming Transformer for Locally Consistent Online Adaptation in  Multi-Agent Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tao Li</a>, 
<a href="/search/cs?searchtype=author&query=Guevara%2C+J">Juan Guevara</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xinghong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Quanyan Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Offline reinforcement learning (RL) leverages previously collected data to
extract policies that return satisfying performance in online environments.
However, offline RL suffers from the distribution shift between the offline
dataset and the online environment. In the multi-agent RL (MARL) setting, this
distribution shift may arise from the nonstationary opponents (exogenous agents
beyond control) in the online testing who display distinct behaviors from those
recorded in the offline dataset. Hence, the key to the broader deployment of
offline MARL is the online adaptation to nonstationary opponents. Recent
advances in large language models have demonstrated the surprising
generalization ability of the transformer architecture in sequence modeling,
which prompts one to wonder \textit{whether the offline-trained transformer
policy adapts to nonstationary opponents during online testing}. This work
proposes the self-confirming loss (SCL) in offline transformer training to
address the online nonstationarity, which is motivated by the self-confirming
equilibrium (SCE) in game theory. The gist is that the transformer learns to
predict the opponents' future moves based on which it acts accordingly. As a
weaker variant of Nash equilibrium (NE), SCE (equivalently, SCL) only requires
local consistency: the agent's local observations do not deviate from its
conjectures, leading to a more adaptable policy than the one dictated by NE
focusing on global optimality. We evaluate the online adaptability of the
self-confirming transformer (SCT) by playing against nonstationary opponents
employing a variety of policies, from the random one to the benchmark MARL
policies. Experimental results demonstrate that SCT can adapt to nonstationary
opponents online, achieving higher returns than vanilla transformers and
offline MARL baselines.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04580" title="Abstract">arXiv:2310.04580</a> [<a href="/pdf/2310.04580" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Operational Data-Driven Malfunction Detection Framework for Enhanced  Power Distribution System Monitoring -- The DeMaDs Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fellner%2C+D">David Fellner</a>, 
<a href="/search/eess?searchtype=author&query=Strasser%2C+T+I">Thomas I. Strasser</a>, 
<a href="/search/eess?searchtype=author&query=Kastner%2C+W">Wolfgang Kastner</a>, 
<a href="/search/eess?searchtype=author&query=Behnam%2C+F">Feizifar Behnam</a>, 
<a href="/search/eess?searchtype=author&query=Abdulhadi%2C+I+F">Ibrahim F. Abdulhadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27th International Conference on Electricity Distribution (CIRED 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The changes in the electric energy system toward a sustainable future are
inevitable and already on the way today. This often entails a change of
paradigm for the electric energy grid, for example, the switch from central to
decentralized power generation which also has to provide grid-supporting
functionalities. However, due to the scarcity of distributed sensors, new
solutions for grid operators for monitoring these functionalities are needed.
The framework presented in this work allows to apply and assess data-driven
detection methods in order to implement such monitoring capabilities.
Furthermore, an approach to a multi-stage detection of misconfigurations is
introduced. Details on implementations of the single stages as well as their
requirements are also presented. Furthermore, testing and validation results
are discussed. Due to its feature of being seamlessly integrable into system
operators' current metering infrastructure, clear benefits of the proposed
solution are pointed out.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04582" title="Abstract">arXiv:2310.04582</a> [<a href="/pdf/2310.04582" title="Download PDF">pdf</a>, <a href="/format/2310.04582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Humanoid Motion Representations for Physics-Based Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhengyi Luo</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jinkun Cao</a>, 
<a href="/search/cs?searchtype=author&query=Merel%2C+J">Josh Merel</a>, 
<a href="/search/cs?searchtype=author&query=Winkler%2C+A">Alexander Winkler</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Kitani%2C+K">Kris Kitani</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weipeng Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://zhengyiluo.github.io/PULSE/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Robotics (cs.RO)

</div>
<p class="mathjax">We present a universal motion representation that encompasses a comprehensive
range of motor skills for physics-based humanoid control. Due to the
high-dimensionality of humanoid control as well as the inherent difficulties in
reinforcement learning, prior methods have focused on learning skill embeddings
for a narrow range of movement styles (e.g. locomotion, game characters) from
specialized motion datasets. This limited scope hampers its applicability in
complex tasks. Our work closes this gap, significantly increasing the coverage
of motion representation space. To achieve this, we first learn a motion
imitator that can imitate all of human motion from a large, unstructured motion
dataset. We then create our motion representation by distilling skills directly
from the imitator. This is achieved using an encoder-decoder structure with a
variational information bottleneck. Additionally, we jointly learn a prior
conditioned on proprioception (humanoid's own pose and velocities) to improve
model expressiveness and sampling efficiency for downstream tasks. Sampling
from the prior, we can generate long, stable, and diverse human motions. Using
this latent space for hierarchical RL, we show that our policies solve tasks
using natural and realistic human behavior. We demonstrate the effectiveness of
our motion representation by solving generative tasks (e.g. strike, terrain
traversal) and motion tracking using VR controllers.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04584" title="Abstract">arXiv:2310.04584</a> [<a href="/pdf/2310.04584" title="Download PDF">pdf</a>, <a href="/format/2310.04584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Algorithm to Train Unrestricted Sequential Discrete Morphological  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marcondes%2C+D">Diego Marcondes</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+M">Mariana Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Barrera%2C+J">Junior Barrera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">With the advent of deep learning, there have been attempts to insert
mathematical morphology (MM) operators into convolutional neural networks
(CNN), and the most successful endeavor to date has been the morphological
neural networks (MNN). Although MNN have performed better than CNN in solving
some problems, they inherit their black-box nature. Furthermore, in the case of
binary images, they are approximations, which loose the Boolean lattice
structure of MM operators and, thus, it is not possible to represent a specific
class of W-operators with desired properties. In a recent work, we proposed the
Discrete Morphological Neural Networks (DMNN) for binary image transformation
to represent specific classes of W-operators and estimate them via machine
learning. We also proposed a stochastic lattice gradient descent algorithm
(SLGDA) to learn the parameters of Canonical Discrete Morphological Neural
Networks (CDMNN), whose architecture is composed only of operators that can be
decomposed as the supremum, infimum, and complement of erosions and dilations.
In this paper, we propose an algorithm to learn unrestricted sequential DMNN
(USDMNN), whose architecture is given by the composition of general
W-operators. We consider the representation of a W-operator by its
characteristic Boolean function, and then learn it via a SLGDA in the Boolean
lattice of functions. Although both the CDMNN and USDMNN have the Boolean
lattice structure, USDMNN are not as dependent on prior information about the
problem at hand, and may be more suitable in instances in which the
practitioner does not have strong domain knowledge. We illustrate the algorithm
in a practical example.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04586" title="Abstract">arXiv:2310.04586</a> [<a href="/pdf/2310.04586" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TrialView: An AI-powered Visual Analytics System for Temporal Event Data  in Clinical Trials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zuotian Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zelei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yingjie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+W">Wanzhu Tu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jing Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, accepted by HICSS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Randomized controlled trials (RCT) are the gold standards for evaluating the
efficacy and safety of therapeutic interventions in human subjects. In addition
to the pre-specified endpoints, trial participants' experience reveals the time
course of the intervention. Few analytical tools exist to summarize and
visualize the individual experience of trial participants. Visual analytics
allows integrative examination of temporal event patterns of patient
experience, thus generating insights for better care decisions. Towards this
end, we introduce TrialView, an information system that combines graph
artificial intelligence (AI) and visual analytics to enhance the dissemination
of trial data. TrialView offers four distinct yet interconnected views:
Individual, Cohort, Progression, and Statistics, enabling an interactive
exploration of individual and group-level data. The TrialView system is a
general-purpose analytical tool for a broad class of clinical trials. The
system is powered by graph AI, knowledge-guided clustering, explanatory
modeling, and graph-based agglomeration algorithms. We demonstrate the system's
effectiveness in analyzing temporal event data through a case study.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04589" title="Abstract">arXiv:2310.04589</a> [<a href="/pdf/2310.04589" title="Download PDF">pdf</a>, <a href="/format/2310.04589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shufflecake: Plausible Deniability for Multiple Hidden Filesystems on  Linux
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anzuoni%2C+E">Elia Anzuoni</a>, 
<a href="/search/cs?searchtype=author&query=Gagliardoni%2C+T">Tommaso Gagliardoni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A 15-page abstract of this work appears (with the same title) in the proceedings of the ACM Conference on Computer and Communications Security (CCS) 2023. This is the authors' full version. This document supersedes any previous versions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">We present Shufflecake, a new plausible deniability design to hide the
existence of encrypted data on a storage medium making it very difficult for an
adversary to prove the existence of such data. Shufflecake can be considered a
``spiritual successor'' of tools such as TrueCrypt and VeraCrypt, but vastly
improved: it works natively on Linux, it supports any filesystem of choice, and
can manage multiple volumes per device, so to make deniability of the existence
of hidden partitions really plausible.
<br />Compared to ORAM-based solutions, Shufflecake is extremely fast and simpler
but does not offer native protection against multi-snapshot adversaries.
However, we discuss security extensions that are made possible by its
architecture, and we show evidence why these extensions might be enough to
thwart more powerful adversaries.
<br />We implemented Shufflecake as an in-kernel tool for Linux, adding useful
features, and we benchmarked its performance showing only a minor slowdown
compared to a base encrypted system. We believe Shufflecake represents a useful
tool for people whose freedom of expression is threatened by repressive
authorities or dangerous criminal organizations, in particular: whistleblowers,
investigative journalists, and activists for human rights in oppressive
regimes.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04590" title="Abstract">arXiv:2310.04590</a> [<a href="/pdf/2310.04590" title="Download PDF">pdf</a>, <a href="/format/2310.04590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Model Predictive Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sacks%2C+J">Jacob Sacks</a>, 
<a href="/search/cs?searchtype=author&query=Rana%2C+R">Rwik Rana</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kevin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Spitzer%2C+A">Alex Spitzer</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Guanya Shi</a>, 
<a href="/search/cs?searchtype=author&query=Boots%2C+B">Byron Boots</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main paper is 6 pages with 4 figures and 1 table. Code available at: <a href="https://github.com/jisacks/dmpo">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A major challenge in robotics is to design robust policies which enable
complex and agile behaviors in the real world. On one end of the spectrum, we
have model-free reinforcement learning (MFRL), which is incredibly flexible and
general but often results in brittle policies. In contrast, model predictive
control (MPC) continually re-plans at each time step to remain robust to
perturbations and model inaccuracies. However, despite its real-world
successes, MPC often under-performs the optimal strategy. This is due to model
quality, myopic behavior from short planning horizons, and approximations due
to computational constraints. And even with a perfect model and enough compute,
MPC can get stuck in bad local optima, depending heavily on the quality of the
optimization algorithm. To this end, we propose Deep Model Predictive
Optimization (DMPO), which learns the inner-loop of an MPC optimization
algorithm directly via experience, specifically tailored to the needs of the
control problem. We evaluate DMPO on a real quadrotor agile trajectory tracking
task, on which it improves performance over a baseline MPC algorithm for a
given computational budget. It can outperform the best MPC algorithm by up to
27% with fewer samples and an end-to-end policy trained with MFRL by 19%.
Moreover, because DMPO requires fewer samples, it can also achieve these
benefits with 4.3X less memory. When we subject the quadrotor to turbulent wind
fields with an attached drag plate, DMPO can adapt zero-shot while still
outperforming all baselines. Additional results can be found at
https://tinyurl.com/mr2ywmnw.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04592" title="Abstract">arXiv:2310.04592</a> [<a href="/pdf/2310.04592" title="Download PDF">pdf</a>, <a href="/format/2310.04592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Nuisance to News Sense: Augmenting the News with Cross-Document  Evidence and Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Milbauer%2C+J">Jeremiah Milbauer</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Ziqi Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhijin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tongshuang Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Demo Track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Reading and understanding the stories in the news is increasingly difficult.
Reporting on stories evolves rapidly, politicized news venues offer different
perspectives (and sometimes different facts), and misinformation is rampant.
However, existing solutions merely aggregate an overwhelming amount of
information from heterogenous sources, such as different news outlets, social
media, and news bias rating agencies. We present NEWSSENSE, a novel sensemaking
tool and reading interface designed to collect and integrate information from
multiple news articles on a central topic, using a form of reference-free fact
verification. NEWSSENSE augments a central, grounding article of the user's
choice by linking it to related articles from different sources, providing
inline highlights on how specific claims in the chosen article are either
supported or contradicted by information from other articles. Using NEWSSENSE,
users can seamlessly digest and cross-check multiple information sources
without disturbing their natural reading flow. Our pilot study shows that
NEWSSENSE has the potential to help users identify key information, verify the
credibility of news articles, and explore different perspectives.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04595" title="Abstract">arXiv:2310.04595</a> [<a href="/pdf/2310.04595" title="Download PDF">pdf</a>, <a href="/format/2310.04595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segmented Harmonic Loss: Handling Class-Imbalanced Multi-Label Clinical  Data for Medical Coding with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ray%2C+S">Surjya Ray</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+P">Pratik Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chaman%2C+A">Ada Chaman</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+C">Chung-Jen Ho</a>, 
<a href="/search/cs?searchtype=author&query=Chiou%2C+M">Michael Chiou</a>, 
<a href="/search/cs?searchtype=author&query=Suleman%2C+T">Tashfeen Suleman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages,3 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The precipitous rise and adoption of Large Language Models (LLMs) have
shattered expectations with the fastest adoption rate of any consumer-facing
technology in history. Healthcare, a field that traditionally uses NLP
techniques, was bound to be affected by this meteoric rise. In this paper, we
gauge the extent of the impact by evaluating the performance of LLMs for the
task of medical coding on real-life noisy data. We conducted several
experiments on MIMIC III and IV datasets with encoder-based LLMs, such as BERT.
Furthermore, we developed Segmented Harmonic Loss, a new loss function to
address the extreme class imbalance that we found to prevail in most medical
data in a multi-label scenario by segmenting and decoupling co-occurring
classes of the dataset with a new segmentation algorithm. We also devised a
technique based on embedding similarity to tackle noisy data. Our experimental
results show that when trained with the proposed loss, the LLMs achieve
significant performance gains even on noisy long-tailed datasets, outperforming
the F1 score of the state-of-the-art by over ten percentage points.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04598" title="Abstract">arXiv:2310.04598</a> [<a href="/pdf/2310.04598" title="Download PDF">pdf</a>, <a href="/format/2310.04598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A neuro-symbolic framework for answering conjunctive queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barcel%C3%B3%2C+P">Pablo Barcel&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Cucumides%2C+T">Tamara Cucumides</a>, 
<a href="/search/cs?searchtype=author&query=Geerts%2C+F">Floris Geerts</a>, 
<a href="/search/cs?searchtype=author&query=Reutter%2C+J">Juan Reutter</a>, 
<a href="/search/cs?searchtype=author&query=Romero%2C+M">Miguel Romero</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB)

</div>
<p class="mathjax">The problem of answering logical queries over incomplete knowledge graphs is
receiving significant attention in the machine learning community.
Neuro-symbolic models are a promising recent approach, showing good performance
and allowing for good interpretability properties. These models rely on trained
architectures to execute atomic queries, combining them with modules that
simulate the symbolic operators in queries. Unfortunately, most neuro-symbolic
query processors are limited to the so-called tree-like logical queries that
admit a bottom-up execution, where the leaves are constant values or anchors,
and the root is the target variable. Tree-like queries, while expressive, fail
short to express properties in knowledge graphs that are important in practice,
such as the existence of multiple edges between entities or the presence of
triangles.
<br />We propose a framework for answering arbitrary conjunctive queries over
incomplete knowledge graphs. The main idea of our method is to approximate a
cyclic query by an infinite family of tree-like queries, and then leverage
existing models for the latter. Our approximations achieve strong guarantees:
they are complete, i.e. there are no false negatives, and optimal, i.e. they
provide the best possible approximation using tree-like queries. Our method
requires the approximations to be tree-like queries where the leaves are
anchors or existentially quantified variables. Hence, we also show how some of
the existing neuro-symbolic models can handle these queries, which is of
independent interest. Experiments show that our approximation strategy achieves
competitive results, and that including queries with existentially quantified
variables tends to improve the general performance of these models, both on
tree-like queries and on our approximation strategy.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04601" title="Abstract">arXiv:2310.04601</a> [<a href="/pdf/2310.04601" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eight Transaction Papers by Jim Gray
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernstein%2C+P+A">Philip A. Bernstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">This article is a summary of eight of Jim Gray's transaction papers. It was
written at the invitation of Pat Helland to be a chapter of a forthcoming book
in the ACM Turing Award winners' series, "Curiosity, Clarity, and Caring: How
Jim Gray's Passion for Learning, Teaching, and People Changed Computing."
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04602" title="Abstract">arXiv:2310.04602</a> [<a href="/pdf/2310.04602" title="Download PDF">pdf</a>, <a href="/format/2310.04602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discrete energy balance equation via a symplectic second-order method  for two-phase flow in porous media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jones%2C+G+S">Giselle Sosa Jones</a>, 
<a href="/search/math?searchtype=author&query=Trenchea%2C+C">Catalin Trenchea</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose and analyze a second-order partitioned time-stepping method for a
two-phase flow problem in porous media. The algorithm is based on a
refactorization of Cauchy's one-leg $\theta$-method. The main part consists of
the implicit backward Euler method on $[t^n, t^{n+\theta}]$, while part two
uses a linear extrapolation on $[t^{n+\theta},t^{n+1}]$ to obtain the solution
at $t^{n+1}$, equivalent to the forward Euler method.
<br />In the backward Euler step, the decoupled equations are solved iteratively.
We prove that the iterations converge linearly to the solution of the coupled
problem, under some conditions on the data. When $\theta = 1/2$, the algorithm
is equivalent to the symplectic midpoint method. In the absence of the chain
rule for time-discrete setting, we approximate the change in the free energy by
the product of a second-order accurate discrete gradient (chemical potential)
and the one-step increment of the state variables. Similar to the continuous
case, we also prove a discrete Gibbs free energy balance equation, without
numerical dissipation. In the numerical tests we compare this implicit midpoint
method with the classic backward Euler method, and two implicit-explicit
time-lagging schemes. The midpoint method outperforms the other schemes in
terms of rates of convergence, long-time behavior and energy approximation, for
small and large values of the time step.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04604" title="Abstract">arXiv:2310.04604</a> [<a href="/pdf/2310.04604" title="Download PDF">pdf</a>, <a href="/format/2310.04604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PriViT: Vision Transformers for Fast Private Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhyani%2C+N">Naren Dhyani</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+J">Jianqiao Mo</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+M">Minsu Cho</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+A">Ameya Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Siddharth Garg</a>, 
<a href="/search/cs?searchtype=author&query=Reagen%2C+B">Brandon Reagen</a>, 
<a href="/search/cs?searchtype=author&query=Hegde%2C+C">Chinmay Hegde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The Vision Transformer (ViT) architecture has emerged as the backbone of
choice for state-of-the-art deep models for computer vision applications.
However, ViTs are ill-suited for private inference using secure multi-party
computation (MPC) protocols, due to the large number of non-polynomial
operations (self-attention, feed-forward rectifiers, layer normalization). We
propose PriViT, a gradient based algorithm to selectively "Taylorize"
nonlinearities in ViTs while maintaining their prediction accuracy. Our
algorithm is conceptually simple, easy to implement, and achieves improved
performance over existing approaches for designing MPC-friendly transformer
architectures in terms of achieving the Pareto frontier in latency-accuracy. We
confirm these improvements via experiments on several standard image
classification tasks. Public code is available at
https://github.com/NYU-DICE-Lab/privit.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04605" title="Abstract">arXiv:2310.04605</a> [<a href="/pdf/2310.04605" title="Download PDF">pdf</a>, <a href="/format/2310.04605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Optimal Power Flow Value Functions with Input-Convex Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rosemberg%2C+A">Andrew Rosemberg</a>, 
<a href="/search/cs?searchtype=author&query=Tanneau%2C+M">Mathieu Tanneau</a>, 
<a href="/search/cs?searchtype=author&query=Fanzeres%2C+B">Bruno Fanzeres</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+J">Joaquim Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Van+Hentenryck%2C+P">Pascal Van Hentenryck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">The Optimal Power Flow (OPF) problem is integral to the functioning of power
systems, aiming to optimize generation dispatch while adhering to technical and
operational constraints. These constraints are far from straightforward; they
involve intricate, non-convex considerations related to Alternating Current
(AC) power flow, which are essential for the safety and practicality of
electrical grids. However, solving the OPF problem for varying conditions
within stringent time frames poses practical challenges. To address this,
operators resort to model simplifications of varying accuracy. Unfortunately,
better approximations (tight convex relaxations) are often computationally
intractable. This research explores machine learning (ML) to learn convex
approximate solutions for faster analysis in the online setting while still
allowing for coupling into other convex dependent decision problems. By trading
off a small amount of accuracy for substantial gains in speed, they enable the
efficient exploration of vast solution spaces in these complex problems.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04607" title="Abstract">arXiv:2310.04607</a> [<a href="/pdf/2310.04607" title="Download PDF">pdf</a>, <a href="/format/2310.04607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Performance Study of Large Language Models on Novel AI  Accelerators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Emani%2C+M">Murali Emani</a>, 
<a href="/search/cs?searchtype=author&query=Foreman%2C+S">Sam Foreman</a>, 
<a href="/search/cs?searchtype=author&query=Sastry%2C+V">Varuni Sastry</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhen Xie</a>, 
<a href="/search/cs?searchtype=author&query=Raskar%2C+S">Siddhisanket Raskar</a>, 
<a href="/search/cs?searchtype=author&query=Arnold%2C+W">William Arnold</a>, 
<a href="/search/cs?searchtype=author&query=Thakur%2C+R">Rajeev Thakur</a>, 
<a href="/search/cs?searchtype=author&query=Vishwanath%2C+V">Venkatram Vishwanath</a>, 
<a href="/search/cs?searchtype=author&query=Papka%2C+M+E">Michael E. Papka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Artificial intelligence (AI) methods have become critical in scientific
applications to help accelerate scientific discovery. Large language models
(LLMs) are being considered as a promising approach to address some of the
challenging problems because of their superior generalization capabilities
across domains. The effectiveness of the models and the accuracy of the
applications is contingent upon their efficient execution on the underlying
hardware infrastructure. Specialized AI accelerator hardware systems have
recently become available for accelerating AI applications. However, the
comparative performance of these AI accelerators on large language models has
not been previously studied. In this paper, we systematically study LLMs on
multiple AI accelerators and GPUs and evaluate their performance
characteristics for these models. We evaluate these systems with (i) a
micro-benchmark using a core transformer block, (ii) a GPT- 2 model, and (iii)
an LLM-driven science use case, GenSLM. We present our findings and analyses of
the models' performance to better understand the intrinsic capabilities of AI
accelerators. Furthermore, our analysis takes into account key factors such as
sequence lengths, scaling behavior, sparsity, and sensitivity to gradient
accumulation steps.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04610" title="Abstract">arXiv:2310.04610</a> [<a href="/pdf/2310.04610" title="Download PDF">pdf</a>, <a href="/format/2310.04610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery  through Sophisticated AI System Technologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+S+L">Shuaiwen Leon Song</a>, 
<a href="/search/cs?searchtype=author&query=Kruft%2C+B">Bonnie Kruft</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Minjia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Conglong Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shiyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chengming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+M">Masahiro Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaoxia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Rasley%2C+J">Jeff Rasley</a>, 
<a href="/search/cs?searchtype=author&query=Awan%2C+A+A">Ammar Ahmad Awan</a>, 
<a href="/search/cs?searchtype=author&query=Holmes%2C+C">Connor Holmes</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+M">Martin Cai</a>, 
<a href="/search/cs?searchtype=author&query=Ghanem%2C+A">Adam Ghanem</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhongzhu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuxiong He</a>, 
<a href="/search/cs?searchtype=author&query=Bishop%2C+C">Christopher Bishop</a>, 
<a href="/search/cs?searchtype=author&query=Welling%2C+M">Max Welling</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tie-Yan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bodnar%2C+C">Christian Bodnar</a>, 
<a href="/search/cs?searchtype=author&query=Brandsetter%2C+J">Johannes Brandsetter</a>, 
<a href="/search/cs?searchtype=author&query=Bruinsma%2C+W">Wessel Bruinsma</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+C">Chan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuan-Jyue Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+P">Peggy Dai</a>, 
<a href="/search/cs?searchtype=author&query=Garvan%2C+P">Patrick Garvan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Liang He</a>, 
<a href="/search/cs?searchtype=author&query=Heider%2C+E">Elizabeth Heider</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+P">Pipi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+P">Peiran Jin</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+F">Fusong Ju</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yatao Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+R">Renqian Luo</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Q">Qi Meng</a>, 
<a href="/search/cs?searchtype=author&query=Noe%2C+F">Frank Noe</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+T">Tao Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Janwei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+B">Bin Shao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wenlei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Simm%2C+G">Gregor Simm</a>, 
<a href="/search/cs?searchtype=author&query=Stanley%2C+M">Megan Stanley</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lixin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lijun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yingce Xia</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Leo Xia</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Shufang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shuxin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jianwei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Luferenko%2C+P">Pete Luferenko</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+D">Divya Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Weyn%2C+J">Jonathan Weyn</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruixiong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Klocek%2C+S">Sylwester Klocek</a>, 
<a href="/search/cs?searchtype=author&query=Vragov%2C+V">Volodymyr Vragov</a>, 
<a href="/search/cs?searchtype=author&query=AlQuraishi%2C+M">Mohammed AlQuraishi</a>, 
<a href="/search/cs?searchtype=author&query=Ahdritz%2C+G">Gustaf Ahdritz</a>,  et al. (71 additional authors not shown)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the upcoming decade, deep learning may revolutionize the natural sciences,
enhancing our capacity to model and predict natural occurrences. This could
herald a new era of scientific exploration, bringing significant advancements
across sectors from drug development to renewable energy. To answer this call,
we present DeepSpeed4Science initiative (deepspeed4science.ai) which aims to
build unique capabilities through AI system technology innovations to help
domain experts to unlock today's biggest science mysteries. By leveraging
DeepSpeed's current technology pillars (training, inference and compression) as
base technology enablers, DeepSpeed4Science will create a new set of AI system
technologies tailored for accelerating scientific discoveries by addressing
their unique complexity beyond the common technical approaches used for
accelerating generic large language models (LLMs). In this paper, we showcase
the early progress we made with DeepSpeed4Science in addressing two of the
critical system challenges in structural biology research.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04612" title="Abstract">arXiv:2310.04612</a> [<a href="/pdf/2310.04612" title="Download PDF">pdf</a>, <a href="/format/2310.04612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Topological Perspective on Demystifying GNN-Based Link Prediction  Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuying Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yunchao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Neil Shah</a>, 
<a href="/search/cs?searchtype=author&query=Derr%2C+T">Tyler Derr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Graph Neural Networks (GNNs) have shown great promise in learning node
embeddings for link prediction (LP). While numerous studies aim to improve the
overall LP performance of GNNs, none have explored its varying performance
across different nodes and its underlying reasons. To this end, we aim to
demystify which nodes will perform better from the perspective of their local
topology. Despite the widespread belief that low-degree nodes exhibit poorer LP
performance, our empirical findings provide nuances to this viewpoint and
prompt us to propose a better metric, Topological Concentration (TC), based on
the intersection of the local subgraph of each node with the ones of its
neighbors. We empirically demonstrate that TC has a higher correlation with LP
performance than other node-level topological metrics like degree and subgraph
density, offering a better way to identify low-performing nodes than using
cold-start. With TC, we discover a novel topological distribution shift issue
in which newly joined neighbors of a node tend to become less interactive with
that node's existing neighbors, compromising the generalizability of node
embeddings for LP at testing time. To make the computation of TC scalable, We
further propose Approximated Topological Concentration (ATC) and
theoretically/empirically justify its efficacy in approximating TC and reducing
the computation complexity. Given the positive correlation between node TC and
its LP performance, we explore the potential of boosting LP performance via
enhancing TC by re-weighting edges in the message-passing and discuss its
effectiveness with limitations. Our code is publicly available at
https://github.com/YuWVandy/Topo_LP_GNN.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04617" title="Abstract">arXiv:2310.04617</a> [<a href="/pdf/2310.04617" title="Download PDF">pdf</a>, <a href="/format/2310.04617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SlotGNN: Unsupervised Discovery of Multi-Object Representations and  Visual Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rezazadeh%2C+A">Alireza Rezazadeh</a>, 
<a href="/search/cs?searchtype=author&query=Badithela%2C+A">Athreyi Badithela</a>, 
<a href="/search/cs?searchtype=author&query=Desingh%2C+K">Karthik Desingh</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+C">Changhyun Choi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Learning multi-object dynamics from visual data using unsupervised techniques
is challenging due to the need for robust, object representations that can be
learned through robot interactions. This paper presents a novel framework with
two new architectures: SlotTransport for discovering object representations
from RGB images and SlotGNN for predicting their collective dynamics from RGB
images and robot interactions. Our SlotTransport architecture is based on slot
attention for unsupervised object discovery and uses a feature transport
mechanism to maintain temporal alignment in object-centric representations.
This enables the discovery of slots that consistently reflect the composition
of multi-object scenes. These slots robustly bind to distinct objects, even
under heavy occlusion or absence. Our SlotGNN, a novel unsupervised graph-based
dynamics model, predicts the future state of multi-object scenes. SlotGNN
learns a graph representation of the scene using the discovered slots from
SlotTransport and performs relational and spatial reasoning to predict the
future appearance of each slot conditioned on robot actions. We demonstrate the
effectiveness of SlotTransport in learning object-centric features that
accurately encode both visual and positional information. Further, we highlight
the accuracy of SlotGNN in downstream robotic tasks, including challenging
multi-object rearrangement and long-horizon prediction. Finally, our
unsupervised approach proves effective in the real world. With only minimal
additional data, our framework robustly predicts slots and their corresponding
dynamics in real-world control tasks.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04618" title="Abstract">arXiv:2310.04618</a> [<a href="/pdf/2310.04618" title="Download PDF">pdf</a>, <a href="/format/2310.04618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KyberMat: Efficient Accelerator for Matrix-Vector Polynomial  Multiplication in CRYSTALS-Kyber Scheme via NTT and Polyphase Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+W">Weihang Tan</a>, 
<a href="/search/cs?searchtype=author&query=Lao%2C+Y">Yingjie Lao</a>, 
<a href="/search/cs?searchtype=author&query=Parhi%2C+K+K">Keshab K. Parhi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proc. 2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD), San Francisco, CA, Oct. 29 - Nov. 2, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">CRYSTAL-Kyber (Kyber) is one of the post-quantum cryptography (PQC)
key-encapsulation mechanism (KEM) schemes selected during the standardization
process. This paper addresses optimization for Kyber architecture with respect
to latency and throughput constraints. Specifically, matrix-vector
multiplication and number theoretic transform (NTT)-based polynomial
multiplication are critical operations and bottlenecks that require
optimization. To address this challenge, we propose an algorithm and hardware
co-design approach to systematically optimize matrix-vector multiplication and
NTT-based polynomial multiplication by employing a novel sub-structure sharing
technique in order to reduce computational complexity, i.e., the number of
modular multiplications and modular additions/subtractions consumed. The
sub-structure sharing approach is inspired by prior fast parallel approaches
based on polyphase decomposition. The proposed efficient feed-forward
architecture achieves high speed, low latency, and full utilization of all
hardware components, which can significantly enhance the overall efficiency of
the Kyber scheme. The FPGA implementation results show that our proposed
design, using the fast two-parallel structure, leads to an approximate
reduction of 90% in execution time, along with a 66 times improvement in
throughput performance.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04621" title="Abstract">arXiv:2310.04621</a> [<a href="/pdf/2310.04621" title="Download PDF">pdf</a>, <a href="/format/2310.04621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Compression in Practice: Lessons Learned from Practitioners  Creating On-device Machine Learning Experiences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hohman%2C+F">Fred Hohman</a>, 
<a href="/search/cs?searchtype=author&query=Kery%2C+M+B">Mary Beth Kery</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+D">Donghao Ren</a>, 
<a href="/search/cs?searchtype=author&query=Moritz%2C+D">Dominik Moritz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">On-device machine learning (ML) promises to improve the privacy,
responsiveness, and proliferation of new, intelligent user experiences by
moving ML computation onto everyday personal devices. However, today's large ML
models must be drastically compressed to run efficiently on-device, a hurtle
that requires deep, yet currently niche expertise. To engage the broader
human-centered ML community in on-device ML experiences, we present the results
from an interview study with 30 experts at Apple that specialize in producing
efficient models. We compile tacit knowledge that experts have developed
through practical experience with model compression across different hardware
platforms. Our findings offer pragmatic considerations missing from prior work,
covering the design process, trade-offs, and technical strategies that go into
creating efficient models. Finally, we distill design recommendations for
tooling to help ease the difficulty of this work and bring on-device ML into to
more widespread practice.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04623" title="Abstract">arXiv:2310.04623</a> [<a href="/pdf/2310.04623" title="Download PDF">pdf</a>, <a href="/format/2310.04623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deconstructing Cooperation and Ostracism via Multi-Agent Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ueshima%2C+A">Atsushi Ueshima</a>, 
<a href="/search/cs?searchtype=author&query=Omidshafiei%2C+S">Shayegan Omidshafiei</a>, 
<a href="/search/cs?searchtype=author&query=Shirado%2C+H">Hirokazu Shirado</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Cooperation is challenging in biological systems, human societies, and
multi-agent systems in general. While a group can benefit when everyone
cooperates, it is tempting for each agent to act selfishly instead. Prior human
studies show that people can overcome such social dilemmas while choosing
interaction partners, i.e., strategic network rewiring. However, little is
known about how agents, including humans, can learn about cooperation from
strategic rewiring and vice versa. Here, we perform multi-agent reinforcement
learning simulations in which two agents play the Prisoner's Dilemma game
iteratively. Each agent has two policies: one controls whether to cooperate or
defect; the other controls whether to rewire connections with another agent.
This setting enables us to disentangle complex causal dynamics between
cooperation and network rewiring. We find that network rewiring facilitates
mutual cooperation even when one agent always offers cooperation, which is
vulnerable to free-riding. We then confirm that the network-rewiring effect is
exerted through agents' learning of ostracism, that is, connecting to
cooperators and disconnecting from defectors. However, we also find that
ostracism alone is not sufficient to make cooperation emerge. Instead,
ostracism emerges from the learning of cooperation, and existing cooperation is
subsequently reinforced due to the presence of ostracism. Our findings provide
insights into the conditions and mechanisms necessary for the emergence of
cooperation with network rewiring.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04625" title="Abstract">arXiv:2310.04625</a> [<a href="/pdf/2310.04625" title="Download PDF">pdf</a>, <a href="/format/2310.04625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Copy Suppression: Comprehensively Understanding an Attention Head
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McDougall%2C+C">Callum McDougall</a>, 
<a href="/search/cs?searchtype=author&query=Conmy%2C+A">Arthur Conmy</a>, 
<a href="/search/cs?searchtype=author&query=Rushing%2C+C">Cody Rushing</a>, 
<a href="/search/cs?searchtype=author&query=McGrath%2C+T">Thomas McGrath</a>, 
<a href="/search/cs?searchtype=author&query=Nanda%2C+N">Neel Nanda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We present a single attention head in GPT-2 Small that has one main role
across the entire training distribution. If components in earlier layers
predict a certain token, and this token appears earlier in the context, the
head suppresses it: we call this copy suppression. Attention Head 10.7 (L10H7)
suppresses naive copying behavior which improves overall model calibration.
This explains why multiple prior works studying certain narrow tasks found
negative heads that systematically favored the wrong answer. We uncover the
mechanism that the Negative Heads use for copy suppression with weights-based
evidence and are able to explain 76.9% of the impact of L10H7 in GPT-2 Small.
To the best of our knowledge, this is the most comprehensive description of the
complete role of a component in a language model to date. One major effect of
copy suppression is its role in self-repair. Self-repair refers to how ablating
crucial model components results in downstream neural network parts
compensating for this ablation. Copy suppression leads to self-repair: if an
initial overconfident copier is ablated, then there is nothing to suppress. We
show that self-repair is implemented by several mechanisms, one of which is
copy suppression, which explains 39% of the behavior in a narrow task.
Interactive visualisations of the copy suppression phenomena may be seen at our
web app https://copy-suppression.streamlit.app/
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04627" title="Abstract">arXiv:2310.04627</a> [<a href="/pdf/2310.04627" title="Download PDF">pdf</a>, <a href="/format/2310.04627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Profit: Benchmarking Personalization and Robustness Trade-off in  Federated Prompt Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Collins%2C+L">Liam Collins</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shanshan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Sewoong Oh</a>, 
<a href="/search/cs?searchtype=author&query=Sim%2C+K+C">Khe Chai Sim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In many applications of federated learning (FL), clients desire models that
are personalized using their local data, yet are also robust in the sense that
they retain general global knowledge. However, the presence of data
heterogeneity across clients induces a fundamental trade-off between
personalization (i.e., adaptation to a local distribution) and robustness
(i.e., not forgetting previously learned general knowledge). It is critical to
understand how to navigate this personalization vs robustness trade-off when
designing federated systems, which are increasingly moving towards a paradigm
of fine-tuning large foundation models. Due to limited computational and
communication capabilities in most federated settings, this foundation model
fine-tuning must be done using parameter-efficient fine-tuning (PEFT)
approaches. While some recent work has studied federated approaches to PEFT,
the personalization vs robustness trade-off of federated PEFT has been largely
unexplored. In this work, we take a step towards bridging this gap by
benchmarking fundamental FL algorithms -- FedAvg and FedSGD plus
personalization (via client local fine-tuning) -- applied to one of the most
ubiquitous PEFT approaches to large language models (LLMs) -- prompt tuning --
in a multitude of hyperparameter settings under varying levels of data
heterogeneity. Our results show that federated-trained prompts can be
surprisingly robust when using a small learning rate with many local epochs for
personalization, especially when using an adaptive optimizer as the client
optimizer during federated training. We also demonstrate that simple approaches
such as adding regularization and interpolating two prompts are effective in
improving the personalization vs robustness trade-off in computation-limited
settings with few local updates allowed for personalization.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04628" title="Abstract">arXiv:2310.04628</a> [<a href="/pdf/2310.04628" title="Download PDF">pdf</a>, <a href="/format/2310.04628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> (Re)framing Built Heritage through the Machinic Gaze
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arora%2C+V">Vanicka Arora</a>, 
<a href="/search/cs?searchtype=author&query=Magee%2C+L">Liam Magee</a>, 
<a href="/search/cs?searchtype=author&query=Munn%2C+L">Luke Munn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Built heritage has been both subject and product of a gaze that has been
sustained through moments of colonial fixation on ruins and monuments,
technocratic examination and representation, and fetishisation by aglobal
tourist industry. We argue that the recent proliferation of machine learning
and vision technologies create new scopic regimes for heritage: storing and
retrieving existing images from vast digital archives, and further imparting
their own distortions upon its visual representation. We introduce the term
`machinic gaze' to conceptualise the reconfiguration of heritage representation
via AI models. To explore how this gaze reframes heritage, we deploy an
image-text-image pipeline that reads, interprets, and resynthesizes images of
several UNESCO World Heritage Sites. Employing two concepts from media studies
-- heteroscopia and anamorphosis -- we describe the reoriented perspective that
machine vision systems introduce. We propose that the machinic gaze highlights
the artifice of the human gaze and its underlying assumptions and practices
that combine to form established notions of heritage.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04631" title="Abstract">arXiv:2310.04631</a> [<a href="/pdf/2310.04631" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trust in Generative AI among students: An Exploratory Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amoozadeh%2C+M">Matin Amoozadeh</a>, 
<a href="/search/cs?searchtype=author&query=Daniels%2C+D">David Daniels</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+D">Daye Nam</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Stella Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hilton%2C+M">Michael Hilton</a>, 
<a href="/search/cs?searchtype=author&query=Ragavan%2C+S+S">Sruti Srinivasa Ragavan</a>, 
<a href="/search/cs?searchtype=author&query=Alipour%2C+M+A">Mohammad Amin Alipour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at SIGCSE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Generative artificial systems (GenAI) have experienced exponential growth in
the past couple of years. These systems offer exciting capabilities, such as
generating programs, that students can well utilize for their learning. Among
many dimensions that might affect the effective adoption of GenAI, in this
paper, we investigate students' \textit{trust}. Trust in GenAI influences the
extent to which students adopt GenAI, in turn affecting their learning. In this
study, we surveyed 253 students at two large universities to understand how
much they trust \genai tools and their feedback on how GenAI impacts their
performance in CS courses. Our results show that students have different levels
of trust in GenAI. We also observe different levels of confidence and
motivation, highlighting the need for further understanding of factors
impacting trust.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04632" title="Abstract">arXiv:2310.04632</a> [<a href="/pdf/2310.04632" title="Download PDF">pdf</a>, <a href="/format/2310.04632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Anonymization of Swiss Federal Supreme Court Rulings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niklaus%2C+J">Joel Niklaus</a>, 
<a href="/search/cs?searchtype=author&query=Mami%C3%A9%2C+R">Robin Mami&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=St%C3%BCrmer%2C+M">Matthias St&#xfc;rmer</a>, 
<a href="/search/cs?searchtype=author&query=Brunner%2C+D">Daniel Brunner</a>, 
<a href="/search/cs?searchtype=author&query=Gygli%2C+M">Marcel Gygli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Releasing court decisions to the public relies on proper anonymization to
protect all involved parties, where necessary. The Swiss Federal Supreme Court
relies on an existing system that combines different traditional computational
methods with human experts. In this work, we enhance the existing anonymization
software using a large dataset annotated with entities to be anonymized. We
compared BERT-based models with models pre-trained on in-domain data. Our
results show that using in-domain data to pre-train the models further improves
the F1-score by more than 5\% compared to existing models. Our work
demonstrates that combining existing anonymization methods, such as regular
expressions, with machine learning can further reduce manual labor and enhance
automatic suggestions.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04633" title="Abstract">arXiv:2310.04633</a> [<a href="/pdf/2310.04633" title="Download PDF">pdf</a>, <a href="/format/2310.04633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unbiased and Robust: External Attention-enhanced Graph Contrastive  Learning for Cross-domain Sequential Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinhua Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+H">Houping Yue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zizheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Liancheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures, accepted by ICDM 2023 (workshop-GML4Rec)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Cross-domain sequential recommenders (CSRs) are gaining considerable research
attention as they can capture user sequential preference by leveraging side
information from multiple domains. However, these works typically follow an
ideal setup, i.e., different domains obey similar data distribution, which
ignores the bias brought by asymmetric interaction densities (a.k.a. the
inter-domain density bias). Besides, the frequently adopted mechanism (e.g.,
the self-attention network) in sequence encoder only focuses on the
interactions within a local view, which overlooks the global correlations
between different training batches. To this end, we propose an External
Attention-enhanced Graph Contrastive Learning framework, namely EA-GCL.
Specifically, to remove the impact of the inter-domain density bias, an
auxiliary Self-Supervised Learning (SSL) task is attached to the traditional
graph encoder under a multi-task learning manner. To robustly capture users'
behavioral patterns, we develop an external attention-based sequence encoder
that contains an MLP-based memory-sharing structure. Unlike the self-attention
mechanism, such a structure can effectively alleviate the bias interference
from the batch-based training scheme. Extensive experiments on two real-world
datasets demonstrate that EA-GCL outperforms several state-of-the-art baselines
on CSR tasks. The source codes and relevant datasets are available at
https://github.com/HoupingY/EA-GCL.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04637" title="Abstract">arXiv:2310.04637</a> [<a href="/pdf/2310.04637" title="Download PDF">pdf</a>, <a href="/format/2310.04637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient State Estimation with Constrained Rao-Blackwellized Particle  Filter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuai Li</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+S">Siwei Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Trinkle%2C+J">Jeff Trinkle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Due to the limitations of the robotic sensors, during a robotic manipulation
task, the acquisition of the object's state can be unreliable and noisy.
Combining an accurate model of multi-body dynamic system with Bayesian
filtering methods has been shown to be able to filter out noise from the
object's observed states. However, efficiency of these filtering methods
suffers from samples that violate the physical constraints, e.g., no
penetration constraint.
<br />In this paper, we propose a Rao-Blackwellized Particle Filter (RBPF) that
samples the contact states and updates the object's poses using Kalman filters.
This RBPF also enforces the physical constraints on the samples by solving a
quadratic programming problem. By comparing our method with methods that does
not consider physical constraints, we show that our proposed RBPF is not only
able to estimate the object's states, e.g., poses, more accurately but also
able to infer unobserved states, e.g., velocities, with higher precision.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04639" title="Abstract">arXiv:2310.04639</a> [<a href="/pdf/2310.04639" title="Download PDF">pdf</a>, <a href="/format/2310.04639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> X-Transfer: A Transfer Learning-Based Framework for Robust GAN-Generated  Fake Image Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jinrong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures and 4 tables; references added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Generative adversarial networks (GANs) have remarkably advanced in diverse
domains, especially image generation and editing. However, the misuse of GANs
for generating deceptive images raises significant security concerns, including
face replacement and fake accounts, which have gained widespread attention.
Consequently, there is an urgent need for effective detection methods to
distinguish between real and fake images. Some of the current research centers
around the application of transfer learning. Nevertheless, it encounters
challenges such as knowledge forgetting from the original dataset and
inadequate performance when dealing with imbalanced data during training. To
alleviate the above issues, this paper introduces a novel GAN-generated image
detection algorithm called X-Transfer. This model enhances transfer learning by
utilizing two sibling neural networks that employ interleaved parallel gradient
transmission. This approach also effectively mitigates the problem of excessive
knowledge forgetting. In addition, we combine AUC loss term and cross-entropy
loss to enhance the model's performance comprehensively. The AUC loss
approximates the AUC metric using WMW statistics, ensuring differentiability
and improving the performance of traditional AUC evaluation. We carry out
comprehensive experiments on multiple facial image datasets. The results show
that our model outperforms the general transferring approach, and the best
accuracy achieves 99.04%, which is increased by approximately 10%. Furthermore,
we demonstrate excellent performance on non-face datasets, validating its
generality and broader application prospects.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04641" title="Abstract">arXiv:2310.04641</a> [<a href="/pdf/2310.04641" title="Download PDF">pdf</a>, <a href="/format/2310.04641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Equitable Peering: A Proposal for a Fair Peering Fee Between  ISPs and Content Providers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nikkhah%2C+A">Ali Nikkhah</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+S">Scott Jordan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Disagreements over peering fees have risen to the level of potential
government regulation. ISPs assert that content providers should pay them based
on the volume of downstream traffic. Transit providers and content providers
assert that consumers have already paid ISPs to transmit the content they
request and that peering agreements should be settlement-free.
<br />Our goal is to determine the fair payment between an ISP and an
interconnecting network. We consider fair cost sharing between two Tier-1 ISPs,
and derive the peering fee that equalizes their net backbone transportation
costs. We then consider fair cost sharing between an ISP and a transit
provider. We derive the peering fee that equalizes their net backbone
transportation costs, and illustrate how it depends on the traffic ratio and
the amount of localization of that content. Finally, we consider the fair
peering fee between an ISP and a content provider. We derive the peering fee
that results in the same net cost to the ISP, and illustrate how the peering
fee depends on the number of interconnection points and the amount of
localization of that content. We dispense with the ISP argument that it should
be paid regardless of the amount of localization of content.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04643" title="Abstract">arXiv:2310.04643</a> [<a href="/pdf/2310.04643" title="Download PDF">pdf</a>, <a href="/format/2310.04643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-implicit method of high-index saddle dynamics and application to  construct solution landscape
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Luo%2C+Y">Yue Luo</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+P">Pingwen Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Z">Zhiyi Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zheng%2C+X">Xiangcheng Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We analyze the semi-implicit scheme of high-index saddle dynamics, which
provides a powerful numerical method for finding the any-index saddle points
and constructing the solution landscape. Compared with the explicit schemes of
saddle dynamics, the semi-implicit discretization relaxes the step size and
accelerates the convergence, but the corresponding numerical analysis
encounters new difficulties compared to the explicit scheme. Specifically, the
orthonormal property of the eigenvectors at each time step could not be fully
employed due to the semi-implicit treatment, and computations of the
eigenvectors are coupled with the orthonormalization procedure, which further
complicates the numerical analysis. We address these issues to prove error
estimates of the semi-implicit scheme via, e.g. technical splittings and
multi-variable circulating induction procedure. We further analyze the
convergence rate of the generalized minimum residual solver for solving the
semi-implicit system. Extensive numerical experiments are carried out to
substantiate the efficiency and accuracy of the semi-implicit scheme in
constructing solution landscapes of complex systems.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04644" title="Abstract">arXiv:2310.04644</a> [<a href="/pdf/2310.04644" title="Download PDF">pdf</a>, <a href="/format/2310.04644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural2Speech: A Transfer Learning Framework for Neural-Driven Speech  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+C">Chunxu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+L">Li Fu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Lu Fan</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+E+F">Edward F. Chang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanning Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to 2024 IEEE International Conference on Acoustics, Speech and Signal Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Reconstructing natural speech from neural activity is vital for enabling
direct communication via brain-computer interfaces. Previous efforts have
explored the conversion of neural recordings into speech using complex deep
neural network (DNN) models trained on extensive neural recording data, which
is resource-intensive under regular clinical constraints. However, achieving
satisfactory performance in reconstructing speech from limited-scale neural
recordings has been challenging, mainly due to the complexity of speech
representations and the neural data constraints. To overcome these challenges,
we propose a novel transfer learning framework for neural-driven speech
reconstruction, called Neural2Speech, which consists of two distinct training
phases. First, a speech autoencoder is pre-trained on readily available speech
corpora to decode speech waveforms from the encoded speech representations.
Second, a lightweight adaptor is trained on the small-scale neural recordings
to align the neural activity and the speech representation for decoding.
Remarkably, our proposed Neural2Speech demonstrates the feasibility of
neural-driven speech reconstruction even with only 20 minutes of intracranial
data, which significantly outperforms existing baseline methods in terms of
speech fidelity and intelligibility.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04646" title="Abstract">arXiv:2310.04646</a> [<a href="/pdf/2310.04646" title="Download PDF">pdf</a>, <a href="/ps/2310.04646" title="Download PostScript">ps</a>, <a href="/format/2310.04646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Experimental Comparison of Methods for Computing the Numerical Radius
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mitchell%2C+T">Tim Mitchell</a>, 
<a href="/search/math?searchtype=author&query=Overton%2C+M+L">Michael L. Overton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We make an experimental comparison of methods for computing the numerical
radius of an $n\times n$ complex matrix, based on two well-known
characterizations, the first a nonconvex optimization problem in one real
variable and the second a convex optimization problem in $n^{2}+1$ real
variables. We make comparisons with respect to both accuracy and computation
time using publicly available software.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04648" title="Abstract">arXiv:2310.04648</a> [<a href="/pdf/2310.04648" title="Download PDF">pdf</a>, <a href="/format/2310.04648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DxPU: Large Scale Disaggregated GPU Pools in the Datacenter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bowen He</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weinan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yajin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+X">Xin Long</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pengcheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xiaowei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Linquan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Dennis Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiantao Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 6 figures, published in ACM Transactions on Architecture and Code Optimization
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">The rapid adoption of AI and convenience offered by cloud services have
resulted in the growing demands for GPUs in the cloud. Generally, GPUs are
physically attached to host servers as PCIe devices. However, the fixed
assembly combination of host servers and GPUs is extremely inefficient in
resource utilization, upgrade, and maintenance. Due to these issues, the GPU
disaggregation technique has been proposed to decouple GPUs from host servers.
It aggregates GPUs into a pool, and allocates GPU node(s) according to user
demands. However, existing GPU disaggregation systems have flaws in
software-hardware compatibility, disaggregation scope, and capacity. In this
paper, we present a new implementation of datacenter-scale GPU disaggregation,
named DxPU. DxPU efficiently solves the above problems and can flexibly
allocate as many GPU node(s) as users demand. In order to understand the
performance overhead incurred by DxPU, we build up a performance model for AI
specific workloads. With the guidance of modeling results, we develop a
prototype system, which has been deployed into the datacenter of a leading
cloud provider for a test run. We also conduct detailed experiments to evaluate
the performance overhead caused by our system. The results show that the
overhead of DxPU is less than 10%, compared with native GPU servers, in most of
user scenarios.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04649" title="Abstract">arXiv:2310.04649</a> [<a href="/pdf/2310.04649" title="Download PDF">pdf</a>, <a href="/format/2310.04649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NPEFF: Non-Negative Per-Example Fisher Factorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Matena%2C+M">Michael Matena</a>, 
<a href="/search/cs?searchtype=author&query=Raffel%2C+C">Colin Raffel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">As deep learning models are deployed in more and more settings, it becomes
increasingly important to be able to understand why they produce a given
prediction, but interpretation of these models remains a challenge. In this
paper, we introduce a novel interpretability method called NPEFF that is
readily applicable to any end-to-end differentiable model. It operates on the
principle that processing of a characteristic shared across different examples
involves a specific subset of model parameters. We perform NPEFF by decomposing
each example's Fisher information matrix as a non-negative sum of components.
These components take the form of either non-negative vectors or rank-1
positive semi-definite matrices depending on whether we are using diagonal or
low-rank Fisher representations, respectively. For the latter form, we
introduce a novel and highly scalable algorithm. We demonstrate that components
recovered by NPEFF have interpretable tunings through experiments on language
and vision models. Using unique properties of NPEFF's parameter-space
representations, we ran extensive experiments to verify that the connections
between directions in parameters space and examples recovered by NPEFF actually
reflect the model's processing. We further demonstrate NPEFF's ability to
uncover the actual processing strategies used by a TRACR-compiled model. We
further explore a potential application of NPEFF in uncovering and correcting
flawed heuristics used by a model. We release our code to facilitate research
using NPEFF.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04651" title="Abstract">arXiv:2310.04651</a> [<a href="/pdf/2310.04651" title="Download PDF">pdf</a>, <a href="/format/2310.04651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Peering Costs and Fees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nikkhah%2C+A">Ali Nikkhah</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+S">Scott Jordan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was accepted for presentation at the Pacific Telecommunications Council 2023 (PTC'23 conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Internet users have suffered collateral damage in tussles over paid peering
between large ISPs and large content providers. In order to qualify for
settlement-free peering, large Internet Service Providers (ISPs) require that
peers meet certain requirements. However, the academic literature has not yet
shown the relationship between these settlement-free peering requirements and
the value to each interconnecting network.
<br />We first consider the effect of paid peering on broadband prices. We adopt a
two-sided market model in which an ISP maximizes profit by setting broadband
prices and a paid peering price. Our result shows that paid peering fees reduce
the premium plan price, and increase the video streaming price and the total
price for premium tier customers who subscribe to video streaming services.
<br />We next consider the effect of paid peering on consumer surplus. We find that
consumer surplus is a uni-modal function of the paid peering fee. The peering
price depends critically on the incremental ISP cost per video streaming
subscriber; at different costs, it can be negative, zero, or positive.
<br />Last, we construct a network cost model. We show that the traffic-sensitive
network cost decreases as the number of interconnection points increases, but
with decreasing returns. Interconnecting at 6 to 8 interconnection points is
rational, and requiring interconnection at more than 8 points is of little
value. We show that if the content delivery network (CDN) delivers traffic to
the ISP locally, then a requirement to interconnect at a minimum number of
interconnection points is rational. We also show that if the CDN delivers
traffic using hot potato routing, the ISP is unlikely to perceive sufficient
value to offer settlement-free peering.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04652" title="Abstract">arXiv:2310.04652</a> [<a href="/pdf/2310.04652" title="Download PDF">pdf</a>, <a href="/format/2310.04652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Oracle Efficient Algorithms for Groupwise Regret
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Acharya%2C+K">Krishna Acharya</a>, 
<a href="/search/cs?searchtype=author&query=Arunachaleswaran%2C+E+R">Eshwar Ram Arunachaleswaran</a>, 
<a href="/search/cs?searchtype=author&query=Kannan%2C+S">Sampath Kannan</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+A">Aaron Roth</a>, 
<a href="/search/cs?searchtype=author&query=Ziani%2C+J">Juba Ziani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study the problem of online prediction, in which at each time step $t$, an
individual $x_t$ arrives, whose label we must predict. Each individual is
associated with various groups, defined based on their features such as age,
sex, race etc., which may intersect. Our goal is to make predictions that have
regret guarantees not just overall but also simultaneously on each sub-sequence
comprised of the members of any single group. Previous work such as [Blum &amp;
Lykouris] and [Lee et al] provide attractive regret guarantees for these
problems; however, these are computationally intractable on large model
classes. We show that a simple modification of the sleeping experts technique
of [Blum &amp; Lykouris] yields an efficient reduction to the well-understood
problem of obtaining diminishing external regret absent group considerations.
Our approach gives similar regret guarantees compared to [Blum &amp; Lykouris];
however, we run in time linear in the number of groups, and are
oracle-efficient in the hypothesis class. This in particular implies that our
algorithm is efficient whenever the number of groups is polynomially bounded
and the external-regret problem can be solved efficiently, an improvement on
[Blum &amp; Lykouris]'s stronger condition that the model class must be small. Our
approach can handle online linear regression and online combinatorial
optimization problems like online shortest paths. Beyond providing theoretical
regret bounds, we evaluate this algorithm with an extensive set of experiments
on synthetic data and on two real data sets -- Medical costs and the Adult
income dataset, both instantiated with intersecting groups defined in terms of
race, sex, and other demographic characteristics. We find that uniformly across
groups, our algorithm gives substantial error improvements compared to running
a standard online linear regression algorithm with no groupwise regret
guarantees.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04655" title="Abstract">arXiv:2310.04655</a> [<a href="/pdf/2310.04655" title="Download PDF">pdf</a>, <a href="/format/2310.04655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via  Pre-trained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Ziyi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+M">Muchao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianrong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+T">Tianyu Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jinguo Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Han Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinghui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Ting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fenglong Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Vision-Language (VL) pre-trained models have shown their superiority on many
multimodal tasks. However, the adversarial robustness of such models has not
been fully explored. Existing approaches mainly focus on exploring the
adversarial robustness under the white-box setting, which is unrealistic. In
this paper, we aim to investigate a new yet practical task to craft image and
text perturbations using pre-trained VL models to attack black-box fine-tuned
models on different downstream tasks. Towards this end, we propose VLAttack to
generate adversarial samples by fusing perturbations of images and texts from
both single-modal and multimodal levels. At the single-modal level, we propose
a new block-wise similarity attack (BSA) strategy to learn image perturbations
for disrupting universal representations. Besides, we adopt an existing text
attack strategy to generate text perturbations independent of the image-modal
attack. At the multimodal level, we design a novel iterative cross-search
attack (ICSA) method to update adversarial image-text pairs periodically,
starting with the outputs from the single-modal level. We conduct extensive
experiments to attack three widely-used VL pretrained models for six tasks on
eight datasets. Experimental results show that the proposed VLAttack framework
achieves the highest attack success rates on all tasks compared with
state-of-the-art baselines, which reveals a significant blind spot in the
deployment of pre-trained VL models. Codes will be released soon.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04656" title="Abstract">arXiv:2310.04656</a> [<a href="/pdf/2310.04656" title="Download PDF">pdf</a>, <a href="/format/2310.04656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerated high-index saddle dynamics method for searching high-index  saddle points
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Luo%2C+Y">Yue Luo</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zheng%2C+X">Xiangcheng Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The High-index saddle dynamics (HiSD) method serves as an efficient tool for
computing saddle points and constructing solution landscapes. Nevertheless, the
conventional HiSD method often encounters slow convergence rates on
ill-conditioned problems. To address this challenge, we propose an accelerated
high-index saddle dynamics (A-HiSD) by incorporating the heavy ball method. We
prove the linear stability theory of the continuous A-HiSD, and subsequently
estimate the local convergence rate for the discrete A-HiSD. Our analysis
demonstrates that the A-HiSD method exhibits a faster convergence rate compared
to the conventional HiSD method, especially when dealing with ill-conditioned
problems. We also perform various numerical experiments including the loss
function of neural network to substantiate the effectiveness and acceleration
of the A-HiSD method.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04662" title="Abstract">arXiv:2310.04662</a> [<a href="/pdf/2310.04662" title="Download PDF">pdf</a>, <a href="/format/2310.04662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HalluciDet: Hallucinating RGB Modality for Person Detection Through  Privileged Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Medeiros%2C+H+R">Heitor Rapela Medeiros</a>, 
<a href="/search/cs?searchtype=author&query=Pena%2C+F+A+G">Fidel A. Guerrero Pena</a>, 
<a href="/search/cs?searchtype=author&query=Aminbeidokhti%2C+M">Masih Aminbeidokhti</a>, 
<a href="/search/cs?searchtype=author&query=Dubail%2C+T">Thomas Dubail</a>, 
<a href="/search/cs?searchtype=author&query=Granger%2C+E">Eric Granger</a>, 
<a href="/search/cs?searchtype=author&query=Pedersoli%2C+M">Marco Pedersoli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A powerful way to adapt a visual recognition model to a new domain is through
image translation. However, common image translation approaches only focus on
generating data from the same distribution of the target domain. In visual
recognition tasks with complex images, such as pedestrian detection on aerial
images with a large cross-modal shift in data distribution from Infrared (IR)
to RGB images, a translation focused on generation might lead to poor
performance as the loss focuses on irrelevant details for the task. In this
paper, we propose HalluciDet, an IR-RGB image translation model for object
detection that, instead of focusing on reconstructing the original image on the
IR modality, is guided directly on reducing the detection loss of an RGB
detector, and therefore avoids the need to access RGB data. This model produces
a new image representation that enhances the object of interest in the scene
and greatly improves detection performance. We empirically compare our approach
against state-of-the-art image translation methods as well as with the commonly
used fine-tuning on IR, and show that our method improves detection accuracy in
most cases, by exploiting the privileged information encoded in a pre-trained
RGB detector.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04664" title="Abstract">arXiv:2310.04664</a> [<a href="/pdf/2310.04664" title="Download PDF">pdf</a>, <a href="/format/2310.04664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Rank Onset-Occurring-Offset Representations for  Micro-Expression Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jie Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zong%2C+Y">Yuan Zong</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jingang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Cheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+H">Hongli Chang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wenming Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper focuses on the research of micro-expression recognition (MER) and
proposes a flexible and reliable deep learning method called learning to rank
onset-occurring-offset representations (LTR3O). The LTR3O method introduces a
dynamic and reduced-size sequence structure known as 3O, which consists of
onset, occurring, and offset frames, for representing micro-expressions (MEs).
This structure facilitates the subsequent learning of ME-discriminative
features. A noteworthy advantage of the 3O structure is its flexibility, as the
occurring frame is randomly extracted from the original ME sequence without the
need for accurate frame spotting methods. Based on the 3O structures, LTR3O
generates multiple 3O representation candidates for each ME sample and
incorporates well-designed modules to measure and calibrate their emotional
expressiveness. This calibration process ensures that the distribution of these
candidates aligns with that of macro-expressions (MaMs) over time.
Consequently, the visibility of MEs can be implicitly enhanced, facilitating
the reliable learning of more discriminative features for MER. Extensive
experiments were conducted to evaluate the performance of LTR3O using three
widely-used ME databases: CASME II, SMIC, and SAMM. The experimental results
demonstrate the effectiveness and superior performance of LTR3O, particularly
in terms of its flexibility and reliability, when compared to recent
state-of-the-art MER methods.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04668" title="Abstract">arXiv:2310.04668</a> [<a href="/pdf/2310.04668" title="Download PDF">pdf</a>, <a href="/format/2310.04668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label-free Node Classification on Graphs with Large Language Models  (LLMS)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhikai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Haitao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">Hongzhi Wen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+H">Haoyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wei Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haiyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">In recent years, there have been remarkable advancements in node
classification achieved by Graph Neural Networks (GNNs). However, they
necessitate abundant high-quality labels to ensure promising performance. In
contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency
on text-attributed graphs. Yet, they face challenges in efficiently processing
structural data and suffer from high inference costs. In light of these
observations, this work introduces a label-free node classification on graphs
with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs
while mitigating their limitations. Specifically, LLMs are leveraged to
annotate a small portion of nodes and then GNNs are trained on LLMs'
annotations to make predictions for the remaining large portion of nodes. The
implementation of LLM-GNN faces a unique challenge: how can we actively select
nodes for LLMs to annotate and consequently enhance the GNN training? How can
we leverage LLMs to obtain annotations of high quality, representativeness, and
diversity, thereby enhancing GNN performance with less cost? To tackle this
challenge, we develop an annotation quality heuristic and leverage the
confidence scores derived from LLMs to advanced node selection. Comprehensive
experimental results validate the effectiveness of LLM-GNN. In particular,
LLM-GNN can achieve an accuracy of 74.9% on a vast-scale dataset \products with
a cost less than 1 dollar.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04671" title="Abstract">arXiv:2310.04671</a> [<a href="/pdf/2310.04671" title="Download PDF">pdf</a>, <a href="/format/2310.04671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Abductive Reasoning Meets Driving Hazard Prediction: Problem  Formulation and Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Charoenpitaks%2C+K">Korawat Charoenpitaks</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+V">Van-Quang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Suganuma%2C+M">Masanori Suganuma</a>, 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+M">Masahiro Takahashi</a>, 
<a href="/search/cs?searchtype=author&query=Niihara%2C+R">Ryoma Niihara</a>, 
<a href="/search/cs?searchtype=author&query=Okatani%2C+T">Takayuki Okatani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main Paper: 10 pages, Supplementary Materials: 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper addresses the problem of predicting hazards that drivers may
encounter while driving a car. We formulate it as a task of anticipating
impending accidents using a single input image captured by car dashcams. Unlike
existing approaches to driving hazard prediction that rely on computational
simulations or anomaly detection from videos, this study focuses on high-level
inference from static images. The problem needs predicting and reasoning about
future events based on uncertain observations, which falls under visual
abductive reasoning. To enable research in this understudied area, a new
dataset named the DHPR (Driving Hazard Prediction and Reasoning) dataset is
created. The dataset consists of 15K dashcam images of street scenes, and each
image is associated with a tuple containing car speed, a hypothesized hazard
description, and visual entities present in the scene. These are annotated by
human annotators, who identify risky scenes and provide descriptions of
potential accidents that could occur a few seconds later. We present several
baseline methods and evaluate their performance on our dataset, identifying
remaining issues and discussing future directions. This study contributes to
the field by introducing a novel problem formulation and dataset, enabling
researchers to explore the potential of multi-modal AI for driving hazard
prediction.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04672" title="Abstract">arXiv:2310.04672</a> [<a href="/pdf/2310.04672" title="Download PDF">pdf</a>, <a href="/format/2310.04672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EasyPhoto: Your Smart AI Photo Generator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Ziheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiaqi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+X">Xinyi Zou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kunzhe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xing Shi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jun Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Stable Diffusion web UI (SD-WebUI) is a comprehensive project that provides a
browser interface based on Gradio library for Stable Diffusion models. In this
paper, We propose a novel WebUI plugin called EasyPhoto, which enables the
generation of AI portraits. By training a digital doppelganger of a specific
user ID using 5 to 20 relevant images, the finetuned model (according to the
trained LoRA model) allows for the generation of AI photos using arbitrary
templates. Our current implementation supports the modification of multiple
persons and different photo styles. Furthermore, we allow users to generate
fantastic template image with the strong SDXL model, enhancing EasyPhoto's
capabilities to deliver more diverse and satisfactory results. The source code
for EasyPhoto is available at: https://github.com/aigc-apps/sd-webui-EasyPhoto.
We also support a webui-free version by using diffusers:
https://github.com/aigc-apps/EasyPhoto. We are continuously enhancing our
efforts to expand the EasyPhoto pipeline, making it suitable for any
identification (not limited to just the face), and we enthusiastically welcome
any intriguing ideas or suggestions.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04673" title="Abstract">arXiv:2310.04673</a> [<a href="/pdf/2310.04673" title="Download PDF">pdf</a>, <a href="/format/2310.04673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%5E%2C+J">Jiaming Wang^</a>, 
<a href="/search/cs?searchtype=author&query=Du%5E%2C+Z">Zhihao Du^</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+Y">Yunfei Chu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhifu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zerui Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+K">Kai Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaohuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Ziyang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Siqi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhijie Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiliang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Generative Pre-trained Transformer (GPT) models have achieved remarkable
performance on various natural language processing tasks. However, there has
been limited research on applying similar frameworks to audio tasks. Previously
proposed large language models for audio tasks either lack sufficient
quantitative evaluations, or are limited to tasks for recognizing and
understanding audio content, or significantly underperform existing
state-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified
GPT model for audio recognition, understanding, and generation. LauraGPT is a
versatile language model that can process both audio and text inputs and
generate outputs in either modalities. It can perform a wide range of tasks
related to content, semantics, paralinguistics, and audio-signal analysis. Some
of its noteworthy tasks include automatic speech recognition, speech-to-text
translation, text-to-speech synthesis, machine translation, speech enhancement,
automated audio captioning, speech emotion recognition, and spoken language
understanding. To achieve this goal, we use a combination of continuous and
discrete features for audio. We encode input audio into continuous
representations using an audio encoder and decode output audio from discrete
codec codes. We then fine-tune a large decoder-only Transformer-based language
model on multiple audio-to-text, text-to-audio, audio-to-audio, and
text-to-text tasks using a supervised multitask learning approach. Extensive
experiments show that LauraGPT achieves competitive or superior performance
compared to existing SOTA models on various audio processing benchmarks.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04674" title="Abstract">arXiv:2310.04674</a> [<a href="/pdf/2310.04674" title="Download PDF">pdf</a>, <a href="/format/2310.04674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling non-uniform uncertainty in Reaction Prediction via Boosting and  Dropout
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Taicheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Changsheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiuying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+B">Bozhao Nan</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+K">Kehan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+S">Shichao Pei</a>, 
<a href="/search/cs?searchtype=author&query=Chawla%2C+N+V">Nitesh V. Chawla</a>, 
<a href="/search/cs?searchtype=author&query=Wiest%2C+O">Olaf Wiest</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangliang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Chemical Physics (physics.chem-ph)

</div>
<p class="mathjax">Reaction prediction has been recognized as a critical task in synthetic
chemistry, where the goal is to predict the outcome of a reaction based on the
given reactants. With the widespread adoption of generative models, the
Variational Autoencoder(VAE) framework has typically been employed to tackle
challenges in reaction prediction, where the reactants are encoded as a
condition for the decoder, which then generates the product. Despite
effectiveness, these conditional VAE (CVAE) models still fail to adequately
account for the inherent uncertainty in reaction prediction, which primarily
stems from the stochastic reaction process. The principal limitations are
twofold. Firstly, in these CVAE models, the prior is independent of the
reactants, leading to a default wide and assumed uniform distribution variance
of the generated product. Secondly, reactants with analogous molecular
representations are presumed to undergo similar electronic transition
processes, thereby producing similar products. This hinders the ability to
model diverse reaction mechanisms effectively. Since the variance in outcomes
is inherently non-uniform, we are thus motivated to develop a framework that
generates reaction products with non-uniform uncertainty. Firstly, we eliminate
the latent variable in previous CVAE models to mitigate uncontrol-label noise.
Instead, we introduce randomness into product generation via boosting to
ensemble diverse models and cover the range of potential outcomes, and through
dropout to secure models with minor variations. Additionally, we design a
ranking method to union the predictions from boosting and dropout, prioritizing
the most plausible products. Experimental results on the largest reaction
prediction benchmark USPTO-MIT show the superior performance of our proposed
method in modeling the non-uniform uncertainty compared to baselines.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04675" title="Abstract">arXiv:2310.04675</a> [<a href="/pdf/2310.04675" title="Download PDF">pdf</a>, <a href="/format/2310.04675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Terrain-Aware Quadrupedal Locomotion via Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Haojie Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qingxu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Lei Han</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+W">Wanchao Chi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tingguang Li</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+M+Q+-">Max Q.-H. Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In nature, legged animals have developed the ability to adapt to challenging
terrains through perception, allowing them to plan safe body and foot
trajectories in advance, which leads to safe and energy-efficient locomotion.
Inspired by this observation, we present a novel approach to train a Deep
Neural Network (DNN) policy that integrates proprioceptive and exteroceptive
states with a parameterized trajectory generator for quadruped robots to
traverse rough terrains. Our key idea is to use a DNN policy that can modify
the parameters of the trajectory generator, such as foot height and frequency,
to adapt to different terrains. To encourage the robot to step on safe regions
and save energy consumption, we propose foot terrain reward and lifting foot
height reward, respectively. By incorporating these rewards, our method can
learn a safer and more efficient terrain-aware locomotion policy that can move
a quadruped robot flexibly in any direction. To evaluate the effectiveness of
our approach, we conduct simulation experiments on challenging terrains,
including stairs, stepping stones, and poles. The simulation results
demonstrate that our approach can successfully direct the robot to traverse
such tough terrains in any direction. Furthermore, we validate our method on a
real legged robot, which learns to traverse stepping stones with gaps over
25.5cm.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04676" title="Abstract">arXiv:2310.04676</a> [<a href="/pdf/2310.04676" title="Download PDF">pdf</a>, <a href="/format/2310.04676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surgical Gym: A high-performance GPU-based platform for reinforcement  learning with surgical robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmidgall%2C+S">Samuel Schmidgall</a>, 
<a href="/search/cs?searchtype=author&query=Krieger%2C+A">Axel Krieger</a>, 
<a href="/search/cs?searchtype=author&query=Eshraghian%2C+J">Jason Eshraghian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances in robot-assisted surgery have resulted in progressively more
precise, efficient, and minimally invasive procedures, sparking a new era of
robotic surgical intervention. This enables doctors, in collaborative
interaction with robots, to perform traditional or minimally invasive surgeries
with improved outcomes through smaller incisions. Recent efforts are working
toward making robotic surgery more autonomous which has the potential to reduce
variability of surgical outcomes and reduce complication rates. Deep
reinforcement learning methodologies offer scalable solutions for surgical
automation, but their effectiveness relies on extensive data acquisition due to
the absence of prior knowledge in successfully accomplishing tasks. Due to the
intensive nature of simulated data collection, previous works have focused on
making existing algorithms more efficient. In this work, we focus on making the
simulator more efficient, making training data much more accessible than
previously possible. We introduce Surgical Gym, an open-source high performance
platform for surgical robot learning where both the physics simulation and
reinforcement learning occur directly on the GPU. We demonstrate between
100-5000x faster training times compared with previous surgical learning
platforms. The code is available at:
https://github.com/SamuelSchmidgall/SurgicalGym.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04678" title="Abstract">arXiv:2310.04678</a> [<a href="/pdf/2310.04678" title="Download PDF">pdf</a>, <a href="/format/2310.04678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based  Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianyou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaicheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Naidu%2C+P">Prudhviraj Naidu</a>, 
<a href="/search/cs?searchtype=author&query=Bergen%2C+L">Leon Bergen</a>, 
<a href="/search/cs?searchtype=author&query=Paturi%2C+R">Ramamohan Paturi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In scientific research, the ability to effectively retrieve relevant
documents based on complex, multifaceted queries is critical. Existing
evaluation datasets for this task are limited, primarily due to the high cost
and effort required to annotate resources that effectively represent complex
queries. To address this, we propose a novel task, Scientific DOcument
Retrieval using Multi-level Aspect-based quEries (DORIS-MAE), which is designed
to handle the complex nature of user queries in scientific research. We
developed a benchmark dataset within the field of computer science, consisting
of 100 human-authored complex query cases. For each complex query, we assembled
a collection of 100 relevant documents and produced annotated relevance scores
for ranking them. Recognizing the significant labor of expert annotation, we
also introduce Anno-GPT, a scalable framework for validating the performance of
Large Language Models (LLMs) on expert-level dataset annotation tasks. LLM
annotation of the DORIS-MAE dataset resulted in a 500x reduction in cost,
without compromising quality. Furthermore, due to the multi-tiered structure of
these complex queries, the DORIS-MAE dataset can be extended to over 4,000
sub-query test cases without requiring additional annotation. We evaluated 17
recent retrieval methods on DORIS-MAE, observing notable performance drops
compared to traditional datasets. This highlights the need for better
approaches to handle complex, multifaceted queries in scientific research. Our
dataset and codebase are available at
https://github.com/Real-Doris-Mae/Doris-Mae-Dataset.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04680" title="Abstract">arXiv:2310.04680</a> [<a href="/pdf/2310.04680" title="Download PDF">pdf</a>, <a href="/format/2310.04680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Cost of Down-Scaling Language Models: Fact Recall Deteriorates  before In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+T">Tian Jin</a>, 
<a href="/search/cs?searchtype=author&query=Clement%2C+N">Nolan Clement</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xin Dong</a>, 
<a href="/search/cs?searchtype=author&query=Nagarajan%2C+V">Vaishnavh Nagarajan</a>, 
<a href="/search/cs?searchtype=author&query=Carbin%2C+M">Michael Carbin</a>, 
<a href="/search/cs?searchtype=author&query=Ragan-Kelley%2C+J">Jonathan Ragan-Kelley</a>, 
<a href="/search/cs?searchtype=author&query=Dziugaite%2C+G+K">Gintare Karolina Dziugaite</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">How does scaling the number of parameters in large language models (LLMs)
affect their core capabilities? We study two natural scaling techniques --
weight pruning and simply training a smaller or larger model, which we refer to
as dense scaling -- and their effects on two core capabilities of LLMs: (a)
recalling facts presented during pre-training and (b) processing information
presented in-context during inference. By curating a suite of tasks that help
disentangle these two capabilities, we find a striking difference in how these
two abilities evolve due to scaling. Reducing the model size by more than 30\%
(via either scaling approach) significantly decreases the ability to recall
facts seen in pre-training. Yet, a 60--70\% reduction largely preserves the
various ways the model can process in-context information, ranging from
retrieving answers from a long context to learning parameterized functions from
in-context exemplars. The fact that both dense scaling and weight pruning
exhibit this behavior suggests that scaling model size has an inherently
disparate effect on fact recall and in-context learning.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04681" title="Abstract">arXiv:2310.04681</a> [<a href="/pdf/2310.04681" title="Download PDF">pdf</a>, <a href="/format/2310.04681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VoiceExtender: Short-utterance Text-independent Speaker Verification  with Guided Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yayun He</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Z">Zuheng Kang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianzong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+J">Junqing Peng</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jing Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Speaker verification (SV) performance deteriorates as utterances become
shorter. To this end, we propose a new architecture called VoiceExtender which
provides a promising solution for improving SV performance when handling
short-duration speech signals. We use two guided diffusion models, the built-in
and the external speaker embedding (SE) guided diffusion model, both of which
utilize a diffusion model-based sample generator that leverages SE guidance to
augment the speech features based on a short utterance. Extensive experimental
results on the VoxCeleb1 dataset show that our method outperforms the baseline,
with relative improvements in equal error rate (EER) of 46.1%, 35.7%, 10.4%,
and 5.7% for the short utterance conditions of 0.5, 1.0, 1.5, and 2.0 seconds,
respectively.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04682" title="Abstract">arXiv:2310.04682</a> [<a href="/pdf/2310.04682" title="Download PDF">pdf</a>, <a href="/format/2310.04682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hypergraph Analysis Based on a Compatible Tensor Product Structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gu%2C+J">Jiaqi Gu</a>, 
<a href="/search/math?searchtype=author&query=Feng%2C+S">Shenghao Feng</a>, 
<a href="/search/math?searchtype=author&query=Wei%2C+Y">Yimin Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose a tensor product structure that is compatible with the hypergraph
structure. We define the algebraic connectivity of the $(m+1)$-uniform
hypergraph in this product, and prove the relationship with the vertex
connectivity. We introduce some connectivity optimization problem into the
hypergraph, and solve them with the algebraic connectivity. We introduce the
Laplacian eigenmap algorithm to the hypergraph under our tensor product.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04684" title="Abstract">arXiv:2310.04684</a> [<a href="/pdf/2310.04684" title="Download PDF">pdf</a>, <a href="/ps/2310.04684" title="Download PostScript">ps</a>, <a href="/format/2310.04684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Invariant Relations: A Bridge from Programs to Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghardallou%2C+W">Wided Ghardallou</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+H">Hessamaldin Mohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Brick%2C+E">Elijah Brick</a>, 
<a href="/search/cs?searchtype=author&query=Mili%2C+A">Ali Mili</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Discrete Mathematics (cs.DM); Programming Languages (cs.PL); Software Engineering (cs.SE)

</div>
<p class="mathjax">Great advances in program analysis would be enabled if it were possible to
derive the function of a program from inputs to outputs (or from initial states
to final states, depending on how we model program semantics). Efforts to do so
have always stalled against the difficulty to derive the function of loops; the
expedient solution to capture the function of loops by unrolling them an
arbitrary number of iterations is clearly inadequate. In this paper, we propose
a relations-based method to derive the function of a C-like program, including
programs that have loops nested to an arbitrary level. To capture the semantics
of loops, we use the concept of invariant relation.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04685" title="Abstract">arXiv:2310.04685</a> [<a href="/pdf/2310.04685" title="Download PDF">pdf</a>, <a href="/format/2310.04685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic and Efficient Customization of Neural Networks for ML  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuhan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+C">Chengcheng Wan</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+K">Kuntai Du</a>, 
<a href="/search/cs?searchtype=author&query=Hoffmann%2C+H">Henry Hoffmann</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junchen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Maire%2C+M">Michael Maire</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">ML APIs have greatly relieved application developers of the burden to design
and train their own neural network models -- classifying objects in an image
can now be as simple as one line of Python code to call an API. However, these
APIs offer the same pre-trained models regardless of how their output is used
by different applications. This can be suboptimal as not all ML inference
errors can cause application failures, and the distinction between inference
errors that can or cannot cause failures varies greatly across applications.
<br />To tackle this problem, we first study 77 real-world applications, which
collectively use six ML APIs from two providers, to reveal common patterns of
how ML API output affects applications' decision processes. Inspired by the
findings, we propose ChameleonAPI, an optimization framework for ML APIs, which
takes effect without changing the application source code. ChameleonAPI
provides application developers with a parser that automatically analyzes the
application to produce an abstract of its decision process, which is then used
to devise an application-specific loss function that only penalizes API output
errors critical to the application. ChameleonAPI uses the loss function to
efficiently train a neural network model customized for each application and
deploys it to serve API invocations from the respective application via
existing interface. Compared to a baseline that selects the best-of-all
commercial ML API, we show that ChameleonAPI reduces incorrect application
decisions by 43%.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04686" title="Abstract">arXiv:2310.04686</a> [<a href="/pdf/2310.04686" title="Download PDF">pdf</a>, <a href="/format/2310.04686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Rates in Supervised Outlier Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalan%2C+M+M">Mohammadreza M. Kalan</a>, 
<a href="/search/cs?searchtype=author&query=Kpotufe%2C+S">Samory Kpotufe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A critical barrier to learning an accurate decision rule for outlier
detection is the scarcity of outlier data. As such, practitioners often turn to
the use of similar but imperfect outlier data from which they might transfer
information to the target outlier detection task. Despite the recent empirical
success of transfer learning approaches in outlier detection, a fundamental
understanding of when and how knowledge can be transferred from a source to a
target outlier detection task remains elusive. In this work, we adopt the
traditional framework of Neyman-Pearson classification -- which formalizes
supervised outlier detection -- with the added assumption that one has access
to some related but imperfect outlier data. Our main results are as follows:
<br />We first determine the information-theoretic limits of the problem under a
measure of discrepancy that extends some existing notions from traditional
balanced classification; interestingly, unlike in balanced classification,
seemingly very dissimilar sources can provide much information about a target,
thus resulting in fast transfer.
<br />We then show that, in principle, these information-theoretic limits are
achievable by adaptive procedures, i.e., procedures with no a priori
information on the discrepancy between source and target outlier distributions.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04687" title="Abstract">arXiv:2310.04687</a> [<a href="/pdf/2310.04687" title="Download PDF">pdf</a>, <a href="/format/2310.04687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding and Improving Adversarial Attacks on Latent Diffusion  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+B">Boyang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+C">Chumeng Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaoyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yan Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Latent Diffusion Model (LDM) has emerged as a leading tool in image
generation, particularly with its capability in few-shot generation. This
capability also presents risks, notably in unauthorized artwork replication and
misinformation generation. In response, adversarial attacks have been designed
to safeguard personal images from being used as reference data. However,
existing adversarial attacks are predominantly empirical, lacking a solid
theoretical foundation. In this paper, we introduce a comprehensive theoretical
framework for understanding adversarial attacks on LDM. Based on the framework,
we propose a novel adversarial attack that exploits a unified target to guide
the adversarial attack both in the forward and the reverse process of LDM. We
provide empirical evidences that our method overcomes the offset problem of the
optimization of adversarial attacks in existing methods. Through rigorous
experiments, our findings demonstrate that our method outperforms current
attacks and is able to generalize over different state-of-the-art few-shot
generation pipelines based on LDM. Our method can serve as a stronger and
efficient tool for people exposed to the risk of data privacy and security to
protect themselves in the new era of powerful generative models. The code is
available on GitHub: https://github.com/CaradryanLiang/ImprovedAdvDM.git.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04688" title="Abstract">arXiv:2310.04688</a> [<a href="/pdf/2310.04688" title="Download PDF">pdf</a>, <a href="/format/2310.04688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PatchProto Networks for Few-shot Visual Anomaly Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+Y">Yue Zhuo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The visual anomaly diagnosis can automatically analyze the defective
products, which has been widely applied in industrial quality inspection. The
anomaly classification can classify the defective products into different
categories. However, the anomaly samples are hard to access in practice, which
impedes the training of canonical machine learning models. This paper studies a
practical issue that anomaly samples for training are extremely scarce, i.e.,
few-shot learning (FSL). Utilizing the sufficient normal samples, we propose
PatchProto networks for few-shot anomaly classification. Different from
classical FSL methods, PatchProto networks only extract CNN features of
defective regions of interest, which serves as the prototypes for few-shot
learning. Compared with basic few-shot classifier, the experiment results on
MVTec-AD dataset show PatchProto networks significantly improve the few-shot
anomaly classification accuracy.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04689" title="Abstract">arXiv:2310.04689</a> [<a href="/pdf/2310.04689" title="Download PDF">pdf</a>, <a href="/format/2310.04689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SeeDS: Semantic Separable Diffusion Synthesizer for Zero-shot Food  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pengfei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+W">Weiqing Min</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiajun Song</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Ying Jin</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shuqiang Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM Multimedia 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Food detection is becoming a fundamental task in food computing that supports
various multimedia applications, including food recommendation and dietary
monitoring. To deal with real-world scenarios, food detection needs to localize
and recognize novel food objects that are not seen during training, demanding
Zero-Shot Detection (ZSD). However, the complexity of semantic attributes and
intra-class feature diversity poses challenges for ZSD methods in
distinguishing fine-grained food classes. To tackle this, we propose the
Semantic Separable Diffusion Synthesizer (SeeDS) framework for Zero-Shot Food
Detection (ZSFD). SeeDS consists of two modules: a Semantic Separable
Synthesizing Module (S$^3$M) and a Region Feature Denoising Diffusion Model
(RFDDM). The S$^3$M learns the disentangled semantic representation for complex
food attributes from ingredients and cuisines, and synthesizes discriminative
food features via enhanced semantic information. The RFDDM utilizes a novel
diffusion model to generate diversified region features and enhances ZSFD via
fine-grained synthesized features. Extensive experiments show the
state-of-the-art ZSFD performance of our proposed method on two food datasets,
ZSFooD and UECFOOD-256. Moreover, SeeDS also maintains effectiveness on general
ZSD datasets, PASCAL VOC and MS COCO. The code and dataset can be found at
https://github.com/LanceZPF/SeeDS.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04690" title="Abstract">arXiv:2310.04690</a> [<a href="/pdf/2310.04690" title="Download PDF">pdf</a>, <a href="/format/2310.04690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A dimension-reduced variational approach for solving physics-based  inverse problems using generative adversarial network priors and normalizing  flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dasgupta%2C+A">Agnimitra Dasgupta</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+D+V">Dhruv V Patel</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+D">Deep Ray</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+E+A">Erik A Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Oberai%2C+A+A">Assad A Oberai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">We propose a novel modular inference approach combining two different
generative models -- generative adversarial networks (GAN) and normalizing
flows -- to approximate the posterior distribution of physics-based Bayesian
inverse problems framed in high-dimensional ambient spaces. We dub the proposed
framework GAN-Flow. The proposed method leverages the intrinsic dimension
reduction and superior sample generation capabilities of GANs to define a
low-dimensional data-driven prior distribution. Once a trained GAN-prior is
available, the inverse problem is solved entirely in the latent space of the
GAN using variational Bayesian inference with normalizing flow-based
variational distribution, which approximates low-dimensional posterior
distribution by transforming realizations from the low-dimensional latent prior
(Gaussian) to corresponding realizations of a low-dimensional variational
posterior distribution. The trained GAN generator then maps realizations from
this approximate posterior distribution in the latent space back to the
high-dimensional ambient space. We also propose a two-stage training strategy
for GAN-Flow wherein we train the two generative models sequentially.
Thereafter, GAN-Flow can estimate the statistics of posterior-predictive
quantities of interest at virtually no additional computational cost. The
synergy between the two types of generative models allows us to overcome many
challenges associated with the application of Bayesian inference to large-scale
inverse problems, chief among which are describing an informative prior and
sampling from the high-dimensional posterior. We demonstrate the efficacy and
flexibility of GAN-Flow on various physics-based inverse problems of varying
ambient dimensionality and prior knowledge using different types of GANs and
normalizing flows.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04691" title="Abstract">arXiv:2310.04691</a> [<a href="/pdf/2310.04691" title="Download PDF">pdf</a>, <a href="/format/2310.04691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EMO: Earth Mover Distance Optimization for Auto-Regressive Language  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Siyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K+Q">Kenny Q. Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Neural language models are probabilistic models of human text. They are
predominantly trained using maximum likelihood estimation (MLE), which is
equivalent to minimizing the forward cross-entropy between the empirical data
distribution and the model distribution. However, various degeneration
phenomena are still widely observed when decoding from the distributions
learned by such models. We establish that the forward cross-entropy is
suboptimal as a distance metric for aligning human and model distribution due
to its (1) recall-prioritization (2) negative diversity ignorance and (3)
train-test mismatch. In this paper, we propose Earth Mover Distance
Optimization (EMO) for auto-regressive language modeling. EMO capitalizes on
the inherent properties of earth mover distance to address the aforementioned
challenges. Due to the high complexity of direct computation, we further
introduce a feasible upper bound for EMO to ease end-to-end training. Upon
extensive evaluation of language models trained using EMO and MLE. We find that
EMO demonstrates a consistently better language modeling performance than MLE
across domains. Moreover, EMO demonstrates noteworthy enhancements in
downstream performance with minimal fine-tuning on merely 25,000 sentences.
This highlights the tremendous potential of EMO as a lightweight calibration
method for enhancing large-scale pre-trained language models.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04693" title="Abstract">arXiv:2310.04693</a> [<a href="/pdf/2310.04693" title="Download PDF">pdf</a>, <a href="/format/2310.04693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustness-enhanced Uplift Modeling with Adversarial Feature  Desensitization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zexu Sun</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bowei He</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Ming Ma</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiakai Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuchen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chen Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dugang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Uplift modeling has shown very promising results in online marketing.
However, most existing works are prone to the robustness challenge in some
practical applications. In this paper, we first present a possible explanation
for the above phenomenon. We verify that there is a feature sensitivity problem
in online marketing using different real-world datasets, where the perturbation
of some key features will seriously affect the performance of the uplift model
and even cause the opposite trend. To solve the above problem, we propose a
novel robustness-enhanced uplift modeling framework with adversarial feature
desensitization (RUAD). Specifically, our RUAD can more effectively alleviate
the feature sensitivity of the uplift model through two customized modules,
including a feature selection module with joint multi-label modeling to
identify a key subset from the input features and an adversarial feature
desensitization module using adversarial training and soft interpolation
operations to enhance the robustness of the model against this selected subset
of features. Finally, we conduct extensive experiments on a public dataset and
a real product dataset to verify the effectiveness of our RUAD in online
marketing. In addition, we also demonstrate the robustness of our RUAD to the
feature sensitivity, as well as the compatibility with different uplift models.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04694" title="Abstract">arXiv:2310.04694</a> [<a href="/pdf/2310.04694" title="Download PDF">pdf</a>, <a href="/format/2310.04694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DNA-Based Data Storage Systems: A Review of Implementations and Code  Constructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Milenkovic%2C+O">Olgica Milenkovic</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Chao Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Review paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">In this review paper, we delve into the nascent field of molecular data
storage, focusing on system implementations and code constructions. We start by
providing an overview of basic concepts in synthetic and computational biology.
Afterwards, we proceed with a review of the diverse approaches followed to
implement such systems. In the process, we identify new problems in
communication and coding theory, and discuss some relevant results pertaining
to DNA sequence profiles, coded trace reconstruction, coding for DNA punchcard
systems and coding for unique reconstruction.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04696" title="Abstract">arXiv:2310.04696</a> [<a href="/pdf/2310.04696" title="Download PDF">pdf</a>, <a href="/format/2310.04696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Serving Deep Learning Model in Relational Databases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eichenberger%2C+A">Alexandre Eichenberger</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Masood%2C+S">Saif Masood</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+H">Hong Min</a>, 
<a href="/search/cs?searchtype=author&query=Sim%2C+A">Alexander Sim</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yida Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kesheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+B">Binhang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Lixi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">Jia Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Authors are ordered alphabetically; Jia Zou is the corresponding author
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Serving deep learning (DL) models on relational data has become a critical
requirement across diverse commercial and scientific domains, sparking growing
interest recently. In this visionary paper, we embark on a comprehensive
exploration of representative architectures to address the requirement. We
highlight three pivotal paradigms: The state-of-the-artDL-Centricarchitecture
offloadsDL computations to dedicated DL frameworks. The potential UDF-Centric
architecture encapsulates one or more tensor computations into User Defined
Functions (UDFs) within the database system. The
potentialRelation-Centricarchitecture aims to represent a large-scale tensor
computation through relational operators. While each of these architectures
demonstrates promise in specific use scenarios, we identify urgent requirements
for seamless integration of these architectures and the middle ground between
these architectures. We delve into the gaps that impede the integration and
explore innovative strategies to close them. We present a pathway to establish
a novel database system for enabling a broad class of data-intensive DL
inference applications.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04698" title="Abstract">arXiv:2310.04698</a> [<a href="/pdf/2310.04698" title="Download PDF">pdf</a>, <a href="/format/2310.04698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tree-GPT: Modular Large Language Model Expert System for Forest Remote  Sensing Image Understanding and Interactive Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+S">Siqi Du</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Shengjun Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weixi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoming Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+R">Renzhong Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper introduces a novel framework, Tree-GPT, which incorporates Large
Language Models (LLMs) into the forestry remote sensing data workflow, thereby
enhancing the efficiency of data analysis. Currently, LLMs are unable to
extract or comprehend information from images and may generate inaccurate text
due to a lack of domain knowledge, limiting their use in forestry data
analysis. To address this issue, we propose a modular LLM expert system,
Tree-GPT, that integrates image understanding modules, domain knowledge bases,
and toolchains. This empowers LLMs with the ability to comprehend images,
acquire accurate knowledge, generate code, and perform data analysis in a local
environment. Specifically, the image understanding module extracts structured
information from forest remote sensing images by utilizing automatic or
interactive generation of prompts to guide the Segment Anything Model (SAM) in
generating and selecting optimal tree segmentation results. The system then
calculates tree structural parameters based on these results and stores them in
a database. Upon receiving a specific natural language instruction, the LLM
generates code based on a thought chain to accomplish the analysis task. The
code is then executed by an LLM agent in a local environment and . For
ecological parameter calculations, the system retrieves the corresponding
knowledge from the knowledge base and inputs it into the LLM to guide the
generation of accurate code. We tested this system on several tasks, including
Search, Visualization, and Machine Learning Analysis. The prototype system
performed well, demonstrating the potential for dynamic usage of LLMs in
forestry research and environmental sciences.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04699" title="Abstract">arXiv:2310.04699</a> [<a href="/pdf/2310.04699" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Review of Machine Learning Techniques for Power Electronics Control and  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bahrami%2C+M">Maryam Bahrami</a>, 
<a href="/search/eess?searchtype=author&query=Khashroum%2C+Z">Zeyad Khashroum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> COMPUTATIONAL RESEARCH PROGRESS IN APPLIED SCIENCE &amp; ENGINEERING
  Vol 9, Issue 3 (2023), 1-8
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In the rapidly advancing landscape of contemporary technology, power
electronics assume a pivotal role across diverse applications, ranging from
renewable energy systems to electric vehicles and consumer electronics. The
efficacy and precision of these power electronics systems stand as cornerstones
of their functionality. Within this context, the integration of machine
learning techniques assumes paramount significance. This article endeavors to
present an extensive and comprehensive review of the machine learning
techniques that find application in power electronics control and optimization.
Through meticulous exploration, we aim to elucidate the profound potential of
these methods in shaping the future of power electronics control and
optimization.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04701" title="Abstract">arXiv:2310.04701</a> [<a href="/pdf/2310.04701" title="Download PDF">pdf</a>, <a href="/format/2310.04701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Twin Graph-based Anomaly Detection via Attentive Multi-Modal Learning  for Microservice System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiao Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper have been accepted by ASE 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Microservice architecture has sprung up over recent years for managing
enterprise applications, due to its ability to independently deploy and scale
services. Despite its benefits, ensuring the reliability and safety of a
microservice system remains highly challenging. Existing anomaly detection
algorithms based on a single data modality (i.e., metrics, logs, or traces)
fail to fully account for the complex correlations and interactions between
different modalities, leading to false negatives and false alarms, whereas
incorporating more data modalities can offer opportunities for further
performance gain. As a fresh attempt, we propose in this paper a
semi-supervised graph-based anomaly detection method, MSTGAD, which seamlessly
integrates all available data modalities via attentive multi-modal learning.
First, we extract and normalize features from the three modalities, and further
integrate them using a graph, namely MST (microservice system twin) graph,
where each node represents a service instance and the edge indicates the
scheduling relationship between different service instances. The MST graph
provides a virtual representation of the status and scheduling relationships
among service instances of a real-world microservice system. Second, we
construct a transformer-based neural network with both spatial and temporal
attention mechanisms to model the inter-correlations between different
modalities and temporal dependencies between the data points. This enables us
to detect anomalies automatically and accurately in real-time. The source code
of MSTGAD is publicly available at
https://github.com/alipay/microservice_system_twin_graph_based_anomaly_detection.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04703" title="Abstract">arXiv:2310.04703</a> [<a href="/pdf/2310.04703" title="Download PDF">pdf</a>, <a href="/format/2310.04703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Contrastive Learning into a Multitask Transformer Model for  Effective Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahn%2C+C">Chung-Soo Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Rajapakse%2C+J+C">Jagath C. Rajapakse</a>, 
<a href="/search/cs?searchtype=author&query=Rana%2C+R">Rajib Rana</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">While speech emotion recognition (SER) research has made significant
progress, achieving generalization across various corpora continues to pose a
problem. We propose a novel domain adaptation technique that embodies a
multitask framework with SER as the primary task, and contrastive learning and
information maximisation loss as auxiliary tasks, underpinned by fine-tuning of
transformers pre-trained on large language models. Empirical results obtained
through experiments on well-established datasets like IEMOCAP and MSP-IMPROV,
illustrate that our proposed model achieves state-of-the-art performance in SER
within cross-corpus scenarios.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04704" title="Abstract">arXiv:2310.04704</a> [<a href="/pdf/2310.04704" title="Download PDF">pdf</a>, <a href="/format/2310.04704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EdgeFD: An Edge-Friendly Drift-Aware Fault Diagnosis System for  Industrial IoT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiao%2C+C">Chen Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Fengjian%2C+M">Mao Fengjian</a>, 
<a href="/search/cs?searchtype=author&query=Zuohong%2C+L">Lv Zuohong</a>, 
<a href="/search/cs?searchtype=author&query=Jianhua%2C+T">Tang Jianhua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE The 23rd International Conference on Communication Technology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent transfer learning (TL) approaches in industrial intelligent fault
diagnosis (FD) mostly follow the "pre-train and fine-tuning" paradigm to
address data drift, which emerges from variable working conditions. However, we
find that this approach is prone to the phenomenon known as catastrophic
forgetting. Furthermore, performing frequent models fine-tuning on the
resource-constrained edge nodes can be computationally expensive and
unnecessary, given the excellent transferability demonstrated by existing
models. In this work, we propose the Drift-Aware Weight Consolidation (DAWC), a
method optimized for edge deployments, mitigating the challenges posed by
frequent data drift in the industrial Internet of Things (IIoT). DAWC
efficiently manages multiple data drift scenarios, minimizing the need for
constant model fine-tuning on edge devices, thereby conserving computational
resources. By detecting drift using classifier confidence and estimating
parameter importance with the Fisher Information Matrix, a tool that measures
parameter sensitivity in probabilistic models, we introduce a drift detection
module and a continual learning module to gradually equip the FD model with
powerful generalization capabilities. Experimental results demonstrate that our
proposed DAWC achieves superior performance compared to existing techniques
while also ensuring compatibility with edge computing constraints.
Additionally, we have developed a comprehensive diagnosis and visualization
platform.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04706" title="Abstract">arXiv:2310.04706</a> [<a href="/pdf/2310.04706" title="Download PDF">pdf</a>, <a href="/format/2310.04706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline Imitation Learning with Variational Counterfactual Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bowei He</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zexu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chen Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In offline Imitation Learning (IL), an agent aims to learn an optimal expert
behavior policy without additional online environment interactions. However, in
many real-world scenarios, such as robotics manipulation, the offline dataset
is collected from suboptimal behaviors without rewards. Due to the scarce
expert data, the agents usually suffer from simply memorizing poor trajectories
and are vulnerable to the variations in the environments, lacking the
capability of generalizing to new environments. To effectively remove spurious
features that would otherwise bias the agent and hinder generalization, we
propose a framework named \underline{O}ffline \underline{I}mitation
\underline{L}earning with \underline{C}ounterfactual data
\underline{A}ugmentation (OILCA). In particular, we leverage the identifiable
variational autoencoder to generate \textit{counterfactual} samples. We
theoretically analyze the counterfactual identification and the improvement of
generalization. Moreover, we conduct extensive experiments to demonstrate that
our approach significantly outperforms various baselines on both
\textsc{DeepMind Control Suite} benchmark for in-distribution robustness and
\textsc{CausalWorld} benchmark for out-of-distribution generalization.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04711" title="Abstract">arXiv:2310.04711</a> [<a href="/pdf/2310.04711" title="Download PDF">pdf</a>, <a href="/format/2310.04711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DP-starJ: A Differential Private Scheme towards Analytical Star-Join  Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Congcong Fu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hui Li</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">Jian Lou</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jiangtao Cui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Databases (cs.DB)

</div>
<p class="mathjax">Star-join query is the fundamental task in data warehouse and has wide
applications in On-line Analytical Processing (OLAP) scenarios. Due to the
large number of foreign key constraints and the asymmetric effect in the
neighboring instance between the fact and dimension tables, even those latest
DP efforts specifically designed for join, if directly applied to star-join
query, will suffer from extremely large estimation errors and expensive
computational cost. In this paper, we are thus motivated to propose DP-starJ, a
novel Differentially Private framework for star-Join queries. DP-starJ consists
of a series of strategies tailored to specific features of star-join, including
1) we unveil the different effect of fact and dimension tables on the
neighboring database instances, and accordingly revisit the definitions
tailored to different cases of star-join; 2) we propose Predicate Mechanism
(PM), which utilizes predicate perturbation to inject noise into the join
procedure instead of the results; 3) to further boost the robust performance,
we propose a DP-compliant star-join algorithm for various types of star-join
tasks based on PM. We provide both theoretical analysis and empirical study,
which demonstrate the superiority of the proposed methods over the
state-of-the-art solutions in terms of accuracy, efficiency, and scalability.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04712" title="Abstract">arXiv:2310.04712</a> [<a href="/pdf/2310.04712" title="Download PDF">pdf</a>, <a href="/format/2310.04712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UFD-PRiME: Unsupervised Joint Learning of Optical Flow and Stereo Depth  through Pixel-Level Rigid Motion Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shuai Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Tomasi%2C+C">Carlo Tomasi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Both optical flow and stereo disparities are image matches and can therefore
benefit from joint training. Depth and 3D motion provide geometric rather than
photometric information and can further improve optical flow. Accordingly, we
design a first network that estimates flow and disparity jointly and is trained
without supervision. A second network, trained with optical flow from the first
as pseudo-labels, takes disparities from the first network, estimates 3D rigid
motion at every pixel, and reconstructs optical flow again. A final stage fuses
the outputs from the two networks. In contrast with previous methods that only
consider camera motion, our method also estimates the rigid motions of dynamic
objects, which are of key interest in applications. This leads to better
optical flow with visibly more detailed occlusions and object boundaries as a
result. Our unsupervised pipeline achieves 7.36% optical flow error on the
KITTI-2015 benchmark and outperforms the previous state-of-the-art 9.38% by a
wide margin. It also achieves slightly better or comparable stereo depth
results. Code will be made available.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04714" title="Abstract">arXiv:2310.04714</a> [<a href="/pdf/2310.04714" title="Download PDF">pdf</a>, <a href="/format/2310.04714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Robust Test-Time Adaptation in Continuous Dynamic Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Longhui Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+B">Binhui Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Test-time adaptation (TTA) adapts the pre-trained models to test
distributions during the inference phase exclusively employing unlabeled test
data streams, which holds great value for the deployment of models in
real-world applications. Numerous studies have achieved promising performance
on simplistic test streams, characterized by independently and uniformly
sampled test data originating from a fixed target data distribution. However,
these methods frequently prove ineffective in practical scenarios, where both
continual covariate shift and continual label shift occur simultaneously, i.e.,
data and label distributions change concurrently and continually over time. In
this study, a more challenging Practical Test-Time Adaptation (PTTA) setup is
introduced, which takes into account the concurrent presence of continual
covariate shift and continual label shift, and we propose a Generalized Robust
Test-Time Adaptation (GRoTTA) method to effectively address the difficult
problem. We start by steadily adapting the model through Robust Parameter
Adaptation to make balanced predictions for test samples. To be specific,
firstly, the effects of continual label shift are eliminated by enforcing the
model to learn from a uniform label distribution and introducing recalibration
of batch normalization to ensure stability. Secondly, the continual covariate
shift is alleviated by employing a source knowledge regularization with the
teacher-student model to update parameters. Considering the potential
information in the test stream, we further refine the balanced predictions by
Bias-Guided Output Adaptation, which exploits latent structure in the feature
space and is adaptive to the imbalanced label distribution. Extensive
experiments demonstrate GRoTTA outperforms the existing competitors by a large
margin under PTTA setting, rendering it highly conducive for adoption in
real-world applications.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04716" title="Abstract">arXiv:2310.04716</a> [<a href="/pdf/2310.04716" title="Download PDF">pdf</a>, <a href="/format/2310.04716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforced UI Instruction Grounding: Towards a Generic UI Task  Automation API
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhizheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Wenxuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yan Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent popularity of Large Language Models (LLMs) has opened countless
possibilities in automating numerous AI tasks by connecting LLMs to various
domain-specific models or APIs, where LLMs serve as dispatchers while
domain-specific models or APIs are action executors. Despite the vast numbers
of domain-specific models/APIs, they still struggle to comprehensively cover
super diverse automation demands in the interaction between human and User
Interfaces (UIs). In this work, we build a multimodal model to ground natural
language instructions in given UI screenshots as a generic UI task automation
executor. This metadata-free grounding model, consisting of a visual encoder
and a language decoder, is first pretrained on well studied document
understanding tasks and then learns to decode spatial information from UI
screenshots in a promptable way. To facilitate the exploitation of
image-to-text pretrained knowledge, we follow the pixel-to-sequence paradigm to
predict geometric coordinates in a sequence of tokens using a language decoder.
We further propose an innovative Reinforcement Learning (RL) based algorithm to
supervise the tokens in such sequence jointly with visually semantic metrics,
which effectively strengthens the spatial decoding capability of the
pixel-to-sequence paradigm. Extensive experiments demonstrate our proposed
reinforced UI instruction grounding model outperforms the state-of-the-art
methods by a clear margin and shows the potential as a generic UI task
automation API.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04719" title="Abstract">arXiv:2310.04719</a> [<a href="/pdf/2310.04719" title="Download PDF">pdf</a>, <a href="/format/2310.04719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey on Deep Neural Image Deblurring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Biyouki%2C+S+A">Sajjad Amrollahi Biyouki</a>, 
<a href="/search/cs?searchtype=author&query=Hwangbo%2C+H">Hoon Hwangbo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Image deblurring tries to eliminate degradation elements of an image causing
blurriness and improve the quality of an image for better texture and object
visualization. Traditionally, prior-based optimization approaches predominated
in image deblurring, but deep neural networks recently brought a major
breakthrough in the field. In this paper, we comprehensively review the recent
progress of the deep neural architectures in both blind and non-blind image
deblurring. We outline the most popular deep neural network structures used in
deblurring applications, describe their strengths and novelties, summarize
performance metrics, and introduce broadly used datasets. In addition, we
discuss the current challenges and research gaps in this domain and suggest
potential research directions for future works.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04721" title="Abstract">arXiv:2310.04721</a> [<a href="/pdf/2310.04721" title="Download PDF">pdf</a>, <a href="/format/2310.04721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory-Constrained Semantic Segmentation for Ultra-High Resolution UAV  Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qi Li</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jiaxin Cai</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yuanlong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jason Gu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jia Pan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenxi Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Amidst the swift advancements in photography and sensor technologies,
high-definition cameras have become commonplace in the deployment of Unmanned
Aerial Vehicles (UAVs) for diverse operational purposes. Within the domain of
UAV imagery analysis, the segmentation of ultra-high resolution images emerges
as a substantial and intricate challenge, especially when grappling with the
constraints imposed by GPU memory-restricted computational devices. This paper
delves into the intricate problem of achieving efficient and effective
segmentation of ultra-high resolution UAV imagery, while operating under
stringent GPU memory limitation. The strategy of existing approaches is to
downscale the images to achieve computationally efficient segmentation.
However, this strategy tends to overlook smaller, thinner, and curvilinear
regions. To address this problem, we propose a GPU memory-efficient and
effective framework for local inference without accessing the context beyond
local patches. In particular, we introduce a novel spatial-guided
high-resolution query module, which predicts pixel-wise segmentation results
with high quality only by querying nearest latent embeddings with the guidance
of high-resolution information. Additionally, we present an efficient
memory-based interaction scheme to correct potential semantic bias of the
underlying high-resolution information by associating cross-image contextual
semantics. For evaluation of our approach, we perform comprehensive experiments
over public benchmarks and achieve superior performance under both conditions
of small and large GPU memory usage limitations. We will release the model and
codes in the future.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04722" title="Abstract">arXiv:2310.04722</a> [<a href="/pdf/2310.04722" title="Download PDF">pdf</a>, <a href="/format/2310.04722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Holistic Evaluation of Piano Sound Quality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Monan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shangda Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Shaohua Ji</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zijin Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper aims to develop a holistic evaluation method for piano sound
quality to assist in purchasing decisions. Unlike previous studies that focused
on the effect of piano performance techniques on sound quality, this study
evaluates the inherent sound quality of different pianos. To derive quality
evaluation systems, the study uses subjective questionnaires based on a piano
sound quality dataset. The method selects the optimal piano classification
models by comparing the fine-tuning results of different pre-training models of
Convolutional Neural Networks (CNN). To improve the interpretability of the
models, the study applies Equivalent Rectangular Bandwidth (ERB) analysis. The
results reveal that musically trained individuals are better able to
distinguish between the sound quality differences of different pianos. The best
fine-tuned CNN pre-trained backbone achieves a high accuracy of 98.3\% as the
piano classifier. However, the dataset is limited, and the audio is sliced to
increase its quantity, resulting in a lack of diversity and balance, so we use
focal loss to reduce the impact of data imbalance. To optimize the method, the
dataset will be expanded, or few-shot learning techniques will be employed in
future research.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04723" title="Abstract">arXiv:2310.04723</a> [<a href="/pdf/2310.04723" title="Download PDF">pdf</a>, <a href="/format/2310.04723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subspace Identification for Multi-Source Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zijian Li</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+R">Ruichu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Boyang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zhifeng Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Multi-source domain adaptation (MSDA) methods aim to transfer knowledge from
multiple labeled source domains to an unlabeled target domain. Although current
methods achieve target joint distribution identifiability by enforcing minimal
changes across domains, they often necessitate stringent conditions, such as an
adequate number of domains, monotonic transformation of latent variables, and
invariant label distributions. These requirements are challenging to satisfy in
real-world applications. To mitigate the need for these strict assumptions, we
propose a subspace identification theory that guarantees the disentanglement of
domain-invariant and domain-specific variables under less restrictive
constraints regarding domain numbers and transformation properties, thereby
facilitating domain adaptation by minimizing the impact of domain shifts on
invariant variables. Based on this theory, we develop a Subspace Identification
Guarantee (SIG) model that leverages variational inference. Furthermore, the
SIG model incorporates class-aware conditional alignment to accommodate target
shifts where label distributions change with the domains. Experimental results
demonstrate that our SIG model outperforms existing MSDA techniques on various
benchmark datasets, highlighting its effectiveness in real-world applications.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04724" title="Abstract">arXiv:2310.04724</a> [<a href="/pdf/2310.04724" title="Download PDF">pdf</a>, <a href="/format/2310.04724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Activate and Reject: Towards Safe Domain Generalization under Category  Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chaoqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+L">Luyao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+L">Leitian Tao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hong-Yu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yue Huang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiaoguang Han</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yizhou Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Albeit the notable performance on in-domain test points, it is non-trivial
for deep neural networks to attain satisfactory accuracy when deploying in the
open world, where novel domains and object classes often occur. In this paper,
we study a practical problem of Domain Generalization under Category Shift
(DGCS), which aims to simultaneously detect unknown-class samples and classify
known-class samples in the target domains. Compared to prior DG works, we face
two new challenges: 1) how to learn the concept of ``unknown'' during training
with only source known-class samples, and 2) how to adapt the source-trained
model to unseen environments for safe model deployment. To this end, we propose
a novel Activate and Reject (ART) framework to reshape the model's decision
boundary to accommodate unknown classes and conduct post hoc modification to
further discriminate known and unknown classes using unlabeled test data.
Specifically, during training, we promote the response to the unknown by
optimizing the unknown probability and then smoothing the overall output to
mitigate the overconfidence issue. At test time, we introduce a step-wise
online adaptation method that predicts the label by virtue of the cross-domain
nearest neighbor and class prototype information without updating the network's
parameters or using threshold-based mechanisms. Experiments reveal that ART
consistently improves the generalization capability of deep networks on
different vision tasks. For image classification, ART improves the H-score by
6.1% on average compared to the previous best method. For object detection and
semantic segmentation, we establish new benchmarks and achieve competitive
performance.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04726" title="Abstract">arXiv:2310.04726</a> [<a href="/pdf/2310.04726" title="Download PDF">pdf</a>, <a href="/format/2310.04726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-shot Cross-lingual Transfer without Parallel Corpus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiaofeng Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baojun Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, although pre-trained language models have achieved great success on
multilingual NLP (Natural Language Processing) tasks, the lack of training data
on many tasks in low-resource languages still limits their performance. One
effective way of solving that problem is to transfer knowledge from
rich-resource languages to low-resource languages. However, many previous works
on cross-lingual transfer rely heavily on the parallel corpus or translation
models, which are often difficult to obtain. We propose a novel approach to
conduct zero-shot cross-lingual transfer with a pre-trained model. It consists
of a Bilingual Task Fitting module that applies task-related bilingual
information alignment; a self-training module generates pseudo soft and hard
labels for unlabeled data and utilizes them to conduct self-training. We got
the new SOTA on different tasks without any dependencies on the parallel corpus
or translation models.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04727" title="Abstract">arXiv:2310.04727</a> [<a href="/pdf/2310.04727" title="Download PDF">pdf</a>, <a href="/format/2310.04727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task Aware Modulation using Representation Learning: An Approach for Few  Shot Learning in Heterogeneous Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Renganathan%2C+A">Arvind Renganathan</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+R">Rahul Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Khandelwal%2C+A">Ankush Khandelwal</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V">Vipin Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present a Task-aware modulation using Representation Learning (TAM-RL)
framework that enhances personalized predictions in few-shot settings for
heterogeneous systems when individual task characteristics are not known.
TAM-RL extracts embeddings representing the actual inherent characteristics of
these entities and uses these characteristics to personalize the predictions
for each entity/task. Using real-world hydrological and flux tower benchmark
data sets, we show that TAM-RL can significantly outperform existing baseline
approaches such as MAML and multi-modal MAML (MMAML) while being much faster
and simpler to train due to less complexity. Specifically, TAM-RL eliminates
the need for sensitive hyper-parameters like inner loop steps and inner loop
learning rate, which are crucial for model convergence in MAML, MMAML. We
further present an empirical evaluation via synthetic data to explore the
impact of heterogeneity amongst the entities on the relative performance of
MAML, MMAML, and TAM-RL. We show that TAM-RL significantly improves predictive
performance for cases where it is possible to learn distinct representations
for different tasks.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04729" title="Abstract">arXiv:2310.04729</a> [<a href="/pdf/2310.04729" title="Download PDF">pdf</a>, <a href="/format/2310.04729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Current Trends and Advances in Quantum Navigation for Maritime  Applications: A Comprehensive Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sambataro%2C+O">Olga Sambataro</a>, 
<a href="/search/cs?searchtype=author&query=Costanzi%2C+R">Riccardo Costanzi</a>, 
<a href="/search/cs?searchtype=author&query=Alves%2C+J">Joao Alves</a>, 
<a href="/search/cs?searchtype=author&query=Caiti%2C+A">Andrea Caiti</a>, 
<a href="/search/cs?searchtype=author&query=Paglierani%2C+P">Pietro Paglierani</a>, 
<a href="/search/cs?searchtype=author&query=Petroccia%2C+R">Roberto Petroccia</a>, 
<a href="/search/cs?searchtype=author&query=Munafo%2C+A">Andrea Munafo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper presents a comprehensive review of the current state of the art in
quantum navigation systems, with a specific focus on their application in
maritime navigation. Quantum technologies have the potential to revolutionise
navigation and positioning systems due to their ability to provide highly
accurate and secure information. The review covers the principles of quantum
navigation and highlights the latest developments in quantum-enhanced sensors,
atomic clocks, and quantum communication protocols. The paper also discusses
the challenges and opportunities of using quantum technologies in maritime
navigation, including the effects that the maritime environment and the
specificity of marine applications can have on the performance of quantum
sensors. Finally, the paper concludes with a discussion on the future of
quantum navigation systems and their potential impact on the maritime industry.
This review aims at providing a valuable resource for researchers and engineers
interested in the development and deployment of quantum navigation systems.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04731" title="Abstract">arXiv:2310.04731</a> [<a href="/pdf/2310.04731" title="Download PDF">pdf</a>, <a href="/ps/2310.04731" title="Download PostScript">ps</a>, <a href="/format/2310.04731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kawaii Game Vocalics: A Preliminary Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seaborn%2C+K">Katie Seaborn</a>, 
<a href="/search/cs?searchtype=author&query=Rogers%2C+K">Katja Rogers</a>, 
<a href="/search/cs?searchtype=author&query=Name%2C+S">Somang Name</a>, 
<a href="/search/cs?searchtype=author&query=Kojima%2C+M">Miu Kojima</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CHI PLAY '23
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SCompanion Proceedings of the Annual Symposium on Computer-Human
  Interaction in Play, 202-208 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Kawaii is the Japanese concept of cute++, a global export with local
characteristics. Recent work has explored kawaii as a feature of user
experience (UX) with social robots, virtual characters, and voice assistants,
i.e., kawaii vocalics. Games have a long history of incorporating characters
that use voice as a means of expressing kawaii. Nevertheless, no work to date
has evaluated kawaii game voices or mapped out a model of kawaii game vocalics.
In this work, we explored whether and how a model of kawaii vocalics maps onto
game character voices. We conducted an online perceptions study (N=157) using
18 voices from kawaii characters in Japanese games. We replicated the results
for computer voice and discovered nuanced relationships between gender and age,
especially youthfulness, agelessness, gender ambiguity, and gender neutrality.
We provide our initial model and advocate for future work on character visuals
and within play contexts.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04733" title="Abstract">arXiv:2310.04733</a> [<a href="/pdf/2310.04733" title="Download PDF">pdf</a>, <a href="/ps/2310.04733" title="Download PostScript">ps</a>, <a href="/format/2310.04733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meaningful play and malicious delight: Exploring maldaimonic game UX
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seaborn%2C+K">Katie Seaborn</a>, 
<a href="/search/cs?searchtype=author&query=Iseya%2C+S">Satoru Iseya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CHI PLAY '23
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Companion Proceedings of the Annual Symposium on Computer-Human
  Interaction in Play, 174-180 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Maldaimonia is a new experiential concept that refers to self-actualization
and self-expression through egocentric, destructive, and/or exploitative
activities. Still, it is unclear whether maldaimonia is an actual facet of real
experience. As a subversive orientation, it may be rare or socially challenging
to discuss openly. However, video games provide a space in which people can be
expressive in different ways without the same repercussions as in real life.
Indeed, game spaces may be one of the few contexts in which to study
maldaimonic experiences. In this study, we examined whether and how maldaimonia
exists as a feature of game user experiences by analyzing critical self-reports
of gaming activities, confirming its existence. We contribute this new
construct to work on "dark play" in games research.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04734" title="Abstract">arXiv:2310.04734</a> [<a href="/pdf/2310.04734" title="Download PDF">pdf</a>, <a href="/format/2310.04734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient solution strategies for cabin noise assessment of a wave  resolving aircraft fuselage model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blech%2C+C">Christopher Blech</a>, 
<a href="/search/cs?searchtype=author&query=Sreekumar%2C+H+K">Harikrishnan K. Sreekumar</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCpel%2C+Y">Yannik H&#xfc;pel</a>, 
<a href="/search/cs?searchtype=author&query=Langer%2C+S+C">Sabine C. Langer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 7 figures, Submitted to IJNME
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">For the purpose of high-fidelity aircraft cabin noise simulations during
early design phases, we study three efficient solving approaches for the fully
coupled finite element model of an aircraft fuselage segment. Obtaining an
efficient solution with respect to consumed computational time and resources is
challenging within a conventional simulation pipeline, as large-scale and
complex vibroacoustic models demand crucially high computational costs with
increasing frequency. In this contribution, we adopt (1) frequency and
domain-adaptive discretisation, (2) domain-decomposition techniques, and (3)
model order reduction with rational Arnoldi Krylov subspace methods for an
aircraft fuselage model. The three approaches have shown remarkable advantage
thereby reducing the solving time as well as the memory requirement that are
essential when solving large-scale models. While the discretisation and the
model order reduction approaches accelerate the solving process by efficiently
handling the complexity of the system to be solved, domain-decomposition
techniques further handle the aspect of reducing the overall memory
consumption. Finally with the help of active research aircraft models, we
implement and showcase the achieved efficiency.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04735" title="Abstract">arXiv:2310.04735</a> [<a href="/pdf/2310.04735" title="Download PDF">pdf</a>, <a href="/format/2310.04735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Influence of Legal Case Retrieval Systems on Users&#x27;  Decision Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Beining Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruizhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yueyue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+Q">Qingyao Ai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiqun Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Given a specific query case, legal case retrieval systems aim to retrieve a
set of case documents relevant to the case at hand. Previous studies on user
behavior analysis have shown that information retrieval (IR) systems can
significantly influence users' decisions by presenting results in varying
orders and formats. However, whether such influence exists in legal case
retrieval remains largely unknown. This study presents the first investigation
into the influence of legal case retrieval systems on the decision-making
process of legal users. We conducted an online user study involving more than
ninety participants, and our findings suggest that the result distribution of
legal case retrieval systems indeed affect users' judgements on the sentences
in cases. Notably, when users are presented with biased results that involve
harsher sentences, they tend to impose harsher sentences on the current case as
well. This research highlights the importance of optimizing the unbiasedness of
legal case retrieval systems.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04737" title="Abstract">arXiv:2310.04737</a> [<a href="/pdf/2310.04737" title="Download PDF">pdf</a>, <a href="/format/2310.04737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Super Synthesis Pros., or why CHI PLAY needs research synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seaborn%2C+K">Katie Seaborn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CHI PLAY '23
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Companion Proceedings of the Annual Symposium on Computer-Human
  Interaction in Play, 235-237 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Games user research is a-booming -- or maybe a-goomba-ing -- with a boundless
parade of papers popping up from every nook and pipe. We may need a super power
-- or super method -- from another world. I outline three motivations for
jump-starting research synthesis in games user research. I argue that: research
synthesis will validate this field of study and enrich primary research
(meta-scholarship); we must level up both primary and secondary research
(education); and we should reflect this epistemological stance in community
structures and adopt established tools and protocols (standardization). I offer
power-ups to get the toads rolling.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04739" title="Abstract">arXiv:2310.04739</a> [<a href="/pdf/2310.04739" title="Download PDF">pdf</a>, <a href="/format/2310.04739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Link, user-centred designer: Game characters as transcendent models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seaborn%2C+K">Katie Seaborn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CHI PLAY '23
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Companion Proceedings of the Annual Symposium on Computer-Human
  Interaction in Play, 238-240 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Games allow us to construct and explore identities and offer us role models,
good and bad. Game characters are a reflection of us -- players and creators
alike -- or could be. But do games also encode identities, values, and
orientations that transcend diegetic categories and player self-insertion? I
explore the notion of game characters as conduits of transcendent models
through the case study of Link from the Legend of Zelda series. I propose that
designers embed tacit, nondiegetic patterns of praxis and complex value models,
such as user-centred design, when crafting the embodiment of characters in
gameplay, even unawares.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04741" title="Abstract">arXiv:2310.04741</a> [<a href="/pdf/2310.04741" title="Download PDF">pdf</a>, <a href="/format/2310.04741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balancing stability and plasticity in continual learning: the  readout-decomposition of activation change (RDAC) framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anthes%2C+D">Daniel Anthes</a>, 
<a href="/search/cs?searchtype=author&query=Thorat%2C+S">Sushrut Thorat</a>, 
<a href="/search/cs?searchtype=author&query=Kietzmann%2C+T+C">Tim C. Kietzmann</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6nig%2C+P">Peter K&#xf6;nig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Continual learning (CL) algorithms strive to acquire new knowledge while
preserving prior information. However, this stability-plasticity trade-off
remains a central challenge. This paper introduces a framework that dissects
this trade-off, offering valuable insights into CL algorithms. The
Readout-Decomposition of Activation Change (RDAC) framework first addresses the
stability-plasticity dilemma and its relation to catastrophic forgetting. It
relates learning-induced activation changes in the range of prior readouts to
the degree of stability and changes in the null space to the degree of
plasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the
framework clarifies the stability-plasticity trade-offs of the popular
regularization algorithms Synaptic intelligence (SI), Elastic-weight
consolidation (EWC), and learning without Forgetting (LwF), and replay-based
algorithms Gradient episodic memory (GEM), and data replay. GEM and data replay
preserved stability and plasticity, while SI, EWC, and LwF traded off
plasticity for stability. The inability of the regularization algorithms to
maintain plasticity was linked to them restricting the change of activations in
the null space of the prior readout. Additionally, for one-hidden-layer linear
neural networks, we derived a gradient decomposition algorithm to restrict
activation change only in the range of the prior readouts, to maintain high
stability while not further sacrificing plasticity. Results demonstrate that
the algorithm maintained stability without significant plasticity loss. The
RDAC framework informs the behavior of existing CL algorithms and paves the way
for novel CL approaches. Finally, it sheds light on the connection between
learning-induced activation/representation changes and the stability-plasticity
dilemma, also offering insights into representational drift in biological
systems.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04742" title="Abstract">arXiv:2310.04742</a> [<a href="/pdf/2310.04742" title="Download PDF">pdf</a>, <a href="/format/2310.04742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter Efficient Multi-task Model Fusion with Partial Linearization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+A">Anke Tang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Y">Yibing Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Han Hu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bo Du</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yixin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large pre-trained models have enabled significant advances in machine
learning and served as foundation components. Model fusion methods, such as
task arithmetic, have been proven to be powerful and scalable to incorporate
fine-tuned weights from different tasks into a multi-task model. However,
efficiently fine-tuning large pre-trained models on multiple downstream tasks
remains challenging, leading to inefficient multi-task model fusion. In this
work, we propose a novel method to improve multi-task fusion for
parameter-efficient fine-tuning techniques like LoRA fine-tuning. Specifically,
our approach partially linearizes only the adapter modules and applies task
arithmetic over the linearized adapters. This allows us to leverage the the
advantages of model fusion over linearized fine-tuning, while still performing
fine-tuning and inference efficiently. We demonstrate that our partial
linearization technique enables a more effective fusion of multiple tasks into
a single model, outperforming standard adapter tuning and task arithmetic
alone. Experimental results demonstrate the capabilities of our proposed
partial linearization technique to effectively construct unified multi-task
models via the fusion of fine-tuned task vectors. We evaluate performance over
an increasing number of tasks and find that our approach outperforms standard
parameter-efficient fine-tuning techniques. The results highlight the benefits
of partial linearization for scalable and efficient multi-task model fusion.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04743" title="Abstract">arXiv:2310.04743</a> [<a href="/pdf/2310.04743" title="Download PDF">pdf</a>, <a href="/format/2310.04743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resprompt: Residual Connection Prompting Advances Multi-Step Reasoning  in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Song Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Shakeri%2C+Z">Zahra Shakeri</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+A">Aaron Chan</a>, 
<a href="/search/cs?searchtype=author&query=Sanjabi%2C+M">Maziar Sanjabi</a>, 
<a href="/search/cs?searchtype=author&query=Firooz%2C+H">Hamed Firooz</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yinglong Xia</a>, 
<a href="/search/cs?searchtype=author&query=Akyildiz%2C+B">Bugra Akyildiz</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yizhou Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinchao Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Celikyilmaz%2C+A">Asli Celikyilmaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Chain-of-thought (CoT) prompting, which offers step-by-step problem-solving
rationales, has impressively unlocked the reasoning potential of large language
models (LLMs). Yet, the standard CoT is less effective in problems demanding
multiple reasoning steps. This limitation arises from the complex reasoning
process in multi-step problems: later stages often depend on the results of
several steps earlier, not just the results of the immediately preceding step.
Such complexities suggest the reasoning process is naturally represented as a
graph. The almost linear and straightforward structure of CoT prompting,
however, struggles to capture this complex reasoning graph. To address this
challenge, we propose Residual Connection Prompting (RESPROMPT), a new
prompting strategy that advances multi-step reasoning in LLMs. Our key idea is
to reconstruct the reasoning graph within prompts. We achieve this by
integrating necessary connections-links present in the reasoning graph but
missing in the linear CoT flow-into the prompts. Termed "residual connections",
these links are pivotal in morphing the linear CoT structure into a graph
representation, effectively capturing the complex reasoning graphs inherent in
multi-step problems. We evaluate RESPROMPT on six benchmarks across three
diverse domains: math, sequential, and commonsense reasoning. For the
open-sourced LLaMA family of models, RESPROMPT yields a significant average
reasoning accuracy improvement of 12.5% on LLaMA-65B and 6.8% on LLaMA2-70B.
Breakdown analysis further highlights RESPROMPT particularly excels in complex
multi-step reasoning: for questions demanding at least five reasoning steps,
RESPROMPT outperforms the best CoT based benchmarks by a remarkable average
improvement of 21.1% on LLaMA-65B and 14.3% on LLaMA2-70B. Through extensive
ablation studies and analyses, we pinpoint how to most effectively build
residual connections.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04747" title="Abstract">arXiv:2310.04747</a> [<a href="/pdf/2310.04747" title="Download PDF">pdf</a>, <a href="/format/2310.04747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Dynamic and Small Objects Refinement for Unsupervised Domain  Adaptative Nighttime Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jingyi Pan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sihang Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yucheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jinjing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Nighttime semantic segmentation is essential for various applications, e.g.,
autonomous driving, which often faces challenges due to poor illumination and
the lack of well-annotated datasets. Unsupervised domain adaptation (UDA) has
shown potential for addressing the challenges and achieved remarkable results
for nighttime semantic segmentation. However, existing methods still face
limitations in 1) their reliance on style transfer or relighting models, which
struggle to generalize to complex nighttime environments, and 2) their
ignorance of dynamic and small objects like vehicles and traffic signs, which
are difficult to be directly learned from other domains. This paper proposes a
novel UDA method that refines both label and feature levels for dynamic and
small objects for nighttime semantic segmentation. First, we propose a dynamic
and small object refinement module to complement the knowledge of dynamic and
small objects from the source domain to target nighttime domain. These dynamic
and small objects are normally context-inconsistent in under-exposed
conditions. Then, we design a feature prototype alignment module to reduce the
domain gap by deploying contrastive learning between features and prototypes of
the same class from different domains, while re-weighting the categories of
dynamic and small objects. Extensive experiments on four benchmark datasets
demonstrate that our method outperforms prior arts by a large margin for
nighttime segmentation. Project page: https://rorisis.github.io/DSRNSS/.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04748" title="Abstract">arXiv:2310.04748</a> [<a href="/pdf/2310.04748" title="Download PDF">pdf</a>, <a href="/format/2310.04748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging yield buckling to achieve ideal shock absorbers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenfeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Janbaz%2C+S">Shahram Janbaz</a>, 
<a href="/search/cs?searchtype=author&query=Dykstra%2C+D">David Dykstra</a>, 
<a href="/search/cs?searchtype=author&query=Ennis%2C+B">Bernard Ennis</a>, 
<a href="/search/cs?searchtype=author&query=Coulais%2C+C">Corentin Coulais</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">The ideal shock absorber combines high stiffness with high energy absorption
whilst retaining structural integrity after impact and is scalable for
industrial production. So far no structure meets all of these criteria. Here,
we introduce a special occurrence of plastic buckling as a design concept for
mechanical metamaterials that combine all the elements required of an ideal
shock absorber. By striking a balance between plastic deformation and buckling,
which we term yield buckling, these metamaterials exhibit sequential, maximally
dissipative collapse combined with high strength and the preservation of
structural integrity. Unlike existing structures, this design paradigm is
applicable to all elastoplastic materials at any length scale and hence will
lead to a new generation of shock absorbers with enhanced safety and
sustainabilty in a myriad of high-tech applications.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04749" title="Abstract">arXiv:2310.04749</a> [<a href="/pdf/2310.04749" title="Download PDF">pdf</a>, <a href="/format/2310.04749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConvNeXtv2 Fusion with Mask R-CNN for Automatic Region Based Coronary  Artery Stenosis Detection for Disease Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pokhrel%2C+S">Sandesh Pokhrel</a>, 
<a href="/search/cs?searchtype=author&query=Bhandari%2C+S">Sanjay Bhandari</a>, 
<a href="/search/cs?searchtype=author&query=Vazquez%2C+E">Eduard Vazquez</a>, 
<a href="/search/cs?searchtype=author&query=Shrestha%2C+Y+R">Yash Raj Shrestha</a>, 
<a href="/search/cs?searchtype=author&query=Bhattarai%2C+B">Binod Bhattarai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Coronary Artery Diseases although preventable are one of the leading cause of
mortality worldwide. Due to the onerous nature of diagnosis, tackling CADs has
proved challenging. This study addresses the automation of resource-intensive
and time-consuming process of manually detecting stenotic lesions in coronary
arteries in X-ray coronary angiography images. To overcome this challenge, we
employ a specialized Convnext-V2 backbone based Mask RCNN model pre-trained for
instance segmentation tasks. Our empirical findings affirm that the proposed
model exhibits commendable performance in identifying stenotic lesions.
Notably, our approach achieves a substantial F1 score of 0.5353 in this
demanding task, underscoring its effectiveness in streamlining this intensive
process.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04750" title="Abstract">arXiv:2310.04750</a> [<a href="/pdf/2310.04750" title="Download PDF">pdf</a>, <a href="/format/2310.04750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffNAS: Bootstrapping Diffusion Models by Prompting for Better  Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+X">Xiu Su</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+S">Shan You</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chang Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Diffusion models have recently exhibited remarkable performance on synthetic
data. After a diffusion path is selected, a base model, such as UNet, operates
as a denoising autoencoder, primarily predicting noises that need to be
eliminated step by step. Consequently, it is crucial to employ a model that
aligns with the expected budgets to facilitate superior synthetic performance.
In this paper, we meticulously analyze the diffusion model and engineer a base
model search approach, denoted "DiffNAS". Specifically, we leverage GPT-4 as a
supernet to expedite the search, supplemented with a search memory to enhance
the results. Moreover, we employ RFID as a proxy to promptly rank the
experimental outcomes produced by GPT-4. We also adopt a rapid-convergence
training strategy to boost search efficiency. Rigorous experimentation
corroborates that our algorithm can augment the search efficiency by 2 times
under GPT-based scenarios, while also attaining a performance of 2.82 with 0.37
improvement in FID on CIFAR10 relative to the benchmark IDDPM algorithm.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04752" title="Abstract">arXiv:2310.04752</a> [<a href="/pdf/2310.04752" title="Download PDF">pdf</a>, <a href="/format/2310.04752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Generalization Analysis of Re-Weighting and Logit-Adjustment  for Imbalanced Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zitai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qianqian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhiyong Yang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuan He</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaochun Cao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qingming Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Real-world datasets are typically imbalanced in the sense that only a few
classes have numerous samples, while many classes are associated with only a
few samples. As a result, a na\"ive ERM learning process will be biased towards
the majority classes, making it difficult to generalize to the minority
classes. To address this issue, one simple but effective approach is to modify
the loss function to emphasize the learning on minority classes, such as
re-weighting the losses or adjusting the logits via class-dependent terms.
However, existing generalization analysis of such losses is still
coarse-grained and fragmented, failing to explain some empirical results. To
bridge this gap, we propose a novel technique named data-dependent contraction
to capture how these modified losses handle different classes. On top of this
technique, a fine-grained generalization bound is established for imbalanced
learning, which helps reveal the mystery of re-weighting and logit-adjustment
in a unified manner. Furthermore, a principled learning algorithm is developed
based on the theoretical insights. Finally, the empirical results on benchmark
datasets not only validate the theoretical results but also demonstrate the
effectiveness of the proposed method.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04753" title="Abstract">arXiv:2310.04753</a> [<a href="/pdf/2310.04753" title="Download PDF">pdf</a>, <a href="/format/2310.04753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Dataset for End-to-End Sign Language Translation: The Greek  Elementary School Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Voskou%2C+A">Andreas Voskou</a>, 
<a href="/search/cs?searchtype=author&query=Panousis%2C+K+P">Konstantinos P. Panousis</a>, 
<a href="/search/cs?searchtype=author&query=Partaourides%2C+H">Harris Partaourides</a>, 
<a href="/search/cs?searchtype=author&query=Tolias%2C+K">Kyriakos Tolias</a>, 
<a href="/search/cs?searchtype=author&query=Chatzis%2C+S">Sotirios Chatzis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCVW2023 - ACVR
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE/CVF International Conference on Computer
  Vision. 2023. p. 1966-1975
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Automatic Sign Language Translation (SLT) is a research avenue of great
societal impact. End-to-End SLT facilitates the interaction of Hard-of-Hearing
(HoH) with hearing people, thus improving their social life and opportunities
for participation in social life. However, research within this frame of
reference is still in its infancy, and current resources are particularly
limited. Existing SLT methods are either of low translation ability or are
trained and evaluated on datasets of restricted vocabulary and questionable
real-world value. A characteristic example is Phoenix2014T benchmark dataset,
which only covers weather forecasts in German Sign Language. To address this
shortage of resources, we introduce a newly constructed collection of 29653
Greek Sign Language video-translation pairs which is based on the official
syllabus of Greek Elementary School. Our dataset covers a wide range of
subjects. We use this novel dataset to train recent state-of-the-art
Transformer-based methods widely used in SLT research. Our results demonstrate
the potential of our introduced dataset to advance SLT research by offering a
favourable balance between usability and real-world value.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04755" title="Abstract">arXiv:2310.04755</a> [<a href="/pdf/2310.04755" title="Download PDF">pdf</a>, <a href="/format/2310.04755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pairwise GUI Dataset Construction Between Android Phones and Tablets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Han Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+H">Haolan Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yujin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Di Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures. arXiv admin note: substantial text overlap with <a href="/abs/2307.13225">arXiv:2307.13225</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In the current landscape of pervasive smartphones and tablets, apps
frequently exist across both platforms. Although apps share most graphic user
interfaces (GUIs) and functionalities across phones and tablets, developers
often rebuild from scratch for tablet versions, escalating costs and
squandering existing design resources. Researchers are attempting to collect
data and employ deep learning in automated GUIs development to enhance
developers' productivity. There are currently several publicly accessible GUI
page datasets for phones, but none for pairwise GUIs between phones and
tablets. This poses a significant barrier to the employment of deep learning in
automated GUI development. In this paper, we introduce the Papt dataset, a
pioneering pairwise GUI dataset tailored for Android phones and tablets,
encompassing 10,035 phone-tablet GUI page pairs sourced from 5,593 unique app
pairs. We propose novel pairwise GUI collection approaches for constructing
this dataset and delineate its advantages over currently prevailing datasets in
the field. Through preliminary experiments on this dataset, we analyze the
present challenges of utilizing deep learning in automated GUI development.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04757" title="Abstract">arXiv:2310.04757</a> [<a href="/pdf/2310.04757" title="Download PDF">pdf</a>, <a href="/format/2310.04757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAD Models to Real-World Images: A Practical Approach to Unsupervised  Domain Adaptation in Industrial Object Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ritter%2C+D">Dennis Ritter</a>, 
<a href="/search/cs?searchtype=author&query=Hemberger%2C+M">Mike Hemberger</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B6nig%2C+M">Marc H&#xf6;nig</a>, 
<a href="/search/cs?searchtype=author&query=Stopp%2C+V">Volker Stopp</a>, 
<a href="/search/cs?searchtype=author&query=Rodner%2C+E">Erik Rodner</a>, 
<a href="/search/cs?searchtype=author&query=Hildebrand%2C+K">Kristian Hildebrand</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at ECML-PKDD 2023 Workshop "Adapting to Change: Reliable Multimodal Learning Across Domains", Student Paper Award
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we systematically analyze unsupervised domain adaptation
pipelines for object classification in a challenging industrial setting. In
contrast to standard natural object benchmarks existing in the field, our
results highlight the most important design choices when only category-labeled
CAD models are available but classification needs to be done with real-world
images. Our domain adaptation pipeline achieves SoTA performance on the VisDA
benchmark, but more importantly, drastically improves recognition performance
on our new open industrial dataset comprised of 102 mechanical parts. We
conclude with a set of guidelines that are relevant for practitioners needing
to apply state-of-the-art unsupervised domain adaptation in practice. Our code
is available at https://github.com/dritter-bht/synthnet-transfer-learning.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04764" title="Abstract">arXiv:2310.04764</a> [<a href="/pdf/2310.04764" title="Download PDF">pdf</a>, <a href="/format/2310.04764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizations of Definable Context-Free Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iosif%2C+R">Radu Iosif</a>, 
<a href="/search/cs?searchtype=author&query=Zuleger%2C+F">Florian Zuleger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We give a characterization of those sets of graphs that are both definable in
Counting Monadic Second Order Logic (CMS) and context-free, i.e., least
solutions of Hyperedge-Replacement (HR)-grammars introduced by Courcelle and
Engelfriet. We give the following equivalent characterizations:
<br />(a) a set of graphs is recognizable (in the algebra that consists of all
graphs and HR-operations) and has bounded tree-width; further, we refine this
condition and show equivalence with recognizability in a finite-sort subalgebra
of the graph algebra;
<br />(b) the set is parsable, i.e., there is an MS-definable transduction from
graphs to a set of derivation trees labelled by HR-operations, such that the
set of graphs is the image of this set of trees under the evaluation of the
HR-operations;
<br />(c) the set of graphs is the image of unranked recognizable set of trees
under an MS-definable transduction whose inverse is also MS-definable.
<br />The main goal of this paper is to present the above characterization, of
which several directions are already known, in an accessible and unified way.
We rely on a novel connection between two seminal results, a logical
characterization of context-free graph languages in terms of tree to graph
MS-definable transductions, by Courcelle and Engelfriet~, and a proof that an
optimal-width tree decomposition of a graph can be built by an MS-definable
transduction, by Bojanczyk and Pilipczuk.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04766" title="Abstract">arXiv:2310.04766</a> [<a href="/pdf/2310.04766" title="Download PDF">pdf</a>, <a href="/format/2310.04766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying Independence Redundancy in Systems: Measurement, Factors,  and Impact Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hong Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Redundancy represents a strategy for achieving high availability. However,
various factors, known as singleness factors, necessitate corresponding
redundancy measures. The absence of a systematic approach for identifying these
singleness factors and the lack of a quantifiable method to assess system
redundancy degrees are notable challenges. In this paper, we initially present
methodologies to evaluate system redundancy, specifically quantifying
independent redundancy in complex systems. This approach considers the
interactions among various factors that influence redundancy, treating
different factors as distinct dimensions to comprehensively account for all
potential impact factors. Additionally, we propose methodologies to calculate
the Independent Redundancy Degree (IRD) when combining or removing system
components, offering insights into system resilience during integration or
separation. Furthermore, we broaden the scope of known singleness factors by
exploring time and space dimensions, aiming to identify additional related
singleness factors. This process helps us pinpoint critical system aspects that
necessitate redundancy for enhanced fault-tolerance and reliability. The
verification results underscore the influence of different dimensions and
reveal the significance of addressing weak dimensions for enhancing system
reliability.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04768" title="Abstract">arXiv:2310.04768</a> [<a href="/pdf/2310.04768" title="Download PDF">pdf</a>, <a href="/format/2310.04768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Corrupted User Detection and Regret Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiyong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jize Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuai Li</a>, 
<a href="/search/cs?searchtype=author&query=Lui%2C+J+C+S">John C.S. Lui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In real-world online web systems, multiple users usually arrive sequentially
into the system. For applications like click fraud and fake reviews, some users
can maliciously perform corrupted (disrupted) behaviors to trick the system.
Therefore, it is crucial to design efficient online learning algorithms to
robustly learn from potentially corrupted user behaviors and accurately
identify the corrupted users in an online manner. Existing works propose bandit
algorithms robust to adversarial corruption. However, these algorithms are
designed for a single user, and cannot leverage the implicit social relations
among multiple users for more efficient learning. Moreover, none of them
consider how to detect corrupted users online in the multiple-user scenario. In
this paper, we present an important online learning problem named LOCUD to
learn and utilize unknown user relations from disrupted behaviors to speed up
learning, and identify the corrupted users in an online setting. To robustly
learn and utilize the unknown relations among potentially corrupted users, we
propose a novel bandit algorithm RCLUB-WCU. To detect the corrupted users, we
devise a novel online detection algorithm OCCUD based on RCLUB-WCU's inferred
user relations. We prove a regret upper bound for RCLUB-WCU, which
asymptotically matches the lower bound with respect to $T$ up to logarithmic
factors, and matches the state-of-the-art results in degenerate cases. We also
give a theoretical guarantee for the detection accuracy of OCCUD. With
extensive experiments, our methods achieve superior performance over previous
bandit algorithms and high corrupted user detection accuracy.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04769" title="Abstract">arXiv:2310.04769</a> [<a href="/pdf/2310.04769" title="Download PDF">pdf</a>, <a href="/format/2310.04769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 1st Place Solution of Egocentric 3D Hand Pose Estimation Challenge 2023  Technical Report:A Concise Pipeline for Egocentric Hand Pose Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhishan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Z">Zhi Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shihao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+M">Minqiang Zou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Mochen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiajun Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This report introduce our work on Egocentric 3D Hand Pose Estimation
workshop. Using AssemblyHands, this challenge focuses on egocentric 3D hand
pose estimation from a single-view image. In the competition, we adopt ViT
based backbones and a simple regressor for 3D keypoints prediction, which
provides strong model baselines. We noticed that Hand-objects occlusions and
self-occlusions lead to performance degradation, thus proposed a non-model
method to merge multi-view results in the post-process stage. Moreover, We
utilized test time augmentation and model ensemble to make further improvement.
We also found that public dataset and rational preprocess are beneficial. Our
method achieved 12.21mm MPJPE on test dataset, achieve the first place in
Egocentric 3D Hand Pose Estimation challenge.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04771" title="Abstract">arXiv:2310.04771</a> [<a href="/pdf/2310.04771" title="Download PDF">pdf</a>, <a href="/format/2310.04771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embodied Cognition Guides Virtual-Real Interaction Design to Help  Yicheng Flower Drum Intangible Cultural Heritage Dissemination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuhan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weiran Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaolin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Ze Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">In order to make the non-heritage culture of Yicheng Flower Drum more
relevant to the trend of the digital era and promote its dissemination and
inheritance, the design and application of gesture recognition and virtual
reality technologies guided by embodied cognition theory in the process of
non-heritage culture dissemination is studied. At the same time, it will
enhance the interaction between people and NRM culture, stimulate the
audience's interest in understanding NRM and spreading NRM, and create
awareness of preserving NRM culture. Using embodied cognition as a theoretical
guide, expanding the unidirectional communication mode through human-computer
interaction close to natural behavior and cooperating with multisensory
information reception channels, so as to construct an embodied and immersive
interactive atmosphere for the participants and enable them to naturally form
the cognition and understanding of the traditional culture in the process of
interaction. The dissemination of the non-heritage culture Yicheng Flower Drum
can take the theory of embodied cognition as an entry point, and through the
virtual and real scenes of Yicheng Flower Drum and the immersive experience, we
can empower the interaction design of non-heritage culture dissemination of the
virtual and real, and provide a new method for the research of digital design
of non-heritage culture.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04772" title="Abstract">arXiv:2310.04772</a> [<a href="/pdf/2310.04772" title="Download PDF">pdf</a>, <a href="/format/2310.04772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Sequential Decision-Making in Geosteering: A Reinforcement  Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muhammad%2C+R+B">Ressi Bonti Muhammad</a>, 
<a href="/search/cs?searchtype=author&query=Alyaev%2C+S">Sergey Alyaev</a>, 
<a href="/search/cs?searchtype=author&query=Bratvold%2C+R+B">Reidar Brumer Bratvold</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Geophysics (physics.geo-ph)

</div>
<p class="mathjax">Trajectory adjustment decisions throughout the drilling process, called
geosteering, affect subsequent choices and information gathering, thus
resulting in a coupled sequential decision problem. Previous works on applying
decision optimization methods in geosteering rely on greedy optimization or
Approximate Dynamic Programming (ADP). Either decision optimization method
requires explicit uncertainty and objective function models, making developing
decision optimization methods for complex and realistic geosteering
environments challenging to impossible. We use the Deep Q-Network (DQN) method,
a model-free reinforcement learning (RL) method that learns directly from the
decision environment, to optimize geosteering decisions. The expensive
computations for RL are handled during the offline training stage. Evaluating
DQN needed for real-time decision support takes milliseconds and is faster than
the traditional alternatives. Moreover, for two previously published synthetic
geosteering scenarios, our results show that RL achieves high-quality outcomes
comparable to the quasi-optimal ADP. Yet, the model-free nature of RL means
that by replacing the training environment, we can extend it to problems where
the solution to ADP is prohibitively expensive to compute. This flexibility
will allow applying it to more complex environments and make hybrid versions
trained with real data in the future.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04778" title="Abstract">arXiv:2310.04778</a> [<a href="/pdf/2310.04778" title="Download PDF">pdf</a>, <a href="/ps/2310.04778" title="Download PostScript">ps</a>, <a href="/format/2310.04778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On $\ell$-MDS codes and a conjecture on infinite families of $1$-MDS  codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Shixin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Moro%2C+E">Edgar Mart&#xed;nez-Moro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">The class of $\ell$-maximum distance separable ($\ell$-MDS) codes {is a}
generalization of maximum distance separable (MDS) codes {that} has attracted a
lot of attention due to its applications in several areas such as secret
sharing schemes, index coding problems, informed source coding problems, and
combinatorial $t$-designs. In this paper, for $\ell=1$, we completely solve a
conjecture recently proposed by Heng $et~al.$ (Discrete Mathematics, 346(10):
113538, 2023) and obtain infinite families of $1$-MDS codes with general
dimensions holding $2$-designs. These later codes are also been proven to be
optimal locally recoverable codes. For general {positive integers} $\ell$ and
$\ell'$, we construct new $\ell$-MDS codes from known $\ell'$-MDS codes via
some classical propagation rules involving the extended, expurgated, and
$(u,u+v)$ constructions. Finally, we study some general results including
characterization, weight distributions, and bounds on maximum lengths of
$\ell$-MDS codes, which generalize, simplify, or improve some known results in
the literature.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04780" title="Abstract">arXiv:2310.04780</a> [<a href="/pdf/2310.04780" title="Download PDF">pdf</a>, <a href="/format/2310.04780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IPMix: Label-Preserving Data Augmentation Method for Training Robust  Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhenglin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+X">Xianan Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Na Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+X">Xiaomei Tu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Biao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xi Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Data augmentation has been proven effective for training high-accuracy
convolutional neural network classifiers by preventing overfitting. However,
building deep neural networks in real-world scenarios requires not only high
accuracy on clean data but also robustness when data distributions shift. While
prior methods have proposed that there is a trade-off between accuracy and
robustness, we propose IPMix, a simple data augmentation approach to improve
robustness without hurting clean accuracy. IPMix integrates three levels of
data augmentation (image-level, patch-level, and pixel-level) into a coherent
and label-preserving technique to increase the diversity of training data with
limited computational overhead. To further improve the robustness, IPMix
introduces structural complexity at different levels to generate more diverse
images and adopts the random mixing method for multi-scale information fusion.
Experiments demonstrate that IPMix outperforms state-of-the-art corruption
robustness on CIFAR-C and ImageNet-C. In addition, we show that IPMix also
significantly improves the other safety measures, including robustness to
adversarial perturbations, calibration, prediction consistency, and anomaly
detection, achieving state-of-the-art or comparable results on several
benchmarks, including ImageNet-R, ImageNet-A, and ImageNet-O.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04781" title="Abstract">arXiv:2310.04781</a> [<a href="/pdf/2310.04781" title="Download PDF">pdf</a>, <a href="/format/2310.04781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unifying Foundation Models with Quadrotor Control for Visual Tracking  Beyond Object Categories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saviolo%2C+A">Alessandro Saviolo</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+P">Pratyaksh Rao</a>, 
<a href="/search/cs?searchtype=author&query=Radhakrishnan%2C+V">Vivek Radhakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jiuhong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Loianno%2C+G">Giuseppe Loianno</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Visual control enables quadrotors to adaptively navigate using real-time
sensory data, bridging perception with action. Yet, challenges persist,
including generalization across scenarios, maintaining reliability, and
ensuring real-time responsiveness. This paper introduces a perception framework
grounded in foundation models for universal object detection and tracking,
moving beyond specific training categories. Integral to our approach is a
multi-layered tracker integrated with the foundation detector, ensuring
continuous target visibility, even when faced with motion blur, abrupt light
shifts, and occlusions. Complementing this, we introduce a model-free
controller tailored for resilient quadrotor visual tracking. Our system
operates efficiently on limited hardware, relying solely on an onboard camera
and an inertial measurement unit. Through extensive validation in diverse
challenging indoor and outdoor environments, we demonstrate our system's
effectiveness and adaptability. In conclusion, our research represents a step
forward in quadrotor visual tracking, moving from task-specific methods to more
versatile and adaptable operations.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04782" title="Abstract">arXiv:2310.04782</a> [<a href="/pdf/2310.04782" title="Download PDF">pdf</a>, <a href="/format/2310.04782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving the Reliability of Large Language Models by Leveraging  Uncertainty-Aware In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuchen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Houqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In recent years, large-scale language models (LLMs) have gained attention for
their impressive text generation capabilities. However, these models often face
the challenge of "hallucination," which undermines their reliability. In this
study, we introduce an uncertainty-aware in-context learning framework to
empower the model to enhance or reject its output in response to uncertainty.
Human-defined methods for estimating uncertainty typically assume that
"uncertainty is lower when the model's response is correct compared to when it
is incorrect." However, setting a precise threshold to distinguish correctness
is challenging. Therefore, we introduce uncertainty information as an
intermediary variable that implicitly influences the model's behavior. Our
innovative uncertainty-aware in-context learning framework involves fine-tuning
the LLM using a calibration dataset. Our aim is to improve the model's
responses by filtering out answers with high uncertainty while considering the
model's knowledge limitations. We evaluate the model's knowledge by examining
multiple responses to the same question for the presence of a correct answer.
When the model lacks relevant knowledge, the response should indicate that the
question cannot be answered. Conversely, when the model has relevant knowledge,
the response should provide the correct answer. Extensive experiments confirm
the effectiveness of our framework, leading to two key findings. First, the
logit output values of the LLM partly reflect inherent uncertainty. Second, our
model autonomously recognizes uncertainty, resulting in improved responses.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04787" title="Abstract">arXiv:2310.04787</a> [<a href="/pdf/2310.04787" title="Download PDF">pdf</a>, <a href="/format/2310.04787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HI-SLAM: Monocular Real-time Dense Mapping with Hybrid Implicit Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tiecheng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qing Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Haala%2C+N">Norbert Haala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this letter, we present a neural field-based real-time monocular mapping
framework for accurate and dense Simultaneous Localization and Mapping (SLAM).
Recent neural mapping frameworks show promising results, but rely on RGB-D or
pose inputs, or cannot run in real-time. To address these limitations, our
approach integrates dense-SLAM with neural implicit fields. Specifically, our
dense SLAM approach runs parallel tracking and global optimization, while a
neural field-based map is constructed incrementally based on the latest SLAM
estimates. For the efficient construction of neural fields, we employ
multi-resolution grid encoding and signed distance function (SDF)
representation. This allows us to keep the map always up-to-date and adapt
instantly to global updates via loop closing. For global consistency, we
propose an efficient Sim(3)-based pose graph bundle adjustment (PGBA) approach
to run online loop closing and mitigate the pose and scale drift. To enhance
depth accuracy further, we incorporate learned monocular depth priors. We
propose a novel joint depth and scale adjustment (JDSA) module to solve the
scale ambiguity inherent in depth priors. Extensive evaluations across
synthetic and real-world datasets validate that our approach outperforms
existing methods in accuracy and map completeness while preserving real-time
performance.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04788" title="Abstract">arXiv:2310.04788</a> [<a href="/pdf/2310.04788" title="Download PDF">pdf</a>, <a href="/format/2310.04788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PMNN:Physical Model-driven Neural Network for solving time-fractional  differential equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhiying Ma</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Jie Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wenhao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yaxin Peng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Ying Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this paper, an innovative Physical Model-driven Neural Network (PMNN)
method is proposed to solve time-fractional differential equations. It
establishes a temporal iteration scheme based on physical model-driven neural
networks which effectively combines deep neural networks (DNNs) with
interpolation approximation of fractional derivatives. Specifically, once the
fractional differential operator is discretized, DNNs are employed as a bridge
to integrate interpolation approximation techniques with differential
equations. On the basis of this integration, we construct a neural-based
iteration scheme. Subsequently, by training DNNs to learn this temporal
iteration scheme, approximate solutions to the differential equations can be
obtained. The proposed method aims to preserve the intrinsic physical
information within the equations as far as possible. It fully utilizes the
powerful fitting capability of neural networks while maintaining the efficiency
of the difference schemes for fractional differential equations. Moreover, we
validate the efficiency and accuracy of PMNN through several numerical
experiments.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04789" title="Abstract">arXiv:2310.04789</a> [<a href="/pdf/2310.04789" title="Download PDF">pdf</a>, <a href="/format/2310.04789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HNS: An Efficient Hermite Neural Solver for Solving Time-Fractional  Partial Differential Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Jie Hou</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhiying Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+S">Shihui Ying</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Ying Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Neural network solvers represent an innovative and promising approach for
tackling time-fractional partial differential equations by utilizing deep
learning techniques. L1 interpolation approximation serves as the standard
method for addressing time-fractional derivatives within neural network
solvers. However, we have discovered that neural network solvers based on L1
interpolation approximation are unable to fully exploit the benefits of neural
networks, and the accuracy of these models is constrained to interpolation
errors. In this paper, we present the high-precision Hermite Neural Solver
(HNS) for solving time-fractional partial differential equations. Specifically,
we first construct a high-order explicit approximation scheme for fractional
derivatives using Hermite interpolation techniques, and rigorously analyze its
approximation accuracy. Afterward, taking into account the infinitely
differentiable properties of deep neural networks, we integrate the high-order
Hermite interpolation explicit approximation scheme with deep neural networks
to propose the HNS. The experimental results show that HNS achieves higher
accuracy than methods based on the L1 scheme for both forward and inverse
problems, as well as in high-dimensional scenarios. This indicates that HNS has
significantly improved accuracy and flexibility compared to existing L1-based
methods, and has overcome the limitations of explicit finite difference
approximation methods that are often constrained to function value
interpolation. As a result, the HNS is not a simple combination of numerical
computing methods and neural networks, but rather achieves a complementary and
mutually reinforcing advantages of both approaches. The data and code can be
found at \url{https://github.com/hsbhc/HNS}.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04793" title="Abstract">arXiv:2310.04793</a> [<a href="/pdf/2310.04793" title="Download PDF">pdf</a>, <a href="/format/2310.04793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FinGPT: Instruction Tuning Benchmark for Open-Source Large Language  Models in Financial Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Neng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongyang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C+D">Christina Dan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Trading and Market Microstructure (q-fin.TR)

</div>
<p class="mathjax">In the swiftly expanding domain of Natural Language Processing (NLP), the
potential of GPT-based models for the financial sector is increasingly evident.
However, the integration of these models with financial datasets presents
challenges, notably in determining their adeptness and relevance. This paper
introduces a distinctive approach anchored in the Instruction Tuning paradigm
for open-source large language models, specifically adapted for financial
contexts. Through this methodology, we capitalize on the interoperability of
open-source models, ensuring a seamless and transparent integration. We begin
by explaining the Instruction Tuning paradigm, highlighting its effectiveness
for immediate integration. The paper presents a benchmarking scheme designed
for end-to-end training and testing, employing a cost-effective progression.
Firstly, we assess basic competencies and fundamental tasks, such as Named
Entity Recognition (NER) and sentiment analysis to enhance specialization.
Next, we delve into a comprehensive model, executing multi-task operations by
amalgamating all instructional tunings to examine versatility. Finally, we
explore the zero-shot capabilities by earmarking unseen tasks and incorporating
novel datasets to understand adaptability in uncharted terrains. Such a
paradigm fortifies the principles of openness and reproducibility, laying a
robust foundation for future investigations in open-source financial large
language models (FinLLMs).
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04794" title="Abstract">arXiv:2310.04794</a> [<a href="/pdf/2310.04794" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Validating Drone Trust Testing in Navigation Deviation Cases in  Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khavas%2C+Z+R">Zahra Rezaei Khavas</a>, 
<a href="/search/cs?searchtype=author&query=Meriaux%2C+E">Edwin Meriaux</a>, 
<a href="/search/cs?searchtype=author&query=Majdi%2C+A">Amin Majdi</a>, 
<a href="/search/cs?searchtype=author&query=Robinette%2C+P">Paul Robinette</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 8 Figures, IROS 2023 workshop Social Robot Navigation: Advances and Evaluation (<a href="https://seanavbench23.pages.dev/papers">this https URL</a>),
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Developing videos for trust testing is very time-consuming, expensive and
potentially dangerous. For trust tests, it requires a person to be flying the
drone while another might be filming. The drones can be very expensive and if
something goes wrong the costs might be very high. In previous work, we have
looked at how collisions and basic communication loss can be accurately modeled
in simulation and to be able to generate the same trust results from users.
That work looked at two specific cases using two drones, but to expand upon
this in other cases more testing is required. This paper looks to propose how
to test and evaluate the change in user's trust of a drone when it is
experiencing path deviation in simulation. If the environment is very realistic
can simulations be a good alternative to real life videos for trust testing
when there is path deviation? This deviation can occur due to the physical
conditions of the space, faulty piloting, or communication loss.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04795" title="Abstract">arXiv:2310.04795</a> [<a href="/pdf/2310.04795" title="Download PDF">pdf</a>, <a href="/format/2310.04795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning a Better Control Barrier Function Under Uncertain Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+B">Bolun Dai</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+P">Prashanth Krishnamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Khorrami%2C+F">Farshad Khorrami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Using control barrier functions (CBFs) as safety filters provides a
computationally inexpensive yet effective method for constructing controllers
in safety-critical applications. However, using CBFs requires the construction
of a valid CBF, which is well known to be a challenging task, and accurate
system dynamics, which are often unavailable. This paper presents a
learning-based approach to learn a valid CBF and the system dynamics starting
from a conservative handcrafted CBF (HCBF) and the nominal system dynamics. We
devise new loss functions that better suit the CBF refinement pipeline and are
able to produce well-behaved CBFs with the usage of distance functions. By
adopting an episodic learning approach, our proposed method is able to learn
the system dynamics while not requiring additional interactions with the
environment. Additionally, we provide a theoretical analysis of the quality of
the learned system dynamics. We show that our proposed learning approach can
effectively learn a valid CBF and an estimation of the actual system dynamics.
The effectiveness of our proposed method is empirically demonstrated through
simulation studies on three systems, a double integrator, a unicycle, and a
two-link arm.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04796" title="Abstract">arXiv:2310.04796</a> [<a href="/pdf/2310.04796" title="Download PDF">pdf</a>, <a href="/format/2310.04796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerate Multi-Agent Reinforcement Learning in Zero-Sum Games with  Subgame Curriculum Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiayu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zelai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunfei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiaming Song</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huazhong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+F">Fei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Learning Nash equilibrium (NE) in complex zero-sum games with multi-agent
reinforcement learning (MARL) can be extremely computationally expensive.
Curriculum learning is an effective way to accelerate learning, but an
under-explored dimension for generating a curriculum is the difficulty-to-learn
of the subgames -- games induced by starting from a specific state. In this
work, we present a novel subgame curriculum learning framework for zero-sum
games. It adopts an adaptive initial state distribution by resetting agents to
some previously visited states where they can quickly learn to improve
performance. Building upon this framework, we derive a subgame selection metric
that approximates the squared distance to NE values and further adopt a
particle-based state sampler for subgame generation. Integrating these
techniques leads to our new algorithm, Subgame Automatic Curriculum Learning
(SACL), which is a realization of the subgame curriculum learning framework.
SACL can be combined with any MARL algorithm such as MAPPO. Experiments in the
particle-world environment and Google Research Football environment show SACL
produces much stronger policies than baselines. In the challenging
hide-and-seek quadrant environment, SACL produces all four emergent stages and
uses only half the samples of MAPPO with self-play. The project website is at
https://sites.google.com/view/sacl-rl.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04799" title="Abstract">arXiv:2310.04799</a> [<a href="/pdf/2310.04799" title="Download PDF">pdf</a>, <a href="/format/2310.04799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chat Vector: A Simple Approach to Equip LLMs With New Language Chat  Capabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shih-Cheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pin-Zu Li</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+Y">Yu-Chi Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kuang-Ming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y+T">Yu Tung Lin</a>, 
<a href="/search/cs?searchtype=author&query=Hsiao%2C+S">Shih-Kai Hsiao</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+R+T">Richard Tzong-Han Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the advancements in conversational AI, such as ChatGPT, this paper
focuses on exploring developing Large Language Models (LLMs) for non-English
languages, especially emphasizing alignment with human preferences. We
introduce a computationally efficient method, leveraging chat vector, to
synergize pre-existing knowledge and behaviors in LLMs, restructuring the
conventional training paradigm from continual pre-train -&gt; SFT -&gt; RLHF to
continual pre-train + chat vector. Our empirical studies, primarily focused on
Traditional Chinese, employ LLaMA2 as the base model and acquire the chat
vector by subtracting the pre-trained weights, LLaMA2, from the weights of
LLaMA2-chat. Evaluating from three distinct facets, which are toxicity, ability
of instruction following, and multi-turn dialogue demonstrates the chat
vector's superior efficacy in chatting. To confirm the adaptability of our
approach, we extend our experiments to include models pre-trained in both
Korean and Simplified Chinese, illustrating the versatility of our methodology.
Overall, we present a significant solution in aligning LLMs with human
preferences efficiently across various languages, accomplished by the chat
vector.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04800" title="Abstract">arXiv:2310.04800</a> [<a href="/pdf/2310.04800" title="Download PDF">pdf</a>, <a href="/format/2310.04800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Sparse Long Range 3D Object Detection Using Range Experts and  Multimodal Virtual Points
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khoche%2C+A">Ajinkya Khoche</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%A1nchez%2C+L+P">Laura Pereira S&#xe1;nchez</a>, 
<a href="/search/cs?searchtype=author&query=Batool%2C+N">Nazre Batool</a>, 
<a href="/search/cs?searchtype=author&query=Mansouri%2C+S+S">Sina Sharif Mansouri</a>, 
<a href="/search/cs?searchtype=author&query=Jensfelt%2C+P">Patric Jensfelt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">3D object detection at long-range is crucial for ensuring the safety and
efficiency of self-driving cars, allowing them to accurately perceive and react
to objects, obstacles, and potential hazards from a distance. But most current
state-of-the-art LiDAR based methods are limited by the sparsity of range
sensors, which generates a form of domain gap between points closer to and
farther away from the ego vehicle. Another related problem is the label
imbalance for faraway objects, which inhibits the performance of Deep Neural
Networks at long-range. Although image features could be beneficial for
long-range detections, and some recently proposed multimodal methods
incorporate image features, they do not scale well computationally at long
ranges or are limited by depth estimation accuracy. To address the above
limitations, we propose to combine two LiDAR based 3D detection networks, one
specializing at near to mid-range objects, and one at long-range 3D detection.
To train a detector at long range under a scarce label regime, we further
propose to weigh the loss according to the labelled objects' distance from ego
vehicle. To mitigate the LiDAR sparsity issue, we leverage Multimodal Virtual
Points (MVP), an image based depth completion algorithm, to enrich our data
with virtual points. Our method, combining two range experts trained with MVP,
which we refer to as RangeFSD, achieves state-of-the-art performance on the
Argoverse2 (AV2) dataset, with improvements at long range. The code will be
released soon.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04801" title="Abstract">arXiv:2310.04801</a> [<a href="/pdf/2310.04801" title="Download PDF">pdf</a>, <a href="/format/2310.04801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameterizing Context: Unleashing the Power of Parameter-Efficient  Fine-Tuning and In-Context Tuning for Continual Table Semantic Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yongrui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shenyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+G">Guilin Qi</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xinnan Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS-2023 (Poster)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Continual table semantic parsing aims to train a parser on a sequence of
tasks, where each task requires the parser to translate natural language into
SQL based on task-specific tables but only offers limited training examples.
Conventional methods tend to suffer from overfitting with limited supervision,
as well as catastrophic forgetting due to parameter updates. Despite recent
advancements that partially alleviate these issues through semi-supervised data
augmentation and retention of a few past examples, the performance is still
limited by the volume of unsupervised data and stored examples. To overcome
these challenges, this paper introduces a novel method integrating
\textit{parameter-efficient fine-tuning} (PEFT) and \textit{in-context tuning}
(ICT) for training a continual table semantic parser. Initially, we present a
task-adaptive PEFT framework capable of fully circumventing catastrophic
forgetting, which is achieved by freezing the pre-trained model backbone and
fine-tuning small-scale prompts. Building on this, we propose a teacher-student
framework-based solution. The teacher addresses the few-shot problem using ICT,
which procures contextual information by demonstrating a few training examples.
In turn, the student leverages the proposed PEFT framework to learn from the
teacher's output distribution, and subsequently compresses and saves the
contextual information to the prompts, eliminating the need to store any
training examples. Experimental evaluations on two benchmarks affirm the
superiority of our method over prevalent few-shot and continual learning
baselines across various metrics.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04802" title="Abstract">arXiv:2310.04802</a> [<a href="/pdf/2310.04802" title="Download PDF">pdf</a>, <a href="/format/2310.04802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Unsupervised Topological SLAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Ayush Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Mehan%2C+Y">Yash Mehan</a>, 
<a href="/search/cs?searchtype=author&query=Dasu%2C+P">Pradyumna Dasu</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Sourav Garg</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+M">Madhava Krishna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE ITSC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this paper we present a novel framework for unsupervised topological
clustering resulting in improved loop. In this paper we present a novel
framework for unsupervised topological clustering resulting in improved loop
detection and closure for SLAM. A navigating mobile robot clusters its
traversal into visually similar topologies where each cluster (topology)
contains a set of similar looking images typically observed from spatially
adjacent locations. Each such set of spatially adjacent and visually similar
grouping of images constitutes a topology obtained without any supervision. We
formulate a hierarchical loop discovery strategy that first detects loops at
the level of topologies and subsequently at the level of images between the
looped topologies. We show over a number of traversals across different Habitat
environments that such a hierarchical pipeline significantly improves SOTA
image based loop detection and closure methods. Further, as a consequence of
improved loop detection, we enhance the loop closure and backend SLAM
performance. Such a rendering of a traversal into topological segments is
beneficial for downstream tasks such as navigation that can now build a
topological graph where spatially adjacent topological clusters are connected
by an edge and navigate over such topological graphs.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04804" title="Abstract">arXiv:2310.04804</a> [<a href="/pdf/2310.04804" title="Download PDF">pdf</a>, <a href="/ps/2310.04804" title="Download PostScript">ps</a>, <a href="/format/2310.04804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ten Challenges in Industrial Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhenhua Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jieming Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weiwen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Ruiming Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Huawei's vision and mission is to build a fully connected intelligent world.
Since 2013, Huawei Noah's Ark Lab has helped many products build recommender
systems and search engines for getting the right information to the right
users. Every day, our recommender systems serve hundreds of millions of mobile
phone users and recommend different kinds of content and services such as apps,
news feeds, songs, videos, books, themes, and instant services. The big data
and various scenarios provide us with great opportunities to develop advanced
recommendation technologies. Furthermore, we have witnessed the technical trend
of recommendation models in the past ten years, from the shallow and simple
models like collaborative filtering, linear models, low rank models to deep and
complex models like neural networks, pre-trained language models. Based on the
mission, opportunities and technological trends, we have also met several hard
problems in our recommender systems. In this talk, we will share ten important
and interesting challenges and hope that the RecSys community can get inspired
and create better recommender systems.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04805" title="Abstract">arXiv:2310.04805</a> [<a href="/pdf/2310.04805" title="Download PDF">pdf</a>, <a href="/format/2310.04805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User&#x27;s Position-Dependent Strategies in Consumer-Generated Media with  Monetary Rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ueki%2C+S">Shintaro Ueki</a>, 
<a href="/search/cs?searchtype=author&query=Toriumi%2C+F">Fujio Toriumi</a>, 
<a href="/search/cs?searchtype=author&query=Sugawara%2C+T">Toshiharu Sugawara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to The 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2023), 9 pages, 6 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Numerous forms of consumer-generated media (CGM), such as social networking
services (SNS), are widely used. Their success relies on users' voluntary
participation, often driven by psychological rewards like recognition and
connection from reactions by other users. Furthermore, a few CGM platforms
offer monetary rewards to users, serving as incentives for sharing items such
as articles, images, and videos. However, users have varying preferences for
monetary and psychological rewards, and the impact of monetary rewards on user
behaviors and the quality of the content they post remains unclear. Hence, we
propose a model that integrates some monetary reward schemes into the SNS-norms
game, which is an abstraction of CGM. Subsequently, we investigate the effect
of each monetary reward scheme on individual agents (users), particularly in
terms of their proactivity in posting items and their quality, depending on
agents' positions in a CGM network. Our experimental results suggest that these
factors distinctly affect the number of postings and their quality. We believe
that our findings will help CGM platformers in designing better monetary reward
schemes.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04807" title="Abstract">arXiv:2310.04807</a> [<a href="/pdf/2310.04807" title="Download PDF">pdf</a>, <a href="/format/2310.04807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OEDG: Oscillation-eliminating discontinuous Galerkin method for  hyperbolic conservation laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Peng%2C+M">Manting Peng</a>, 
<a href="/search/math?searchtype=author&query=Sun%2C+Z">Zheng Sun</a>, 
<a href="/search/math?searchtype=author&query=Wu%2C+K">Kailiang Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 14 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Controlling spurious oscillations is crucial for designing reliable numerical
schemes for hyperbolic conservation laws. This paper proposes a novel, robust,
and efficient oscillation-eliminating discontinuous Galerkin (OEDG) method on
general meshes, motivated by the damping technique in [Lu, Liu, and Shu, SIAM
J. Numer. Anal., 59:1299-1324, 2021]. The OEDG method incorporates an OE
procedure after each Runge-Kutta stage, devised by alternately evolving
conventional semidiscrete DG scheme and a damping equation. A novel damping
operator is carefully designed to possess scale-invariant and
evolution-invariant properties. We rigorously prove optimal error estimates of
the fully discrete OEDG method for linear scalar conservation laws. This might
be the first generic fully-discrete error estimates for nonlinear DG schemes
with automatic oscillation control mechanism. The OEDG method exhibits many
notable advantages. It effectively eliminates spurious oscillations for
challenging problems across various scales and wave speeds, without
problem-specific parameters. It obviates the need for characteristic
decomposition in hyperbolic systems. It retains key properties of conventional
DG method, such as conservation, optimal convergence rates, and
superconvergence. Moreover, it remains stable under normal CFL condition. The
OE procedure is non-intrusive, facilitating integration into existing DG codes
as an independent module. Its implementation is easy and efficient, involving
only simple multiplications of modal coefficients by scalars. The OEDG approach
provides new insights into the damping mechanism for oscillation control. It
reveals the role of damping operator as a modal filter and establishes close
relations between the damping and spectral viscosity techniques. Extensive
numerical results confirm the theoretical analysis and validate the
effectiveness and advantages of the OEDG method.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04808" title="Abstract">arXiv:2310.04808</a> [<a href="/pdf/2310.04808" title="Download PDF">pdf</a>, <a href="/format/2310.04808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining UPerNet and ConvNeXt for Contrails Identification to reduce  Global Warming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenkuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Semantic segmentation is a critical tool in computer vision, applied in
various domains like autonomous driving and medical imaging. This study focuses
on aircraft contrail detection in global satellite images to improve contrail
models and mitigate their impact on climate change.An innovative data
preprocessing technique for NOAA GOES-16 satellite images is developed, using
brightness temperature data from the infrared channel to create false-color
images, enhancing model perception. To tackle class imbalance, the training
dataset exclusively includes images with positive contrail labels.The model
selection is based on the UPerNet architecture, implemented using the
MMsegmentation library, with the integration of two ConvNeXt configurations for
improved performance. Cross-entropy loss with positive class weights enhances
contrail recognition. Fine-tuning employs the AdamW optimizer with a learning
rate of $2.5 \times 10^{-4}$.During inference, a multi-model prediction fusion
strategy and a contrail determination threshold of 0.75 yield a binary
prediction mask. RLE encoding is used for efficient prediction result
organization.The approach achieves exceptional results, boasting a high Dice
coefficient score, placing it in the top 5\% of participating teams. This
underscores the innovative nature of the segmentation model and its potential
for enhanced contrail recognition in satellite imagery.For further exploration,
the code and models are available on GitHub:
\url{https://github.com/biluko/2023GRIC.git}.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04809" title="Abstract">arXiv:2310.04809</a> [<a href="/pdf/2310.04809" title="Download PDF">pdf</a>, <a href="/format/2310.04809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging LLVM&#x27;s ScalarEvolution for Symbolic Data Cache Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Touzeau%2C+V">Valentin Touzeau</a>, 
<a href="/search/cs?searchtype=author&query=Reineke%2C+J">Jan Reineke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of RTSS 2023 paper including definitions and proofs omitted in the conference version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">While instruction cache analysis is essentially a solved problem, data cache
analysis is more challenging. In contrast to instruction fetches, the data
accesses generated by a memory instruction may vary with the program's inputs
and across dynamic occurrences of the same instruction in loops.
<br />We observe that the plain control-flow graph (CFG) abstraction employed in
classical cache analyses is inadequate to capture the dynamic behavior of
memory instructions. On top of plain CFGs, accurate analysis of the underlying
program's cache behavior is impossible.
<br />Thus, our first contribution is the definition of a more expressive program
abstraction coined symbolic control-flow graphs, which can be obtained from
LLVM's ScalarEvolution analysis. To exploit this richer abstraction, our main
contribution is the development of symbolic data cache analysis, a smooth
generalization of classical LRU must analysis from plain to symbolic
control-flow graphs.
<br />The experimental evaluation demonstrates that symbolic data cache analysis
consistently outperforms classical LRU must analysis both in terms of accuracy
and analysis runtime.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04811" title="Abstract">arXiv:2310.04811</a> [<a href="/pdf/2310.04811" title="Download PDF">pdf</a>, <a href="/format/2310.04811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FM Tone Transfer with Envelope Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Caspe%2C+F">Franco Caspe</a>, 
<a href="/search/cs?searchtype=author&query=McPherson%2C+A">Andrew McPherson</a>, 
<a href="/search/cs?searchtype=author&query=Sandler%2C+M">Mark Sandler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Audio Mostly 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Neural and Evolutionary Computing (cs.NE); Audio and Speech Processing (eess.AS); Systems and Control (eess.SY)

</div>
<p class="mathjax">Tone Transfer is a novel deep-learning technique for interfacing a sound
source with a synthesizer, transforming the timbre of audio excerpts while
keeping their musical form content. Due to its good audio quality results and
continuous controllability, it has been recently applied in several audio
processing tools. Nevertheless, it still presents several shortcomings related
to poor sound diversity, and limited transient and dynamic rendering, which we
believe hinder its possibilities of articulation and phrasing in a real-time
performance context.
<br />In this work, we present a discussion on current Tone Transfer architectures
for the task of controlling synthetic audio with musical instruments and
discuss their challenges in allowing expressive performances. Next, we
introduce Envelope Learning, a novel method for designing Tone Transfer
architectures that map musical events using a training objective at the
synthesis parameter level. Our technique can render note beginnings and endings
accurately and for a variety of sounds; these are essential steps for improving
musical articulation, phrasing, and sound diversity with Tone Transfer.
Finally, we implement a VST plugin for real-time live use and discuss
possibilities for improvement.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04812" title="Abstract">arXiv:2310.04812</a> [<a href="/pdf/2310.04812" title="Download PDF">pdf</a>, <a href="/ps/2310.04812" title="Download PostScript">ps</a>, <a href="/format/2310.04812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applications of Littlestone dimension to query learning and to  compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chase%2C+H">Hunter Chase</a>, 
<a href="/search/cs?searchtype=author&query=Freitag%2C+J">James Freitag</a>, 
<a href="/search/cs?searchtype=author&query=Reyzin%2C+L">Lev Reyzin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/1904.10122">arXiv:1904.10122</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Logic (math.LO)

</div>
<p class="mathjax">In this paper we give several applications of Littlestone dimension. The
first is to the model of \cite{angluin2017power}, where we extend their results
for learning by equivalence queries with random counterexamples. Second, we
extend that model to infinite concept classes with an additional source of
randomness. Third, we give improved results on the relationship of Littlestone
dimension to classes with extended $d$-compression schemes, proving a strong
version of a conjecture of \cite{floyd1995sample} for Littlestone dimension.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04813" title="Abstract">arXiv:2310.04813</a> [<a href="/pdf/2310.04813" title="Download PDF">pdf</a>, <a href="/format/2310.04813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Age of Information Guaranteed Scheduling for Asynchronous Status Updates  in Collaborative Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lehan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingzhou Sun</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuxuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Zhisheng Niu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures, presented at 2023 Workshop on Modeling and Optimization in Semantic Communications (MOSC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">We consider collaborative perception (CP) systems where a fusion center
monitors various regions by multiple sources. The center has different age of
information (AoI) constraints for different regions. Multi-view sensing data
for a region generated by sources can be fused by the center for a reliable
representation of the region. To ensure accurate perception, differences
between generation time of asynchronous status updates for CP fusion should not
exceed a certain threshold. An algorithm named scheduling for CP with
asynchronous status updates (SCPA) is proposed to minimize the number of
required channels and subject to AoI constraints with asynchronous status
updates. SCPA first identifies a set of sources that can satisfy the
constraints with minimum updating rates. It then chooses scheduling intervals
and offsets for the sources such that the number of required channels is
optimized. According to numerical results, the number of channels required by
SCPA can reach only 12% more than a derived lower bound.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04815" title="Abstract">arXiv:2310.04815</a> [<a href="/pdf/2310.04815" title="Download PDF">pdf</a>, <a href="/format/2310.04815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Critique Ability of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Liangchen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yinxiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+L">Lei Shu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+L">Lei Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Critical thinking is essential for rational decision-making and
problem-solving. This skill hinges on the ability to provide precise and
reasoned critiques and is a hallmark of human intelligence. In the era of large
language models (LLMs), this study explores the ability of LLMs to deliver
accurate critiques across various tasks. We are interested in this topic as a
capable critic model could not only serve as a reliable evaluator, but also as
a source of supervised signals for model tuning. Particularly, if a model can
self-critique, it has the potential for autonomous self-improvement. To examine
this, we introduce a unified evaluation framework for assessing the critique
abilities of LLMs. We develop a benchmark called CriticBench, which comprises
3K high-quality natural language queries and corresponding model responses; and
annotate the correctness of these responses. The benchmark cover tasks such as
math problem-solving, code completion, and question answering. We evaluate
multiple LLMs on the collected dataset and our analysis reveals several
noteworthy insights: (1) Critique is generally challenging for most LLMs, and
this capability often emerges only when models are sufficiently large. (2) In
particular, self-critique is especially difficult. Even top-performing LLMs
struggle to achieve satisfactory performance. (3) Models tend to have lower
critique accuracy on problems where they are most uncertain. To this end, we
introduce a simple yet effective baseline named self-check, which leverages
self-critique to improve task performance for various models. We hope this
study serves as an initial exploration into understanding the critique
abilities of LLMs, and aims to inform future research, including the
development of more proficient critic models and the application of critiques
across diverse tasks.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04816" title="Abstract">arXiv:2310.04816</a> [<a href="/pdf/2310.04816" title="Download PDF">pdf</a>, <a href="/format/2310.04816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hacking Generative Models with Differentiable Network Bending
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aldegheri%2C+G">Giacomo Aldegheri</a>, 
<a href="/search/cs?searchtype=author&query=Rogalska%2C+A">Alina Rogalska</a>, 
<a href="/search/cs?searchtype=author&query=Youssef%2C+A">Ahmed Youssef</a>, 
<a href="/search/cs?searchtype=author&query=Iofinova%2C+E">Eugenia Iofinova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we propose a method to 'hack' generative models, pushing their
outputs away from the original training distribution towards a new objective.
We inject a small-scale trainable module between the intermediate layers of the
model and train it for a low number of iterations, keeping the rest of the
network frozen. The resulting output images display an uncanny quality, given
by the tension between the original and new objectives that can be exploited
for artistic purposes.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04817" title="Abstract">arXiv:2310.04817</a> [<a href="/pdf/2310.04817" title="Download PDF">pdf</a>, <a href="/format/2310.04817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Grouping-based Scheduler for Efficient Channel Utilization under Age  of Information Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lehan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingzhou Sun</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuxuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Zhisheng Niu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures, presented at the 34th international teletraffic congress (ITC34)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">We consider a status information updating system where a fusion center
collects the status information from a large number of sources and each of them
has its own age of information (AoI) constraints. A novel grouping-based
scheduler is proposed to solve this complex large-scale problem by dividing the
sources into different scheduling groups. The problem is then transformed into
deriving the optimal grouping scheme. A two-step grouping algorithm (TGA) is
proposed: 1) Given AoI constraints, we first identify the sources with harmonic
AoI constraints, then design a fast grouping method and an optimal scheduler
for these sources. Under harmonic AoI constraints, each constraint is divisible
by the smallest one and the sum of reciprocals of the constraints with the same
value is divisible by the reciprocal of the smallest one. 2) For the other
sources without such a special property, we pack the sources which can be
scheduled together with minimum update rates into the same group. Simulations
show the channel usage of the proposed TGA is significantly reduced as compared
to a recent work and is 0.42% larger than a derived lower bound when the number
of sources is large.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04820" title="Abstract">arXiv:2310.04820</a> [<a href="/pdf/2310.04820" title="Download PDF">pdf</a>, <a href="/ps/2310.04820" title="Download PostScript">ps</a>, <a href="/format/2310.04820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The BCH Family of Storage Codes on Triangle-Free Graphs is of Unit Rate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+H">Haihua Deng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hexiang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+G">Guobiao Weng</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Q">Qing Xiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">Let $\Gamma$ be a simple connected graph on $n$ vertices, and let $C$ be a
code of length $n$ whose coordinates are indexed by the vertices of $\Gamma$.
We say that $C$ is a \textit{storage code} on $\Gamma$ if for any codeword $c
\in C$, one can recover the information on each coordinate of $c$ by accessing
its neighbors in $\Gamma$. The main problem here is to construct high-rate
storage codes on triangle-free graphs. In this paper, we solve an open problem
posed by Barg and Z\'emor in 2022, showing that the BCH family of storage codes
is of unit rate. Furthermore, we generalize the construction of the BCH family
and obtain more storage codes of unit rate on triangle-free graphs.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04821" title="Abstract">arXiv:2310.04821</a> [<a href="/pdf/2310.04821" title="Download PDF">pdf</a>, <a href="/format/2310.04821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethink Baseline of Integrated Gradients from the Perspective of Shapley  Value
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zixuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+S">Shi Ge</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Ji Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Changjie Fan</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yu Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R+W+Y">Runze Wu Yujing Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Ze Ji</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Numerous approaches have attempted to interpret deep neural networks (DNNs)
by attributing the prediction of DNN to its input features. One of the
well-studied attribution methods is Integrated Gradients (IG). Specifically,
the choice of baselines for IG is a critical consideration for generating
meaningful and unbiased explanations for model predictions in different
scenarios. However, current practice of exploiting a single baseline fails to
fulfill this ambition, thus demanding multiple baselines. Fortunately, the
inherent connection between IG and Aumann-Shapley Value forms a unique
perspective to rethink the design of baselines. Under certain hypothesis, we
theoretically analyse that a set of baseline aligns with the coalitions in
Shapley Value. Thus, we propose a novel baseline construction method called
Shapley Integrated Gradients (SIG) that searches for a set of baselines by
proportional sampling to partly simulate the computation path of Shapley Value.
Simulations on GridWorld show that SIG approximates the proportion of Shapley
Values. Furthermore, experiments conducted on various image tasks demonstrate
that compared to IG using other baseline methods, SIG exhibits an improved
estimation of feature's contribution, offers more consistent explanations
across diverse applications, and is generic to distinct data types or instances
with insignificant computational overhead.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04822" title="Abstract">arXiv:2310.04822</a> [<a href="/pdf/2310.04822" title="Download PDF">pdf</a>, <a href="/format/2310.04822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Sampling- and Gradient-based Planning for Contact-rich  Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rozzi%2C+F">Filippo Rozzi</a>, 
<a href="/search/cs?searchtype=author&query=Roveda%2C+L">Loris Roveda</a>, 
<a href="/search/cs?searchtype=author&query=Haninger%2C+K">Kevin Haninger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted ICRA24. Video available at <a href="https://youtu.be/COqR90392Kw">this https URL</a> Code available at <a href="https://gitlab.cc-asp.fraunhofer.de/hanikevi/contact_mpc">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Planning over discontinuous dynamics is needed for robotics tasks like
contact-rich manipulation, which presents challenges in the numerical stability
and speed of planning methods when either neural network or analytical models
are used. On the one hand, sampling-based planners require higher sample
complexity in high-dimensional problems and cannot describe safety constraints
such as force limits. On the other hand, gradient-based solvers can suffer from
local optima and convergence issues when the Hessian is poorly conditioned. We
propose a planning method with both sampling- and gradient-based elements,
using the Cross-entropy Method to initialize a gradient-based solver, providing
better search over local minima and the ability to handle explicit constraints.
We show the approach allows smooth, stable contact-rich planning for an
impedance-controlled robot making contact with a stiff environment,
benchmarking against gradient-only MPC and CEM.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04824" title="Abstract">arXiv:2310.04824</a> [<a href="/pdf/2310.04824" title="Download PDF">pdf</a>, <a href="/format/2310.04824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PaperCard for Reporting Machine Assistance in Academic Writing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+W+I">Won Ik Cho</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+E">Eunjung Cho</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+K">Kyunghyun Cho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EAAMO'23 as a poster presentation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Academic writing process has benefited from various technological
developments over the years including search engines, automatic translators,
and editing tools that review grammar and spelling mistakes. They have enabled
human writers to become more efficient in writing academic papers, for example
by helping with finding relevant literature more effectively and polishing
texts. While these developments have so far played a relatively assistive role,
recent advances in large-scale language models (LLMs) have enabled LLMs to play
a more major role in the writing process, such as coming up with research
questions and generating key contents. This raises critical questions
surrounding the concept of authorship in academia. ChatGPT, a
question-answering system released by OpenAI in November 2022, has demonstrated
a range of capabilities that could be utilised in producing academic papers.
The academic community will have to address relevant pressing questions,
including whether Artificial Intelligence (AI) should be merited authorship if
it made significant contributions in the writing process, or whether its use
should be restricted such that human authorship would not be undermined. In
this paper, we aim to address such questions, and propose a framework we name
"PaperCard", a documentation for human authors to transparently declare the use
of AI in their writing process.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04825" title="Abstract">arXiv:2310.04825</a> [<a href="/pdf/2310.04825" title="Download PDF">pdf</a>, <a href="/format/2310.04825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative study of multi-person tracking methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akola%2C+D+M">Denis Mbey Akola</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents a study of two tracking algorithms (SORT~\cite{7533003}
and Tracktor++~\cite{2019}) that were ranked first positions on the MOT
Challenge leaderboard (The MOTChallenge web page: https://motchallenge.net ).
The purpose of this study is to discover the techniques used and to provide
useful insights about these algorithms in the tracking pipeline that could
improve the performance of MOT tracking algorithms. To this end, we adopted the
popular tracking-by-detection approach. We trained our own Pedestrian Detection
model using the MOT17Det dataset (MOT17Det :
https://motchallenge.net/data/MOT17Det/ ). We also used a re-identification
model trained on MOT17 dataset (MOT17 : https://motchallenge.net/data/MOT17/ )
for Tracktor++ to reduce the false re-identification alarms. We then present
experimental results which shows that Tracktor++ is a better multi-person
tracking algorithm than SORT. We also performed ablation studies to discover
the contribution of re-identification(RE-ID) network and motion to the results
of Tracktor++. We finally conclude by providing some recommendations for future
research.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04826" title="Abstract">arXiv:2310.04826</a> [<a href="/pdf/2310.04826" title="Download PDF">pdf</a>, <a href="/format/2310.04826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmenting Static Visualizations with PapARVis Designer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu-Tian%2C+C">Chen Zhu-Tian</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+W">Wai Tong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianwen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bach%2C+B">Benjamin Bach</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Huamin Qu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">This paper presents an authoring environment for augmenting static
visualizations with virtual content in augmented reality. Augmenting static
visualizations can leverage the best of both physical and digital worlds, but
its creation currently involves different tools and devices, without any means
to explicitly design and debug both static and virtual content simultaneously.
To address these issues, we design an environment that seamlessly integrates
all steps of a design and deployment workflow through its main features: i) an
extension to Vega, ii) a preview, and iii) debug hints that facilitate valid
combinations of static and augmented content. We inform our design through a
design space with four ways to augment static visualizations. We demonstrate
the expressiveness of our tool through examples, including books, posters,
projections, wall-sized visualizations. A user study shows high user
satisfaction of our environment and confirms that participants can create
augmented visualizations in an average of 4.63 minutes.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04828" title="Abstract">arXiv:2310.04828</a> [<a href="/pdf/2310.04828" title="Download PDF">pdf</a>, <a href="/format/2310.04828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guardians as You Fall: Active Mode Transition for Safe Falling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yikai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mengdi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Guanya Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Ding Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> website: <a href="https://sites.google.com/view/guardians-as-you-fall/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Recent advancements in optimal control and reinforcement learning have
enabled quadrupedal robots to perform various agile locomotion tasks over
diverse terrains. During these agile motions, ensuring the stability and
resiliency of the robot is a primary concern to prevent catastrophic falls and
mitigate potential damages. Previous methods primarily focus on recovery
policies after the robot falls. There is no active safe falling solution to the
best of our knowledge. In this paper, we proposed Guardians as You Fall (GYF),
a safe falling/tumbling and recovery framework that can actively tumble and
recover to stable modes to reduce damage in highly dynamic scenarios. The key
idea of GYF is to adaptively traverse different stable modes via active
tumbling before the robot shifts to irrecoverable poses. Via comprehensive
simulation and real-world experiments, we show that GYF significantly reduces
the maximum acceleration and jerk of the robot base compared to the baselines.
In particular, GYF reduces the maximum acceleration and jerk by 20%~73% in
different scenarios in simulation and real-world experiments. GYF offers a new
perspective on safe falling and recovery in locomotion tasks, potentially
enabling much more aggressive explorations of existing agile locomotion skills.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04829" title="Abstract">arXiv:2310.04829</a> [<a href="/pdf/2310.04829" title="Download PDF">pdf</a>, <a href="/format/2310.04829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to effectively train an ensemble of Faster R-CNN object detectors to  quantify uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akola%2C+D+M">Denis Mbey Akola</a>, 
<a href="/search/cs?searchtype=author&query=Franchi%2C+G">Gianni Franchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents a new approach for training two-stage object detection
ensemble models, more specifically, Faster R-CNN models to estimate
uncertainty. We propose training one Region Proposal
Network(RPN)~\cite{https://doi.org/10.48550/arxiv.<a href="/abs/1506.01497">1506.01497</a>} and multiple Fast
R-CNN prediction heads is all you need to build a robust deep ensemble network
for estimating uncertainty in object detection. We present this approach and
provide experiments to show that this approach is much faster than the naive
method of fully training all $n$ models in an ensemble. We also estimate the
uncertainty by measuring this ensemble model's Expected Calibration Error
(ECE). We then further compare the performance of this model with that of
Gaussian YOLOv3, a variant of YOLOv3 that models uncertainty using predicted
bounding box coordinates. The source code is released at
\url{https://github.com/Akola-Mbey-Denis/EfficientEnsemble}
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04830" title="Abstract">arXiv:2310.04830</a> [<a href="/pdf/2310.04830" title="Download PDF">pdf</a>, <a href="/format/2310.04830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extract-Transform-Load for Video Streams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kossmann%2C+F">Ferdinand Kossmann</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Ziniu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+E">Eugenie Lai</a>, 
<a href="/search/cs?searchtype=author&query=Tatbul%2C+N">Nesime Tatbul</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Lei Cao</a>, 
<a href="/search/cs?searchtype=author&query=Kraska%2C+T">Tim Kraska</a>, 
<a href="/search/cs?searchtype=author&query=Madden%2C+S">Samuel Madden</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 23 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proc. VLDB Endow. 16, 9 (May 2023), 2302-2315
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Social media, self-driving cars, and traffic cameras produce video streams at
large scales and cheap cost. However, storing and querying video at such scales
is prohibitively expensive. We propose to treat large-scale video analytics as
a data warehousing problem: Video is a format that is easy to produce but needs
to be transformed into an application-specific format that is easy to query.
Analogously, we define the problem of Video Extract-Transform-Load (V-ETL).
V-ETL systems need to reduce the cost of running a user-defined V-ETL job while
also giving throughput guarantees to keep up with the rate at which data is
produced. We find that no current system sufficiently fulfills both needs and
therefore propose Skyscraper, a system tailored to V-ETL. Skyscraper can
execute arbitrary video ingestion pipelines and adaptively tunes them to reduce
cost at minimal or no quality degradation, e.g., by adjusting sampling rates
and resolutions to the ingested content. Skyscraper can hereby be provisioned
with cheap on-premises compute and uses a combination of buffering and cloud
bursting to deal with peaks in workload caused by expensive processing
configurations. In our experiments, we find that Skyscraper significantly
reduces the cost of V-ETL ingestion compared to adaptions of current SOTA
systems, while at the same time giving robustness guarantees that these systems
are lacking.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04832" title="Abstract">arXiv:2310.04832</a> [<a href="/pdf/2310.04832" title="Download PDF">pdf</a>, <a href="/format/2310.04832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperSINDy: Deep Generative Modeling of Nonlinear Stochastic Governing  Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jacobs%2C+M">Mozes Jacobs</a>, 
<a href="/search/cs?searchtype=author&query=Brunton%2C+B+W">Bingni W. Brunton</a>, 
<a href="/search/cs?searchtype=author&query=Brunton%2C+S+L">Steven L. Brunton</a>, 
<a href="/search/cs?searchtype=author&query=Kutz%2C+J+N">J. Nathan Kutz</a>, 
<a href="/search/cs?searchtype=author&query=Raut%2C+R+V">Ryan V. Raut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 4 figures (main text), 4 figures (appendix)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The discovery of governing differential equations from data is an open
frontier in machine learning. The sparse identification of nonlinear dynamics
(SINDy) \citep{brunton_discovering_2016} framework enables data-driven
discovery of interpretable models in the form of sparse, deterministic
governing laws. Recent works have sought to adapt this approach to the
stochastic setting, though these adaptations are severely hampered by the curse
of dimensionality. On the other hand, Bayesian-inspired deep learning methods
have achieved widespread success in high-dimensional probabilistic modeling via
computationally efficient approximate inference techniques, suggesting the use
of these techniques for efficient stochastic equation discovery. Here, we
introduce HyperSINDy, a framework for modeling stochastic dynamics via a deep
generative model of sparse governing equations whose parametric form is
discovered from data. HyperSINDy employs a variational encoder to approximate
the distribution of observed states and derivatives. A hypernetwork
\citep{ha_hypernetworks_2016} transforms samples from this distribution into
the coefficients of a differential equation whose sparse form is learned
simultaneously using a trainable binary mask \citep{louizos_learning_2018}.
Once trained, HyperSINDy generates stochastic dynamics via a differential
equation whose coefficients are driven by a Gaussian white noise. In
experiments, HyperSINDy accurately recovers ground truth stochastic governing
equations, with learned stochasticity scaling to match that of the data.
Finally, HyperSINDy provides uncertainty quantification that scales to
high-dimensional systems. Taken together, HyperSINDy offers a promising
framework for model discovery and uncertainty quantification in real-world
systems, integrating sparse equation discovery methods with advances in
statistical machine learning and deep generative modeling.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04835" title="Abstract">arXiv:2310.04835</a> [<a href="/pdf/2310.04835" title="Download PDF">pdf</a>, <a href="/format/2310.04835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Evolution of Knowledge Graphs: A Survey and Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xuhui Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chengjin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yinghan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+L">Lumingyuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Saizhuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhongwu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuanzhuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jian Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Knowledge graphs (KGs) are structured representations of diversified
knowledge. They are widely used in various intelligent applications. In this
article, we provide a comprehensive survey on the evolution of various types of
knowledge graphs (i.e., static KGs, dynamic KGs, temporal KGs, and event KGs)
and techniques for knowledge extraction and reasoning. Furthermore, we
introduce the practical applications of different types of KGs, including a
case study in financial analysis. Finally, we propose our perspective on the
future directions of knowledge engineering, including the potential of
combining the power of knowledge graphs and large language models (LLMs), and
the evolution of knowledge extraction, reasoning, and representation.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04836" title="Abstract">arXiv:2310.04836</a> [<a href="/pdf/2310.04836" title="Download PDF">pdf</a>, <a href="/format/2310.04836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Luoming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+W">Wen Fei</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Weijia Wu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yefei He</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+Z">Zhenyu Lou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hong Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) pose significant hardware challenges related to
memory requirements and computational ability. There are two mainstream
quantization schemes for LLMs: coarse-grained ($\textit{e.g.,}$ channel-wise)
quantization and fine-grained ($\textit{e.g.,}$ group-wise) quantization.
Fine-grained quantization has smaller quantization loss, consequently achieving
superior performance. However, when applied to weight-activation quantization,
it disrupts continuous integer matrix multiplication, leading to inefficient
inference. In this paper, we introduce Dual Grained Quantization (DGQ), a novel
A8W4 quantization for LLM that maintains superior performance while ensuring
fast inference speed. DSQ dequantizes the fine-grained INT4 weight into
coarse-grained INT8 representation and preform matrix multiplication using INT8
kernels. Besides, we develop a two-phase grid search algorithm to simplify the
determination of fine-grained and coarse-grained quantization scales. We also
devise a percentile clipping schema for smoothing the activation outliers
without the need for complex optimization techniques. Experimental results
demonstrate that DGQ consistently outperforms prior methods across various LLM
architectures and a wide range of tasks. Remarkably, by our implemented
efficient CUTLASS kernel, we achieve $\textbf{1.12}$ $\times$ memory reduction
and $\textbf{3.24}$ $\times$ speed gains comparing A16W4 implementation. These
advancements enable efficient deployment of A8W4 LLMs for real-world
applications.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04837" title="Abstract">arXiv:2310.04837</a> [<a href="/pdf/2310.04837" title="Download PDF">pdf</a>, <a href="/format/2310.04837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Self-Supervised Learning of Monocular Depth Estimators for  Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+S.+Soares%2C+E+F">Elton F. de S. Soares</a>, 
<a href="/search/cs?searchtype=author&query=Campos%2C+C+A+V">Carlos Alberto V. Campos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 8 figures, journal preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Image-based depth estimation has gained significant attention in recent
research on computer vision for autonomous vehicles in intelligent
transportation systems. This focus stems from its cost-effectiveness and wide
range of potential applications. Unlike binocular depth estimation methods that
require two fixed cameras, monocular depth estimation methods only rely on a
single camera, making them highly versatile. While state-of-the-art approaches
for this task leverage self-supervised learning of deep neural networks in
conjunction with tasks like pose estimation and semantic segmentation, none of
them have explored the combination of federated learning and self-supervision
to train models using unlabeled and private data captured by autonomous
vehicles. The utilization of federated learning offers notable benefits,
including enhanced privacy protection, reduced network consumption, and
improved resilience to connectivity issues. To address this gap, we propose
FedSCDepth, a novel method that combines federated learning and deep
self-supervision to enable the learning of monocular depth estimators with
comparable effectiveness and superior efficiency compared to the current
state-of-the-art methods. Our evaluation experiments conducted on Eigen's Split
of the KITTI dataset demonstrate that our proposed method achieves near
state-of-the-art performance, with a test loss below 0.13 and requiring, on
average, only 1.5k training steps and up to 0.415 GB of weight data transfer
per autonomous vehicle on each round.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04842" title="Abstract">arXiv:2310.04842</a> [<a href="/pdf/2310.04842" title="Download PDF">pdf</a>, <a href="/format/2310.04842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sub-linear Regret in Adaptive Model Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tranos%2C+D">Damianos Tranos</a>, 
<a href="/search/eess?searchtype=author&query=Proutiere%2C+A">Alexandre Proutiere</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We consider the problem of adaptive Model Predictive Control (MPC) for
uncertain linear-systems with additive disturbances and with state and input
constraints. We present STT-MPC (Self-Tuning Tube-based Model Predictive
Control), an online algorithm that combines the certainty-equivalence principle
and polytopic tubes. Specifically, at any given step, STT-MPC infers the system
dynamics using the Least Squares Estimator (LSE), and applies a controller
obtained by solving an MPC problem using these estimates. The use of polytopic
tubes is so that, despite the uncertainties, state and input constraints are
satisfied, and recursive-feasibility and asymptotic stability hold. In this
work, we analyze the regret of the algorithm, when compared to an oracle
algorithm initially aware of the system dynamics. We establish that the
expected regret of STT-MPC does not exceed $O(T^{1/2 + \epsilon})$, where
$\epsilon \in (0,1)$ is a design parameter tuning the persistent excitation
component of the algorithm. Our result relies on a recently proposed
exponential decay of sensitivity property and, to the best of our knowledge, is
the first of its kind in this setting. We illustrate the performance of our
algorithm using a simple numerical example.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04843" title="Abstract">arXiv:2310.04843</a> [<a href="/pdf/2310.04843" title="Download PDF">pdf</a>, <a href="/format/2310.04843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MARVisT: Authoring Glyph-based Visualization in Mobile Augmented Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu-Tian%2C+C">Chen Zhu-Tian</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yijia Su</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianwen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Huamin Qu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yingcai Wu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Visualization and Computer Graphics ( Volume:
  26, Issue: 8, 01 August 2020)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Recent advances in mobile augmented reality (AR) techniques have shed new
light on personal visualization for their advantages of fitting visualization
within personal routines, situating visualization in a real-world context, and
arousing users' interests. However, enabling non-experts to create data
visualization in mobile AR environments is challenging given the lack of tools
that allow in-situ design while supporting the binding of data to AR content.
Most existing AR authoring tools require working on personal computers or
manually creating each virtual object and modifying its visual attributes. We
systematically study this issue by identifying the specificity of AR
glyph-based visualization authoring tool and distill four design
considerations. Following these design considerations, we design and implement
MARVisT, a mobile authoring tool that leverages information from reality to
assist non-experts in addressing relationships between data and virtual glyphs,
real objects and virtual glyphs, and real objects and data. With MARVisT, users
without visualization expertise can bind data to real-world objects to create
expressive AR glyph-based visualizations rapidly and effortlessly, reshaping
the representation of the real world with data. We use several examples to
demonstrate the expressiveness of MARVisT. A user study with non-experts is
also conducted to evaluate the authoring experience of MARVisT.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04845" title="Abstract">arXiv:2310.04845</a> [<a href="/pdf/2310.04845" title="Download PDF">pdf</a>, <a href="/format/2310.04845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Facial Relationships and Feature Aggregation for Multi-Face  Forgery Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenhao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+F">Fangbin Yi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qian Li</a>, 
<a href="/search/cs?searchtype=author&query=Jingyi%2C+D">Deng Jingyi</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chao Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Face forgery techniques have emerged as a forefront concern, and numerous
detection approaches have been proposed to address this challenge. However,
existing methods predominantly concentrate on single-face manipulation
detection, leaving the more intricate and realistic realm of multi-face
forgeries relatively unexplored. This paper proposes a novel framework
explicitly tailored for multi-face forgery detection,filling a critical gap in
the current research. The framework mainly involves two modules:(i) a facial
relationships learning module, which generates distinguishable local features
for each face within images,(ii) a global feature aggregation module that
leverages the mutual constraints between global and local information to
enhance forgery detection accuracy.Our experimental results on two publicly
available multi-face forgery datasets demonstrate that the proposed approach
achieves state-of-the-art performance in multi-face forgery detection
scenarios.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04846" title="Abstract">arXiv:2310.04846</a> [<a href="/pdf/2310.04846" title="Download PDF">pdf</a>, <a href="/format/2310.04846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soft finger dynamic stability and slip by Coulomb friction and bulk  stiffness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jang%2C+H">Hun Jang</a>, 
<a href="/search/cs?searchtype=author&query=Petrichenko%2C+V">Valentyn Petrichenko</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+J">Joonbum Bae</a>, 
<a href="/search/cs?searchtype=author&query=Haninger%2C+K">Kevin Haninger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted ICRA24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Soft robotic fingers can safely grasp fragile or non-uniform objects, but
their force capacity is limited, especially with less contact area: objects
which are smaller, not round, or where an enclosing grasp is not feasible. To
improve force capacity, this paper considers two types of grip failure, slip
and dynamic rotational stability. For slip, a Coulomb model for soft fingers
based on total normal and tangential force is validated, identifying the effect
of contact area, pressure, and grip position on effective Coulomb coefficient,
normal force and transverse stiffness. For rotational stability, bulk stiffness
of the fingers is used to develop conditions for dynamic stability about the
initial grasp, and a condition for when the rotation leads to slip. Together,
these models suggest contact area improves grip by increasing transverse
stiffness and normal force. The models are validated in a range of grasp
conditions, shown to predict the influence of object radius and finger distance
on grip stability limits.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04848" title="Abstract">arXiv:2310.04848</a> [<a href="/pdf/2310.04848" title="Download PDF">pdf</a>, <a href="/format/2310.04848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interference analysis of shared last-level cache on embedded GP-GPUs  with multiple CUDA streams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brilli%2C+G">Gianluca Brilli</a>, 
<a href="/search/cs?searchtype=author&query=Burgio%2C+P">Paolo Burgio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 6 figures, 2 code listings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">In modern heterogeneous architectures, the access to data that the
application needs is a key factor, in order to make the compute task efficient,
in terms of power dissipation and execution time. The new generation SoCs are
equipped with large LLCs, in order to make data access as efficient as
possible. However, these systems introduce a new level of complexity in terms
of the system's predictability, because concurrent tasks must compete for the
same resource and contribute to generating interference between them. This
paper aims to provide a preliminary qualitative analysis in terms of
interference degree that is generated when several concurrent streams are in
execution, for example one that performs useful computing tasks and one that
generates interference. Specifically, we tested two important primitives: vadd
and gemm, respectively subjected to interference with: i) a concurrent kernel
that performs read from shared memory. ii) concurrent stream that performs
host-to-device memory copy.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04852" title="Abstract">arXiv:2310.04852</a> [<a href="/pdf/2310.04852" title="Download PDF">pdf</a>, <a href="/format/2310.04852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balancing utility and cognitive cost in social representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taylor-Davies%2C+M">Max Taylor-Davies</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+C+G">Christopher G. Lucas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Currently under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">To successfully navigate its environment, an agent must construct and
maintain representations of the other agents that it encounters. Such
representations are useful for many tasks, but they are not without cost. As a
result, agents must make decisions regarding how much information they choose
to represent about the agents in their environment. Using selective imitation
as an example task, we motivate the problem of finding agent representations
that optimally trade off between downstream utility and information cost, and
illustrate two example approaches to resource-constrained social
representation.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04855" title="Abstract">arXiv:2310.04855</a> [<a href="/pdf/2310.04855" title="Download PDF">pdf</a>, <a href="/format/2310.04855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Epsilon non-Greedy: A Bandit Approach for Unbiased Recommendation via  Uniform Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sani%2C+S+M+F">S.M.F. Sani</a>, 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+S+A">Seyed Abbas Hosseini</a>, 
<a href="/search/cs?searchtype=author&query=Rabiee%2C+H+R">Hamid R. Rabiee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Often, recommendation systems employ continuous training, leading to a
self-feedback loop bias in which the system becomes biased toward its previous
recommendations. Recent studies have attempted to mitigate this bias by
collecting small amounts of unbiased data. While these studies have
successfully developed less biased models, they ignore the crucial fact that
the recommendations generated by the model serve as the training data for
subsequent training sessions. To address this issue, we propose a framework
that learns an unbiased estimator using a small amount of uniformly collected
data and focuses on generating improved training data for subsequent training
iterations. To accomplish this, we view recommendation as a contextual
multi-arm bandit problem and emphasize on exploring items that the model has a
limited understanding of. We introduce a new offline sequential training schema
that simulates real-world continuous training scenarios in recommendation
systems, offering a more appropriate framework for studying self-feedback bias.
We demonstrate the superiority of our model over state-of-the-art debiasing
methods by conducting extensive experiments using the proposed training schema.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04856" title="Abstract">arXiv:2310.04856</a> [<a href="/pdf/2310.04856" title="Download PDF">pdf</a>, <a href="/format/2310.04856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LIPEx -- Locally Interpretable Probabilistic Explanations -- To Look  Beyond The True Class
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hongbo Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Cangelosi%2C+A">Angelo Cangelosi</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+P">Procheta Sen</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Anirbit Mukherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages,14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this work, we instantiate a novel perturbation-based multi-class
explanation framework, LIPEx (Locally Interpretable Probabilistic Explanation).
We demonstrate that LIPEx not only locally replicates the probability
distributions output by the widely used complex classification models but also
provides insight into how every feature deemed to be important affects the
prediction probability for each of the possible classes. We achieve this by
defining the explanation as a matrix obtained via regression with respect to
the Hellinger distance in the space of probability distributions. Ablation
tests on text and image data, show that LIPEx-guided removal of important
features from the data causes more change in predictions for the underlying
model than similar tests on other saliency-based or feature importance-based
XAI methods. It is also shown that compared to LIME, LIPEx is much more data
efficient in terms of the number of perturbations needed for reliable
evaluation of the explanation.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04857" title="Abstract">arXiv:2310.04857</a> [<a href="/pdf/2310.04857" title="Download PDF">pdf</a>, <a href="/format/2310.04857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aerial Base Stations: Practical Considerations for Power Consumption and  Service Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seeram%2C+S+S+S+G">Siva Satya Sri Ganesh Seeram</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ozger%2C+M">Mustafa Ozger</a>, 
<a href="/search/cs?searchtype=author&query=Grabs%2C+A">Andre Grabs</a>, 
<a href="/search/cs?searchtype=author&query=Holis%2C+J">Jaroslav Holis</a>, 
<a href="/search/cs?searchtype=author&query=Cavdar%2C+C">Cicek Cavdar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, 5 tables, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Aerial base stations (ABSs) have emerged as a promising solution to meet the
high traffic demands of future wireless networks. Nevertheless, their practical
implementation requires efficient utilization of limited payload and onboard
energy. Understanding the power consumption streams, such as mechanical and
communication power, and their relationship to the payload is crucial for
analyzing its feasibility. Specifically, we focus on rotary-wing drones (RWDs),
fixed-wing drones (FWDs), and high-altitude platforms (HAPs), analyzing their
energy consumption models and key performance metrics such as power
consumption, energy harvested-to-consumption ratio, and service time with
varying wingspans, battery capacities, and regions. Our findings indicate that
FWDs have longer service times and HAPs have energy harvested-to-consumption
ratios greater than one, indicating theoretically infinite service time,
especially when deployed in near-equator regions or have a large wingspan.
Additionally, we investigate the case study of RWD-BS deployment, assessing
aerial network dimensioning aspects such as ABS coverage radius based on
altitude, environment, and frequency of operation. Our findings provide
valuable insights for researchers and telecom operators, facilitating effective
cost planning by determining the number of ABSs and backup batteries required
for uninterrupted operations.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04858" title="Abstract">arXiv:2310.04858</a> [<a href="/pdf/2310.04858" title="Download PDF">pdf</a>, <a href="/format/2310.04858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-End Lip Reading in Romanian with Cross-Lingual Domain Adaptation  and Lateral Inhibition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C4%83nescu%2C+E">Emilian-Claudiu M&#x103;nescu</a>, 
<a href="/search/cs?searchtype=author&query=Sm%C4%83du%2C+R">R&#x103;zvan-Alexandru Sm&#x103;du</a>, 
<a href="/search/cs?searchtype=author&query=Avram%2C+A">Andrei-Marius Avram</a>, 
<a href="/search/cs?searchtype=author&query=Cercel%2C+D">Dumitru-Clementin Cercel</a>, 
<a href="/search/cs?searchtype=author&query=Pop%2C+F">Florin Pop</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures, Accepted by WI-IAT 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Lip reading or visual speech recognition has gained significant attention in
recent years, particularly because of hardware development and innovations in
computer vision. While considerable progress has been obtained, most models
have only been tested on a few large-scale datasets. This work addresses this
shortcoming by analyzing several architectures and optimizations on the
underrepresented, short-scale Romanian language dataset called Wild LRRo. Most
notably, we compare different backend modules, demonstrating the effectiveness
of adding ample regularization methods. We obtain state-of-the-art results
using our proposed method, namely cross-lingual domain adaptation and unlabeled
videos from English and German datasets to help the model learn
language-invariant features. Lastly, we assess the performance of adding a
layer inspired by the neural inhibition mechanism.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04861" title="Abstract">arXiv:2310.04861</a> [<a href="/pdf/2310.04861" title="Download PDF">pdf</a>, <a href="/format/2310.04861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncovering hidden geometry in Transformers via disentangling position  and context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiajun Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yiqiao Zhong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 56 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Transformers are widely used to extract complex semantic meanings from input
tokens, yet they usually operate as black-box models. In this paper, we present
a simple yet informative decomposition of hidden states (or embeddings) of
trained transformers into interpretable components. For any layer, embedding
vectors of input sequence samples are represented by a tensor $\boldsymbol{h}
\in \mathbb{R}^{C \times T \times d}$. Given embedding vector
$\boldsymbol{h}_{c,t} \in \mathbb{R}^d$ at sequence position $t \le T$ in a
sequence (or context) $c \le C$, extracting the mean effects yields the
decomposition \[ \boldsymbol{h}_{c,t} = \boldsymbol{\mu} + \mathbf{pos}_t +
\mathbf{ctx}_c + \mathbf{resid}_{c,t} \] where $\boldsymbol{\mu}$ is the global
mean vector, $\mathbf{pos}_t$ and $\mathbf{ctx}_c$ are the mean vectors across
contexts and across positions respectively, and $\mathbf{resid}_{c,t}$ is the
residual vector. For popular transformer architectures and diverse text
datasets, empirically we find pervasive mathematical structure: (1)
$(\mathbf{pos}_t)_{t}$ forms a low-dimensional, continuous, and often spiral
shape across layers, (2) $(\mathbf{ctx}_c)_c$ shows clear cluster structure
that falls into context topics, and (3) $(\mathbf{pos}_t)_{t}$ and
$(\mathbf{ctx}_c)_c$ are mutually incoherent -- namely $\mathbf{pos}_t$ is
almost orthogonal to $\mathbf{ctx}_c$ -- which is canonical in compressed
sensing and dictionary learning. This decomposition offers structural insights
about input formats in in-context learning (especially for induction heads) and
in arithmetic tasks.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04863" title="Abstract">arXiv:2310.04863</a> [<a href="/pdf/2310.04863" title="Download PDF">pdf</a>, <a href="/format/2310.04863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SA-Paraformer: Non-autoregressive End-to-End Speaker-Attributed ASR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yangze Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuhao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Pengcheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+M">Mohan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Z">Zhihao Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lei Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Joint modeling of multi-speaker ASR and speaker diarization has recently
shown promising results in speaker-attributed automatic speech recognition
(SA-ASR).Although being able to obtain state-of-the-art (SOTA) performance,
most of the studies are based on an autoregressive (AR) decoder which generates
tokens one-by-one and results in a large real-time factor (RTF). To speed up
inference, we introduce a recently proposed non-autoregressive model Paraformer
as an acoustic model in the SA-ASR model.Paraformer uses a single-step decoder
to enable parallel generation, obtaining comparable performance to the SOTA AR
transformer models. Besides, we propose a speaker-filling strategy to reduce
speaker identification errors and adopt an inter-CTC strategy to enhance the
encoder's ability in acoustic modeling. Experiments on the AliMeeting corpus
show that our model outperforms the cascaded SA-ASR model by a 6.1% relative
speaker-dependent character error rate (SD-CER) reduction on the test set.
Moreover, our model achieves a comparable SD-CER of 34.8% with only 1/10 RTF
compared with the SOTA joint AR SA-ASR model.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04865" title="Abstract">arXiv:2310.04865</a> [<a href="/pdf/2310.04865" title="Download PDF">pdf</a>, <a href="/format/2310.04865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ForeSeer: Product Aspect Forecasting Using Temporal Graph Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zixuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hiranandani%2C+G">Gaurush Hiranandani</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+K">Kun Qian</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+E+W">Eddie W. Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+B">Belinda Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Subbian%2C+K">Karthik Subbian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sheng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Developing text mining approaches to mine aspects from customer reviews has
been well-studied due to its importance in understanding customer needs and
product attributes. In contrast, it remains unclear how to predict the future
emerging aspects of a new product that currently has little review information.
This task, which we named product aspect forecasting, is critical for
recommending new products, but also challenging because of the missing reviews.
Here, we propose ForeSeer, a novel textual mining and product embedding
approach progressively trained on temporal product graphs for this novel
product aspect forecasting task. ForeSeer transfers reviews from similar
products on a large product graph and exploits these reviews to predict aspects
that might emerge in future reviews. A key novelty of our method is to jointly
provide review, product, and aspect embeddings that are both time-sensitive and
less affected by extremely imbalanced aspect frequencies. We evaluated ForeSeer
on a real-world product review system containing 11,536,382 reviews and 11,000
products over 3 years. We observe that ForeSeer substantially outperformed
existing approaches with at least 49.1\% AUPRC improvement under the real
setting where aspect associations are not given. ForeSeer further improves
future link prediction on the product graph and the review aspect association
prediction. Collectively, Foreseer offers a novel framework for review
forecasting by effectively integrating review text, product network, and
temporal information, opening up new avenues for online shopping recommendation
and e-commerce applications.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04867" title="Abstract">arXiv:2310.04867</a> [<a href="/pdf/2310.04867" title="Download PDF">pdf</a>, <a href="/format/2310.04867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Randomized Sparse Neural Galerkin Schemes for Solving Evolution  Equations with Deep Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berman%2C+J">Jules Berman</a>, 
<a href="/search/cs?searchtype=author&query=Peherstorfer%2C+B">Benjamin Peherstorfer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
<p class="mathjax">Training neural networks sequentially in time to approximate solution fields
of time-dependent partial differential equations can be beneficial for
preserving causality and other physics properties; however, the
sequential-in-time training is numerically challenging because training errors
quickly accumulate and amplify over time. This work introduces Neural Galerkin
schemes that update randomized sparse subsets of network parameters at each
time step. The randomization avoids overfitting locally in time and so helps
prevent the error from accumulating quickly over the sequential-in-time
training, which is motivated by dropout that addresses a similar issue of
overfitting due to neuron co-adaptation. The sparsity of the update reduces the
computational costs of training without losing expressiveness because many of
the network parameters are redundant locally at each time step. In numerical
experiments with a wide range of evolution equations, the proposed scheme with
randomized sparse updates is up to two orders of magnitude more accurate at a
fixed computational budget and up to two orders of magnitude faster at a fixed
accuracy than schemes with dense updates.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04869" title="Abstract">arXiv:2310.04869</a> [<a href="/pdf/2310.04869" title="Download PDF">pdf</a>, <a href="/format/2310.04869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ILuvUI: Instruction-tuned LangUage-Vision modeling of UIs from Machine  Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yue Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Schoop%2C+E">Eldon Schoop</a>, 
<a href="/search/cs?searchtype=author&query=Swearngin%2C+A">Amanda Swearngin</a>, 
<a href="/search/cs?searchtype=author&query=Nichols%2C+J">Jeffrey Nichols</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Multimodal Vision-Language Models (VLMs) enable powerful applications from
their fused understanding of images and language, but many perform poorly on UI
tasks due to the lack of UI training data. In this paper, we adapt a recipe for
generating paired text-image training data for VLMs to the UI domain by
combining existing pixel-based methods with a Large Language Model (LLM).
Unlike prior art, our method requires no human-provided annotations, and it can
be applied to any dataset of UI screenshots. We generate a dataset of 335K
conversational examples paired with UIs that cover Q&amp;A, UI descriptions, and
planning, and use it to fine-tune a conversational VLM for UI tasks. To assess
the performance of our model, we benchmark it on UI element detection tasks,
evaluate response quality, and showcase its applicability to multi-step UI
navigation and planning.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04870" title="Abstract">arXiv:2310.04870</a> [<a href="/pdf/2310.04870" title="Download PDF">pdf</a>, <a href="/format/2310.04870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lemur: Integrating Large Language Models in Automated Program  Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haoze Wu</a>, 
<a href="/search/cs?searchtype=author&query=Barrett%2C+C">Clark Barrett</a>, 
<a href="/search/cs?searchtype=author&query=Narodytska%2C+N">Nina Narodytska</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">The demonstrated code-understanding capability of LLMs raises the question of
whether they can be used for automated program verification, a task that
typically demands high-level abstract reasoning about program properties that
is challenging for verification tools. We propose a general methodology to
combine the power of LLMs and automated reasoners for automated program
verification. We formally describe this methodology as a set of derivation
rules and prove its soundness. We instantiate the calculus as a sound automated
verification procedure, which led to practical improvements on a set of
synthetic and competition benchmarks.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04874" title="Abstract">arXiv:2310.04874</a> [<a href="/pdf/2310.04874" title="Download PDF">pdf</a>, <a href="/format/2310.04874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AirIMU: Learning Uncertainty Propagation for Inertial Odometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yuheng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xunfei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Youjie Xia</a>, 
<a href="/search/cs?searchtype=author&query=Scherer%2C+S">Sebastian Scherer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Accurate uncertainty estimation for inertial odometry is the foundation to
achieve optimal fusion in multi-sensor systems, such as visual or LiDAR
inertial odometry. Prior studies often simplify the assumptions regarding the
uncertainty of inertial measurements, presuming fixed covariance parameters and
empirical IMU sensor models. However, the inherent physical limitations and
non-linear characteristics of sensors are difficult to capture. Moreover,
uncertainty may fluctuate based on sensor rates and motion modalities, leading
to variations across different IMUs. To address these challenges, we formulate
a learning-based method that not only encapsulate the non-linearities inherent
to IMUs but also ensure the accurate propagation of covariance in a data-driven
manner. We extend the PyPose library to enable differentiable batched IMU
integration with covariance propagation on manifolds, leading to significant
runtime speedup. To demonstrate our method's adaptability, we evaluate it on
several benchmarks as well as a large-scale helicopter dataset spanning over
262 kilometers. The drift rate of the inertial odometry on these datasets is
reduced by a factor of between 2.2 and 4 times. Our method lays the groundwork
for advanced developments in inertial odometry.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04875" title="Abstract">arXiv:2310.04875</a> [<a href="/pdf/2310.04875" title="Download PDF">pdf</a>, <a href="/format/2310.04875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt-to-OS (P2OS): Revolutionizing Operating Systems and  Human-Computer Interaction with Integrated AI Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tolomei%2C+G">Gabriele Tolomei</a>, 
<a href="/search/cs?searchtype=author&query=Campagnano%2C+C">Cesare Campagnano</a>, 
<a href="/search/cs?searchtype=author&query=Silvestri%2C+F">Fabrizio Silvestri</a>, 
<a href="/search/cs?searchtype=author&query=Trappolini%2C+G">Giovanni Trappolini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure. Accepted at IEEE CogMI 2023 (IEEE International Conference on Cognitive Machine Intelligence)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Operating Systems (cs.OS)

</div>
<p class="mathjax">In this paper, we present a groundbreaking paradigm for human-computer
interaction that revolutionizes the traditional notion of an operating system.
<br />Within this innovative framework, user requests issued to the machine are
handled by an interconnected ecosystem of generative AI models that seamlessly
integrate with or even replace traditional software applications. At the core
of this paradigm shift are large generative models, such as language and
diffusion models, which serve as the central interface between users and
computers. This pioneering approach leverages the abilities of advanced
language models, empowering users to engage in natural language conversations
with their computing devices. Users can articulate their intentions, tasks, and
inquiries directly to the system, eliminating the need for explicit commands or
complex navigation. The language model comprehends and interprets the user's
prompts, generating and displaying contextual and meaningful responses that
facilitate seamless and intuitive interactions.
<br />This paradigm shift not only streamlines user interactions but also opens up
new possibilities for personalized experiences. Generative models can adapt to
individual preferences, learning from user input and continuously improving
their understanding and response generation. Furthermore, it enables enhanced
accessibility, as users can interact with the system using speech or text,
accommodating diverse communication preferences.
<br />However, this visionary concept raises significant challenges, including
privacy, security, trustability, and the ethical use of generative models.
Robust safeguards must be in place to protect user data and prevent potential
misuse or manipulation of the language model.
<br />While the full realization of this paradigm is still far from being achieved,
this paper serves as a starting point for envisioning this transformative
potential.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04878" title="Abstract">arXiv:2310.04878</a> [<a href="/pdf/2310.04878" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Recommendation System using Graph Neural Network and BERT  Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Javaji%2C+S+R">Shashidhar Reddy Javaji</a>, 
<a href="/search/cs?searchtype=author&query=Sarode%2C+K">Krutika Sarode</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recommender systems have emerged as a crucial component of the modern web
ecosystem. The effectiveness and accuracy of such systems are critical for
providing users with personalized recommendations that meet their specific
interests and needs. In this paper, we introduce a novel model that utilizes a
Graph Neural Network (GNN) in conjunction with sentence transformer embeddings
to predict anime recommendations for different users. Our model employs the
task of link prediction to create a recommendation system that considers both
the features of anime and user interactions with different anime. The
hybridization of the GNN and transformer embeddings enables us to capture both
inter-level and intra-level features of anime data.Our model not only
recommends anime to users but also predicts the rating a specific user would
give to an anime. We utilize the GraphSAGE network for model building and
weighted root mean square error (RMSE) to evaluate the performance of the
model. Our approach has the potential to significantly enhance the accuracy and
effectiveness of anime recommendation systems and can be extended to other
domains that require personalized recommendations.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04880" title="Abstract">arXiv:2310.04880</a> [<a href="/pdf/2310.04880" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Question-focused Summarization by Decomposing Articles into Facts and  Opinions and Retrieving Entities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarode%2C+K">Krutika Sarode</a>, 
<a href="/search/cs?searchtype=author&query=Javaji%2C+S+R">Shashidhar Reddy Javaji</a>, 
<a href="/search/cs?searchtype=author&query=Kalakonnavar%2C+V">Vishal Kalakonnavar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This research focuses on utilizing natural language processing techniques to
predict stock price fluctuations, with a specific interest in early detection
of economic, political, social, and technological changes that can be leveraged
for capturing market opportunities. The proposed approach includes the
identification of salient facts and events from news articles, then use these
facts to form tuples with entities which can be used to get summaries of market
changes for particular entity and then finally combining all the summaries to
form a final abstract summary of the whole article. The research aims to
establish relationships between companies and entities through the analysis of
Wikipedia data and articles from the Economist. Large Language Model GPT 3.5 is
used for getting the summaries and also forming the final summary. The ultimate
goal of this research is to develop a comprehensive system that can provide
financial analysts and investors with more informed decision-making tools by
enabling early detection of market trends and events.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04881" title="Abstract">arXiv:2310.04881</a> [<a href="/pdf/2310.04881" title="Download PDF">pdf</a>, <a href="/format/2310.04881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phasor Noise for Dehomogenisation in 2D Multiscale Topology Optimisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Woldseth%2C+R+V">Rebekka V. Woldseth</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A6rentzen%2C+J+A">J. Andreas B&#xe6;rentzen</a>, 
<a href="/search/cs?searchtype=author&query=Sigmund%2C+O">Ole Sigmund</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In proceedings for publication in Computer Methods in Applied Mechanics and Engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">This paper presents an alternative approach to dehomogenisation of elastic
Rank-N laminate structures based on the computer graphics discipline of phasor
noise. The proposed methodology offers an improvement of existing methods,
where high-quality single-scale designs can be obtained efficiently without the
utilisation of any least-squares problem or pre-trained models. By utilising a
continuous and periodic representation of the translation at each intermediate
step, appropriate length-scale and thicknesses can be obtained. Numerical tests
verifies the performance of the proposed methodology compared to
state-of-the-art alternatives, and the dehomogenised designs achieve structural
performance within a few percentages of the optimised homogenised solution. The
nature of the phasor-based dehomogenisation is inherently mesh-independent and
highly parallelisable, allowing for further efficient implementations and
future extensions to 3D problems on unstructured meshes.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04884" title="Abstract">arXiv:2310.04884</a> [<a href="/pdf/2310.04884" title="Download PDF">pdf</a>, <a href="/ps/2310.04884" title="Download PostScript">ps</a>, <a href="/format/2310.04884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regret Analysis of Repeated Delegated Choice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hajiaghayi%2C+M">MohammadTaghi Hajiaghayi</a>, 
<a href="/search/cs?searchtype=author&query=Mahdavi%2C+M">Mohammad Mahdavi</a>, 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+K">Keivan Rezaei</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+S">Suho Shin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a study on a repeated delegated choice problem, which is the first
to consider an online learning variant of Kleinberg and Kleinberg, EC'18. In
this model, a principal interacts repeatedly with an agent who possesses an
exogenous set of solutions to search for efficient ones. Each solution can
yield varying utility for both the principal and the agent, and the agent may
propose a solution to maximize its own utility in a selfish manner. To mitigate
this behavior, the principal announces an eligible set which screens out a
certain set of solutions. The principal, however, does not have any information
on the distribution of solutions in advance. Therefore, the principal
dynamically announces various eligible sets to efficiently learn the
distribution. The principal's objective is to minimize cumulative regret
compared to the optimal eligible set in hindsight. We explore two dimensions of
the problem setup, whether the agent behaves myopically or strategizes across
the rounds, and whether the solutions yield deterministic or stochastic
utility. Our analysis mainly characterizes some regimes under which the
principal can recover the sublinear regret, thereby shedding light on the rise
and fall of the repeated delegation procedure in various regimes.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04886" title="Abstract">arXiv:2310.04886</a> [<a href="/pdf/2310.04886" title="Download PDF">pdf</a>, <a href="/format/2310.04886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Closed-form Solution for the Strapdown Inertial Navigation Initial  Value Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Goppert%2C+J">James Goppert</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+L">Li-Yu Lin</a>, 
<a href="/search/eess?searchtype=author&query=Pant%2C+K">Kartik Pant</a>, 
<a href="/search/eess?searchtype=author&query=Perseghetti%2C+B">Benjamin Perseghetti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Strapdown inertial navigation systems (SINS) are ubiquitious in robotics and
engineering since they can estimate a rigid body pose using onboard kinematic
measurements without knowledge of the dynamics of the vehicle to which they are
attached. While recent work has focused on the closed-form evolution of the
estimation error for SINS, which is critical for Kalman filtering, the
propagation of the kinematics has received less attention. Runge-Kutta
integration approaches have been widely used to solve the initial value
problem; however, we show that leveraging the special structure of the SINS
problem and viewing it as a mixed-invariant vector field on a Lie group, yields
a closed form solution. Our closed form solution is exact given fixed gyroscope
and accelerometer measurements over a sampling period, and it is utilizes 12
times less floating point operations compared to a single integration step of a
4th order Runge-Kutta integrator. We believe the wide applicability of this
work and the efficiency and accuracy gains warrant general adoption of this
algorithm for SINS.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04889" title="Abstract">arXiv:2310.04889</a> [<a href="/pdf/2310.04889" title="Download PDF">pdf</a>, <a href="/format/2310.04889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GradXKG: A Universal Explain-per-use Temporal Knowledge Graph Explainer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chenhan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Eldardiry%2C+H">Hoda Eldardiry</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Temporal knowledge graphs (TKGs) have shown promise for reasoning tasks by
incorporating a temporal dimension to represent how facts evolve over time.
However, existing TKG reasoning (TKGR) models lack explainability due to their
black-box nature. Recent work has attempted to address this through customized
model architectures that generate reasoning paths, but these recent approaches
have limited generalizability and provide sparse explanatory output. To enable
interpretability for most TKGR models, we propose GradXKG, a novel two-stage
gradient-based approach for explaining Relational Graph Convolution Network
(RGCN)-based TKGR models. First, a Grad-CAM-inspired RGCN explainer tracks
gradients to quantify each node's contribution across timesteps in an efficient
"explain-per-use" fashion. Second, an integrated gradients explainer
consolidates importance scores for RGCN outputs, extending compatibility across
diverse TKGR architectures based on RGCN. Together, the two explainers
highlight the most critical nodes at each timestep for a given prediction. Our
extensive experiments demonstrated that, by leveraging gradient information,
GradXKG provides insightful explanations grounded in the model's logic in a
timely manner for most RGCN-based TKGR models. This helps address the lack of
interpretability in existing TKGR models and provides a universal explanation
approach applicable across various models.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04892" title="Abstract">arXiv:2310.04892</a> [<a href="/pdf/2310.04892" title="Download PDF">pdf</a>, <a href="/format/2310.04892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Commercialized Generative AI: A Critical Study of the Feasibility and  Ethics of Generating Native Advertising Using Large Language Models in  Conversational Web Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zelch%2C+I">Ines Zelch</a>, 
<a href="/search/cs?searchtype=author&query=Hagen%2C+M">Matthias Hagen</a>, 
<a href="/search/cs?searchtype=author&query=Potthast%2C+M">Martin Potthast</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at OSSYM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">How will generative AI pay for itself? Unless charging users for access,
selling advertising is the only alternative. Especially in the multi-billion
dollar web search market with ads as the main source of revenue, the
introduction of a subscription model seems unlikely. The recent disruption of
search by generative large language models could thus ultimately be accompanied
by generated ads. Our concern is that the commercialization of generative AI in
general and large language models in particular could lead to native
advertising in the form of quite subtle brand or product placements. In web
search, the evolution of search engine results pages (SERPs) from traditional
lists of ``ten blue links'' (lists SERPs) to generated text with web page
references (text SERPs) may further blur the line between advertising-based and
organic search results, making it difficult for users to distinguish between
the two, depending on how advertising is integrated and disclosed. To raise
awareness of this potential development, we conduct a pilot study analyzing the
capabilities of current large language models to blend ads with organic search
results. Although the models still struggle to subtly frame ads in an unrelated
context, their potential is evident when integrating ads into related topics
which calls for further investigation.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04893" title="Abstract">arXiv:2310.04893</a> [<a href="/pdf/2310.04893" title="Download PDF">pdf</a>, <a href="/ps/2310.04893" title="Download PostScript">ps</a>, <a href="/format/2310.04893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Densest Subgraph in Multiplex Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Behrouz%2C+A">Ali Behrouz</a>, 
<a href="/search/cs?searchtype=author&query=Hashemi%2C+F">Farnoosh Hashemi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Finding dense subgraphs of a large network is a fundamental problem in graph
mining that has been studied extensively both for its theoretical richness and
its many practical applications over the last five decades. However, most
existing studies have focused on graphs with a single type of connection. In
applications such as biological, social, and transportation networks,
interactions between objects span multiple aspects, yielding multiplex graphs.
Existing dense subgraph mining methods in multiplex graphs consider the same
importance for different types of connections, while in real-world
applications, one relation type can be noisy, insignificant, or irrelevant.
Moreover, they are limited to the edge-density measure, unable to change the
emphasis on larger/smaller degrees depending on the application. To this end,
we define a new family of dense subgraph objectives, parametrized by two
variables $p$ and $\beta$, that can (1) consider different importance weights
for each relation type, and (2) change the emphasis on the larger/smaller
degrees, depending on the application. Due to the NP-hardness of this problem,
we first extend the FirmCore, $k$-core counterpart in multiplex graphs, to
layer-weighted multiplex graphs, and based on it, we propose two
polynomial-time approximation algorithms for the generalized densest subgraph
problem, when $p \geq 1$ and the general case. Our experimental results show
the importance of considering different weights for different relation types
and the effectiveness and efficiency of our algorithms.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04894" title="Abstract">arXiv:2310.04894</a> [<a href="/pdf/2310.04894" title="Download PDF">pdf</a>, <a href="/format/2310.04894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Spacecraft Predictive Sensor Tasking for Cislunar Space  Situational Awareness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tomita%2C+K">Kento Tomita</a>, 
<a href="/search/eess?searchtype=author&query=Shimane%2C+Y">Yuri Shimane</a>, 
<a href="/search/eess?searchtype=author&query=Ho%2C+K">Koki Ho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper delves into the predictive sensor tasking algorithm for the
multi-observer, multi-target sensor setting, leveraging the Extended
Information Filter (EIF). Conventional predictive formulations suffer from the
curse of dimensionality due to the dependence of the performance metric on the
target-observer assignment history. This paper exploits the EIF's additive
structure of measurement information to break the dependence and devises an
efficient linear integer programming formulation. We further investigate the
resulting formulation to study how the cislunar dynamics expands and shrinks
the measurement information, and discuss when the information gain is maximized
in relation to the observation space and the uncertainty deformation caused by
the dynamics. We numerically demonstrate that the predictive sensor tasking
algorithm outperforms the myopic algorithm in two different metrics, depending
on the formulation.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04895" title="Abstract">arXiv:2310.04895</a> [<a href="/pdf/2310.04895" title="Download PDF">pdf</a>, <a href="/format/2310.04895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cell Tracking-by-detection using Elliptical Bounding Boxes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kirten%2C+L+N">Lucas N. Kirten</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+C+R">Cl&#xe1;udio R. Jung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper under review on IEEE/ACM Transactions on Computational Biology and Bioinformatics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Cell detection and tracking are paramount for bio-analysis. Recent approaches
rely on the tracking-by-model evolution paradigm, which usually consists of
training end-to-end deep learning models to detect and track the cells on the
frames with promising results. However, such methods require extensive amounts
of annotated data, which is time-consuming to obtain and often requires
specialized annotators. This work proposes a new approach based on the
classical tracking-by-detection paradigm that alleviates the requirement of
annotated data. More precisely, it approximates the cell shapes as oriented
ellipses and then uses generic-purpose oriented object detectors to identify
the cells in each frame. We then rely on a global data association algorithm
that explores temporal cell similarity using probability distance metrics,
considering that the ellipses relate to two-dimensional Gaussian distributions.
Our results show that our method can achieve detection and tracking results
competitively with state-of-the-art techniques that require considerably more
extensive data annotation. Our code is available at:
https://github.com/LucasKirsten/Deep-Cell-Tracking-EBB.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04896" title="Abstract">arXiv:2310.04896</a> [<a href="/pdf/2310.04896" title="Download PDF">pdf</a>, <a href="/format/2310.04896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guaranteeing Anonymity in Attribute-Based Authorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lanus%2C+E">Erin Lanus</a>, 
<a href="/search/cs?searchtype=author&query=Colbourn%2C+C+J">Charles J. Colbourn</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+G">Gail-Joon Ahn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Attribute-based methods, such as attribute-based access control and
attribute-based encryption, make decisions based on attributes possessed by a
subject rather than the subject's identity. While this allows for anonymous
authorization -- determining that a subject is authorized without knowing the
identity of the subject -- it does not guarantee anonymity. If a policy can be
composed such that few subjects possess attributes satisfying the policy, then
when the policy is used for access control, in addition to making a grant or
deny decision, the system can also guess with high probability the identity of
the subject making the request. Other approaches to achieving anonymity in
attribute-based authorization do not address this attribute distribution
problem. Suppose polices contain conjunctions of at most $t$ attributes and the
system must not be able to guess with probability greater than $\frac{1}{r}$
the identity of a subject using a policy for authorization. We say the
anonymity guarantee is $r$ for maximum credential size $t$. An anonymizing
array is a combinatorial array proposed as an abstraction to address the
underlying attribute distribution problem by ensuring that any assignment of
values to $t$ attributes appearing in the array appears at least $r$ times.
Anonymizing arrays are related to covering arrays with higher coverage, but
have an additional desired property, homogeneity, due to their application
domain. In this work, we discuss the application of anonymizing arrays to
guarantee anonymous authorization in attribute-based methods. Additionally, we
develop metrics, local and global homogeneity, to compare anonymizing arrays
with the same parameters.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04897" title="Abstract">arXiv:2310.04897</a> [<a href="/pdf/2310.04897" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI May Prefer to Present National-level Characteristics of  Cities Based on Stereotypical Geographic Impressions at the Continental Level
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+S">Shan Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A simple experiment was conducted to test the ability of the Chinese-based
generative artificial intelligence (AI) platform, Wenxin Yige, to render images
of urban street views of different countries. The study found that images
generated by this AI platform may contain continental-level stereotypes in
terms of showing the level of economic development and modernization. Street
view images generated from Wenxin Yige do not adequately represent the diverse
range of urban landscapes found across different nations. Using these generated
images for geography education or outreach initiatives could inadvertently
strengthen people's existing stereotypical views about individual countries.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04898" title="Abstract">arXiv:2310.04898</a> [<a href="/pdf/2310.04898" title="Download PDF">pdf</a>, <a href="/format/2310.04898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Multi-domain Trust Infrastructures for Segmented Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grierson%2C+S">Sam Grierson</a>, 
<a href="/search/cs?searchtype=author&query=Buchanan%2C+W+J">William J Buchanan</a>, 
<a href="/search/cs?searchtype=author&query=Thomson%2C+C">Craig Thomson</a>, 
<a href="/search/cs?searchtype=author&query=Ghaleb%2C+B">Baraq Ghaleb</a>, 
<a href="/search/cs?searchtype=author&query=Maglaras%2C+L">Leandros Maglaras</a>, 
<a href="/search/cs?searchtype=author&query=Eckle%2C+C">Chris Eckle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Within a trust infrastructure, a private key is often used to digitally sign
a transaction, which can be verified with an associated public key. Using PKI
(Public Key Infrastructure), a trusted entity can produce a digital signature,
verifying the authenticity of the public key. However, what happens when
external entities are not trusted to verify the public key or in cases where
there is no Internet connection within an isolated or autonomously acting
collection of devices? For this, a trusted entity can be elected to generate a
key pair and then split the private key amongst trusted devices. Each node can
then sign part of the transaction using their split of the shared secret. The
aggregated signature can then define agreement on a consensus within the
infrastructure. Unfortunately, this process has two significant problems. The
first is when no trusted node can act as a dealer of the shares. The second is
the difficulty of scaling the digital signature scheme. This paper outlines a
method of creating a leaderless approach to defining trust domains to overcome
weaknesses in the scaling of the elliptic curve digital signature algorithm.
Instead, it proposes the usage of the Edwards curve digital signature algorithm
for the definition of multiple trust zones. The paper shows that the
computational overhead of the distributed key generation phase increases with
the number of nodes in the trust domain but that the distributed signing has a
relatively constant computational overhead.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04899" title="Abstract">arXiv:2310.04899</a> [<a href="/pdf/2310.04899" title="Download PDF">pdf</a>, <a href="/format/2310.04899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixing Solutions in Bitcoin and Ethereum Ecosystems: A Review and  Tutorial
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arbabi%2C+A">Alireza Arbabi</a>, 
<a href="/search/cs?searchtype=author&query=Shojaeinasab%2C+A">Ardeshir Shojaeinasab</a>, 
<a href="/search/cs?searchtype=author&query=Bahrak%2C+B">Behnam Bahrak</a>, 
<a href="/search/cs?searchtype=author&query=Najjaran%2C+H">Homayoun Najjaran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Alireza Arbabi and Ardeshir Shojaeinasab contributed equally to this paper. Homayoun Najjaran is the corresponding author
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">This manuscript presents an exhaustive review of blockchain-based mixing
services, aiming to fill the existing gap between academic innovations and
real-world implementations. Starting with an identification of the core
functionalities and techniques employed by mixing services, the paper delves
into detailed explanations of these operational mechanisms. It further outlines
an evaluation framework tailored for a rigorous assessment, highlighting the
key vulnerabilities and strengths of various solutions. In addition, the study
identifies potential attack vectors that compromise these services. The paper
explores the dual nature of mixing services, while they contribute to the
preservation of privacy, a cornerstone of blockchain technologies, they can
also facilitate illicit activities. By addressing key research questions, this
study not only offers a comprehensive overview of the current state of mixing
services but also sets the stage for future academic discourse in this evolving
field.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04900" title="Abstract">arXiv:2310.04900</a> [<a href="/pdf/2310.04900" title="Download PDF">pdf</a>, <a href="/format/2310.04900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HowToCaption: Prompting LLMs to Transform Video Annotations at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shvetsova%2C+N">Nina Shvetsova</a>, 
<a href="/search/cs?searchtype=author&query=Kukleva%2C+A">Anna Kukleva</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+X">Xudong Hong</a>, 
<a href="/search/cs?searchtype=author&query=Rupprecht%2C+C">Christian Rupprecht</a>, 
<a href="/search/cs?searchtype=author&query=Schiele%2C+B">Bernt Schiele</a>, 
<a href="/search/cs?searchtype=author&query=Kuehne%2C+H">Hilde Kuehne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://github.com/ninatu/howtocaption">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Instructional videos are an excellent source for learning multimodal
representations by leveraging video-subtitle pairs extracted with automatic
speech recognition systems (ASR) from the audio signal in the videos. However,
in contrast to human-annotated captions, both speech and subtitles naturally
differ from the visual content of the videos and thus provide only noisy
supervision for multimodal learning. As a result, large-scale annotation-free
web video training data remains sub-optimal for training text-video models. In
this work, we propose to leverage the capability of large language models
(LLMs) to obtain fine-grained video descriptions aligned with videos.
Specifically, we prompt an LLM to create plausible video descriptions based on
ASR narrations of the video for a large-scale instructional video dataset. To
this end, we introduce a prompting method that is able to take into account a
longer text of subtitles, allowing us to capture context beyond a single
sentence. To align the captions to the video temporally, we prompt the LLM to
generate timestamps for each produced caption based on the subtitles. In this
way, we obtain human-style video captions at scale without human supervision.
We apply our method to the subtitles of the HowTo100M dataset, creating a new
large-scale dataset, HowToCaption. Our evaluation shows that the resulting
captions not only significantly improve the performance over many different
benchmark datasets for text-video retrieval but also lead to a disentangling of
textual narration from the audio, boosting performance in text-video-audio
tasks.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04901" title="Abstract">arXiv:2310.04901</a> [<a href="/pdf/2310.04901" title="Download PDF">pdf</a>, <a href="/format/2310.04901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WAIT: Feature Warping for Animation to Illustration video Translation  using GANs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hicsonmez%2C+S">Samet Hicsonmez</a>, 
<a href="/search/cs?searchtype=author&query=Samet%2C+N">Nermin Samet</a>, 
<a href="/search/cs?searchtype=author&query=Samet%2C+F">Fidan Samet</a>, 
<a href="/search/cs?searchtype=author&query=Bakir%2C+O">Oguz Bakir</a>, 
<a href="/search/cs?searchtype=author&query=Akbas%2C+E">Emre Akbas</a>, 
<a href="/search/cs?searchtype=author&query=Duygulu%2C+P">Pinar Duygulu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we explore a new domain for video-to-video translation.
Motivated by the availability of animation movies that are adopted from
illustrated books for children, we aim to stylize these videos with the style
of the original illustrations. Current state-of-the-art video-to-video
translation models rely on having a video sequence or a single style image to
stylize an input video. We introduce a new problem for video stylizing where an
unordered set of images are used. This is a challenging task for two reasons:
i) we do not have the advantage of temporal consistency as in video sequences;
ii) it is more difficult to obtain consistent styles for video frames from a
set of unordered images compared to using a single image.
<br />Most of the video-to-video translation methods are built on an image-to-image
translation model, and integrate additional networks such as optical flow, or
temporal predictors to capture temporal relations. These additional networks
make the model training and inference complicated and slow down the process. To
ensure temporal coherency in video-to-video style transfer, we propose a new
generator network with feature warping layers which overcomes the limitations
of the previous methods. We show the effectiveness of our method on three
datasets both qualitatively and quantitatively. Code and pretrained models are
available at https://github.com/giddyyupp/wait.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04910" title="Abstract">arXiv:2310.04910</a> [<a href="/pdf/2310.04910" title="Download PDF">pdf</a>, <a href="/format/2310.04910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faithful Knowledge Graph Explanations for Commonsense Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+W">Weihe Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Zubiaga%2C+A">Arkaitz Zubiaga</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bingquan Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While fusing language models (LMs) and knowledge graphs (KGs) has become
common in commonsense question answering research, enabling faithful
chain-of-thought explanations in these models remains an open problem. One
major weakness of current KG-based explanation techniques is that they overlook
the faithfulness of generated explanations during evaluation. To address this
gap, we make two main contributions: (1) We propose and validate two
quantitative metrics - graph consistency and graph fidelity - to measure the
faithfulness of KG-based explanations. (2) We introduce Consistent GNN (CGNN),
a novel training method that adds a consistency regularization term to improve
explanation faithfulness. Our analysis shows that predictions from KG often
diverge from original model predictions. The proposed CGNN approach boosts
consistency and fidelity, demonstrating its potential for producing more
faithful explanations. Our work emphasises the importance of explicitly
evaluating suggest a path forward for developing architectures for faithful
graph-based explanations.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04911" title="Abstract">arXiv:2310.04911</a> [<a href="/pdf/2310.04911" title="Download PDF">pdf</a>, <a href="/ps/2310.04911" title="Download PostScript">ps</a>, <a href="/format/2310.04911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interference Networks with Random User Activity and Heterogeneous Delay  Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nikbakht%2C+H">Homa Nikbakht</a>, 
<a href="/search/cs?searchtype=author&query=Wigger%2C+M">Mich&#xe8;le Wigger</a>, 
<a href="/search/cs?searchtype=author&query=Shamai%2C+S">Shlomo Shamai</a> (Shitz), 
<a href="/search/cs?searchtype=author&query=Gorce%2C+J">Jean-Marie Gorce</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">To answer the call for a new theoretical framework to simultaneously
accommodate random user activity and heterogeneous delay traffic in Internet of
Things (IoT) systems, in this paper we propose coding schemes and
information-theoretic converse results for the transmission of heterogeneous
delay traffic over interference networks with random user activity and random
data arrivals. The heterogeneous traffic is composed of delay-tolerant traffic
and delay-sensitive traffic where only the former can benefit from transmitter
and receiver cooperation since the latter is subject to stringent decoding
delays. The total number of cooperation rounds at transmitter and receiver
sides is limited to $\D$ rounds. Each transmitter is active with probability
$\rho \in [0,1]$. We consider two different models for the arrival of the
mixed-delay traffic: in Model~$1$, each active transmitter sends a
delay-tolerant message, and with probability $\rho_f \in [0,1]$ also transmits
an additional delay-sensitive message; in Model~$2$, each active transmitter
sends either a delay-sensitive message with probability $\rho_f$ or a
delay-tolerant message with probability $1-\rho_f$. We derive inner and outer
bounds on the fundamental per-user multiplexing gain (MG) region of the
symmetric Wyner network as well as inner bounds on the fundamental MG region of
the hexagonal model. Our inner and outer bounds are generally very close and
coincide in special cases. They also show that when both transmitters and
receivers can cooperate, then under Model~$1$, transmitting delay-sensitive
messages hardly causes any penalty on the sum per-user MG, and under Model~$2$,
operating at large delay-sensitive per-user MGs incurs no penalty on the
delay-tolerant per-user MG and thus increases the sum per-user MG.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04912" title="Abstract">arXiv:2310.04912</a> [<a href="/pdf/2310.04912" title="Download PDF">pdf</a>, <a href="/format/2310.04912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $H$-RANSAC, an algorithmic variant for Homography image transform from  featureless point sets: application to video-based football analytics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nousias%2C+G">George Nousias</a>, 
<a href="/search/cs?searchtype=author&query=Delibasis%2C+K">Konstantinos Delibasis</a>, 
<a href="/search/cs?searchtype=author&query=Maglogiannis%2C+I">Ilias Maglogiannis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Estimating homography matrix between two images has various applications like
image stitching or image mosaicing and spatial information retrieval from
multiple camera views, but has been proved to be a complicated problem,
especially in cases of radically different camera poses and zoom factors. Many
relevant approaches have been proposed, utilizing direct feature based, or deep
learning methodologies. In this paper, we propose a generalized RANSAC
algorithm, H-RANSAC, to retrieve homography image transformations from sets of
points without descriptive local feature vectors and point pairing. We allow
the points to be optionally labelled in two classes. We propose a robust
criterion that rejects implausible point selection before each iteration of
RANSAC, based on the type of the quadrilaterals formed by random point pair
selection (convex or concave and (non)-self-intersecting). A similar post-hoc
criterion rejects implausible homography transformations is included at the end
of each iteration. The expected maximum iterations of $H$-RANSAC are derived
for different probabilities of success, according to the number of points per
image and per class, and the percentage of outliers. The proposed methodology
is tested on a large dataset of images acquired by 12 cameras during real
football matches, where radically different views at each timestamp are to be
matched. Comparisons with state-of-the-art implementations of RANSAC combined
with classic and deep learning image salient point detection indicates the
superiority of the proposed $H$-RANSAC, in terms of average reprojection error
and number of successfully processed pairs of frames, rendering it the method
of choice in cases of image homography alignment with few tens of points, while
local features are not available, or not descriptive enough. The implementation
of $H$-RANSAC is available in https://github.com/gnousias/H-RANSAC
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04914" title="Abstract">arXiv:2310.04914</a> [<a href="/pdf/2310.04914" title="Download PDF">pdf</a>, <a href="/format/2310.04914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Zero-Shot Abilities of Vision-Language Models on Video  Understanding Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madasu%2C+A">Avinash Madasu</a>, 
<a href="/search/cs?searchtype=author&query=Bhiwandiwalla%2C+A">Anahita Bhiwandiwalla</a>, 
<a href="/search/cs?searchtype=author&query=Lal%2C+V">Vasudev Lal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Foundational multimodal models pre-trained on large scale image-text pairs or
video-text pairs or both have shown strong generalization abilities on
downstream tasks. However unlike image-text models, pretraining video-text
models is always not feasible due to the difficulty in collecting large-scale
clean and aligned data, and exponential computational costs involved in the
pretraining phase. Therefore, the pertinent question to ask is: Can image-text
models be adapted to video tasks and is there any benefit to using these models
over pretraining directly on videos? In this work, we focus on this question by
proposing a detailed study on the generalization abilities of image-text models
when evaluated on video understanding tasks in a zero-shot setting. We
investigate 9 foundational image-text models on a diverse set of video tasks
that include video action recognition (video AR), video retrieval (video RT),
video question answering (video QA), video multiple choice (video MC) and video
captioning (video CP). Our experiments show that image-text models exhibit
impressive performance on video AR, video RT and video MC. Furthermore, they
perform moderately on video captioning and poorly on video QA. These findings
shed a light on the benefits of adapting foundational image-text models to an
array of video tasks while avoiding the costly pretraining step.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04918" title="Abstract">arXiv:2310.04918</a> [<a href="/pdf/2310.04918" title="Download PDF">pdf</a>, <a href="/format/2310.04918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Network Pruning With Sparse Entropic Wasserstein Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+L">Lei You</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H+V">Hei Victor Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This study unveils a cutting-edge technique for neural network pruning that
judiciously addresses noisy gradients during the computation of the empirical
Fisher Information Matrix (FIM). We introduce an entropic Wasserstein
regression (EWR) formulation, capitalizing on the geometric attributes of the
optimal transport (OT) problem. This is analytically showcased to excel in
noise mitigation by adopting neighborhood interpolation across data points. The
unique strength of the Wasserstein distance is its intrinsic ability to strike
a balance between noise reduction and covariance information preservation.
Extensive experiments performed on various networks show comparable performance
of the proposed method with state-of-the-art (SoTA) network pruning algorithms.
Our proposed method outperforms the SoTA when the network size or the target
sparsity is large, the gain is even larger with the existence of noisy
gradients, possibly from noisy data, analog memory, or adversarial attacks.
Notably, our proposed method achieves a gain of 6% improvement in accuracy and
8% improvement in testing loss for MobileNetV1 with less than one-fourth of the
network parameters remaining.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04921" title="Abstract">arXiv:2310.04921</a> [<a href="/pdf/2310.04921" title="Download PDF">pdf</a>, <a href="/format/2310.04921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crystal: Introspective Reasoners Reinforced with Self-Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiacheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pasunuru%2C+R">Ramakanth Pasunuru</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Celikyilmaz%2C+A">Asli Celikyilmaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Extensive work has shown that the performance and interpretability of
commonsense reasoning can be improved via knowledge-augmented reasoning
methods, where the knowledge that underpins the reasoning process is explicitly
verbalized and utilized. However, existing implementations, including
"chain-of-thought" and its variants, fall short in capturing the introspective
nature of knowledge required in commonsense reasoning, and in accounting for
the mutual adaptation between the generation and utilization of knowledge. We
propose a novel method to develop an introspective commonsense reasoner,
Crystal. To tackle commonsense problems, it first introspects for knowledge
statements related to the given question, and subsequently makes an informed
prediction that is grounded in the previously introspected knowledge. The
knowledge introspection and knowledge-grounded reasoning modes of the model are
tuned via reinforcement learning to mutually adapt, where the reward derives
from the feedback given by the model itself. Experiments show that Crystal
significantly outperforms both the standard supervised finetuning and
chain-of-thought distilled methods, and enhances the transparency of the
commonsense reasoning process. Our work ultimately validates the feasibility
and potential of reinforcing a neural model with self-feedback.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04922" title="Abstract">arXiv:2310.04922</a> [<a href="/pdf/2310.04922" title="Download PDF">pdf</a>, <a href="/ps/2310.04922" title="Download PostScript">ps</a>, <a href="/format/2310.04922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Multivariate Detection and Estimation with Fault Frequency  Content Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dong%2C+J">Jingwei Dong</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+K">Kaikai Pan</a>, 
<a href="/search/eess?searchtype=author&query=Pequito%2C+S">Sergio Pequito</a>, 
<a href="/search/eess?searchtype=author&query=Esfahani%2C+P+M">Peyman Mohajerin Esfahani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper studies the problem of fault detection and estimation (FDE) for
LTI systems with a particular focus on frequency content information for the
faults, possibly as a continuum range, and under both disturbances and
stochastic noise. Considering the worst-case fault sensitivity in the frequency
range and the effects of disturbances and noise, we introduce a mixed
$\mathcal{H}_2/\mathcal{H}_{\_}$ performance index and develop an optimization
framework to compute the optimal detection filter. We further propose a
thresholding rule that provides guarantees on both false alarm rate (FAR) and
fault detection rate (FDR). Next, shifting our attention to the estimation
problem, we introduce the restricted $\mathcal{H}_{\infty}$ performance index
and obtain an exact reformulation of the optimal filter design. This problem is
inherently non-convex, however, focusing on finite frequency samples and fixed
poles, we then establish a lower bound via a highly tractable quadratic
programming (QP) problem. This lower bound together with an alternating
optimization approach to the original estimation problem leads to a
suboptimality gap for the overall filter design. The effectiveness of the
proposed approaches is validated through a synthetic non-minimum phase system
and an application of the multi-area power system.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04923" title="Abstract">arXiv:2310.04923</a> [<a href="/pdf/2310.04923" title="Download PDF">pdf</a>, <a href="/format/2310.04923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Optimal Unequal Error Protection LDPC Coded Recording System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chou%2C+H">Hong-fu Chou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">For efficient modulation and error control coding, the deliberate flipping
approach imposes the run-length-limited(RLL) constraint by bit error before
recording. From the read side, a high coding rate limits the correcting
capability of RLL bit error. In this paper, we study the low-density
parity-check (LDPC) coding for RLL constrained recording system based on the
Unequal Error Protection (UEP) coding scheme design. The UEP capability of
irregular LDPC codes is used for recovering flipped bits. We provide an
allocation technique to limit the occurrence of flipped bits on the bit with
robust correction capability. In addition, we consider the signal labeling
design to decrease the number of nearest neighbors to enhance the robust bit.
We also apply the density evolution technique to the proposed system for
evaluating the code performances. In addition, we utilize the EXIT
characteristic to reveal the decoding behavior of the recommended code
distribution. Finally, the optimization approach for the best distribution is
proven by differential evolution for the proposed system.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04925" title="Abstract">arXiv:2310.04925</a> [<a href="/pdf/2310.04925" title="Download PDF">pdf</a>, <a href="/format/2310.04925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crystal-GFN: sampling crystals with desirable properties and constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=AI4Science%2C+M">Mila AI4Science</a>, 
<a href="/search/cs?searchtype=author&query=Hernandez-Garcia%2C+A">Alex Hernandez-Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Duval%2C+A">Alexandre Duval</a>, 
<a href="/search/cs?searchtype=author&query=Volokhova%2C+A">Alexandra Volokhova</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+D">Divya Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Carrier%2C+P+L">Pierre Luc Carrier</a>, 
<a href="/search/cs?searchtype=author&query=Koziarski%2C+M">Micha&#x142; Koziarski</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+V">Victor Schmidt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main paper (9 pages) + references + appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Accelerating material discovery holds the potential to greatly help mitigate
the climate crisis. Discovering new solid-state crystals such as
electrocatalysts, ionic conductors or photovoltaics can have a crucial impact,
for instance, in improving the efficiency of renewable energy production and
storage. In this paper, we introduce Crystal-GFlowNet, a generative model of
crystal structures that sequentially samples a crystal's composition, space
group and lattice parameters. This domain-inspired approach enables the
flexible incorporation of physical and geometrical constraints, as well as the
use of any available predictive model of a desired property as an objective
function. We evaluate the capabilities of Crystal-GFlowNet by using as
objective the formation energy of a crystal structure, as predicted by a new
proxy model trained on MatBench. The results demonstrate that Crystal-GFlowNet
is able to sample diverse crystals with low formation energy.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04928" title="Abstract">arXiv:2310.04928</a> [<a href="/pdf/2310.04928" title="Download PDF">pdf</a>, <a href="/format/2310.04928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Only Pass Primary School Exams in Indonesia: A  Comprehensive Test on IndoMMLU
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koto%2C+F">Fajri Koto</a>, 
<a href="/search/cs?searchtype=author&query=Aisyah%2C+N">Nurul Aisyah</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haonan Li</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Timothy Baldwin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models have made significant advancements in natural language
processing (NLP), exhibiting human performance across various classic NLP
tasks. These tasks, however, focus on structure and semantics, and few are
designed to assess reasoning abilities and real-world knowledge, which are
increasingly vital given that these models are trained on extensive textual
data and information. While prior research primarily focuses on English, in
this work, we gather a collection of exam problems from primary school to
university entrance tests in Indonesia, and evaluate whether large language
models can pass the exams. We obtain 14,906 questions across 63 tasks and
levels, with 46\% of the questions focusing on assessing proficiency in the
Indonesian language and knowledge of nine local languages and cultures in
Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the
Indonesian primary school level, with limited knowledge of the Indonesian local
languages and cultures. Other smaller models such as BLOOMZ and Falcon fail the
exams.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04929" title="Abstract">arXiv:2310.04929</a> [<a href="/pdf/2310.04929" title="Download PDF">pdf</a>, <a href="/format/2310.04929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DISCOVER: Making Vision Networks Interpretable via Competition and  Dissection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panousis%2C+K+P">Konstantinos P. Panousis</a>, 
<a href="/search/cs?searchtype=author&query=Chatzis%2C+S">Sotirios Chatzis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted @ NeuIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Modern deep networks are highly complex and their inferential outcome very
hard to interpret. This is a serious obstacle to their transparent deployment
in safety-critical or bias-aware applications. This work contributes to
post-hoc interpretability, and specifically Network Dissection. Our goal is to
present a framework that makes it easier to discover the individual
functionality of each neuron in a network trained on a vision task; discovery
is performed in terms of textual description generation. To achieve this
objective, we leverage: (i) recent advances in multimodal vision-text models
and (ii) network layers founded upon the novel concept of stochastic local
competition between linear units. In this setting, only a small subset of layer
neurons are activated for a given input, leading to extremely high activation
sparsity (as low as only $\approx 4\%$). Crucially, our proposed method infers
(sparse) neuron activation patterns that enables the neurons to
activate/specialize to inputs with specific characteristics, diversifying their
individual functionality. This capacity of our method supercharges the
potential of dissection processes: human understandable descriptions are
generated only for the very few active neurons, thus facilitating the direct
investigation of the network's decision process. As we experimentally show, our
approach: (i) yields Vision Networks that retain or improve classification
performance, and (ii) realizes a principled framework for text-based
description and examination of the generated neuronal representations.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04930" title="Abstract">arXiv:2310.04930</a> [<a href="/pdf/2310.04930" title="Download PDF">pdf</a>, <a href="/format/2310.04930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diff-Transfer: Model-based Robotic Manipulation Skill Transfer via  Differentiable Physics Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yuqi Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feitong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qinsi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gang%2C+Y">Yang Gang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinghao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xingyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+L">Lin Shao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The capability to transfer mastered skills to accomplish a range of similar
yet novel tasks is crucial for intelligent robots. In this work, we introduce
$\textit{Diff-Transfer}$, a novel framework leveraging differentiable physics
simulation to efficiently transfer robotic skills. Specifically,
$\textit{Diff-Transfer}$ discovers a feasible path within the task space that
brings the source task to the target task. At each pair of adjacent points
along this task path, which is two sub-tasks, $\textit{Diff-Transfer}$ adapts
known actions from one sub-task to tackle the other sub-task successfully. The
adaptation is guided by the gradient information from differentiable physics
simulations. We propose a novel path-planning method to generate sub-tasks,
leveraging $Q$-learning with a task-level state and reward. We implement our
framework in simulation experiments and execute four challenging transfer tasks
on robotic manipulation, demonstrating the efficacy of $\textit{Diff-Transfer}$
through comprehensive experiments. Supplementary and Videos are on the website
$~\href{https://sites.google.com/view/difftransfer}{https://sites.google.com/view/difftransfer}$
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04933" title="Abstract">arXiv:2310.04933</a> [<a href="/pdf/2310.04933" title="Download PDF">pdf</a>, <a href="/format/2310.04933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithms for the Ridesharing with Profit Constraint Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Qian-Ping Gu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J+L">Jiajian Leo Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 6 figures, and 10 tables; to be appeared in COCOA 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Mobility-on-demand (MoD) ridesharing is a promising way to improve the
occupancy rate of personal vehicles and reduce traffic congestion and
emissions. Maximizing the number of passengers served and maximizing a profit
target are major optimization goals in MoD ridesharing. We study the
ridesharing with profit constraint problem (labeled as RPC) which considers
both optimization goals altogether: maximize the total number of passengers
subject to an overall drivers' profit target. We give a mathematical
formulation for the RPC problem. We present a polynomial-time exact algorithm
framework (including two practical implementations of the algorithm) and a
(1/2)-approximation algorithm for the case that each vehicle serves at most one
passenger. We propose a (2/3*lambda)-approximation algorithm for the case that
each vehicle serves at most lambda &gt;= 2 passengers. Our algorithms revolve
around the idea of maximum cardinality matching in bipartite graphs and
hypergraphs (set packing) with general edge weight. Based on a real-world
ridesharing dataset in Chicago City and price schemes of Uber, we conduct an
extensive empirical study on our model and algorithms. Experimental results
show that practical price schemes can be incorporated into our model, our exact
algorithms are efficient, and our approximation algorithms achieve about 90% of
optimal solutions, in the number of passengers served.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04935" title="Abstract">arXiv:2310.04935</a> [<a href="/pdf/2310.04935" title="Download PDF">pdf</a>, <a href="/format/2310.04935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Guarantees for Variational Autoencoders using PAC-Bayesian  Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mbacke%2C+S+D">Sokhna Diarra Mbacke</a>, 
<a href="/search/cs?searchtype=author&query=Clerc%2C+F">Florence Clerc</a>, 
<a href="/search/cs?searchtype=author&query=Germain%2C+P">Pascal Germain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Since their inception, Variational Autoencoders (VAEs) have become central in
machine learning. Despite their widespread use, numerous questions regarding
their theoretical properties remain open. Using PAC-Bayesian theory, this work
develops statistical guarantees for VAEs. First, we derive the first
PAC-Bayesian bound for posterior distributions conditioned on individual
samples from the data-generating distribution. Then, we utilize this result to
develop generalization guarantees for the VAE's reconstruction loss, as well as
upper bounds on the distance between the input and the regenerated
distributions. More importantly, we provide upper bounds on the Wasserstein
distance between the input distribution and the distribution defined by the
VAE's generative model.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04940" title="Abstract">arXiv:2310.04940</a> [<a href="/pdf/2310.04940" title="Download PDF">pdf</a>, <a href="/format/2310.04940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEE-MCAM: Scalable Multi-bit FeFET Content Addressable Memories for  Energy Efficient Associative Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shou%2C+S">Shengxi Shou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Che-Kai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Sanggeon Yun</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zishen Wan</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+K">Kai Ni</a>, 
<a href="/search/cs?searchtype=author&query=Imani%2C+M">Mohsen Imani</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X+S">X. Sharon Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+C">Cheng Zhuo</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+X">Xunzhao Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Internation Conference on Computer-Aided Design (ICCAD), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">In this work, we propose SEE-MCAM, scalable and compact multi-bit CAM (MCAM)
designs that utilize the three-terminal ferroelectric FET (FeFET) as the proxy.
By exploiting the multi-level-cell characteristics of FeFETs, our proposed
SEE-MCAM designs enable multi-bit associative search functions and achieve
better energy efficiency and performance than existing FeFET-based CAM designs.
We validated the functionality of our proposed designs by achieving 3 bits per
cell CAM functionality, resulting in 3x improvement in storage density. The
area per bit of the proposed SEE-MCAM cell is 8% of the conventional CMOS CAM.
We thoroughly investigated the scalability and robustness of the proposed
design. Evaluation results suggest that the proposed 2FeFET-1T SEE-MCAM
achieves 9.8x more energy efficiency and 1.6x less search latency compared to
the CMOS CAM, respectively. When compared to existing MCAM designs, the
proposed SEE-MCAM can achieve 8.7x and 4.9x more energy efficiency than
ReRAM-based and FeFET-based MCAMs, respectively. Benchmarking results show that
our approach provides up to 3 orders of magnitude improvement in speedup and
energy efficiency over a GPU implementation in accelerating a novel quantized
hyperdimensional computing (HDC) application.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04941" title="Abstract">arXiv:2310.04941</a> [<a href="/pdf/2310.04941" title="Download PDF">pdf</a>, <a href="/format/2310.04941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reliable Test-Time Adaptation via Agreement-on-the-Line
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+E">Eungyeup Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingjie Sun</a>, 
<a href="/search/cs?searchtype=author&query=Raghunathan%2C+A">Aditi Raghunathan</a>, 
<a href="/search/cs?searchtype=author&query=Kolter%2C+Z">Zico Kolter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Test-time adaptation (TTA) methods aim to improve robustness to distribution
shifts by adapting models using unlabeled data from the shifted test
distribution. However, there remain unresolved challenges that undermine the
reliability of TTA, which include difficulties in evaluating TTA performance,
miscalibration after TTA, and unreliable hyperparameter tuning for adaptation.
In this work, we make a notable and surprising observation that TTAed models
strongly show the agreement-on-the-line phenomenon (Baek et al., 2022) across a
wide range of distribution shifts. We find such linear trends occur
consistently in a wide range of models adapted with various hyperparameters,
and persist in distributions where the phenomenon fails to hold in vanilla
models (i.e., before adaptation). We leverage these observations to make TTA
methods more reliable in three perspectives: (i) estimating OOD accuracy
(without labeled data) to determine when TTA helps and when it hurts, (ii)
calibrating TTAed models without label information, and (iii) reliably
determining hyperparameters for TTA without any labeled validation data.
Through extensive experiments, we demonstrate that various TTA methods can be
precisely evaluated, both in terms of their improvements and degradations.
Moreover, our proposed methods on unsupervised calibration and hyperparameters
tuning for TTA achieve results close to the ones assuming access to
ground-truth labels, in terms of both OOD accuracy and calibration error.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04942" title="Abstract">arXiv:2310.04942</a> [<a href="/pdf/2310.04942" title="Download PDF">pdf</a>, <a href="/ps/2310.04942" title="Download PostScript">ps</a>, <a href="/format/2310.04942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Spatial Trajectory Patterns Mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Amiri%2C+H">Hossein Amiri</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenke Liu</a>, 
<a href="/search/cs?searchtype=author&query=Z%C3%BCfle%2C+A">Andreas Z&#xfc;fle</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Identifying anomalous human spatial trajectory patterns can indicate dynamic
changes in mobility behavior with applications in domains like infectious
disease monitoring and elderly care. Recent advancements in large language
models (LLMs) have demonstrated their ability to reason in a manner akin to
humans. This presents significant potential for analyzing temporal patterns in
human mobility. In this paper, we conduct empirical studies to assess the
capabilities of leading LLMs like GPT-4 and Claude-2 in detecting anomalous
behaviors from mobility data, by comparing to specialized methods. Our key
findings demonstrate that LLMs can attain reasonable anomaly detection
performance even without any specific cues. In addition, providing contextual
clues about potential irregularities could further enhances their prediction
efficacy. Moreover, LLMs can provide reasonable explanations for their
judgments, thereby improving transparency. Our work provides insights on the
strengths and limitations of LLMs for human spatial trajectory analysis.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04944" title="Abstract">arXiv:2310.04944</a> [<a href="/pdf/2310.04944" title="Download PDF">pdf</a>, <a href="/ps/2310.04944" title="Download PostScript">ps</a>, <a href="/format/2310.04944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Text: A Deep Dive into Large Language Models&#x27; Ability on  Understanding Graph Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yuntong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large language models (LLMs) have achieved impressive performance on many
natural language processing tasks. However, their capabilities on
graph-structured data remain relatively unexplored. In this paper, we conduct a
series of experiments benchmarking leading LLMs on diverse graph prediction
tasks spanning node, edge, and graph levels. We aim to assess whether LLMs can
effectively process graph data and leverage topological structures to enhance
performance, compared to specialized graph neural networks. Through varied
prompt formatting and task/dataset selection, we analyze how well LLMs can
interpret and utilize graph structures. By comparing LLMs' performance with
specialized graph models, we offer insights into the strengths and limitations
of employing LLMs for graph analytics. Our findings provide insights into LLMs'
capabilities and suggest avenues for further exploration in applying them to
graph analytics.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04945" title="Abstract">arXiv:2310.04945</a> [<a href="/pdf/2310.04945" title="Download PDF">pdf</a>, <a href="/format/2310.04945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balancing Specialized and General Skills in LLMs: The Impact of Modern  Tuning and Data Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+D">Da Tang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Ke Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yukun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+Y">Yingtong Bu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper introduces a multifaceted methodology for fine-tuning and
evaluating large language models (LLMs) for specialized monetization tasks. The
goal is to balance general language proficiency with domain-specific skills.
The methodology has three main components: 1) Carefully blending in-domain and
general-purpose data during fine-tuning to achieve an optimal balance between
general and specialized capabilities; 2) Designing a comprehensive evaluation
framework with 45 questions tailored to assess performance on functionally
relevant dimensions like reliability, consistency, and business impact; 3)
Analyzing how model size and continual training influence metrics to guide
efficient resource allocation during fine-tuning. The paper details the design,
data collection, analytical techniques, and results validating the proposed
frameworks. It aims to provide businesses and researchers with actionable
insights on effectively adapting LLMs for specialized contexts. We also intend
to make public the comprehensive evaluation framework, which includes the 45
tailored questions and their respective scoring guidelines, to foster
transparency and collaboration in adapting LLMs for specialized tasks.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04946" title="Abstract">arXiv:2310.04946</a> [<a href="/pdf/2310.04946" title="Download PDF">pdf</a>, <a href="/format/2310.04946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transferable Deep Clustering Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Deep learning has shown remarkable success in the field of clustering
recently. However, how to transfer a trained clustering model on a source
domain to a target domain by leveraging the acquired knowledge to guide the
clustering process remains challenging. Existing deep clustering methods often
lack generalizability to new domains because they typically learn a group of
fixed cluster centroids, which may not be optimal for the new domain
distributions. In this paper, we propose a novel transferable deep clustering
model that can automatically adapt the cluster centroids according to the
distribution of data samples. Rather than learning a fixed set of centroids,
our approach introduces a novel attention-based module that can adapt the
centroids by measuring their relationship with samples. In addition, we
theoretically show that our model is strictly more powerful than some classical
clustering algorithms such as k-means or Gaussian Mixture Model (GMM).
Experimental results on both synthetic and real-world datasets demonstrate the
effectiveness and efficiency of our proposed transfer learning framework, which
significantly improves the performance on target domain and reduces the
computational cost.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04948" title="Abstract">arXiv:2310.04948</a> [<a href="/pdf/2310.04948" title="Download PDF">pdf</a>, <a href="/format/2310.04948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+D">Defu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+F">Furong Jia</a>, 
<a href="/search/cs?searchtype=author&query=Arik%2C+S+O">Sercan O Arik</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+T">Tomas Pfister</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yixiang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wen Ye</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 19 figures, 17 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The past decade has witnessed significant advances in time series modeling
with deep learning. While achieving state-of-the-art results, the
best-performing architectures vary highly across applications and domains. On
the other hand, for natural language processing, Generative Pre-trained
Transformer (GPT) has demonstrated impressive performance via training one
general-purpose model across various textual datasets. It is intriguing to
explore whether GPT-type architectures can be effective for time series,
capturing the intrinsic dynamic attributes and leading to significant accuracy
improvements. In this paper, we propose a novel framework, TEMPO, that can
effectively learn time series representations. We focus on utilizing two
essential inductive biases of the time series task for pre-trained models: (i)
decomposition of the complex interaction between trend, seasonal and residual
components; and (ii) introducing the selection-based prompts to facilitate
distribution adaptation in non-stationary time series. TEMPO expands the
capability for dynamically modeling real-world temporal phenomena from data
within diverse domains. Our experiments demonstrate the superior performance of
TEMPO, with 20\%-60\% improvement over state-of-the-art methods on a number of
time series benchmark datasets. This performance gain is observed not only in
standard supervised learning settings but also in scenarios involving
previously unseen datasets. This compelling finding highlights \modelname's
potential to constitute a foundational model building framework.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04949" title="Abstract">arXiv:2310.04949</a> [<a href="/pdf/2310.04949" title="Download PDF">pdf</a>, <a href="/format/2310.04949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Knowledge Graph Construction Via A Simple Checker
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yueling Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Li-C. Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the availability of large language models, there is a growing interest
for semiconductor chip design companies to leverage the technologies. For those
companies, deployment of a new methodology must include two important
considerations: confidentiality and scalability. In this context, this work
tackles the problem of knowledge graph construction from hardware-design domain
texts. We propose an oracle-checker scheme to leverage the power of GPT3.5 and
demonstrate that the essence of the problem is in distillation of domain
expert's background knowledge. Using RISC-V unprivileged ISA specification as
an example, we explain key ideas and discuss practicality of our proposed
oracle-checker approach.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04951" title="Abstract">arXiv:2310.04951</a> [<a href="/pdf/2310.04951" title="Download PDF">pdf</a>, <a href="/format/2310.04951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeTransOcean: A Comprehensive Multilingual Benchmark for Code  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+W">Weixiang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuchen Tian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunzhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wen Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">Recent code translation techniques exploit neural machine translation models
to translate source code from one programming language to another to satisfy
production compatibility or to improve efficiency of codebase maintenance. Most
existing code translation datasets only focus on a single pair of popular
programming languages. To advance research on code translation and meet diverse
requirements of real-world applications, we construct CodeTransOcean, a
large-scale comprehensive benchmark that supports the largest variety of
languages for code translation. CodeTransOcean consists of three novel
multilingual datasets, namely, MultilingualTrans supporting translations
between multiple popular programming languages, NicheTrans for translating
between niche programming languages and popular ones, and LLMTrans for
evaluating compilability of translated code by large language models (LLMs).
CodeTransOcean also includes a novel cross-framework dataset, DLTrans, for
translating deep learning code across different frameworks. We develop
multilingual modeling approaches for code translation and demonstrate their
great potential in improving the translation quality of both low-resource and
high-resource language pairs and boosting the training efficiency. We also
propose a novel evaluation metric Debugging Success Rate@K for program-level
code translation. Last but not least, we evaluate LLM ChatGPT on our datasets
and investigate its potential for fuzzy compilation predictions. We build
baselines for CodeTransOcean and analyze challenges of code translation for
guiding future research.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04955" title="Abstract">arXiv:2310.04955</a> [<a href="/pdf/2310.04955" title="Download PDF">pdf</a>, <a href="/format/2310.04955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information-Theoretic Bounds on The Removal of Attribute-Specific Bias  From Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiazhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Khayatkhoei%2C+M">Mahyar Khayatkhoei</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiageng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Hanchen Xie</a>, 
<a href="/search/cs?searchtype=author&query=Hussein%2C+M+E">Mohamed E. Hussein</a>, 
<a href="/search/cs?searchtype=author&query=AbdAlmageed%2C+W">Wael AbdAlmageed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Ensuring a neural network is not relying on protected attributes (e.g., race,
sex, age) for predictions is crucial in advancing fair and trustworthy AI.
While several promising methods for removing attribute bias in neural networks
have been proposed, their limitations remain under-explored. In this work, we
mathematically and empirically reveal an important limitation of attribute bias
removal methods in presence of strong bias. Specifically, we derive a general
non-vacuous information-theoretical upper bound on the performance of any
attribute bias removal method in terms of the bias strength. We provide
extensive experiments on synthetic, image, and census datasets to verify the
theoretical bound and its consequences in practice. Our findings show that
existing attribute bias removal methods are effective only when the inherent
bias in the dataset is relatively weak, thus cautioning against the use of
these methods in smaller datasets where strong attribute bias can occur, and
advocating the need for methods that can overcome this limitation.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04959" title="Abstract">arXiv:2310.04959</a> [<a href="/pdf/2310.04959" title="Download PDF">pdf</a>, <a href="/format/2310.04959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Better Chain-of-Thought Prompting Strategies: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zihan Yu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Liang He</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xinyu Dai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiajun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Chain-of-Thought (CoT), a step-wise and coherent reasoning chain, shows its
impressive strength when used as a prompting strategy for large language models
(LLM). Recent years, the prominent effect of CoT prompting has attracted
emerging research. However, there still lacks of a systematic summary about key
factors of CoT prompting and comprehensive guide for prompts utilizing. For a
deeper understanding about CoT prompting, we survey on a wide range of current
research, presenting a systematic and comprehensive analysis on several factors
that may influence the effect of CoT prompting, and introduce how to better
apply it in different applications under these discussions. We further analyze
the challenges and propose some future directions about CoT prompting. This
survey could provide an overall reference on related research.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04960" title="Abstract">arXiv:2310.04960</a> [<a href="/pdf/2310.04960" title="Download PDF">pdf</a>, <a href="/format/2310.04960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Usage of Chinese Pinyin in Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baojun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Unlike alphabetic languages, Chinese spelling and pronunciation are
different. Both characters and pinyin take an important role in Chinese
language understanding. In Chinese NLP tasks, we almost adopt characters or
words as model input, and few works study how to use pinyin. However, pinyin is
essential in many scenarios, such as error correction and fault tolerance for
ASR-introduced errors. Most of these errors are caused by the same or similar
pronunciation words, and we refer to this type of error as SSP(the same or
similar pronunciation) errors for short. In this work, we explore various ways
of using pinyin in pretraining models and propose a new pretraining method
called PmBERT. Our method uses characters and pinyin in parallel for
pretraining. Through delicate pretraining tasks, the characters and pinyin
representation are fused, which can enhance the error tolerance for SSP errors.
We do comprehensive experiments and ablation tests to explore what makes a
robust phonetic enhanced Chinese language model. The experimental results on
both the constructed noise-added dataset and the public error-correction
dataset demonstrate that our model is more robust compared to SOTA models.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04961" title="Abstract">arXiv:2310.04961</a> [<a href="/pdf/2310.04961" title="Download PDF">pdf</a>, <a href="/ps/2310.04961" title="Download PostScript">ps</a>, <a href="/format/2310.04961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reach-avoid Analysis for Sampled-data Systems with Measurement  Uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+T">Taoran Wu</a>, 
<a href="/search/eess?searchtype=author&query=Ren%2C+D">Dejin Ren</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+S">Shuyuan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/eess?searchtype=author&query=Xue%2C+B">Bai Xue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Digital control has become increasingly prevalent in modern systems, making
continuous-time plants controlled by discrete-time (digital) controllers
ubiquitous and crucial across industries, including aerospace, automotive, and
manufacturing. This paper focuses on investigating the reach-avoid problem in
such systems, where the objective is to reach a goal set while avoiding unsafe
states, especially in the presence of state measurement uncertainties. We
propose an approach that builds upon the concept of exponential control
guidance barrier functions, originally used for synthesizing continuous-time
feedback controllers. We introduce a sufficient condition that, if met by a
given continuous-time feedback controller, ensures the safe guidance of the
system into the goal set in its sampled-data implementation, despite state
measurement uncertainties. The event of reaching the goal set is determined
based on state measurements obtained at the sampling time instants. Numerical
examples are provided to demonstrate the validity of our theoretical
developments, showcasing successful implementation in solving the reach-avoid
problem in sampled-data systems with state measurement uncertainties.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04963" title="Abstract">arXiv:2310.04963</a> [<a href="/pdf/2310.04963" title="Download PDF">pdf</a>, <a href="/format/2310.04963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Munley%2C+C">Christian Munley</a>, 
<a href="/search/cs?searchtype=author&query=Jarmusch%2C+A">Aaron Jarmusch</a>, 
<a href="/search/cs?searchtype=author&query=Chandrasekaran%2C+S">Sunita Chandrasekaran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Large language models (LLMs) are a new and powerful tool for a wide span of
applications involving natural language and demonstrate impressive code
generation abilities. In this paper, we explore the capabilitity of
state-of-the-art LLMs, including closed-source options like OpenAI GPT-4 and
open-source alternatives like Meta AI Codellama, to automatically generate
tests and use these tests to validate and verify compiler implementations of a
directive-based programming paradigm, OpenACC. Our approach entails exploring
various prompt engineering techniques including a code template,
retrieval-augmented generation (RAG) with code template, expressive prompt
using RAG with code template, one-shot example, and RAG with one-shot example.
This paper focusses on (a) exploring the capabilities of the latest LLMs for
code generation, (b) investigating prompt and fine tuning methods, and (c)
analyzing the outcome of LLMs generated tests
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04964" title="Abstract">arXiv:2310.04964</a> [<a href="/pdf/2310.04964" title="Download PDF">pdf</a>, <a href="/format/2310.04964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Many-to-Many Mapping for Unpaired Real-World Image  Super-resolution and Downscaling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wanjie Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenzhong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Learning based single image super-resolution (SISR) for real-world images has
been an active research topic yet a challenging task, due to the lack of paired
low-resolution (LR) and high-resolution (HR) training images. Most of the
existing unsupervised real-world SISR methods adopt a two-stage training
strategy by synthesizing realistic LR images from their HR counterparts first,
then training the super-resolution (SR) models in a supervised manner. However,
the training of image degradation and SR models in this strategy are separate,
ignoring the inherent mutual dependency between downscaling and its inverse
upscaling process. Additionally, the ill-posed nature of image degradation is
not fully considered. In this paper, we propose an image downscaling and SR
model dubbed as SDFlow, which simultaneously learns a bidirectional
many-to-many mapping between real-world LR and HR images unsupervisedly. The
main idea of SDFlow is to decouple image content and degradation information in
the latent space, where content information distribution of LR and HR images is
matched in a common latent space. Degradation information of the LR images and
the high-frequency information of the HR images are fitted to an easy-to-sample
conditional distribution. Experimental results on real-world image SR datasets
indicate that SDFlow can generate diverse realistic LR and SR images both
quantitatively and qualitatively.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04965" title="Abstract">arXiv:2310.04965</a> [<a href="/pdf/2310.04965" title="Download PDF">pdf</a>, <a href="/format/2310.04965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain  Everyday Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Jingyuan Qi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Minqian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Ying Shen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lifu Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 9 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Automatically generating scripts (i.e. sequences of key steps described in
text) from video demonstrations and reasoning about the subsequent steps are
crucial to the modern AI virtual assistants to guide humans to complete
everyday tasks, especially unfamiliar ones. However, current methods for
generative script learning rely heavily on well-structured preceding steps
described in text and/or images or are limited to a certain domain, resulting
in a disparity with real-world user scenarios. To address these limitations, we
present a new benchmark challenge -- MultiScript, with two new tasks on
task-oriented multimodal script learning: (1) multimodal script generation, and
(2) subsequent step prediction. For both tasks, the input consists of a target
task name and a video illustrating what has been done to complete the target
task, and the expected output is (1) a sequence of structured step descriptions
in text based on the demonstration video, and (2) a single text description for
the subsequent step, respectively. Built from WikiHow, MultiScript covers
multimodal scripts in videos and text descriptions for over 6,655 human
everyday tasks across 19 diverse domains. To establish baseline performance on
MultiScript, we propose two knowledge-guided multimodal generative frameworks
that incorporate the task-related knowledge prompted from large language models
such as Vicuna. Experimental results show that our proposed approaches
significantly improve over the competitive baselines.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04966" title="Abstract">arXiv:2310.04966</a> [<a href="/pdf/2310.04966" title="Download PDF">pdf</a>, <a href="/format/2310.04966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Active Learning via Dependent Leverage Score Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shimizu%2C+A">Atsushi Shimizu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xiaoou Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Musco%2C+C">Christopher Musco</a>, 
<a href="/search/cs?searchtype=author&query=Weare%2C+J">Jonathan Weare</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We show how to obtain improved active learning methods in the agnostic
(adversarial noise) setting by combining marginal leverage score sampling with
non-independent sampling strategies that promote spatial coverage. In
particular, we propose an easily implemented method based on the pivotal
sampling algorithm, which we test on problems motivated by learning-based
methods for parametric PDEs and uncertainty quantification. In comparison to
independent sampling, our method reduces the number of samples needed to reach
a given target accuracy by up to $50\%$. We support our findings with two
theoretical results. First, we show that any non-independent leverage score
sampling method that obeys a weak one-sided $\ell_{\infty}$ independence
condition (which includes pivotal sampling) can actively learn $d$ dimensional
linear functions with $O(d\log d)$ samples, matching independent sampling. This
result extends recent work on matrix Chernoff bounds under $\ell_{\infty}$
independence, and may be of interest for analyzing other sampling strategies
beyond pivotal sampling. Second, we show that, for the important case of
polynomial regression, our pivotal method obtains an improved bound of $O(d)$
samples.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04970" title="Abstract">arXiv:2310.04970</a> [<a href="/pdf/2310.04970" title="Download PDF">pdf</a>, <a href="/ps/2310.04970" title="Download PostScript">ps</a>, <a href="/format/2310.04970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Big Data Privacy in Emerging Market Fintech and Financial Services: A  Research Agenda
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blumenstock%2C+J+E">Joshua E. Blumenstock</a>, 
<a href="/search/cs?searchtype=author&query=Kohli%2C+N">Nitin Kohli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The data revolution in low- and middle-income countries is quickly
transforming how companies approach emerging markets. As mobile phones and
mobile money proliferate, they generate new streams of data that enable
innovation in consumer finance, credit, and insurance. Already, this new
generation of products are being used by hundreds of millions of consumers,
often to use financial services for the first time. However, the collection,
analysis, and use of these data, particularly from economically disadvantaged
populations, raises serious privacy concerns. This white paper describes a
research agenda to advance our understanding of the problem and solution space
of data privacy in emerging market fintech and financial services. We highlight
five priority areas for research: conducting comprehensive landscape analyses;
understanding local definitions of ``data privacy''; documenting key sources of
risk, and potential technical solutions (such as differential privacy and
homomorphic encryption); improving non-technical approaches to data privacy
(such as policies and practices); and understanding the tradeoffs involved in
deploying privacy-enhancing solutions. Taken together, we hope this research
agenda will focus attention on the multi-faceted nature of privacy in emerging
markets, and catalyze efforts to develop responsible and consumer-oriented
approaches to data-intensive applications.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04971" title="Abstract">arXiv:2310.04971</a> [<a href="/pdf/2310.04971" title="Download PDF">pdf</a>, <a href="/format/2310.04971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Robustness of Multi-modal Contrastive Learning to  Distribution Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yihao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+S">Siddharth Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Dang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Mirzasoleiman%2C+B">Baharan Mirzasoleiman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, multimodal contrastive learning (MMCL) approaches, such as CLIP,
have achieved a remarkable success in learning representations that are robust
against distribution shift and generalize to new domains. Despite the empirical
success, the mechanism behind learning such generalizable representations is
not understood. In this work, we rigorously analyze this problem and uncover
two mechanisms behind MMCL's robustness: \emph{intra-class contrasting}, which
allows the model to learn features with a high variance, and \emph{inter-class
feature sharing}, where annotated details in one class help learning other
classes better. Both mechanisms prevent spurious features that are
over-represented in the training data to overshadow the generalizable core
features. This yields superior zero-shot classification accuracy under
distribution shift. Furthermore, we theoretically demonstrate the benefits of
using rich captions on robustness and explore the effect of annotating
different types of details in the captions. We validate our theoretical
findings through experiments, including a well-designed synthetic experiment
and an experiment involving training CLIP on MS COCO and evaluating the model
on variations of shifted ImageNet.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04972" title="Abstract">arXiv:2310.04972</a> [<a href="/pdf/2310.04972" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secondary frequency control of islanded microgrid considering wind and  solar stochastics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhong%2C+C">Cheng Zhong</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Z">Zhifu Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xiangyu Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+J">Jikai Chen</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Acta energiae solaris sinica [In Chinese]
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">As the high penetration of wind and photovoltaic distributed generation (DG)
in the microgrid, the stochastic and low inertia emerge, bringing more
challenges especially when the microgrid operates in isolated islands.
Nevertheless, the reserve power of DGs in deloading control mode can be
utilized for frequency regulation and mitigating frequency excursion. This
paper proposed a model predictive control (MPC) secondary frequency control
method considering wind and solar power generation stochastics. The extended
state-space matrix including unknown stochastic power disturbance is
established, and a Kalman filter is used to observe the unknown disturbance.
The maximum available power of wind and solar DGs is estimated for establishing
real-time variable constraints that prevent DGs output power from exceeding the
limits. Through setting proper weight coefficients, wind and photovoltaic DGs
are given priority to participate in secondary frequency control. The
distributed restorative power of each DG is obtained by solving the quadratic
programming(QP) optimal problem with variable constraints. Finally, a microgrid
simulation model including multiple PV and wind DGs is built and performed in
various scenarios compared to the traditional secondary frequency control
method. The simulation results validated that the proposed method can enhance
the frequency recovery speed and reDGce the frequency deviation, especially in
severe photovoltaic and wind fluctuations scenarios.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04975" title="Abstract">arXiv:2310.04975</a> [<a href="/pdf/2310.04975" title="Download PDF">pdf</a>, <a href="/ps/2310.04975" title="Download PostScript">ps</a>, <a href="/format/2310.04975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Trustworthy and Consistent Blockchain Oracle Scheme for Industrial  Internet of Things
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+Y">Youquan Xian</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+C">Chuanjian Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Li-e Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xianxian Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Rejected after the third round of review of IEEE Internet of Things Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Blockchain provides decentralization and trustlessness features for the
Industrial Internet of Things (IIoT), which expands the application scenarios
of IIoT. To address the problem that the blockchain cannot actively obtain
off-chain data, the blockchain oracle is proposed as a bridge between the
blockchain and external data. However, the existing oracle schemes are
difficult to solve the problem of low quality of service caused by frequent
data changes and heterogeneous devices in IIoT, and the current oracle node
selection schemes are difficult to balance security and quality of service. To
tackle these problems, this paper proposes a secure and reliable oracle scheme
that can obtain high-quality off-chain data. Specifically, we first design an
oracle node selection algorithm based on Verifiable Random Function (VRF) and
reputation mechanism to securely select high-quality nodes. Second, we propose
a data filtering algorithm based on a sliding window to further improve the
consistency of the collected data. We verify the security of the proposed
scheme through security analysis. The experimental results show that the
proposed scheme can effectively improve the service quality of the oracle.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04978" title="Abstract">arXiv:2310.04978</a> [<a href="/pdf/2310.04978" title="Download PDF">pdf</a>, <a href="/format/2310.04978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TopicAdapt- An Inter-Corpora Topics Adaptation Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akash%2C+P+S">Pritom Saha Akash</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+T">Trisha Das</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K+C">Kevin Chen-Chuan Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Topic models are popular statistical tools for detecting latent semantic
topics in a text corpus. They have been utilized in various applications across
different fields. However, traditional topic models have some limitations,
including insensitivity to user guidance, sensitivity to the amount and quality
of data, and the inability to adapt learned topics from one corpus to another.
To address these challenges, this paper proposes a neural topic model,
TopicAdapt, that can adapt relevant topics from a related source corpus and
also discover new topics in a target corpus that are absent in the source
corpus. The proposed model offers a promising approach to improve topic
modeling performance in practical scenarios. Experiments over multiple datasets
from diverse domains show the superiority of the proposed model against the
state-of-the-art topic models.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04979" title="Abstract">arXiv:2310.04979</a> [<a href="/pdf/2310.04979" title="Download PDF">pdf</a>, <a href="/format/2310.04979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Initial Task Assignment in Multi-Human Multi-Robot Teams: An  Attention-enhanced Hierarchical Reinforcement Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dezhong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gupte%2C+A">Arjun Gupte</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+B">Byung-Cheol Min</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Multi-human multi-robot teams (MH-MR) obtain tremendous potential in tackling
intricate and massive missions by merging distinct strengths and expertise of
individual members. The inherent heterogeneity of these teams necessitates
advanced initial task assignment (ITA) methods that align tasks with the
intrinsic capabilities of team members from the outset. While existing
reinforcement learning approaches show encouraging results, they might fall
short in addressing the nuances of long-horizon ITA problems, particularly in
settings with large-scale MH-MR teams or multifaceted tasks. To bridge this
gap, we propose an attention-enhanced hierarchical reinforcement learning
approach that decomposes the complex ITA problem into structured sub-problems,
facilitating more efficient allocations. To bolster sub-policy learning, we
introduce a hierarchical cross-attribute attention (HCA) mechanism, encouraging
each sub-policy within the hierarchy to discern and leverage the specific
nuances in the state space that are crucial for its respective decision-making
phase. Through an extensive environmental surveillance case study, we
demonstrate the benefits of our model and the HCA inside.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04981" title="Abstract">arXiv:2310.04981</a> [<a href="/pdf/2310.04981" title="Download PDF">pdf</a>, <a href="/format/2310.04981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compositional Semantics for Open Vocabulary Spatio-semantic  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karlsson%2C+R">Robin Karlsson</a>, 
<a href="/search/cs?searchtype=author&query=Lepe-Salazar%2C+F">Francisco Lepe-Salazar</a>, 
<a href="/search/cs?searchtype=author&query=Takeda%2C+K">Kazuya Takeda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">General-purpose mobile robots need to complete tasks without exact human
instructions. Large language models (LLMs) is a promising direction for
realizing commonsense world knowledge and reasoning-based planning.
Vision-language models (VLMs) transform environment percepts into
vision-language semantics interpretable by LLMs. However, completing complex
tasks often requires reasoning about information beyond what is currently
perceived. We propose latent compositional semantic embeddings z* as a
principled learning-based knowledge representation for queryable
spatio-semantic memories. We mathematically prove that z* can always be found,
and the optimal z* is the centroid for any set Z. We derive a probabilistic
bound for estimating separability of related and unrelated semantics. We prove
that z* is discoverable by iterative optimization by gradient descent from
visual appearance and singular descriptions. We experimentally verify our
findings on four embedding spaces incl. CLIP and SBERT. Our results show that
z* can represent up to 10 semantics encoded by SBERT, and up to 100 semantics
for ideal uniformly distributed high-dimensional embeddings. We demonstrate
that a simple dense VLM trained on the COCO-Stuff dataset can learn z* for 181
overlapping semantics by 42.23 mIoU, while improving conventional
non-overlapping open-vocabulary segmentation performance by +3.48 mIoU compared
with a popular SOTA model.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04982" title="Abstract">arXiv:2310.04982</a> [<a href="/pdf/2310.04982" title="Download PDF">pdf</a>, <a href="/format/2310.04982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative Analysis of Transfer Learning in Deep Learning  Text-to-Speech Models on a Few-Shot, Low-Resource, Customized Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ze Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 86 pages, Bachelor's thesis, 34 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Text-to-Speech (TTS) synthesis using deep learning relies on voice quality.
Modern TTS models are advanced, but they need large amount of data. Given the
growing computational complexity of these models and the scarcity of large,
high-quality datasets, this research focuses on transfer learning, especially
on few-shot, low-resource, and customized datasets. In this research,
"low-resource" specifically refers to situations where there are limited
amounts of training data, such as a small number of audio recordings and
corresponding transcriptions for a particular language or dialect. This thesis,
is rooted in the pressing need to find TTS models that require less training
time, fewer data samples, yet yield high-quality voice output. The research
evaluates TTS state-of-the-art model transfer learning capabilities through a
thorough technical analysis. It then conducts a hands-on experimental analysis
to compare models' performance in a constrained dataset. This study
investigates the efficacy of modern TTS systems with transfer learning on
specialized datasets and a model that balances training efficiency and
synthesis quality. Initial hypotheses suggest that transfer learning could
significantly improve TTS models' performance on compact datasets, and an
optimal model may exist for such unique conditions. This thesis predicts a rise
in transfer learning in TTS as data scarcity increases. In the future, custom
TTS applications will favour models optimized for specific datasets over
generic, data-intensive ones.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04984" title="Abstract">arXiv:2310.04984</a> [<a href="/pdf/2310.04984" title="Download PDF">pdf</a>, <a href="/format/2310.04984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-adapted Fourier sampling for generative compressed sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berk%2C+A">Aaron Berk</a>, 
<a href="/search/cs?searchtype=author&query=Brugiapaglia%2C+S">Simone Brugiapaglia</a>, 
<a href="/search/cs?searchtype=author&query=Plan%2C+Y">Yaniv Plan</a>, 
<a href="/search/cs?searchtype=author&query=Scott%2C+M">Matthew Scott</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+X">Xia Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Yilmaz%2C+O">Ozgur Yilmaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures. Submitted to the NeurIPS 2023 Workshop on Deep Learning and Inverse Problems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP); Probability (math.PR); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study generative compressed sensing when the measurement matrix is
randomly subsampled from a unitary matrix (with the DFT as an important special
case). It was recently shown that $\textit{O}(kdn\|
\boldsymbol{\alpha}\|_{\infty}^{2})$ uniformly random Fourier measurements are
sufficient to recover signals in the range of a neural network $G:\mathbb{R}^k
\to \mathbb{R}^n$ of depth $d$, where each component of the so-called local
coherence vector $\boldsymbol{\alpha}$ quantifies the alignment of a
corresponding Fourier vector with the range of $G$. We construct a
model-adapted sampling strategy with an improved sample complexity of
$\textit{O}(kd\| \boldsymbol{\alpha}\|_{2}^{2})$ measurements. This is enabled
by: (1) new theoretical recovery guarantees that we develop for nonuniformly
random sampling distributions and then (2) optimizing the sampling distribution
to minimize the number of measurements needed for these guarantees. This
development offers a sample complexity applicable to natural signal classes,
which are often almost maximally coherent with low Fourier frequencies.
Finally, we consider a surrogate sampling scheme, and validate its performance
in recovery experiments using the CelebA dataset.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04985" title="Abstract">arXiv:2310.04985</a> [<a href="/pdf/2310.04985" title="Download PDF">pdf</a>, <a href="/format/2310.04985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VQPL: Vector Quantized Protein Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhangyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Is there a foreign language describing protein sequences and structures
simultaneously? Protein structures, represented by continuous 3D points, have
long posed a challenge due to the contrasting modeling paradigms of discrete
sequences. To represent protein sequence-structure as discrete symbols, we
propose a VQProteinformer to project residue types and structures into a
discrete space, supervised by a reconstruction loss to ensure information
preservation. The sequential latent codes of residues introduce a new quantized
protein language, transforming the protein sequence-structure into a unified
modality. We demonstrate the potential of the created protein language on
predictive and generative tasks, which may not only advance protein research
but also establish a connection between the protein-related and NLP-related
fields. The proposed method will be continually improved to unify more protein
modalities, including text and point cloud.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04987" title="Abstract">arXiv:2310.04987</a> [<a href="/pdf/2310.04987" title="Download PDF">pdf</a>, <a href="/format/2310.04987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-centric Graph Learning: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bo%2C+D">Deyu Bo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jixi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yufei Peng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Haoran Dai</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+A">Ao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yixin Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chunchen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yuxin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chuan Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The history of artificial intelligence (AI) has witnessed the significant
impact of high-quality data on various deep learning models, such as ImageNet
for AlexNet and ResNet. Recently, instead of designing more complex neural
architectures as model-centric approaches, the attention of AI community has
shifted to data-centric ones, which focuses on better processing data to
strengthen the ability of neural models. Graph learning, which operates on
ubiquitous topological data, also plays an important role in the era of deep
learning. In this survey, we comprehensively review graph learning approaches
from the data-centric perspective, and aim to answer two crucial questions: (1)
when to modify graph data and (2) how to modify graph data to unlock the
potential of various graph models. Accordingly, we propose a novel taxonomy
based on the stages in the graph learning pipeline, and highlight the
processing methods for different data structures in the graph data, i.e.,
topology, feature and label. Furthermore, we analyze some potential problems
embedded in graph data and discuss how to solve them in a data-centric manner.
Finally, we provide some promising future directions for data-centric graph
learning.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04988" title="Abstract">arXiv:2310.04988</a> [<a href="/pdf/2310.04988" title="Download PDF">pdf</a>, <a href="/format/2310.04988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Troubling Emergence of Hallucination in Large Language Models -- An  Extensive Definition, Quantification, and Prescriptive Remediations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rawte%2C+V">Vipula Rawte</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Swagata Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Pathak%2C+A">Agnibh Pathak</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Anubhav Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Tonmoy%2C+S+M+T+I">S.M Towhidul Islam Tonmoy</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Sheth%2C+A+P">Amit P. Sheth</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Amitava Das</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The recent advancements in Large Language Models (LLMs) have garnered
widespread acclaim for their remarkable emerging capabilities. However, the
issue of hallucination has parallelly emerged as a by-product, posing
significant concerns. While some recent endeavors have been made to identify
and mitigate different types of hallucination, there has been a limited
emphasis on the nuanced categorization of hallucination and associated
mitigation methods. To address this gap, we offer a fine-grained discourse on
profiling hallucination based on its degree, orientation, and category, along
with offering strategies for alleviation. As such, we define two overarching
orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining
(SL). To provide a more comprehensive understanding, both orientations are
further sub-categorized into intrinsic and extrinsic, with three degrees of
severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously
categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric
nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum,
and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a
publicly available dataset comprising of 75,000 samples generated using 15
contemporary LLMs along with human annotations for the aforementioned
categories. Finally, to establish a method for quantifying and to offer a
comparative spectrum that allows us to evaluate and rank LLMs based on their
vulnerability to producing hallucinations, we propose Hallucination
Vulnerability Index (HVI). We firmly believe that HVI holds significant value
as a tool for the wider NLP community, with the potential to serve as a rubric
in AI-related policy-making. In conclusion, we propose two solution strategies
for mitigating hallucinations.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04990" title="Abstract">arXiv:2310.04990</a> [<a href="/pdf/2310.04990" title="Download PDF">pdf</a>, <a href="/format/2310.04990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Waveformer for modelling dynamical systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Navaneeth%2C+N">N Navaneeth</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Souvik Chakraborty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Neural operators have gained recognition as potent tools for learning
solutions of a family of partial differential equations. The state-of-the-art
neural operators excel at approximating the functional relationship between
input functions and the solution space, potentially reducing computational
costs and enabling real-time applications. However, they often fall short when
tackling time-dependent problems, particularly in delivering accurate long-term
predictions. In this work, we propose "waveformer", a novel operator learning
approach for learning solutions of dynamical systems. The proposed waveformer
exploits wavelet transform to capture the spatial multi-scale behavior of the
solution field and transformers for capturing the long horizon dynamics. We
present four numerical examples involving Burgers's equation, KS-equation,
Allen Cahn equation, and Navier Stokes equation to illustrate the efficacy of
the proposed approach. Results obtained indicate the capability of the proposed
waveformer in learning the solution operator and show that the proposed
Waveformer can learn the solution operator with high accuracy, outperforming
existing state-of-the-art operator learning algorithms by up to an order, with
its advantage particularly visible in the extrapolation region
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04991" title="Abstract">arXiv:2310.04991</a> [<a href="/pdf/2310.04991" title="Download PDF">pdf</a>, <a href="/format/2310.04991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video-Teller: Enhancing Cross-Modal Generation with Fusion and  Decoupling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haogeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Q">Qihang Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tingkai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linjie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Y">Yunzhe Tao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Huaibo Huang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ran He</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper proposes Video-Teller, a video-language foundation model that
leverages multi-modal fusion and fine-grained modality alignment to
significantly enhance the video-to-text generation task. Video-Teller boosts
the training efficiency by utilizing frozen pretrained vision and language
modules. It capitalizes on the robust linguistic capabilities of large language
models, enabling the generation of both concise and elaborate video
descriptions. To effectively integrate visual and auditory information,
Video-Teller builds upon the image-based BLIP-2 model and introduces a cascaded
Q-Former which fuses information across frames and ASR texts. To better guide
video summarization, we introduce a fine-grained modality alignment objective,
where the cascaded Q-Former's output embedding is trained to align with the
caption/summary embedding created by a pretrained text auto-encoder.
Experimental results demonstrate the efficacy of our proposed video-language
foundation model in accurately comprehending videos and generating coherent and
precise language descriptions. It is worth noting that the fine-grained
alignment enhances the model's capabilities (4% improvement of CIDEr score on
MSR-VTT) with only 13% extra parameters in training and zero additional cost in
inference.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04993" title="Abstract">arXiv:2310.04993</a> [<a href="/pdf/2310.04993" title="Download PDF">pdf</a>, <a href="/format/2310.04993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt-augmented Temporal Point Process for Streaming Event Sequence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+S">Siqiao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+Z">Zhixuan Chu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiaoming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Caigao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+H">Hongyan Hao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+G">Gangwei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+X">Xiaoyun Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+Y">James Y. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jun Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Neural Temporal Point Processes (TPPs) are the prevalent paradigm for
modeling continuous-time event sequences, such as user activities on the web
and financial transactions. In real-world applications, event data is typically
received in a \emph{streaming} manner, where the distribution of patterns may
shift over time. Additionally, \emph{privacy and memory constraints} are
commonly observed in practical scenarios, further compounding the challenges.
Therefore, the continuous monitoring of a TPP to learn the streaming event
sequence is an important yet under-explored problem. Our work paper addresses
this challenge by adopting Continual Learning (CL), which makes the model
capable of continuously learning a sequence of tasks without catastrophic
forgetting under realistic constraints. Correspondingly, we propose a simple
yet effective framework, PromptTPP\footnote{Our code is available at {\small
\url{ https://github.com/yanyanSann/PromptTPP}}}, by integrating the base TPP
with a continuous-time retrieval prompt pool. The prompts, small learnable
parameters, are stored in a memory space and jointly optimized with the base
TPP, ensuring that the model learns event streams sequentially without
buffering past examples or task-specific attributes. We present a novel and
realistic experimental setup for modeling event streams, where PromptTPP
consistently achieves state-of-the-art performance across three real user
behavior datasets.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04994" title="Abstract">arXiv:2310.04994</a> [<a href="/pdf/2310.04994" title="Download PDF">pdf</a>, <a href="/format/2310.04994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distantly-Supervised Joint Entity and Relation Extraction with  Noise-Robust Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yufei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yanghong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanchi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Cong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Joint entity and relation extraction is a process that identifies entity
pairs and their relations using a single model. We focus on the problem of
training these models on distantly-labeled data, which is generated by aligning
entity mentions in a text corpus with their corresponding entity and relation
types in a knowledge base. One key challenge here is the presence of noisy
labels, which arises from both entity and relation annotations, and
significantly impair the effectiveness of supervised learning applications.
However, existing research primarily addresses only one type of noise, thereby
limiting the effectiveness of noise reduction. To fill this gap, we introduce a
new noise-robust approach, that 1)~incorporates a pre-trained GPT-2 into a
sequence tagging scheme for simultaneous entity and relation detection, and
2)~employs a noise-robust learning framework which includes a new loss function
that penalizes inconsistency with both significant relation patterns and
entity-relation dependencies, as well as a self-adaptive learning step that
iteratively selects and trains on high-quality instances. Experiments on two
datasets show that our method outperforms the existing state-of-the-art methods
in both joint extraction performance and noise reduction effect.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04995" title="Abstract">arXiv:2310.04995</a> [<a href="/pdf/2310.04995" title="Download PDF">pdf</a>, <a href="/format/2310.04995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SemST: Semantically Consistent Multi-Scale Image Translation via  Structure-Texture Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+G">Ganning Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+W">Wenhui Cui</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+S">Suya You</a>, 
<a href="/search/cs?searchtype=author&query=Kuo%2C+C+-+J">C.-C. Jay Kuo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Unsupervised image-to-image (I2I) translation learns cross-domain image
mapping that transfers input from the source domain to output in the target
domain while preserving its semantics. One challenge is that different semantic
statistics in source and target domains result in content discrepancy known as
semantic distortion. To address this problem, a novel I2I method that maintains
semantic consistency in translation is proposed and named SemST in this work.
SemST reduces semantic distortion by employing contrastive learning and
aligning the structural and textural properties of input and output by
maximizing their mutual information. Furthermore, a multi-scale approach is
introduced to enhance translation performance, thereby enabling the
applicability of SemST to domain adaptation in high-resolution images.
Experiments show that SemST effectively mitigates semantic distortion and
achieves state-of-the-art performance. Also, the application of SemST to domain
adaptation (DA) is explored. It is demonstrated by preliminary experiments that
SemST can be utilized as a beneficial pre-training for the semantic
segmentation task.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04996" title="Abstract">arXiv:2310.04996</a> [<a href="/pdf/2310.04996" title="Download PDF">pdf</a>, <a href="/format/2310.04996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experiences with CAMRE: Single-Device Collaborative Adaptive Mixed  Reality Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hung-Jui Guo</a>, 
<a href="/search/cs?searchtype=author&query=Ashtiani%2C+O+E">Omeed Eshaghi Ashtiani</a>, 
<a href="/search/cs?searchtype=author&query=Prabhakaran%2C+B">Balakrishnan Prabhakaran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">During collaboration in XR (eXtended Reality), users typically share and
interact with virtual objects in a common, shared virtual environment.
Specifically, collaboration among users in Mixed Reality (MR) requires knowing
their position, movement, and understanding of the visual scene surrounding
their physical environments. Otherwise, one user could move an important
virtual object to a position blocked by the physical environment for others.
However, even for a single physical environment, 3D reconstruction takes a long
time and the produced 3D data is typically very large in size. Also, these
large amounts of 3D data take a long time to be streamed to receivers making
real-time updates on the rendered scene challenging. Furthermore, many
collaboration systems in MR require multiple devices, which take up space and
make setup difficult. To address these challenges, in this paper, we describe a
single-device system called Collaborative Adaptive Mixed Reality Environment
(CAMRE). We build CAMRE using the scene understanding capabilities of HoloLens
2 devices to create shared MR virtual environments for each connected user and
demonstrate using a Leader-Follower(s) paradigm: faster reconstruction and
scene update times due to smaller data. Consequently, multiple users can
receive shared, synchronized, and close-to-real-time latency virtual scenes
from a chosen Leader, based on their physical position and movement. We also
illustrate other expanded features of CAMRE MR virtual environment such as
navigation using a real-time virtual mini-map and X-ray vision for handling
adaptive wall opacity. We share several experimental results that evaluate the
performance of CAMRE in terms of the network latency in sharing virtual objects
and other capabilities.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04997" title="Abstract">arXiv:2310.04997</a> [<a href="/pdf/2310.04997" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unmasking Biases and Navigating Pitfalls in the Ophthalmic Artificial  Intelligence Lifecycle: A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakayama%2C+L+F">Luis Filipe Nakayama</a>, 
<a href="/search/cs?searchtype=author&query=Matos%2C+J">Jo&#xe3;o Matos</a>, 
<a href="/search/cs?searchtype=author&query=Quion%2C+J">Justin Quion</a>, 
<a href="/search/cs?searchtype=author&query=Novaes%2C+F">Frederico Novaes</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+W+G">William Greig Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Mwavu%2C+R">Rogers Mwavu</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+J+J">Ju-Yi Ji Hung</a>, 
<a href="/search/cs?searchtype=author&query=Santiago%2C+A+P+d">Alvina Pauline dy Santiago</a>, 
<a href="/search/cs?searchtype=author&query=Phanphruk%2C+W">Warachaya Phanphruk</a>, 
<a href="/search/cs?searchtype=author&query=Cardoso%2C+J+S">Jaime S. Cardoso</a>, 
<a href="/search/cs?searchtype=author&query=Celi%2C+L+A">Leo Anthony Celi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Over the past two decades, exponential growth in data availability,
computational power, and newly available modeling techniques has led to an
expansion in interest, investment, and research in Artificial Intelligence (AI)
applications. Ophthalmology is one of many fields that seek to benefit from AI
given the advent of telemedicine screening programs and the use of ancillary
imaging. However, before AI can be widely deployed, further work must be done
to avoid the pitfalls within the AI lifecycle. This review article breaks down
the AI lifecycle into seven steps: data collection; defining the model task;
data pre-processing and labeling; model development; model evaluation and
validation; deployment; and finally, post-deployment evaluation, monitoring,
and system recalibration and delves into the risks for harm at each step and
strategies for mitigating them.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04998" title="Abstract">arXiv:2310.04998</a> [<a href="/pdf/2310.04998" title="Download PDF">pdf</a>, <a href="/ps/2310.04998" title="Download PostScript">ps</a>, <a href="/format/2310.04998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High Order Mimetic Symplectic Methods For Hamiltonian Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Srinivasan%2C+A">Anand Srinivasan</a>, 
<a href="/search/math?searchtype=author&query=Castillo%2C+J+E">Jose E. Castillo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Hamiltonian systems are known to conserve the Hamiltonian function, which
describes the energy evolution over time. Obtaining a numerical spatio-temporal
scheme that accurately preserves the discretized Hamiltonian function is often
a challenge. In this paper, the use of high order mimetic spatial schemes is
investigated for the numerical solution of Hamiltonian equations. The mimetic
operators are based on developing high order discrete analogs of the vector
calculus quantities divergence and gradient. The resulting high order operators
preserve the properties of their continuum ones, and are therefore said to
mimic properties of conservation laws and symmetries. Symplectic fourth order
schemes are implemented in this paper for the time integration of Hamiltonian
systems. A theoretical framework for the energy preserving nature of the
resulting schemes is also presented, followed by numerical examples.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04999" title="Abstract">arXiv:2310.04999</a> [<a href="/pdf/2310.04999" title="Download PDF">pdf</a>, <a href="/format/2310.04999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symmetrical Linguistic Feature Distillation with CLIP for Scene Text  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zixiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Hongtao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jianjun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Boqiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we explore the potential of the Contrastive Language-Image
Pretraining (CLIP) model in scene text recognition (STR), and establish a novel
Symmetrical Linguistic Feature Distillation framework (named CLIP-OCR) to
leverage both visual and linguistic knowledge in CLIP. Different from previous
CLIP-based methods mainly considering feature generalization on visual
encoding, we propose a symmetrical distillation strategy (SDS) that further
captures the linguistic knowledge in the CLIP text encoder. By cascading the
CLIP image encoder with the reversed CLIP text encoder, a symmetrical structure
is built with an image-to-text feature flow that covers not only visual but
also linguistic information for distillation.Benefiting from the natural
alignment in CLIP, such guidance flow provides a progressive optimization
objective from vision to language, which can supervise the STR feature
forwarding process layer-by-layer.Besides, a new Linguistic Consistency Loss
(LCL) is proposed to enhance the linguistic capability by considering
second-order statistics during the optimization. Overall, CLIP-OCR is the first
to design a smooth transition between image and text for the STR task.Extensive
experiments demonstrate the effectiveness of CLIP-OCR with 93.8% average
accuracy on six popular STR benchmarks.Code will be available at
https://github.com/wzx99/CLIPOCR.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05000" title="Abstract">arXiv:2310.05000</a> [<a href="/pdf/2310.05000" title="Download PDF">pdf</a>, <a href="/ps/2310.05000" title="Download PostScript">ps</a>, <a href="/format/2310.05000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Reinforce Policy Gradient Algorithm Revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhatnagar%2C+S">Shalabh Bhatnagar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">We revisit the Reinforce policy gradient algorithm from the literature. Note
that this algorithm typically works with cost returns obtained over random
length episodes obtained from either termination upon reaching a goal state (as
with episodic tasks) or from instants of visit to a prescribed recurrent state
(in the case of continuing tasks). We propose a major enhancement to the basic
algorithm. We estimate the policy gradient using a function measurement over a
perturbed parameter by appealing to a class of random search approaches. This
has advantages in the case of systems with infinite state and action spaces as
it relax some of the regularity requirements that would otherwise be needed for
proving convergence of the Reinforce algorithm. Nonetheless, we observe that
even though we estimate the gradient of the performance objective using the
performance objective itself (and not via the sample gradient), the algorithm
converges to a neighborhood of a local minimum. We also provide a proof of
convergence for this new algorithm.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05001" title="Abstract">arXiv:2310.05001</a> [<a href="/pdf/2310.05001" title="Download PDF">pdf</a>, <a href="/format/2310.05001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PromptSpeaker: Speaker Generation Based on Text Descriptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongmao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guanghou Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yi Lei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunlin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hao Yin</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lei Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhifei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Recently, text-guided content generation has received extensive attention. In
this work, we explore the possibility of text description-based speaker
generation, i.e., using text prompts to control the speaker generation process.
Specifically, we propose PromptSpeaker, a text-guided speaker generation
system. PromptSpeaker consists of a prompt encoder, a zero-shot VITS, and a
Glow model, where the prompt encoder predicts a prior distribution based on the
text description and samples from this distribution to obtain a semantic
representation. The Glow model subsequently converts the semantic
representation into a speaker representation, and the zero-shot VITS finally
synthesizes the speaker's voice based on the speaker representation. We verify
that PromptSpeaker can generate speakers new from the training set by objective
metrics, and the synthetic speaker voice has reasonable subjective matching
quality with the speaker prompt.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05002" title="Abstract">arXiv:2310.05002</a> [<a href="/pdf/2310.05002" title="Download PDF">pdf</a>, <a href="/format/2310.05002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Knowledge Guided Retrieval Augmentation for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yile Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have shown superior performance without
task-specific fine-tuning. Despite the success, the knowledge stored in the
parameters of LLMs could still be incomplete and difficult to update due to the
computational costs. As complementary, retrieval-based methods can offer
non-parametric world knowledge and improve the performance on tasks such as
question answering. However, we find that the retrieved knowledge does not
always help and even has a negative impact on original responses occasionally.
To better make use of both internal knowledge and external world knowledge, we
investigate eliciting the model's ability to recognize what they know and do
not know (which is also called self-knowledge) and propose Self-Knowledge
guided Retrieval augmentation (SKR), a simple yet effective method which can
let LLMs refer to the questions they have previously encountered and adaptively
call for external resources when dealing with new questions. We evaluate SKR on
multiple datasets and demonstrate that it outperforms chain-of-thought based
and fully retrieval-based methods by using either InstructGPT or ChatGPT.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05004" title="Abstract">arXiv:2310.05004</a> [<a href="/pdf/2310.05004" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complementing AC &amp; DC Terminal Stability Analyses of MMC with Inner Loop  Impedance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhao%2C+C">Chongbin Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Q">Qirong Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Learning from two-level voltage source converters, the existing
impedance-based stability analyses of modular multilevel converters (MMCs)
primarily focus on system modes with finite closed-loop transfer functions,
which consider perturbations of the current flowing into the public AC/DC
terminal as the input. However, this approach may be insufficient for MMCs due
to their actively controlled circulating circuit, resulting from the
distributed modulation of each arm and the circulating current control (CCC).
To address this limitation, two cases that are not covered by the AC/DC
terminal stability analysis are initially presented to support the conjecture.
Subsequently, an inner loop impedance for the circulating circuit is
established, which considers the dynamics of public terminals and divides the
injected voltage perturbation by the corresponding current perturbation at the
same frequency. To avoid the need for a right-half plane pole check when
applying the Nyquist criterion, a logarithmic derivative-based criterion is
proposed to directly identify the system modes. By utilizing the inner loop
impedance, it becomes possible to achieve CCC parameter tuning with stability
constraints and conduct an internal stability analysis of MMC-based systems.
This work provides a strong foundation for the integration of
power-electronicized power systems from the perspective of classical control
theories.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05007" title="Abstract">arXiv:2310.05007</a> [<a href="/pdf/2310.05007" title="Download PDF">pdf</a>, <a href="/format/2310.05007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot  Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiusi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jyun-Yu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+W">Wei-Cheng Chang</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hsiang-Fu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Few-shot question answering (QA) aims at achieving satisfactory results on
machine question answering when only a few training samples are available.
Recent advances mostly rely on the power of pre-trained large language models
(LLMs) and fine-tuning in specific settings. Although the pre-training stage
has already equipped LLMs with powerful reasoning capabilities, LLMs still need
to be fine-tuned to adapt to specific domains to achieve the best results. In
this paper, we propose to select the most informative data for fine-tuning,
thereby improving the efficiency of the fine-tuning process with comparative or
even better accuracy on the open-domain QA task. We present MinPrompt, a
minimal data augmentation framework for open-domain QA based on an approximate
graph algorithm and unsupervised question generation. We transform the raw text
into a graph structure to build connections between different factual
sentences, then apply graph algorithms to identify the minimal set of sentences
needed to cover the most information in the raw text. We then generate QA pairs
based on the identified sentence subset and train the model on the selected
sentences to obtain the final model. Empirical results on several benchmark
datasets and theoretical analysis show that MinPrompt is able to achieve
comparable or better results than baselines with a high degree of efficiency,
bringing improvements in F-1 scores by up to 27.5%.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05009" title="Abstract">arXiv:2310.05009</a> [<a href="/pdf/2310.05009" title="Download PDF">pdf</a>, <a href="/format/2310.05009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WikiIns: A High-Quality Dataset for Controlled Text Editing by Natural  Language Instruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiaojun Wan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, NLPCC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Text editing, i.e., the process of modifying or manipulating text, is a
crucial step in human writing process. In this paper, we study the problem of
controlled text editing by natural language instruction. According to a given
instruction that conveys the edit intention and necessary information, an
original draft text is required to be revised into a target text. Existing
automatically constructed datasets for this task are limited because they do
not have informative natural language instruction. The informativeness requires
the information contained in the instruction to be enough to produce the
revised text. To address this limitation, we build and release WikiIns, a
high-quality controlled text editing dataset with improved informativeness. We
first preprocess the Wikipedia edit history database to extract the raw data
(WikiIns-Raw). Then we crowdsource high-quality validation and test sets, as
well as a small-scale training set (WikiIns-Gold). With the high-quality
annotated dataset, we further propose automatic approaches to generate a
large-scale ``silver'' training set (WikiIns-Silver). Finally, we provide some
insightful analysis on our WikiIns dataset, including the evaluation results
and the edit intention analysis. Our analysis and the experiment results on
WikiIns may assist the ongoing research on text editing. The dataset, source
code and annotation guideline are available at
https://github.com/CasparSwift/WikiIns.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05010" title="Abstract">arXiv:2310.05010</a> [<a href="/pdf/2310.05010" title="Download PDF">pdf</a>, <a href="/format/2310.05010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building an Open-Vocabulary Video CLIP Model with Better Architectures,  Optimization and Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zuxuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+Z">Zejia Weng</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Wujian Peng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xitong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Ang Li</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+L+S">Larry S. Davis</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yu-Gang Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2302.00624">arXiv:2302.00624</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Despite significant results achieved by Contrastive Language-Image
Pretraining (CLIP) in zero-shot image recognition, limited effort has been made
exploring its potential for zero-shot video recognition. This paper presents
Open-VCLIP++, a simple yet effective framework that adapts CLIP to a strong
zero-shot video classifier, capable of identifying novel actions and events
during testing. Open-VCLIP++ minimally modifies CLIP to capture
spatial-temporal relationships in videos, thereby creating a specialized video
classifier while striving for generalization. We formally demonstrate that
training Open-VCLIP++ is tantamount to continual learning with zero historical
data. To address this problem, we introduce Interpolated Weight Optimization, a
technique that leverages the advantages of weight interpolation during both
training and testing. Furthermore, we build upon large language models to
produce fine-grained video descriptions. These detailed descriptions are
further aligned with video features, facilitating a better transfer of CLIP to
the video domain. Our approach is evaluated on three widely used action
recognition datasets, following a variety of zero-shot evaluation protocols.
The results demonstrate that our method surpasses existing state-of-the-art
techniques by significant margins. Specifically, we achieve zero-shot accuracy
scores of 88.1%, 58.7%, and 81.2% on UCF, HMDB, and Kinetics-600 datasets
respectively, outpacing the best-performing alternative methods by 8.5%, 8.2%,
and 12.3%. We also evaluate our approach on the MSR-VTT video-text retrieval
dataset, where it delivers competitive video-to-text and text-to-video
retrieval performance, while utilizing substantially less fine-tuning data
compared to other methods. Code is released at
https://github.com/wengzejia1/Open-VCLIP.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05012" title="Abstract">arXiv:2310.05012</a> [<a href="/pdf/2310.05012" title="Download PDF">pdf</a>, <a href="/format/2310.05012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Abnormal Health Conditions in Smart Home Using a Drone
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barman%2C+P+K">Pronob Kumar Barman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Nowadays, detecting aberrant health issues is a difficult process. Falling,
especially among the elderly, is a severe concern worldwide. Falls can result
in deadly consequences, including unconsciousness, internal bleeding, and often
times, death. A practical and optimal, smart approach of detecting falling is
currently a concern. The use of vision-based fall monitoring is becoming more
common among scientists as it enables senior citizens and those with other
health conditions to live independently. For tracking, surveillance, and
rescue, unmanned aerial vehicles use video or image segmentation and object
detection methods. The Tello drone is equipped with a camera and with this
device we determined normal and abnormal behaviors among our participants. The
autonomous falling objects are classified using a convolutional neural network
(CNN) classifier. The results demonstrate that the systems can identify falling
objects with a precision of 0.9948.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05014" title="Abstract">arXiv:2310.05014</a> [<a href="/pdf/2310.05014" title="Download PDF">pdf</a>, <a href="/format/2310.05014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Congruence Closure Modulo Groups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dohan Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">This paper presents a new framework for constructing congruence closure of a
finite set of ground equations over uninterpreted symbols and interpreted
symbols for the group axioms. In this framework, ground equations are flattened
into certain forms by introducing new constants, and a completion procedure is
performed on ground flat equations. The proposed completion procedure uses
equational inference rules and constructs a ground convergent rewrite system
for congruence closure with such interpreted symbols. If the completion
procedure terminates, then it yields a decision procedure for the word problem
for a finite set of ground equations with respect to the group axioms. This
paper also provides a sufficient terminating condition of the completion
procedure for constructing a ground convergent rewrite system from ground flat
equations containing interpreted symbols for the group axioms. In addition,
this paper presents a new method for constructing congruence closure of a
finite set of ground equations containing interpreted symbols for the
semigroup, monoid, and the multiple disjoint sets of group axioms,
respectively, using the proposed framework.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05015" title="Abstract">arXiv:2310.05015</a> [<a href="/pdf/2310.05015" title="Download PDF">pdf</a>, <a href="/format/2310.05015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compresso: Structured Pruning with Collaborative Prompting Learns  Compact Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Song Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiahang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L+L">Li Lyna Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Despite the remarkable success of Large Language Models (LLMs), the massive
size poses significant deployment challenges, particularly on
resource-constrained hardware. While existing LLM compression methods focus on
quantization, pruning remains relatively unexplored due to the high cost of
training-based approaches and data collection challenges. One-shot pruning
methods, although cost-effective and data-free, have become dominant in LLM
pruning, but lead to performance decline under the structured pruning setting.
In this work, we introduce a new paradigm for structurally pruning LLMs, called
Compresso. Our approach, through the collaboration of the proposed
resource-efficient pruning algorithm and the LLM itself, learns optimal pruning
decisions during the training process. Compresso addresses the challenges of
expensive training costs and data collection by incorporating Low-Rank
Adaptation (LoRA) into the $L_0$ regularization during the instruction tuning
process. Then, we further augment the pruning algorithm by introducing a
collaborative prompt that fosters collaboration between the LLM and the pruning
algorithm, significantly boosting the overall performance. To this end,
Compresso prunes LLaMA-7B to 5.4B, maintaining original performance and even
surpassing LLaMA-7B in reading comprehension by 2.62%. Extensive experiments
demonstrate that Compresso significantly outperforms one-shot pruning baselines
across various sparsity ratios, achieving up to 2.21%, 11.43%, 7.04%, and 4.81%
higher scores on the commonsense reasoning, reading comprehension, MMLU, and
BBH benchmarks, respectively.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05019" title="Abstract">arXiv:2310.05019</a> [<a href="/pdf/2310.05019" title="Download PDF">pdf</a>, <a href="/format/2310.05019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compressed online Sinkhorn
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fengpei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Poon%2C+C">Clarice Poon</a>, 
<a href="/search/cs?searchtype=author&query=Shardlow%2C+T">Tony Shardlow</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The use of optimal transport (OT) distances, and in particular
entropic-regularised OT distances, is an increasingly popular evaluation metric
in many areas of machine learning and data science. Their use has largely been
driven by the availability of efficient algorithms such as the Sinkhorn
algorithm. One of the drawbacks of the Sinkhorn algorithm for large-scale data
processing is that it is a two-phase method, where one first draws a large
stream of data from the probability distributions, before applying the Sinkhorn
algorithm to the discrete probability measures. More recently, there have been
several works developing stochastic versions of Sinkhorn that directly handle
continuous streams of data. In this work, we revisit the recently introduced
online Sinkhorn algorithm of [Mensch and Peyr\'e, 2020]. Our contributions are
twofold: We improve the convergence analysis for the online Sinkhorn algorithm,
the new rate that we obtain is faster than the previous rate under certain
parameter choices. We also present numerical results to verify the sharpness of
our result. Secondly, we propose the compressed online Sinkhorn algorithm which
combines measure compression techniques with the online Sinkhorn algorithm. We
provide numerical experiments to show practical numerical gains, as well as
theoretical guarantees on the efficiency of our approach.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05020" title="Abstract">arXiv:2310.05020</a> [<a href="/pdf/2310.05020" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing Automation in Data Mining: A Review on the Impact of PyESAPI  in Radiation Oncology Data Extraction and Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alomari%2C+G">Ghaith Alomari</a>, 
<a href="/search/cs?searchtype=author&query=Aljarah%2C+A">Anas Aljarah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Data extraction and management are crucial components of research and
clinical workflows in Radiation Oncology (RO), where accurate and comprehensive
data are imperative to inform treatment planning and delivery. The advent of
automated data mining scripts, particularly using the Python Environment for
Scripting APIs (PyESAPI), has been a promising stride towards enhancing
efficiency, accuracy, and reliability in extracting data from RO Information
Systems (ROIS) and Treatment Planning Systems (TPS). This review dissects the
role, efficiency, and challenges of implementing PyESAPI in RO data extraction
and management, juxtaposing manual data extraction techniques and explicating
future avenues
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05021" title="Abstract">arXiv:2310.05021</a> [<a href="/pdf/2310.05021" title="Download PDF">pdf</a>, <a href="/format/2310.05021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Intelligent Emergency Control for Large-scale Power Systems:  Convergence of Learning, Physics, Computing and Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+Q">Qiuhua Huang</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+R">Renke Huang</a>, 
<a href="/search/eess?searchtype=author&query=Yin%2C+T">Tianzhixi Yin</a>, 
<a href="/search/eess?searchtype=author&query=Datta%2C+S">Sohom Datta</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+X">Xueqing Sun</a>, 
<a href="/search/eess?searchtype=author&query=Hou%2C+J">Jason Hou</a>, 
<a href="/search/eess?searchtype=author&query=Tan%2C+J">Jie Tan</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+W">Wenhao Yu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yuan Liu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xinya Li</a>, 
<a href="/search/eess?searchtype=author&query=Palmer%2C+B">Bruce Palmer</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+A">Ang Li</a>, 
<a href="/search/eess?searchtype=author&query=Ke%2C+X">Xinda Ke</a>, 
<a href="/search/eess?searchtype=author&query=Vaiman%2C+M">Marianna Vaiman</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Song Wang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yousu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to PSCC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper has delved into the pressing need for intelligent emergency
control in large-scale power systems, which are experiencing significant
transformations and are operating closer to their limits with more
uncertainties. Learning-based control methods are promising and have shown
effectiveness for intelligent power system control. However, when they are
applied to large-scale power systems, there are multifaceted challenges such as
scalability, adaptiveness, and security posed by the complex power system
landscape, which demand comprehensive solutions. The paper first proposes and
instantiates a convergence framework for integrating power systems physics,
machine learning, advanced computing, and grid control to realize intelligent
grid control at a large scale. Our developed methods and platform based on the
convergence framework have been applied to a large (more than 3000 buses) Texas
power system, and tested with 56000 scenarios. Our work achieved a 26%
reduction in load shedding on average and outperformed existing rule-based
control in 99.7% of the test scenarios. The results demonstrated the potential
of the proposed convergence framework and DRL-based intelligent control for the
future grid.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05022" title="Abstract">arXiv:2310.05022</a> [<a href="/pdf/2310.05022" title="Download PDF">pdf</a>, <a href="/format/2310.05022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Spiking Neural Network for Legged Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xiaoyang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingkai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Renjing Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent years, legged robots based on deep reinforcement learning have made
remarkable progress. Quadruped robots have demonstrated the ability to complete
challenging tasks in complex environments and have been deployed in real-world
scenarios to assist humans. Simultaneously, bipedal and humanoid robots have
achieved breakthroughs in various demanding tasks. Current reinforcement
learning methods can utilize diverse robot bodies and historical information to
perform actions. However, prior research has not emphasized the speed and
energy consumption of network inference, as well as the biological significance
of the neural networks themselves. Most of the networks employed are
traditional artificial neural networks that utilize multilayer perceptrons
(MLP). In this paper, we successfully apply a novel Spiking Neural Network
(SNN) to process legged robots, achieving outstanding results across a range of
simulated terrains. SNN holds a natural advantage over traditional neural
networks in terms of inference speed and energy consumption, and their
pulse-form processing of body perception signals offers improved biological
interpretability. To the best of our knowledge, this is the first work to
implement SNN in legged robots.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05024" title="Abstract">arXiv:2310.05024</a> [<a href="/pdf/2310.05024" title="Download PDF">pdf</a>, <a href="/format/2310.05024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single Stage Warped Cloth Learning and Semantic-Contextual Attention  Feature Fusion for Virtual TryOn
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pathak%2C+S">Sanhita Pathak</a>, 
<a href="/search/cs?searchtype=author&query=Kaushik%2C+V">Vinay Kaushik</a>, 
<a href="/search/cs?searchtype=author&query=Lall%2C+B">Brejesh Lall</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image-based virtual try-on aims to fit an in-shop garment onto a clothed
person image. Garment warping, which aligns the target garment with the
corresponding body parts in the person image, is a crucial step in achieving
this goal. Existing methods often use multi-stage frameworks to handle clothes
warping, person body synthesis and tryon generation separately or rely on noisy
intermediate parser-based labels. We propose a novel single-stage framework
that implicitly learns the same without explicit multi-stage learning. Our
approach utilizes a novel semantic-contextual fusion attention module for
garment-person feature fusion, enabling efficient and realistic cloth warping
and body synthesis from target pose keypoints. By introducing a lightweight
linear attention framework that attends to garment regions and fuses multiple
sampled flow fields, we also address misalignment and artifacts present in
previous methods. To achieve simultaneous learning of warped garment and try-on
results, we introduce a Warped Cloth Learning Module. WCLM uses segmented
warped garments as ground truth, operating within a single-stage paradigm. Our
proposed approach significantly improves the quality and efficiency of virtual
try-on methods, providing users with a more reliable and realistic virtual
try-on experience. We evaluate our method on the VITON dataset and demonstrate
its state-of-the-art performance in terms of both qualitative and quantitative
metrics.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05025" title="Abstract">arXiv:2310.05025</a> [<a href="/pdf/2310.05025" title="Download PDF">pdf</a>, <a href="/format/2310.05025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synslator: An Interactive Machine Translation Tool with Online Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiayi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Ke Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fengming Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Z">Zhiyong Fu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zeyu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuqi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Interactive machine translation (IMT) has emerged as a progression of the
computer-aided translation paradigm, where the machine translation system and
the human translator collaborate to produce high-quality translations. This
paper introduces Synslator, a user-friendly computer-aided translation (CAT)
tool that not only supports IMT, but is adept at online learning with real-time
translation memories. To accommodate various deployment environments for CAT
services, Synslator integrates two different neural translation models to
handle translation memories for online learning. Additionally, the system
employs a language model to enhance the fluency of translations in an
interactive mode. In evaluation, we have confirmed the effectiveness of online
learning through the translation models, and have observed a 13% increase in
post-editing efficiency with the interactive functionalities of Synslator. A
tutorial video is available at:https://youtu.be/K0vRsb2lTt8.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05026" title="Abstract">arXiv:2310.05026</a> [<a href="/pdf/2310.05026" title="Download PDF">pdf</a>, <a href="/format/2310.05026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Resolution Self-Attention for Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yu-Huan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shi-Chen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Le Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+X">Xin Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Daquan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiashi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Ming-Ming Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhen%2C+L">Liangli Zhen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 11 tables, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Semantic segmentation tasks naturally require high-resolution information for
pixel-wise segmentation and global context information for class prediction.
While existing vision transformers demonstrate promising performance, they
often utilize high resolution context modeling, resulting in a computational
bottleneck. In this work, we challenge conventional wisdom and introduce the
Low-Resolution Self-Attention (LRSA) mechanism to capture global context at a
significantly reduced computational cost. Our approach involves computing
self-attention in a fixed low-resolution space regardless of the input image's
resolution, with additional 3x3 depth-wise convolutions to capture fine details
in the high-resolution space. We demonstrate the effectiveness of our LRSA
approach by building the LRFormer, a vision transformer with an encoder-decoder
structure. Extensive experiments on the ADE20K, COCO-Stuff, and Cityscapes
datasets demonstrate that LRFormer outperforms state-of-the-art models. The
code will be made available at https://github.com/yuhuan-wu/LRFormer.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05027" title="Abstract">arXiv:2310.05027</a> [<a href="/pdf/2310.05027" title="Download PDF">pdf</a>, <a href="/format/2310.05027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rigid Clumps in the MercuryDPM Particle Dynamics Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ostanin%2C+I">Igor Ostanin</a>, 
<a href="/search/math?searchtype=author&query=Angelidakis%2C+V">Vasileios Angelidakis</a>, 
<a href="/search/math?searchtype=author&query=Plath%2C+T">Timo Plath</a>, 
<a href="/search/math?searchtype=author&query=Pourandi%2C+S">Sahar Pourandi</a>, 
<a href="/search/math?searchtype=author&query=Thornton%2C+A">Anthony Thornton</a>, 
<a href="/search/math?searchtype=author&query=Weinhart%2C+T">Thomas Weinhart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph)

</div>
<p class="mathjax">Discrete particle simulations have become the standard in science and
industrial applications exploring the properties of particulate systems. Most
of such simulations rely on the concept of interacting spherical particles to
describe the properties of particulates, although, the correct representation
of the nonspherical particle shape is crucial for a number of applications. In
this work we describe the implementation of clumps, i.e. assemblies of rigidly
connected spherical particles, which can approximate given nonspherical shapes,
within the \textit{MercuryDPM} particle dynamics code. \textit{MercuryDPM}
contact detection algorithm is particularly efficient for polydisperse particle
systems, which is essential for multilevel clumps approximating complex
surfaces. We employ the existing open-source \texttt{CLUMP} library to generate
clump particles. We detail the pre-processing tools providing necessary initial
data, as well as the necessary adjustments of the algorithms of contact
detection, collision/migration and numerical time integration. The capabilities
of our implementation are illustrated for a variety of examples.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05028" title="Abstract">arXiv:2310.05028</a> [<a href="/pdf/2310.05028" title="Download PDF">pdf</a>, <a href="/format/2310.05028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Large Language Models as Zero-shot Relation Extractors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guozheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+W">Wenjun Ke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Relation extraction (RE) consistently involves a certain degree of labeled or
unlabeled data even if under zero-shot setting. Recent studies have shown that
large language models (LLMs) transfer well to new tasks out-of-the-box simply
given a natural language prompt, which provides the possibility of extracting
relations from text without any data and parameter tuning. This work focuses on
the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors.
On the one hand, we analyze the drawbacks of existing RE prompts and attempt to
incorporate recent prompt techniques such as chain-of-thought (CoT) to improve
zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a
simple prompt recursively using LLMs to transform RE inputs to the effective
question answering (QA) format. On the other hand, we conduct comprehensive
experiments on various benchmarks and settings to investigate the capabilities
of LLMs on zero-shot RE. Specifically, we have the following findings: (i)
\textsc{SumAsk} consistently and significantly improves LLMs performance on
different model sizes, benchmarks and settings; (ii) Zero-shot prompting with
ChatGPT achieves competitive or superior results compared with zero-shot and
fully supervised methods; (iii) LLMs deliver promising performance in
extracting overlapping relations; (iv) The performance varies greatly regarding
different relations. Different from small language models, LLMs are effective
in handling challenge none-of-the-above (NoTA) relation.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05029" title="Abstract">arXiv:2310.05029</a> [<a href="/pdf/2310.05029" title="Download PDF">pdf</a>, <a href="/format/2310.05029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Walking Down the Memory Maze: Beyond Context Limit through Interactive  Reading
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Howard Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pasunuru%2C+R">Ramakanth Pasunuru</a>, 
<a href="/search/cs?searchtype=author&query=Weston%2C+J">Jason Weston</a>, 
<a href="/search/cs?searchtype=author&query=Celikyilmaz%2C+A">Asli Celikyilmaz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have advanced in large strides due to the
effectiveness of the self-attention mechanism that processes and compares all
tokens at once. However, this mechanism comes with a fundamental issue -- the
predetermined context window is bound to be limited. Despite attempts to extend
the context window through methods like extrapolating the positional embedding,
using recurrence, or selectively retrieving essential parts of the long
sequence, long-text understanding continues to be a challenge. We propose an
alternative approach which instead treats the LLM as an interactive agent,
allowing it to decide how to read the text via iterative prompting. We
introduce MemWalker, a method that first processes the long context into a tree
of summary nodes. Upon receiving a query, the model navigates this tree in
search of relevant information, and responds once it gathers sufficient
information. On long-text question answering tasks our method outperforms
baseline approaches that use long context windows, recurrence, and retrieval.
We show that, beyond effective reading, MemWalker enhances explainability by
highlighting the reasoning steps as it interactively reads the text;
pinpointing the relevant text segments related to the query.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05030" title="Abstract">arXiv:2310.05030</a> [<a href="/pdf/2310.05030" title="Download PDF">pdf</a>, <a href="/format/2310.05030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as  You May Think -- Introducing AI Detectability Index
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+M">Megha Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Tonmoy%2C+S+M+T+I">S.M Towhidul Islam Tonmoy</a>, 
<a href="/search/cs?searchtype=author&query=Zaman%2C+S+M+M">S M Mehedi Zaman</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+K">Krish Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Barman%2C+N+R">Niyar R Barman</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+C">Chandan Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Gautam%2C+S">Shreya Gautam</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+T">Tanay Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+V">Vinija Jain</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Sheth%2C+A+P">Amit P. Sheth</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Amitava Das</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the rise of prolific ChatGPT, the risk and consequences of AI-generated
text has increased alarmingly. To address the inevitable question of ownership
attribution for AI-generated artifacts, the US Copyright Office released a
statement stating that 'If a work's traditional elements of authorship were
produced by a machine, the work lacks human authorship and the Office will not
register it'. Furthermore, both the US and the EU governments have recently
drafted their initial proposals regarding the regulatory framework for AI.
Given this cynosural spotlight on generative AI, AI-generated text detection
(AGTD) has emerged as a topic that has already received immediate attention in
research, with some initial methods having been proposed, soon followed by
emergence of techniques to bypass detection. This paper introduces the Counter
Turing Test (CT^2), a benchmark consisting of techniques aiming to offer a
comprehensive evaluation of the robustness of existing AGTD techniques. Our
empirical findings unequivocally highlight the fragility of the proposed AGTD
methods under scrutiny. Amidst the extensive deliberations on policy-making for
regulating AI development, it is of utmost importance to assess the
detectability of content generated by LLMs. Thus, to establish a quantifiable
spectrum facilitating the evaluation and ranking of LLMs according to their
detectability levels, we propose the AI Detectability Index (ADI). We conduct a
thorough examination of 15 contemporary LLMs, empirically demonstrating that
larger LLMs tend to have a higher ADI, indicating they are less detectable
compared to smaller LLMs. We firmly believe that ADI holds significant value as
a tool for the wider NLP community, with the potential to serve as a rubric in
AI-related policy-making.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05032" title="Abstract">arXiv:2310.05032</a> [<a href="/pdf/2310.05032" title="Download PDF">pdf</a>, <a href="/format/2310.05032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PASSION: Permissioned Access Control for Segmented Devices and Identity  for IoT Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ali%2C+H">Hisham Ali</a>, 
<a href="/search/cs?searchtype=author&query=Abubakar%2C+M">Mwrwan Abubakar</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+J">Jawad Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Buchanan%2C+W+J">William J. Buchanan</a>, 
<a href="/search/cs?searchtype=author&query=Jaroucheh%2C+Z">Zakwan Jaroucheh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In recent years, there has been a significant proliferation of industrial
Internet of Things (IoT) applications, with a wide variety of use cases being
developed and put into operation. As the industrial IoT landscape expands, the
establishment of secure and reliable infrastructure becomes crucial to instil
trust among users and stakeholders, particularly in addressing fundamental
concerns such as traceability, integrity protection, and privacy that some
industries still encounter today. This paper introduces a privacy-preserving
method in the industry's IoT systems using blockchain-based data access control
for remote industry safety monitoring and maintaining event information
confidentiality, integrity and authenticity.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05033" title="Abstract">arXiv:2310.05033</a> [<a href="/pdf/2310.05033" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RSMS: Towards Reliable and Secure Metaverse Service Provision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yanwei Gong</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xiaolin Chang</a>, 
<a href="/search/cs?searchtype=author&query=Mi%C5%A1i%C4%87%2C+J">Jelena Mi&#x161;i&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Mi%C5%A1i%C4%87%2C+V+B">Vojislav B. Mi&#x161;i&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yingying Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Establishing and sustaining Metaverse service necessitates an unprecedented
scale of resources. This paper considers the deployment of Metaverse service in
a cloud-edge resource architecture, which can satisfy the escalating demand for
Metaverse service resources while ensuring both high bandwidth and low latency.
We propose a novel mechanism, named Reliable and Secure Metaverse Service
(RSMS), to ensure Metaverse service reliability and security without
sacrificing performance. RSMS consists of two protocols: (1) One is a
blockchain-based lightweight mutual authentication protocol concerning
heterogeneous Metaverse service resource nodes (RNs) dynamically joining a
Metaverse service resource pool while guaranteeing their trustworthiness, which
guarantees the security of Metaverse service. (2) The other is a group
authentication protocol used to form and maintain a stable and secure Metaverse
service group composed by RNs, which ensures the reliability and enhances the
security of Metaverse service. The reliability and security of Metaverse
service under RSMS are thoroughly discussed, and informal and formal security
analysis are conducted. Additionally, we study the impact of RSMS on Metaverse
service throughput, demonstrating its lightweight feature.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05034" title="Abstract">arXiv:2310.05034</a> [<a href="/pdf/2310.05034" title="Download PDF">pdf</a>, <a href="/format/2310.05034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning Based Cross-Layer Design in Terahertz Mesh  Backhaul Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhifeng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chong Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xudong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Supporting ultra-high data rates and flexible reconfigurability, Terahertz
(THz) mesh networks are attractive for next-generation wireless backhaul
systems that empower the integrated access and backhaul (IAB). In THz mesh
backhaul networks, the efficient cross-layer routing and long-term resource
allocation is yet an open problem due to dynamic traffic demands as well as
possible link failures caused by the high directivity and high
non-line-of-sight (NLoS) path loss of THz spectrum. In addition, unpredictable
data traffic and the mixed integer programming property with the NP-hard nature
further challenge the effective routing and long-term resource allocation
design. In this paper, a deep reinforcement learning (DRL) based cross-layer
design in THz mesh backhaul networks (DEFLECT) is proposed, by considering
dynamic traffic demands and possible sudden link failures. In DEFLECT, a
heuristic routing metric is first devised to facilitate resource efficiency
(RE) enhancement regarding energy and sub-array usages. Furthermore, a DRL
based resource allocation algorithm is developed to realize long-term RE
maximization and fast recovery from broken links. Specifically in the DRL
method, the exploited multi-task structure cooperatively benefits joint power
and sub-array allocation. Additionally, the leveraged hierarchical architecture
realizes tailored resource allocation for each base station and learned
knowledge transfer for fast recovery. Simulation results show that DEFLECT
routing consumes less resource, compared to the minimal hop-count metric.
Moreover, unlike conventional DRL methods causing packet loss and second-level
latency, DEFLECT DRL realizes the long-term RE maximization with no packet loss
and millisecond-level latency, and recovers resource-efficient backhaul from
broken links within 1s.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05035" title="Abstract">arXiv:2310.05035</a> [<a href="/pdf/2310.05035" title="Download PDF">pdf</a>, <a href="/format/2310.05035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Convinced Prompting: Few-Shot Question Answering with Repeated  Introspection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haodi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+M">Min Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C+J">Chen Jason Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+R">Rui Mao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kaishun Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While large language models (LLMs) such as ChatGPT and PaLM have demonstrated
remarkable performance in various language understanding and generation tasks,
their capabilities in complex reasoning and intricate knowledge utilization
still fall short of human-level proficiency. Recent studies have established
the effectiveness of prompts in steering LLMs towards generating desired
outputs. Building on these insights, we introduce a novel framework that
harnesses the potential of large-scale pre-trained language models, to
iteratively enhance performance of the LLMs. Our framework incorporates three
components: \textit{Normal CoT}, a \textit{Convincer}, and an
\textit{Answerer}. It processes the output of a typical few-shot
chain-of-thought prompt, assesses the correctness of the response, scrutinizes
the answer, refines the reasoning, and ultimately produces a new solution.
Experimental results on the 7 datasets of miscellaneous problems validate the
efficacy of the Self-Convince framework, achieving substantial improvements
compared to the baselines. This study contributes to the burgeoning body of
research focused on integrating pre-trained language models with tailored
prompts and iterative refinement processes to augment their performance in
complex tasks.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05036" title="Abstract">arXiv:2310.05036</a> [<a href="/pdf/2310.05036" title="Download PDF">pdf</a>, <a href="/format/2310.05036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Text to Tactic: Evaluating LLMs Playing the Game of Avalon
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Light%2C+J">Jonathan Light</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+M">Min Cai</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Sheng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Ziniu Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In this paper, we explore the potential of Large Language Models (LLMs)
Agents in playing the strategic social deduction game, Resistance Avalon.
Players in Avalon are challenged not only to make informed decisions based on
dynamically evolving game phases, but also to engage in discussions where they
must deceive, deduce, and negotiate with other players. These characteristics
make Avalon a compelling test-bed to study the decision-making and
language-processing capabilities of LLM Agents. To facilitate research in this
line, we introduce AvalonBench - a comprehensive game environment tailored for
evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game
environment for Avalon, (2) rule-based bots as baseline opponents, and (3)
ReAct-style LLM agents with tailored prompts for each role. Notably, our
evaluations based on AvalonBench highlight a clear capability gap. For
instance, models like ChatGPT playing good-role got a win rate of 22.2% against
rule-based bots playing evil, while good-role bot achieves 38.2% win rate in
the same setting. We envision AvalonBench could be a good test-bed for
developing more advanced LLMs (with self-playing) and agent frameworks that can
effectively model the layered complexities of such game environments.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05041" title="Abstract">arXiv:2310.05041</a> [<a href="/pdf/2310.05041" title="Download PDF">pdf</a>, <a href="/format/2310.05041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Anomaly Behavior Analysis Framework for Securing Autonomous Vehicle  Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abrar%2C+M+M">Murad Mehrab Abrar</a>, 
<a href="/search/cs?searchtype=author&query=Hariri%2C+S">Salim Hariri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20th ACS/IEEE International Conference on Computer Systems and Applications (Accepted for publication)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">As a rapidly growing cyber-physical platform, Autonomous Vehicles (AVs) are
encountering more security challenges as their capabilities continue to expand.
In recent years, adversaries are actively targeting the perception sensors of
autonomous vehicles with sophisticated attacks that are not easily detected by
the vehicles' control systems. This work proposes an Anomaly Behavior Analysis
approach to detect a perception sensor attack against an autonomous vehicle.
The framework relies on temporal features extracted from a physics-based
autonomous vehicle behavior model to capture the normal behavior of vehicular
perception in autonomous driving. By employing a combination of model-based
techniques and machine learning algorithms, the proposed framework
distinguishes between normal and abnormal vehicular perception behavior. To
demonstrate the application of the framework in practice, we performed a depth
camera attack experiment on an autonomous vehicle testbed and generated an
extensive dataset. We validated the effectiveness of the proposed framework
using this real-world data and released the dataset for public access. To our
knowledge, this dataset is the first of its kind and will serve as a valuable
resource for the research community in evaluating their intrusion detection
techniques effectively.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05046" title="Abstract">arXiv:2310.05046</a> [<a href="/pdf/2310.05046" title="Download PDF">pdf</a>, <a href="/format/2310.05046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing the Power of ChatGPT in Fake News: An In-Depth Exploration in  Generation, Detection and Explanation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yue Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The rampant spread of fake news has adversely affected society, resulting in
extensive research on curbing its spread. As a notable milestone in large
language models (LLMs), ChatGPT has gained significant attention due to its
exceptional natural language processing capabilities. In this study, we present
a thorough exploration of ChatGPT's proficiency in generating, explaining, and
detecting fake news as follows. Generation -- We employ four prompt methods to
generate fake news samples and prove the high quality of these samples through
both self-assessment and human evaluation. Explanation -- We obtain nine
features to characterize fake news based on ChatGPT's explanations and analyze
the distribution of these factors across multiple public datasets. Detection --
We examine ChatGPT's capacity to identify fake news. We explore its detection
consistency and then propose a reason-aware prompt method to improve its
performance. Although our experiments demonstrate that ChatGPT shows
commendable performance in detecting fake news, there is still room for its
improvement. Consequently, we further probe into the potential extra
information that could bolster its effectiveness in detecting fake news.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05047" title="Abstract">arXiv:2310.05047</a> [<a href="/pdf/2310.05047" title="Download PDF">pdf</a>, <a href="/format/2310.05047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Learning in Contextual Second-Price Pay-Per-Click Auctions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengxiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haipeng Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study online learning in contextual pay-per-click auctions where at each
of the $T$ rounds, the learner receives some context along with a set of ads
and needs to make an estimate on their click-through rate (CTR) in order to run
a second-price pay-per-click auction. The learner's goal is to minimize her
regret, defined as the gap between her total revenue and that of an oracle
strategy that always makes perfect CTR predictions. We first show that
$\sqrt{T}$-regret is obtainable via a computationally inefficient algorithm and
that it is unavoidable since our algorithm is no easier than the classical
multi-armed bandit problem. A by-product of our results is a $\sqrt{T}$-regret
bound for the simpler non-contextual setting, improving upon a recent work of
[Feng et al., 2023] by removing the inverse CTR dependency that could be
arbitrarily large. Then, borrowing ideas from recent advances on efficient
contextual bandit algorithms, we develop two practically efficient contextual
auction algorithms: the first one uses the exponential weight scheme with
optimistic square errors and maintains the same $\sqrt{T}$-regret bound, while
the second one reduces the problem to online regression via a simple
epsilon-greedy strategy, albeit with a worse regret bound. Finally, we conduct
experiments on a synthetic dataset to showcase the effectiveness and superior
performance of our algorithms.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05051" title="Abstract">arXiv:2310.05051</a> [<a href="/pdf/2310.05051" title="Download PDF">pdf</a>, <a href="/format/2310.05051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SALT: Distinguishable Speaker Anonymization Through Latent Space  Transformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+Y">Yuanjun Lv</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jixun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Peikun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hongbin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Heng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lei Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures; Accepted by ASRU2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Speaker anonymization aims to conceal a speaker's identity without degrading
speech quality and intelligibility. Most speaker anonymization systems
disentangle the speaker representation from the original speech and achieve
anonymization by averaging or modifying the speaker representation. However,
the anonymized speech is subject to reduction in pseudo speaker
distinctiveness, speech quality and intelligibility for out-of-distribution
speaker. To solve this issue, we propose SALT, a Speaker Anonymization system
based on Latent space Transformation. Specifically, we extract latent features
by a self-supervised feature extractor and randomly sample multiple speakers
and their weights, and then interpolate the latent vectors to achieve speaker
anonymization. Meanwhile, we explore the extrapolation method to further extend
the diversity of pseudo speakers. Experiments on Voice Privacy Challenge
dataset show our system achieves a state-of-the-art distinctiveness metric
while preserving speech quality and intelligibility. Our code and demo is
availible at https://github.com/BakerBunker/SALT .
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05053" title="Abstract">arXiv:2310.05053</a> [<a href="/pdf/2310.05053" title="Download PDF">pdf</a>, <a href="/format/2310.05053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FP3O: Enabling Proximal Policy Optimization in Multi-Agent Cooperation  with Parameter-Sharing Versatility
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Lang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+D">Dong Xing</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+G">Gang Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Existing multi-agent PPO algorithms lack compatibility with different types
of parameter sharing when extending the theoretical guarantee of PPO to
cooperative multi-agent reinforcement learning (MARL). In this paper, we
propose a novel and versatile multi-agent PPO algorithm for cooperative MARL to
overcome this limitation. Our approach is achieved upon the proposed
full-pipeline paradigm, which establishes multiple parallel optimization
pipelines by employing various equivalent decompositions of the advantage
function. This procedure successfully formulates the interconnections among
agents in a more general manner, i.e., the interconnections among pipelines,
making it compatible with diverse types of parameter sharing. We provide a
solid theoretical foundation for policy improvement and subsequently develop a
practical algorithm called Full-Pipeline PPO (FP3O) by several approximations.
Empirical evaluations on Multi-Agent MuJoCo and StarCraftII tasks demonstrate
that FP3O outperforms other strong baselines and exhibits remarkable
versatility across various parameter-sharing configurations.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05054" title="Abstract">arXiv:2310.05054</a> [<a href="/pdf/2310.05054" title="Download PDF">pdf</a>, <a href="/ps/2310.05054" title="Download PostScript">ps</a>, <a href="/format/2310.05054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Latency Video Conferencing System for Geo-Distributed Data Centers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sitian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A+C">Amelie Chi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+R">Rui Mao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xuan Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to ICDE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">In the face of rising global demand for audio/video meetings, managing
traffic across geographically distributed (geo-distributed) data centers
presents a significant challenge due to the dynamic and limited nature of
inter-DC network performance. Facing these issues, this paper introduces two
novel techniques, VCRoute and WMJitter, to optimize the performance of
geo-distributed video conferencing systems. VCRoute is a routing method
designed for video conferencing data packets. It treats the routing problem as
a Multi-Armed Bandit issue, and utilizes a tailored Thompson Sampling algorithm
for resolution. Unlike traditional approaches, VCRoute uses predicted
end-to-end latency as the routing selection reward for each packet, enabling
effective and timely end-to-end latency optimization. In conjunction with
VCRoute, we present WMJitter, a watermark-based mechanism for managing network
jitter. Leveraging a window-based statistic method, WMJitter enables real-time
network jitter estimation, leading to significant reductions in end-to-end
delay and an improved balance between latency and loss rate. Evaluations based
on real geo-distributed network performance demonstrate the effectiveness and
scalability of VCRoute and WMJitter, offering robust solutions for optimizing
video conferencing systems in geo-distributed settings.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05055" title="Abstract">arXiv:2310.05055</a> [<a href="/pdf/2310.05055" title="Download PDF">pdf</a>, <a href="/format/2310.05055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in  Medical Image Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dutt%2C+R">Raman Dutt</a>, 
<a href="/search/cs?searchtype=author&query=Bohdal%2C+O">Ondrej Bohdal</a>, 
<a href="/search/cs?searchtype=author&query=Tsaftaris%2C+S+A">Sotirios A. Tsaftaris</a>, 
<a href="/search/cs?searchtype=author&query=Hospedales%2C+T">Timothy Hospedales</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 tables, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Training models with robust group fairness properties is crucial in ethically
sensitive application areas such as medical diagnosis. Despite the growing body
of work aiming to minimise demographic bias in AI, this problem remains
challenging. A key reason for this challenge is the fairness generalisation
gap: High-capacity deep learning models can fit all training data nearly
perfectly, and thus also exhibit perfect fairness during training. In this
case, bias emerges only during testing when generalisation performance differs
across subgroups. This motivates us to take a bi-level optimisation perspective
on fair learning: Optimising the learning strategy based on validation
fairness. Specifically, we consider the highly effective workflow of adapting
pre-trained models to downstream medical imaging tasks using
parameter-efficient fine-tuning (PEFT) techniques. There is a trade-off between
updating more parameters, enabling a better fit to the task of interest vs.
fewer parameters, potentially reducing the generalisation gap. To manage this
tradeoff, we propose FairTune, a framework to optimise the choice of PEFT
parameters with respect to fairness. We demonstrate empirically that FairTune
leads to improved fairness on a range of medical imaging datasets.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05056" title="Abstract">arXiv:2310.05056</a> [<a href="/pdf/2310.05056" title="Download PDF">pdf</a>, <a href="/format/2310.05056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-driven Open-Vocabulary Keypoint Detection for Animal Body and  Face
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lumin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+S">Shenqi Lai</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wenqi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Naning Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Current approaches for image-based keypoint detection on animal (including
human) body and face are limited to specific keypoints and species. We address
the limitation by proposing the Open-Vocabulary Keypoint Detection (OVKD) task.
It aims to use text prompts to localize arbitrary keypoints of any species. To
accomplish this objective, we propose Open-Vocabulary Keypoint Detection with
Semantic-feature Matching (KDSM), which utilizes both vision and language
models to harness the relationship between text and vision and thus achieve
keypoint detection through associating text prompt with relevant keypoint
features. Additionally, KDSM integrates domain distribution matrix matching and
some special designs to reinforce the relationship between language and vision,
thereby improving the model's generalizability and performance. Extensive
experiments show that our proposed components bring significant performance
improvements, and our overall method achieves impressive results in OVKD.
Remarkably, our method outperforms the state-of-the-art few-shot keypoint
detection methods using a zero-shot fashion. We will make the source code
publicly accessible.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05057" title="Abstract">arXiv:2310.05057</a> [<a href="/pdf/2310.05057" title="Download PDF">pdf</a>, <a href="/format/2310.05057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BRAINTEASER: Lateral Thinking Puzzles for Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yifan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ilievski%2C+F">Filip Ilievski</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+K">Kaixin Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The success of language models has inspired the NLP community to attend to
tasks that require implicit and complex reasoning, relying on human-like
commonsense mechanisms. While such vertical thinking tasks have been relatively
popular, lateral thinking puzzles have received little attention. To bridge
this gap, we devise BRAINTEASER: a multiple-choice Question Answering task
designed to test the model's ability to exhibit lateral thinking and defy
default commonsense associations. We design a three-step procedure for creating
the first lateral thinking benchmark, consisting of data collection, distractor
generation, and generation of adversarial examples, leading to 1,100 puzzles
with high-quality annotations. To assess the consistency of lateral reasoning
by models, we enrich BRAINTEASER based on a semantic and contextual
reconstruction of its questions. Our experiments with state-of-the-art
instruction- and commonsense language models reveal a significant gap between
human and model performance, which is further widened when consistency across
adversarial formats is considered. We make all of our code and data available
to stimulate work on developing and evaluating lateral thinking models.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05058" title="Abstract">arXiv:2310.05058</a> [<a href="/pdf/2310.05058" title="Download PDF">pdf</a>, <a href="/format/2310.05058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Separable Hidden Unit Contributions for Speaker-Adaptive  Lip-Reading
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Songtao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+S">Shiguang Shan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xilin Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to BMVC 2023 20pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this paper, we propose a novel method for speaker adaptation in lip
reading, motivated by two observations. Firstly, a speaker's own
characteristics can always be portrayed well by his/her few facial images or
even a single image with shallow networks, while the fine-grained dynamic
features associated with speech content expressed by the talking face always
need deep sequential networks to represent accurately. Therefore, we treat the
shallow and deep layers differently for speaker adaptive lip reading. Secondly,
we observe that a speaker's unique characteristics ( e.g. prominent oral cavity
and mandible) have varied effects on lip reading performance for different
words and pronunciations, necessitating adaptive enhancement or suppression of
the features for robust lip reading. Based on these two observations, we
propose to take advantage of the speaker's own characteristics to automatically
learn separable hidden unit contributions with different targets for shallow
layers and deep layers respectively. For shallow layers where features related
to the speaker's characteristics are stronger than the speech content related
features, we introduce speaker-adaptive features to learn for enhancing the
speech content features. For deep layers where both the speaker's features and
the speech content features are all expressed well, we introduce the
speaker-adaptive features to learn for suppressing the speech content
irrelevant noise for robust lip reading. Our approach consistently outperforms
existing methods, as confirmed by comprehensive analysis and comparison across
different settings. Besides the evaluation on the popular LRW-ID and GRID
datasets, we also release a new dataset for evaluation, CAS-VSR-S68h, to
further assess the performance in an extreme setting where just a few speakers
are available but the speech content covers a large and diversified range.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05060" title="Abstract">arXiv:2310.05060</a> [<a href="/pdf/2310.05060" title="Download PDF">pdf</a>, <a href="/format/2310.05060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video-CSR: Complex Video Digest Creation for Visual-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tingkai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Y">Yunzhe Tao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haogeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Q">Qihang Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Ding Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Huaibo Huang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ran He</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present a novel task and human annotated dataset for evaluating the
ability for visual-language models to generate captions and summaries for
real-world video clips, which we call Video-CSR (Captioning, Summarization and
Retrieval). The dataset contains 4.8K YouTube video clips of 20-60 seconds in
duration and covers a wide range of topics and interests. Each video clip
corresponds to 5 independently annotated captions (1 sentence) and summaries
(3-10 sentences). Given any video selected from the dataset and its
corresponding ASR information, we evaluate visual-language models on either
caption or summary generation that is grounded in both the visual and auditory
content of the video. Additionally, models are also evaluated on caption- and
summary-based retrieval tasks, where the summary-based retrieval task requires
the identification of a target video given excerpts of a corresponding summary.
Given the novel nature of the paragraph-length video summarization task, we
perform extensive comparative analyses of different existing evaluation metrics
and their alignment with human preferences. Finally, we propose a foundation
model with competitive generation and retrieval capabilities that serves as a
baseline for the Video-CSR task. We aim for Video-CSR to serve as a useful
evaluation set in the age of large language models and complex multi-modal
tasks.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05063" title="Abstract">arXiv:2310.05063</a> [<a href="/pdf/2310.05063" title="Download PDF">pdf</a>, <a href="/format/2310.05063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pushing the Limits of Pre-training for Time Series Forecasting in the  CloudOps Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Woo%2C+G">Gerald Woo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Akshat Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Sahoo%2C+D">Doyen Sahoo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Time series has been left behind in the era of pre-training and transfer
learning. While research in the fields of natural language processing and
computer vision are enjoying progressively larger datasets to train massive
models, the most popular time series datasets consist of only tens of thousands
of time steps, limiting our ability to study the effectiveness of pre-training
and scaling. Recent studies have also cast doubt on the need for expressive
models and scale. To alleviate these issues, we introduce three large-scale
time series forecasting datasets from the cloud operations (CloudOps) domain,
the largest having billions of observations, enabling further study into
pre-training and scaling of time series models. We build the empirical
groundwork for studying pre-training and scaling of time series models and pave
the way for future research by identifying a promising candidate architecture.
We show that it is a strong zero-shot baseline and benefits from further
scaling, both in model and dataset size. Accompanying these datasets and
results is a suite of comprehensive benchmark results comparing classical and
deep learning baselines to our pre-trained method - achieving a 27% reduction
in error on the largest dataset. Code and datasets will be released.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05064" title="Abstract">arXiv:2310.05064</a> [<a href="/pdf/2310.05064" title="Download PDF">pdf</a>, <a href="/format/2310.05064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> sign.mt: Real-Time Multilingual Sign Language Translation Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moryossef%2C+A">Amit Moryossef</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This demo paper presents sign.mt, an open-source application pioneering
real-time multilingual bi-directional translation between spoken and signed
languages. Harnessing state-of-the-art open-source models, this tool aims to
address the communication divide between the hearing and the deaf, facilitating
seamless translation in both spoken-to-signed and signed-to-spoken translation
directions.
<br />Promising reliable and unrestricted communication, sign.mt offers offline
functionality, crucial in areas with limited internet connectivity. It further
enhances user engagement by offering customizable photo-realistic sign language
avatars, thereby encouraging a more personalized and authentic user experience.
<br />Licensed under CC BY-NC-SA 4.0, sign.mt signifies an important stride towards
open, inclusive communication. The app can be used, and modified for personal
and academic uses, and even supports a translation API, fostering integration
into a wider range of applications. However, it is by no means a finished
product.
<br />We invite the NLP community to contribute towards the evolution of sign.mt.
Whether it be the integration of more refined models, the development of
innovative pipelines, or user experience improvements, your contributions can
propel this project to new heights. Available at https://sign.mt, it stands as
a testament to what we can achieve together, as we strive to make communication
accessible to all.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05066" title="Abstract">arXiv:2310.05066</a> [<a href="/pdf/2310.05066" title="Download PDF">pdf</a>, <a href="/format/2310.05066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guideline Learning for In-context Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+C">Chaoxu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yixuan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Q">Qiang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) can perform a new task by merely conditioning on
task instructions and a few input-output examples, without optimizing any
parameters. This is called In-Context Learning (ICL). In-context Information
Extraction has recently garnered attention in the research community. However,
current experiment results are generally suboptimal. We attribute this
primarily to the fact that the complex task settings and a variety of edge
cases are hard to be fully expressed in the length-limited context. In this
paper, we propose a Guideline Learning (GL) framework for In-context IE which
learns to generate and follow guidelines. During the learning phrase, GL
automatically synthesizes a set of guidelines from a few annotations, and
during inference, helpful guidelines are retrieved for better ICL.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05067" title="Abstract">arXiv:2310.05067</a> [<a href="/pdf/2310.05067" title="Download PDF">pdf</a>, <a href="/format/2310.05067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust-GBDT: A Novel Gradient Boosting Model for Noise-Robust  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jiaqi Luo</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+Y">Yuedong Quan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shixin Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Robust boosting algorithms have emerged as alternative solutions to
traditional boosting techniques for addressing label noise in classification
tasks. However, these methods have predominantly focused on binary
classification, limiting their applicability to multi-class tasks. Furthermore,
they encounter challenges with imbalanced datasets, missing values, and
computational efficiency. In this paper, we establish that the loss function
employed in advanced Gradient Boosting Decision Trees (GBDT), particularly
Newton's method-based GBDT, need not necessarily exhibit global convexity.
Instead, the loss function only requires convexity within a specific region.
Consequently, these GBDT models can leverage the benefits of nonconvex robust
loss functions, making them resilient to noise. Building upon this theoretical
insight, we introduce a new noise-robust boosting model called Robust-GBDT,
which seamlessly integrates the advanced GBDT framework with robust losses.
Additionally, we enhance the existing robust loss functions and introduce a
novel robust loss function, Robust Focal Loss, designed to address class
imbalance. As a result, Robust-GBDT generates more accurate predictions,
significantly enhancing its generalization capabilities, especially in
scenarios marked by label noise and class imbalance. Furthermore, Robust-GBDT
is user-friendly and can easily integrate existing open-source code, enabling
it to effectively handle complex datasets while improving computational
efficiency. Numerous experiments confirm the superiority of Robust-GBDT over
other noise-robust methods.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05069" title="Abstract">arXiv:2310.05069</a> [<a href="/pdf/2310.05069" title="Download PDF">pdf</a>, <a href="/format/2310.05069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot  Performance via Probability Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nie%2C+E">Ercong Nie</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+H">Helmut Schmid</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+H">Hinrich Sch&#xfc;tze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pretraiend multilingual encoder models can directly perform zero-shot
multilingual tasks or linguistic probing by reformulating the input examples
into cloze-style prompts. This is accomplished by predicting the probabilities
of the label words at the masked token position, without requiring any updates
to the model parameters. However, the performance of this pattern is limited by
the model's bias toward predicting label words which frequently occurred during
the pretraining. These words typically receive high probabilities. To address
this issue, we combine the models with various calibration techniques which
modify the probabilities of label words predicted by the models. We evaluate
the effectiveness of these calibration methods on monolingual encoders as well
as multilingual encoders. Across a diverse range of tasks, we achieve
substantial performance gains through calibration. Furthermore, with only very
few training samples, the trained calibration parameters are able to yield
additional enhancements.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05070" title="Abstract">arXiv:2310.05070</a> [<a href="/pdf/2310.05070" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CO-ASnet :A Smart Contract Architecture Design based on Blockchain  Technology with Active Sensor Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Feng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kun-peng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+C">Cang-long Pu</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Jiayin Qi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Figure 5, Table 2, 17 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The influence of opinion leaders impacts different aspects of social finance.
How to analyse the utility of opinion leaders' influence in realizing assets on
the blockchain and adopt a compliant regulatory scheme is worth exploring and
pondering. Taking Musk's call on social media to buy Dogecoin as an example,
this paper uses an event study to empirically investigate the phenomenon in
which opinion leaders use ICOs (initial coin offerings) to exert influence. The
results show that opinion leaders can use ICOs to influence the price of token
assets with money and data traffic in their social network. They can obtain
excess returns and reduce the cost of realization so that the closed loop of
influence realization will be accelerated. Based on this phenomenon and the
results of its impact, we use the ChainLink Oracle with Active Sensor
Networks(CO-ASnet) to design a safe and applicable decentralized regulatory
scheme that can constructively provide risk assessment strategies and early
warning measures for token issuance. The influence realization of opinion
leaders in blockchain issuance is bound to receive widespread attention, and
this paper will provide an exemplary reference for regulators and enterprises
to explore the boundaries of blockchain financial product development and
governance.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05072" title="Abstract">arXiv:2310.05072</a> [<a href="/pdf/2310.05072" title="Download PDF">pdf</a>, <a href="/format/2310.05072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Analysis of RIS-Aided Double Spatial Scattering Modulation  for mmWave MIMO Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xusheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+N">Nan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Fangjiong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Changle Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, we investigate a practical structure of reconfigurable
intelligent surface (RIS)-based double spatial scattering modulation (DSSM) for
millimeter-wave (mmWave) multiple-input multiple-output (MIMO) systems. A
suboptimal detector is proposed, in which the beam direction is first
demodulated according to the received beam strength, and then the remaining
information is demodulated by adopting the maximum likelihood algorithm. Based
on the proposed suboptimal detector, we derive the conditional pairwise error
probability expression. Further, the exact numerical integral and closed-form
expressions of unconditional pairwise error probability (UPEP) are derived via
two different approaches. To provide more insights, we derive the upper bound
and asymptotic expressions of UPEP. In addition, the diversity gain of the
RIS-DSSM scheme was also given. Furthermore, the union upper bound of average
bit error probability (ABEP) is obtained by combining the UPEP and the number
of error bits. Simulation results are provided to validate the derived upper
bound and asymptotic expressions of ABEP. We found an interesting phenomenon
that the ABEP performance of the proposed system-based phase shift keying is
better than that of the quadrature amplitude modulation. Additionally, the
performance advantage of ABEP is more significant with the increase in the
number of RIS elements.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05073" title="Abstract">arXiv:2310.05073</a> [<a href="/pdf/2310.05073" title="Download PDF">pdf</a>, <a href="/format/2310.05073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Argument Structure Extraction with Efficient Leverage of  Contextual Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fandong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Argument structure extraction (ASE) aims to identify the discourse structure
of arguments within documents. Previous research has demonstrated that
contextual information is crucial for developing an effective ASE model.
However, we observe that merely concatenating sentences in a contextual window
does not fully utilize contextual information and can sometimes lead to
excessive attention on less informative sentences. To tackle this challenge, we
propose an Efficient Context-aware ASE model (ECASE) that fully exploits
contextual information by enhancing modeling capacity and augmenting training
data. Specifically, we introduce a sequence-attention module and
distance-weighted similarity loss to aggregate contextual information and
argumentative information. Additionally, we augment the training data by
randomly masking discourse markers and sentences, which reduces the model's
reliance on specific words or less informative sentences. Our experiments on
five datasets from various domains demonstrate that our model achieves
state-of-the-art performance. Furthermore, ablation studies confirm the
effectiveness of each module in our model.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05074" title="Abstract">arXiv:2310.05074</a> [<a href="/pdf/2310.05074" title="Download PDF">pdf</a>, <a href="/format/2310.05074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chengcheng Han</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xiaowei Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Che Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+Y">Yixin Lian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Ming Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baoyuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the
reasoning capabilities of Large Language Models (LLMs) with at least 100
billion parameters. However, it is ineffective or even detrimental when applied
to reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion
parameters. To address this limitation, we introduce Dialogue-guided
Chain-of-Thought (DialCoT) which employs a dialogue format to generate
intermediate reasoning steps, guiding the model toward the final answer.
Additionally, we optimize the model's reasoning path selection using the
Proximal Policy Optimization (PPO) algorithm, further enhancing its reasoning
capabilities. Our method offers several advantages compared to previous
approaches. Firstly, we transform the process of solving complex reasoning
questions by breaking them down into a series of simpler sub-questions,
significantly reducing the task difficulty and making it more suitable for
SLMs. Secondly, we optimize the model's reasoning path selection through the
PPO algorithm. We conduct comprehensive experiments on four arithmetic
reasoning datasets, demonstrating that our method achieves significant
performance improvements compared to state-of-the-art competitors.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05076" title="Abstract">arXiv:2310.05076</a> [<a href="/pdf/2310.05076" title="Download PDF">pdf</a>, <a href="/format/2310.05076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Scalable Wireless Federated Learning: Challenges and Solutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuanming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Haibo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingjing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+L">Liqun Fu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by IEEE Internet of Things Magazine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
<p class="mathjax">The explosive growth of smart devices (e.g., mobile phones, vehicles, drones)
with sensing, communication, and computation capabilities gives rise to an
unprecedented amount of data. The generated massive data together with the
rapid advancement of machine learning (ML) techniques spark a variety of
intelligent applications. To distill intelligence for supporting these
applications, federated learning (FL) emerges as an effective distributed ML
framework, given its potential to enable privacy-preserving model training at
the network edge. In this article, we discuss the challenges and solutions of
achieving scalable wireless FL from the perspectives of both network design and
resource orchestration. For network design, we discuss how task-oriented model
aggregation affects the performance of wireless FL, followed by proposing
effective wireless techniques to enhance the communication scalability via
reducing the model aggregation distortion and improving the device
participation. For resource orchestration, we identify the limitations of the
existing optimization-based algorithms and propose three task-oriented learning
algorithms to enhance the algorithmic scalability via achieving
computation-efficient resource allocation for wireless FL. We highlight several
potential research issues that deserve further study.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05077" title="Abstract">arXiv:2310.05077</a> [<a href="/pdf/2310.05077" title="Download PDF">pdf</a>, <a href="/format/2310.05077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedFed: Feature Distillation against Data Heterogeneity in Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhiqin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yonggang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+X">Xinmei Tian</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated learning (FL) typically faces data heterogeneity, i.e.,
distribution shifting among clients. Sharing clients' information has shown
great potentiality in mitigating data heterogeneity, yet incurs a dilemma in
preserving privacy and promoting model performance. To alleviate the dilemma,
we raise a fundamental question: \textit{Is it possible to share partial
features in the data to tackle data heterogeneity?} In this work, we give an
affirmative answer to this question by proposing a novel approach called
{\textbf{Fed}erated \textbf{Fe}ature \textbf{d}istillation} (FedFed).
Specifically, FedFed partitions data into performance-sensitive features (i.e.,
greatly contributing to model performance) and performance-robust features
(i.e., limitedly contributing to model performance). The performance-sensitive
features are globally shared to mitigate data heterogeneity, while the
performance-robust features are kept locally. FedFed enables clients to train
models over local and shared data. Comprehensive experiments demonstrate the
efficacy of FedFed in promoting model performance.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05079" title="Abstract">arXiv:2310.05079</a> [<a href="/pdf/2310.05079" title="Download PDF">pdf</a>, <a href="/format/2310.05079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM  Inference?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Jianyi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Shumailov%2C+I">Ilia Shumailov</a>, 
<a href="/search/cs?searchtype=author&query=Constantinides%2C+G+A">George A. Constantinides</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yiren Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The inference of Large language models (LLMs) requires immense computation
and memory resources. To curtail these costs, quantisation has merged as a
promising solution, but existing LLM quantisation mainly focuses on 8-bit. In
this work, we explore the statistical and learning properties of the LLM layer
and attribute the bottleneck of LLM quantisation to numerical scaling offsets.
To address this, we adapt block quantisations for LLMs, a family of methods
that share scaling factors across packed numbers. Block quantisations
efficiently reduce the numerical scaling offsets solely from an arithmetic
perspective, without additional treatments in the computational path. Our
nearly-lossless quantised 6-bit LLMs achieve a $19\times$ higher arithmetic
density and $5\times$ memory density than the float32 baseline, surpassing the
prior art 8-bit quantisation by $2.5\times$ in arithmetic density and
$1.2\times$ in memory density, without requiring any data calibration or
re-training. We also share our insights into sub-8-bit LLM quantisation,
including the mismatch between activation and weight distributions, optimal
fine-tuning strategies, and a lower quantisation granularity inherent in the
statistical properties of LLMs. The latter two tricks enable nearly-lossless
4-bit LLMs on downstream tasks. The proposed framework will be open-sourced
upon publication.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05082" title="Abstract">arXiv:2310.05082</a> [<a href="/pdf/2310.05082" title="Download PDF">pdf</a>, <a href="/format/2310.05082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-head mutual Mean-Teaching for semi-supervised medical image  segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+R">Ruifeng Bian</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenyi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huihua Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Semi-supervised medical image segmentation (SSMIS) has witnessed substantial
advancements by leveraging limited labeled data and abundant unlabeled data.
Nevertheless, existing state-of-the-art methods encounter challenges in
accurately predicting labels for the unlabeled data, resulting in disruptive
noise during training and susceptibility to erroneous information overfitting.
Additionally, applying perturbations to inaccurate predictions further reduces
consistent learning. To address these concerns, a novel \textbf{C}ross-head
\textbf{m}utual \textbf{m}ean-\textbf{t}eaching Network (CMMT-Net) is proposed
to address these issues. The CMMT-Net comprises teacher-student networks and
incorporates strong-weak data augmentation within a shared encoder,
facilitating cross-head co-training by capitalizing on both self-training and
consistent learning. The consistent learning is enhanced by averaging teacher
networks and mutual virtual adversarial training, leading to deterministic and
higher-quality predictions. The diversity of consistency training samples can
be enhanced through the use of Cross-Set CutMix, which also helps mitigate
issues related to distribution mismatch. Notably, CMMT-Net simultaneously
implements data-level, feature-level, and network-level perturbations, boosting
model diversity and generalization performance. The proposed method
consistently outperforms existing SSMIS methods on three publicly available
datasets across various semi-supervised settings. Code and logs will be
available at \url{https://github.com/Leesoon1984/CMMT-Net}.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05083" title="Abstract">arXiv:2310.05083</a> [<a href="/pdf/2310.05083" title="Download PDF">pdf</a>, <a href="/format/2310.05083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLatS: Principled Out-of-Distribution Detection with Feature-Based  Likelihood Ratio Score
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haowei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuntian Gu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Detecting out-of-distribution (OOD) instances is crucial for NLP models in
practical applications. Although numerous OOD detection methods exist, most of
them are empirical. Backed by theoretical analysis, this paper advocates for
the measurement of the "OOD-ness" of a test case $\boldsymbol{x}$ through the
likelihood ratio between out-distribution $\mathcal P_{\textit{out}}$ and
in-distribution $\mathcal P_{\textit{in}}$. We argue that the state-of-the-art
(SOTA) feature-based OOD detection methods, such as Maha and KNN, are
suboptimal since they only estimate in-distribution density
$p_{\textit{in}}(\boldsymbol{x})$. To address this issue, we propose FLatS, a
principled solution for OOD detection based on likelihood ratio. Moreover, we
demonstrate that FLatS can serve as a general framework capable of enhancing
other OOD detection methods by incorporating out-distribution density
$p_{\textit{out}}(\boldsymbol{x})$ estimation. Experiments show that FLatS
establishes a new SOTA on popular benchmarks. Our code is publicly available at
https://github.com/linhaowei1/FLatS.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05084" title="Abstract">arXiv:2310.05084</a> [<a href="/pdf/2310.05084" title="Download PDF">pdf</a>, <a href="/ps/2310.05084" title="Download PostScript">ps</a>, <a href="/format/2310.05084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of multiphysics finite element method for quasi-static  thermo-poroelasticity with a nonlinear convective transport term
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ge%2C+Z">Zhihao Ge</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+D">Dandan Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">In this paper, we propose a multiphysics finite element method for a
quasi-static thermo-poroelasticity model with a nonlinear convective transport
term. To design some stable numerical methods and reveal the multi-physical
processes of deformation, diffusion and heat, we introduce three new variables
to reformulate the original model into a fluid coupled problem. Then, we
introduce an Newton's iterative algorithm by replacing the convective transport
term with $\nabla T^{i}\cdot(\bm{K}\nabla p^{i-1})$, $\nabla
T^{i-1}\cdot(\bm{K}\nabla p^{i})$ and $\nabla T^{i-1}\cdot(\bm{K}\nabla
p^{i-1})$, and apply the Banach fixed point theorem to prove the convergence of
the proposed method. Then, we propose a multiphysics finite element method with
Newton's iterative algorithm, which is equivalent to a stabilized method, can
effectively overcome the numerical oscillation caused by the nonlinear thermal
convection term. Also, we prove that the fully discrete multiphysics finite
element method has an optimal convergence order. Finally, we draw conclusions
to summarize the main results of this paper.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05086" title="Abstract">arXiv:2310.05086</a> [<a href="/pdf/2310.05086" title="Download PDF">pdf</a>, <a href="/format/2310.05086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Generalizable Agents via Saliency-Guided Features Decorrelation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Sili Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanchao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jifeng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Siyuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hechang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yi Chang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bo Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In visual-based Reinforcement Learning (RL), agents often struggle to
generalize well to environmental variations in the state space that were not
observed during training. The variations can arise in both task-irrelevant
features, such as background noise, and task-relevant features, such as robot
configurations, that are related to the optimal decisions. To achieve
generalization in both situations, agents are required to accurately understand
the impact of changed features on the decisions, i.e., establishing the true
associations between changed features and decisions in the policy model.
However, due to the inherent correlations among features in the state space,
the associations between features and decisions become entangled, making it
difficult for the policy to distinguish them. To this end, we propose
Saliency-Guided Features Decorrelation (SGFD) to eliminate these correlations
through sample reweighting. Concretely, SGFD consists of two core techniques:
Random Fourier Functions (RFF) and the saliency map. RFF is utilized to
estimate the complex non-linear correlations in high-dimensional images, while
the saliency map is designed to identify the changed features. Under the
guidance of the saliency map, SGFD employs sample reweighting to minimize the
estimated correlations related to changed features, thereby achieving
decorrelation in visual RL tasks. Our experimental results demonstrate that
SGFD can generalize well on a wide range of test environments and significantly
outperforms state-of-the-art methods in handling both task-irrelevant
variations and task-relevant variations.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05088" title="Abstract">arXiv:2310.05088</a> [<a href="/pdf/2310.05088" title="Download PDF">pdf</a>, <a href="/ps/2310.05088" title="Download PostScript">ps</a>, <a href="/format/2310.05088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe Exit Controllers Synthesis for Continuous-time Stochastic Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xue%2C+B">Bai Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper tackles the problem of generating safe exit controllers for
continuous-time systems described by stochastic differential equations (SDEs).
The primary aim is to develop controllers that maximize the lower bounds of the
exit probability that the system escapes from a safe but uncomfortable set
within a specified time frame and guide it towards a comfortable set. The paper
considers two distinct cases: one in which the boundary of the safe set is a
subset of the boundary of the uncomfortable set, and the other where the
boundaries of the two sets do not intersect. To begin, we present a sufficient
condition for establishing lower bounds on the exit probability in the first
case. This condition serves as a guideline for constructing an online linear
programming problem. The linear programming problem is designed to implicitly
synthesize an optimal exit controller that maximizes the lower bounds of the
exit probability. The method employed in the first case is then extended to the
second one. Finally, we demonstrate the effectiveness of the proposed
approaches on one example.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05091" title="Abstract">arXiv:2310.05091</a> [<a href="/pdf/2310.05091" title="Download PDF">pdf</a>, <a href="/ps/2310.05091" title="Download PostScript">ps</a>, <a href="/format/2310.05091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Privacy-Preserving Trajectory Synthesis Method Based on Vector  Translation Invariance Supporting Traffic Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zechen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+W">Wei Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuhan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">With the popularization of different kinds of smart terminals and the
development of autonomous driving technology, more and more services based on
spatio-temporal data have emerged in our lives, such as online taxi services,
traffic flow prediction, and tracking virus propagation. However, the privacy
concerns of spatio-temporal data greatly limit the use of them. To address this
issue, differential privacy method based on spatio-temporal data has been
proposed. In differential privacy, a good aggregation query can highly improve
the data utility. But the mainstream aggregation query methods are based on
area partitioning, which is difficult to generate trajectory with high utility
for they are hard to take time and constraints into account. Motivated by this,
we propose an aggregation query based on the relationships between
trajectories, so it can greatly improve the data utility as compared to the
existing methods. The trajectory synthesis task can be regarded as an
optimization problem of finding trajectories that match the relationships
between trajectories. We adopt gradient descent to find new trajectories that
meet the conditions, and during the gradient descent, we can easily take the
constraints into account by adding penalty terms which area partitioning based
query is hard to achieve. We carry out extensive experiments to validate that
the trajectories generated by our method have higher utility and the theoretic
analysis shows that our method is safe and reliable.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05092" title="Abstract">arXiv:2310.05092</a> [<a href="/pdf/2310.05092" title="Download PDF">pdf</a>, <a href="/format/2310.05092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Large Language Models with Augmented Instructions for  Fine-grained Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Huan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yice Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Changlong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Information Extraction (IE) is an essential task in Natural Language
Processing. Traditional methods have relied on coarse-grained extraction with
simple instructions. However, with the emergence of Large Language Models
(LLMs), there is a need to adapt IE techniques to leverage the capabilities of
these models. This paper introduces a fine-grained IE benchmark dataset
tailored for LLMs, employing augmented instructions for each information type,
which includes task descriptions, extraction rules, output formats, and
examples. Through extensive evaluations, we observe that encoder-decoder
models, particularly T5 and FLAN-T5, perform well in generalizing to unseen
information types, while ChatGPT exhibits greater adaptability to new task
forms. Our results also indicate that performance is not solely dictated by
model scale, and highlight the significance of architecture, data diversity,
and learning techniques. This work paves the way for a more refined and
versatile utilization of LLMs in Information Extraction.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05093" title="Abstract">arXiv:2310.05093</a> [<a href="/pdf/2310.05093" title="Download PDF">pdf</a>, <a href="/format/2310.05093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymmetrically Decentralized Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qinglun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Miao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+N">Nan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Q">Quanjun Yin</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">To address the communication burden and privacy concerns associated with the
centralized server in Federated Learning (FL), Decentralized Federated Learning
(DFL) has emerged, which discards the server with a peer-to-peer (P2P)
communication framework. However, most existing DFL algorithms are based on
symmetric topologies, such as ring and grid topologies, which can easily lead
to deadlocks and are susceptible to the impact of network link quality in
practice. To address these issues, this paper proposes the DFedSGPSM algorithm,
which is based on asymmetric topologies and utilizes the Push-Sum protocol to
effectively solve consensus optimization problems. To further improve algorithm
performance and alleviate local heterogeneous overfitting in Federated Learning
(FL), our algorithm combines the Sharpness Aware Minimization (SAM) optimizer
and local momentum. The SAM optimizer employs gradient perturbations to
generate locally flat models and searches for models with uniformly low loss
values, mitigating local heterogeneous overfitting. The local momentum
accelerates the optimization process of the SAM optimizer. Theoretical analysis
proves that DFedSGPSM achieves a convergence rate of
$\mathcal{O}(\frac{1}{\sqrt{T}})$ in a non-convex smooth setting under mild
assumptions. This analysis also reveals that better topological connectivity
achieves tighter upper bounds. Empirically, extensive experiments are conducted
on the MNIST, CIFAR10, and CIFAR100 datasets, demonstrating the superior
performance of our algorithm compared to state-of-the-art optimizers.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05095" title="Abstract">arXiv:2310.05095</a> [<a href="/pdf/2310.05095" title="Download PDF">pdf</a>, <a href="/format/2310.05095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Reliable Are AI-Generated-Text Detectors? An Assessment Framework  Using Evasive Soft Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumarage%2C+T">Tharindu Kumarage</a>, 
<a href="/search/cs?searchtype=author&query=Sheth%2C+P">Paras Sheth</a>, 
<a href="/search/cs?searchtype=author&query=Moraffah%2C+R">Raha Moraffah</a>, 
<a href="/search/cs?searchtype=author&query=Garland%2C+J">Joshua Garland</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent years, there has been a rapid proliferation of AI-generated text,
primarily driven by the release of powerful pre-trained language models (PLMs).
To address the issue of misuse associated with AI-generated text, various
high-performing detectors have been developed, including the OpenAI detector
and the Stanford DetectGPT. In our study, we ask how reliable these detectors
are. We answer the question by designing a novel approach that can prompt any
PLM to generate text that evades these high-performing detectors. The proposed
approach suggests a universal evasive prompt, a novel type of soft prompt,
which guides PLMs in producing "human-like" text that can mislead the
detectors. The novel universal evasive prompt is achieved in two steps: First,
we create an evasive soft prompt tailored to a specific PLM through prompt
tuning; and then, we leverage the transferability of soft prompts to transfer
the learned evasive soft prompt from one PLM to another. Employing multiple
PLMs in various writing tasks, we conduct extensive experiments to evaluate the
efficacy of the evasive soft prompts in their evasion of state-of-the-art
detectors.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05099" title="Abstract">arXiv:2310.05099</a> [<a href="/pdf/2310.05099" title="Download PDF">pdf</a>, <a href="/format/2310.05099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent DRL-Based Adaptive Region of Interest for Delay-sensitive  Telemedicine Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soliman%2C+A">Abdulrahman Soliman</a>, 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+A">Amr Mohamed</a>, 
<a href="/search/cs?searchtype=author&query=Yaacoub%2C+E">Elias Yaacoub</a>, 
<a href="/search/cs?searchtype=author&query=Navkar%2C+N+V">Nikhil V. Navkar</a>, 
<a href="/search/cs?searchtype=author&query=Erbad%2C+A">Aiman Erbad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Telemedicine applications have recently received substantial potential and
interest, especially after the COVID-19 pandemic. Remote experience will help
people get their complex surgery done or transfer knowledge to local surgeons,
without the need to travel abroad. Even with breakthrough improvements in
internet speeds, the delay in video streaming is still a hurdle in telemedicine
applications. This imposes using image compression and region of interest (ROI)
techniques to reduce the data size and transmission needs. This paper proposes
a Deep Reinforcement Learning (DRL) model that intelligently adapts the ROI
size and non-ROI quality depending on the estimated throughput. The delay and
structural similarity index measure (SSIM) comparison are used to assess the
DRL model. The comparison findings and the practical application reveal that
DRL is capable of reducing the delay by 13% and keeping the overall quality in
an acceptable range. Since the latency has been significantly reduced, these
findings are a valuable enhancement to telemedicine applications.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05102" title="Abstract">arXiv:2310.05102</a> [<a href="/pdf/2310.05102" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Federated Learning Algorithms Development Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Popovic%2C+M">Miroslav Popovic</a>, 
<a href="/search/cs?searchtype=author&query=Popovic%2C+M">Marko Popovic</a>, 
<a href="/search/cs?searchtype=author&query=Kastelan%2C+I">Ivan Kastelan</a>, 
<a href="/search/cs?searchtype=author&query=Djukic%2C+M">Miodrag Djukic</a>, 
<a href="/search/cs?searchtype=author&query=Basicevic%2C+I">Ilija Basicevic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 3 figures, 5 algorithms, submitted to ECBS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">At present many distributed and decentralized frameworks for federated
learning algorithms are already available. However, development of such a
framework targeting smart Internet of Things in edge systems is still an open
challenge. A solution to that challenge named Python Testbed for Federated
Learning Algorithms (PTB-FLA) appeared recently. This solution is written in
pure Python, it supports both centralized and decentralized algorithms, and its
usage was validated and illustrated by three simple algorithm examples. In this
paper, we present the federated learning algorithms development paradigm based
on PTB-FLA. The paradigm comprises the four phases named by the code they
produce: (1) the sequential code, (2) the federated sequential code, (3) the
federated sequential code with callbacks, and (4) the PTB-FLA code. The
development paradigm is validated and illustrated in the case study on logistic
regression, where both centralized and decentralized algorithms are developed.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05103" title="Abstract">arXiv:2310.05103</a> [<a href="/pdf/2310.05103" title="Download PDF">pdf</a>, <a href="/format/2310.05103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Detection of Machine-Generated Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xianjun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kexun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Petzold%2C+L">Linda Petzold</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">This work proposes a training-free approach for the detection of
LLMs-generated codes, mitigating the risks associated with their indiscriminate
usage. To the best of our knowledge, our research is the first to investigate
zero-shot detection techniques applied to code generated by advanced black-box
LLMs like ChatGPT. Firstly, we find that existing training-based or zero-shot
text detectors are ineffective in detecting code, likely due to the unique
statistical properties found in code structures. We then modify the previous
zero-shot text detection method, DetectGPT (Mitchell et al., 2023) by utilizing
a surrogate white-box model to estimate the probability of the rightmost
tokens, allowing us to identify code snippets generated by language models.
Through extensive experiments conducted on the python codes of the CodeContest
and APPS dataset, our approach demonstrates its effectiveness by achieving
state-of-the-art detection results on text-davinci-003, GPT-3.5, and GPT-4
models. Moreover, our method exhibits robustness against revision attacks and
generalizes well to Java codes. We also find that the smaller code language
model like PolyCoder-160M performs as a universal code detector, outperforming
the billion-scale counterpart. The codes will be available at
https://github.com/ Xianjun-Yang/Code_detection.git
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05105" title="Abstract">arXiv:2310.05105</a> [<a href="/pdf/2310.05105" title="Download PDF">pdf</a>, <a href="/format/2310.05105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Graph Neural Networks Learn: Lessons from Training Dynamics in  Function Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chenxiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qitian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wipf%2C+D">David Wipf</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruoyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A long-standing goal in deep learning has been to characterize the learning
behavior of black-box models in a more interpretable manner. For graph neural
networks (GNNs), considerable advances have been made in formalizing what
functions they can represent, however it remains less clear whether and how
GNNs learn desired functions during the optimization process. To fill this
critical gap, we study the learning dynamics of GNNs in function space via the
analytic framework of overparameterization. In particular, we find that the
seemingly complicated training process of GNNs can be re-cast into a more
familiar label propagation framework, due to the graph inductive bias implicit
in this process. From this vantage point, we provide explanations for why the
learned GNN functions successfully generalize and for their pathological
behavior on heterophilic graphs, which are consistent with observations.
Practically, sparsifying and implementing the learning dynamics lead to a
minimalist semi-supervised learning algorithm with the efficiency of classic
algorithms and the effectiveness of modern GNNs.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05107" title="Abstract">arXiv:2310.05107</a> [<a href="/pdf/2310.05107" title="Download PDF">pdf</a>, <a href="/format/2310.05107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OV-PARTS: Towards Open-Vocabulary Part Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+M">Meng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiaoyu Yue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+S">Shu Kong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xihui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+J">Jiangmiao Pang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS Dataset and Benchmark Track 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Segmenting and recognizing diverse object parts is a crucial ability in
applications spanning various computer vision and robotic tasks. While
significant progress has been made in object-level Open-Vocabulary Semantic
Segmentation (OVSS), i.e., segmenting objects with arbitrary text, the
corresponding part-level research poses additional challenges. Firstly, part
segmentation inherently involves intricate boundaries, while limited annotated
data compounds the challenge. Secondly, part segmentation introduces an open
granularity challenge due to the diverse and often ambiguous definitions of
parts in the open world. Furthermore, the large-scale vision and language
models, which play a key role in the open vocabulary setting, struggle to
recognize parts as effectively as objects. To comprehensively investigate and
tackle these challenges, we propose an Open-Vocabulary Part Segmentation
(OV-PARTS) benchmark. OV-PARTS includes refined versions of two publicly
available datasets: Pascal-Part-116 and ADE20K-Part-234. And it covers three
specific tasks: Generalized Zero-Shot Part Segmentation, Cross-Dataset Part
Segmentation, and Few-Shot Part Segmentation, providing insights into
analogical reasoning, open granularity and few-shot adapting abilities of
models. Moreover, we analyze and adapt two prevailing paradigms of existing
object-level OVSS methods for OV-PARTS. Extensive experimental analysis is
conducted to inspire future research in leveraging foundational models for
OV-PARTS. The code and dataset are available at
https://github.com/OpenRobotLab/OV_PARTS.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05108" title="Abstract">arXiv:2310.05108</a> [<a href="/pdf/2310.05108" title="Download PDF">pdf</a>, <a href="/format/2310.05108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Representations through Heterogeneous Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhong-Yu Li</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+B">Bo-Wen Yin</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Shanghua Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yongxiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Li Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Ming-Ming Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Incorporating heterogeneous representations from different architectures has
facilitated various vision tasks, e.g., some hybrid networks combine
transformers and convolutions. However, complementarity between such
heterogeneous architectures has not been well exploited in self-supervised
learning. Thus, we propose Heterogeneous Self-Supervised Learning (HSSL), which
enforces a base model to learn from an auxiliary head whose architecture is
heterogeneous from the base model. In this process, HSSL endows the base model
with new characteristics in a representation learning way without structural
changes. To comprehensively understand the HSSL, we conduct experiments on
various heterogeneous pairs containing a base model and an auxiliary head. We
discover that the representation quality of the base model moves up as their
architecture discrepancy grows. This observation motivates us to propose a
search strategy that quickly determines the most suitable auxiliary head for a
specific base model to learn and several simple but effective methods to
enlarge the model discrepancy. The HSSL is compatible with various
self-supervised methods, achieving superior performances on various downstream
tasks, including image classification, semantic segmentation, instance
segmentation, and object detection. Our source code will be made publicly
available.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05109" title="Abstract">arXiv:2310.05109</a> [<a href="/pdf/2310.05109" title="Download PDF">pdf</a>, <a href="/format/2310.05109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lightweight In-Context Tuning for Multimodal Unified Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yixin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Boran Han</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jiaya Jia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In-context learning (ICL) involves reasoning from given contextual examples.
As more modalities comes, this procedure is becoming more challenging as the
interleaved input modalities convolutes the understanding process. This is
exemplified by the observation that multimodal models often struggle to
effectively extrapolate from contextual examples to perform ICL. To address
these challenges, we introduce MultiModal In-conteXt Tuning (M$^2$IXT), a
lightweight module to enhance the ICL capabilities of multimodal unified
models. The proposed M$^2$IXT module perceives an expandable context window to
incorporate various labeled examples of multiple modalities (e.g., text, image,
and coordinates). It can be prepended to various multimodal unified models
(e.g., OFA, Unival, LLaVA) of different architectures and trained via a
mixed-tasks strategy to enable rapid few-shot adaption on multiple tasks and
datasets. When tuned on as little as 50K multimodal data, M$^2$IXT can boost
the few-shot ICL performance significantly (e.g., 18\% relative increase for
OFA), and obtained state-of-the-art results across an array of tasks including
visual question answering, image captioning, visual grounding, and visual
entailment, while being considerably small in terms of model parameters (e.g.,
$\sim$$20\times$ smaller than Flamingo or MMICL), highlighting the flexibility
and effectiveness of M$^2$IXT as a multimodal in-context learner.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05115" title="Abstract">arXiv:2310.05115</a> [<a href="/pdf/2310.05115" title="Download PDF">pdf</a>, <a href="/format/2310.05115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking Down Word Semantics from Pre-trained Language Models through  Layer-wise Dimension Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+N">Nayoung Choi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Contextual word embeddings obtained from pre-trained language model (PLM)
have proven effective for various natural language processing tasks at the word
level. However, interpreting the hidden aspects within embeddings, such as
syntax and semantics, remains challenging. Disentangled representation learning
has emerged as a promising approach, which separates specific aspects into
distinct embeddings. Furthermore, different linguistic knowledge is believed to
be stored in different layers of PLM. This paper aims to disentangle semantic
sense from BERT by applying a binary mask to middle outputs across the layers,
without updating pre-trained parameters. The disentangled embeddings are
evaluated through binary classification to determine if the target word in two
different sentences has the same meaning. Experiments with cased
BERT$_{\texttt{base}}$ show that leveraging layer-wise information is effective
and disentangling semantic sense further improve performance.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05116" title="Abstract">arXiv:2310.05116</a> [<a href="/pdf/2310.05116" title="Download PDF">pdf</a>, <a href="/format/2310.05116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CARLG: Leveraging Contextual Clues and Role Correlations for Improving  Document-level Event Argument Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wanlong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+D">Dingyi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Li Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Hong Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Document-level event argument extraction (EAE) is a crucial but challenging
subtask in information extraction. Most existing approaches focus on the
interaction between arguments and event triggers, ignoring two critical points:
the information of contextual clues and the semantic correlations among
argument roles. In this paper, we propose the CARLG model, which consists of
two modules: Contextual Clues Aggregation (CCA) and Role-based Latent
Information Guidance (RLIG), effectively leveraging contextual clues and role
correlations for improving document-level EAE. The CCA module adaptively
captures and integrates contextual clues by utilizing context attention weights
from a pre-trained encoder. The RLIG module captures semantic correlations
through role-interactive encoding and provides valuable information guidance
with latent role representation. Notably, our CCA and RLIG modules are compact,
transplantable and efficient, which introduce no more than 1% new parameters
and can be easily equipped on other span-base methods with significant
performance boost. Extensive experiments on the RAMS, WikiEvents, and MLEE
datasets demonstrate the superiority of the proposed CARLG model. It
outperforms previous state-of-the-art approaches by 1.26 F1, 1.22 F1, and 1.98
F1, respectively, while reducing the inference time by 31%. Furthermore, we
provide detailed experimental analyses based on the performance gains and
illustrate the interpretability of our model.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05118" title="Abstract">arXiv:2310.05118</a> [<a href="/pdf/2310.05118" title="Download PDF">pdf</a>, <a href="/format/2310.05118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VITS-based Singing Voice Conversion System with DSPGAN post-processing  for SVCC2023
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yiquan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Meng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yi Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jihua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weifeng Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ASRU2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper presents the T02 team's system for the Singing Voice Conversion
Challenge 2023 (SVCC2023). Our system entails a VITS-based SVC model,
incorporating three modules: a feature extractor, a voice converter, and a
post-processor. Specifically, the feature extractor provides F0 contours and
extracts speaker-independent linguistic content from the input singing voice by
leveraging a HuBERT model. The voice converter is employed to recompose the
speaker timbre, F0, and linguistic content to generate the waveform of the
target speaker. Besides, to further improve the audio quality, a fine-tuned
DSPGAN vocoder is introduced to re-synthesise the waveform. Given the limited
target speaker data, we utilize a two-stage training strategy to adapt the base
model to the target speaker. During model adaptation, several tricks, such as
data augmentation and joint training with auxiliary singer data, are involved.
Official challenge results show that our system achieves superior performance,
especially in the cross-domain task, ranking 1st and 2nd in naturalness and
similarity, respectively. Further ablation justifies the effectiveness of our
system design.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05119" title="Abstract">arXiv:2310.05119</a> [<a href="/pdf/2310.05119" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Multi-Domain Knowledge Networks for Chest X-ray Report  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weihua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Youyuan Xue</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chaochao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Boumaraf%2C+S">Said Boumaraf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The automated generation of radiology diagnostic reports helps radiologists
make timely and accurate diagnostic decisions while also enhancing clinical
diagnostic efficiency. However, the significant imbalance in the distribution
of data between normal and abnormal samples (including visual and textual
biases) poses significant challenges for a data-driven task like automatically
generating diagnostic radiology reports. Therefore, we propose a Dynamic
Multi-Domain Knowledge(DMDK) network for radiology diagnostic report
generation. The DMDK network consists of four modules: Chest Feature
Extractor(CFE), Dynamic Knowledge Extractor(DKE), Specific Knowledge
Extractor(SKE), and Multi-knowledge Integrator(MKI) module. Specifically, the
CFE module is primarily responsible for extracting the unprocessed visual
medical features of the images. The DKE module is responsible for extracting
dynamic disease topic labels from the retrieved radiology diagnostic reports.
We then fuse the dynamic disease topic labels with the original visual features
of the images to highlight the abnormal regions in the original visual features
to alleviate the visual data bias problem. The SKE module expands upon the
conventional static knowledge graph to mitigate textual data biases and amplify
the interpretability capabilities of the model via domain-specific dynamic
knowledge graphs. The MKI distills all the knowledge and generates the final
diagnostic radiology report. We performed extensive experiments on two widely
used datasets, IU X-Ray and MIMIC-CXR. The experimental results demonstrate the
effectiveness of our method, with all evaluation metrics outperforming previous
state-of-the-art models.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05120" title="Abstract">arXiv:2310.05120</a> [<a href="/pdf/2310.05120" title="Download PDF">pdf</a>, <a href="/format/2310.05120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Loop Synthesis for Quadratic Invariants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hitarth%2C+S">S. Hitarth</a>, 
<a href="/search/cs?searchtype=author&query=Kenison%2C+G">George Kenison</a>, 
<a href="/search/cs?searchtype=author&query=Kov%C3%A1cs%2C+L">Laura Kov&#xe1;cs</a>, 
<a href="/search/cs?searchtype=author&query=Varonka%2C+A">Anton Varonka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Number Theory (math.NT)

</div>
<p class="mathjax">Invariants are key to formal loop verification as they capture loop
properties that are valid before and after each loop iteration. Yet, generating
invariants is a notorious task already for syntactically restricted classes of
loops. Rather than generating invariants for given loops, in this paper we
synthesise loops that exhibit a predefined behaviour given by an invariant.
From the perspective of formal loop verification, the synthesised loops are
thus correct by design and no longer need to be verified.
<br />To overcome the hardness of reasoning with arbitrarily strong invariants, in
this paper we construct simple (non-nested) while loops with linear updates
that exhibit polynomial equality invariants. Rather than solving arbitrary
polynomial equations, we consider loop properties defined by a single quadratic
invariant in any number of variables. We present a procedure that, given a
quadratic equation, decides whether a loop with affine updates satisfying this
equation exists. Furthermore, if the answer is positive, the procedure
synthesises a loop and ensures its variables achieve infinitely many different
values.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05123" title="Abstract">arXiv:2310.05123</a> [<a href="/pdf/2310.05123" title="Download PDF">pdf</a>, <a href="/format/2310.05123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distribution-Based Trajectory Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z+J">Zi Jing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Ye Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ting%2C+K+M">Kai Ming Ting</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Trajectory clustering enables the discovery of common patterns in trajectory
data. Current methods of trajectory clustering rely on a distance measure
between two points in order to measure the dissimilarity between two
trajectories. The distance measures employed have two challenges: high
computational cost and low fidelity. Independent of the distance measure
employed, existing clustering algorithms have another challenge: either
effectiveness issues or high time complexity. In this paper, we propose to use
a recent Isolation Distributional Kernel (IDK) as the main tool to meet all
three challenges. The new IDK-based clustering algorithm, called TIDKC, makes
full use of the distributional kernel for trajectory similarity measuring and
clustering. TIDKC identifies non-linearly separable clusters with irregular
shapes and varied densities in linear time. It does not rely on random
initialisation and is robust to outliers. An extensive evaluation on 7 large
real-world trajectory datasets confirms that IDK is more effective in capturing
complex structures in trajectories than traditional and deep learning-based
distance measures. Furthermore, the proposed TIDKC has superior clustering
performance and efficiency to existing trajectory clustering algorithms.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05124" title="Abstract">arXiv:2310.05124</a> [<a href="/pdf/2310.05124" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-domain Robust Deepfake Bias Expansion Network for Face Forgery  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weihua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chaochao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Boumaraf%2C+S">Said Boumaraf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The rapid advancement of deepfake technologies raises significant concerns
about the security of face recognition systems. While existing methods leverage
the clues left by deepfake techniques for face forgery detection, malicious
users may intentionally manipulate forged faces to obscure the traces of
deepfake clues and thereby deceive detection tools. Meanwhile, attaining
cross-domain robustness for data-based methods poses a challenge due to
potential gaps in the training data, which may not encompass samples from all
relevant domains. Therefore, in this paper, we introduce a solution - a
Cross-Domain Robust Bias Expansion Network (BENet) - designed to enhance face
forgery detection. BENet employs an auto-encoder to reconstruct input faces,
maintaining the invariance of real faces while selectively enhancing the
difference between reconstructed fake faces and their original counterparts.
This enhanced bias forms a robust foundation upon which dependable forgery
detection can be built. To optimize the reconstruction results in BENet, we
employ a bias expansion loss infused with contrastive concepts to attain the
aforementioned objective. In addition, to further heighten the amplification of
forged clues, BENet incorporates a Latent-Space Attention (LSA) module. This
LSA module effectively captures variances in latent features between the
auto-encoder's encoder and decoder, placing emphasis on inconsistent
forgery-related information. Furthermore, BENet incorporates a cross-domain
detector with a threshold to determine whether the sample belongs to a known
distribution. The correction of classification results through the cross-domain
detector enables BENet to defend against unknown deepfake attacks from
cross-domain. Extensive experiments demonstrate the superiority of BENet
compared with state-of-the-art methods in intra-database and cross-database
evaluations.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05125" title="Abstract">arXiv:2310.05125</a> [<a href="/pdf/2310.05125" title="Download PDF">pdf</a>, <a href="/format/2310.05125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bidirectional Knowledge Reconfiguration for Lightweight Point Cloud  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peipei Li</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+X">Xing Cui</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yibo Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Man Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+T">Ting Yao</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+T">Tao Mei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Multimedia (TMM)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Multimedia ( Early Access ), 02 October 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Point cloud analysis faces computational system overhead, limiting its
application on mobile or edge devices. Directly employing small models may
result in a significant drop in performance since it is difficult for a small
model to adequately capture local structure and global shape information
simultaneously, which are essential clues for point cloud analysis. This paper
explores feature distillation for lightweight point cloud models. To mitigate
the semantic gap between the lightweight student and the cumbersome teacher, we
propose bidirectional knowledge reconfiguration (BKR) to distill informative
contextual knowledge from the teacher to the student. Specifically, a top-down
knowledge reconfiguration and a bottom-up knowledge reconfiguration are
developed to inherit diverse local structure information and consistent global
shape knowledge from the teacher, respectively. However, due to the farthest
point sampling in most point cloud models, the intermediate features between
teacher and student are misaligned, deteriorating the feature distillation
performance. To eliminate it, we propose a feature mover's distance (FMD) loss
based on optimal transportation, which can measure the distance between
unordered point cloud features effectively. Extensive experiments conducted on
shape classification, part segmentation, and semantic segmentation benchmarks
demonstrate the universality and superiority of our method.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05126" title="Abstract">arXiv:2310.05126</a> [<a href="/pdf/2310.05126" title="Download PDF">pdf</a>, <a href="/format/2310.05126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UReader: Universal OCR-free Visually-situated Language Understanding  with Multimodal Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jiabo Ye</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+A">Anwen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qinghao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guohai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Junfeng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Q">Qi Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qin Jin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Liang He</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X+A">Xin Alex Lin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Text is ubiquitous in our visual world, conveying crucial information, such
as in documents, websites, and everyday photographs. In this work, we propose
UReader, a first exploration of universal OCR-free visually-situated language
understanding based on the Multimodal Large Language Model (MLLM). By
leveraging the shallow text recognition ability of the MLLM, we only finetuned
1.2% parameters and the training cost is much lower than previous work
following domain-specific pretraining and finetuning paradigms. Concretely,
UReader is jointly finetuned on a wide range of Visually-situated Language
Understanding tasks via a unified instruction format. To enhance the visual
text and semantic understanding, we further apply two auxiliary tasks with the
same format, namely text reading and key points generation tasks. We design a
shape-adaptive cropping module before the encoder-decoder architecture of MLLM
to leverage the frozen low-resolution vision encoder for processing
high-resolution images. Without downstream finetuning, our single model
achieves state-of-the-art ocr-free performance in 8 out of 10 visually-situated
language understanding tasks, across 5 domains: documents, tables, charts,
natural images, and webpage screenshots. Codes and instruction-tuning datasets
will be released.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05128" title="Abstract">arXiv:2310.05128</a> [<a href="/pdf/2310.05128" title="Download PDF">pdf</a>, <a href="/format/2310.05128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instances and Labels: Hierarchy-aware Joint Supervised Contrastive  Learning for Hierarchical Multi-Label Text Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=U%2C+S+C+L">Simon Chi Lok U</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jie He</a>, 
<a href="/search/cs?searchtype=author&query=Guti%C3%A9rrez-Basulto%2C+V">V&#xed;ctor Guti&#xe9;rrez-Basulto</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J+Z">Jeff Z. Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as findings at EMNLP-2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Hierarchical multi-label text classification (HMTC) aims at utilizing a label
hierarchy in multi-label classification. Recent approaches to HMTC deal with
the problem of imposing an overconstrained premise on the output space by using
contrastive learning on generated samples in a semi-supervised manner to bring
text and label embeddings closer. However, the generation of samples tends to
introduce noise as it ignores the correlation between similar samples in the
same batch. One solution to this issue is supervised contrastive learning, but
it remains an underexplored topic in HMTC due to its complex structured labels.
To overcome this challenge, we propose HJCL, a \textbf{H}ierarchy-aware
\textbf{J}oint Supervised \textbf{C}ontrastive \textbf{L}earning method that
bridges the gap between supervised contrastive learning and HMTC. Specifically,
we employ both instance-wise and label-wise contrastive learning techniques and
carefully construct batches to fulfill the contrastive learning objective.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05129" title="Abstract">arXiv:2310.05129</a> [<a href="/pdf/2310.05129" title="Download PDF">pdf</a>, <a href="/format/2310.05129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ed-cec: improving rare word recognition using asr postprocessing based  on error detection and context-aware error correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiajun He</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zekun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Toda%2C+T">Tomoki Toda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Automatic speech recognition (ASR) systems often encounter difficulties in
accurately recognizing rare words, leading to errors that can have a negative
impact on downstream tasks such as keyword spotting, intent detection, and text
summarization. To address this challenge, we present a novel ASR postprocessing
method that focuses on improving the recognition of rare words through error
detection and context-aware error correction. Our method optimizes the decoding
process by targeting only the predicted error positions, minimizing unnecessary
computations. Moreover, we leverage a rare word list to provide additional
contextual knowledge, enabling the model to better correct rare words.
Experimental results across five datasets demonstrate that our proposed method
achieves significantly lower word error rates (WERs) than previous approaches
while maintaining a reasonable inference speed. Furthermore, our approach
exhibits promising robustness across different ASR systems.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05130" title="Abstract">arXiv:2310.05130</a> [<a href="/pdf/2310.05130" title="Download PDF">pdf</a>, <a href="/format/2310.05130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text  via Conditional Probability Curvature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+G">Guangsheng Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yanbin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+Z">Zhiyang Teng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures, 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have shown the ability to produce fluent and
cogent content, presenting both productivity opportunities and societal risks.
To build trustworthy AI systems, it is imperative to distinguish between
machine-generated and human-authored content. The leading zero-shot detector,
DetectGPT, showcases commendable performance but is marred by its intensive
computational costs. In this paper, we introduce the concept of conditional
probability curvature to elucidate discrepancies in word choices between LLMs
and humans within a given context. Utilizing this curvature as a foundational
metric, we present Fast-DetectGPT, an optimized zero-shot detector, which
substitutes DetectGPT's perturbation step with a more efficient sampling step.
Our evaluations on various datasets, source models, and test conditions
indicate that Fast-DetectGPT not only outperforms DetectGPT in both the
white-box and black-box settings but also accelerates the detection process by
a factor of 340, as detailed in Table 1.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05133" title="Abstract">arXiv:2310.05133</a> [<a href="/pdf/2310.05133" title="Download PDF">pdf</a>, <a href="/format/2310.05133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometry Aware Field-to-field Transformations for 3D Semantic  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hollidt%2C+D">Dominik Hollidt</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Clinton Wang</a>, 
<a href="/search/cs?searchtype=author&query=Golland%2C+P">Polina Golland</a>, 
<a href="/search/cs?searchtype=author&query=Pollefeys%2C+M">Marc Pollefeys</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a novel approach to perform 3D semantic segmentation solely from
2D supervision by leveraging Neural Radiance Fields (NeRFs). By extracting
features along a surface point cloud, we achieve a compact representation of
the scene which is sample-efficient and conducive to 3D reasoning. Learning
this feature space in an unsupervised manner via masked autoencoding enables
few-shot segmentation. Our method is agnostic to the scene parameterization,
working on scenes fit with any type of NeRF.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05134" title="Abstract">arXiv:2310.05134</a> [<a href="/pdf/2310.05134" title="Download PDF">pdf</a>, <a href="/format/2310.05134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LocoNeRF: A NeRF-based Approach for Local Structure from Motion for  Precise Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nenashev%2C+A">Artem Nenashev</a>, 
<a href="/search/cs?searchtype=author&query=Kurenkov%2C+M">Mikhail Kurenkov</a>, 
<a href="/search/cs?searchtype=author&query=Potapov%2C+A">Andrei Potapov</a>, 
<a href="/search/cs?searchtype=author&query=Zhura%2C+I">Iana Zhura</a>, 
<a href="/search/cs?searchtype=author&query=Katerishich%2C+M">Maksim Katerishich</a>, 
<a href="/search/cs?searchtype=author&query=Tsetserukou%2C+D">Dzmitry Tsetserukou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Visual localization is a critical task in mobile robotics, and researchers
are continuously developing new approaches to enhance its efficiency. In this
article, we propose a novel approach to improve the accuracy of visual
localization using Structure from Motion (SfM) techniques. We highlight the
limitations of global SfM, which suffers from high latency, and the challenges
of local SfM, which requires large image databases for accurate reconstruction.
To address these issues, we propose utilizing Neural Radiance Fields (NeRF), as
opposed to image databases, to cut down on the space required for storage. We
suggest that sampling reference images around the prior query position can lead
to further improvements. We evaluate the accuracy of our proposed method
against ground truth obtained using LIDAR and Advanced Lidar Odometry and
Mapping in Real-time (A-LOAM), and compare its storage usage against local SfM
with COLMAP in the conducted experiments. Our proposed method achieves an
accuracy of 0.068 meters compared to the ground truth, which is slightly lower
than the most advanced method COLMAP, which has an accuracy of 0.022 meters.
However, the size of the database required for COLMAP is 400 megabytes, whereas
the size of our NeRF model is only 160 megabytes. Finally, we perform an
ablation study to assess the impact of using reference images from the NeRF
reconstruction.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05135" title="Abstract">arXiv:2310.05135</a> [<a href="/pdf/2310.05135" title="Download PDF">pdf</a>, <a href="/format/2310.05135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Emily and Greg Still More Employable than Lakisha and Jamal?  Investigating Algorithmic Hiring Bias in the Era of ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Veldanda%2C+A+K">Akshaj Kumar Veldanda</a>, 
<a href="/search/cs?searchtype=author&query=Grob%2C+F">Fabian Grob</a>, 
<a href="/search/cs?searchtype=author&query=Thakur%2C+S">Shailja Thakur</a>, 
<a href="/search/cs?searchtype=author&query=Pearce%2C+H">Hammond Pearce</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+B">Benjamin Tan</a>, 
<a href="/search/cs?searchtype=author&query=Karri%2C+R">Ramesh Karri</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Siddharth Garg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) such as GPT-3.5, Bard, and Claude exhibit
applicability across numerous tasks. One domain of interest is their use in
algorithmic hiring, specifically in matching resumes with job categories. Yet,
this introduces issues of bias on protected attributes like gender, race and
maternity status. The seminal work of Bertrand &amp; Mullainathan (2003) set the
gold-standard for identifying hiring bias via field experiments where the
response rate for identical resumes that differ only in protected attributes,
e.g., racially suggestive names such as Emily or Lakisha, is compared. We
replicate this experiment on state-of-art LLMs (GPT-3.5, Bard, Claude and
Llama) to evaluate bias (or lack thereof) on gender, race, maternity status,
pregnancy status, and political affiliation. We evaluate LLMs on two tasks: (1)
matching resumes to job categories; and (2) summarizing resumes with employment
relevant information. Overall, LLMs are robust across race and gender. They
differ in their performance on pregnancy status and political affiliation. We
use contrastive input decoding on open-source LLMs to uncover potential sources
of bias.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05136" title="Abstract">arXiv:2310.05136</a> [<a href="/pdf/2310.05136" title="Download PDF">pdf</a>, <a href="/format/2310.05136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstructDET: Diversifying Referring Object Detection with Generalized  Instructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dang%2C+R">Ronghao Dang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiangyan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haodong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+C">Chongjian Ge</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Lin Song</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+L">Lijun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chengju Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qijun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Feng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Rui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yibing Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages (include appendix), under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We propose InstructDET, a data-centric method for referring object detection
(ROD) that localizes target objects based on user instructions. While deriving
from referring expressions (REC), the instructions we leverage are greatly
diversified to encompass common user intentions related to object detection.
For one image, we produce tremendous instructions that refer to every single
object and different combinations of multiple objects. Each instruction and its
corresponding object bounding boxes (bbxs) constitute one training data pair.
In order to encompass common detection expressions, we involve emerging
vision-language model (VLM) and large language model (LLM) to generate
instructions guided by text prompts and object bbxs, as the generalizations of
foundation models are effective to produce human-like expressions (e.g.,
describing object property, category, and relationship). We name our
constructed dataset as InDET. It contains images, bbxs and generalized
instructions that are from foundation models. Our InDET is developed from
existing REC datasets and object detection datasets, with the expanding
potential that any image with object bbxs can be incorporated through using our
InstructDET method. By using our InDET dataset, we show that a conventional ROD
model surpasses existing methods on standard REC datasets and our InDET test
set. Our data-centric method InstructDET, with automatic data expansion by
leveraging foundation models, directs a promising field that ROD can be greatly
diversified to execute common object detection instructions.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05139" title="Abstract">arXiv:2310.05139</a> [<a href="/pdf/2310.05139" title="Download PDF">pdf</a>, <a href="/format/2310.05139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximizing Utilitarian and Egalitarian Welfare of Fractional Hedonic  Games on Tree-like Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanaka%2C+T">Tesshu Hanaka</a>, 
<a href="/search/cs?searchtype=author&query=Ikeyama%2C+A">Airi Ikeyama</a>, 
<a href="/search/cs?searchtype=author&query=Ono%2C+H">Hirotaka Ono</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Fractional hedonic games are coalition formation games where a player's
utility is determined by the average value they assign to the members of their
coalition. These games are a variation of graph hedonic games, which are a
class of coalition formation games that can be succinctly represented. Due to
their applicability in network clustering and their relationship to graph
hedonic games, fractional hedonic games have been extensively studied from
various perspectives. However, finding welfare-maximizing partitions in
fractional hedonic games is a challenging task due to the nonlinearity of
utilities. In fact, it has been proven to be NP-hard and can be solved in
polynomial time only for a limited number of graph classes, such as trees. This
paper presents (pseudo)polynomial-time algorithms to compute welfare-maximizing
partitions in fractional hedonic games on tree-like graphs. We consider two
types of social welfare measures: utilitarian and egalitarian. Tree-like graphs
refer to graphs with bounded treewidth and block graphs. A hardness result is
provided, demonstrating that the pseudopolynomial-time solvability is the best
possible under the assumption P$\neq$NP.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05140" title="Abstract">arXiv:2310.05140</a> [<a href="/pdf/2310.05140" title="Download PDF">pdf</a>, <a href="/format/2310.05140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing the Power of Large Language Models for Empathetic Response  Generation: Empirical Investigations and Improvements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yushan Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei-Nan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Ting Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> the Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Empathetic dialogue is an indispensable part of building harmonious social
relationships and contributes to the development of a helpful AI. Previous
approaches are mainly based on fine small-scale language models. With the
advent of ChatGPT, the application effect of large language models (LLMs) in
this field has attracted great attention. This work empirically investigates
the performance of LLMs in generating empathetic responses and proposes three
improvement methods of semantically similar in-context learning, two-stage
interactive generation, and combination with the knowledge base. Extensive
experiments show that LLMs can significantly benefit from our proposed methods
and is able to achieve state-of-the-art performance in both automatic and human
evaluations. Additionally, we explore the possibility of GPT-4 simulating human
evaluators.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05141" title="Abstract">arXiv:2310.05141</a> [<a href="/pdf/2310.05141" title="Download PDF">pdf</a>, <a href="/format/2310.05141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transferable Availability Poisoning Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiyong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Backes%2C+M">Michael Backes</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider availability data poisoning attacks, where an adversary aims to
degrade the overall test accuracy of a machine learning model by crafting small
perturbations to its training data. Existing poisoning strategies can achieve
the attack goal but assume the victim to employ the same learning method as
what the adversary uses to mount the attack. In this paper, we argue that this
assumption is strong, since the victim may choose any learning algorithm to
train the model as long as it can achieve some targeted performance on clean
data. Empirically, we observe a large decrease in the effectiveness of prior
poisoning attacks if the victim uses a different learning paradigm to train the
model and show marked differences in frequency-level characteristics between
perturbations generated with respect to different learners and attack methods.
To enhance the attack transferability, we propose Transferable Poisoning, which
generates high-frequency poisoning perturbations by alternately leveraging the
gradient information with two specific algorithms selected from supervised and
unsupervised contrastive learning paradigms. Through extensive experiments on
benchmark image datasets, we show that our transferable poisoning attack can
produce poisoned samples with significantly improved transferability, not only
applicable to the two learners used to devise the attack but also for learning
algorithms and even paradigms beyond.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05142" title="Abstract">arXiv:2310.05142</a> [<a href="/pdf/2310.05142" title="Download PDF">pdf</a>, <a href="/ps/2310.05142" title="Download PostScript">ps</a>, <a href="/format/2310.05142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secure Short-Packet Transmission with Aerial Relaying: Blocklength and  Trajectory Co-Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mamaghani%2C+M+T">Milad Tatar Mamaghani</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiangyun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+N">Nan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Swindlehurst%2C+A+L">A. Lee Swindlehurst</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, 1 table, Accepted by IEEE Global Communications Conference, 4-8 December 2023, Kuala Lumpur, Malaysia
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, we propose a secure short-packet communication (SPC) system
involving an unmanned aerial vehicle (UAV)-aided relay in the presence of a
terrestrial passive eavesdropper. The considered system, which is applicable to
various next-generation Internet-of-Things (IoT) networks, exploits a UAV as a
mobile relay, facilitating the reliable and secure exchange of intermittent
short packets between a pair of remote IoT devices with strict latency. Our
objective is to improve the overall secrecy throughput performance of the
system by carefully designing key parameters such as the coding blocklengths
and the UAV trajectory. However, this inherently poses a challenging
optimization problem that is difficult to solve optimally. To address the
issue, we propose a low-complexity algorithm inspired by the block successive
convex approximation approach, where we divide the original problem into two
subproblems and solve them alternately until convergence. Numerical results
demonstrate that the proposed design achieves significant performance
improvements relative to other benchmarks, and offer valuable insights into
determining appropriate coding blocklengths and UAV trajectory.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05143" title="Abstract">arXiv:2310.05143</a> [<a href="/pdf/2310.05143" title="Download PDF">pdf</a>, <a href="/format/2310.05143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ZooPFL: Exploring Black-box Foundation Models for Personalized Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Teney%2C+D">Damien Teney</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haohan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiqiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+X">Xiangyang Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report; 26 pages; code will be available at <a href="https://github.com/microsoft/PersonalizedFL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">When personalized federated learning (FL) meets large foundation models, new
challenges arise from various limitations in resources. In addition to typical
limitations such as data, computation, and communication costs, access to the
models is also often limited. This paper endeavors to solve both the challenges
of limited resources and personalization. i.e., distribution shifts between
clients. To do so, we propose a method named ZOOPFL that uses Zeroth-Order
Optimization for Personalized Federated Learning. ZOOPFL avoids direct
interference with the foundation models and instead learns to adapt its inputs
through zeroth-order optimization. In addition, we employ simple yet effective
linear projections to remap its predictions for personalization. To reduce the
computation costs and enhance personalization, we propose input surgery to
incorporate an auto-encoder with low-dimensional and client-specific
embeddings. We provide theoretical support for ZOOPFL to analyze its
convergence. Extensive empirical experiments on computer vision and natural
language processing tasks using popular foundation models demonstrate its
effectiveness for FL on black-box foundation models.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05145" title="Abstract">arXiv:2310.05145</a> [<a href="/pdf/2310.05145" title="Download PDF">pdf</a>, <a href="/ps/2310.05145" title="Download PostScript">ps</a>, <a href="/format/2310.05145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuralFastLAS: Fast Logic-Based Learning from Raw Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Charalambous%2C+T">Theo Charalambous</a>, 
<a href="/search/cs?searchtype=author&query=Aspis%2C+Y">Yaniv Aspis</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+A">Alessandra Russo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print, work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Symbolic rule learners generate interpretable solutions, however they require
the input to be encoded symbolically. Neuro-symbolic approaches overcome this
issue by mapping raw data to latent symbolic concepts using a neural network.
Training the neural and symbolic components jointly is difficult, due to slow
and unstable learning, hence many existing systems rely on hand-engineered
rules to train the network. We introduce NeuralFastLAS, a scalable and fast
end-to-end approach that trains a neural network jointly with a symbolic
learner. For a given task, NeuralFastLAS computes a relevant set of rules,
proved to contain an optimal symbolic solution, trains a neural network using
these rules, and finally finds an optimal symbolic solution to the task while
taking network predictions into account. A key novelty of our approach is
learning a posterior distribution on rules while training the neural network to
improve stability during training. We provide theoretical results for a
sufficient condition on network training to guarantee correctness of the final
solution. Experimental results demonstrate that NeuralFastLAS is able to
achieve state-of-the-art accuracy in arithmetic and logical tasks, with a
training time that is up to two orders of magnitude faster than other jointly
trained neuro-symbolic methods.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05146" title="Abstract">arXiv:2310.05146</a> [<a href="/pdf/2310.05146" title="Download PDF">pdf</a>, <a href="/format/2310.05146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model (LLM) as a System of Multiple Expert Agents: An  Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+J+C+M">John Chong Min Tan</a>, 
<a href="/search/cs?searchtype=author&query=Motani%2C+M">Mehul Motani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 main pages, 1 page references, 18 pages appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">We attempt to solve the Abstraction and Reasoning Corpus (ARC) Challenge
using Large Language Models (LLMs) as a system of multiple expert agents. Using
the flexibility of LLMs to be prompted to do various novel tasks using
zero-shot, few-shot, context-grounded prompting, we explore the feasibility of
using LLMs to solve the ARC Challenge. We firstly convert the input image into
multiple suitable text-based abstraction spaces. We then utilise the
associative power of LLMs to derive the input-output relationship and map this
to actions in the form of a working program, similar to Voyager / Ghost in the
MineCraft. In addition, we use iterative environmental feedback in order to
guide LLMs to solve the task. Our proposed approach achieves 50 solves out of
111 training set problems (45%) with just three abstraction spaces - grid,
object and pixel - and we believe that with more abstraction spaces and
learnable actions, we will be able to solve more.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05149" title="Abstract">arXiv:2310.05149</a> [<a href="/pdf/2310.05149" title="Download PDF">pdf</a>, <a href="/format/2310.05149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval-Generation Synergy Augmented Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhangyin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+X">Xiaocheng Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dezhi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Maojin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bing Qin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models augmented with task-relevant documents have
demonstrated impressive performance on knowledge-intensive tasks. However,
regarding how to obtain effective documents, the existing methods are mainly
divided into two categories. One is to retrieve from an external knowledge
base, and the other is to utilize large language models to generate documents.
We propose an iterative retrieval-generation collaborative framework. It is not
only able to leverage both parametric and non-parametric knowledge, but also
helps to find the correct reasoning path through retrieval-generation
interactions, which is very important for tasks that require multi-step
reasoning. We conduct experiments on four question answering datasets,
including single-hop QA and multi-hop QA tasks. Empirical results show that our
method significantly improves the reasoning ability of large language models
and outperforms previous baselines.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05150" title="Abstract">arXiv:2310.05150</a> [<a href="/pdf/2310.05150" title="Download PDF">pdf</a>, <a href="/format/2310.05150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Data to Dialogue: Leveraging the Structure of Knowledge Graphs for  Conversational Exploratory Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schneider%2C+P">Phillip Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Rehtanz%2C+N">Nils Rehtanz</a>, 
<a href="/search/cs?searchtype=author&query=Jokinen%2C+K">Kristiina Jokinen</a>, 
<a href="/search/cs?searchtype=author&query=Matthes%2C+F">Florian Matthes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to PACLIC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Exploratory search is an open-ended information retrieval process that aims
at discovering knowledge about a topic or domain rather than searching for a
specific answer or piece of information. Conversational interfaces are
particularly suitable for supporting exploratory search, allowing users to
refine queries and examine search results through interactive dialogues. In
addition to conversational search interfaces, knowledge graphs are also useful
in supporting information exploration due to their rich semantic representation
of data items. In this study, we demonstrate the synergistic effects of
combining knowledge graphs and conversational interfaces for exploratory
search, bridging the gap between structured and unstructured information
retrieval. To this end, we propose a knowledge-driven dialogue system for
exploring news articles by asking natural language questions and using the
graph structure to navigate between related topics. Based on a user study with
54 participants, we empirically evaluate the effectiveness of the graph-based
exploratory search and discuss design implications for developing such systems.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05155" title="Abstract">arXiv:2310.05155</a> [<a href="/pdf/2310.05155" title="Download PDF">pdf</a>, <a href="/format/2310.05155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on  Open-Source Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Cheng Qian</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Chenyan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated remarkable progress in
utilizing tools, but their closed-source nature and high inference costs pose
limitations on their adaptability, necessitating a valid method that leverages
smaller, open-sourced models. In this paper, we introduce Toolink, a
comprehensive framework that performs task-solving by first creating a toolkit
and then integrating the planning and calling of tools through a
chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in
harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we
curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and
finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source
model with advanced tool-planning and tool-calling capabilities. Evaluation on
diverse tasks from BIG-bench demonstrates its CoS ability matches that of
ChatGPT while its performance surpasses the chain-of-thought approach. Further
studies highlight the generalization of LLaMA-CoS to unseen tasks and showcase
its capability in using toolkits not explicitly tailored for the target task,
affirming its robustness in real-world scenarios. All codes and data are
released.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05157" title="Abstract">arXiv:2310.05157</a> [<a href="/pdf/2310.05157" title="Download PDF">pdf</a>, <a href="/format/2310.05157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MenatQA: A New Dataset for Testing the Temporal Comprehension and  Reasoning Abilities of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yifan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yisong Su</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Huanhuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiaoyan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+F">Fangyu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuanzhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have shown nearly saturated performance on many
natural language processing (NLP) tasks. As a result, it is natural for people
to believe that LLMs have also mastered abilities such as time understanding
and reasoning. However, research on the temporal sensitivity of LLMs has been
insufficiently emphasized. To fill this gap, this paper constructs Multiple
Sensitive Factors Time QA (MenatQA), which encompasses three temporal factors
(scope factor, order factor, counterfactual factor) with total 2,853 samples
for evaluating the time comprehension and reasoning abilities of LLMs. This
paper tests current mainstream LLMs with different parameter sizes, ranging
from billions to hundreds of billions. The results show most LLMs fall behind
smaller temporal reasoning models with different degree on these factors. In
specific, LLMs show a significant vulnerability to temporal biases and depend
heavily on the temporal information provided in questions. Furthermore, this
paper undertakes a preliminary investigation into potential improvement
strategies by devising specific prompts and leveraging external tools. These
approaches serve as valuable baselines or references for future research
endeavors.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05158" title="Abstract">arXiv:2310.05158</a> [<a href="/pdf/2310.05158" title="Download PDF">pdf</a>, <a href="/format/2310.05158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ITRE: Low-light Image Enhancement Based on Illumination Transmission  Ratio Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yihong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+X">Xiubao Sui</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Noise, artifacts, and over-exposure are significant challenges in the field
of low-light image enhancement. Existing methods often struggle to address
these issues simultaneously. In this paper, we propose a novel Retinex-based
method, called ITRE, which suppresses noise and artifacts from the origin of
the model, prevents over-exposure throughout the enhancement process.
Specifically, we assume that there must exist a pixel which is least disturbed
by low light within pixels of same color. First, clustering the pixels on the
RGB color space to find the Illumination Transmission Ratio (ITR) matrix of the
whole image, which determines that noise is not over-amplified easily. Next, we
consider ITR of the image as the initial illumination transmission map to
construct a base model for refined transmission map, which prevents artifacts.
Additionally, we design an over-exposure module that captures the fundamental
characteristics of pixel over-exposure and seamlessly integrate it into the
base model. Finally, there is a possibility of weak enhancement when
inter-class distance of pixels with same color is too small. To counteract
this, we design a Robust-Guard module that safeguards the robustness of the
image enhancement process. Extensive experiments demonstrate the effectiveness
of our approach in suppressing noise, preventing artifacts, and controlling
over-exposure level simultaneously. Our method performs superiority in
qualitative and quantitative performance evaluations by comparing with
state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05161" title="Abstract">arXiv:2310.05161</a> [<a href="/pdf/2310.05161" title="Download PDF">pdf</a>, <a href="/format/2310.05161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recurrent Neural Language Models as Probabilistic Finite-state Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Svete%2C+A">Anej Svete</a>, 
<a href="/search/cs?searchtype=author&query=Cotterell%2C+R">Ryan Cotterell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computational Complexity (cs.CC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Studying language models (LMs) in terms of well-understood formalisms allows
us to precisely characterize their abilities and limitations. Previous work has
investigated the representational capacity of recurrent neural network (RNN)
LMs in terms of their capacity to recognize unweighted formal languages.
However, LMs do not describe unweighted formal languages -- rather, they define
probability distributions over strings. In this work, we study what classes of
such probability distributions RNN LMs can represent, which allows us to make
more direct statements about their capabilities. We show that simple RNNs are
equivalent to a subclass of probabilistic finite-state automata, and can thus
model a strict subset of probability distributions expressible by finite-state
models. Furthermore, we study the space complexity of representing finite-state
LMs with RNNs. We show that, to represent an arbitrary deterministic
finite-state LM with $N$ states over an alphabet $\Sigma$, an RNN requires
$\Omega\left(N |\Sigma|\right)$ neurons. These results present a first step
towards characterizing the classes of distributions RNN LMs can represent and
thus help us understand their capabilities and limitations.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05163" title="Abstract">arXiv:2310.05163</a> [<a href="/pdf/2310.05163" title="Download PDF">pdf</a>, <a href="/format/2310.05163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Investigation of LLMs&#x27; Inefficacy in Understanding Converse Relations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+C">Chengwen Qi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bowen Li</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+B">Binyuan Hui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bailin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jinwang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Laili%2C+Y">Yuanjun Laili</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have achieved remarkable success in many formal
language oriented tasks, such as structural data-to-text and semantic parsing.
However current benchmarks mostly follow the data distribution of the
pre-training data of LLMs. Therefore, a natural question rises that do LLMs
really understand the structured semantics of formal languages. In this paper,
we investigate this problem on a special case, converse binary relation. We
introduce a new benchmark ConvRe focusing on converse relations, which contains
17 relations and 1240 triples extracted from popular knowledge graph completion
datasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are
formulated as multi-choice question answering to evaluate LLMs' ability to
determine the matching between relations and associated text. For the
evaluation protocol, apart from different prompting methods, we further
introduce variants to the test text and few-shot example text. We conduct
experiments on three popular LLM families and have observed various scaling
trends. The results suggest that LLMs often resort to shortcut learning and
still face challenges on our proposed benchmark.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05165" title="Abstract">arXiv:2310.05165</a> [<a href="/pdf/2310.05165" title="Download PDF">pdf</a>, <a href="/format/2310.05165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Zero-Shot Generalization of Machine-Generated Text Detectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pu%2C+X">Xiao Pu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiaochuang Han</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianxing He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The rampant proliferation of large language models, fluent enough to generate
text indistinguishable from human-written language, gives unprecedented
importance to the detection of machine-generated text. This work is motivated
by an important research question: How will the detectors of machine-generated
text perform on outputs of a new generator, that the detectors were not trained
on? We begin by collecting generation data from a wide range of LLMs, and train
neural detectors on data from each generator and test its performance on
held-out generators. While none of the detectors can generalize to all
generators, we observe a consistent and interesting pattern that the detectors
trained on data from a medium-size LLM can zero-shot generalize to the larger
version. As a concrete application, we demonstrate that robust detectors can be
built on an ensemble of training data from medium-sized models.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05166" title="Abstract">arXiv:2310.05166</a> [<a href="/pdf/2310.05166" title="Download PDF">pdf</a>, <a href="/format/2310.05166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Corrected Expected Improvement Acquisition Function Under Noisy  Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Han Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xingchen Ma</a>, 
<a href="/search/cs?searchtype=author&query=Blaschko%2C+M+B">Matthew B Blaschko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Sequential maximization of expected improvement (EI) is one of the most
widely used policies in Bayesian optimization because of its simplicity and
ability to handle noisy observations. In particular, the improvement function
often uses the best posterior mean as the best incumbent in noisy settings.
However, the uncertainty associated with the incumbent solution is often
neglected in many analytic EI-type methods: a closed-form acquisition function
is derived in the noise-free setting, but then applied to the setting with
noisy observations. To address this limitation, we propose a modification of EI
that corrects its closed-form expression by incorporating the covariance
information provided by the Gaussian Process (GP) model. This acquisition
function specializes to the classical noise-free result, and we argue should
replace that formula in Bayesian optimization software packages, tutorials, and
textbooks. This enhanced acquisition provides good generality for noisy and
noiseless settings. We show that our method achieves a sublinear convergence
rate on the cumulative regret bound under heteroscedastic observation noise.
Our empirical results demonstrate that our proposed acquisition function can
outperform EI in the presence of noisy observations on benchmark functions for
black-box optimization, as well as on parameter search for neural network model
compression.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05167" title="Abstract">arXiv:2310.05167</a> [<a href="/pdf/2310.05167" title="Download PDF">pdf</a>, <a href="/format/2310.05167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hieros: Hierarchical Imagination on Structured State Space Sequence  World Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mattes%2C+P">Paul Mattes</a>, 
<a href="/search/cs?searchtype=author&query=Schlosser%2C+R">Rainer Schlosser</a>, 
<a href="/search/cs?searchtype=author&query=Herbrich%2C+R">Ralf Herbrich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICLR 2024, 23 pages, 11 figures, code available at: <a href="https://github.com/Snagnar/Hieros">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">One of the biggest challenges to modern deep reinforcement learning (DRL)
algorithms is sample efficiency. Many approaches learn a world model in order
to train an agent entirely in imagination, eliminating the need for direct
environment interaction during training. However, these methods often suffer
from either a lack of imagination accuracy, exploration capabilities, or
runtime efficiency. We propose \hieros, a hierarchical policy that learns time
abstracted world representations and imagines trajectories at multiple time
scales in latent space. \hieros uses an S5 layer-based world model, which
predicts next world states in parallel during training and iteratively during
environment interaction. Due to the special properties of S5 layers, our method
can train in parallel and predict next world states iteratively during
imagination. This allows for more efficient training than RNN-based world
models and more efficient imagination than Transformer-based world models.
<br />We show that our approach outperforms the state of the art in terms of mean
and median normalized human score on the Atari 100k benchmark, and that our
proposed world model is able to predict complex dynamics very accurately. We
also show that \hieros displays superior exploration capabilities compared to
existing approaches.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05169" title="Abstract">arXiv:2310.05169</a> [<a href="/pdf/2310.05169" title="Download PDF">pdf</a>, <a href="/format/2310.05169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Ability of PINNs To Solve Burgers&#x27; PDE Near  Finite-Time BlowUp
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+D">Dibyakanti Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Anirbit Mukherjee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Analysis of PDEs (math.AP); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Physics Informed Neural Networks (PINNs) have been achieving ever newer feats
of solving complicated PDEs numerically while offering an attractive trade-off
between accuracy and speed of inference. A particularly challenging aspect of
PDEs is that there exist simple PDEs which can evolve into singular solutions
in finite time starting from smooth initial conditions. In recent times some
striking experiments have suggested that PINNs might be good at even detecting
such finite-time blow-ups. In this work, we embark on a program to investigate
this stability of PINNs from a rigorous theoretical viewpoint. Firstly, we
derive generalization bounds for PINNs for Burgers' PDE, in arbitrary
dimensions, under conditions that allow for a finite-time blow-up. Then we
demonstrate via experiments that our bounds are significantly correlated to the
$\ell_2$-distance of the neurally found surrogate from the true blow-up
solution, when computed on sequences of PDEs that are getting increasingly
close to a blow-up.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05170" title="Abstract">arXiv:2310.05170</a> [<a href="/pdf/2310.05170" title="Download PDF">pdf</a>, <a href="/format/2310.05170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepQTest: Testing Autonomous Driving Systems with Reinforcement  Learning and Real-world Weather Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chengjie Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+T">Tao Yue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Man Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+S">Shaukat Ali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 7 figures, 13 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Autonomous driving systems (ADSs) are capable of sensing the environment and
making driving decisions autonomously. These systems are safety-critical, and
testing them is one of the important approaches to ensure their safety.
However, due to the inherent complexity of ADSs and the high dimensionality of
their operating environment, the number of possible test scenarios for ADSs is
infinite. Besides, the operating environment of ADSs is dynamic, continuously
evolving, and full of uncertainties, which requires a testing approach adaptive
to the environment. In addition, existing ADS testing techniques have limited
effectiveness in ensuring the realism of test scenarios, especially the realism
of weather conditions and their changes over time. Recently, reinforcement
learning (RL) has demonstrated great potential in addressing challenging
problems, especially those requiring constant adaptations to dynamic
environments. To this end, we present DeepQTest, a novel ADS testing approach
that uses RL to learn environment configurations with a high chance of
revealing abnormal ADS behaviors. Specifically, DeepQTest employs Deep
Q-Learning and adopts three safety and comfort measures to construct the reward
functions. To ensure the realism of generated scenarios, DeepQTest defines a
set of realistic constraints and introduces real-world weather conditions into
the simulated environment. We employed three comparison baselines, i.e.,
random, greedy, and a state-of-the-art RL-based approach DeepCOllision, for
evaluating DeepQTest on an industrial-scale ADS. Evaluation results show that
DeepQTest demonstrated significantly better effectiveness in terms of
generating scenarios leading to collisions and ensuring scenario realism
compared with the baselines. In addition, among the three reward functions
implemented in DeepQTest, Time-To-Collision is recommended as the best design
according to our study.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05171" title="Abstract">arXiv:2310.05171</a> [<a href="/pdf/2310.05171" title="Download PDF">pdf</a>, <a href="/format/2310.05171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Ship Tracking by Robust Similarity metric
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hongyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Gongming Wei</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+X">Xianglei Xing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Multi-ship tracking (MST) as a core technology has been proven to be applied
to situational awareness at sea and the development of a navigational system
for autonomous ships. Despite impressive tracking outcomes achieved by
multi-object tracking (MOT) algorithms for pedestrian and vehicle datasets,
these models and techniques exhibit poor performance when applied to ship
datasets. Intersection of Union (IoU) is the most popular metric for computing
similarity used in object tracking. The low frame rates and severe image shake
caused by wave turbulence in ship datasets often result in minimal, or even
zero, Intersection of Union (IoU) between the predicted and detected bounding
boxes. This issue contributes to frequent identity switches of tracked objects,
undermining the tracking performance. In this paper, we address the weaknesses
of IoU by incorporating the smallest convex shapes that enclose both the
predicted and detected bounding boxes. The calculation of the tracking version
of IoU (TIoU) metric considers not only the size of the overlapping area
between the detection bounding box and the prediction box, but also the
similarity of their shapes. Through the integration of the TIoU into
state-of-the-art object tracking frameworks, such as DeepSort and ByteTrack, we
consistently achieve improvements in the tracking performance of these
frameworks.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05172" title="Abstract">arXiv:2310.05172</a> [<a href="/pdf/2310.05172" title="Download PDF">pdf</a>, <a href="/format/2310.05172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Amplification of Cache Occupancy Attacks in Randomized Cache  Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+A">Anirban Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+N">Nimish Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Sayandeep Saha</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S">Sarani Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Mukhopadhyay%2C+D">Debdeep Mukhopadhyay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">In this work, we explore the applicability of cache occupancy attacks and the
implications of secured cache design rationales on such attacks. In particular,
we show that one of the well-known cache randomization schemes, MIRAGE, touted
to be resilient against eviction-based attacks, amplifies the chances of cache
occupancy attack, making it more vulnerable compared to contemporary designs.
We leverage MIRAGE's global eviction property to demonstrate covert channel
with byte-level granularity, with far less cache occupancy requirement (just
$10\%$ of LLC) than other schemes. For instance, ScatterCache (a randomisation
scheme with lesser security guarantees than MIRAGE) and generic set-associative
caches require $40\%$ and $30\%$ cache occupancy, respectively, to exhibit
covert communication. Furthermore, we extend our attack vectors to include
side-channel, template-based fingerprinting of workloads in a cross-core
setting. We demonstrate the potency of such fingerprinting on both inhouse LLC
simulator as well as on SPEC2017 workloads on gem5. Finally, we pinpoint
implementation inconsistencies in MIRAGE's publicly available gem5 artifact
which motivates a re-evaluation of the performance statistics of MIRAGE with
respect to ScatterCache and baseline set-associative cache. We find MIRAGE, in
reality, performs worse than what is previously reported in literature, a
concern that should be addressed in successor generations of secured caches.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05174" title="Abstract">arXiv:2310.05174</a> [<a href="/pdf/2310.05174" title="Download PDF">pdf</a>, <a href="/format/2310.05174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GSLB: The Graph Structure Learning Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhixun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yifan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yanqiao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dingshuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yingtao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiangxin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J+X">Jeffrey Xu Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS Datasets and Benchmarks Track 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph Structure Learning (GSL) has recently garnered considerable attention
due to its ability to optimize both the parameters of Graph Neural Networks
(GNNs) and the computation graph structure simultaneously. Despite the
proliferation of GSL methods developed in recent years, there is no standard
experimental setting or fair comparison for performance evaluation, which
creates a great obstacle to understanding the progress in this field. To fill
this gap, we systematically analyze the performance of GSL in different
scenarios and develop a comprehensive Graph Structure Learning Benchmark (GSLB)
curated from 20 diverse graph datasets and 16 distinct GSL algorithms.
Specifically, GSLB systematically investigates the characteristics of GSL in
terms of three dimensions: effectiveness, robustness, and complexity. We
comprehensively evaluate state-of-the-art GSL algorithms in node- and
graph-level tasks, and analyze their performance in robust learning and model
complexity. Further, to facilitate reproducible research, we have developed an
easy-to-use library for training, evaluating, and visualizing different GSL
methods. Empirical results of our extensive experiments demonstrate the ability
of GSL and reveal its potential benefits on various downstream tasks, offering
insights and opportunities for future research. The code of GSLB is available
at: https://github.com/GSL-Benchmark/GSLB.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05175" title="Abstract">arXiv:2310.05175</a> [<a href="/pdf/2310.05175" title="Download PDF">pdf</a>, <a href="/format/2310.05175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for  Pruning LLMs to High Sparsity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+L">Lu Yin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">You Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cheng-Yu Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yiling Jia</a>, 
<a href="/search/cs?searchtype=author&query=Pechenizkiy%2C+M">Mykola Pechenizkiy</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yi Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiwei Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large Language Models (LLMs), renowned for their remarkable performance,
present a challenge due to their colossal model size when it comes to practical
deployment. In response to this challenge, efforts have been directed toward
the application of traditional network pruning techniques to LLMs, uncovering a
massive number of parameters can be pruned in one-shot without hurting
performance. Building upon insights gained from pre-LLM models, prevailing LLM
pruning strategies have consistently adhered to the practice of uniformly
pruning all layers at equivalent sparsity. However, this observation stands in
contrast to the prevailing trends observed in the field of vision models, where
non-uniform layerwise sparsity typically yields substantially improved results.
To elucidate the underlying reasons for this disparity, we conduct a
comprehensive analysis of the distribution of token features within LLMs. In
doing so, we discover a strong correlation with the emergence of outliers,
defined as features exhibiting significantly greater magnitudes compared to
their counterparts in feature dimensions. Inspired by this finding, we
introduce a novel LLM pruning methodology that incorporates a tailored set of
non-uniform layerwise sparsity ratios specifically designed for LLM pruning,
termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL
is directly proportional to the outlier ratio observed within each layer,
facilitating a more effective alignment between layerwise weight sparsity and
outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family
and OPT, spanning various benchmarks, demonstrates the distinct advantages
offered by OWL over previous methods. For instance, our approach exhibits a
remarkable performance gain, surpassing the state-of-the-art Wanda and
SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70%,
respectively.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05177" title="Abstract">arXiv:2310.05177</a> [<a href="/pdf/2310.05177" title="Download PDF">pdf</a>, <a href="/format/2310.05177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Large Language Models Know about Facts?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xuming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junzhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaochuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yufei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+L">Lijie Wen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhijiang Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have recently driven striking performance
improvements across a range of natural language processing tasks. The factual
knowledge acquired during pretraining and instruction tuning can be useful in
various downstream tasks, such as question answering, and language generation.
Unlike conventional Knowledge Bases (KBs) that explicitly store factual
knowledge, LLMs implicitly store facts in their parameters. Content generated
by the LLMs can often exhibit inaccuracies or deviations from the truth, due to
facts that can be incorrectly induced or become obsolete over time. To this
end, we aim to comprehensively evaluate the extent and scope of factual
knowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains
20K diverse factual questions that span different sources, timelines, domains,
regions, and languages. Furthermore, we investigate whether LLMs are able to
compose multiple facts, update factual knowledge temporally, reason over
multiple pieces of facts, identify subtle factual differences, and resist
adversarial examples. Extensive experiments on different sizes and types of
LLMs show that existing LLMs still lack factual knowledge and suffer from
various spurious correlations. We believe this is a critical bottleneck for
realizing trustworthy artificial intelligence. The dataset Pinocchio and our
codes will be publicly available.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05178" title="Abstract">arXiv:2310.05178</a> [<a href="/pdf/2310.05178" title="Download PDF">pdf</a>, <a href="/format/2310.05178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Large Language Models to Expedite the Development of Smart  Contracts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dade%2C+N+O+O">Nii Osae Osae Dade</a>, 
<a href="/search/cs?searchtype=author&query=Lartey-Quaye%2C+M">Margaret Lartey-Quaye</a>, 
<a href="/search/cs?searchtype=author&query=Odonkor%2C+E+T">Emmanuel Teye-Kofi Odonkor</a>, 
<a href="/search/cs?searchtype=author&query=Ammah%2C+P">Paul Ammah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Programming has always been at the heart of technological innovation in the
21st century. With the advent of blockchain technologies and the proliferation
of web3 paradigms of decentralised applications, smart contracts have been very
instrumental in enabling developers to build applications that reside on
decentralised blockchains. Despite the huge interest and potential of smart
contracts, there is still a significant knowledge and skill gap that developers
need to cross in order to build web3 applications. In light of this, we
introduce MazzumaGPT, a large language model that has been optimised to
generate smart contract code and aid developers to scaffold development and
improve productivity. As part of this research, we outline the optimisation and
fine-tuning parameters, evaluate the model's performance on functional
correctness and address the limitations and broader impacts of our research.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05179" title="Abstract">arXiv:2310.05179</a> [<a href="/pdf/2310.05179" title="Download PDF">pdf</a>, <a href="/format/2310.05179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributional Reinforcement Learning with Online Risk-awareness  Adaption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yupeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenjie Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The use of reinforcement learning (RL) in practical applications requires
considering sub-optimal outcomes, which depend on the agent's familiarity with
the uncertain environment. Dynamically adjusting the level of epistemic risk
over the course of learning can tactically achieve reliable optimal policy in
safety-critical environments and tackle the sub-optimality of a static risk
level. In this work, we introduce a novel framework, Distributional RL with
Online Risk Adaption (DRL-ORA), which can quantify the aleatory and epistemic
uncertainties compositely and dynamically select the epistemic risk levels via
solving a total variation minimization problem online. The risk level selection
can be efficiently achieved through grid search using a Follow-The-Leader type
algorithm, and its offline oracle is related to "satisficing measure" (in the
decision analysis community) under a special modification of the loss function.
We show multiple classes of tasks where DRL-ORA outperforms existing methods
that rely on either a fixed risk level or manually predetermined risk level
adaption. Given the simplicity of our modifications, we believe the framework
can be easily incorporated into most RL algorithm variants.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05180" title="Abstract">arXiv:2310.05180</a> [<a href="/pdf/2310.05180" title="Download PDF">pdf</a>, <a href="/format/2310.05180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blockchain-Envisioned Disaster Relief Networks: Architecture,  Opportunities, and Open Issues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuntao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qinnan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zhou Su</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+X">Xiang Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jian Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Natural or man-made disasters pose significant challenges for delivering
critical relief to affected populations due to disruptions in critical
infrastructures and logistics networks. Unmanned aerial vehicles (UAVs)-aided
disaster relief networks (UDRNs) leverage UAVs to assist existing ground relief
networks by swiftly assessing affected areas and timely delivering lifesaving
supplies. However, severe challenges in mutual coordination, trust, and
security hinder the deployment of UDRNs. This paper leverages the promising
blockchain technology to address collaborative, trust-free, and traceable
disaster relief services. Specifically, we first present a general
blockchain-oriented UDRN architecture incorporating space, air, and ground
layers. Subsequently, we propose to optimize the blockchain-based UDRN system,
including (i) a series of smart contracts for transparent and automated relief
assignment; (ii) a dynamic contract audit mechanism to prevent known/unknown
contract vulnerabilities; and (iii) a transaction forensics strategy with
on/off-chain cooperation for traceable relief services. Through a prototype
implementation, experimental results demonstrate the feasibility and
effectiveness of our approach in terms of threat awareness latency and
vulnerability detection rate. Lastly, we outline key open research issues
crucial to this emerging field.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05183" title="Abstract">arXiv:2310.05183</a> [<a href="/pdf/2310.05183" title="Download PDF">pdf</a>, <a href="/format/2310.05183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChiMera: Learning with noisy labels by contrasting mixed-up  augmentations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zixuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junjun He</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+D">Dan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Samaras%2C+D">Dimitris Samaras</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+R">Robby Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sheng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Learning with noisy labels has been studied to address incorrect label
annotations in real-world applications. In this paper, we present ChiMera, a
two-stage learning-from-noisy-labels framework based on semi-supervised
learning, developed based on a novel contrastive learning technique MixCLR. The
key idea of MixCLR is to learn and refine the representations of mixed
augmentations from two different images to better resist label noise. ChiMera
jointly learns the representations of the original data distribution and
mixed-up data distribution via MixCLR, introducing many additional augmented
samples to fill in the gap between different classes. This results in a more
smoothed representation space learned by contrastive learning with better
alignment and a more robust decision boundary. By exploiting MixCLR, ChiMera
also improves the label diffusion process in the semi-supervised noise recovery
stage and further boosts its ability to diffuse correct label information. We
evaluated ChiMera on seven real-world datasets and obtained state-of-the-art
performance on both symmetric noise and asymmetric noise. Our method opens up
new avenues for using contrastive learning on learning with noisy labels and we
envision MixCLR to be broadly applicable to other applications.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05184" title="Abstract">arXiv:2310.05184</a> [<a href="/pdf/2310.05184" title="Download PDF">pdf</a>, <a href="/format/2310.05184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AANet: Aggregation and Alignment Network with Semi-hard Positive Sample  Mining for Hierarchical Place Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+F">Feng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lijun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+S">Shuting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Baifan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chun Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICRA2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Visual place recognition (VPR) is one of the research hotspots in robotics,
which uses visual information to locate robots. Recently, the hierarchical
two-stage VPR methods have become popular in this field due to the trade-off
between accuracy and efficiency. These methods retrieve the top-k candidate
images using the global features in the first stage, then re-rank the
candidates by matching the local features in the second stage. However, they
usually require additional algorithms (e.g. RANSAC) for geometric consistency
verification in re-ranking, which is time-consuming. Here we propose a
Dynamically Aligning Local Features (DALF) algorithm to align the local
features under spatial constraints. It is significantly more efficient than the
methods that need geometric consistency verification. We present a unified
network capable of extracting global features for retrieving candidates via an
aggregation module and aligning local features for re-ranking via the DALF
alignment module. We call this network AANet. Meanwhile, many works use the
simplest positive samples in triplet for weakly supervised training, which
limits the ability of the network to recognize harder positive pairs. To
address this issue, we propose a Semi-hard Positive Sample Mining (ShPSM)
strategy to select appropriate hard positive images for training more robust
VPR networks. Extensive experiments on four benchmark VPR datasets show that
the proposed AANet can outperform several state-of-the-art methods with less
time consumption. The code is released at https://github.com/Lu-Feng/AANet.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05185" title="Abstract">arXiv:2310.05185</a> [<a href="/pdf/2310.05185" title="Download PDF">pdf</a>, <a href="/format/2310.05185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational  Knowledge Graph Construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haoran Luo</a>, 
<a href="/search/cs?searchtype=author&query=E%2C+H">Haihong E</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuhao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+T">Tianyu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yikai Guo</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zichen Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wentai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+K">Kaiyang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">Shiyao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Meina Song</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Wei Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Beyond traditional binary relational facts, n-ary relational knowledge graphs
(NKGs) are comprised of n-ary relational facts containing more than two
entities, which are closer to real-world facts with broader applications.
However, the construction of NKGs still significantly relies on manual labor,
and n-ary relation extraction still remains at a course-grained level, which is
always in a single schema and fixed arity of entities. To address these
restrictions, we propose Text2NKG, a novel fine-grained n-ary relation
extraction framework for n-ary relational knowledge graph construction. We
introduce a span-tuple classification approach with hetero-ordered merging to
accomplish fine-grained n-ary relation extraction in different arity.
Furthermore, Text2NKG supports four typical NKG schemas: hyper-relational
schema, event-based schema, role-based schema, and hypergraph-based schema,
with high flexibility and practicality. Experimental results demonstrate that
Text2NKG outperforms the previous state-of-the-art model by nearly 20\% points
in the $F_1$ scores on the fine-grained n-ary relation extraction benchmark in
the hyper-relational schema. Our code and datasets are publicly available.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05186" title="Abstract">arXiv:2310.05186</a> [<a href="/pdf/2310.05186" title="Download PDF">pdf</a>, <a href="/format/2310.05186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolutionary Retrosynthetic Route Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+H">Hao Hao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiao He</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Shuanhu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Aimin Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Molecular retrosynthesis is a significant and complex problem in the field of
chemistry, however, traditional manual synthesis methods not only need
well-trained experts but also are time-consuming. With the development of big
data and machine learning, artificial intelligence (AI) based retrosynthesis is
attracting more attention and is becoming a valuable tool for molecular
retrosynthesis. At present, Monte Carlo tree search is a mainstream search
framework employed to address this problem. Nevertheless, its search efficiency
is compromised by its large search space. Therefore, we propose a novel
approach for retrosynthetic route planning based on evolutionary optimization,
marking the first use of Evolutionary Algorithm (EA) in the field of multi-step
retrosynthesis. The proposed method involves modeling the retrosynthetic
problem into an optimization problem, defining the search space and operators.
Additionally, to improve the search efficiency, a parallel strategy is
implemented. The new approach is applied to four case products, and is compared
with Monte Carlo tree search. The experimental results show that, in comparison
to the Monte Carlo tree search algorithm, EA significantly reduces the number
of calling single-step model by an average of 53.9%. The time required to
search three solutions decreased by an average of 83.9%, and the number of
feasible search routes increases by 5 times.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05187" title="Abstract">arXiv:2310.05187</a> [<a href="/pdf/2310.05187" title="Download PDF">pdf</a>, <a href="/format/2310.05187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lifelong Learning for Fog Load Balancing: A Transfer Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ebrahim%2C+M">Maad Ebrahim</a>, 
<a href="/search/cs?searchtype=author&query=Hafid%2C+A+S">Abdelhakim Senhaji Hafid</a>, 
<a href="/search/cs?searchtype=author&query=Abid%2C+M+R">Mohamed Riduan Abid</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages and 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Fog computing emerged as a promising paradigm to address the challenges of
processing and managing data generated by the Internet of Things (IoT). Load
balancing (LB) plays a crucial role in Fog computing environments to optimize
the overall system performance. It requires efficient resource allocation to
improve resource utilization, minimize latency, and enhance the quality of
service for end-users. In this work, we improve the performance of
privacy-aware Reinforcement Learning (RL) agents that optimize the execution
delay of IoT applications by minimizing the waiting delay. To maintain privacy,
these agents optimize the waiting delay by minimizing the change in the number
of queued requests in the whole system, i.e., without explicitly observing the
actual number of requests that are queued in each Fog node nor observing the
compute resource capabilities of those nodes. Besides improving the performance
of these agents, we propose in this paper a lifelong learning framework for
these agents, where lightweight inference models are used during deployment to
minimize action delay and only retrained in case of significant environmental
changes. To improve the performance, minimize the training cost, and adapt the
agents to those changes, we explore the application of Transfer Learning (TL).
TL transfers the knowledge acquired from a source domain and applies it to a
target domain, enabling the reuse of learned policies and experiences. TL can
be also used to pre-train the agent in simulation before fine-tuning it in the
real environment; this significantly reduces failure probability compared to
learning from scratch in the real environment. To our knowledge, there are no
existing efforts in the literature that use TL to address lifelong learning for
RL-based Fog LB; this is one of the main obstacles in deploying RL LB solutions
in Fog systems.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05189" title="Abstract">arXiv:2310.05189</a> [<a href="/pdf/2310.05189" title="Download PDF">pdf</a>, <a href="/ps/2310.05189" title="Download PostScript">ps</a>, <a href="/format/2310.05189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factuality Challenges in the Era of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Augenstein%2C+I">Isabelle Augenstein</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Timothy Baldwin</a>, 
<a href="/search/cs?searchtype=author&query=Cha%2C+M">Meeyoung Cha</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+T">Tanmoy Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Ciampaglia%2C+G+L">Giovanni Luca Ciampaglia</a>, 
<a href="/search/cs?searchtype=author&query=Corney%2C+D">David Corney</a>, 
<a href="/search/cs?searchtype=author&query=DiResta%2C+R">Renee DiResta</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>, 
<a href="/search/cs?searchtype=author&query=Hale%2C+S">Scott Hale</a>, 
<a href="/search/cs?searchtype=author&query=Halevy%2C+A">Alon Halevy</a>, 
<a href="/search/cs?searchtype=author&query=Hovy%2C+E">Eduard Hovy</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Menczer%2C+F">Filippo Menczer</a>, 
<a href="/search/cs?searchtype=author&query=Miguez%2C+R">Ruben Miguez</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>, 
<a href="/search/cs?searchtype=author&query=Scheufele%2C+D">Dietram Scheufele</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Shivam Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Zagni%2C+G">Giovanni Zagni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our article offers a comprehensive examination of the challenges and risks associated with Large Language Models (LLMs), focusing on their potential impact on the veracity of information in today's digital landscape (ongoing work)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The emergence of tools based on Large Language Models (LLMs), such as
OpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered
immense public attention. These incredibly useful, natural-sounding tools mark
significant advances in natural language generation, yet they exhibit a
propensity to generate false, erroneous, or misleading content -- commonly
referred to as "hallucinations". Moreover, LLMs can be exploited for malicious
applications, such as generating false but credible-sounding content and
profiles at scale. This poses a significant challenge to society in terms of
the potential deception of users and the increasing dissemination of inaccurate
information. In light of these risks, we explore the kinds of technological
innovations, regulatory reforms, and AI literacy initiatives needed from
fact-checkers, news organizations, and the broader research and policy
communities. By identifying the risks, the imminent threats, and some viable
solutions, we seek to shed light on navigating various aspects of veracity in
the era of generative AI.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05191" title="Abstract">arXiv:2310.05191</a> [<a href="/pdf/2310.05191" title="Download PDF">pdf</a>, <a href="/format/2310.05191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FABRIC: Automated Scoring and Feedback Generation for Essays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jieun Han</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+H">Haneul Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Myung%2C+J">Junho Myung</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minsun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+H">Hyunseung Lim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yoonsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+T+Y">Tak Yeon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+H">Hwajung Hong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+S">So-Yeon Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+A">Alice Oh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automated essay scoring (AES) provides a useful tool for students and
instructors in writing classes by generating essay scores in real-time.
However, previous AES models do not provide more specific rubric-based scores
nor feedback on how to improve the essays, which can be even more important
than the overall scores for learning. We present FABRIC, a pipeline to help
students and instructors in English writing classes by automatically generating
1) the overall scores, 2) specific rubric-based scores, and 3) detailed
feedback on how to improve the essays. Under the guidance of English education
experts, we chose the rubrics for the specific scores as content, organization,
and language. The first component of the FABRIC pipeline is DREsS, a real-world
Dataset for Rubric-based Essay Scoring (DREsS). The second component is CASE, a
Corruption-based Augmentation Strategy for Essays, with which we can improve
the accuracy of the baseline model by 45.44%. The third component is EssayCoT,
the Essay Chain-of-Thought prompting strategy which uses scores predicted from
the AES model to generate better feedback. We evaluate the effectiveness of the
new dataset DREsS and the augmentation strategy CASE quantitatively and show
significant improvements over the models trained with existing datasets. We
evaluate the feedback generated by EssayCoT with English education experts to
show significant improvements in the helpfulness of the feedback across all
rubrics. Lastly, we evaluate the FABRIC pipeline with students in a college
English writing class who rated the generated scores and feedback with an
average of 6 on the Likert scale from 1 to 7.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05192" title="Abstract">arXiv:2310.05192</a> [<a href="/pdf/2310.05192" title="Download PDF">pdf</a>, <a href="/format/2310.05192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HOD: A Benchmark Dataset for Harmful Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ha%2C+E">Eungyeom Ha</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Heemook Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+S+C">Sung Chul Hong</a>, 
<a href="/search/cs?searchtype=author&query=Na%2C+D">Dongbin Na</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent multi-media data such as images and videos have been rapidly spread
out on various online services such as social network services (SNS). With the
explosive growth of online media services, the number of image content that may
harm users is also growing exponentially. Thus, most recent online platforms
such as Facebook and Instagram have adopted content filtering systems to
prevent the prevalence of harmful content and reduce the possible risk of
adverse effects on users. Unfortunately, computer vision research on detecting
harmful content has not yet attracted attention enough. Users of each platform
still manually click the report button to recognize patterns of harmful content
they dislike when exposed to harmful content. However, the problem with manual
reporting is that users are already exposed to harmful content. To address
these issues, our research goal in this work is to develop automatic harmful
object detection systems for online services. We present a new benchmark
dataset for harmful object detection. Unlike most related studies focusing on a
small subset of object categories, our dataset addresses various categories.
Specifically, our proposed dataset contains more than 10,000 images across 6
categories that might be harmful, consisting of not only normal cases but also
hard cases that are difficult to detect. Moreover, we have conducted extensive
experiments to evaluate the effectiveness of our proposed dataset. We have
utilized the recently proposed state-of-the-art (SOTA) object detection
architectures and demonstrated our proposed dataset can be greatly useful for
the real-time harmful object detection task. The whole source codes and
datasets are publicly accessible at
https://github.com/poori-nuna/HOD-Benchmark-Dataset.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05193" title="Abstract">arXiv:2310.05193</a> [<a href="/pdf/2310.05193" title="Download PDF">pdf</a>, <a href="/format/2310.05193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Discriminative Multi-Modal Learning with Large-Scale  Pre-Trained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chenzhuang Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yue Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+C">Chonghua Liao</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+J">Jiacheng You</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper investigates how to better leverage large-scale pre-trained
uni-modal models to further enhance discriminative multi-modal learning. Even
when fine-tuned with only uni-modal data, these models can outperform previous
multi-modal models in certain tasks. It's clear that their incorporation into
multi-modal learning would significantly improve performance. However,
multi-modal learning with these models still suffers from insufficient learning
of uni-modal features, which weakens the resulting multi-modal model's
generalization ability. While fine-tuning uni-modal models separately and then
aggregating their predictions is straightforward, it doesn't allow for adequate
adaptation between modalities, also leading to sub-optimal results. To this
end, we introduce Multi-Modal Low-Rank Adaptation learning (MMLoRA). By
freezing the weights of uni-modal fine-tuned models, adding extra trainable
rank decomposition matrices to them, and subsequently performing multi-modal
joint training, our method enhances adaptation between modalities and boosts
overall performance. We demonstrate the effectiveness of MMLoRA on three
dataset categories: audio-visual (e.g., AVE, Kinetics-Sound, CREMA-D),
vision-language (e.g., MM-IMDB, UPMC Food101), and RGB-Optical Flow (UCF101).
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05195" title="Abstract">arXiv:2310.05195</a> [<a href="/pdf/2310.05195" title="Download PDF">pdf</a>, <a href="/format/2310.05195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GMMFormer: Gaussian-Mixture-Model based Transformer for Efficient  Partially Relevant Video Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinpeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Ziyun Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+S">Shu-Tao Xia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress. The code will be released
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Multimedia (cs.MM)

</div>
<p class="mathjax">Given a text query, partially relevant video retrieval (PRVR) seeks to find
untrimmed videos containing pertinent moments in a database. For PRVR, clip
modeling is essential to capture the partial relationship between texts and
videos. Current PRVR methods adopt scanning-based clip construction to achieve
explicit clip modeling, which is information-redundant and requires a large
storage overhead. To solve the efficiency problem of PRVR methods, this paper
proposes GMMFormer, a \textbf{G}aussian-\textbf{M}ixture-\textbf{M}odel based
Trans\textbf{former} which models clip representations implicitly. During frame
interactions, we incorporate Gaussian-Mixture-Model constraints to focus each
frame on its adjacent frames instead of the whole video. Then generated
representations will contain multi-scale clip information, achieving implicit
clip modeling. In addition, PRVR methods ignore semantic differences between
text queries relevant to the same video, leading to a sparse embedding space.
We propose a query diverse loss to distinguish these text queries, making the
embedding space more intensive and contain more semantic information. Extensive
experiments on three large-scale video datasets (\ie, TVR, ActivityNet
Captions, and Charades-STA) demonstrate the superiority and efficiency of
GMMFormer.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05198" title="Abstract">arXiv:2310.05198</a> [<a href="/pdf/2310.05198" title="Download PDF">pdf</a>, <a href="/format/2310.05198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Indoor Localization for an Autonomous Model Car: A Marker-Based  Multi-Sensor Fusion Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xibo Li</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+S">Shruti Patel</a>, 
<a href="/search/cs?searchtype=author&query=Stronzek-Pfeifer%2C+D">David Stronzek-Pfeifer</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%BCskens%2C+C">Christof B&#xfc;skens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Global navigation satellite systems readily provide accurate position
information when localizing a robot outdoors. However, an analogous standard
solution does not exist yet for mobile robots operating indoors. This paper
presents an integrated framework for indoor localization and experimental
validation of an autonomous driving system based on an advanced
driver-assistance system (ADAS) model car. The global pose of the model car is
obtained by fusing information from fiducial markers, inertial sensors and
wheel odometry. In order to achieve robust localization, we investigate and
compare two extensions to the Extended Kalman Filter; first with adaptive noise
tuning and second with Chi-squared test for measurement outlier detection. An
efficient and low-cost ground truth measurement method using a single LiDAR
sensor is also proposed to validate the results. The performance of the
localization algorithms is tested on a complete autonomous driving system with
trajectory planning and model predictive control.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05199" title="Abstract">arXiv:2310.05199</a> [<a href="/pdf/2310.05199" title="Download PDF">pdf</a>, <a href="/format/2310.05199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning  from Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Wei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Rui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wenyu Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+S">Shihan Dou</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP 2023 findings (preprint)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Reinforcement learning from human feedback serves as a crucial bridge,
aligning large language models with human and societal values. This alignment
requires a vast corpus of human feedback to learn a reward model, which is
subsequently used to finetune language models. However, we have identified that
the reward model often finds shortcuts to bypass its intended objectives,
misleadingly assuming that humans prefer longer responses. The emergence of
length bias often induces the model to favor longer outputs, yet it doesn't
equate to an increase in helpful information within these outputs. In this
paper, we propose an innovative solution, applying the Product-of-Experts (PoE)
technique to separate reward modeling from the influence of sequence length. In
our framework, the main expert concentrates on understanding human intents,
while the biased expert targets the identification and capture of length bias.
To further enhance the learning of bias, we introduce perturbations into the
bias-focused expert, disrupting the flow of semantic information. Experimental
results validate the effectiveness of our approach, indicating that language
model performance is improved, irrespective of sequence length.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05202" title="Abstract">arXiv:2310.05202</a> [<a href="/pdf/2310.05202" title="Download PDF">pdf</a>, <a href="/format/2310.05202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Cross-Dataset Performance of Distracted Driving Detection With  Score-Softmax Classifier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+C">Cong Duan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zixuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+J">Jiahao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Minghai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+J">Jiacai Liao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Libo Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep neural networks enable real-time monitoring of in-vehicle driver,
facilitating the timely prediction of distractions, fatigue, and potential
hazards. This technology is now integral to intelligent transportation systems.
Recent research has exposed unreliable cross-dataset end-to-end driver behavior
recognition due to overfitting, often referred to as ``shortcut learning",
resulting from limited data samples. In this paper, we introduce the
Score-Softmax classifier, which addresses this issue by enhancing inter-class
independence and Intra-class uncertainty. Motivated by human rating patterns,
we designed a two-dimensional supervisory matrix based on marginal Gaussian
distributions to train the classifier. Gaussian distributions help amplify
intra-class uncertainty while ensuring the Score-Softmax classifier learns
accurate knowledge. Furthermore, leveraging the summation of independent
Gaussian distributed random variables, we introduced a multi-channel
information fusion method. This strategy effectively resolves the
multi-information fusion challenge for the Score-Softmax classifier.
Concurrently, we substantiate the necessity of transfer learning and
multi-dataset combination. We conducted cross-dataset experiments using the
SFD, AUCDD-V1, and 100-Driver datasets, demonstrating that Score-Softmax
improves cross-dataset performance without modifying the model architecture.
This provides a new approach for enhancing neural network generalization.
Additionally, our information fusion approach outperforms traditional methods.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05204" title="Abstract">arXiv:2310.05204</a> [<a href="/pdf/2310.05204" title="Download PDF">pdf</a>, <a href="/format/2310.05204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Optimizing with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Pei-Fu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Ying-Hsuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+Y">Yun-Da Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Shou-De Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this work, we conduct an assessment of the optimization capabilities of
LLMs across various tasks and data sizes. Each of these tasks corresponds to
unique optimization domains, and LLMs are required to execute these tasks with
interactive prompting. That is, in each optimization step, the LLM generates
new solutions from the past generated solutions with their values, and then the
new solutions are evaluated and considered in the next optimization step.
Additionally, we introduce three distinct metrics for a comprehensive
assessment of task performance from various perspectives. These metrics offer
the advantage of being applicable for evaluating LLM performance across a broad
spectrum of optimization tasks and are less sensitive to variations in test
samples. By applying these metrics, we observe that LLMs exhibit strong
optimization capabilities when dealing with small-sized samples. However, their
performance is significantly influenced by factors like data size and values,
underscoring the importance of further research in the domain of optimization
tasks for LLMs.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05205" title="Abstract">arXiv:2310.05205</a> [<a href="/pdf/2310.05205" title="Download PDF">pdf</a>, <a href="/format/2310.05205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GEAR: A GPU-Centric Experience Replay System for Large Reinforcement  Learning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hanjing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sit%2C+M">Man-Kit Sit</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Congjie He</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Ying Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+L">Luo Mai</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICML2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">This paper introduces a distributed, GPU-centric experience replay system,
GEAR, designed to perform scalable reinforcement learning (RL) with large
sequence models (such as transformers). With such models, existing systems such
as Reverb face considerable bottlenecks in memory, computation, and
communication. GEAR, however, optimizes memory efficiency by enabling the
memory resources on GPU servers (including host memory and device memory) to
manage trajectory data. Furthermore, it facilitates decentralized GPU devices
to expedite various trajectory selection strategies, circumventing
computational bottlenecks. GEAR is equipped with GPU kernels capable of
collecting trajectories using zero-copy access to host memory, along with
remote-directed-memory access over InfiniBand, improving communication
efficiency. Cluster experiments have shown that GEAR can achieve performance
levels up to 6x greater than Reverb when training state-of-the-art large RL
models. GEAR is open-sourced at https://github.com/bigrl-team/gear.
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05207" title="Abstract">arXiv:2310.05207</a> [<a href="/pdf/2310.05207" title="Download PDF">pdf</a>, <a href="/format/2310.05207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Facial Action Unit Detection Through Jointly Learning Facial  Landmark Detection and Domain Separation and Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shang%2C+Z">Ziqiao Shang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Li Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure, published to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently how to introduce large amounts of unlabeled facial images in the
wild into supervised Facial Action Unit (AU) detection frameworks has become a
challenging problem. In this paper, we propose a new AU detection framework
where multi-task learning is introduced to jointly learn AU domain separation
and reconstruction and facial landmark detection by sharing the parameters of
homostructural facial extraction modules. In addition, we propose a new feature
alignment scheme based on contrastive learning by simple projectors and an
improved contrastive loss, which adds four additional intermediate supervisors
to promote the feature reconstruction process. Experimental results on two
benchmarks demonstrate our superiority against the state-of-the-art methods for
AU detection in the wild.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05208" title="Abstract">arXiv:2310.05208</a> [<a href="/pdf/2310.05208" title="Download PDF">pdf</a>, <a href="/format/2310.05208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying Zero-shot Coordination Capability with Behavior Preferring  Partners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xihuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+W">Wentao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingxiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Ying Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Zero-shot coordination (ZSC) is a new challenge focusing on generalizing
learned coordination skills to unseen partners. Existing methods train the ego
agent with partners from pre-trained or evolving populations. The agent's ZSC
capability is typically evaluated with a few evaluation partners, including
human and agent, and reported by mean returns. Current evaluation methods for
ZSC capability still need to improve in constructing diverse evaluation
partners and comprehensively measuring the ZSC capability. We aim to create a
reliable, comprehensive, and efficient evaluation method for ZSC capability. We
formally define the ideal 'diversity-complete' evaluation partners and propose
the best response (BR) diversity, which is the population diversity of the BRs
to the partners, to approximate the ideal evaluation partners. We propose an
evaluation workflow including 'diversity-complete' evaluation partners
construction and a multi-dimensional metric, the Best Response Proximity
(BR-Prox) metric. BR-Prox quantifies the ZSC capability as the performance
similarity to each evaluation partner's approximate best response,
demonstrating generalization capability and improvement potential. We
re-evaluate strong ZSC methods in the Overcooked environment using the proposed
evaluation workflow. Surprisingly, the results in some of the most used layouts
fail to distinguish the performance of different ZSC methods. Moreover, the
evaluated ZSC methods must produce more diverse and high-performing training
partners. Our proposed evaluation workflow calls for a change in how we
efficiently evaluate ZSC methods as a supplement to human evaluation.
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05209" title="Abstract">arXiv:2310.05209</a> [<a href="/pdf/2310.05209" title="Download PDF">pdf</a>, <a href="/format/2310.05209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Laws of RoPE-based Extrapolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoran Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+C">Chenxin An</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The extrapolation capability of Large Language Models (LLMs) based on Rotary
Position Embedding is currently a topic of considerable interest. The
mainstream approach to addressing extrapolation with LLMs involves modifying
RoPE by replacing 10000, the rotary base of $\theta_n={10000}^{-2n/d}$ in the
original RoPE, with a larger value and providing longer fine-tuning text. In
this work, we first observe that fine-tuning a RoPE-based LLM with either a
smaller or larger base in pre-training context length could significantly
enhance its extrapolation performance. After that, we propose
\textbf{\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework
from the periodic perspective, to describe the relationship between the
extrapolation performance and base value as well as tuning context length. In
this process, we also explain the origin of the RoPE-based extrapolation issue
by \textbf{\textit{critical dimension for extrapolation}}. Besides these
observations and analyses, we achieve extrapolation up to 1 million context
length within only 16K training length on LLaMA2 7B and 13B.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05210" title="Abstract">arXiv:2310.05210</a> [<a href="/pdf/2310.05210" title="Download PDF">pdf</a>, <a href="/format/2310.05210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TILFA: A Unified Framework for Text, Image, and Layout Fusion in  Argument Mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zong%2C+Q">Qing Zong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Baixuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tianshi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Haochen Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+G+Y">Ginny Y. Wong</a>, 
<a href="/search/cs?searchtype=author&query=See%2C+S">Simon See</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 10th Workshop on Argument Mining, co-located with EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">A main goal of Argument Mining (AM) is to analyze an author's stance. Unlike
previous AM datasets focusing only on text, the shared task at the 10th
Workshop on Argument Mining introduces a dataset including both text and
images. Importantly, these images contain both visual elements and optical
characters. Our new framework, TILFA (A Unified Framework for Text, Image, and
Layout Fusion in Argument Mining), is designed to handle this mixed data. It
excels at not only understanding text but also detecting optical characters and
recognizing layout details in images. Our model significantly outperforms
existing baselines, earning our team, KnowComp, the 1st place in the
leaderboard of Argumentative Stance Classification subtask in this shared task.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05212" title="Abstract">arXiv:2310.05212</a> [<a href="/pdf/2310.05212" title="Download PDF">pdf</a>, <a href="/format/2310.05212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Semiotics Networks Representing Awareness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kupeev%2C+D">David Kupeev</a>, 
<a href="/search/cs?searchtype=author&query=Nitcany%2C+E">Eyal Nitcany</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 58 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Humans perceive objects daily and communicate their perceptions using various
channels. Here, we describe a computational model that track and simulate
objects' perception, and their representations as they pass in communication.
<br />We describe two key components of our internal representation ('observed' and
'seen') and relate them to familiar computer vision terms (encoding and
decoding). These elements joined together to form semiotic networks, which
simulate awareness in object perception and human communication.
<br />Nowadays, most neural networks are uninterpretable. On the other hand, our
model is free from this disadvantages. We performed several experiments and
demonstrated the visibility of our model.
<br />We describe how our network may be used as preprocessing unit to any
classification network. In our experiments the compound network overperforms in
average the classification network at datasets with small training data.
<br />Future work would leverage our model to gain better understanding of human
communications and personal representations.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05215" title="Abstract">arXiv:2310.05215</a> [<a href="/pdf/2310.05215" title="Download PDF">pdf</a>, <a href="/format/2310.05215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Motion Matching for Character Animation and Virtual Reality Avatars in  Unity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ponton%2C+J+L">Jose Luis Ponton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Master Thesis. GitHub Project: <a href="https://github.com/JLPM22/MotionMatching">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Real-time animation of virtual characters has traditionally been accomplished
by playing short sequences of animations structured in the form of a graph.
These methods are time-consuming to set up and scale poorly with the number of
motions required in modern virtual environments. The ever-increasing need for
highly-realistic virtual characters in fields such as entertainment, virtual
reality, or the metaverse has led to significant advances in the field of
data-driven character animation. Techniques like Motion Matching have provided
enough versatility to conveniently animate virtual characters using a selection
of features from an animation database. Data-driven methods retain the quality
of the captured animations, thus delivering smoother and more natural-looking
animations. In this work, we researched and developed a Motion Matching
technique for the Unity game engine. In this thesis, we present our findings on
how to implement an animation system based on Motion Matching. We also
introduce a novel method combining body orientation prediction with Motion
Matching to animate avatars for consumer-grade virtual reality systems.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05216" title="Abstract">arXiv:2310.05216</a> [<a href="/pdf/2310.05216" title="Download PDF">pdf</a>, <a href="/format/2310.05216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probing Language Models from A Human Behavioral Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingshan Li</a>, 
<a href="/search/cs?searchtype=author&query=Biemann%2C+C">Chris Biemann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have emerged as dominant foundational models in
modern NLP. However, the understanding of their prediction process and internal
mechanisms, such as feed-forward networks and multi-head self-attention,
remains largely unexplored. In this study, we probe LLMs from a human
behavioral perspective, correlating values from LLMs with eye-tracking
measures, which are widely recognized as meaningful indicators of reading
patterns. Our findings reveal that LLMs exhibit a prediction pattern distinct
from that of RNN-based LMs. Moreover, with the escalation of FFN layers, the
capacity for memorization and linguistic knowledge encoding also surges until
it peaks, subsequently pivoting to focus on comprehension capacity. The
functions of self-attention are distributed across multiple heads. Lastly, we
scrutinize the gate mechanisms, finding that they control the flow of
information, with some gates promoting, while others eliminating information.
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05218" title="Abstract">arXiv:2310.05218</a> [<a href="/pdf/2310.05218" title="Download PDF">pdf</a>, <a href="/ps/2310.05218" title="Download PostScript">ps</a>, <a href="/format/2310.05218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Machine Learning Primitives on Commodity Hardware
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Snytsar%2C+R">Roman Snytsar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Sliding Window Sum algorithms have been successfully used for training and
inference of Deep Neural Networks. We have shown before how both pooling and
convolution 1-D primitives could be expressed as sliding sums and evaluated by
the compute kernels with a shared structure. In this paper, we present an
extensive study of the Sliding Window convolution technique as a more efficient
alternative to the commonly used General Matrix Multiplication (GEMM) based
convolution in Deep Neural Networks (DNNs). The Sliding Window technique
addresses the memory bloating problem and demonstrates a significant speedup in
2-D convolution. We explore the performance of this technique on a range of
implementations, including custom kernels for specific filter sizes. Our
results suggest that the Sliding Window computation kernels can outperform
GEMM-based convolution on a CPU and even on dedicated hardware accelerators.
This could promote a wider adoption of AI on low-power and low-memory devices
without the need for specialized hardware. We also discuss the compatibility of
model compression methods and optimized network architectures with the Sliding
Window technique, encouraging further research in these areas.
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05222" title="Abstract">arXiv:2310.05222</a> [<a href="/pdf/2310.05222" title="Download PDF">pdf</a>, <a href="/format/2310.05222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ROSTAM: A Passwordless Web Single Sign-on Solution Mitigating Server  Breaches and Integrating Credential Manager and Federated Identity Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahnamfar%2C+A">Amin Mahnamfar</a>, 
<a href="/search/cs?searchtype=author&query=Bicakci%2C+K">Kemal Bicakci</a>, 
<a href="/search/cs?searchtype=author&query=Uzunay%2C+Y">Yusuf Uzunay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The challenge of achieving passwordless user authentication is real given the
prevalence of web applications that keep asking passwords. Complicating this
issue further, in an enterprise environment, a single sign-on (SSO) service is
often maintained but not all applications can be integrated with it. We
envision a passwordless future which provides a frictionless and trustworthy
online experience for users by integrating credential management and federated
identity systems. In this regard, our implementation ROSTAM offers a dashboard
that presents all applications the user can access with a single click after a
passwordless SSO. The security of web passwords on the credential manager is
ensured with a Master Key, rather than a Master Password, so that encrypted
passwords can remain secure even if stolen from the server. We propose and
implement novel techniques for synchronization (pairing) and recovery of this
Master Key. We compare our solution to previous work using different evaluation
frameworks, demonstrating that our hybrid solution combines the benefits of
credential management and federated identity systems.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05223" title="Abstract">arXiv:2310.05223</a> [<a href="/pdf/2310.05223" title="Download PDF">pdf</a>, <a href="/format/2310.05223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Automatic Test Generation Tools Generate Flaky Tests?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gruber%2C+M">Martin Gruber</a>, 
<a href="/search/cs?searchtype=author&query=Roslan%2C+M+F">Muhammad Firhard Roslan</a>, 
<a href="/search/cs?searchtype=author&query=Parry%2C+O">Owain Parry</a>, 
<a href="/search/cs?searchtype=author&query=Scharnb%C3%B6ck%2C+F">Fabian Scharnb&#xf6;ck</a>, 
<a href="/search/cs?searchtype=author&query=McMinn%2C+P">Phil McMinn</a>, 
<a href="/search/cs?searchtype=author&query=Fraser%2C+G">Gordon Fraser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, to be published in the Proceedings of the IEEE/ACM International Conference on Software Engineering (ICSE 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Non-deterministic test behavior, or flakiness, is common and dreaded among
developers. Researchers have studied the issue and proposed approaches to
mitigate it. However, the vast majority of previous work has only considered
developer-written tests. The prevalence and nature of flaky tests produced by
test generation tools remain largely unknown. We ask whether such tools also
produce flaky tests and how these differ from developer-written ones.
Furthermore, we evaluate mechanisms that suppress flaky test generation. We
sample 6 356 projects written in Java or Python. For each project, we generate
tests using EvoSuite (Java) and Pynguin (Python), and execute each test 200
times, looking for inconsistent outcomes. Our results show that flakiness is at
least as common in generated tests as in developer-written tests. Nevertheless,
existing flakiness suppression mechanisms implemented in EvoSuite are effective
in alleviating this issue (71.7 % fewer flaky tests). Compared to
developer-written flaky tests, the causes of generated flaky tests are
distributed differently. Their non-deterministic behavior is more frequently
caused by randomness, rather than by networking and concurrency. Using
flakiness suppression, the remaining flaky tests differ significantly from any
flakiness previously reported, where most are attributable to runtime
optimizations and EvoSuite-internal resource thresholds. These insights, with
the accompanying dataset, can help maintainers to improve test generation
tools, give recommendations for developers using these tools, and serve as a
foundation for future research in test flakiness or test generation.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05224" title="Abstract">arXiv:2310.05224</a> [<a href="/pdf/2310.05224" title="Download PDF">pdf</a>, <a href="/format/2310.05224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Spoken Language Model based on continuous word-sized audio  tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Algayres%2C+R">Robin Algayres</a>, 
<a href="/search/cs?searchtype=author&query=Adi%2C+Y">Yossi Adi</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+A">Tu Anh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Copet%2C+J">Jade Copet</a>, 
<a href="/search/cs?searchtype=author&query=Synnaeve%2C+G">Gabriel Synnaeve</a>, 
<a href="/search/cs?searchtype=author&query=Sagot%2C+B">Benoit Sagot</a>, 
<a href="/search/cs?searchtype=author&query=Dupoux%2C+E">Emmanuel Dupoux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference paper at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In NLP, text language models based on words or subwords are known to
outperform their character-based counterparts. Yet, in the speech community,
the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter
than a phoneme). Taking inspiration from word-based LM, we introduce a
Generative Spoken Language Model (GSLM) based on word-size continuous-valued
audio embeddings that can generate diverse and expressive language output. This
is obtained by replacing lookup table for lexical types with a Lexical
Embedding function, the cross entropy loss by a contrastive loss, and
multinomial sampling by k-NN sampling. The resulting model is the first
generative language model based on word-size continuous embeddings. Its
performance is on par with discrete unit GSLMs regarding generation quality as
measured by automatic metrics and subjective human judgements. Moreover, it is
five times more memory efficient thanks to its large 200ms units. In addition,
the embeddings before and after the Lexical Embedder are phonetically and
semantically interpretable.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05227" title="Abstract">arXiv:2310.05227</a> [<a href="/pdf/2310.05227" title="Download PDF">pdf</a>, <a href="/format/2310.05227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-aware Machine Learning Revolutionizes Scientific Paradigm for  Machine Learning and Process-based Hydrology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qingsong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yilei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Bamber%2C+J">Jonathan Bamber</a>, 
<a href="/search/cs?searchtype=author&query=Tuo%2C+Y">Ye Tuo</a>, 
<a href="/search/cs?searchtype=author&query=Ludwig%2C+R">Ralf Ludwig</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X+X">Xiao Xiang Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Accurate hydrological understanding and water cycle prediction are crucial
for addressing scientific and societal challenges associated with the
management of water resources, particularly under the dynamic influence of
anthropogenic climate change. Existing reviews predominantly concentrate on the
development of machine learning (ML) in this field, yet there is a clear
distinction between hydrology and ML as separate paradigms. Here, we introduce
physics-aware ML as a transformative approach to overcome the perceived barrier
and revolutionize both fields. Specifically, we present a comprehensive review
of the physics-aware ML methods, building a structured community (PaML) of
existing methodologies that integrate prior physical knowledge or physics-based
modeling into ML. We systematically analyze these PaML methodologies with
respect to four aspects: physical data-guided ML, physics-informed ML,
physics-embedded ML, and physics-aware hybrid learning. PaML facilitates
ML-aided hypotheses, accelerating insights from big data and fostering
scientific discoveries. We first conduct a systematic review of hydrology in
PaML, including rainfall-runoff hydrological processes and hydrodynamic
processes, and highlight the most promising and challenging directions for
different objectives and PaML methods. Finally, a new PaML-based hydrology
platform, termed HydroPML, is released as a foundation for hydrological
applications. HydroPML enhances the explainability and causality of ML and lays
the groundwork for the digital water cycle's realization. The HydroPML platform
is publicly available at https://github.com/HydroPML.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05231" title="Abstract">arXiv:2310.05231</a> [<a href="/pdf/2310.05231" title="Download PDF">pdf</a>, <a href="/format/2310.05231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MindfulDiary: Harnessing Large Language Model to Support Psychiatric  Patients&#x27; Journaling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taewan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+S">Seolyeong Bae</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H+A">Hyun Ah Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Su-woo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+H">Hwajung Hong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chanmo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Young-Ho Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">In the mental health domain, Large Language Models (LLMs) offer promising new
opportunities, though their inherent complexity and low controllability have
raised questions about their suitability in clinical settings. We present
MindfulDiary, a mobile journaling app incorporating an LLM to help psychiatric
patients document daily experiences through conversation. Designed in
collaboration with mental health professionals (MHPs), MindfulDiary takes a
state-based approach to safely comply with the experts' guidelines while
carrying on free-form conversations. Through a four-week field study involving
28 patients with major depressive disorder and five psychiatrists, we found
that MindfulDiary supported patients in consistently enriching their daily
records and helped psychiatrists better empathize with their patients through
an understanding of their thoughts and daily contexts. Drawing on these
findings, we discuss the implications of leveraging LLMs in the mental health
domain, bridging the technical feasibility and their integration into clinical
settings.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05235" title="Abstract">arXiv:2310.05235</a> [<a href="/pdf/2310.05235" title="Download PDF">pdf</a>, <a href="/format/2310.05235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XLS-R fine-tuning on noisy word boundaries for unsupervised speech  segmentation into words
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Algayres%2C+R">Robin Algayres</a>, 
<a href="/search/cs?searchtype=author&query=Diego-Simon%2C+P">Pablo Diego-Simon</a>, 
<a href="/search/cs?searchtype=author&query=Sagot%2C+B">Benoit Sagot</a>, 
<a href="/search/cs?searchtype=author&query=Dupoux%2C+E">Emmanuel Dupoux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Due to the absence of explicit word boundaries in the speech stream, the task
of segmenting spoken sentences into word units without text supervision is
particularly challenging. In this work, we leverage the most recent
self-supervised speech models that have proved to quickly adapt to new tasks
through fine-tuning, even in low resource conditions. Taking inspiration from
semi-supervised learning, we fine-tune an XLS-R model to predict word
boundaries themselves produced by top-tier speech segmentation systems: DPDP,
VG-HuBERT, GradSeg and DP-Parse. Once XLS-R is fine-tuned, it is used to infer
new word boundary labels that are used in turn for another fine-tuning step.
Our method consistently improves the performance of each system and sets a new
state-of-the-art that is, on average 130% higher than the previous one as
measured by the F1 score on correctly discovered word tokens on five corpora
featuring different languages. Finally, our system can segment speech from
languages unseen during fine-tuning in a zero-shot fashion.
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05236" title="Abstract">arXiv:2310.05236</a> [<a href="/pdf/2310.05236" title="Download PDF">pdf</a>, <a href="/format/2310.05236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Kernel Flexibility via Learning Asymmetric Locally-Adaptive  Kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+F">Fan He</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+M">Mingzhen He</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Lei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaolin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Suykens%2C+J+A+K">Johan A.K. Suykens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The lack of sufficient flexibility is the key bottleneck of kernel-based
learning that relies on manually designed, pre-given, and non-trainable
kernels. To enhance kernel flexibility, this paper introduces the concept of
Locally-Adaptive-Bandwidths (LAB) as trainable parameters to enhance the Radial
Basis Function (RBF) kernel, giving rise to the LAB RBF kernel. The parameters
in LAB RBF kernels are data-dependent, and its number can increase with the
dataset, allowing for better adaptation to diverse data patterns and enhancing
the flexibility of the learned function. This newfound flexibility also brings
challenges, particularly with regards to asymmetry and the need for an
efficient learning algorithm. To address these challenges, this paper for the
first time establishes an asymmetric kernel ridge regression framework and
introduces an iterative kernel learning algorithm. This novel approach not only
reduces the demand for extensive support data but also significantly improves
generalization by training bandwidths on the available training data.
Experimental results on real datasets underscore the remarkable performance of
the proposed algorithm, showcasing its superior capability in handling
large-scale datasets compared to Nystr\"om approximation-based algorithms.
Moreover, it demonstrates a significant improvement in regression accuracy over
existing kernel-based learning methods and even surpasses residual neural
networks.
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05239" title="Abstract">arXiv:2310.05239</a> [<a href="/pdf/2310.05239" title="Download PDF">pdf</a>, <a href="/format/2310.05239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LAN-grasp: Using Large Language Models for Semantic Object Grasping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mirjalili%2C+R">Reihaneh Mirjalili</a>, 
<a href="/search/cs?searchtype=author&query=Krawez%2C+M">Michael Krawez</a>, 
<a href="/search/cs?searchtype=author&query=Silenzi%2C+S">Simone Silenzi</a>, 
<a href="/search/cs?searchtype=author&query=Blei%2C+Y">Yannik Blei</a>, 
<a href="/search/cs?searchtype=author&query=Burgard%2C+W">Wolfram Burgard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this paper, we propose LAN-grasp, a novel approach towards more
appropriate semantic grasping. We use foundation models to provide the robot
with a deeper understanding of the objects, the right place to grasp an object,
or even the parts to avoid. This allows our robot to grasp and utilize objects
in a more meaningful and safe manner. We leverage the combination of a Large
Language Model, a Vision Language Model, and a traditional grasp planner to
generate grasps demonstrating a deeper semantic understanding of the objects.
We first prompt the Large Language Model about which object part is appropriate
for grasping. Next, the Vision Language Model identifies the corresponding part
in the object image. Finally, we generate grasp proposals in the region
proposed by the Vision Language Model. Building on foundation models provides
us with a zero-shot grasp method that can handle a wide range of objects
without the need for further training or fine-tuning. We evaluated our method
in real-world experiments on a custom object data set. We present the results
of a survey that asks the participants to choose an object part appropriate for
grasping. The results show that the grasps generated by our method are
consistently ranked higher by the participants than those generated by a
conventional grasping planner and a recent semantic grasping approach.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05240" title="Abstract">arXiv:2310.05240</a> [<a href="/pdf/2310.05240" title="Download PDF">pdf</a>, <a href="/ps/2310.05240" title="Download PostScript">ps</a>, <a href="/format/2310.05240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Limitations of Stochastic Selection Problems with Pairwise Independent  Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dughmi%2C+S">Shaddin Dughmi</a>, 
<a href="/search/cs?searchtype=author&query=Kalayci%2C+Y+H">Yusuf Hakan Kalayci</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+N">Neel Patel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Motivated by the growing interest in correlation-robust stochastic
optimization, we investigate stochastic selection problems beyond independence.
Specifically, we consider the instructive case of pairwise-independent priors
and matroid constraints. We obtain essentially-optimal bounds for offline
contention resolution and prophet inequalities against the almighty online
adversary. The impetus for our work comes from the recent work of
\cite{pi-uniform-prophet}, who derived a constant-approximation for the
single-choice prophet inequality with pairwise-independent priors.
<br />For general matroids, our results are tight and largely negative. For both
contention resolution and prophet inequalities, our impossibility results hold
for the full linear matroid over a finite field. We explicitly construct
pairwise-independent distributions which rule out an
$\omega\left(\frac{1}{\rank}\right)$-balanced offline CRS and an
$\omega\left(\frac{1}{\log \rank}\right)$-competitive prophet inequality. For
both results, we employ a generic approach for constructing
pairwise-independent random vectors -- one which unifies and generalizes
existing pairwise-independence constructions from the literature on universal
hash functions and pseudorandomness. Specifically, our approach is based on our
observation that random linear maps turn linear independence into stochastic
independence.
<br />We then examine the class of matroids which satisfy the so-called partition
property -- these include most common matroids encountered in optimization. We
obtain positive results for both contention resolution and prophet inequalities
with pairwise-independent priors on such matroids, approximately matching the
corresponding guarantees for fully independent priors.
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05241" title="Abstract">arXiv:2310.05241</a> [<a href="/pdf/2310.05241" title="Download PDF">pdf</a>, <a href="/format/2310.05241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SCANet: Scene Complexity Aware Network for Weakly-Supervised Video  Moment Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Sunjae Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Koo%2C+G">Gwanhyeong Koo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dahyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+C+D">Chang D. Yoo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, Accepted in ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Video moment retrieval aims to localize moments in video corresponding to a
given language query. To avoid the expensive cost of annotating the temporal
moments, weakly-supervised VMR (wsVMR) systems have been studied. For such
systems, generating a number of proposals as moment candidates and then
selecting the most appropriate proposal has been a popular approach. These
proposals are assumed to contain many distinguishable scenes in a video as
candidates. However, existing proposals of wsVMR systems do not respect the
varying numbers of scenes in each video, where the proposals are heuristically
determined irrespective of the video. We argue that the retrieval system should
be able to counter the complexities caused by varying numbers of scenes in each
video. To this end, we present a novel concept of a retrieval system referred
to as Scene Complexity Aware Network (SCANet), which measures the `scene
complexity' of multiple scenes in each video and generates adaptive proposals
responding to variable complexities of scenes in each video. Experimental
results on three retrieval benchmarks (i.e., Charades-STA, ActivityNet, TVR)
achieve state-of-the-art performances and demonstrate the effectiveness of
incorporating the scene complexity.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05242" title="Abstract">arXiv:2310.05242</a> [<a href="/pdf/2310.05242" title="Download PDF">pdf</a>, <a href="/format/2310.05242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatRadio-Valuer: A Chat Large Language Model for Generalizable  Radiology Report Generation Based on Multi-institution and Multi-system Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+T">Tianyang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yutong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yi Pan</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+P">Peixin Dong</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zuowei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Kui%2C+X">Xiaoyan Kui</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+Y">Youlan Shang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Li Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yaonai Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Longtao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Huan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuxiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+N">Ning Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yisong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiaqi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Ying Zeng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Lei He</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhixue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Haixing Dai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zihao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xiaoyan Cai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xintao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shijie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Dajiang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+D">Dinggang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Junwei Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tuo Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Radiology report generation, as a key step in medical image analysis, is
critical to the quantitative analysis of clinically informed decision-making
levels. However, complex and diverse radiology reports with cross-source
heterogeneity pose a huge generalizability challenge to the current methods
under massive data volume, mainly because the style and normativity of
radiology reports are obviously distinctive among institutions, body regions
inspected and radiologists. Recently, the advent of large language models (LLM)
offers great potential for recognizing signs of health conditions. To resolve
the above problem, we collaborate with the Second Xiangya Hospital in China and
propose ChatRadio-Valuer based on the LLM, a tailored model for automatic
radiology report generation that learns generalizable representations and
provides a basis pattern for model adaptation in sophisticated analysts' cases.
Specifically, ChatRadio-Valuer is trained based on the radiology reports from a
single institution by means of supervised fine-tuning, and then adapted to
disease diagnosis tasks for human multi-system evaluation (i.e., chest,
abdomen, muscle-skeleton, head, and maxillofacial $\&amp;$ neck) from six different
institutions in clinical-level events. The clinical dataset utilized in this
study encompasses a remarkable total of \textbf{332,673} observations. From the
comprehensive results on engineering indicators, clinical efficacy and
deployment cost metrics, it can be shown that ChatRadio-Valuer consistently
outperforms state-of-the-art models, especially ChatGPT (GPT-3.5-Turbo) and
GPT-4 et al., in terms of the diseases diagnosis from radiology reports.
ChatRadio-Valuer provides an effective avenue to boost model generalization
performance and alleviate the annotation workload of experts to enable the
promotion of clinical AI applications in radiology reports.
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05245" title="Abstract">arXiv:2310.05245</a> [<a href="/pdf/2310.05245" title="Download PDF">pdf</a>, <a href="/format/2310.05245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Influence of Camera-LiDAR Configuration on 3D Object Detection for  Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Ye Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hanjiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Ding Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Cameras and LiDARs are both important sensors for autonomous driving, playing
critical roles for 3D object detection. Camera-LiDAR Fusion has been a
prevalent solution for robust and accurate autonomous driving perception. In
contrast to the vast majority of existing arts that focus on how to improve the
performance of 3D target detection through cross-modal schemes, deep learning
algorithms, and training tricks, we devote attention to the impact of sensor
configurations on the performance of learning-based methods. To achieve this,
we propose a unified information-theoretic surrogate metric for camera and
LiDAR evaluation based on the proposed sensor perception model. We also design
an accelerated high-quality framework for data acquisition, model training, and
performance evaluation that functions with the CARLA simulator. To show the
correlation between detection performance and our surrogate metrics, We conduct
experiments using several camera-LiDAR placements and parameters inspired by
self-driving companies and research institutions. Extensive experimental
results of representative algorithms on NuScenes dataset validate the
effectiveness of our surrogate metric, demonstrating that sensor configurations
significantly impact point-cloud-image fusion based detection models, which
contribute up to 30% discrepancy in terms of average precision.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05249" title="Abstract">arXiv:2310.05249</a> [<a href="/pdf/2310.05249" title="Download PDF">pdf</a>, <a href="/format/2310.05249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Convergence of Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yingbin Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 74 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Transformers have recently revolutionized many domains in modern machine
learning and one salient discovery is their remarkable in-context learning
capability, where models can solve an unseen task by utilizing task-specific
prompts without further parameters fine-tuning. This also inspired recent
theoretical studies aiming to understand the in-context learning mechanism of
transformers, which however focused only on linear transformers. In this work,
we take the first step toward studying the learning dynamics of a one-layer
transformer with softmax attention trained via gradient descent in order to
in-context learn linear function classes. We consider a structured data model,
where each token is randomly sampled from a set of feature vectors in either
balanced or imbalanced fashion. For data with balanced features, we establish
the finite-time convergence guarantee with near-zero prediction error by
navigating our analysis over two phases of the training dynamics of the
attention map. More notably, for data with imbalanced features, we show that
the learning dynamics take a stage-wise convergence process, where the
transformer first converges to a near-zero prediction error for the query
tokens of dominant features, and then converges later to a near-zero prediction
error for the query tokens of under-represented features, respectively via one
and four training phases. Our proof features new techniques for analyzing the
competing strengths of two types of attention weights, the change of which
determines different training phases.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05250" title="Abstract">arXiv:2310.05250</a> [<a href="/pdf/2310.05250" title="Download PDF">pdf</a>, <a href="/format/2310.05250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simplifying GNN Performance with Low Rank Kernel Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vinas%2C+L">Luciano Vinas</a>, 
<a href="/search/cs?searchtype=author&query=Amini%2C+A+A">Arash A. Amini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We revisit recent spectral GNN approaches to semi-supervised node
classification (SSNC). We posit that many of the current GNN architectures may
be over-engineered. Instead, simpler, traditional methods from nonparametric
estimation, applied in the spectral domain, could replace many deep-learning
inspired GNN designs. These conventional techniques appear to be well suited
for a variety of graph types reaching state-of-the-art performance on many of
the common SSNC benchmarks. Additionally, we show that recent performance
improvements in GNN approaches may be partially attributed to shifts in
evaluation conventions. Lastly, an ablative study is conducted on the various
hyperparameters associated with GNN spectral filtering techniques. Code
available at: https://github.com/lucianoAvinas/lowrank-gnn-kernels
</p>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05253" title="Abstract">arXiv:2310.05253</a> [<a href="/pdf/2310.05253" title="Download PDF">pdf</a>, <a href="/format/2310.05253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Claim Verification via Knowledge-Grounded Reasoning with  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+K">Kai Shu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Claim verification plays a crucial role in combating misinformation. While
existing works on claim verification have shown promising results, a crucial
piece of the puzzle that remains unsolved is to understand how to verify claims
without relying on human-annotated data, which is expensive to create at a
large scale. Additionally, it is important for models to provide comprehensive
explanations that can justify their decisions and assist human fact-checkers.
This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK)
Reasoning that can verify complex claims and generate explanations without the
need for annotated evidence using Large Language Models (LLMs). FOLK leverages
the in-context learning ability of LLMs to translate the claim into a
First-Order-Logic (FOL) clause consisting of predicates, each corresponding to
a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning
over a set of knowledge-grounded question-and-answer pairs to make veracity
predictions and generate explanations to justify its decision-making process.
This process makes our model highly explanatory, providing clear explanations
of its reasoning process in human-readable form. Our experiment results
indicate that FOLK outperforms strong baselines on three datasets encompassing
various claim verification challenges. Our code and data are available.
</p>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05255" title="Abstract">arXiv:2310.05255</a> [<a href="/pdf/2310.05255" title="Download PDF">pdf</a>, <a href="/format/2310.05255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Persis: A Persian Font Recognition Pipeline Using Convolutional Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohammadian%2C+M">Mehrdad Mohammadian</a>, 
<a href="/search/cs?searchtype=author&query=Maleki%2C+N">Neda Maleki</a>, 
<a href="/search/cs?searchtype=author&query=Olsson%2C+T">Tobias Olsson</a>, 
<a href="/search/cs?searchtype=author&query=Ahlgren%2C+F">Fredrik Ahlgren</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 12th International Conference on Computer and Knowledge
  Engineering (ICCKE) (2022) 196-204
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">What happens if we encounter a suitable font for our design work but do not
know its name? Visual Font Recognition (VFR) systems are used to identify the
font typeface in an image. These systems can assist graphic designers in
identifying fonts used in images. A VFR system also aids in improving the speed
and accuracy of Optical Character Recognition (OCR) systems. In this paper, we
introduce the first publicly available datasets in the field of Persian font
recognition and employ Convolutional Neural Networks (CNN) to address this
problem. The results show that the proposed pipeline obtained 78.0% top-1
accuracy on our new datasets, 89.1% on the IDPL-PFOD dataset, and 94.5% on the
KAFD dataset. Furthermore, the average time spent in the entire pipeline for
one sample of our proposed datasets is 0.54 and 0.017 seconds for CPU and GPU,
respectively. We conclude that CNN methods can be used to recognize Persian
fonts without the need for additional pre-processing steps such as feature
extraction, binarization, normalization, etc.
</p>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05258" title="Abstract">arXiv:2310.05258</a> [<a href="/pdf/2310.05258" title="Download PDF">pdf</a>, <a href="/format/2310.05258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Knowledge Graph-Based Search Engine for Robustly Finding Doctors and  Locations in the Healthcare Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kejriwal%2C+M">Mayank Kejriwal</a>, 
<a href="/search/cs?searchtype=author&query=Haidarian%2C+H">Hamid Haidarian</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+M">Min-Hsueh Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+A">Andy Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Shrestha%2C+D">Deep Shrestha</a>, 
<a href="/search/cs?searchtype=author&query=Javed%2C+F">Faizan Javed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented as an applied data science poster in KDD 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Databases (cs.DB); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Efficiently finding doctors and locations is an important search problem for
patients in the healthcare domain, for which traditional information retrieval
methods tend not to work optimally. In the last ten years, knowledge graphs
(KGs) have emerged as a powerful way to combine the benefits of gleaning
insights from semi-structured data using semantic modeling, natural language
processing techniques like information extraction, and robust querying using
structured query languages like SPARQL and Cypher. In this short paper, we
present a KG-based search engine architecture for robustly finding doctors and
locations in the healthcare domain. Early results demonstrate that our approach
can lead to significantly higher coverage for complex queries without degrading
quality.
</p>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05261" title="Abstract">arXiv:2310.05261</a> [<a href="/pdf/2310.05261" title="Download PDF">pdf</a>, <a href="/format/2310.05261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-Varying Soft-Maximum Control Barrier Functions for Safety in an A  Priori Unknown Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Safari%2C+A">Amirsaeid Safari</a>, 
<a href="/search/eess?searchtype=author&query=Hoagg%2C+J+B">Jesse B. Hoagg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to 2024 American Control Conference (ACC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper presents a time-varying soft-maximum composite control barrier
function (CBF) that can be used to ensure safety in an a priori unknown
environment, where local perception information regarding the safe set is
periodically obtained. We consider the scenario where the periodically obtained
perception feedback can be used to construct a local CBF that models a local
subset of the unknown safe set. Then, we use a novel smooth time-varying
soft-maximum function to compose the N most recently obtained local CBFs into a
single CBF. This composite CBF models an approximate union of the N most
recently obtained local subsets of the safe set. Notably, this composite CBF
can have arbitrary relative degree r. Next, this composite CBF is used as a
rth-order CBF constraint in a real-time-optimization to determine a control
that minimizes a quadratic cost while guaranteeing that the state stays in a
time-varying subset of the unknown safe set. We also present 2 applications of
the time-varying soft-maximum composite CBF method: (1) a nonholonomic ground
robot with nonnegligible inertia, and (2) a quadrotor aerial robot. In these
applications, we present a simple new approach to generate the local CBFs from
the periodically obtained perception data.
</p>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05262" title="Abstract">arXiv:2310.05262</a> [<a href="/pdf/2310.05262" title="Download PDF">pdf</a>, <a href="/format/2310.05262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure-Preserving Instance Segmentation via Skeleton-Aware Distance  Transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zudi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+D">Donglai Wei</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Aarush Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xingyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+D">Deqing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+H">Hanspeter Pfister</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MICCAI 2023 (Oral Presentation)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Objects with complex structures pose significant challenges to existing
instance segmentation methods that rely on boundary or affinity maps, which are
vulnerable to small errors around contacting pixels that cause noticeable
connectivity change. While the distance transform (DT) makes instance interiors
and boundaries more distinguishable, it tends to overlook the intra-object
connectivity for instances with varying width and result in over-segmentation.
To address these challenges, we propose a skeleton-aware distance transform
(SDT) that combines the merits of object skeleton in preserving connectivity
and DT in modeling geometric arrangement to represent instances with arbitrary
structures. Comprehensive experiments on histopathology image segmentation
demonstrate that SDT achieves state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05263" title="Abstract">arXiv:2310.05263</a> [<a href="/pdf/2310.05263" title="Download PDF">pdf</a>, <a href="/format/2310.05263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confidence-driven Sampling for Backdoor Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengfei He</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Han Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+Y">Yue Xing</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yingqian Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Shenglai Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yamada%2C+M">Makoto Yamada</a>, 
<a href="/search/cs?searchtype=author&query=Sabokrou%2C+M">Mohammad Sabokrou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Backdoor attacks aim to surreptitiously insert malicious triggers into DNN
models, granting unauthorized control during testing scenarios. Existing
methods lack robustness against defense strategies and predominantly focus on
enhancing trigger stealthiness while randomly selecting poisoned samples. Our
research highlights the overlooked drawbacks of random sampling, which make
that attack detectable and defensible. The core idea of this paper is to
strategically poison samples near the model's decision boundary and increase
defense difficulty. We introduce a straightforward yet highly effective
sampling methodology that leverages confidence scores. Specifically, it selects
samples with lower confidence scores, significantly increasing the challenge
for defenders in identifying and countering these attacks. Importantly, our
method operates independently of existing trigger designs, providing
versatility and compatibility with various backdoor attack techniques. We
substantiate the effectiveness of our approach through a comprehensive set of
empirical experiments, demonstrating its potential to significantly enhance
resilience against backdoor attacks in DNNs.
</p>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05264" title="Abstract">arXiv:2310.05264</a> [<a href="/pdf/2310.05264" title="Download PDF">pdf</a>, <a href="/format/2310.05264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Emergence of Reproducibility and Consistency in Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huijie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jinfan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yifu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Minzhe Guo</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Liyue Shen</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Q">Qing Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recently, diffusion models have emerged as powerful deep generative models,
showcasing cutting-edge performance across various applications such as image
generation, solving inverse problems, and text-to-image synthesis. These models
generate new data (e.g., images) by transforming random noise inputs through a
reverse diffusion process. In this work, we uncover a distinct and prevalent
phenomenon within diffusion models in contrast to most other generative models,
which we refer to as ``consistent model reproducibility''. To elaborate, our
extensive experiments have consistently shown that when starting with the same
initial noise input and sampling with a deterministic solver, diffusion models
tend to produce nearly identical output content. This consistency holds true
regardless of the choices of model architectures and training procedures.
Additionally, our research has unveiled that this exceptional model
reproducibility manifests in two distinct training regimes: (i) ``memorization
regime,'' characterized by a significantly overparameterized model which
attains reproducibility mainly by memorizing the training data; (ii)
``generalization regime,'' in which the model is trained on an extensive
dataset, and its reproducibility emerges with the model's generalization
capabilities. Our analysis provides theoretical justification for the model
reproducibility in ``memorization regime''. Moreover, our research reveals that
this valuable property generalizes to many variants of diffusion models,
including conditional diffusion models, diffusion models for solving inverse
problems, and fine-tuned diffusion models. A deeper understanding of this
phenomenon has the potential to yield more interpretable and controllable data
generative processes based on diffusion models.
</p>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05266" title="Abstract">arXiv:2310.05266</a> [<a href="/pdf/2310.05266" title="Download PDF">pdf</a>, <a href="/format/2310.05266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DELTAHANDS: A Synergistic Dexterous Hand Framework Based on Delta Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Si%2C+Z">Zilin Si</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kevin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kroemer%2C+O">Oliver Kroemer</a>, 
<a href="/search/cs?searchtype=author&query=Temel%2C+F+Z">F. Zeynep Temel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Dexterous robotic manipulation in unstructured environments can aid in
everyday tasks such as cleaning and caretaking. Anthropomorphic robotic hands
are highly dexterous and theoretically well-suited for working in human
domains, but their complex designs and dynamics often make them difficult to
control. By contrast, parallel-jaw grippers are easy to control and are used
extensively in industrial applications, but they lack the dexterity for various
kinds of grasps and in-hand manipulations. In this work, we present DELTAHANDS,
a synergistic dexterous hand framework with Delta robots. The DELTAHANDS are
soft, easy to reconfigure, simple to manufacture with low-cost off-the-shelf
materials, and possess high degrees of freedom that can be easily controlled.
DELTAHANDS' dexterity can be adjusted for different applications by leveraging
actuation synergies, which can further reduce the control complexity, overall
cost, and energy consumption. We characterize the Delta robots' kinematics
accuracy, force profiles, and workspace range to assist with hand design.
Finally, we evaluate the versatility of DELTAHANDS by grasping a diverse set of
objects and by using teleoperation to complete three dexterous manipulation
tasks: cloth folding, cap opening, and cable arrangement.
</p>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05269" title="Abstract">arXiv:2310.05269</a> [<a href="/pdf/2310.05269" title="Download PDF">pdf</a>, <a href="/format/2310.05269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning: A Cutting-Edge Survey of the Latest Advancements and  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akhtarshenas%2C+A">Azim Akhtarshenas</a>, 
<a href="/search/cs?searchtype=author&query=Vahedifar%2C+M+A">Mohammad Ali Vahedifar</a>, 
<a href="/search/cs?searchtype=author&query=Ayoobi%2C+N">Navid Ayoobi</a>, 
<a href="/search/cs?searchtype=author&query=Maham%2C+B">Behrouz Maham</a>, 
<a href="/search/cs?searchtype=author&query=Alizadeh%2C+T">Tohid Alizadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">In the realm of machine learning (ML) systems featuring client-host
connections, the enhancement of privacy security can be effectively achieved
through federated learning (FL) as a secure distributed ML methodology. FL
effectively integrates cloud infrastructure to transfer ML models onto edge
servers using blockchain technology. Through this mechanism, it guarantees the
streamlined processing and data storage requirements of both centralized and
decentralized systems, with an emphasis on scalability, privacy considerations,
and cost-effective communication. In current FL implementations, data owners
locally train their models, and subsequently upload the outcomes in the form of
weights, gradients, and parameters to the cloud for overall model aggregation.
This innovation obviates the necessity of engaging Internet of Things (IoT)
clients and participants to communicate raw and potentially confidential data
directly with a cloud center. This not only reduces the costs associated with
communication networks but also enhances the protection of private data. This
survey conducts an analysis and comparison of recent FL applications, aiming to
assess their efficiency, accuracy, and privacy protection. However, in light of
the complex and evolving nature of FL, it becomes evident that additional
research is imperative to address lingering knowledge gaps and effectively
confront the forthcoming challenges in this field. In this study, we categorize
recent literature into the following clusters: privacy protection, resource
allocation, case study analysis, and applications. Furthermore, at the end of
each section, we tabulate the open areas and future directions presented in the
referenced literature, affording researchers and scholars an insightful view of
the evolution of the field.
</p>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05270" title="Abstract">arXiv:2310.05270</a> [<a href="/pdf/2310.05270" title="Download PDF">pdf</a>, <a href="/format/2310.05270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transforming Pixels into a Masterpiece: AI-Powered Art Restoration using  a Novel Distributed Denoising CNN (DDCNN)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=B.%2C+S">Sankar B.</a>, 
<a href="/search/cs?searchtype=author&query=Saravanan%2C+M">Mukil Saravanan</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+K">Kalaivanan Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Dubbaka%2C+S">Siri Dubbaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Art restoration is crucial for preserving cultural heritage, but traditional
methods have limitations in faithfully reproducing original artworks while
addressing issues like fading, staining, and damage. We present an innovative
approach using deep learning, specifically Convolutional Neural Networks
(CNNs), and Computer Vision techniques to revolutionize art restoration. We
start by creating a diverse dataset of deteriorated art images with various
distortions and degradation levels. This dataset trains a Distributed Denoising
CNN (DDCNN) to remove distortions while preserving intricate details. Our
method is adaptable to different distortion types and levels, making it
suitable for various deteriorated artworks, including paintings, sketches, and
photographs. Extensive experiments demonstrate our approach's efficiency and
effectiveness compared to other Denoising CNN models. We achieve a substantial
reduction in distortion, transforming deteriorated artworks into masterpieces.
Quantitative evaluations confirm our method's superiority over traditional
techniques, reshaping the art restoration field and preserving cultural
heritage. In summary, our paper introduces an AI-powered solution that combines
Computer Vision and deep learning with DDCNN to restore artworks accurately,
overcoming limitations and paving the way for future advancements in art
restoration.
</p>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05276" title="Abstract">arXiv:2310.05276</a> [<a href="/pdf/2310.05276" title="Download PDF">pdf</a>, <a href="/format/2310.05276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Pre-Trained Language Models with Sentence Position Embeddings  for Rhetorical Roles Recognition in Legal Opinions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belfathi%2C+A">Anas Belfathi</a>, 
<a href="/search/cs?searchtype=author&query=Hernandez%2C+N">Nicolas Hernandez</a>, 
<a href="/search/cs?searchtype=author&query=Monceaux%2C+L">Laura Monceaux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Workshop on Automated Semantic Analysis of Information in Legal Text
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ASAIL 2023: Proceedings of the Sixth Workshop on Automated
  Semantic Analysis of Information in Legal Text (ASAIL 2023), June 23, 2023,
  Braga, Portugal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The legal domain is a vast and complex field that involves a considerable
amount of text analysis, including laws, legal arguments, and legal opinions.
Legal practitioners must analyze these texts to understand legal cases,
research legal precedents, and prepare legal documents. The size of legal
opinions continues to grow, making it increasingly challenging to develop a
model that can accurately predict the rhetorical roles of legal opinions given
their complexity and diversity. In this research paper, we propose a novel
model architecture for automatically predicting rhetorical roles using
pre-trained language models (PLMs) enhanced with knowledge of sentence position
information within a document. Based on an annotated corpus from the
LegalEval@SemEval2023 competition, we demonstrate that our approach requires
fewer parameters, resulting in lower computational costs when compared to
complex architectures employing a hierarchical model in a global-context, yet
it achieves great performance. Moreover, we show that adding more attention to
a hierarchical model based only on BERT in the local-context, along with
incorporating sentence position information, enhances the results.
</p>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05280" title="Abstract">arXiv:2310.05280</a> [<a href="/pdf/2310.05280" title="Download PDF">pdf</a>, <a href="/format/2310.05280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona  Biases in Dialogue Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yixin Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jieyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements in Large Language Models empower them to follow freeform
instructions, including imitating generic or specific demographic personas in
conversations. Generic personas refer to an individual from a demographic group
(e.g. an Asian person), whereas specific personas can be actual names of
historical figures. While the adoption of personas allows dialogue systems to
be more engaging and approachable to users, it also carries the potential risk
of exacerbating social biases in model responses, further causing societal
harms through interactions with users. In this paper, we systematically study
"persona biases", which we define to be the sensitivity of harmful dialogue
model behaviors to different persona adoptions. We categorize persona biases
into biases in harmful expression and harmful agreement, as well as establish a
comprehensive evaluation framework to measure persona biases in five aspects:
Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and Toxic
Agreement. Additionally, we propose to comprehensively investigate persona
biases through experimenting with UniversalPersona, a systematized persona
dataset with a comprehensive list of both generic and specific model personas.
Through benchmarking on four different models, including Blender, ChatGPT,
Alpaca, and Vicuna, our study uncovers significant persona biases in these
dialogue systems.Findings of our study underscores the immediate need to
revisit the use of persona traits in dialogue agents, to ensure their safe
application.
</p>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05285" title="Abstract">arXiv:2310.05285</a> [<a href="/pdf/2310.05285" title="Download PDF">pdf</a>, <a href="/format/2310.05285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmented Flexible Krylov Subspace methods with applications to Bayesian  inverse problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Landman%2C+M+S">Malena Sabate Landman</a>, 
<a href="/search/math?searchtype=author&query=Jiang%2C+J">Jiahua Jiang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+J">Jianru Zhang</a>, 
<a href="/search/math?searchtype=author&query=Ren%2C+W">Wuwei Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper presents two new augmented flexible (AF)-Krylov subspace methods,
AF-GMRES and AF-LSQR, to compute solutions of large-scale linear discrete
ill-posed problems that can be modeled as the sum of two independent random
variables, exhibiting smooth and sparse stochastic characteristics
respectively. Following a Bayesian modelling approach, this corresponds to
adding a covariance-weighted quadratic term and a sparsity enforcing $\ell_1$
term in the original least-squares minimization scheme. To handle the $\ell_1$
regularization term, the proposed approach constructs a sequence approximating
quadratic problems that are partially solved using augmented flexible
Krylov-Tikhonov methods.
<br />Compared to other traditional methods used to solve this minimization
problem, such as those based on iteratively reweighted norm schemes, the new
algorithms build a single (augmented, flexible) approximation (Krylov) subspace
that encodes information about the different regularization terms through
adaptable "preconditioning". The solution space is then expanded as soon as a
new problem within the sequence is defined. This also allows for the
regularization parameters to be chosen on-the-fly at each iteration. Compared
to most recent work on generalized flexible Krylov methods, our methods offer
theoretical assurance of convergence and a more stable numerical performance.
The efficiency of the new methods is shown through a variety of experiments,
including a synthetic image deblurring problem, a synthetic atmospheric
transport problem, and fluorescence molecular tomography reconstructions using
both synthetic and real-world experimental data.
</p>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05286" title="Abstract">arXiv:2310.05286</a> [<a href="/pdf/2310.05286" title="Download PDF">pdf</a>, <a href="/format/2310.05286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizable Error Modeling for Search Relevance Data Annotation Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peters%2C+H">Heinrich Peters</a>, 
<a href="/search/cs?searchtype=author&query=Hashemi%2C+A">Alireza Hashemi</a>, 
<a href="/search/cs?searchtype=author&query=Rae%2C+J">James Rae</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Human data annotation is critical in shaping the quality of machine learning
(ML) and artificial intelligence (AI) systems. One significant challenge in
this context is posed by annotation errors, as their effects can degrade the
performance of ML models. This paper presents a predictive error model trained
to detect potential errors in search relevance annotation tasks for three
industry-scale ML applications (music streaming, video streaming, and mobile
apps) and assesses its potential to enhance the quality and efficiency of the
data annotation process. Drawing on real-world data from an extensive search
relevance annotation program, we illustrate that errors can be predicted with
moderate model performance (AUC=0.65-0.75) and that model performance
generalizes well across applications (i.e., a global, task-agnostic model
performs on par with task-specific models). We present model explainability
analyses to identify which types of features are the main drivers of predictive
performance. Additionally, we demonstrate the usefulness of the model in the
context of auditing, where prioritizing tasks with high predicted error
probabilities considerably increases the amount of corrected annotation errors
(e.g., 40% efficiency gains for the music streaming application). These results
underscore that automated error detection models can yield considerable
improvements in the efficiency and quality of data annotation processes. Thus,
our findings reveal critical insights into effective error management in the
data annotation process, thereby contributing to the broader field of
human-in-the-loop ML.
</p>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05290" title="Abstract">arXiv:2310.05290</a> [<a href="/pdf/2310.05290" title="Download PDF">pdf</a>, <a href="/format/2310.05290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MSight: An Edge-Cloud Infrastructure-based Perception System for  Connected Automated Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rusheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+D">Depu Meng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shengyin Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Z">Zhengxia Zou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Houqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H+X">Henry X. Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE T-ITS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">As vehicular communication and networking technologies continue to advance,
infrastructure-based roadside perception emerges as a pivotal tool for
connected automated vehicle (CAV) applications. Due to their elevated
positioning, roadside sensors, including cameras and lidars, often enjoy
unobstructed views with diminished object occlusion. This provides them a
distinct advantage over onboard perception, enabling more robust and accurate
detection of road objects. This paper presents MSight, a cutting-edge roadside
perception system specifically designed for CAVs. MSight offers real-time
vehicle detection, localization, tracking, and short-term trajectory
prediction. Evaluations underscore the system's capability to uphold lane-level
accuracy with minimal latency, revealing a range of potential applications to
enhance CAV safety and efficiency. Presently, MSight operates 24/7 at a
two-lane roundabout in the City of Ann Arbor, Michigan.
</p>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05292" title="Abstract">arXiv:2310.05292</a> [<a href="/pdf/2310.05292" title="Download PDF">pdf</a>, <a href="/format/2310.05292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HypoCompass: Large-Language-Model-based Tutor for Hypothesis  Construction in Debugging for Novices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qianou Ma</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Hua Shen</a>, 
<a href="/search/cs?searchtype=author&query=Koedinger%2C+K">Kenneth Koedinger</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tongshuang Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">With the prevalence of imperfect but capable LLMs in software development, it
becomes increasingly important for developers to cultivate debugging skills --
to form hypotheses about the source of error in both their own codes and codes
produced by their AI pair programmers. Despite the necessity, hypothesis
construction in debugging is rarely taught due to a lack of explicit
instruction. In this work, we explore whether LLMs can be used to train novices
on hypothesis construction, by designing a theoretically motivated,
LLM-augmented tutor -- HypoCompass. HypoCompass relies on LLMs for generating
rich training materials guided by learning principles and presents them in a
learning-by-teaching environment, where LLMs act as students who write bugs and
attempt to fix them, and human novices focus on debugging in the role of a
Teaching Assistant. Evaluations show that HypoCompass consistently generates
high-quality training materials, and brings significant learning gain: In a
pre-to-post test setup, 10 novices improved their performances by 17%, with a
reduced completion time of 13%.
</p>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05293" title="Abstract">arXiv:2310.05293</a> [<a href="/pdf/2310.05293" title="Download PDF">pdf</a>, <a href="/format/2310.05293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wait-free Trees with Asymptotically-Efficient Range Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kokorin%2C+I">Ilya Kokorin</a>, 
<a href="/search/cs?searchtype=author&query=Alistarh%2C+D">Dan Alistarh</a>, 
<a href="/search/cs?searchtype=author&query=Aksenov%2C+V">Vitaly Aksenov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Tree data structures, such as red-black trees, quad trees, treaps, or tries,
are fundamental tools in computer science. A classical problem in concurrency
is to obtain expressive, efficient, and scalable versions of practical tree
data structures. We are interested in concurrent trees supporting range
queries, i.e., queries that involve multiple consecutive data items. Existing
implementations with this capability can list keys in a specific range, but do
not support aggregate range queries: for instance, if we want to calculate the
number of keys in a range, the only choice is to retrieve a whole list and
return its size. This is suboptimal: in the sequential setting, one can augment
a balanced search tree with counters and, consequently, perform these aggregate
requests in logarithmic rather than linear time.
<br />In this paper, we propose a generic approach to implement a broad class of
range queries on concurrent trees in a way that is wait-free, asymptotically
efficient, and practically scalable. The key idea is a new mechanism for
maintaining metadata concurrently at tree nodes, which can be seen as a
wait-free variant of hand-over-hand locking (which we call hand-over-hand
helping). We implement, test, and benchmark a balanced binary search tree with
wait-free insert, delete, contains, and count operations, returning the number
of keys in a given range which validates the expected speedups because of our
method in practice.
</p>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05294" title="Abstract">arXiv:2310.05294</a> [<a href="/pdf/2310.05294" title="Download PDF">pdf</a>, <a href="/format/2310.05294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hi Guys or Hi Folks? Benchmarking Gender-Neutral Machine Translation  with the GeNTE Corpus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piergentili%2C+A">Andrea Piergentili</a>, 
<a href="/search/cs?searchtype=author&query=Savoldi%2C+B">Beatrice Savoldi</a>, 
<a href="/search/cs?searchtype=author&query=Fucci%2C+D">Dennis Fucci</a>, 
<a href="/search/cs?searchtype=author&query=Negri%2C+M">Matteo Negri</a>, 
<a href="/search/cs?searchtype=author&query=Bentivogli%2C+L">Luisa Bentivogli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Gender inequality is embedded in our communication practices and perpetuated
in translation technologies. This becomes particularly apparent when
translating into grammatical gender languages, where machine translation (MT)
often defaults to masculine and stereotypical representations by making undue
binary gender assumptions. Our work addresses the rising demand for inclusive
language by focusing head-on on gender-neutral translation from English to
Italian. We start from the essentials: proposing a dedicated benchmark and
exploring automated evaluation methods. First, we introduce GeNTE, a natural,
bilingual test set for gender-neutral translation, whose creation was informed
by a survey on the perception and use of neutral language. Based on GeNTE, we
then overview existing reference-based evaluation approaches, highlight their
limits, and propose a reference-free method more suitable to assess
gender-neutral translation.
</p>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05295" title="Abstract">arXiv:2310.05295</a> [<a href="/pdf/2310.05295" title="Download PDF">pdf</a>, <a href="/format/2310.05295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Storytelling with Question-Answer Plans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Danyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lapata%2C+M">Mirella Lapata</a>, 
<a href="/search/cs?searchtype=author&query=Keller%2C+F">Frank Keller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Visual storytelling aims to generate compelling narratives from image
sequences. Existing models often focus on enhancing the representation of the
image sequence, e.g., with external knowledge sources or advanced graph
structures. Despite recent progress, the stories are often repetitive,
illogical, and lacking in detail. To mitigate these issues, we present a novel
framework which integrates visual representations with pretrained language
models and planning. Our model translates the image sequence into a visual
prefix, a sequence of continuous embeddings which language models can
interpret. It also leverages a sequence of question-answer pairs as a blueprint
plan for selecting salient visual concepts and determining how they should be
assembled into a narrative. Automatic and human evaluation on the VIST
benchmark (Huang et al., 2016) demonstrates that blueprint-based models
generate stories that are more coherent, interesting, and natural compared to
competitive baselines and state-of-the-art systems.
</p>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05296" title="Abstract">arXiv:2310.05296</a> [<a href="/pdf/2310.05296" title="Download PDF">pdf</a>, <a href="/format/2310.05296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tailoring Self-Attention for Graph via Rooted Subtrees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Siyuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yunchong Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiayue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhouhan Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023. 23 pages in total with the appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Attention mechanisms have made significant strides in graph learning, yet
they still exhibit notable limitations: local attention faces challenges in
capturing long-range information due to the inherent problems of the
message-passing scheme, while global attention cannot reflect the hierarchical
neighborhood structure and fails to capture fine-grained local information. In
this paper, we propose a novel multi-hop graph attention mechanism, named
Subtree Attention (STA), to address the aforementioned issues. STA seamlessly
bridges the fully-attentional structure and the rooted subtree, with
theoretical proof that STA approximates the global attention under extreme
settings. By allowing direct computation of attention weights among multi-hop
neighbors, STA mitigates the inherent problems in existing graph attention
mechanisms. Further we devise an efficient form for STA by employing kernelized
softmax, which yields a linear time complexity. Our resulting GNN architecture,
the STAGNN, presents a simple yet performant STA-based graph neural network
leveraging a hop-aware attention strategy. Comprehensive evaluations on ten
node classification datasets demonstrate that STA-based models outperform
existing graph transformers and mainstream GNNs. The code is available at
https://github.com/LUMIA-Group/SubTree-Attention.
</p>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05298" title="Abstract">arXiv:2310.05298</a> [<a href="/pdf/2310.05298" title="Download PDF">pdf</a>, <a href="/format/2310.05298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Self-Adjusting Search Trees via Lazy Updates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Slastin%2C+A">Alexander Slastin</a>, 
<a href="/search/cs?searchtype=author&query=Alistarh%2C+D">Dan Alistarh</a>, 
<a href="/search/cs?searchtype=author&query=Aksenov%2C+V">Vitaly Aksenov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Self-adjusting data structures are a classic approach to adapting the
complexity of operations to the data access distribution. While several
self-adjusting variants are known for both binary search trees and B-Trees,
existing constructions come with limitations. For instance, existing works on
self-adjusting B-Trees do not provide static-optimality and tend to be complex
and inefficient to implement in practice. In this paper, we provide a new
approach to build efficient self-adjusting search trees based on
state-of-the-art non-adaptive structures. We illustrate our approach to obtain
a new efficient self-adjusting Interpolation Search Tree (IST) and B-Tree, as
well as a new self-adjusting tree called the Log Tree. Of note, our
self-adjusting IST has expected complexity in $O(\log \frac{\log m}{\log
ac(x)})$, where $m$ is the total number of requests and $ac(x)$ is the number
of requests to key $x$. Our technique leads to simple constructions with a
reduced number of pointer manipulations: this improves cache efficiency and
even allows an efficient concurrent implementation.
</p>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05304" title="Abstract">arXiv:2310.05304</a> [<a href="/pdf/2310.05304" title="Download PDF">pdf</a>, <a href="/format/2310.05304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GestSync: Determining who is speaking without a talking head
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hegde%2C+S+B">Sindhu B Hegde</a>, 
<a href="/search/cs?searchtype=author&query=Zisserman%2C+A">Andrew Zisserman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in BMVC 2023, 10 pages paper, 7 pages supplementary, 7 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper we introduce a new synchronisation task, Gesture-Sync:
determining if a person's gestures are correlated with their speech or not. In
comparison to Lip-Sync, Gesture-Sync is far more challenging as there is a far
looser relationship between the voice and body movement than there is between
voice and lip motion. We introduce a dual-encoder model for this task, and
compare a number of input representations including RGB frames, keypoint
images, and keypoint vectors, assessing their performance and advantages. We
show that the model can be trained using self-supervised learning alone, and
evaluate its performance on the LRS3 dataset. Finally, we demonstrate
applications of Gesture-Sync for audio-visual synchronisation, and in
determining who is the speaker in a crowd, without seeing their faces. The
code, datasets and pre-trained models can be found at:
\url{https://www.robots.ox.ac.uk/~vgg/research/gestsync}.
</p>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05305" title="Abstract">arXiv:2310.05305</a> [<a href="/pdf/2310.05305" title="Download PDF">pdf</a>, <a href="/format/2310.05305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Equitable Ride-sharing Car Distribution in a Congested Traffic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ramezani%2C+M">Mohamadreza Ramezani</a>, 
<a href="/search/eess?searchtype=author&query=Rastgoftar%2C+H">Hossein Rastgoftar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The usability of ride-sharing services like Uber and Lyft has been
considerably improved by advancements in cellular communications. Such a
tech-driven transportation system can reduce the number of private cars, in
roads with limited physical capacity, effectively match drivers with passengers
requesting service, and save drivers and passengers time by optimal scheduling
and estimating of the trips. However, the existing services offered by
ride-sharing companies may not necessarily reduce congestion, if they are not
equitably accessed in urban areas. This paper addresses this important issue by
classifying cars as ride-sharing and non-ride-sharing cars and developing a
novel dissensus-based traffic evolution dynamics to effectively model their
distribution in a shared network of interconnected roads (NOIR). This new
dynamics models the ride-sharing car evolution as a non-stationary Markov
process with uncertainty matrices decomposed into outflow and tendency
probability matrices, where outflow probabilities are constrained by the
non-ride-sharing cars behavior, but the tendency probabilities are considered
as decision variables (or distributed controls) for achieving an equitable
distribution of ride-sharing cars in a congested traffic.
</p>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05306" title="Abstract">arXiv:2310.05306</a> [<a href="/pdf/2310.05306" title="Download PDF">pdf</a>, <a href="/format/2310.05306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progressive Neural Compression for Adaptive Image Offloading under  Timing Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hanyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+J">Jiaming Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Moran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Guerin%2C+R">Roch Guerin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chenyang Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE the 44th Real-Time System Symposium (RTSS), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">IoT devices are increasingly the source of data for machine learning (ML)
applications running on edge servers. Data transmissions from devices to
servers are often over local wireless networks whose bandwidth is not just
limited but, more importantly, variable. Furthermore, in cyber-physical systems
interacting with the physical environment, image offloading is also commonly
subject to timing constraints. It is, therefore, important to develop an
adaptive approach that maximizes the inference performance of ML applications
under timing constraints and the resource constraints of IoT devices. In this
paper, we use image classification as our target application and propose
progressive neural compression (PNC) as an efficient solution to this problem.
Although neural compression has been used to compress images for different ML
applications, existing solutions often produce fixed-size outputs that are
unsuitable for timing-constrained offloading over variable bandwidth. To
address this limitation, we train a multi-objective rateless autoencoder that
optimizes for multiple compression rates via stochastic taildrop to create a
compression solution that produces features ordered according to their
importance to inference performance. Features are then transmitted in that
order based on available bandwidth, with classification ultimately performed
using the (sub)set of features received by the deadline. We demonstrate the
benefits of PNC over state-of-the-art neural compression approaches and
traditional compression methods on a testbed comprising an IoT device and an
edge server connected over a wireless network with varying bandwidth.
</p>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05307" title="Abstract">arXiv:2310.05307</a> [<a href="/pdf/2310.05307" title="Download PDF">pdf</a>, <a href="/format/2310.05307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Successive Data Injection in Conditional Quantum GAN Applied to Time  Series Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalfon%2C+B">Benjamin Kalfon</a>, 
<a href="/search/cs?searchtype=author&query=Cherkaoui%2C+S">Soumaya Cherkaoui</a>, 
<a href="/search/cs?searchtype=author&query=Laprade%2C+J">Jean-Fr&#xe9;d&#xe9;ric Laprade</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+O">Ola Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shengrui Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Classical GAN architectures have shown interesting results for solving
anomaly detection problems in general and for time series anomalies in
particular, such as those arising in communication networks. In recent years,
several quantum GAN architectures have been proposed in the literature. When
detecting anomalies in time series using QGANs, huge challenges arise due to
the limited number of qubits compared to the size of the data. To address these
challenges, we propose a new high-dimensional encoding approach, named
Successive Data Injection (SuDaI). In this approach, we explore a larger
portion of the quantum state than that in the conventional angle encoding, the
method used predominantly in the literature, through repeated data injections
into the quantum state. SuDaI encoding allows us to adapt the QGAN for anomaly
detection with network data of a much higher dimensionality than with the
existing known QGANs implementations. In addition, SuDaI encoding applies to
other types of high-dimensional time series and can be used in contexts beyond
anomaly detection and QGANs, opening up therefore multiple fields of
application.
</p>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05308" title="Abstract">arXiv:2310.05308</a> [<a href="/pdf/2310.05308" title="Download PDF">pdf</a>, <a href="/format/2310.05308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Attacks on Combinatorial Multi-Armed Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+R">Rishab Balasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Tadepalli%2C+P">Prasad Tadepalli</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huazheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingyun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haoyu Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study reward poisoning attacks on Combinatorial Multi-armed Bandits
(CMAB). We first provide a sufficient and necessary condition for the
attackability of CMAB, which depends on the intrinsic properties of the
corresponding CMAB instance such as the reward distributions of super arms and
outcome distributions of base arms. Additionally, we devise an attack algorithm
for attackable CMAB instances. Contrary to prior understanding of multi-armed
bandits, our work reveals a surprising fact that the attackability of a
specific CMAB instance also depends on whether the bandit instance is known or
unknown to the adversary. This finding indicates that adversarial attacks on
CMAB are difficult in practice and a general attack strategy for any CMAB
instance does not exist since the environment is mostly unknown to the
adversary. We validate our theoretical findings via extensive experiments on
real-world CMAB applications including probabilistic maximum covering problem,
online minimum spanning tree, cascading bandits for online ranking, and online
shortest path.
</p>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05309" title="Abstract">arXiv:2310.05309</a> [<a href="/pdf/2310.05309" title="Download PDF">pdf</a>, <a href="/format/2310.05309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Solution-Samplers for Combinatorial Problems: The Landscape  of Policy-Gradient Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Caramanis%2C+C">Constantine Caramanis</a>, 
<a href="/search/cs?searchtype=author&query=Fotakis%2C+D">Dimitris Fotakis</a>, 
<a href="/search/cs?searchtype=author&query=Kalavasis%2C+A">Alkis Kalavasis</a>, 
<a href="/search/cs?searchtype=author&query=Kontonis%2C+V">Vasilis Kontonis</a>, 
<a href="/search/cs?searchtype=author&query=Tzamos%2C+C">Christos Tzamos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)

</div>
<p class="mathjax">Deep Neural Networks and Reinforcement Learning methods have empirically
shown great promise in tackling challenging combinatorial problems. In those
methods a deep neural network is used as a solution generator which is then
trained by gradient-based methods (e.g., policy gradient) to successively
obtain better solution distributions. In this work we introduce a novel
theoretical framework for analyzing the effectiveness of such methods. We ask
whether there exist generative models that (i) are expressive enough to
generate approximately optimal solutions; (ii) have a tractable, i.e,
polynomial in the size of the input, number of parameters; (iii) their
optimization landscape is benign in the sense that it does not contain
sub-optimal stationary points. Our main contribution is a positive answer to
this question. Our result holds for a broad class of combinatorial problems
including Max- and Min-Cut, Max-$k$-CSP, Maximum-Weight-Bipartite-Matching, and
the Traveling Salesman Problem. As a byproduct of our analysis we introduce a
novel regularization process over vanilla gradient descent and provide
theoretical and experimental evidence that it helps address vanishing-gradient
issues and escape bad stationary points.
</p>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05312" title="Abstract">arXiv:2310.05312</a> [<a href="/pdf/2310.05312" title="Download PDF">pdf</a>, <a href="/format/2310.05312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality Assurance of A GPT-based Sentiment Analysis System: Adversarial  Review Data Generation and Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+T">Tinghui Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen-Son%2C+H">Hoang-Quoc Nguyen-Son</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H+H">Huy H. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Echizen%2C+I">Isao Echizen</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+Y">Yoshiki Seo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have been garnering significant attention of AI
researchers, especially following the widespread popularity of ChatGPT.
However, due to LLMs' intricate architecture and vast parameters, several
concerns and challenges regarding their quality assurance require to be
addressed. In this paper, a fine-tuned GPT-based sentiment analysis model is
first constructed and studied as the reference in AI quality analysis. Then,
the quality analysis related to data adequacy is implemented, including
employing the content-based approach to generate reasonable adversarial review
comments as the wrongly-annotated data, and developing surprise adequacy
(SA)-based techniques to detect these abnormal data. Experiments based on
Amazon.com review data and a fine-tuned GPT model were implemented. Results
were thoroughly discussed from the perspective of AI quality assurance to
present the quality analysis of an LLM model on generated adversarial textual
data and the effectiveness of using SA on anomaly detection in data quality
assurance.
</p>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05313" title="Abstract">arXiv:2310.05313</a> [<a href="/pdf/2310.05313" title="Download PDF">pdf</a>, <a href="/format/2310.05313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Deep Neural Network guided MCTS using Adaptive Parallelism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+Y">Yuan Meng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zu%2C+T">Tianxin Zu</a>, 
<a href="/search/cs?searchtype=author&query=Prasanna%2C+V">Viktor Prasanna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally. Accepted to the 13th Workshop on Irregular Applications: Architectures and Algorithms (IA^3) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Deep Neural Network guided Monte-Carlo Tree Search (DNN-MCTS) is a powerful
class of AI algorithms. In DNN-MCTS, a Deep Neural Network model is trained
collaboratively with a dynamic Monte-Carlo search tree to guide the agent
towards actions that yields the highest returns. While the DNN operations are
highly parallelizable, the search tree operations involved in MCTS are
sequential and often become the system bottleneck. Existing MCTS parallel
schemes on shared-memory multi-core CPU platforms either exploit data
parallelism but sacrifice memory access latency, or take advantage of local
cache for low-latency memory accesses but constrain the tree search to a single
thread. In this work, we analyze the tradeoff of these parallel schemes and
develop performance models for both parallel schemes based on the application
and hardware parameters. We propose a novel implementation that addresses the
tradeoff by adaptively choosing the optimal parallel scheme for the MCTS
component on the CPU. Furthermore, we propose an efficient method for searching
the optimal communication batch size as the MCTS component on the CPU
interfaces with DNN operations offloaded to an accelerator (GPU). Using a
representative DNN-MCTS algorithm - Alphazero on board game benchmarks, we show
that the parallel framework is able to adaptively generate the best-performing
parallel implementation, leading to a range of $1.5\times - 3\times$ speedup
compared with the baseline methods on CPU and CPU-GPU platforms.
</p>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05315" title="Abstract">arXiv:2310.05315</a> [<a href="/pdf/2310.05315" title="Download PDF">pdf</a>, <a href="/ps/2310.05315" title="Download PostScript">ps</a>, <a href="/format/2310.05315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sub-quadratic (1+\eps)-approximate Euclidean Spanners, with Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andoni%2C+A">Alexandr Andoni</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hengjie Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We study graph spanners for point-set in the high-dimensional Euclidean
space. On the one hand, we prove that spanners with stretch &lt;\sqrt{2} and
subquadratic size are not possible, even if we add Steiner points. On the other
hand, if we add extra nodes to the graph (non-metric Steiner points), then we
can obtain (1+\eps)-approximate spanners of subquadratic size. We show how to
construct a spanner of size n^{2-\Omega(\eps^3)}, as well as a directed version
of the spanner of size n^{2-\Omega(\eps^2)}.
<br />We use our directed spanner to obtain an algorithm for computing
(1+\eps)-approximation to Earth-Mover Distance (optimal transport) between two
sets of size n in time n^{2-\Omega(\eps^2)}.
</p>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05316" title="Abstract">arXiv:2310.05316</a> [<a href="/pdf/2310.05316" title="Download PDF">pdf</a>, <a href="/format/2310.05316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Feature Norm for Out-of-Distribution Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jaewoo Park</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+J+C+L">Jacky Chen Long Chai</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jaeho Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Teoh%2C+A+B+J">Andrew Beng Jin Teoh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">A neural network trained on a classification dataset often exhibits a higher
vector norm of hidden layer features for in-distribution (ID) samples, while
producing relatively lower norm values on unseen instances from
out-of-distribution (OOD). Despite this intriguing phenomenon being utilized in
many applications, the underlying cause has not been thoroughly investigated.
In this study, we demystify this very phenomenon by scrutinizing the
discriminative structures concealed in the intermediate layers of a neural
network. Our analysis leads to the following discoveries: (1) The feature norm
is a confidence value of a classifier hidden in the network layer, specifically
its maximum logit. Hence, the feature norm distinguishes OOD from ID in the
same manner that a classifier confidence does. (2) The feature norm is
class-agnostic, thus it can detect OOD samples across diverse discriminative
models. (3) The conventional feature norm fails to capture the deactivation
tendency of hidden layer neurons, which may lead to misidentification of ID
samples as OOD instances. To resolve this drawback, we propose a novel
negative-aware norm (NAN) that can capture both the activation and deactivation
tendencies of hidden layer neurons. We conduct extensive experiments on NAN,
demonstrating its efficacy and compatibility with existing OOD detectors, as
well as its capability in label-free environments.
</p>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05317" title="Abstract">arXiv:2310.05317</a> [<a href="/pdf/2310.05317" title="Download PDF">pdf</a>, <a href="/format/2310.05317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Long-form Text Generation in Mental Health\\ with  Task-adaptive Tokenization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+N">Naihao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sabour%2C+S">Sahand Sabour</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yilin Jia</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at main conference of The 2023 Conference on Empirical Methods in Natural Language Processing; 8 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We propose task-adaptive tokenization as a way to adapt the generation
pipeline to the specifics of a downstream task and enhance long-form generation
in mental health. Inspired by insights from cognitive science, our
task-adaptive tokenizer samples variable segmentations from multiple outcomes,
with sampling probabilities optimized based on task-specific data. We introduce
a strategy for building a specialized vocabulary and introduce a vocabulary
merging protocol that allows for the integration of task-specific tokens into
the pre-trained model's tokenization step. Through extensive experiments on
psychological question-answering tasks in both Chinese and English, we find
that our task-adaptive tokenization approach brings a significant improvement
in generation performance while using up to 60% fewer tokens. Preliminary
experiments point to promising results when using our tokenization approach
with very large language models.
</p>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05318" title="Abstract">arXiv:2310.05318</a> [<a href="/pdf/2310.05318" title="Download PDF">pdf</a>, <a href="/format/2310.05318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resolving the Imbalance Issue in Hierarchical Disciplinary Topic  Inference via LLM-based Data Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xunxin Cai</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+M">Meng Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+Z">Zhiyuan Ning</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuanchun Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, accepted by ICDM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In addressing the imbalanced issue of data within the realm of Natural
Language Processing, text data augmentation methods have emerged as pivotal
solutions. This data imbalance is prevalent in the research proposals submitted
during the funding application process. Such imbalances, resulting from the
varying popularity of disciplines or the emergence of interdisciplinary
studies, significantly impede the precision of downstream topic models that
deduce the affiliated disciplines of these proposals. At the data level,
proposals penned by experts and scientists are inherently complex technological
texts, replete with intricate terminologies, which augmenting such specialized
text data poses unique challenges. At the system level, this, in turn,
compromises the fairness of AI-assisted reviewer assignment systems, which
raises a spotlight on solving this issue. This study leverages large language
models (Llama V1) as data generators to augment research proposals categorized
within intricate disciplinary hierarchies, aiming to rectify data imbalances
and enhance the equity of expert assignments. We first sample within the
hierarchical structure to find the under-represented class. Then we designed a
prompt for keyword-based research proposal generation. Our experiments attests
to the efficacy of the generated data, demonstrating that research proposals
produced using the prompts can effectively address the aforementioned issues
and generate high quality scientific text data, thus help the model overcome
the imbalanced issue.
</p>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05321" title="Abstract">arXiv:2310.05321</a> [<a href="/pdf/2310.05321" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge Computing-Enabled Road Condition Monitoring: System Development and  Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daud%2C+A">Abdulateef Daud</a>, 
<a href="/search/cs?searchtype=author&query=Amo-Boateng%2C+M">Mark Amo-Boateng</a>, 
<a href="/search/cs?searchtype=author&query=Owor%2C+N+J">Neema Jakisa Owor</a>, 
<a href="/search/cs?searchtype=author&query=Aboah%2C+A">Armstrong Aboah</a>, 
<a href="/search/cs?searchtype=author&query=Adu-Gyamfi%2C+Y">Yaw Adu-Gyamfi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Real-time pavement condition monitoring provides highway agencies with timely
and accurate information that could form the basis of pavement maintenance and
rehabilitation policies. Existing technologies rely heavily on manual data
processing, are expensive and therefore, difficult to scale for frequent,
networklevel pavement condition monitoring. Additionally, these systems require
sending large packets of data to the cloud which requires large storage space,
are computationally expensive to process, and results in high latency. The
current study proposes a solution that capitalizes on the widespread
availability of affordable Micro Electro-Mechanical System (MEMS) sensors, edge
computing and internet connection capabilities of microcontrollers, and
deployable machine learning (ML) models to (a) design an Internet of Things
(IoT)-enabled device that can be mounted on axles of vehicles to stream live
pavement condition data (b) reduce latency through on-device processing and
analytics of pavement condition sensor data before sending to the cloud
servers. In this study, three ML models including Random Forest, LightGBM and
XGBoost were trained to predict International Roughness Index (IRI) at every
0.1-mile segment. XGBoost had the highest accuracy with an RMSE and MAPE of
16.89in/mi and 20.3%, respectively. In terms of the ability to classify the IRI
of pavement segments based on ride quality according to MAP-21 criteria, our
proposed device achieved an average accuracy of 96.76% on I-70EB and 63.15% on
South Providence. Overall, our proposed device demonstrates significant
potential in providing real-time pavement condition data to State Highway
Agencies (SHA) and Department of Transportation (DOTs) with a satisfactory
level of accuracy.
</p>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05324" title="Abstract">arXiv:2310.05324</a> [<a href="/pdf/2310.05324" title="Download PDF">pdf</a>, <a href="/format/2310.05324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Increasing Entropy to Boost Policy Gradient Performance on  Personalization Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Starnes%2C+A">Andrew Starnes</a>, 
<a href="/search/cs?searchtype=author&query=Dereventsov%2C+A">Anton Dereventsov</a>, 
<a href="/search/cs?searchtype=author&query=Webster%2C+C">Clayton Webster</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures, accepted to WAIN 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this effort, we consider the impact of regularization on the diversity of
actions taken by policies generated from reinforcement learning agents trained
using a policy gradient. Policy gradient agents are prone to entropy collapse,
which means certain actions are seldomly, if ever, selected. We augment the
optimization objective function for the policy with terms constructed from
various $\varphi$-divergences and Maximum Mean Discrepancy which encourages
current policies to follow different state visitation and/or action choice
distribution than previously computed policies. We provide numerical
experiments using MNIST, CIFAR10, and Spotify datasets. The results demonstrate
the advantage of diversity-promoting policy regularization and that its use on
gradient-based approaches have significantly improved performance on a variety
of personalization tasks. Furthermore, numerical evidence is given to show that
policy regularization increases performance without losing accuracy.
</p>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05327" title="Abstract">arXiv:2310.05327</a> [<a href="/pdf/2310.05327" title="Download PDF">pdf</a>, <a href="/format/2310.05327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provable Compositional Generalization for Object-Centric Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wiedemer%2C+T">Thadd&#xe4;us Wiedemer</a>, 
<a href="/search/cs?searchtype=author&query=Brady%2C+J">Jack Brady</a>, 
<a href="/search/cs?searchtype=author&query=Panfilov%2C+A">Alexander Panfilov</a>, 
<a href="/search/cs?searchtype=author&query=Juhos%2C+A">Attila Juhos</a>, 
<a href="/search/cs?searchtype=author&query=Bethge%2C+M">Matthias Bethge</a>, 
<a href="/search/cs?searchtype=author&query=Brendel%2C+W">Wieland Brendel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first four authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Learning representations that generalize to novel compositions of known
concepts is crucial for bridging the gap between human and machine perception.
One prominent effort is learning object-centric representations, which are
widely conjectured to enable compositional generalization. Yet, it remains
unclear when this conjecture will be true, as a principled theoretical or
empirical understanding of compositional generalization is lacking. In this
work, we investigate when compositional generalization is guaranteed for
object-centric representations through the lens of identifiability theory. We
show that autoencoders that satisfy structural assumptions on the decoder and
enforce encoder-decoder consistency will learn object-centric representations
that provably generalize compositionally. We validate our theoretical result
and highlight the practical relevance of our assumptions through experiments on
synthetic image data.
</p>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05330" title="Abstract">arXiv:2310.05330</a> [<a href="/pdf/2310.05330" title="Download PDF">pdf</a>, <a href="/format/2310.05330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Lightweight Video Anomaly Detection Model with Weak Supervision and  Adaptive Instance Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiaogen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+J">Jihong Guan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Video anomaly detection is to determine whether there are any abnormal
events, behaviors or objects in a given video, which enables effective and
intelligent public safety management. As video anomaly labeling is both
time-consuming and expensive, most existing works employ unsupervised or weakly
supervised learning methods. This paper focuses on weakly supervised video
anomaly detection, in which the training videos are labeled whether or not they
contain any anomalies, but there is no information about which frames the
anomalies are located. However, the uncertainty of weakly labeled data and the
large model size prevent existing methods from wide deployment in real
scenarios, especially the resource-limit situations such as edge-computing. In
this paper, we develop a lightweight video anomaly detection model. On the one
hand, we propose an adaptive instance selection strategy, which is based on the
model's current status to select confident instances, thereby mitigating the
uncertainty of weakly labeled data and subsequently promoting the model's
performance. On the other hand, we design a lightweight multi-level temporal
correlation attention module and an hourglass-shaped fully connected layer to
construct the model, which can reduce the model parameters to only 0.56\% of
the existing methods (e.g. RTFM). Our extensive experiments on two public
datasets UCF-Crime and ShanghaiTech show that our model can achieve comparable
or even superior AUC score compared to the state-of-the-art methods, with a
significantly reduced number of model parameters.
</p>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05331" title="Abstract">arXiv:2310.05331</a> [<a href="/pdf/2310.05331" title="Download PDF">pdf</a>, <a href="/format/2310.05331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlearning with Fisher Masking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yufang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Changzhi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuanbin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Aimin Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Machine unlearning aims to revoke some training data after learning in
response to requests from users, model developers, and administrators. Most
previous methods are based on direct fine-tuning, which may neither remove data
completely nor retain full performances on the remain data. In this work, we
find that, by first masking some important parameters before fine-tuning, the
performances of unlearning could be significantly improved. We propose a new
masking strategy tailored to unlearning based on Fisher information.
Experiments on various datasets and network structures show the effectiveness
of the method: without any fine-tuning, the proposed Fisher masking could
unlearn almost completely while maintaining most of the performance on the
remain data. It also exhibits stronger stability compared to other unlearning
baselines
</p>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05333" title="Abstract">arXiv:2310.05333</a> [<a href="/pdf/2310.05333" title="Download PDF">pdf</a>, <a href="/format/2310.05333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffCPS: Diffusion Model based Constrained Policy Search for Offline  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+L">Longxiang He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Linrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Junbo Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xueqian Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 8 figures, 3 tables. Submitted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Constrained policy search (CPS) is a fundamental problem in offline
reinforcement learning, which is generally solved by advantage weighted
regression (AWR). However, previous methods may still encounter
out-of-distribution actions due to the limited expressivity of Gaussian-based
policies. On the other hand, directly applying the state-of-the-art models with
distribution expression capabilities (i.e., diffusion models) in the AWR
framework is insufficient since AWR requires exact policy probability
densities, which is intractable in diffusion models. In this paper, we propose
a novel approach called $\textbf{Diffusion Model based Constrained Policy
Search (DiffCPS)}$, which tackles the diffusion-based constrained policy search
without resorting to AWR. The theoretical analysis reveals our key insights by
leveraging the action distribution of the diffusion model to eliminate the
policy distribution constraint in the CPS and then utilizing the Evidence Lower
Bound (ELBO) of diffusion-based policy to approximate the KL constraint.
Consequently, DiffCPS admits the high expressivity of diffusion models while
circumventing the cumbersome density calculation brought by AWR. Extensive
experimental results based on the D4RL benchmark demonstrate the efficacy of
our approach. We empirically show that DiffCPS achieves better or at least
competitive performance compared to traditional AWR-based baselines as well as
recent diffusion-based offline RL methods. The code is now available at
$\href{https://github.com/felix-thu/DiffCPS}{https://github.com/felix-thu/DiffCPS}$.
</p>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05336" title="Abstract">arXiv:2310.05336</a> [<a href="/pdf/2310.05336" title="Download PDF">pdf</a>, <a href="/format/2310.05336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GReAT: A Graph Regularized Adversarial Training Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bayram%2C+S">Samet Bayram</a>, 
<a href="/search/cs?searchtype=author&query=Barner%2C+K">Kenneth Barner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages including references. 7 figures and 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This paper proposes a regularization method called GReAT, Graph Regularized
Adversarial Training, to improve deep learning models' classification
performance. Adversarial examples are a well-known challenge in machine
learning, where small, purposeful perturbations to input data can mislead
models. Adversarial training, a powerful and one of the most effective defense
strategies, involves training models with both regular and adversarial
examples. However, it often neglects the underlying structure of the data. In
response, we propose GReAT, a method that leverages data graph structure to
enhance model robustness. GReAT deploys the graph structure of the data into
the adversarial training process, resulting in more robust models that better
generalize its testing performance and defend against adversarial attacks.
Through extensive evaluation on benchmark datasets, we demonstrate GReAT's
effectiveness compared to state-of-the-art classification methods, highlighting
its potential in improving deep learning models' classification performance.
</p>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05337" title="Abstract">arXiv:2310.05337</a> [<a href="/pdf/2310.05337" title="Download PDF">pdf</a>, <a href="/format/2310.05337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What do larger image classifiers memorise?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lukasik%2C+M">Michal Lukasik</a>, 
<a href="/search/cs?searchtype=author&query=Nagarajan%2C+V">Vaishnavh Nagarajan</a>, 
<a href="/search/cs?searchtype=author&query=Rawat%2C+A+S">Ankit Singh Rawat</a>, 
<a href="/search/cs?searchtype=author&query=Menon%2C+A+K">Aditya Krishna Menon</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sanjiv Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The success of modern neural networks has prompted study of the connection
between memorisation and generalisation: overparameterised models generalise
well, despite being able to perfectly fit (memorise) completely random labels.
To carefully study this issue, Feldman proposed a metric to quantify the degree
of memorisation of individual training examples, and empirically computed the
corresponding memorisation profile of a ResNet on image classification
bench-marks. While an exciting first glimpse into what real-world models
memorise, this leaves open a fundamental question: do larger neural models
memorise more? We present a comprehensive empirical analysis of this question
on image classification benchmarks. We find that training examples exhibit an
unexpectedly diverse set of memorisation trajectories across model sizes: most
samples experience decreased memorisation under larger models, while the rest
exhibit cap-shaped or increasing memorisation. We show that various proxies for
the Feldman memorization score fail to capture these fundamental trends.
Lastly, we find that knowledge distillation, an effective and popular model
compression technique, tends to inhibit memorisation, while also improving
generalisation. Specifically, memorisation is mostly inhibited on examples with
increasing memorisation trajectories, thus pointing at how distillation
improves generalisation.
</p>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05338" title="Abstract">arXiv:2310.05338</a> [<a href="/pdf/2310.05338" title="Download PDF">pdf</a>, <a href="/format/2310.05338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Negative Object Presence Evaluation (NOPE) to Measure Object  Hallucination in Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lovenia%2C+H">Holy Lovenia</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+W">Wenliang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Cahyawijaya%2C+S">Samuel Cahyawijaya</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Ziwei Ji</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+P">Pascale Fung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Object hallucination poses a significant challenge in vision-language (VL)
models, often leading to the generation of nonsensical or unfaithful responses
with non-existent objects. However, the absence of a general measurement for
evaluating object hallucination in VL models has hindered our understanding and
ability to mitigate this issue. In this work, we present NOPE (Negative Object
Presence Evaluation), a novel benchmark designed to assess object hallucination
in VL models through visual question answering (VQA). We propose a
cost-effective and scalable approach utilizing large language models to
generate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE.
We extensively investigate the performance of 10 state-of-the-art VL models in
discerning the non-existence of objects in visual questions, where the ground
truth answers are denoted as NegP (e.g., "none"). Additionally, we evaluate
their standard performance on visual questions on 9 other VQA datasets. Through
our experiments, we demonstrate that no VL model is immune to the vulnerability
of object hallucination, as all models achieve accuracy below 10\% on NegP.
Furthermore, we uncover that lexically diverse visual questions, question types
with large scopes, and scene-relevant objects capitalize the risk of object
hallucination in VL models.
</p>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05341" title="Abstract">arXiv:2310.05341</a> [<a href="/pdf/2310.05341" title="Download PDF">pdf</a>, <a href="/format/2310.05341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Critical Look at Classic Test-Time Adaptation Methods in Semantic  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+C">Chang&#x27;an Yi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haotian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Test-time adaptation (TTA) aims to adapt a model, initially trained on
training data, to potential distribution shifts in the test data. Most existing
TTA studies, however, focus on classification tasks, leaving a notable gap in
the exploration of TTA for semantic segmentation. This pronounced emphasis on
classification might lead numerous newcomers and engineers to mistakenly assume
that classic TTA methods designed for classification can be directly applied to
segmentation. Nonetheless, this assumption remains unverified, posing an open
question. To address this, we conduct a systematic, empirical study to disclose
the unique challenges of segmentation TTA, and to determine whether classic TTA
strategies can effectively address this task. Our comprehensive results have
led to three key observations. First, the classic batch norm updating strategy,
commonly used in classification TTA, only brings slight performance
improvement, and in some cases it might even adversely affect the results. Even
with the application of advanced distribution estimation techniques like batch
renormalization, the problem remains unresolved. Second, the teacher-student
scheme does enhance training stability for segmentation TTA in the presence of
noisy pseudo-labels. However, it cannot directly result in performance
improvement compared to the original model without TTA. Third, segmentation TTA
suffers a severe long-tailed imbalance problem, which is substantially more
complex than that in TTA for classification. This long-tailed challenge
significantly affects segmentation TTA performance, even when the accuracy of
pseudo-labels is high.
</p>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05342" title="Abstract">arXiv:2310.05342</a> [<a href="/pdf/2310.05342" title="Download PDF">pdf</a>, <a href="/ps/2310.05342" title="Download PostScript">ps</a>, <a href="/format/2310.05342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uniform accuracy of implicit-explicit backward differentiation formulas  (IMEX-BDF) for linear hyperbolic relaxation systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ma%2C+Z">Zhiting Ma</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+J">Juntao Huang</a>, 
<a href="/search/math?searchtype=author&query=Yong%2C+W">Wen-An Yong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This work is concerned with the uniform accuracy of implicit-explicit
backward differentiation formulas for general linear hyperbolic relaxation
systems satisfying the structural stability condition proposed previously by
the third author. We prove the uniform stability and accuracy of a class of
IMEX-BDF schemes discretized spatially by a Fourier spectral method. The result
reveals that the accuracy of the fully discretized schemes is independent of
the relaxation time in all regimes. It is verified by numerical experiments on
several applications to traffic flows, rarefied gas dynamics and kinetic
theory.
</p>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05343" title="Abstract">arXiv:2310.05343</a> [<a href="/pdf/2310.05343" title="Download PDF">pdf</a>, <a href="/format/2310.05343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Continuous Learning in Spiking Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fredieu%2C+C+T">C. Tanner Fredieu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, the use of third-generation machine learning, also known as
spiking neural network architecture, for continuous learning was investigated
and compared to conventional models. The experimentation was divided into three
separate phases. The first phase focused on training the conventional models
via transfer learning. The second phase trains a Nengo model from their
library. Lastly, each conventional model is converted into a spiking neural
network and trained. Initial results from phase 1 are inline with known
knowledge about continuous learning within current machine learning literature.
All models were able to correctly identify the current classes, but they would
immediately see a sharp performance drop in previous classes due to
catastrophic forgetting. However, the SNN models were able to retain some
information about previous classes. Although many of the previous classes were
still identified as the current trained classes, the output probabilities
showed a higher than normal value to the actual class. This indicates that the
SNN models do have potential to overcome catastrophic forgetting but much work
is still needed.
</p>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05344" title="Abstract">arXiv:2310.05344</a> [<a href="/pdf/2310.05344" title="Download PDF">pdf</a>, <a href="/format/2310.05344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to  RLHF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhilin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sreedhar%2C+M+N">Makesh Narsimhan Sreedhar</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xianchao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Kuchaiev%2C+O">Oleksii Kuchaiev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Model alignment with human preferences is an essential step in making Large
Language Models (LLMs) helpful and consistent with human values. It typically
consists of supervised fine-tuning (SFT) and reinforcement learning from human
feedback (RLHF) stages. However, RLHF faces inherent limitations stemming from
a complex training setup and its tendency to align the model with implicit
values that end users cannot control at run-time. Moreover, reward models in
RLHF stage commonly rely on single-dimensional feedback as opposed to explicit,
multifaceted signals that indicate attributes such as helpfulness, humor, and
toxicity. To address these limitations, we propose SteerLM, a supervised
fine-tuning method that empowers end-users to control responses during
inference. SteerLM conditions responses to conform to an explicitly defined
multi-dimensional set of attributes, thereby empowering a steerable AI capable
of generating helpful and high-quality responses while maintaining
customizability. Experiments show that SteerLM trained on open source datasets
generates responses that are preferred by human and automatic evaluators to
many state-of-the-art baselines trained with RLHF while being much easier to
train. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B
</p>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05346" title="Abstract">arXiv:2310.05346</a> [<a href="/pdf/2310.05346" title="Download PDF">pdf</a>, <a href="/format/2310.05346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anyview: Generalizable Indoor 3D Object Detection with Variable Frames
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhenyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiuwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+C">Chong Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Linqing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiwen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Haibin Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">In this paper, we propose a novel network framework for indoor 3D object
detection to handle variable input frame numbers in practical scenarios.
Existing methods only consider fixed frames of input data for a single
detector, such as monocular RGB-D images or point clouds reconstructed from
dense multi-view RGB-D images. While in practical application scenes such as
robot navigation and manipulation, the raw input to the 3D detectors is the
RGB-D images with variable frame numbers instead of the reconstructed scene
point cloud. However, the previous approaches can only handle fixed frame input
data and have poor performance with variable frame input. In order to
facilitate 3D object detection methods suitable for practical tasks, we present
a novel 3D detection framework named AnyView for our practical applications,
which generalizes well across different numbers of input frames with a single
model. To be specific, we propose a geometric learner to mine the local
geometric features of each input RGB-D image frame and implement local-global
feature interaction through a designed spatial mixture module. Meanwhile, we
further utilize a dynamic token strategy to adaptively adjust the number of
extracted features for each frame, which ensures consistent global feature
density and further enhances the generalization after fusion. Extensive
experiments on the ScanNet dataset show our method achieves both great
generalizability and high detection accuracy with a simple and clean
architecture containing a similar amount of parameters with the baselines.
</p>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05347" title="Abstract">arXiv:2310.05347</a> [<a href="/pdf/2310.05347" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Infrared Small Target Detection Using Double-Weighted Multi-Granularity  Patch Tensor Model With Tensor-Train Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guiyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Q">Qunbo Lv</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Z">Zui Tao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Baoyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuan Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Infrared small target detection plays an important role in the remote sensing
fields. Therefore, many detection algorithms have been proposed, in which the
infrared patch-tensor (IPT) model has become a mainstream tool due to its
excellent performance. However, most IPT-based methods face great challenges,
such as inaccurate measure of the tensor low-rankness and poor robustness to
complex scenes, which will leadto poor detection performance. In order to solve
these problems, this paper proposes a novel double-weighted multi-granularity
infrared patch tensor (DWMGIPT) model. First, to capture different granularity
information of tensor from multiple modes, a multi-granularity infrared patch
tensor (MGIPT) model is constructed by collecting nonoverlapping patches and
tensor augmentation based on the tensor train (TT) decomposition. Second, to
explore the latent structure of tensor more efficiently, we utilize the
auto-weighted mechanism to balance the importance of information at different
granularity. Then, the steering kernel (SK) is employed to extract local
structure prior, which suppresses background interference such as strong edges
and noise. Finally, an efficient optimization algorithm based on the
alternating direction method of multipliers (ADMM) is presented to solve the
model. Extensive experiments in various challenging scenes show that the
proposed algorithm is robust to noise and different scenes. Compared with the
other eight state-of-the-art methods, different evaluation metrics demonstrate
that our method achieves better detection performance in various complex
scenes.
</p>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05348" title="Abstract">arXiv:2310.05348</a> [<a href="/pdf/2310.05348" title="Download PDF">pdf</a>, <a href="/format/2310.05348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous Invariance Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+L">Lu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lintao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiameng Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yansu He</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">James Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Invariance learning methods aim to learn invariant features in the hope that
they generalize under distributional shifts. Although many tasks are naturally
characterized by continuous domains, current invariance learning techniques
generally assume categorically indexed domains. For example, auto-scaling in
cloud computing often needs a CPU utilization prediction model that generalizes
across different times (e.g., time of a day and date of a year), where `time'
is a continuous domain index. In this paper, we start by theoretically showing
that existing invariance learning methods can fail for continuous domain
problems. Specifically, the naive solution of splitting continuous domains into
discrete ones ignores the underlying relationship among domains, and therefore
potentially leads to suboptimal performance. To address this challenge, we then
propose Continuous Invariance Learning (CIL), which extracts invariant features
across continuously indexed domains. CIL is a novel adversarial procedure that
measures and controls the conditional independence between the labels and
continuous domain indices given the extracted features. Our theoretical
analysis demonstrates the superiority of CIL over existing invariance learning
methods. Empirical results on both synthetic and real-world datasets (including
data collected from production systems) show that CIL consistently outperforms
strong baselines among all the tasks.
</p>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05349" title="Abstract">arXiv:2310.05349</a> [<a href="/pdf/2310.05349" title="Download PDF">pdf</a>, <a href="/format/2310.05349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALECE: An Attention-based Learned Cardinality Estimator for SPJ Queries  on Dynamic Workloads (Extended)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengfei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wenqing Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+R">Rong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+B">Bolin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hua Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> VLDB 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">For efficient query processing, DBMS query optimizers have for decades relied
on delicate cardinality estimation methods. In this work, we propose an
Attention-based LEarned Cardinality Estimator (ALECE for short) for SPJ
queries. The core idea is to discover the implicit relationships between
queries and underlying dynamic data using attention mechanisms in ALECE's two
modules that are built on top of carefully designed featurizations for data and
queries. In particular, from all attributes in the database, the data-encoder
module obtains organic and learnable aggregations which implicitly represent
correlations among the attributes, whereas the query-analyzer module builds a
bridge between the query featurizations and the data aggregations to predict
the query's cardinality. We experimentally evaluate ALECE on multiple dynamic
workloads. The results show that ALECE enables PostgreSQL's optimizer to
achieve nearly optimal performance, clearly outperforming its built-in
cardinality estimator and other alternatives.
</p>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05350" title="Abstract">arXiv:2310.05350</a> [<a href="/pdf/2310.05350" title="Download PDF">pdf</a>, <a href="/ps/2310.05350" title="Download PostScript">ps</a>, <a href="/format/2310.05350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Studies for Efficient Parameter Search and Parallelism for Large  Language Model Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benington%2C+M">Michael Benington</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+L">Leo Phan</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+C+P">Chris Pierre Paul</a>, 
<a href="/search/cs?searchtype=author&query=Shoemaker%2C+E">Evan Shoemaker</a>, 
<a href="/search/cs?searchtype=author&query=Ranade%2C+P">Priyanka Ranade</a>, 
<a href="/search/cs?searchtype=author&query=Collett%2C+T">Torstein Collett</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+G+H">Grant Hodgson Perez</a>, 
<a href="/search/cs?searchtype=author&query=Krieger%2C+C">Christopher Krieger</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Supercomputing 2023 (SC23) Student Research Poster Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">AI accelerator processing capabilities and memory constraints largely dictate
the scale in which machine learning workloads (e.g., training and inference)
can be executed within a desirable time frame. Training a state of the art,
transformer-based model today requires use of GPU-accelerated high performance
computers with high-speed interconnects. As datasets and models continue to
increase in size, computational requirements and memory demands for AI also
continue to grow. These challenges have inspired the development of distributed
algorithm and circuit-based optimization techniques that enable the ability to
progressively scale models in multi-node environments, efficiently minimize
neural network cost functions for faster convergence, and store more parameters
into a set number of available resources. In our research project, we focus on
parallel and distributed machine learning algorithm development, specifically
for optimizing the data processing and pre-training of a set of 5
encoder-decoder LLMs, ranging from 580 million parameters to 13 billion
parameters. We performed a fine-grained study to quantify the relationships
between three ML parallelism methods, specifically exploring Microsoft
DeepSpeed Zero Redundancy Optimizer (ZeRO) stages.
</p>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05351" title="Abstract">arXiv:2310.05351</a> [<a href="/pdf/2310.05351" title="Download PDF">pdf</a>, <a href="/format/2310.05351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Neural Collapse for a Large Number of Classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiachen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jinxin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Q">Qing Qu</a>, 
<a href="/search/cs?searchtype=author&query=Mixon%2C+D">Dustin Mixon</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+C">Chong You</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhihui Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Information Theory (cs.IT)

</div>
<p class="mathjax">Neural collapse provides an elegant mathematical characterization of learned
last layer representations (a.k.a. features) and classifier weights in deep
classification models. Such results not only provide insights but also motivate
new techniques for improving practical deep models. However, most of the
existing empirical and theoretical studies in neural collapse focus on the case
that the number of classes is small relative to the dimension of the feature
space. This paper extends neural collapse to cases where the number of classes
are much larger than the dimension of feature space, which broadly occur for
language models, retrieval systems, and face recognition applications. We show
that the features and classifier exhibit a generalized neural collapse
phenomenon, where the minimum one-vs-rest margins is maximized.We provide
empirical study to verify the occurrence of generalized neural collapse in
practical deep neural networks. Moreover, we provide theoretical study to show
that the generalized neural collapse provably occurs under unconstrained
feature model with spherical constraint, under certain technical conditions on
feature dimension and number of classes.
</p>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05352" title="Abstract">arXiv:2310.05352</a> [<a href="/pdf/2310.05352" title="Download PDF">pdf</a>, <a href="/format/2310.05352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Glance is Enough: Extract Target Sentence By Looking at A keyword
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Ying Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lantian Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiqing Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper investigates the possibility of extracting a target sentence from
multi-talker speech using only a keyword as input. For example, in social
security applications, the keyword might be "help", and the goal is to identify
what the person who called for help is articulating while ignoring other
speakers. To address this problem, we propose using the Transformer
architecture to embed both the keyword and the speech utterance and then rely
on the cross-attention mechanism to select the correct content from the
concatenated or overlapping speech. Experimental results on Librispeech
demonstrate that our proposed method can effectively extract target sentences
from very noisy and mixed speech (SNR=-3dB), achieving a phone error rate (PER)
of 26\%, compared to the baseline system's PER of 96%.
</p>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05354" title="Abstract">arXiv:2310.05354</a> [<a href="/pdf/2310.05354" title="Download PDF">pdf</a>, <a href="/format/2310.05354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Initial Investigation of Neural Replay Simulator for Over-the-Air  Adversarial Perturbations to Automatic Speaker Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Li Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+L">Liumeng Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhizheng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Deep Learning has advanced Automatic Speaker Verification (ASV) in the past
few years. Although it is known that deep learning-based ASV systems are
vulnerable to adversarial examples in digital access, there are few studies on
adversarial attacks in the context of physical access, where a replay process
(i.e., over the air) is involved. An over-the-air attack involves a
loudspeaker, a microphone, and a replaying environment that impacts the
movement of the sound wave. Our initial experiment confirms that the replay
process impacts the effectiveness of the over-the-air attack performance. This
study performs an initial investigation towards utilizing a neural replay
simulator to improve over-the-air adversarial attack robustness. This is
achieved by using a neural waveform synthesizer to simulate the replay process
when estimating the adversarial perturbations. Experiments conducted on the
ASVspoof2019 dataset confirm that the neural replay simulator can considerably
increase the success rates of over-the-air adversarial attacks. This raises the
concern for adversarial attacks on speaker verification in physical access
applications.
</p>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05355" title="Abstract">arXiv:2310.05355</a> [<a href="/pdf/2310.05355" title="Download PDF">pdf</a>, <a href="/format/2310.05355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> C^2M-DoT: Cross-modal consistent multi-view medical report generation  with domain transfer network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruizhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiangtao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lukasiewicz%2C+T">Thomas Lukasiewicz</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhenghua Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In clinical scenarios, multiple medical images with different views are
usually generated simultaneously, and these images have high semantic
consistency. However, most existing medical report generation methods only
consider single-view data. The rich multi-view mutual information of medical
images can help generate more accurate reports, however, the dependence of
multi-view models on multi-view data in the inference stage severely limits
their application in clinical practice. In addition, word-level optimization
based on numbers ignores the semantics of reports and medical images, and the
generated reports often cannot achieve good performance. Therefore, we propose
a cross-modal consistent multi-view medical report generation with a domain
transfer network (C^2M-DoT). Specifically, (i) a semantic-based multi-view
contrastive learning medical report generation framework is adopted to utilize
cross-view information to learn the semantic representation of lesions; (ii) a
domain transfer network is further proposed to ensure that the multi-view
report generation model can still achieve good inference performance under
single-view input; (iii) meanwhile, optimization using a cross-modal
consistency loss facilitates the generation of textual reports that are
semantically consistent with medical images. Extensive experimental studies on
two public benchmark datasets demonstrate that C^2M-DoT substantially
outperforms state-of-the-art baselines in all metrics. Ablation studies also
confirmed the validity and necessity of each component in C^2M-DoT.
</p>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05364" title="Abstract">arXiv:2310.05364</a> [<a href="/pdf/2310.05364" title="Download PDF">pdf</a>, <a href="/format/2310.05364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Multi-modal Entity Alignment via Iteratively Fusing Modality  Similarity Paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bolin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoze Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+X">Xin Mao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lingbing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The objective of Entity Alignment (EA) is to identify equivalent entity pairs
from multiple Knowledge Graphs (KGs) and create a more comprehensive and
unified KG. The majority of EA methods have primarily focused on the structural
modality of KGs, lacking exploration of multi-modal information. A few
multi-modal EA methods have made good attempts in this field. Still, they have
two shortcomings: (1) inconsistent and inefficient modality modeling that
designs complex and distinct models for each modality; (2) ineffective modality
fusion due to the heterogeneous nature of modalities in EA. To tackle these
challenges, we propose PathFusion, consisting of two main components: (1) MSP,
a unified modeling approach that simplifies the alignment process by
constructing paths connecting entities and modality nodes to represent multiple
modalities; (2) IRF, an iterative fusion method that effectively combines
information from different modalities using the path as an information carrier.
Experimental results on real-world datasets demonstrate the superiority of
PathFusion over state-of-the-art methods, with 22.4%-28.9% absolute improvement
on Hits@1, and 0.194-0.245 absolute improvement on MRR.
</p>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05365" title="Abstract">arXiv:2310.05365</a> [<a href="/pdf/2310.05365" title="Download PDF">pdf</a>, <a href="/format/2310.05365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Molecular De Novo Design through Transformer-based Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Tao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Pengcheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Tianfan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Laghuvarapu%2C+S">Siddhartha Laghuvarapu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jimeng Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this work, we introduce a method to fine-tune a Transformer-based
generative model for molecular de novo design. Leveraging the superior sequence
learning capacity of Transformers over Recurrent Neural Networks (RNNs), our
model can generate molecular structures with desired properties effectively. In
contrast to the traditional RNN-based models, our proposed method exhibits
superior performance in generating compounds predicted to be active against
various biological targets, capturing long-term dependencies in the molecular
structure sequence. The model's efficacy is demonstrated across numerous tasks,
including generating analogues to a query structure and producing compounds
with particular attributes, outperforming the baseline RNN-based methods. Our
approach can be used for scaffold hopping, library expansion starting from a
single molecule, and generating compounds with high predicted activity against
biological targets.
</p>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05366" title="Abstract">arXiv:2310.05366</a> [<a href="/pdf/2310.05366" title="Download PDF">pdf</a>, <a href="/format/2310.05366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rotation Matters: Generalized Monocular 3D Object Detection for Various  Camera Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moon%2C+S">SungHo Moon</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+J">JinWoo Bae</a>, 
<a href="/search/cs?searchtype=author&query=Im%2C+S">SungHoon Im</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CVPRw 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Research on monocular 3D object detection is being actively studied, and as a
result, performance has been steadily improving. However, 3D object detection
performance is significantly reduced when applied to a camera system different
from the system used to capture the training datasets. For example, a 3D
detector trained on datasets from a passenger car mostly fails to regress
accurate 3D bounding boxes for a camera mounted on a bus. In this paper, we
conduct extensive experiments to analyze the factors that cause performance
degradation. We find that changing the camera pose, especially camera
orientation, relative to the road plane caused performance degradation. In
addition, we propose a generalized 3D object detection method that can be
universally applied to various camera systems. We newly design a compensation
module that corrects the estimated 3D bounding box location and heading
direction. The proposed module can be applied to most of the recent 3D object
detection networks. It increases AP3D score (KITTI moderate, IoU $&gt; 70\%$)
about 6-to-10-times above the baselines without additional training. Both
quantitative and qualitative results show the effectiveness of the proposed
method.
</p>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05368" title="Abstract">arXiv:2310.05368</a> [<a href="/pdf/2310.05368" title="Download PDF">pdf</a>, <a href="/format/2310.05368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Acoustics with Collaborative Multiple Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yinfeng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Changan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Lele Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fangkai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fuchun Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main paper (9 pages and 5 figures and 2 tables) and appendix (16 pages and 13 figures and 10 tables). Accepted for publication by IJCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multiagent Systems (cs.MA); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">As humans, we hear sound every second of our life. The sound we hear is often
affected by the acoustics of the environment surrounding us. For example, a
spacious hall leads to more reverberation. Room Impulse Responses (RIR) are
commonly used to characterize environment acoustics as a function of the scene
geometry, materials, and source/receiver locations. Traditionally, RIRs are
measured by setting up a loudspeaker and microphone in the environment for all
source/receiver locations, which is time-consuming and inefficient. We propose
to let two robots measure the environment's acoustics by actively moving and
emitting/receiving sweep signals. We also devise a collaborative multi-agent
policy where these two robots are trained to explore the environment's
acoustics while being rewarded for wide exploration and accurate prediction. We
show that the robots learn to collaborate and move to explore environment
acoustics while minimizing the prediction error. To the best of our knowledge,
we present the very first problem formulation and solution to the task of
collaborative environment acoustics measurements with multiple agents.
</p>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05369" title="Abstract">arXiv:2310.05369</a> [<a href="/pdf/2310.05369" title="Download PDF">pdf</a>, <a href="/format/2310.05369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdvSV: An Over-the-Air Adversarial Attack Dataset for Speaker  Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Li Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yuhao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jiahao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Ke Xu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+C">Chengfang Fang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jie Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhizheng Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">It is known that deep neural networks are vulnerable to adversarial attacks.
Although Automatic Speaker Verification (ASV) built on top of deep neural
networks exhibits robust performance in controlled scenarios, many studies
confirm that ASV is vulnerable to adversarial attacks. The lack of a standard
dataset is a bottleneck for further research, especially reproducible research.
In this study, we developed an open-source adversarial attack dataset for
speaker verification research. As an initial step, we focused on the
over-the-air attack. An over-the-air adversarial attack involves a perturbation
generation algorithm, a loudspeaker, a microphone, and an acoustic environment.
The variations in the recording configurations make it very challenging to
reproduce previous research. The AdvSV dataset is constructed using the
Voxceleb1 Verification test set as its foundation. This dataset employs
representative ASV models subjected to adversarial attacks and records
adversarial samples to simulate over-the-air attack settings. The scope of the
dataset can be easily extended to include more types of adversarial attacks.
The dataset will be released to the public under the CC-BY license. In
addition, we also provide a detection baseline for reproducible research.
</p>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05370" title="Abstract">arXiv:2310.05370</a> [<a href="/pdf/2310.05370" title="Download PDF">pdf</a>, <a href="/format/2310.05370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SocialCircle: Learning the Angle-based Social Interaction Representation  for Pedestrian Trajectory Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wong%2C+C">Conghao Wong</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+B">Beihao Xia</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+X">Xinge You</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Analyzing and forecasting trajectories of agents like pedestrians and cars in
complex scenes has become more and more significant in many intelligent systems
and applications. The diversity and uncertainty in socially interactive
behaviors among a rich variety of agents make this task more challenging than
other deterministic computer vision tasks. Researchers have made a lot of
efforts to quantify the effects of these interactions on future trajectories
through different mathematical models and network structures, but this problem
has not been well solved. Inspired by marine animals that localize the
positions of their companions underwater through echoes, we build a new
anglebased trainable social representation, named SocialCircle, for
continuously reflecting the context of social interactions at different angular
orientations relative to the target agent. We validate the effect of the
proposed SocialCircle by training it along with several newly released
trajectory prediction models, and experiments show that the SocialCircle not
only quantitatively improves the prediction performance, but also qualitatively
helps better consider social interactions when forecasting pedestrian
trajectories in a way that is consistent with human intuitions.
</p>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05373" title="Abstract">arXiv:2310.05373</a> [<a href="/pdf/2310.05373" title="Download PDF">pdf</a>, <a href="/format/2310.05373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+Z">Zhongxiang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+G+K+R">Gregory Kang Ruey Lau</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+A">Arun Verma</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+Y">Yao Shu</a>, 
<a href="/search/cs?searchtype=author&query=Low%2C+B+K+H">Bryan Kian Hsiang Low</a>, 
<a href="/search/cs?searchtype=author&query=Jaillet%2C+P">Patrick Jaillet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Kernelized bandits, also known as Bayesian optimization (BO), has been a
prevalent method for optimizing complicated black-box reward functions. Various
BO algorithms have been theoretically shown to enjoy upper bounds on their
cumulative regret which are sub-linear in the number T of iterations, and a
regret lower bound of Omega(sqrt(T)) has been derived which represents the
unavoidable regrets for any classical BO algorithm. Recent works on quantum
bandits have shown that with the aid of quantum computing, it is possible to
achieve tighter regret upper bounds better than their corresponding classical
lower bounds. However, these works are restricted to either multi-armed or
linear bandits, and are hence not able to solve sophisticated real-world
problems with non-linear reward functions. To this end, we introduce the
quantum-Gaussian process-upper confidence bound (Q-GP-UCB) algorithm. To the
best of our knowledge, our Q-GP-UCB is the first BO algorithm able to achieve a
regret upper bound of O(polylog T), which is significantly smaller than its
regret lower bound of Omega(sqrt(T)) in the classical setting. Moreover, thanks
to our novel analysis of the confidence ellipsoid, our Q-GP-UCB with the linear
kernel achieves a smaller regret than the quantum linear UCB algorithm from the
previous work. We use simulations, as well as an experiment using a real
quantum computer, to verify that the theoretical quantum speedup achieved by
our Q-GP-UCB is also potentially relevant in practice.
</p>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05374" title="Abstract">arXiv:2310.05374</a> [<a href="/pdf/2310.05374" title="Download PDF">pdf</a>, <a href="/format/2310.05374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving End-to-End Speech Processing by Efficient Text Data  Utilization with Latent Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jianqiao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenyong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nianzu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xingshan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+Y+T">Yu Ting Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 8 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Training a high performance end-to-end speech (E2E) processing model requires
an enormous amount of labeled speech data, especially in the era of
data-centric artificial intelligence. However, labeled speech data are usually
scarcer and more expensive for collection, compared to textual data. We propose
Latent Synthesis (LaSyn), an efficient textual data utilization framework for
E2E speech processing models. We train a latent synthesizer to convert textual
data into an intermediate latent representation of a pre-trained speech model.
These pseudo acoustic representations of textual data augment acoustic data for
model training. We evaluate LaSyn on low-resource automatic speech recognition
(ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an
E2E baseline trained on LibriSpeech train-clean-100, with relative word error
rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our
E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for
slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM)
and EM-Tree accuracies on STOP respectively. With fewer parameters, the results
of LaSyn are competitive to published state-of-the-art works. The results
demonstrate the quality of the augmented training data. The source code will be
available to the community.
</p>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05375" title="Abstract">arXiv:2310.05375</a> [<a href="/pdf/2310.05375" title="Download PDF">pdf</a>, <a href="/format/2310.05375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IPDreamer: Appearance-Controllable 3D Object Generation with Image  Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+B">Bohan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shanglin Li</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yutang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hong Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Sicheng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huaxia Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianzhuang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Baochang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advances in text-to-3D generation have been remarkable, with methods
such as DreamFusion leveraging large-scale text-to-image diffusion-based models
to supervise 3D generation. These methods, including the variational score
distillation proposed by ProlificDreamer, enable the synthesis of detailed and
photorealistic textured meshes. However, the appearance of 3D objects generated
by these methods is often random and uncontrollable, posing a challenge in
achieving appearance-controllable 3D objects. To address this challenge, we
introduce IPDreamer, a novel approach that incorporates image prompts to
provide specific and comprehensive appearance information for 3D object
generation. Our results demonstrate that IPDreamer effectively generates
high-quality 3D objects that are consistent with both the provided text and
image prompts, demonstrating its promising capability in
appearance-controllable 3D object generation.
</p>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05377" title="Abstract">arXiv:2310.05377</a> [<a href="/pdf/2310.05377" title="Download PDF">pdf</a>, <a href="/format/2310.05377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Evolution Strategies with Multi-Level Learning for  Large-Scale Black-Box Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+Q">Qiqi Duan</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+C">Chang Shao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guochen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuhui Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">In the post-Moore era, the main performance gains of black-box optimizers are
increasingly depending upon parallelism, especially for large-scale
optimization (LSO). In this paper, we propose to parallelize the
well-established covariance matrix adaptation evolution strategy (CMA-ES) and
in particular its one latest variant called limited-memory CMA (LM-CMA) for
LSO. To achieve scalability while maintaining the invariance property as much
as possible, we present a multilevel learningbased meta-framework. Owing to its
hierarchically organized structure, Meta-ES is well-suited to implement our
distributed meta-framework, wherein the outer-ES controls strategy parameters
while all parallel inner-ESs run the serial LM-CMA with different settings. For
the distribution mean update of the outerES, both the elitist and
multi-recombination strategy are used in parallel to avoid stagnation and
regression, respectively. To exploit spatiotemporal information, the global
step-size adaptation combines Meta-ES with the parallel cumulative stepsize
adaptation. After each isolation time, our meta-framework employs both the
structure and parameter learning strategy to combine aligned evolution paths
for CMA reconstruction. Experiments on a set of large-scale benchmarking
functions with memory-intensive evaluations, arguably reflecting many
data-driven optimization problems, validate the benefits (e.g., scalability
w.r.t. CPU cores, effectiveness w.r.t. solution quality, and adaptability
w.r.t. second-order learning) and costs of our meta-framework.
</p>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05378" title="Abstract">arXiv:2310.05378</a> [<a href="/pdf/2310.05378" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transcending the Attention Paradigm: Implicit Learning from Geospatial  Social Media Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=DiSanto%2C+N">Nick DiSanto</a>, 
<a href="/search/cs?searchtype=author&query=Corso%2C+A">Anthony Corso</a>, 
<a href="/search/cs?searchtype=author&query=Sanders%2C+B">Benjamin Sanders</a>, 
<a href="/search/cs?searchtype=author&query=Harding%2C+G">Gavin Harding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">While transformers have pioneered attention-driven architectures as a
cornerstone of research, their dependence on explicitly contextual information
underscores limitations in their abilities to tacitly learn overarching textual
themes. This study investigates social media data as a source of distributed
patterns, challenging the heuristic paradigm of performance benchmarking. In
stark contrast to networks that rely on capturing complex long-term
dependencies, models of online data inherently lack structure and are forced to
learn underlying patterns in the aggregate. To properly represent these
abstract relationships, this research dissects empirical social media corpora
into their elemental components and analyzes over two billion tweets across
population-dense locations. Exploring the relationship between location and
vernacular in Twitter data, we employ Bag-of-Words models specific to each city
and evaluate their respective representation. This demonstrates that hidden
insights can be uncovered without the crutch of advanced algorithms and
demonstrates that even amidst noisy data, geographic location has a
considerable influence on online communication. This evidence presents tangible
insights regarding geospatial communication patterns and their implications in
social science. It also challenges the notion that intricate models are
prerequisites for pattern recognition in natural language, aligning with the
evolving landscape that questions the embrace of absolute interpretability over
abstract understanding. This study bridges the divide between sophisticated
frameworks and intangible relationships, paving the way for systems that blend
structured models with conjectural reasoning.
</p>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05379" title="Abstract">arXiv:2310.05379</a> [<a href="/pdf/2310.05379" title="Download PDF">pdf</a>, <a href="/format/2310.05379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An adaptive model reduction method leveraging locally supported basis  functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gao%2C+H">Han Gao</a>, 
<a href="/search/math?searchtype=author&query=Zahr%2C+M+J">Matthew J. Zahr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 23 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph)

</div>
<p class="mathjax">We propose a new method, the continuous Galerkin method with globally and
locally supported basis functions (CG-GL), to address the parametric robustness
issues of reduced-order models (ROMs) by incorporating solution-based
adaptivity with locally supported finite element basis functions. The CG-GL
method combines the accuracy of locally supported basis functions with the
efficiency of globally supported data-driven basis functions. Efficient
output-based dual-weighted residual error estimates are derived and implemented
for the CG-GL method and used to drive efficient online trial space adaptation.
An empirical quadrature procedure is introduced for rapid evaluation of
nonlinear terms that does not require retraining throughout the adaptation
process. Two numerical experiments demonstrate the potential of the CG-GL
method to produce accurate approximations with limited training and its tunable
tradeoff between accuracy and computational cost.
</p>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05380" title="Abstract">arXiv:2310.05380</a> [<a href="/pdf/2310.05380" title="Download PDF">pdf</a>, <a href="/format/2310.05380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmented Embeddings for Custom Retrievals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khatry%2C+A">Anirudh Khatry</a>, 
<a href="/search/cs?searchtype=author&query=Bajpai%2C+Y">Yasharth Bajpai</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+P">Priyanshu Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Gulwani%2C+S">Sumit Gulwani</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+A">Ashish Tiwari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Information retrieval involves selecting artifacts from a corpus that are
most relevant to a given search query. The flavor of retrieval typically used
in classical applications can be termed as homogeneous and relaxed, where
queries and corpus elements are both natural language (NL) utterances
(homogeneous) and the goal is to pick most relevant elements from the corpus in
the Top-K, where K is large, such as 10, 25, 50 or even 100 (relaxed).
Recently, retrieval is being used extensively in preparing prompts for large
language models (LLMs) to enable LLMs to perform targeted tasks. These new
applications of retrieval are often heterogeneous and strict -- the queries and
the corpus contain different kinds of entities, such as NL and code, and there
is a need for improving retrieval at Top-K for small values of K, such as K=1
or 3 or 5. Current dense retrieval techniques based on pretrained embeddings
provide a general-purpose and powerful approach for retrieval, but they are
oblivious to task-specific notions of similarity of heterogeneous artifacts. We
introduce Adapted Dense Retrieval, a mechanism to transform embeddings to
enable improved task-specific, heterogeneous and strict retrieval. Adapted
Dense Retrieval works by learning a low-rank residual adaptation of the
pretrained black-box embedding. We empirically validate our approach by showing
improvements over the state-of-the-art general-purpose embeddings-based
baseline.
</p>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05381" title="Abstract">arXiv:2310.05381</a> [<a href="/pdf/2310.05381" title="Download PDF">pdf</a>, <a href="/format/2310.05381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CCAE: A Corpus of Chinese-based Asian Englishes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+M+X">Melissa Xiaohui Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Long Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NLPCC'2023 (12 pages, 3 figures, 4 charts)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Language models have been foundations in various scenarios of NLP
applications, but it has not been well applied in language variety studies,
even for the most popular language like English. This paper represents one of
the few initial efforts to utilize the NLP technology in the paradigm of World
Englishes, specifically in creating a multi-variety corpus for studying Asian
Englishes. We present an overview of the CCAE -- Corpus of Chinese-based Asian
English, a suite of corpora comprising six Chinese-based Asian English
varieties. It is based on 340 million tokens in 448 thousand web documents from
six regions. The ontology of data would make the corpus a helpful resource with
enormous research potential for Asian Englishes (especially for Chinese
Englishes for which there has not been a publicly accessible corpus yet so far)
and an ideal source for variety-specific language modeling and downstream
tasks, thus setting the stage for NLP-based World Englishes studies. And
preliminary experiments on this corpus reveal the practical value of CCAE.
Finally, we make CCAE available at
\href{https://huggingface.co/datasets/CCAE/CCAE-Corpus}{this https URL}.
</p>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05383" title="Abstract">arXiv:2310.05383</a> [<a href="/pdf/2310.05383" title="Download PDF">pdf</a>, <a href="/format/2310.05383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Three-Stage Cascade Framework for Blurry Video Frame Interpolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+P">Pengcheng Lei</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zaoming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tingting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+F">Faming Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guixu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Blurry video frame interpolation (BVFI) aims to generate high-frame-rate
clear videos from low-frame-rate blurry videos, is a challenging but important
topic in the computer vision community. Blurry videos not only provide spatial
and temporal information like clear videos, but also contain additional motion
information hidden in each blurry frame. However, existing BVFI methods usually
fail to fully leverage all valuable information, which ultimately hinders their
performance. In this paper, we propose a simple end-to-end three-stage
framework to fully explore useful information from blurry videos. The frame
interpolation stage designs a temporal deformable network to directly sample
useful information from blurry inputs and synthesize an intermediate frame at
an arbitrary time interval. The temporal feature fusion stage explores the
long-term temporal information for each target frame through a bi-directional
recurrent deformable alignment network. And the deblurring stage applies a
transformer-empowered Taylor approximation network to recursively recover the
high-frequency details. The proposed three-stage framework has clear task
assignment for each module and offers good expandability, the effectiveness of
which are demonstrated by various experimental results. We evaluate our model
on four benchmarks, including the Adobe240 dataset, GoPro dataset, YouTube240
dataset and Sony dataset. Quantitative and qualitative results indicate that
our model outperforms existing SOTA methods. Besides, experiments on real-world
blurry videos also indicate the good generalization ability of our model.
</p>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05385" title="Abstract">arXiv:2310.05385</a> [<a href="/pdf/2310.05385" title="Download PDF">pdf</a>, <a href="/ps/2310.05385" title="Download PostScript">ps</a>, <a href="/format/2310.05385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conjunctive Queries with Negation and Aggregation: A Linear Time  Characterization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hangdong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+A+Z">Austen Z. Fan</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+X">Xiating Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Koutris%2C+P">Paraschos Koutris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">In this paper, we study the complexity of evaluating Conjunctive Queries with
negation (\cqneg). First, we present an algorithm with linear preprocessing
time and constant delay enumeration for a class of CQs with negation called
free-connex signed-acyclic queries. We show that no other queries admit such an
algorithm subject to lower bound conjectures. Second, we extend our algorithm
to Conjunctive Queries with negation and aggregation over a general semiring,
which we call Functional Aggregate Queries with negation (\faqneg). Such an
algorithm achieves constant delay enumeration for the same class of queries,
but with a slightly increased preprocessing time which includes an inverse
Ackermann function. We show that this surprising appearance of the Ackermmann
function is probably unavoidable for general semirings, but can be removed when
the semiring has specific structure. Finally, we show an application of our
results to computing the difference of CQs.
</p>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05387" title="Abstract">arXiv:2310.05387</a> [<a href="/pdf/2310.05387" title="Download PDF">pdf</a>, <a href="/format/2310.05387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equation Discovery with Bayesian Spike-and-Slab Priors and Efficient  Kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+D">Da Long</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+W+W">Wei W. Xing</a>, 
<a href="/search/cs?searchtype=author&query=Krishnapriyan%2C+A+S">Aditi S. Krishnapriyan</a>, 
<a href="/search/cs?searchtype=author&query=Kirby%2C+R+M">Robert M. Kirby</a>, 
<a href="/search/cs?searchtype=author&query=Zhe%2C+S">Shandian Zhe</a>, 
<a href="/search/cs?searchtype=author&query=Mahoney%2C+M+W">Michael W. Mahoney</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Discovering governing equations from data is important to many scientific and
engineering applications. Despite promising successes, existing methods are
still challenged by data sparsity as well as noise issues, both of which are
ubiquitous in practice. Moreover, state-of-the-art methods lack uncertainty
quantification and/or are costly in training. To overcome these limitations, we
propose a novel equation discovery method based on Kernel learning and BAyesian
Spike-and-Slab priors (KBASS). We use kernel regression to estimate the target
function, which is flexible, expressive, and more robust to data sparsity and
noises. We combine it with a Bayesian spike-and-slab prior -- an ideal Bayesian
sparse distribution -- for effective operator selection and uncertainty
quantification. We develop an expectation propagation expectation-maximization
(EP-EM) algorithm for efficient posterior inference and function estimation. To
overcome the computational challenge of kernel regression, we place the
function values on a mesh and induce a Kronecker product construction, and we
use tensor algebra methods to enable efficient computation and optimization. We
show the significant advantages of KBASS on a list of benchmark ODE and PDE
discovery tasks.
</p>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05388" title="Abstract">arXiv:2310.05388</a> [<a href="/pdf/2310.05388" title="Download PDF">pdf</a>, <a href="/format/2310.05388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GROVE: A Retrieval-augmented Complex Story Generation Framework with A  Forest of Evidence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Zhihua Wen</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zhiliang Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuxin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yanqi Shi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongsheng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Conditional story generation is significant in human-machine interaction,
particularly in producing stories with complex plots. While Large language
models (LLMs) perform well on multiple NLP tasks, including story generation,
it is challenging to generate stories with both complex and creative plots.
Existing methods often rely on detailed prompts to guide LLMs to meet target
conditions, which inadvertently restrict the creative potential of the
generated stories. We argue that leveraging information from exemplary
human-written stories facilitates generating more diverse plotlines. Delving
deeper into story details helps build complex and credible plots. In this
paper, we propose a retrieval-au\textbf{G}mented sto\textbf{R}y generation
framework with a f\textbf{O}rest of e\textbf{V}id\textbf{E}nce (GROVE) to
enhance stories' complexity. We build a retrieval repository for target
conditions to produce few-shot examples to prompt LLMs. Additionally, we design
an ``asking-why'' prompting scheme that extracts a forest of evidence,
providing compensation for the ambiguities that may occur in the generated
story. This iterative process uncovers underlying story backgrounds. Finally,
we select the most fitting chains of evidence from the evidence forest and
integrate them into the generated story, thereby enhancing the narrative's
complexity and credibility. Experimental results and numerous examples verify
the effectiveness of our method.
</p>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05391" title="Abstract">arXiv:2310.05391</a> [<a href="/pdf/2310.05391" title="Download PDF">pdf</a>, <a href="/format/2310.05391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Impostor: Editing Neural Radiance Fields with Explicit Shape  Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruiyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+J">Jinxu Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bowen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ran Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingyi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Changxi Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Pacific Graphics 2023 and Computer Graphics Forum
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Neural Radiance Fields (NeRF) have significantly advanced the generation of
highly realistic and expressive 3D scenes. However, the task of editing NeRF,
particularly in terms of geometry modification, poses a significant challenge.
This issue has obstructed NeRF's wider adoption across various applications. To
tackle the problem of efficiently editing neural implicit fields, we introduce
Neural Impostor, a hybrid representation incorporating an explicit tetrahedral
mesh alongside a multigrid implicit field designated for each tetrahedron
within the explicit mesh. Our framework bridges the explicit shape manipulation
and the geometric editing of implicit fields by utilizing multigrid barycentric
coordinate encoding, thus offering a pragmatic solution to deform, composite,
and generate neural implicit fields while maintaining a complex volumetric
appearance. Furthermore, we propose a comprehensive pipeline for editing neural
implicit fields based on a set of explicit geometric editing operations. We
show the robustness and adaptability of our system through diverse examples and
experiments, including the editing of both synthetic objects and real captured
data. Finally, we demonstrate the authoring process of a hybrid
synthetic-captured object utilizing a variety of editing operations,
underlining the transformative potential of Neural Impostor in the field of 3D
content creation and manipulation.
</p>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05392" title="Abstract">arXiv:2310.05392</a> [<a href="/pdf/2310.05392" title="Download PDF">pdf</a>, <a href="/format/2310.05392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lightweight Full-Convolutional Siamese Tracker
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yunfeng%2C+L">Li Yunfeng</a>, 
<a href="/search/cs?searchtype=author&query=Bo%2C+W">Wang Bo</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+L">Li Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhuoyan%2C+L">Liu Zhuoyan</a>, 
<a href="/search/cs?searchtype=author&query=Xueyi%2C+W">Wu Xueyi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Although single object trackers have achieved advanced performance, their
large-scale network models make it difficult to apply them on the platforms
with limited resources. Moreover, existing lightweight trackers only achieve
balance between 2-3 points in terms of parameters, performance, Flops and FPS.
To achieve the balance among all 4 points, this paper propose a lightweight
full-convolutional Siamese tracker called lightFC. LightFC employs a noval
efficient cross-correlation module (ECM) and a noval efficient rep-center head
(ERH) to enhance the nonlinear expressiveness of the convoluational tracking
pipeline. The ECM adopts an architecture of attention-like module and fuses
local spatial and channel features from the pixel-wise correlation fusion
features and enhance model nonlinearity with an inversion activation block.
Additionally, skip-connections and the reuse of search area features are
introduced by the ECM to improve its performance. The ERH reasonably introduces
reparameterization technology and channel attention to enhance the nonlinear
expressiveness of the center head. Comprehensive experiments show that LightFC
achieves a good balance between performance, parameters, Flops and FPS. The
precision score of LightFC outperforms MixFormerV2-S by 3.7 \% and 6.5 \% on
LaSOT and TNL2K, respectively, while using 5x fewer parameters and 4.6x fewer
Flops. Besides, LightFC runs 2x faster than MixFormerV2-S on CPUs. Our code and
raw results can be found at https://github.com/LiYunfengLYF/LightFC
</p>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05393" title="Abstract">arXiv:2310.05393</a> [<a href="/pdf/2310.05393" title="Download PDF">pdf</a>, <a href="/format/2310.05393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Side-Tuning for Vision Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weifeng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Ziheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiayu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wentao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Mingxin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Lianwen Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Fine-tuning pre-trained Vision Transformers (ViT) has consistently
demonstrated promising performance in the realm of visual recognition. However,
adapting large pre-trained models to various tasks poses a significant
challenge. This challenge arises from the need for each model to undergo an
independent and comprehensive fine-tuning process, leading to substantial
computational and memory demands. While recent advancements in
Parameter-efficient Transfer Learning (PETL) have demonstrated their ability to
achieve superior performance compared to full fine-tuning with a smaller subset
of parameter updates, they tend to overlook dense prediction tasks such as
object detection and segmentation. In this paper, we introduce Hierarchical
Side-Tuning (HST), a novel PETL approach that enables ViT transfer to various
downstream tasks effectively. Diverging from existing methods that exclusively
fine-tune parameters within input spaces or certain modules connected to the
backbone, we tune a lightweight and hierarchical side network (HSN) that
leverages intermediate activations extracted from the backbone and generates
multi-scale features to make predictions. To validate HST, we conducted
extensive experiments encompassing diverse visual tasks, including
classification, object detection, instance segmentation, and semantic
segmentation. Notably, our method achieves state-of-the-art average Top-1
accuracy of 76.0% on VTAB-1k, all while fine-tuning a mere 0.78M parameters.
When applied to object detection tasks on COCO testdev benchmark, HST even
surpasses full fine-tuning and obtains better performance with 49.7 box AP and
43.2 mask AP using Cascade Mask R-CNN.
</p>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05394" title="Abstract">arXiv:2310.05394</a> [<a href="/pdf/2310.05394" title="Download PDF">pdf</a>, <a href="/format/2310.05394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAMEL2: Enhancing weakly supervised learning for histopathology images  by incorporating the significance ratio
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Gang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lingyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tongwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhenwei Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dahan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zewen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Aijun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ba%2C+W">Wei Ba</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhigang Song</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Huaiyin Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+D">Dingrong Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jianpeng Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Histopathology image analysis plays a crucial role in cancer diagnosis.
However, training a clinically applicable segmentation algorithm requires
pathologists to engage in labour-intensive labelling. In contrast, weakly
supervised learning methods, which only require coarse-grained labels at the
image level, can significantly reduce the labeling efforts. Unfortunately,
while these methods perform reasonably well in slide-level prediction, their
ability to locate cancerous regions, which is essential for many clinical
applications, remains unsatisfactory. Previously, we proposed CAMEL, which
achieves comparable results to those of fully supervised baselines in
pixel-level segmentation. However, CAMEL requires 1,280x1,280 image-level
binary annotations for positive WSIs. Here, we present CAMEL2, by introducing a
threshold of the cancerous ratio for positive bags, it allows us to better
utilize the information, consequently enabling us to scale up the image-level
setting from 1,280x1,280 to 5,120x5,120 while maintaining the accuracy. Our
results with various datasets, demonstrate that CAMEL2, with the help of
5,120x5,120 image-level binary annotations, which are easy to annotate,
achieves comparable performance to that of a fully supervised baseline in both
instance- and slide-level classifications.
</p>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05395" title="Abstract">arXiv:2310.05395</a> [<a href="/pdf/2310.05395" title="Download PDF">pdf</a>, <a href="/format/2310.05395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Image Watermarking based on Cross-Attention and Invariant Domain  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dasgupta%2C+A">Agnibh Dasgupta</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+X">Xin Zhong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Image watermarking involves embedding and extracting watermarks within a
cover image, with deep learning approaches emerging to bolster generalization
and robustness. Predominantly, current methods employ convolution and
concatenation for watermark embedding, while also integrating conceivable
augmentation in the training process. This paper explores a robust image
watermarking methodology by harnessing cross-attention and invariant domain
learning, marking two novel, significant advancements. First, we design a
watermark embedding technique utilizing a multi-head cross attention mechanism,
enabling information exchange between the cover image and watermark to identify
semantically suitable embedding locations. Second, we advocate for learning an
invariant domain representation that encapsulates both semantic and
noise-invariant information concerning the watermark, shedding light on
promising avenues for enhancing image watermarking techniques.
</p>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05396" title="Abstract">arXiv:2310.05396</a> [<a href="/pdf/2310.05396" title="Download PDF">pdf</a>, <a href="/format/2310.05396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing Barriers and Technology Needs in the Kitchen for Blind  and Low Vision People
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ru Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+N">Nihan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Tam Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+S">Sanbrita Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+B">Bilge Mutlu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuhang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Cooking is a vital yet challenging activity for people with visual
impairments (PVI). It involves tasks that can be dangerous or difficult without
vision, such as handling a knife or adding a suitable amount of salt. A better
understanding of these challenges can inform the design of technologies that
mitigate safety hazards and improve the quality of the lives of PVI.
Furthermore, there is a need to understand the effects of different visual
abilities, including low vision and blindness, and the role of rehabilitation
training where PVI learn cooking skills and assistive technologies. In this
paper, we aim to comprehensively characterize PVI's challenges, strategies, and
needs in the kitchen from the perspectives of both PVI and rehabilitation
professionals. Through a contextual inquiry study, we observed 10 PVI,
including six low vision and four blind participants, when they cooked dishes
of their choices in their own kitchens. We then interviewed six rehabilitation
professionals to explore their training strategies and technology
recommendations. Our findings revealed the differences between low vision and
blind people during cooking as well as the gaps between training and reality.
We suggest improvements for rehabilitation training and distill design
considerations for future assistive technology in the kitchen.
</p>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05397" title="Abstract">arXiv:2310.05397</a> [<a href="/pdf/2310.05397" title="Download PDF">pdf</a>, <a href="/format/2310.05397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Find Your Optimal Assignments On-the-fly: A Holistic Framework for  Clustered Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yongxin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaoying Tang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated Learning (FL) is an emerging distributed machine learning approach
that preserves client privacy by storing data on edge devices. However, data
heterogeneity among clients presents challenges in training models that perform
well on all local distributions. Recent studies have proposed clustering as a
solution to tackle client heterogeneity in FL by grouping clients with
distribution shifts into different clusters. However, the diverse learning
frameworks used in current clustered FL methods make it challenging to
integrate various clustered FL methods, gather their benefits, and make further
improvements.
<br />To this end, this paper presents a comprehensive investigation into current
clustered FL methods and proposes a four-tier framework, namely HCFL, to
encompass and extend existing approaches. Based on the HCFL, we identify the
remaining challenges associated with current clustering methods in each tier
and propose an enhanced clustering method called HCFL+ to address these
challenges. Through extensive numerical evaluations, we showcase the
effectiveness of our clustering framework and the improved components. Our code
will be publicly available.
</p>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05399" title="Abstract">arXiv:2310.05399</a> [<a href="/pdf/2310.05399" title="Download PDF">pdf</a>, <a href="/format/2310.05399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a debuggable kernel design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parimoo%2C+C">Chandrika Parimoo</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Ashish Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Operating Systems (cs.OS)</span>

</div>
<p class="mathjax">This paper describes what it means for a kernel to be debuggable and proposes
a kernel design with debuggability in mind. We evaluate the proposed kernel
design by comparing the iterations required in cyclic debugging for different
classes of bugs in a vanilla monolithic kernel to a variant enhanced with our
design rules for debuggability. We discuss the trade offs involved in designing
a debuggable kernel.
</p>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05400" title="Abstract">arXiv:2310.05400</a> [<a href="/pdf/2310.05400" title="Download PDF">pdf</a>, <a href="/format/2310.05400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient-VQGAN: Towards High-Resolution Image Generation with Efficient  Vision Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+S">Shiyue Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yueqin Yin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lianghua Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Deli Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaiqi Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted to ICCV2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Vector-quantized image modeling has shown great potential in synthesizing
high-quality images. However, generating high-resolution images remains a
challenging task due to the quadratic computational overhead of the
self-attention process. In this study, we seek to explore a more efficient
two-stage framework for high-resolution image generation with improvements in
the following three aspects. (1) Based on the observation that the first
quantization stage has solid local property, we employ a local attention-based
quantization model instead of the global attention mechanism used in previous
methods, leading to better efficiency and reconstruction quality. (2) We
emphasize the importance of multi-grained feature interaction during image
generation and introduce an efficient attention mechanism that combines global
attention (long-range semantic consistency within the whole image) and local
attention (fined-grained details). This approach results in faster generation
speed, higher generation fidelity, and improved resolution. (3) We propose a
new generation pipeline incorporating autoencoding training and autoregressive
generation strategy, demonstrating a better paradigm for image synthesis.
Extensive experiments demonstrate the superiority of our approach in
high-quality and high-resolution image reconstruction and generation.
</p>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05401" title="Abstract">arXiv:2310.05401</a> [<a href="/pdf/2310.05401" title="Download PDF">pdf</a>, <a href="/format/2310.05401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy-MCMC: Sampling from Flat Basins with Ease
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bolian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruqi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Bayesian deep learning counts on the quality of posterior distribution
estimation. However, the posterior of deep neural networks is highly
multi-modal in nature, with local modes exhibiting varying generalization
performance. Given a practical budget, sampling from the original posterior can
lead to suboptimal performance, as some samples may become trapped in "bad"
modes and suffer from overfitting. Leveraging the observation that "good" modes
with low generalization error often reside in flat basins of the energy
landscape, we propose to bias sampling on the posterior toward these flat
regions. Specifically, we introduce an auxiliary guiding variable, the
stationary distribution of which resembles a smoothed posterior free from sharp
modes, to lead the MCMC sampler to flat basins. By integrating this guiding
variable with the model parameter, we create a simple joint distribution that
enables efficient sampling with minimal computational overhead. We prove the
convergence of our method and further show that it converges faster than
several existing flatness-aware methods in the strongly convex setting.
Empirical results demonstrate that our method can successfully sample from flat
basins of the posterior, and outperforms all compared baselines on multiple
benchmarks including classification, calibration, and out-of-distribution
detection.
</p>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05404" title="Abstract">arXiv:2310.05404</a> [<a href="/pdf/2310.05404" title="Download PDF">pdf</a>, <a href="/format/2310.05404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> mBBC: Exploring the Multilingual Maze
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nezhad%2C+S+B">Sina Bagheri Nezhad</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+A">Ameeta Agrawal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multilingual language models have gained significant attention in recent
years, enabling the development of applications that cater to diverse
linguistic contexts. In this paper, we present a comprehensive evaluation of
three prominent multilingual language models: mBERT, XLM-R, and GPT-3. Using
the self-supervised task of next token prediction, we assess their performance
across a diverse set of languages, with a focus on understanding the impact of
resource availability, word order, language family, and script type on model
accuracy. Our findings reveal that resource availability plays a crucial role
in model performance, with higher resource levels leading to improved accuracy.
We also identify the complex relationship between resource availability,
language families, and script types, highlighting the need for further
investigation into language-specific characteristics and structural variations.
Additionally, our statistical inference analysis identifies significant
features contributing to model performance, providing insights for model
selection and deployment. Our study contributes to a deeper understanding of
multilingual language models and informs future research and development to
enhance their performance and generalizability across languages and linguistic
contexts.
</p>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05406" title="Abstract">arXiv:2310.05406</a> [<a href="/pdf/2310.05406" title="Download PDF">pdf</a>, <a href="/format/2310.05406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GradientSurf: Gradient-Domain Neural Surface Reconstruction from RGB  Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C+H">Crane He Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liebelt%2C+J">Joerg Liebelt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper proposes GradientSurf, a novel algorithm for real time surface
reconstruction from monocular RGB video. Inspired by Poisson Surface
Reconstruction, the proposed method builds on the tight coupling between
surface, volume, and oriented point cloud and solves the reconstruction problem
in gradient-domain. Unlike Poisson Surface Reconstruction which finds an
offline solution to the Poisson equation by solving a linear system after the
scanning process is finished, our method finds online solutions from partial
scans with a neural network incrementally where the Poisson layer is designed
to supervise both local and global reconstruction. The main challenge that
existing methods suffer from when reconstructing from RGB signal is a lack of
details in the reconstructed surface. We hypothesize this is due to the
spectral bias of neural networks towards learning low frequency geometric
features. To address this issue, the reconstruction problem is cast onto
gradient domain, where zeroth-order and first-order energies are minimized. The
zeroth-order term penalizes location of the surface. The first-order term
penalizes the difference between the gradient of reconstructed implicit
function and the vector field formulated from oriented point clouds sampled at
adaptive local densities. For the task of indoor scene reconstruction, visual
and quantitative experimental results show that the proposed method
reconstructs surfaces with more details in curved regions and higher fidelity
for small objects than previous methods.
</p>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05407" title="Abstract">arXiv:2310.05407</a> [<a href="/pdf/2310.05407" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPS Attack Detection and Mitigation for Safe Autonomous Driving using  Image and Map based Lateral Direction Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qingming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guoqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenpo Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The accuracy and robustness of vehicle localization are critical for
achieving safe and reliable high-level autonomy. Recent results show that GPS
is vulnerable to spoofing attacks, which is one major threat to autonomous
driving. In this paper, a novel anomaly detection and mitigation method against
GPS attacks that utilizes onboard camera and high-precision maps is proposed to
ensure accurate vehicle localization. First, lateral direction localization in
driving lanes is calculated by camera-based lane detection and map matching
respectively. Then, a real-time detector for GPS spoofing attack is developed
to evaluate the localization data. When the attack is detected, a multi-source
fusion-based localization method using Unscented Kalman filter is derived to
mitigate GPS attack and improve the localization accuracy. The proposed method
is validated in various scenarios in Carla simulator and open-source public
dataset to demonstrate its effectiveness in timely GPS attack detection and
data recovery.
</p>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05410" title="Abstract">arXiv:2310.05410</a> [<a href="/pdf/2310.05410" title="Download PDF">pdf</a>, <a href="/format/2310.05410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Reasoning through Two Layers of Cognition for Improving  Generalization in Visual Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Trang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Okazaki%2C+N">Naoaki Okazaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Generalization in Visual Question Answering (VQA) requires models to answer
questions about images with contexts beyond the training distribution. Existing
attempts primarily refine unimodal aspects, overlooking enhancements in
multimodal aspects. Besides, diverse interpretations of the input lead to
various modes of answer generation, highlighting the role of causal reasoning
between interpreting and answering steps in VQA. Through this lens, we propose
Cognitive pathways VQA (CopVQA) improving the multimodal predictions by
emphasizing causal reasoning factors. CopVQA first operates a pool of pathways
that capture diverse causal reasoning flows through interpreting and answering
stages. Mirroring human cognition, we decompose the responsibility of each
stage into distinct experts and a cognition-enabled component (CC). The two CCs
strategically execute one expert for each stage at a time. Finally, we
prioritize answer predictions governed by pathways involving both CCs while
disregarding answers produced by either CC, thereby emphasizing causal
reasoning and supporting generalization. Our experiments on real-life and
medical data consistently verify that CopVQA improves VQA performance and
generalization across baselines and domains. Notably, CopVQA achieves a new
state-of-the-art (SOTA) on PathVQA dataset and comparable accuracy to the
current SOTA on VQA-CPv2, VQAv2, and VQA RAD, with one-fourth of the model
size.
</p>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05411" title="Abstract">arXiv:2310.05411</a> [<a href="/pdf/2310.05411" title="Download PDF">pdf</a>, <a href="/format/2310.05411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Structure-Preserving Relaxation Method for Charged Systems on  Unstructured Meshes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Qiao%2C+Z">Zhonghua Qiao</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+Z">Zhenli Xu</a>, 
<a href="/search/math?searchtype=author&query=Yin%2C+Q">Qian Yin</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+S">Shenggao Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This work considers charged systems described by the modified
Poisson--Nernst--Planck (PNP) equations, which incorporate ionic steric effects
and the Born solvation energy for dielectric inhomogeneity. Solving the
steady-state modified PNP equations poses numerical challenges due to the
emergence of sharp boundary layers caused by small Debye lengths, particularly
when local ionic concentrations reach saturation. To address this, we first
reformulate the steady-state problem as a constraint optimization, where the
ionic concentrations on unstructured Delaunay nodes are treated as fractional
particles moving along edges between nodes. The electric fields are then
updated to minimize the objective free energy while satisfying the discrete
Gauss's law. We develop a local relaxation method on unstructured meshes that
inherently respects the discrete Gauss's law, ensuring curl-free electric
fields. Numerical analysis demonstrates that the optimal mass of the moving
fractional particles guarantees the positivity of both ionic and solvent
concentrations. Additionally, the free energy of the charged system
consistently decreases during successive updates of ionic concentrations and
electric fields. We conduct numerical tests to validate the expected numerical
accuracy, positivity, free-energy dissipation, and robustness of our method in
simulating charged systems with sharp boundary layers.
</p>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05414" title="Abstract">arXiv:2310.05414</a> [<a href="/pdf/2310.05414" title="Download PDF">pdf</a>, <a href="/format/2310.05414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ethics of Artificial Intelligence and Robotics in the Architecture,  Engineering, and Construction Industry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+C">Ci-Jyun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Thai-Hoa Le</a>, 
<a href="/search/cs?searchtype=author&query=Ham%2C+Y">Youngjib Ham</a>, 
<a href="/search/cs?searchtype=author&query=Mantha%2C+B+R+K">Bharadwaj R. K. Mantha</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M+H">Marvin H. Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J+J">Jacob J. Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 109 pages, 5 figures, submitted to Automation in Construction
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Artificial intelligence (AI) and robotics research and implementation emerged
in the architecture, engineering, and construction (AEC) industry to positively
impact project efficiency and effectiveness concerns such as safety,
productivity, and quality. This shift, however, warrants the need for ethical
considerations of AI and robotics adoption due to its potential negative
impacts on aspects such as job security, safety, and privacy. Nevertheless,
this did not receive sufficient attention, particularly within the academic
community. This research systematically reviews AI and robotics research
through the lens of ethics in the AEC community for the past five years. It
identifies nine key ethical issues namely job loss, data privacy, data
security, data transparency, decision-making conflict, acceptance and trust,
reliability and safety, fear of surveillance, and liability, by summarizing
existing literature and filtering it further based on its AEC relevance.
Furthermore, thirteen research topics along the process were identified based
on existing AEC studies that had direct relevance to the theme of ethics in
general and their parallels are further discussed. Finally, the current
challenges and knowledge gaps are discussed and seven specific future research
directions are recommended. This study not only signifies more stakeholder
awareness of this important topic but also provides imminent steps towards
safer and more efficient realization.
</p>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05418" title="Abstract">arXiv:2310.05418</a> [<a href="/pdf/2310.05418" title="Download PDF">pdf</a>, <a href="/format/2310.05418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Humanoid Agents: Platform for Simulating Human-like Generative Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhilin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+Y+Y">Yu Ying Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+Y+C">Yu Cheung Chiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP System Demonstrations 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Just as computational simulations of atoms, molecules and cells have shaped
the way we study the sciences, true-to-life simulations of human-like agents
can be valuable tools for studying human behavior. We propose Humanoid Agents,
a system that guides Generative Agents to behave more like humans by
introducing three elements of System 1 processing: Basic needs (e.g. hunger,
health and energy), Emotion and Closeness in Relationships. Humanoid Agents are
able to use these dynamic elements to adapt their daily activities and
conversations with other agents, as supported with empirical experiments. Our
system is designed to be extensible to various settings, three of which we
demonstrate, as well as to other elements influencing human behavior (e.g.
empathy, moral values and cultural background). Our platform also includes a
Unity WebGL game interface for visualization and an interactive analytics
dashboard to show agent statuses over time. Our platform is available on
https://www.humanoidagents.com/ and code is on
https://github.com/HumanoidAgents/HumanoidAgents
</p>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05421" title="Abstract">arXiv:2310.05421</a> [<a href="/pdf/2310.05421" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automating Customer Service using LangChain: Building custom open-source  GPT Chatbot for organizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pandya%2C+K">Keivalya Pandya</a>, 
<a href="/search/cs?searchtype=author&query=Holia%2C+M">Mehfuza Holia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 2 figures, Submitted to appear in the Proceedings of the 3rd International Conference on Women in Science &amp; Technology Creating Sustainable Career (ICWSTCSC 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the digital age, the dynamics of customer service are evolving, driven by
technological advancements and the integration of Large Language Models (LLMs).
This research paper introduces a groundbreaking approach to automating customer
service using LangChain, a custom LLM tailored for organizations. The paper
explores the obsolescence of traditional customer support techniques,
particularly Frequently Asked Questions (FAQs), and proposes a paradigm shift
towards responsive, context-aware, and personalized customer interactions. The
heart of this innovation lies in the fusion of open-source methodologies, web
scraping, fine-tuning, and the seamless integration of LangChain into customer
service platforms. This open-source state-of-the-art framework, presented as
"Sahaay," demonstrates the ability to scale across industries and
organizations, offering real-time support and query resolution. Key elements of
this research encompass data collection via web scraping, the role of
embeddings, the utilization of Google's Flan T5 XXL, Base and Small language
models for knowledge retrieval, and the integration of the chatbot into
customer service platforms. The results section provides insights into their
performance and use cases, here particularly within an educational institution.
This research heralds a new era in customer service, where technology is
harnessed to create efficient, personalized, and responsive interactions.
Sahaay, powered by LangChain, redefines the customer-company relationship,
elevating customer retention, value extraction, and brand image. As
organizations embrace LLMs, customer service becomes a dynamic and
customer-centric ecosystem.
</p>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05422" title="Abstract">arXiv:2310.05422</a> [<a href="/pdf/2310.05422" title="Download PDF">pdf</a>, <a href="/format/2310.05422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reward-Consistent Dynamics Models are Strongly Generalizable for Offline  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Fan-Ming Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xingchen Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yang Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Learning a precise dynamics model can be crucial for offline reinforcement
learning, which, unfortunately, has been found to be quite challenging.
Dynamics models that are learned by fitting historical transitions often
struggle to generalize to unseen transitions. In this study, we identify a
hidden but pivotal factor termed dynamics reward that remains consistent across
transitions, offering a pathway to better generalization. Therefore, we propose
the idea of reward-consistent dynamics models: any trajectory generated by the
dynamics model should maximize the dynamics reward derived from the data. We
implement this idea as the MOREC (Model-based Offline reinforcement learning
with Reward Consistency) method, which can be seamlessly integrated into
previous offline model-based reinforcement learning (MBRL) methods. MOREC
learns a generalizable dynamics reward function from offline data, which is
subsequently employed as a transition filter in any offline MBRL method: when
generating transitions, the dynamics model generates a batch of transitions and
selects the one with the highest dynamics reward value. On a synthetic task, we
visualize that MOREC has a strong generalization ability and can surprisingly
recover some distant unseen transitions. On 21 offline tasks in D4RL and NeoRL
benchmarks, MOREC improves the previous state-of-the-art performance by a
significant margin, i.e., 4.6% on D4RL tasks and 25.9% on NeoRL tasks. Notably,
MOREC is the first method that can achieve above 95% online RL performance in 6
out of 12 D4RL tasks and 3 out of 9 NeoRL tasks.
</p>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05423" title="Abstract">arXiv:2310.05423</a> [<a href="/pdf/2310.05423" title="Download PDF">pdf</a>, <a href="/format/2310.05423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequential Tag Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Pengyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Sijin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shijing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hongjian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+L">Liping Jing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">With the development of Internet technology and the expansion of social
networks, online platforms have become an important way for people to obtain
information. The introduction of tags facilitates information categorization
and retrieval. Meanwhile, the development of tag recommendation systems not
only enables users to input tags more efficiently, but also improves the
quality of tags. However, current tag recommendation methods only consider the
content of the current post and do not take into account the influence of user
preferences. Since the main body of tag recommendation is the user, it is very
necessary to obtain the user's tagging habits. Therefore, this paper proposes a
tag recommendation algorithm (MLP4STR) based on the dynamic preference of
user's behavioral sequence, which models the user's historical post information
and historical tag information to obtain the user's dynamic interest changes. A
pure MLP structure across feature dimensions is used in sequence modeling to
model the interaction between tag content and post content to fully extract the
user's interests. Finally tag recommendation is performed.
</p>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05424" title="Abstract">arXiv:2310.05424</a> [<a href="/pdf/2310.05424" title="Download PDF">pdf</a>, <a href="/format/2310.05424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and Robust Early-Exiting Framework for Autoregressive Language  Models with Synchronized Parallel Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bae%2C+S">Sangmin Bae</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">Jongwoo Ko</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H">Hwanjun Song</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Long)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">To tackle the high inference latency exhibited by autoregressive language
models, previous studies have proposed an early-exiting framework that
allocates adaptive computation paths for each token based on the complexity of
generating the subsequent token. However, we observed several shortcomings,
including performance degradation caused by a state copying mechanism or
numerous exit paths, and sensitivity to exit confidence thresholds.
Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework,
which incorporates a shallow-deep module and a synchronized parallel decoding.
Our framework enables faster inference by synchronizing the decoding process of
the current token with previously stacked early-exited tokens. Furthermore, as
parallel decoding allows us to observe predictions from both shallow and deep
models, we present a novel adaptive threshold estimator that exploits a Beta
mixture model to determine suitable confidence thresholds. We empirically
demonstrated the superiority of our proposed framework on extensive generation
tasks.
</p>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05425" title="Abstract">arXiv:2310.05425</a> [<a href="/pdf/2310.05425" title="Download PDF">pdf</a>, <a href="/format/2310.05425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Divide and Ensemble: Progressively Learning for the Unknown
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xin Shen</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Heming Du</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huiqiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+H">Hongwei Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qingzheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+W">MD Wahiduzzaman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qingtao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tianqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chapman%2C+S">Scott Chapman</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xin Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In the wheat nutrient deficiencies classification challenge, we present the
DividE and EnseMble (DEEM) method for progressive test data predictions. We
find that (1) test images are provided in the challenge; (2) samples are
equipped with their collection dates; (3) the samples of different dates show
notable discrepancies. Based on the findings, we partition the dataset into
discrete groups by the dates and train models on each divided group. We then
adopt the pseudo-labeling approach to label the test data and incorporate those
with high confidence into the training set. In pseudo-labeling, we leverage
models ensemble with different architectures to enhance the reliability of
predictions. The pseudo-labeling and ensembled model training are iteratively
conducted until all test samples are labeled. Finally, the separated models for
each group are unified to obtain the model for the whole dataset. Our method
achieves an average of 93.6\% Top-1 test accuracy~(94.0\% on WW2020 and 93.2\%
on WR2021) and wins the 1$st$ place in the Deep Nutrient Deficiency
Challenge~\footnote{https://cvppa2023.github.io/challenges/}.
</p>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05427" title="Abstract">arXiv:2310.05427</a> [<a href="/pdf/2310.05427" title="Download PDF">pdf</a>, <a href="/format/2310.05427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergent chaotic iterations in hard sparse instances of hamiltonian path  problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Lima%2C+C+A">Cicero A. de Lima</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">A hamiltonian path is a path walk P that can be a hamiltonian path or
hamiltonian circuit. Determining whether such hamiltonian path exists in a
given graph G = (V, E) is a NP-Complete problem. In this paper, a novel
algorithm with chaotic behaviour for hamiltonian path problem is proposed. We
show that our algorithm runs in $O(V^5(V + E))$ for hard sparse instances from
FHCP challenge dataset.
</p>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05428" title="Abstract">arXiv:2310.05428</a> [<a href="/pdf/2310.05428" title="Download PDF">pdf</a>, <a href="/format/2310.05428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic-aware Temporal Channel-wise Attention for Cardiac Function  Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guanqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanbin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ISBI 2022 (oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Cardiac function assessment aims at predicting left ventricular ejection
fraction (LVEF) given an echocardiogram video, which requests models to focus
on the changes in the left ventricle during the cardiac cycle. How to assess
cardiac function accurately and automatically from an echocardiogram video is a
valuable topic in intelligent assisted healthcare. Existing video-based methods
do not pay much attention to the left ventricular region, nor the left
ventricular changes caused by motion. In this work, we propose a
semi-supervised auxiliary learning paradigm with a left ventricular
segmentation task, which contributes to the representation learning for the
left ventricular region. To better model the importance of motion information,
we introduce a temporal channel-wise attention (TCA) module to excite those
channels used to describe motion. Furthermore, we reform the TCA module with
semantic perception by taking the segmentation map of the left ventricle as
input to focus on the motion patterns of the left ventricle. Finally, to reduce
the difficulty of direct LVEF regression, we utilize an anchor-based
classification and regression method to predict LVEF. Our approach achieves
state-of-the-art performance on the Stanford dataset with an improvement of
0.22 MAE, 0.26 RMSE, and 1.9% $R^2$.
</p>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05430" title="Abstract">arXiv:2310.05430</a> [<a href="/pdf/2310.05430" title="Download PDF">pdf</a>, <a href="/format/2310.05430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Replication of Multi-agent Reinforcement Learning for the &quot;Hide and  Seek&quot; Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamal%2C+H">Haider Kamal</a>, 
<a href="/search/cs?searchtype=author&query=Niazi%2C+M+A">Muaz A. Niazi</a>, 
<a href="/search/cs?searchtype=author&query=Afzal%2C+H">Hammad Afzal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA); Robotics (cs.RO)

</div>
<p class="mathjax">Reinforcement learning generates policies based on reward functions and
hyperparameters. Slight changes in these can significantly affect results. The
lack of documentation and reproducibility in Reinforcement learning research
makes it difficult to replicate once-deduced strategies. While previous
research has identified strategies using grounded maneuvers, there is limited
work in more complex environments. The agents in this study are simulated
similarly to Open Al's hider and seek agents, in addition to a flying
mechanism, enhancing their mobility, and expanding their range of possible
actions and strategies. This added functionality improves the Hider agents to
develop a chasing strategy from approximately 2 million steps to 1.6 million
steps and hiders
</p>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05431" title="Abstract">arXiv:2310.05431</a> [<a href="/pdf/2310.05431" title="Download PDF">pdf</a>, <a href="/format/2310.05431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RECESS Vaccine for Federated Learning: Proactive Defense Against Model  Poisoning Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Haonan Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenjing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoguang Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wenhai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hui Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xiaodong Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Model poisoning attacks greatly jeopardize the application of federated
learning (FL). The effectiveness of existing defenses is susceptible to the
latest model poisoning attacks, leading to a decrease in prediction accuracy.
Besides, these defenses are intractable to distinguish benign outliers from
malicious gradients, which further compromises the model generalization. In
this work, we propose a novel proactive defense named RECESS against model
poisoning attacks. Different from the passive analysis in previous defenses,
RECESS proactively queries each participating client with a delicately
constructed aggregation gradient, accompanied by the detection of malicious
clients according to their responses with higher accuracy. Furthermore, RECESS
uses a new trust scoring mechanism to robustly aggregate gradients. Unlike
previous methods that score each iteration, RECESS considers clients'
performance correlation across multiple iterations to estimate the trust score,
substantially increasing fault tolerance. Finally, we extensively evaluate
RECESS on typical model architectures and four datasets under various settings.
We also evaluated the defensive effectiveness against other types of poisoning
attacks, the sensitivity of hyperparameters, and adaptive adversarial attacks.
Experimental results show the superiority of RECESS in terms of reducing
accuracy loss caused by the latest model poisoning attacks over five classic
and two state-of-the-art defenses.
</p>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05432" title="Abstract">arXiv:2310.05432</a> [<a href="/pdf/2310.05432" title="Download PDF">pdf</a>, <a href="/format/2310.05432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergency Financing Tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goodell%2C+G">Geoffrey Goodell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">We propose a novel payment mechanism for use by victims of large-scale
conflict or natural disasters to conduct critical economic transactions and
rebuild damaged infrastructure in the absence of both cash and traditional
electronic payment mechanisms linked to bank accounts, such as debit cards or
wire transfers. Claimants shall receive electronic tokens that can be used to
pay registered businesses, such as purveyors of food and other basic goods,
providers of essential services, and contractors to carry out construction
tasks. The system shall be based upon the scalable architecture for retail
payments described in our earlier work, which provides both strong privacy for
consumers and strong compliance enforcement for recipients of funds. The system
shall be designed to achieve three main objectives. First, tokens issued to
claimants would be held directly by the claimants themselves, not via
intermediaries, to avoid the risk of failure or subversion of asset custodians.
Second, transactions shall not be traceable to the identity of the claimants,
thus mitigating the risk that claimants can be pressured by service providers
or other parties to reveal information that can be used to exploit them. Third,
businesses and service providers that receive tokens shall be subject to
rigorous compliance procedures upon redemption for cash or bank deposits, thus
ensuring that only legitimate businesses or service providers can receive value
from tokens, that token transfers will embed the identities of any recipients
beyond the initial claimant, and that tax obligations shall be met at the time
of redemption.
</p>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05440" title="Abstract">arXiv:2310.05440</a> [<a href="/pdf/2310.05440" title="Download PDF">pdf</a>, <a href="/format/2310.05440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Modeling and Simulation of Chemo-Elasto-Plastically Coupled  Battery Active Particles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Schoof%2C+R">Raphael Schoof</a>, 
<a href="/search/math?searchtype=author&query=Niermann%2C+J">Johannes Niermann</a>, 
<a href="/search/math?searchtype=author&query=Dyck%2C+A">Alexander Dyck</a>, 
<a href="/search/math?searchtype=author&query=B%C3%B6hlke%2C+T">Thomas B&#xf6;hlke</a>, 
<a href="/search/math?searchtype=author&query=D%C3%B6rfler%2C+W">Willy D&#xf6;rfler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">As an anode material for lithium-ion batteries, amorphous silicon offers a
significantly higher energy density than the graphite anodes currently used.
Alloying reactions of lithium and silicon, however, induce large deformation
and lead to volume changes up to 300%. We formulate a thermodynamically
consistent continuum model for the chemo-elasto-plastic diffusion-deformation
based on finite deformations. In this paper, a plastic deformation approach
with linear isotropic hardening and a viscoplastic deformation ansatz are
investigated and compared to allow the evolution of plastic deformations and
reduce occurring stresses. For both models, a return mapping can be derived to
update the equivalent plastic strain for the next time step. Using a finite
element method and an efficient space and time adaptive solution algorithm a
large number of charging cycles can be examined. We derive a linearization for
the global Newton scheme and compare it to an automatic differentiation
technique regarding the numerical performance and physical results. Both
plastic approaches lead to a stronger heterogeneous concentration distribution
and to a change to tensile tangential Cauchy stresses at the particle surface
at the end of one charging cycle. Different parameter studies show how an
amplification of the plastic deformation is affected. Interestingly, an
elliptical particle shows only plastic deformation at the smaller half axis.
With the demonstrated efficiency of the applied methods, results after five
charging cycles are also discussed and can provide indications for the
performance of lithium-ion batteries in long term use.
</p>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05442" title="Abstract">arXiv:2310.05442</a> [<a href="/pdf/2310.05442" title="Download PDF">pdf</a>, <a href="/format/2310.05442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Establishing Trustworthiness: Rethinking Tasks and Model Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Litschko%2C+R">Robert Litschko</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller-Eberstein%2C+M">Max M&#xfc;ller-Eberstein</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Goot%2C+R">Rob van der Goot</a>, 
<a href="/search/cs?searchtype=author&query=Weber%2C+L">Leon Weber</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (Main Conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Language understanding is a multi-faceted cognitive capability, which the
Natural Language Processing (NLP) community has striven to model
computationally for decades. Traditionally, facets of linguistic intelligence
have been compartmentalized into tasks with specialized model architectures and
corresponding evaluation protocols. With the advent of large language models
(LLMs) the community has witnessed a dramatic shift towards general purpose,
task-agnostic approaches powered by generative models. As a consequence, the
traditional compartmentalized notion of language tasks is breaking down,
followed by an increasing challenge for evaluation and analysis. At the same
time, LLMs are being deployed in more real-world scenarios, including
previously unforeseen zero-shot setups, increasing the need for trustworthy and
reliable systems. Therefore, we argue that it is time to rethink what
constitutes tasks and model evaluation in NLP, and pursue a more holistic view
on language, placing trustworthiness at the center. Towards this goal, we
review existing compartmentalized approaches for understanding the origins of a
model's functional capacity, and provide recommendations for more multi-faceted
evaluation protocols.
</p>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05444" title="Abstract">arXiv:2310.05444</a> [<a href="/pdf/2310.05444" title="Download PDF">pdf</a>, <a href="/format/2310.05444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Waveform Design for MIMO-OFDM Integrated Sensing and Communication  System: An Information Theoretical Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhiqing Wei</a>, 
<a href="/search/cs?searchtype=author&query=Piao%2C+J">Jinghui Piao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Huici Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+A">J. Andrew Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhiyong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Ping Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Integrated sensing and communication (ISAC) is regarded as the enabling
technology in the future 5th-Generation-Advanced (5G-A) and 6th-Generation (6G)
mobile communication system. ISAC waveform design is critical in ISAC system.
However, the difference of the performance metrics between sensing and
communication brings challenges for the ISAC waveform design. This paper
applies the unified performance metrics in information theory, namely mutual
information (MI), to measure the communication and sensing performance in
multicarrier ISAC system. In multi-input multi-output orthogonal frequency
division multiplexing (MIMO-OFDM) ISAC system, we first derive the sensing and
communication MI with subcarrier correlation and spatial correlation. Then, we
propose optimal waveform designs for maximizing the sensing MI, communication
MI and the weighted sum of sensing and communication MI, respectively. The
optimization results are validated by Monte Carlo simulations. Our work
provides effective closed-form expressions for waveform design, enabling the
realization of MIMO-OFDM ISAC system with balanced performance in communication
and sensing.
</p>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05447" title="Abstract">arXiv:2310.05447</a> [<a href="/pdf/2310.05447" title="Download PDF">pdf</a>, <a href="/format/2310.05447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Fair and Comprehensive Comparisons for Image-Based 3D Object  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xinzhu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yongtao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yinmin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Z">Zhiyi Xia</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Y">Yuan Meng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhihui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haojie Li</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV23, code will be released soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this work, we build a modular-designed codebase, formulate strong training
recipes, design an error diagnosis toolbox, and discuss current methods for
image-based 3D object detection. In particular, different from other highly
mature tasks, e.g., 2D object detection, the community of image-based 3D object
detection is still evolving, where methods often adopt different training
recipes and tricks resulting in unfair evaluations and comparisons. What is
worse, these tricks may overwhelm their proposed designs in performance, even
leading to wrong conclusions. To address this issue, we build a module-designed
codebase and formulate unified training standards for the community.
Furthermore, we also design an error diagnosis toolbox to measure the detailed
characterization of detection models. Using these tools, we analyze current
methods in-depth under varying settings and provide discussions for some open
questions, e.g., discrepancies in conclusions on KITTI-3D and nuScenes
datasets, which have led to different dominant methods for these datasets. We
hope that this work will facilitate future research in image-based 3D object
detection. Our codes will be released at
\url{https://github.com/OpenGVLab/3dodi}
</p>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05450" title="Abstract">arXiv:2310.05450</a> [<a href="/pdf/2310.05450" title="Download PDF">pdf</a>, <a href="/format/2310.05450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empower Nested Boolean Logic via Self-Supervised Curriculum Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hongqiu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Linfeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Beyond the great cognitive powers showcased by language models, it is crucial
to scrutinize whether their reasoning capabilities stem from strong
generalization or merely exposure to relevant data. As opposed to constructing
increasingly complex logic, this paper probes into the boolean logic, the root
capability of a logical reasoner. We find that any pre-trained language models
even including large language models only behave like a random selector in the
face of multi-nested boolean logic, a task that humans can handle with ease. To
empower language models with this fundamental capability, this paper proposes a
new self-supervised learning method \textit{Curriculum Logical Reasoning}
(\textsc{Clr}), where we augment the training data with nested boolean logic
chain step-by-step, and program the training from simpler logical patterns
gradually to harder ones. This new training paradigm allows language models to
effectively generalize to much harder and longer-hop logic, which can hardly be
learned through naive training. Furthermore, we show that boolean logic is a
great foundation for improving the subsequent general logical tasks.
</p>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05452" title="Abstract">arXiv:2310.05452</a> [<a href="/pdf/2310.05452" title="Download PDF">pdf</a>, <a href="/format/2310.05452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining the Complex Task Reasoning of Large Language Models with  Template-Content Structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haotong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fanxu Meng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhouchen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muhan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The continuous evolution of pre-trained large language models with
ever-growing parameters and corpus sizes has augmented their capacity to solve
complex tasks. This ability, which obviates the necessity for task-specific
training or fine-tuning, relies on providing the model with a language
description or some task exemplars -- referred to the prompt -- that guide the
desired autoregressive generation. Despite the remarkable success, the
underlying mechanisms that facilitate such exceptional generalization abilities
remain an open question. In this paper, we present a novel framework that
formally conceptualizes answer generation for complex natural language tasks as
a hierarchical ``template-content'' structure. According to our modeling, there
exist pre-trained models that can automatically decompose tasks into
constituent steps during autoregressive generation, through language modeling
on a sufficiently large corpus, thereby solving them. Our framework offers an
explanatory tool for the complex reasoning abilities of large language models
from the perspective of modeling autoregressive generation tasks. Our
experiments show that practical models exhibit different behaviors for
``template'' and ``content'' providing support for our modeling.
</p>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05453" title="Abstract">arXiv:2310.05453</a> [<a href="/pdf/2310.05453" title="Download PDF">pdf</a>, <a href="/format/2310.05453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yuxiang Lai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinghong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Universal domain adaptation aims to align the classes and reduce the feature
gap between the same category of the source and target domains. The target
private category is set as the unknown class during the adaptation process, as
it is not included in the source domain. However, most existing methods
overlook the intra-class structure within a category, especially in cases where
there exists significant concept shift between the samples belonging to the
same category. When samples with large concept shift are forced to be pushed
together, it may negatively affect the adaptation performance. Moreover, from
the interpretability aspect, it is unreasonable to align visual features with
significant differences, such as fighter jets and civil aircraft, into the same
category. Unfortunately, due to such semantic ambiguity and annotation cost,
categories are not always classified in detail, making it difficult for the
model to perform precise adaptation. To address these issues, we propose a
novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the
differences between samples belonging to the same category and mine sub-classes
when there exists significant concept shift between them. By doing so, our
model learns a more reasonable feature space that enhances the transferability
and reflects the inherent differences among samples annotated as the same
category. We evaluate the effectiveness of our MemSPM method over multiple
scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art
performance on four benchmarks in most cases.
</p>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05456" title="Abstract">arXiv:2310.05456</a> [<a href="/pdf/2310.05456" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensemble-based Hybrid Optimization of Bayesian Neural Networks and  Traditional Machine Learning Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+P">Peiwen Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This research introduces a novel methodology for optimizing Bayesian Neural
Networks (BNNs) by synergistically integrating them with traditional machine
learning algorithms such as Random Forests (RF), Gradient Boosting (GB), and
Support Vector Machines (SVM). Feature integration solidifies these results by
emphasizing the second-order conditions for optimality, including stationarity
and positive definiteness of the Hessian matrix. Conversely, hyperparameter
tuning indicates a subdued impact in improving Expected Improvement (EI),
represented by EI(x). Overall, the ensemble method stands out as a robust,
algorithmically optimized approach.
</p>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05462" title="Abstract">arXiv:2310.05462</a> [<a href="/pdf/2310.05462" title="Download PDF">pdf</a>, <a href="/format/2310.05462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaFuse: Adaptive Medical Image Fusion Based on Spatial-Frequential  Cross Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+X">Xianming Gu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lihui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zeyu Deng</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Ying Cao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xingyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yue-min Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-modal medical image fusion is essential for the precise clinical
diagnosis and surgical navigation since it can merge the complementary
information in multi-modalities into a single image. The quality of the fused
image depends on the extracted single modality features as well as the fusion
rules for multi-modal information. Existing deep learning-based fusion methods
can fully exploit the semantic features of each modality, they cannot
distinguish the effective low and high frequency information of each modality
and fuse them adaptively. To address this issue, we propose AdaFuse, in which
multimodal image information is fused adaptively through frequency-guided
attention mechanism based on Fourier transform. Specifically, we propose the
cross-attention fusion (CAF) block, which adaptively fuses features of two
modalities in the spatial and frequency domains by exchanging key and query
values, and then calculates the cross-attention scores between the spatial and
frequency features to further guide the spatial-frequential information fusion.
The CAF block enhances the high-frequency features of the different modalities
so that the details in the fused images can be retained. Moreover, we design a
novel loss function composed of structure loss and content loss to preserve
both low and high frequency information. Extensive comparison experiments on
several datasets demonstrate that the proposed method outperforms
state-of-the-art methods in terms of both visual quality and quantitative
metrics. The ablation experiments also validate the effectiveness of the
proposed loss and fusion strategy. Our code is publicly available at
https://github.com/xianming-gu/AdaFuse.
</p>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05464" title="Abstract">arXiv:2310.05464</a> [<a href="/pdf/2310.05464" title="Download PDF">pdf</a>, <a href="/format/2310.05464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cost-Sensitive Best Subset Selection for Logistic Regression: A  Mixed-Integer Conic Optimization Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Knauer%2C+R">Ricardo Knauer</a>, 
<a href="/search/cs?searchtype=author&query=Rodner%2C+E">Erik Rodner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> German Conference on Artificial Intelligence (K\"unstliche Intelligenz)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A key challenge in machine learning is to design interpretable models that
can reduce their inputs to the best subset for making transparent predictions,
especially in the clinical domain. In this work, we propose a certifiably
optimal feature selection procedure for logistic regression from a
mixed-integer conic optimization perspective that can take an auxiliary cost to
obtain features into account. Based on an extensive review of the literature,
we carefully create a synthetic dataset generator for clinical prognostic model
research. This allows us to systematically evaluate different heuristic and
optimal cardinality- and budget-constrained feature selection procedures. The
analysis shows key limitations of the methods for the low-data regime and when
confronted with label noise. Our paper not only provides empirical
recommendations for suitable methods and dataset designs, but also paves the
way for future research in the area of meta-learning.
</p>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05467" title="Abstract">arXiv:2310.05467</a> [<a href="/pdf/2310.05467" title="Download PDF">pdf</a>, <a href="/format/2310.05467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Convolutional Explorer Helps Understand 1D-CNN&#x27;s Learning  Behavior in Time Series Classification from Frequency Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Lang Feng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yang He</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuhan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yabo Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as CIKM'23 Long Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">While one-dimensional convolutional neural networks (1D-CNNs) have been
empirically proven effective in time series classification tasks, we find that
there remain undesirable outcomes that could arise in their application,
motivating us to further investigate and understand their underlying
mechanisms. In this work, we propose a Temporal Convolutional Explorer (TCE) to
empirically explore the learning behavior of 1D-CNNs from the perspective of
the frequency domain. Our TCE analysis highlights that deeper 1D-CNNs tend to
distract the focus from the low-frequency components leading to the accuracy
degradation phenomenon, and the disturbing convolution is the driving factor.
Then, we leverage our findings to the practical application and propose a
regulatory framework, which can easily be integrated into existing 1D-CNNs. It
aims to rectify the suboptimal learning behavior by enabling the network to
selectively bypass the specified disturbing convolutions. Finally, through
comprehensive experiments on widely-used UCR, UEA, and UCI benchmarks, we
demonstrate that 1) TCE's insight into 1D-CNN's learning behavior; 2) our
regulatory framework enables state-of-the-art 1D-CNNs to get improved
performances with less consumption of memory and computational overhead.
</p>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05469" title="Abstract">arXiv:2310.05469</a> [<a href="/pdf/2310.05469" title="Download PDF">pdf</a>, <a href="/format/2310.05469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vibroacoustic Frequency Response Prediction with Query-based Operator  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Delden%2C+J">Jan van Delden</a>, 
<a href="/search/cs?searchtype=author&query=Schultz%2C+J">Julius Schultz</a>, 
<a href="/search/cs?searchtype=author&query=Blech%2C+C">Christopher Blech</a>, 
<a href="/search/cs?searchtype=author&query=Langer%2C+S+C">Sabine C. Langer</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%BCddecke%2C+T">Timo L&#xfc;ddecke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Understanding vibroacoustic wave propagation in mechanical structures like
airplanes, cars and houses is crucial to ensure health and comfort of their
users. To analyze such systems, designers and engineers primarily consider the
dynamic response in the frequency domain, which is computed through expensive
numerical simulations like the finite element method. In contrast, data-driven
surrogate models offer the promise of speeding up these simulations, thereby
facilitating tasks like design optimization, uncertainty quantification, and
design space exploration. We present a structured benchmark for a
representative vibroacoustic problem: Predicting the frequency response for
vibrating plates with varying forms of beadings. The benchmark features a total
of 12,000 plate geometries with an associated numerical solution and introduces
evaluation metrics to quantify the prediction quality. To address the frequency
response prediction task, we propose a novel frequency query operator model,
which is trained to map plate geometries to frequency response functions. By
integrating principles from operator learning and implicit models for shape
encoding, our approach effectively addresses the prediction of resonance peaks
of frequency responses. We evaluate the method on our vibrating-plates
benchmark and find that it outperforms DeepONets, Fourier Neural Operators and
more traditional neural network architectures. The code and dataset are
available from https://eckerlab.org/code/delden2023_plate.
</p>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05470" title="Abstract">arXiv:2310.05470</a> [<a href="/pdf/2310.05470" title="Download PDF">pdf</a>, <a href="/format/2310.05470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Judge for Evaluating Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shichao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+W">Weizhe Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+R">Run-Ze Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengfei Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The rapid development of Large Language Models (LLMs) has substantially
expanded the range of tasks they can address. In the field of Natural Language
Processing (NLP), researchers have shifted their focus from conventional NLP
tasks (e.g., sequence tagging and parsing) towards tasks that revolve around
aligning with human needs (e.g., brainstorming and email writing). This shift
in task distribution imposes new requirements on evaluating these aligned
models regarding generality (i.e., assessing performance across diverse
scenarios), flexibility (i.e., examining under different protocols), and
interpretability (i.e., scrutinizing models with explanations). In this paper,
we propose a generative judge with 13B parameters, Auto-J, designed to address
these challenges. Our model is trained on user queries and LLM-generated
responses under massive real-world scenarios and accommodates diverse
evaluation protocols (e.g., pairwise response comparison and single-response
evaluation) with well-structured natural language critiques. To demonstrate the
efficacy of our approach, we construct a new testbed covering 58 different
scenarios. Experimentally, Auto-J outperforms a series of strong competitors,
including both open-source and closed-source models, by a large margin. We also
provide detailed analysis and case studies to further reveal the potential of
our method and make a variety of resources public at
https://github.com/GAIR-NLP/auto-j.
</p>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05471" title="Abstract">arXiv:2310.05471</a> [<a href="/pdf/2310.05471" title="Download PDF">pdf</a>, <a href="/format/2310.05471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Drawn Tree Decomposition: New Approach for Graph Drawing Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Siddharth Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Sa%27ar%2C+G">Guy Sa&#x27;ar</a>, 
<a href="/search/cs?searchtype=author&query=Zehavi%2C+M">Meirav Zehavi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A preliminary version of this paper will appear in the Proceedings of IPEC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">Over the past decade, we witness an increasing amount of interest in the
design of exact exponential-time and parameterized algorithms for problems in
Graph Drawing. Unfortunately, we still lack knowledge of general methods to
develop such algorithms. An even more serious issue is that, here, "standard"
parameters very often yield intractability. In particular, for the most common
structural parameter, namely, treewidth, we frequently observe NP-hardness
already when the input graphs are restricted to have constant (often, being
just $1$ or $2$) treewidth.
<br />Our work deals with both drawbacks simultaneously. We introduce a novel form
of tree decomposition that, roughly speaking, does not decompose (only) a
graph, but an entire drawing. As such, its bags and separators are of geometric
(rather than only combinatorial) nature. While the corresponding parameter --
like treewidth -- can be arbitrarily smaller than the height (and width) of the
drawing, we show that -- unlike treewidth -- it gives rise to efficient
algorithms. Specifically, we get slice-wise polynomial (XP) time algorithms
parameterized by our parameter. We present a general scheme for the design of
such algorithms, and apply it to several central problems in Graph Drawing,
including the recognition of grid graphs, minimization of crossings and bends,
and compaction. Other than for the class of problems we discussed in the paper,
we believe that our decomposition and scheme are of independent interest and
can be further extended or generalized to suit even a wider class of problems.
Additionally, we discuss classes of drawings where our parameter is bounded by
$O(\sqrt{n})$ (where $n$ is the number of vertices of the graph), yielding
subexponential-time algorithms. Lastly, we prove which relations exist between
drawn treewidth and other width measures, including treewidth, pathwidth,
(dual) carving-width and embedded-width.
</p>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05472" title="Abstract">arXiv:2310.05472</a> [<a href="/pdf/2310.05472" title="Download PDF">pdf</a>, <a href="/format/2310.05472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Tutoring System: Experience of Linking Software Engineering  and Programming Teaching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zhiyu Fan</a>, 
<a href="/search/cs?searchtype=author&query=Noller%2C+Y">Yannic Noller</a>, 
<a href="/search/cs?searchtype=author&query=Dandekar%2C+A">Ashish Dandekar</a>, 
<a href="/search/cs?searchtype=author&query=Roychoudhury%2C+A">Abhik Roychoudhury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">The increasing number of computer science students pushes lecturers and
tutors of first-year programming courses to their limits to provide
high-quality feedback to the students. Existing systems that handle automated
grading primarily focus on the automation of test case executions in the
context of programming assignments. However, they cannot provide customized
feedback about the students' errors, and hence, cannot replace the help of
tutors. While recent research works in the area of automated grading and
feedback generation address this issue by using automated repair techniques, so
far, to the best of our knowledge, there has been no real-world deployment of
such techniques. Based on the research advances in recent years, we have built
an intelligent tutoring system that has the capability of providing automated
feedback and grading. Furthermore, we designed a Software Engineering course
that guides third-year undergraduate students in incrementally developing such
a system over the coming years. Each year, students will make contributions
that improve the current implementation, while at the same time, we can deploy
the current system for usage by first year students. This paper describes our
teaching concept, the intelligent tutoring system architecture, and our
experience with the stakeholders. This software engineering project for the
students has the key advantage that the users of the system are available
in-house (i.e., students, tutors, and lecturers from the first-year programming
courses). This helps organize requirements engineering sessions and builds
awareness about their contribution to a "to be deployed" software project. In
this multi-year teaching effort, we have incrementally built a tutoring system
that can be used in first-year programming courses. Further, it represents a
platform that can integrate the latest research results in APR for education.
</p>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05473" title="Abstract">arXiv:2310.05473</a> [<a href="/pdf/2310.05473" title="Download PDF">pdf</a>, <a href="/format/2310.05473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sentence-level Prompts Benefit Composed Image Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xinxing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S">Salman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+F">Fahad Khan</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+W">Wangmeng Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Goh%2C+R+S+M">Rick Siow Mong Goh</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chun-Mei Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Composed image retrieval (CIR) is the task of retrieving specific images by
using a query that involves both a reference image and a relative caption. Most
existing CIR models adopt the late-fusion strategy to combine visual and
language features. Besides, several approaches have also been suggested to
generate a pseudo-word token from the reference image, which is further
integrated into the relative caption for CIR. However, these pseudo-word-based
prompting methods have limitations when target image encompasses complex
changes on reference image, e.g., object removal and attribute modification. In
this work, we demonstrate that learning an appropriate sentence-level prompt
for the relative caption (SPRC) is sufficient for achieving effective composed
image retrieval. Instead of relying on pseudo-word-based prompts, we propose to
leverage pretrained V-L models, e.g., BLIP-2, to generate sentence-level
prompts. By concatenating the learned sentence-level prompt with the relative
caption, one can readily use existing text-based image retrieval models to
enhance CIR performance. Furthermore, we introduce both image-text contrastive
loss and text prompt alignment loss to enforce the learning of suitable
sentence-level prompts. Experiments show that our proposed method performs
favorably against the state-of-the-art CIR methods on the Fashion-IQ and CIRR
datasets. The source code and pretrained model are publicly available at
https://github.com/chunmeifeng/SPRC
</p>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05476" title="Abstract">arXiv:2310.05476</a> [<a href="/pdf/2310.05476" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revitalizing education through ict: a short overview of japan&#x27;s current  landscape
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fujita%2C+T">Takaaki Fujita</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> European Journal of Social Sciences Studies, 8(5). (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">The domain of Information and Communication Technology (ICT) education has
garnered significant consideration in recent times. However, several challenges
are inherent to this area of study, including monetary expense, temporal
factors, pedagogical environment, teacher training programs, incentive,
syllabus design, and health-related concerns. This paper presents an analysis
of the difficulties encountered in the realm of ICT education in Japan, taking
into account ten different perspectives. A peer-reviewed article of this
Preprint also exists "Fujita, T. (2023). REVITALIZING EDUCATION THROUGH ICT: A
SHORT OVERVIEW OF JAPAN'S CURRENT LANDSCAPE. European Journal of Social
Sciences Studies, 8(5)."
</p>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05479" title="Abstract">arXiv:2310.05479</a> [<a href="/pdf/2310.05479" title="Download PDF">pdf</a>, <a href="/format/2310.05479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Optimal Timing Strategies for Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Chen Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xuanwei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinxin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+W">Wenxin Ning</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zi Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+S">Siqiao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">James Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yunhua Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICDM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Deciding the best future execution time is a critical task in many business
activities while evolving time series forecasting, and optimal timing strategy
provides such a solution, which is driven by observed data. This solution has
plenty of valuable applications to reduce the operation costs. In this paper,
we propose a mechanism that combines a probabilistic time series forecasting
task and an optimal timing decision task as a first systematic attempt to
tackle these practical problems with both solid theoretical foundation and
real-world flexibility. Specifically, it generates the future paths of the
underlying time series via probabilistic forecasting algorithms, which does not
need a sophisticated mathematical dynamic model relying on strong prior
knowledge as most other common practices. In order to find the optimal
execution time, we formulate the decision task as an optimal stopping problem,
and employ a recurrent neural network structure (RNN) to approximate the
optimal times. Github repository:
\url{github.com/ChenPopper/optimal_timing_TSF}.
</p>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05480" title="Abstract">arXiv:2310.05480</a> [<a href="/pdf/2310.05480" title="Download PDF">pdf</a>, <a href="/format/2310.05480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collective Graph Exploration Parameterized by Vertex Cover
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Siddharth Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Sa%27ar%2C+G">Guy Sa&#x27;ar</a>, 
<a href="/search/cs?searchtype=author&query=Zehavi%2C+M">Meirav Zehavi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A preliminary version of this paper will appear in the Proceedings of IPEC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We initiate the study of the parameterized complexity of the {\sc Collective
Graph Exploration} ({\sc CGE}) problem. In {\sc CGE}, the input consists of an
undirected connected graph $G$ and a collection of $k$ robots, initially placed
at the same vertex $r$ of $G$, and each one of them has an energy budget of
$B$. The objective is to decide whether $G$ can be \emph{explored} by the $k$
robots in $B$ time steps, i.e., there exist $k$ closed walks in $G$, one
corresponding to each robot, such that every edge is covered by at least one
walk, every walk starts and ends at the vertex $r$, and the maximum length of
any walk is at most $B$. Unfortunately, this problem is \textsf{NP}-hard even
on trees [Fraigniaud {\em et~al.}, 2006]. Further, we prove that the problem
remains \textsf{W[1]}-hard parameterized by $k$ even for trees of treedepth
$3$. Due to the \textsf{para-NP}-hardness of the problem parameterized by
treedepth, and motivated by real-world scenarios, we study the parameterized
complexity of the problem parameterized by the vertex cover number
($\mathsf{vc}$) of the graph, and prove that the problem is fixed-parameter
tractable (\textsf{FPT}) parameterized by $\mathsf{vc}$. Additionally, we study
the optimization version of {\sc CGE}, where we want to optimize $B$, and
design an approximation algorithm with an additive approximation factor of
$O(\mathsf{vc})$.
</p>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05481" title="Abstract">arXiv:2310.05481</a> [<a href="/pdf/2310.05481" title="Download PDF">pdf</a>, <a href="/format/2310.05481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cabbage Sweeter than Cake? Analysing the Potential of Large Language  Models for Learning Conceptual Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+U">Usashi Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Gajbhiye%2C+A">Amit Gajbhiye</a>, 
<a href="/search/cs?searchtype=author&query=Schockaert%2C+S">Steven Schockaert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The theory of Conceptual Spaces is an influential cognitive-linguistic
framework for representing the meaning of concepts. Conceptual spaces are
constructed from a set of quality dimensions, which essentially correspond to
primitive perceptual features (e.g. hue or size). These quality dimensions are
usually learned from human judgements, which means that applications of
conceptual spaces tend to be limited to narrow domains (e.g. modelling colour
or taste). Encouraged by recent findings about the ability of Large Language
Models (LLMs) to learn perceptually grounded representations, we explore the
potential of such models for learning conceptual spaces. Our experiments show
that LLMs can indeed be used for learning meaningful representations to some
extent. However, we also find that fine-tuned models of the BERT family are
able to match or even outperform the largest GPT-3 model, despite being 2 to 3
orders of magnitude smaller.
</p>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05483" title="Abstract">arXiv:2310.05483</a> [<a href="/pdf/2310.05483" title="Download PDF">pdf</a>, <a href="/format/2310.05483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometry-Guided Ray Augmentation for Neural Surface Reconstruction with  Sparse Views
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiawei Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chuming Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we propose a novel method for 3D scene and object
reconstruction from sparse multi-view images. Different from previous methods
that leverage extra information such as depth or generalizable features across
scenes, our approach leverages the scene properties embedded in the multi-view
inputs to create precise pseudo-labels for optimization without any prior
training. Specifically, we introduce a geometry-guided approach that improves
surface reconstruction accuracy from sparse views by leveraging spherical
harmonics to predict the novel radiance while holistically considering all
color observations for a point in the scene. Also, our pipeline exploits proxy
geometry and correctly handles the occlusion in generating the pseudo-labels of
radiance, which previous image-warping methods fail to avoid. Our method,
dubbed Ray Augmentation (RayAug), achieves superior results on DTU and Blender
datasets without requiring prior training, demonstrating its effectiveness in
addressing the problem of sparse view reconstruction. Our pipeline is flexible
and can be integrated into other implicit neural reconstruction methods for
sparse views.
</p>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05484" title="Abstract">arXiv:2310.05484</a> [<a href="/pdf/2310.05484" title="Download PDF">pdf</a>, <a href="/format/2310.05484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IDTraffickers: An Authorship Attribution Dataset to link and connect  Potential Human-Trafficking Operations on Text Escort Advertisements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saxena%2C+V">Vageesh Saxena</a>, 
<a href="/search/cs?searchtype=author&query=Bashpole%2C+B">Benjamin Bashpole</a>, 
<a href="/search/cs?searchtype=author&query=Van+Dijck%2C+G">Gijs Van Dijck</a>, 
<a href="/search/cs?searchtype=author&query=Spanakis%2C+G">Gerasimos Spanakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Human trafficking (HT) is a pervasive global issue affecting vulnerable
individuals, violating their fundamental human rights. Investigations reveal
that a significant number of HT cases are associated with online advertisements
(ads), particularly in escort markets. Consequently, identifying and connecting
HT vendors has become increasingly challenging for Law Enforcement Agencies
(LEAs). To address this issue, we introduce IDTraffickers, an extensive dataset
consisting of 87,595 text ads and 5,244 vendor labels to enable the
verification and identification of potential HT vendors on online escort
markets. To establish a benchmark for authorship identification, we train a
DeCLUTR-small model, achieving a macro-F1 score of 0.8656 in a closed-set
classification environment. Next, we leverage the style representations
extracted from the trained classifier to conduct authorship verification,
resulting in a mean r-precision score of 0.8852 in an open-set ranking
environment. Finally, to encourage further research and ensure responsible data
sharing, we plan to release IDTraffickers for the authorship attribution task
to researchers under specific conditions, considering the sensitive nature of
the data. We believe that the availability of our dataset and benchmarks will
empower future researchers to utilize our findings, thereby facilitating the
effective linkage of escort ads and the development of more robust approaches
for identifying HT indicators.
</p>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05485" title="Abstract">arXiv:2310.05485</a> [<a href="/pdf/2310.05485" title="Download PDF">pdf</a>, <a href="/format/2310.05485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integration-free Training for Spatio-temporal Multimodal Covariate Deep  Kernel Point Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Q">Quyu Kong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Feng Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In this study, we propose a novel deep spatio-temporal point process model,
Deep Kernel Mixture Point Processes (DKMPP), that incorporates multimodal
covariate information. DKMPP is an enhanced version of Deep Mixture Point
Processes (DMPP), which uses a more flexible deep kernel to model complex
relationships between events and covariate data, improving the model's
expressiveness. To address the intractable training procedure of DKMPP due to
the non-integrable deep kernel, we utilize an integration-free method based on
score matching, and further improve efficiency by adopting a scalable denoising
score matching method. Our experiments demonstrate that DKMPP and its
corresponding score-based estimators outperform baseline models, showcasing the
advantages of incorporating covariate information, utilizing a deep kernel, and
employing score-based estimators.
</p>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05489" title="Abstract">arXiv:2310.05489</a> [<a href="/pdf/2310.05489" title="Download PDF">pdf</a>, <a href="/ps/2310.05489" title="Download PostScript">ps</a>, <a href="/format/2310.05489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Some extensions of the $&#x3c6;$-divergence moment closures for the  radiative transfer equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Abdelmalik%2C+M+R+A">Micheal R A Abdelmalik</a> (TU/e), 
<a href="/search/math?searchtype=author&query=Cai%2C+Z">Zhenning Cai</a> (NUS), 
<a href="/search/math?searchtype=author&query=Pichard%2C+T">Teddy Pichard</a> (X)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The $\phi$-divergence-based moment method was recently introduced Abdelmalik
et al. (2023) for the discretization of the radiative transfer equation. At the
continuous level, this method is very close to the entropy-based MN methods and
possesses its main properties, i.e. entropy dissipation, rotational invariance
and energy conservation. However, the $\phi$-divergence based moment systems
are easier to resolve numerically due to the improved conditioning of the
discrete equations. Moreover, exact quadrature rules can be used to compute
moments of the distribution function, which enables the preservation of energy
conservation, entropy dissipation and rotational invariants, discretely. In
this paper we consider different variants of the $\phi$-divergence closures
that are based on different approximations of the exponential function and the
Planck function. We compare the approximation properties of the proposed
closures in the numerical benchmarks.
</p>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05492" title="Abstract">arXiv:2310.05492</a> [<a href="/pdf/2310.05492" title="Download PDF">pdf</a>, <a href="/format/2310.05492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Abilities in Large Language Models are Affected by Supervised  Fine-tuning Data Composition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Hongyi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+K">Keming Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengpeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+M">Mingfeng Xue</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dayiheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) with enormous pre-training tokens and parameter
amounts emerge abilities, including math reasoning, code generation, and
instruction following. These abilities are further enhanced by supervised
fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each
ability, while proprietary LLMs are versatile for all abilities. It is
important to investigate how to unlock them with multiple abilities via SFT. In
this study, we specifically focus on the data composition between mathematical
reasoning, code generation, and general human-aligning abilities during SFT.
From a scaling perspective, we investigate the relationship between model
abilities and various factors including data amounts, data composition ratio,
model parameters, and SFT strategies. Our experiments reveal that different
abilities exhibit different scaling patterns, and larger models generally show
superior performance with the same amount of data. Mathematical reasoning and
code generation improve as data amounts increase consistently, while the
general ability is enhanced with about a thousand samples and improves slowly.
We find data composition results in various abilities improvements with low
data amounts, while conflicts of abilities with high data amounts. Our
experiments further show that composition data amount impacts performance,
while the influence of composition ratio is insignificant. Regarding the SFT
strategies, we evaluate sequential learning multiple abilities are prone to
catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT)
strategy learns specialized abilities first and then learns general abilities
with a small amount of specialized data to prevent forgetting, offering a
promising solution to learn multiple abilities with different scaling patterns.
</p>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05493" title="Abstract">arXiv:2310.05493</a> [<a href="/pdf/2310.05493" title="Download PDF">pdf</a>, <a href="/format/2310.05493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Adaptable IoT Rule Engine Framework for Dataflow Monitoring and  Control Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Ken Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages,10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The monitoring of data generated by a large number of devices in Internet of
Things (IoT) systems is an important and complex issue. Several studies have
explored the use of generic rule engine, primarily based on the RETE algorithm,
for monitoring the flow of device data. In order to solve the performance
problem of the RETE algorithm in IoT scenarios, some studies have also proposed
improved RETE algorithms. However, implementing modifications to the general
rule engine remains challenges in practical applications. The Thingsboard
open-source platform introduces an IoT-specific rule engine that does not rely
on the RETE algorithm. Its interactive mode attracted attention from developers
and researchers. However, the close integration between its rule module and the
platform, as well as the difficulty in formulating rules for multiple devices,
limits its flexibility. This paper presents an adaptable and user-friendly rule
engine framework for monitoring and control IoT device data flows. The
framework is easily extensible and allows for the formulation of rules contain
multiple devices. We designed a Domain-Specific Language (DSL) for rule
description. A prototype system of this framework was implemented to verify the
validity of theoretical method. The framework has potential to be adaptable to
a wide range of IoT scenarios and is especially effective in where real-time
control demands are not as strict.
</p>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05494" title="Abstract">arXiv:2310.05494</a> [<a href="/pdf/2310.05494" title="Download PDF">pdf</a>, <a href="/format/2310.05494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding a Minimum Spanning Tree with a Small Non-Terminal Set
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanaka%2C+T">Tesshu Hanaka</a>, 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+Y">Yasuaki Kobayashi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC); Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">In this paper, we study the problem of finding a minimum weight spanning tree
that contains each vertex in a given subset $V_{\rm NT}$ of vertices as an
internal vertex. This problem, called Minimum Weight Non-Terminal Spanning
Tree, includes $s$-$t$ Hamiltonian Path as a special case, and hence it is
NP-hard. In this paper, we first observe that Non-Terminal Spanning Tree, the
unweighted counterpart of Minimum Weight Non-Terminal Spanning Tree, is already
NP-hard on some special graph classes. Moreover, it is W[1]-hard when
parameterized by clique-width. In contrast, we give a $3k$-vertex kernel and
$O^*(2^k)$-time algorithm, where $k$ is the size of non-terminal set $V_{\rm
NT}$. The latter algorithm can be extended to Minimum Weight Non-Terminal
Spanning Tree with the restriction that each edge has a polynomially bounded
integral weight. We also show that Minimum Weight Non-Terminal Spanning Tree is
fixed-parameter tractable parameterized by the number of edges in the subgraph
induced by the non-terminal set $V_{\rm NT}$, extending the fixed-parameter
tractability of Minimum Weight Non-Terminal Spanning Tree to the general case.
Finally, we give several results for structural parameterization.
</p>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05495" title="Abstract">arXiv:2310.05495</a> [<a href="/pdf/2310.05495" title="Download PDF">pdf</a>, <a href="/format/2310.05495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Neural Tangent Kernel View on Federated Averaging for Deep Linear  Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+D">Dazhi Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+W">Wei Tao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yu Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zhisong Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Federated averaging (FedAvg) is a widely employed paradigm for
collaboratively training models from distributed clients without sharing data.
Nowadays, the neural network has achieved remarkable success due to its
extraordinary performance, which makes it a preferred choice as the model in
FedAvg. However, the optimization problem of the neural network is often
non-convex even non-smooth. Furthermore, FedAvg always involves multiple
clients and local updates, which results in an inaccurate updating direction.
These properties bring difficulties in analyzing the convergence of FedAvg in
training neural networks. Recently, neural tangent kernel (NTK) theory has been
proposed towards understanding the convergence of first-order methods in
tackling the non-convex problem of neural networks. The deep linear neural
network is a classical model in theoretical subject due to its simple
formulation. Nevertheless, there exists no theoretical result for the
convergence of FedAvg in training the deep linear neural network. By applying
NTK theory, we make a further step to provide the first theoretical guarantee
for the global convergence of FedAvg in training deep linear neural networks.
Specifically, we prove FedAvg converges to the global minimum at a linear rate
$\mathcal{O}\big((1-\eta K /N)^t\big)$, where $t$ is the number of iterations,
$\eta$ is the learning rate, $N$ is the number of clients and $K$ is the number
of local updates. Finally, experimental evaluations on two benchmark datasets
are conducted to empirically validate the correctness of our theoretical
findings.
</p>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05498" title="Abstract">arXiv:2310.05498</a> [<a href="/pdf/2310.05498" title="Download PDF">pdf</a>, <a href="/format/2310.05498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Supervised Object Detection with Uncurated Unlabeled Data for  Remote Sensing Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Nanqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yingjie Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Heng-Chao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Annotating remote sensing images (RSIs) presents a notable challenge due to
its labor-intensive nature. Semi-supervised object detection (SSOD) methods
tackle this issue by generating pseudo-labels for the unlabeled data, assuming
that all classes found in the unlabeled dataset are also represented in the
labeled data. However, real-world situations introduce the possibility of
out-of-distribution (OOD) samples being mixed with in-distribution (ID) samples
within the unlabeled dataset. In this paper, we delve into techniques for
conducting SSOD directly on uncurated unlabeled data, which is termed Open-Set
Semi-Supervised Object Detection (OSSOD). Our approach commences by employing
labeled in-distribution data to dynamically construct a class-wise feature bank
(CFB) that captures features specific to each class. Subsequently, we compare
the features of predicted object bounding boxes with the corresponding entries
in the CFB to calculate OOD scores. We design an adaptive threshold based on
the statistical properties of the CFB, allowing us to filter out OOD samples
effectively. The effectiveness of our proposed method is substantiated through
extensive experiments on two widely used remote sensing object detection
datasets: DIOR and DOTA. These experiments showcase the superior performance
and efficacy of our approach for OSSOD on RSIs.
</p>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05499" title="Abstract">arXiv:2310.05499</a> [<a href="/pdf/2310.05499" title="Download PDF">pdf</a>, <a href="/format/2310.05499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Graphs with Large Language Models: Methods and Prospects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yizhen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Large language models (LLMs) such as GPT-4 have emerged as frontrunners,
showcasing unparalleled prowess in diverse applications, including answering
queries, code generation, and more. Parallelly, graph-structured data, an
intrinsic data type, is pervasive in real-world scenarios. Merging the
capabilities of LLMs with graph-structured data has been a topic of keen
interest. This paper bifurcates such integrations into two predominant
categories. The first leverages LLMs for graph learning, where LLMs can not
only augment existing graph algorithms but also stand as prediction models for
various graph tasks. Conversely, the second category underscores the pivotal
role of graphs in advancing LLMs. Mirroring human cognition, we solve complex
tasks by adopting graphs in either reasoning or collaboration. Integrating with
such structures can significantly boost the performance of LLMs in various
complicated tasks. We also discuss and propose open questions for integrating
LLMs with graph-structured data for the future direction of the field.
</p>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05502" title="Abstract">arXiv:2310.05502</a> [<a href="/pdf/2310.05502" title="Download PDF">pdf</a>, <a href="/format/2310.05502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XAL: EXplainable Active Learning Makes Classifiers Better Low-resource  Learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fandong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+F">Fang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Q">Qinglin Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Active learning aims to construct an effective training set by iteratively
curating the most informative unlabeled data for annotation, which is practical
in low-resource tasks. Most active learning techniques in classification rely
on the model's uncertainty or disagreement to choose unlabeled data. However,
previous work indicates that existing models are poor at quantifying predictive
uncertainty, which can lead to over-confidence in superficial patterns and a
lack of exploration. Inspired by the cognitive processes in which humans deduce
and predict through causal information, we propose a novel Explainable Active
Learning framework (XAL) for low-resource text classification, which aims to
encourage classifiers to justify their inferences and delve into unlabeled data
for which they cannot provide reasonable explanations. Specifically, besides
using a pre-trained bi-directional encoder for classification, we employ a
pre-trained uni-directional decoder to generate and score the explanation. A
ranking loss is proposed to enhance the decoder's capability in scoring
explanations. During the selection of unlabeled data, we combine the predictive
uncertainty of the encoder and the explanation score of the decoder to acquire
informative data for annotation.
<br />As XAL is a general framework for text classification, we test our methods on
six different classification tasks. Extensive experiments show that XAL
achieves substantial improvement on all six tasks over previous AL methods.
Ablation studies demonstrate the effectiveness of each component, and human
evaluation shows that the model trained in XAL performs surprisingly well in
explaining its prediction.
</p>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05504" title="Abstract">arXiv:2310.05504</a> [<a href="/pdf/2310.05504" title="Download PDF">pdf</a>, <a href="/format/2310.05504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud  Registration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+C">Chunge Bai</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+R">Ruijie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiang Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">State-of-the-art techniques for monocular camera reconstruction predominantly
rely on the Structure from Motion (SfM) pipeline. However, such methods often
yield reconstruction outcomes that lack crucial scale information, and over
time, accumulation of images leads to inevitable drift issues. In contrast,
mapping methods based on LiDAR scans are popular in large-scale urban scene
reconstruction due to their precise distance measurements, a capability
fundamentally absent in visual-based approaches. Researchers have made attempts
to utilize concurrent LiDAR and camera measurements in pursuit of precise
scaling and color details within mapping outcomes. However, the outcomes are
subject to extrinsic calibration and time synchronization precision. In this
paper, we propose a novel cost-effective reconstruction pipeline that utilizes
a pre-established LiDAR map as a fixed constraint to effectively address the
inherent scale challenges present in monocular camera reconstruction. To our
knowledge, our method is the first to register images onto the point cloud map
without requiring synchronous capture of camera and LiDAR data, granting us the
flexibility to manage reconstruction detail levels across various areas of
interest. To facilitate further research in this domain, we have released
Colmap-PCD${^{3}}$, an open-source tool leveraging the Colmap algorithm, that
enables precise fine-scale registration of images to the point cloud map.
</p>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05505" title="Abstract">arXiv:2310.05505</a> [<a href="/pdf/2310.05505" title="Download PDF">pdf</a>, <a href="/ps/2310.05505" title="Download PostScript">ps</a>, <a href="/format/2310.05505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AbCD: A Component-wise Adjustable Framework for Dynamic Optimization  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mascarenhas%2C+A">Alexandre Mascarenhas</a>, 
<a href="/search/cs?searchtype=author&query=Lavinas%2C+Y">Yuri Lavinas</a>, 
<a href="/search/cs?searchtype=author&query=Aranha%2C+C">Claus Aranha</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of the Companion Conference on Genetic and
  Evolutionary Computation (GECCO '23 Companion). Association for Computing
  Machinery, New York, NY, USA, (2023) 503-506
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Dynamic Optimization Problems (DOPs) are characterized by changes in the
fitness landscape that can occur at any time and are common in real world
applications. The main issues to be considered include detecting the change in
the fitness landscape and reacting in accord. Over the years, several
evolutionary algorithms have been proposed to take into account this
characteristic during the optimization process. However, the number of
available tools or open source codebases for these approaches is limited,
making reproducibility and extensive experimentation difficult. To solve this,
we developed a component-oriented framework for DOPs called Adjustable
Components for Dynamic Problems (AbCD), inspired by similar works in the
Multiobjective static domain. Using this framework, we investigate components
that were proposed in several popular DOP algorithms. Our experiments show that
the performance of these components depends on the problem and the selected
components used in a configuration, which differs from the results reported in
the literature. Using irace, we demonstrate how this framework can
automatically generate DOP algorithm configurations that take into account the
characteristics of the problem to be solved. Our results highlight existing
problems in the DOP field that need to be addressed in the future development
of algorithms and components.
</p>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05506" title="Abstract">arXiv:2310.05506</a> [<a href="/pdf/2310.05506" title="Download PDF">pdf</a>, <a href="/format/2310.05506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengpeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+K">Keming Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiancan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Chuanqi Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chang Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In math reasoning with large language models (LLMs), fine-tuning data
augmentation by query evolution and diverse reasoning paths is empirically
verified effective, profoundly narrowing the gap between open-sourced LLMs and
cutting-edge proprietary LLMs. In this paper, we conduct an investigation for
such data augmentation in math reasoning and are intended to answer: (1) What
strategies of data augmentation are more effective; (2) What is the scaling
relationship between the amount of augmented data and model performance; and
(3) Can data augmentation incentivize generalization to out-of-domain
mathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K,
by complicating and diversifying the queries from GSM8K and sampling multiple
reasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning
on subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art
on GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the
scale of 13B). A log-linear relationship is presented between MuggleMath's
performance and the amount of augmented data. We also find that MuggleMath is
weak in out-of-domain math reasoning generalization to MATH. This is attributed
to the differences in query distribution between AugGSM8K and MATH which
suggest that augmentation on a single benchmark could not help with overall
math reasoning performance. Codes and AugGSM8K will be uploaded to
https://github.com/OFA-Sys/gsm8k-ScRel.
</p>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05507" title="Abstract">arXiv:2310.05507</a> [<a href="/pdf/2310.05507" title="Download PDF">pdf</a>, <a href="/format/2310.05507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MEDUSA: Scalable Biometric Sensing in the Wild through Distributed MIMO  Radars
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yilong Li</a>, 
<a href="/search/cs?searchtype=author&query=Sheshadri%2C+R+K">Ramanujan K Sheshadri</a>, 
<a href="/search/cs?searchtype=author&query=Sundaresan%2C+K">Karthik Sundaresan</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+E">Eugene Chai</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Suman Banerjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Radar-based techniques for detecting vital signs have shown promise for
continuous contactless vital sign sensing and healthcare applications. However,
real-world indoor environments face significant challenges for existing vital
sign monitoring systems. These include signal blockage in non-line-of-sight
(NLOS) situations, movement of human subjects, and alterations in location and
orientation. Additionally, these existing systems failed to address the
challenge of tracking multiple targets simultaneously. To overcome these
challenges, we present MEDUSA, a novel coherent ultra-wideband (UWB) based
distributed multiple-input multiple-output (MIMO) radar system, especially it
allows users to customize and disperse the $16 \times 16$ into sub-arrays.
MEDUSA takes advantage of the diversity benefits of distributed yet wirelessly
synchronized MIMO arrays to enable robust vital sign monitoring in real-world
and daily living environments where human targets are moving and surrounded by
obstacles. We've developed a scalable, self-supervised contrastive learning
model which integrates seamlessly with our hardware platform. Each attention
weight within the model corresponds to a specific antenna pair of Tx and Rx.
The model proficiently recovers accurate vital sign waveforms by decomposing
and correlating the mixed received signals, including comprising human motion,
mobility, noise, and vital signs. Through extensive evaluations involving 21
participants and over 200 hours of collected data (3.75 TB in total, with 1.89
TB for static subjects and 1.86 TB for moving subjects), MEDUSA's performance
has been validated, showing an average gain of 20% compared to existing systems
employing COTS radar sensors. This demonstrates MEDUSA's spatial diversity gain
for real-world vital sign monitoring, encompassing target and environmental
dynamics in familiar and unfamiliar indoor environments.
</p>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05508" title="Abstract">arXiv:2310.05508</a> [<a href="/pdf/2310.05508" title="Download PDF">pdf</a>, <a href="/format/2310.05508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparison between Markov Chain and Koopman Operator Based Data-Driven  Modeling of Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tafazzol%2C+S">Saeid Tafazzol</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+N">Nan Li</a>, 
<a href="/search/eess?searchtype=author&query=Kolmanovsky%2C+I">Ilya Kolmanovsky</a>, 
<a href="/search/eess?searchtype=author&query=Filev%2C+D">Dimitar Filev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Markov chain-based modeling and Koopman operator-based modeling are two
popular frameworks for data-driven modeling of dynamical systems.
Interestingly, while they are based on different theories, these two approaches
share notable similarities from a computational and practitioner's perspective,
especially for modeling autonomous systems. The first part of this paper aims
to elucidate these similarities and clarify the connection between these two
approaches. For modeling systems with control inputs, the models produced by
the two approaches differ. The second part of this paper introduces these
models and their corresponding control design methods. We illustrate the two
approaches and compare them in terms of model accuracy and computational
efficiency for both autonomous and controlled systems in numerical examples.
</p>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05511" title="Abstract">arXiv:2310.05511</a> [<a href="/pdf/2310.05511" title="Download PDF">pdf</a>, <a href="/format/2310.05511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proposal-based Temporal Action Localization with Point-level Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yuan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yifei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Furuta%2C+R">Ryosuke Furuta</a>, 
<a href="/search/cs?searchtype=author&query=Sato%2C+Y">Yoichi Sato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> BMVC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Point-level supervised temporal action localization (PTAL) aims at
recognizing and localizing actions in untrimmed videos where only a single
point (frame) within every action instance is annotated in training data.
Without temporal annotations, most previous works adopt the multiple instance
learning (MIL) framework, where the input video is segmented into
non-overlapped short snippets, and action classification is performed
independently on every short snippet. We argue that the MIL framework is
suboptimal for PTAL because it operates on separated short snippets that
contain limited temporal information. Therefore, the classifier only focuses on
several easy-to-distinguish snippets instead of discovering the whole action
instance without missing any relevant snippets. To alleviate this problem, we
propose a novel method that localizes actions by generating and evaluating
action proposals of flexible duration that involve more comprehensive temporal
information. Moreover, we introduce an efficient clustering algorithm to
efficiently generate dense pseudo labels that provide stronger supervision, and
a fine-grained contrastive loss to further refine the quality of pseudo labels.
Experiments show that our proposed method achieves competitive or superior
performance to the state-of-the-art methods and some fully-supervised methods
on four benchmarks: ActivityNet 1.3, THUMOS 14, GTEA, and BEOID datasets.
</p>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05512" title="Abstract">arXiv:2310.05512</a> [<a href="/pdf/2310.05512" title="Download PDF">pdf</a>, <a href="/format/2310.05512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UAVs and Neural Networks for search and rescue missions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Surmann%2C+H">Hartmut Surmann</a>, 
<a href="/search/cs?searchtype=author&query=Leinweber%2C+A">Artur Leinweber</a>, 
<a href="/search/cs?searchtype=author&query=Senkowski%2C+G">Gerhard Senkowski</a>, 
<a href="/search/cs?searchtype=author&query=Meine%2C+J">Julien Meine</a>, 
<a href="/search/cs?searchtype=author&query=Slomma%2C+D">Dominik Slomma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 56th International Symposium on Robotics (ISR Europe) | September 26-27, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this paper, we present a method for detecting objects of interest,
including cars, humans, and fire, in aerial images captured by unmanned aerial
vehicles (UAVs) usually during vegetation fires. To achieve this, we use
artificial neural networks and create a dataset for supervised learning. We
accomplish the assisted labeling of the dataset through the implementation of
an object detection pipeline that combines classic image processing techniques
with pretrained neural networks. In addition, we develop a data augmentation
pipeline to augment the dataset with automatically labeled images. Finally, we
evaluate the performance of different neural networks.
</p>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05513" title="Abstract">arXiv:2310.05513</a> [<a href="/pdf/2310.05513" title="Download PDF">pdf</a>, <a href="/format/2310.05513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Findings of the 2023 ML-SUPERB Challenge: Pre-Training and Evaluation  over More Languages and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiatong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">William Chen</a>, 
<a href="/search/cs?searchtype=author&query=Berrebbi%2C+D">Dan Berrebbi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hsiu-Hsuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wei-Ping Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+E">En-Pei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chuang%2C+H">Ho-Lam Chuang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xuankai Chang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yuxun Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shang-Wen Li</a>, 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+A">Abdelrahman Mohamed</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ASRU
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB)
Challenge expands upon the acclaimed SUPERB framework, emphasizing
self-supervised models in multilingual speech recognition and language
identification. The challenge comprises a research track focused on applying
ML-SUPERB to specific multilingual subjects, a Challenge Track for model
submissions, and a New Language Track where language resource researchers can
contribute and evaluate their low-resource language data in the context of the
latest progress in multilingual speech recognition. The challenge garnered 12
model submissions and 54 language corpora, resulting in a comprehensive
benchmark encompassing 154 languages. The findings indicate that merely scaling
models is not the definitive solution for multilingual speech tasks, and a
variety of speech/voice types present significant challenges in multilingual
speech processing.
</p>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05515" title="Abstract">arXiv:2310.05515</a> [<a href="/pdf/2310.05515" title="Download PDF">pdf</a>, <a href="/ps/2310.05515" title="Download PostScript">ps</a>, <a href="/format/2310.05515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Broadcast Channel Coding: Algorithmic Aspects and Non-Signaling  Assistance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fawzi%2C+O">Omar Fawzi</a>, 
<a href="/search/cs?searchtype=author&query=Ferm%C3%A9%2C+P">Paul Ferm&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">We address the problem of coding for classical broadcast channels, which
entails maximizing the success probability that can be achieved by sending a
fixed number of messages over a broadcast channel. For point-to-point channels,
Barman and Fawzi found in~\cite{BF18} a $(1-e^{-1})$-approximation algorithm
running in polynomial time, and showed that it is \textrm{NP}-hard to achieve a
strictly better approximation ratio. Furthermore, these algorithmic results
were at the core of the limitations they established on the power of
non-signaling assistance for point-to-point channels. It is natural to ask if
similar results hold for broadcast channels, exploiting links between
approximation algorithms of the channel coding problem and the non-signaling
assisted capacity region.
<br />In this work, we make several contributions on algorithmic aspects and
non-signaling assisted capacity regions of broadcast channels. For the class of
deterministic broadcast channels, we describe a $(1-e^{-1})^2$-approximation
algorithm running in polynomial time, and we show that the capacity region for
that class is the same with or without non-signaling assistance. Finally, we
show that in the value query model, we cannot achieve a better approximation
ratio than $\Omega\left(\frac{1}{\sqrt{m}}\right)$ in polynomial time for the
general broadcast channel coding problem, with $m$ the size of one of the
outputs of the channel.
</p>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05517" title="Abstract">arXiv:2310.05517</a> [<a href="/pdf/2310.05517" title="Download PDF">pdf</a>, <a href="/format/2310.05517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WeatherGNN: Exploiting Complicated Relationships in Numerical Weather  Prediction Bias Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Binqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weiqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wengwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Bingqing Peng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Liang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Ling Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Numerical weather prediction (NWP) may be inaccurate or biased due to
incomplete atmospheric physical processes, insufficient spatial-temporal
resolution, and inherent uncertainty of weather. Previous studies have
attempted to correct biases by using handcrafted features and domain knowledge,
or by applying general machine learning models naively. They do not fully
explore the complicated meteorologic interactions and spatial dependencies in
the atmosphere dynamically, which limits their applicability in NWP
bias-correction. Specifically, weather factors interact with each other in
complex ways, and these interactions can vary regionally. In addition, the
interactions between weather factors are further complicated by the spatial
dependencies between regions, which are influenced by varied terrain and
atmospheric motions. To address these issues, we propose WeatherGNN, an NWP
bias-correction method that utilizes Graph Neural Networks (GNN) to learn
meteorologic and geographic relationships in a unified framework. Our approach
includes a factor-wise GNN that captures meteorological interactions within
each grid (a specific location) adaptively, and a fast hierarchical GNN that
captures spatial dependencies between grids dynamically. Notably, the fast
hierarchical GNN achieves linear complexity with respect to the number of
grids, enhancing model efficiency and scalability. Our experimental results on
two real-world datasets demonstrate the superiority of WeatherGNN in comparison
with other SOTA methods, with an average improvement of 40.50\% on RMSE
compared to the original NWP.
</p>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05518" title="Abstract">arXiv:2310.05518</a> [<a href="/pdf/2310.05518" title="Download PDF">pdf</a>, <a href="/format/2310.05518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Double-Descent in Reinforcement Learning with LSTD and Random  Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brellmann%2C+D">David Brellmann</a>, 
<a href="/search/cs?searchtype=author&query=Berthier%2C+E">Elo&#xef;se Berthier</a>, 
<a href="/search/cs?searchtype=author&query=Filliat%2C+D">David Filliat</a>, 
<a href="/search/cs?searchtype=author&query=Frehse%2C+G">Goran Frehse</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Temporal Difference (TD) algorithms are widely used in Deep Reinforcement
Learning (RL). Their performance is heavily influenced by the size of the
neural network. While in supervised learning, the regime of
over-parameterization and its benefits are well understood, the situation in RL
is much less clear. In this paper, we present a theoretical analysis of the
influence of network size and $l_2$-regularization on performance. We identify
the ratio between the number of parameters and the number of visited states as
a crucial factor and define over-parameterization as the regime when it is
larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a
sudden drop in performance around the parameter/state ratio of one. Leveraging
random features and the lazy training regime, we study the regularized
Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as
both the number of parameters and states go to infinity, maintaining a constant
ratio. We derive deterministic limits of both the empirical and the true
Mean-Square Bellman Error (MSBE) that feature correction terms responsible for
the double-descent. Correction terms vanish when the $l_2$-regularization is
increased or the number of unvisited states goes to zero. Numerical experiments
with synthetic and small real-world environments closely match the theoretical
predictions.
</p>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05520" title="Abstract">arXiv:2310.05520</a> [<a href="/pdf/2310.05520" title="Download PDF">pdf</a>, <a href="/format/2310.05520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One Problem, One Solution: Unifying Robot and Environment Design  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baumg%C3%A4rtner%2C+J">Jan Baumg&#xe4;rtner</a>, 
<a href="/search/cs?searchtype=author&query=Kanagalingam%2C+G">Gajanan Kanagalingam</a>, 
<a href="/search/cs?searchtype=author&query=Fleischer%2C+A+P+J">Alexander Puchtaand J&#xfc;rgen Fleischer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The task-specific optimization of robotic systems has long been divided into
the optimization of the robot and the optimization of the environment. In this
letter, we argue that these two problems are interdependent and should be
treated as such. To this end, we present a unified problem formulation that
enables for the simultaneous optimization of both the robot kinematics and the
environment. We demonstrate the effectiveness of our approach by jointly
optimizing a robotic milling system. To compare our approach to the state of
the art we also optimize the robot kinematics and environment separately. The
results show that our approach outperforms the state of the art and that
simultaneous optimization leads to a much better solution.
</p>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05524" title="Abstract">arXiv:2310.05524</a> [<a href="/pdf/2310.05524" title="Download PDF">pdf</a>, <a href="/format/2310.05524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bi-directional Deformation for Parameterization of Neural Implicit  Surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Baixin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jiangbei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+F">Fei Hou</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+K">Kwan-Yee Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wayne Wu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chen Qian</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Ying He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The growing capabilities of neural rendering have increased the demand for
new techniques that enable the intuitive editing of 3D objects, particularly
when they are represented as neural implicit surfaces. In this paper, we
present a novel neural algorithm to parameterize neural implicit surfaces to
simple parametric domains, such as spheres, cubes or polycubes, where 3D
radiance field can be represented as a 2D field, thereby facilitating
visualization and various editing tasks. Technically, our method computes a
bi-directional deformation between 3D objects and their chosen parametric
domains, eliminating the need for any prior information. We adopt a forward
mapping of points on the zero level set of the 3D object to a parametric
domain, followed by a backward mapping through inverse deformation. To ensure
the map is bijective, we employ a cycle loss while optimizing the smoothness of
both deformations. Additionally, we leverage a Laplacian regularizer to
effectively control angle distortion and offer the flexibility to choose from a
range of parametric domains for managing area distortion. Designed for
compatibility, our framework integrates seamlessly with existing neural
rendering pipelines, taking multi-view images as input to reconstruct 3D
geometry and compute the corresponding texture map. We also introduce a simple
yet effective technique for intrinsic radiance decomposition, facilitating both
view-independent material editing and view-dependent shading editing. Our
method allows for the immediate rendering of edited textures through volume
rendering, without the need for network re-training. Moreover, our approach
supports the co-parameterization of multiple objects and enables texture
transfer between them. We demonstrate the effectiveness of our method on images
of human heads and man-made objects. We will make the source code publicly
available.
</p>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05525" title="Abstract">arXiv:2310.05525</a> [<a href="/pdf/2310.05525" title="Download PDF">pdf</a>, <a href="/format/2310.05525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physical Layer Security in a Private 5G Network for Industrial and  Mobility Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gonde%2C+S+H">Shivraj Hanumant Gonde</a>, 
<a href="/search/cs?searchtype=author&query=Frisch%2C+C">Christoph Frisch</a>, 
<a href="/search/cs?searchtype=author&query=Duhovnikov%2C+S">Svetoslav Duhovnikov</a>, 
<a href="/search/cs?searchtype=author&query=Kubisch%2C+M">Martin Kubisch</a>, 
<a href="/search/cs?searchtype=author&query=Meyerhoff%2C+T">Thomas Meyerhoff</a>, 
<a href="/search/cs?searchtype=author&query=Schupke%2C+D">Dominic Schupke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published at "2023 IEEE World Forum on Internet of Things (WF-IoT)"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Cellular communication technologies such as 5G are deployed on a large scale
around the world. Compared to other communication technologies such as WiFi,
Bluetooth, or Ultra Wideband, the 5G communication standard describes support
for a large variety of use cases, e.g., Internet of Things, vehicular,
industrial, and campus-wide communications. An organization can operate a
Private 5G network to provide connectivity to devices in their manufacturing
environment. Physical Layer Key Generation (PLKG) is a method to generate a
symmetric secret on two nodes despite the presence of a potential passive
eavesdropper. To the best of our knowledge, this work is one of the first to
implement PLKG in a real Private 5G network. Therefore, it highlights the
possibility of integrating PLKG in the communication technology highly relevant
for industrial applications. This paper exemplifies the establishment of a
long-term symmetric key between an aerial vehicle and IT infrastructure both
located in a manufacturing environment and communicating via the radio
interface of the Private 5G network.
</p>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05527" title="Abstract">arXiv:2310.05527</a> [<a href="/pdf/2310.05527" title="Download PDF">pdf</a>, <a href="/ps/2310.05527" title="Download PostScript">ps</a>, <a href="/format/2310.05527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diagonal of Pseudoinverse of Graph Laplacian: Fast Estimation and Exact  Results
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zenan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wanyue Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhongzhi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">The diagonal entries of pseudoinverse of the Laplacian matrix of a graph
appear in many important practical applications, since they contain much
information of the graph and many relevant quantities can be expressed in terms
of them, such as Kirchhoff index and current flow centrality. However, a
na\"{\i}ve approach for computing the diagonal of a matrix inverse has cubic
computational complexity in terms of the matrix dimension, which is not
acceptable for large graphs with millions of nodes. Thus, rigorous solutions to
the diagonal of the Laplacian matrices for general graphs, even for particluar
graphs are much less. In this paper, we propose a theoretically guaranteed
estimation algorithm, which approximates all diagonal entries of the
pseudoinverse of a graph Laplacian in nearly linear time with respect to the
number of edges in the graph. We execute extensive experiments on real-life
networks, which indicate that our algorithm is both efficient and accurate.
Also, we determine exact expressions for the diagonal elements of pseudoinverse
of the Laplacian matrices for Koch networks and uniform recursive trees, and
compare them with those obtained by our approximation algorithm. Finally, we
use our algorithm to evaluate the Kirchhoff index of three deterministic model
networks, for which the Kirchhoff index can be rigorously determined. These
results further show the effectiveness and efficiency of our algorithm.
</p>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05530" title="Abstract">arXiv:2310.05530</a> [<a href="/pdf/2310.05530" title="Download PDF">pdf</a>, <a href="/format/2310.05530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NetTiSA: Extended IP Flow with Time-series Features for Universal  Bandwidth-constrained High-speed Network Traffic Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koumar%2C+J">Josef Koumar</a>, 
<a href="/search/cs?searchtype=author&query=Hynek%2C+K">Karel Hynek</a>, 
<a href="/search/cs?searchtype=author&query=Pe%C5%A1ek%2C+J">Jaroslav Pe&#x161;ek</a>, 
<a href="/search/cs?searchtype=author&query=%C4%8Cejka%2C+T">Tom&#xe1;&#x161; &#x10c;ejka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to The International Journal of Computer and Telecommunications Networking
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Network traffic monitoring based on IP Flows is a standard monitoring
approach that can be deployed to various network infrastructures, even the
large IPS-based networks connecting millions of people. Since flow records
traditionally contain only limited information (addresses, transport ports, and
amount of exchanged data), they are also commonly extended for additional
features that enable network traffic analysis with high accuracy. Nevertheless,
the flow extensions are often too large or hard to compute, which limits their
deployment only to smaller-sized networks. This paper proposes a novel extended
IP flow called NetTiSA (Network Time Series Analysed), which is based on the
analysis of the time series of packet sizes. By thoroughly testing 25 different
network classification tasks, we show the broad applicability and high
usability of NetTiSA, which often outperforms the best-performing related
works. For practical deployment, we also consider the sizes of flows extended
for NetTiSA and evaluate the performance impacts of its computation in the flow
exporter. The novel feature set proved universal and deployable to high-speed
ISP networks with 100\,Gbps lines; thus, it enables accurate and widespread
network security protection.
</p>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05537" title="Abstract">arXiv:2310.05537</a> [<a href="/pdf/2310.05537" title="Download PDF">pdf</a>, <a href="/format/2310.05537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ParFam -- Symbolic Regression Based on Continuous Global Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scholl%2C+P">Philipp Scholl</a>, 
<a href="/search/cs?searchtype=author&query=Bieker%2C+K">Katharina Bieker</a>, 
<a href="/search/cs?searchtype=author&query=Hauger%2C+H">Hillary Hauger</a>, 
<a href="/search/cs?searchtype=author&query=Kutyniok%2C+G">Gitta Kutyniok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/Philipp238/parfam">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The problem of symbolic regression (SR) arises in many different
applications, such as identifying physical laws or deriving mathematical
equations describing the behavior of financial markets from given data. Various
methods exist to address the problem of SR, often based on genetic programming.
However, these methods are usually quite complicated and require a lot of
hyperparameter tuning and computational resources. In this paper, we present
our new method ParFam that utilizes parametric families of suitable symbolic
functions to translate the discrete symbolic regression problem into a
continuous one, resulting in a more straightforward setup compared to current
state-of-the-art methods. In combination with a powerful global optimizer, this
approach results in an effective method to tackle the problem of SR.
Furthermore, it can be easily extended to more advanced algorithms, e.g., by
adding a deep neural network to find good-fitting parametric families. We prove
the performance of ParFam with extensive numerical experiments based on the
common SR benchmark suit SRBench, showing that we achieve state-of-the-art
results. Our code and results can be found at
https://github.com/Philipp238/parfam .
</p>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05541" title="Abstract">arXiv:2310.05541</a> [<a href="/pdf/2310.05541" title="Download PDF">pdf</a>, <a href="/format/2310.05541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Visual Place Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiming Li</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+Z">Zonglin Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+M">Mingxuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Milford%2C+M">Michael Milford</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chen Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://ai4ce.github.io/CoVPR/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Visual place recognition (VPR) capabilities enable autonomous robots to
navigate complex environments by discovering the environment's topology based
on visual input. Most research efforts focus on enhancing the accuracy and
robustness of single-robot VPR but often encounter issues such as occlusion due
to individual viewpoints. Despite a number of research on multi-robot
metric-based localization, there is a notable gap in research concerning more
robust and efficient place-based localization with a multi-robot system. This
work proposes collaborative VPR, where multiple robots share abstracted visual
features to enhance place recognition capabilities. We also introduce a novel
collaborative VPR framework based on similarity-regularized information fusion,
reducing irrelevant noise while harnessing valuable data from collaborators.
This framework seamlessly integrates with well-established single-robot VPR
techniques and supports end-to-end training with a weakly-supervised
contrastive loss. We conduct experiments in urban, rural, and indoor scenes,
achieving a notable improvement over single-agent VPR in urban environments
(~12\%), along with consistent enhancements in rural (~3\%) and indoor (~1\%)
scenarios. Our work presents a promising solution to the pressing challenges of
VPR, representing a substantial step towards safe and robust autonomous
systems.
</p>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05542" title="Abstract">arXiv:2310.05542</a> [<a href="/pdf/2310.05542" title="Download PDF">pdf</a>, <a href="/format/2310.05542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harmful Conspiracies in Temporal Interaction Networks: Understanding the  Dynamics of Digital Wildfires through Phase Transitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%A5sv%C3%A6r%2C+K+S">Kaspara Skovli G&#xe5;sv&#xe6;r</a>, 
<a href="/search/cs?searchtype=author&query=Lind%2C+P+G">Pedro G. Lind</a>, 
<a href="/search/cs?searchtype=author&query=Langguth%2C+J">Johannes Langguth</a>, 
<a href="/search/cs?searchtype=author&query=Hjorth-Jensen%2C+M">Morten Hjorth-Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Kreil%2C+M">Michael Kreil</a>, 
<a href="/search/cs?searchtype=author&query=Schroeder%2C+D+T">Daniel Thilo Schroeder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Shortly after the first COVID-19 cases became apparent in December 2020,
rumors spread on social media suggesting a connection between the virus and the
5G radiation emanating from the recently deployed telecommunications network.
In the course of the following weeks, this idea gained increasing popularity,
and various alleged explanations for how such a connection manifests emerged.
Ultimately, after being amplified by prominent conspiracy theorists, a series
of arson attacks on telecommunication equipment follows, concluding with the
kidnapping of telecommunication technicians in Peru. In this paper, we study
the spread of content related to a conspiracy theory with harmful consequences,
a so-called digital wildfire. In particular, we investigate the 5G and COVID-19
misinformation event on Twitter before, during, and after its peak in April and
May 2020. For this purpose, we examine the community dynamics in complex
temporal interaction networks underlying Twitter user activity. We assess the
evolution of such digital wildfires by appropriately defining the temporal
dynamics of communication in communities within social networks. We show that,
for this specific misinformation event, the number of interactions of the users
participating in a digital wildfire, as well as the size of the engaged
communities, both follow a power-law distribution. Moreover, our research
elucidates the possibility of quantifying the phases of a digital wildfire, as
per established literature. We identify one such phase as a critical
transition, marked by a shift from sporadic tweets to a global spread event,
highlighting the dramatic scaling of misinformation propagation.
</p>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05543" title="Abstract">arXiv:2310.05543</a> [<a href="/pdf/2310.05543" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamics of the Ride-Sourcing Market: A Coevolutionary Model of  Competition between Two-Sided Mobility Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghasemi%2C+F">Farnoud Ghasemi</a>, 
<a href="/search/cs?searchtype=author&query=Drabicki%2C+A">Arkadiusz Drabicki</a>, 
<a href="/search/cs?searchtype=author&query=Kucharski%2C+R">Rafa&#x142; Kucharski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">There is a fierce competition between two-sided mobility platforms (e.g.,
Uber and Lyft) fueled by massive subsidies, yet the underlying dynamics and
interactions between the competing plat-forms are largely unknown. These
platforms rely on the cross-side network effects to grow, they need to attract
agents from both sides to kick-off: travellers are needed for drivers and
drivers are needed for travellers. We use our coevolutionary model featured by
the S-shaped learning curves to simulate the day-to-day dynamics of the
ride-sourcing market at the microscopic level. We run three scenarios to
illustrate the possible equilibria in the market. Our results underline how the
correlation inside the ride-sourcing nest of the agents choice set
significantly affects the plat-forms' market shares. While late entry to the
market decreases the chance of platform success and possibly results in
"winner-takes-all", heavy subsidies can keep the new platform in competition
giving rise to "market sharing" regime.
</p>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05547" title="Abstract">arXiv:2310.05547</a> [<a href="/pdf/2310.05547" title="Download PDF">pdf</a>, <a href="/format/2310.05547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometry-Aware Safety-Critical Local Reactive Controller for Robot  Navigation in Unknown and Cluttered Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yulin Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xindong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chunxin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haichao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jun Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This work proposes a safety-critical local reactive controller that enables
the robot to navigate in unknown and cluttered environments. In particular, the
trajectory tracking task is formulated as a constrained polynomial optimization
problem. Then, safety constraints are imposed on the control variables invoking
the notion of polynomial positivity certificates in conjunction with their
Sum-of-Squares (SOS) approximation, thereby confining the robot motion inside
the locally extracted convex free region. It is noteworthy that, in the process
of devising the proposed safety constraints, the geometry of the robot can be
approximated using any shape that can be characterized with a set of polynomial
functions. The optimization problem is further convexified into a semidefinite
program (SDP) leveraging truncated multi-sequences (tms) and moment relaxation,
which favorably facilitates the effective use of off-the-shelf conic
programming solvers, such that real-time performance is attainable. Various
robot navigation tasks are investigated to demonstrate the effectiveness of the
proposed approach in terms of safety and tracking performance.
</p>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05551" title="Abstract">arXiv:2310.05551</a> [<a href="/pdf/2310.05551" title="Download PDF">pdf</a>, <a href="/format/2310.05551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logic-guided Deep Reinforcement Learning for Stock Trading
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiming Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junzhe Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yushi Cao</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+A">Aixin Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Bozhi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL)

</div>
<p class="mathjax">Deep reinforcement learning (DRL) has revolutionized quantitative finance by
achieving excellent performance without significant manual effort. Whereas we
observe that the DRL models behave unstably in a dynamic stock market due to
the low signal-to-noise ratio nature of the financial data. In this paper, we
propose a novel logic-guided trading framework, termed as SYENS (Program
Synthesis-based Ensemble Strategy). Different from the previous
state-of-the-art ensemble reinforcement learning strategy which arbitrarily
selects the best-performing agent for testing based on a single measurement,
our framework proposes regularizing the model's behavior in a hierarchical
manner using the program synthesis by sketching paradigm. First, we propose a
high-level, domain-specific language (DSL) that is used for the depiction of
the market environment and action. Then based on the DSL, a novel program
sketch is introduced, which embeds human expert knowledge in a logical manner.
Finally, based on the program sketch, we adopt the program synthesis by
sketching a paradigm and synthesizing a logical, hierarchical trading strategy.
We evaluate SYENS on the 30 Dow Jones stocks under the cash trading and the
margin trading settings. Experimental results demonstrate that our proposed
framework can significantly outperform the baselines with much higher
cumulative return and lower maximum drawdown under both settings.
</p>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05553" title="Abstract">arXiv:2310.05553</a> [<a href="/pdf/2310.05553" title="Download PDF">pdf</a>, <a href="/format/2310.05553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regulation and NLP (RegNLP): Taming Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goanta%2C+C">Catalina Goanta</a>, 
<a href="/search/cs?searchtype=author&query=Aletras%2C+N">Nikolaos Aletras</a>, 
<a href="/search/cs?searchtype=author&query=Chalkidis%2C+I">Ilias Chalkidis</a>, 
<a href="/search/cs?searchtype=author&query=Ranchordas%2C+S">Sofia Ranchordas</a>, 
<a href="/search/cs?searchtype=author&query=Spanakis%2C+G">Gerasimos Spanakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, long paper at EMNLP 2023 proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The scientific innovation in Natural Language Processing (NLP) and more
broadly in artificial intelligence (AI) is at its fastest pace to date. As
large language models (LLMs) unleash a new era of automation, important debates
emerge regarding the benefits and risks of their development, deployment and
use. Currently, these debates have been dominated by often polarized narratives
mainly led by the AI Safety and AI Ethics movements. This polarization, often
amplified by social media, is swaying political agendas on AI regulation and
governance and posing issues of regulatory capture. Capture occurs when the
regulator advances the interests of the industry it is supposed to regulate, or
of special interest groups rather than pursuing the general public interest.
Meanwhile in NLP research, attention has been increasingly paid to the
discussion of regulating risks and harms. This often happens without systematic
methodologies or sufficient rooting in the disciplines that inspire an extended
scope of NLP research, jeopardizing the scientific integrity of these
endeavors. Regulation studies are a rich source of knowledge on how to
systematically deal with risk and uncertainty, as well as with scientific
evidence, to evaluate and compare regulatory options. This resource has largely
remained untapped so far. In this paper, we argue how NLP research on these
topics can benefit from proximity to regulatory studies and adjacent fields. We
do so by discussing basic tenets of regulation, and risk and uncertainty, and
by highlighting the shortcomings of current NLP discussions dealing with risk
assessment. Finally, we advocate for the development of a new multidisciplinary
research space on regulation and NLP (RegNLP), focused on connecting scientific
knowledge to regulatory processes based on systematic methodologies.
</p>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05556" title="Abstract">arXiv:2310.05556</a> [<a href="/pdf/2310.05556" title="Download PDF">pdf</a>, <a href="/format/2310.05556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth  Estimation under Adverse Weather Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chunyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+L">Lang Nie</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shujun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xing Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+R">Rui Ai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Depth estimation models have shown promising performance on clear scenes but
fail to generalize to adverse weather conditions due to illumination
variations, weather particles, etc. In this paper, we propose WeatherDepth, a
self-supervised robust depth estimation model with curriculum contrastive
learning, to tackle performance degradation in complex weather conditions.
Concretely, we first present a progressive curriculum learning scheme with
three simple-to-complex curricula to gradually adapt the model from clear to
relative adverse, and then to adverse weather scenes. It encourages the model
to gradually grasp beneficial depth cues against the weather effect, yielding
smoother and better domain adaption. Meanwhile, to prevent the model from
forgetting previous curricula, we integrate contrastive learning into different
curricula. Drawn the reference knowledge from the previous course, our strategy
establishes a depth consistency constraint between different courses towards
robust depth estimation in diverse weather. Besides, to reduce manual
intervention and better adapt to different models, we designed an adaptive
curriculum scheduler to automatically search for the best timing for course
switching. In the experiment, the proposed solution is proven to be easily
incorporated into various architectures and demonstrates state-of-the-art
(SoTA) performance on both synthetic and real weather datasets.
</p>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05557" title="Abstract">arXiv:2310.05557</a> [<a href="/pdf/2310.05557" title="Download PDF">pdf</a>, <a href="/format/2310.05557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating CFD Simulations of Microfluidic Devices by Exploiting  Higher Levels of Abstraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takken%2C+M">Michel Takken</a>, 
<a href="/search/cs?searchtype=author&query=Wille%2C+R">Robert Wille</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures. Source code: <a href="https://github.com/cda-tum/mmft-hybrid-simulator">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">The design of microfluidic devices is a cumbersome and tedious process that
can be significantly improved by simulation. Methods based on Computational
Fluid Dynamics (CFD) are considered state-of-the-art but require extensive
compute time-oftentimes limiting the size of microfluidic devices that can be
simulated. Simulation methods that abstract the underlying physics on a higher
level generally provide results instantly, but the fidelity of these methods is
usually worse. In this work, a simulation method that accelerates CFD
simulations by exploiting simulation methods on higher levels of abstraction is
proposed. Case studies confirm that the proposed method accelerates CFD
simulations by multiple factors (often several orders of magnitude) while
maintaining the fidelity of CFD simulations.
</p>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05563" title="Abstract">arXiv:2310.05563</a> [<a href="/pdf/2310.05563" title="Download PDF">pdf</a>, <a href="/format/2310.05563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STREAM: Social data and knowledge collective intelligence platform for  TRaining Ethical AI Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+E">Enmeng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+Z">Zizhe Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yi Zeng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This paper presents Social data and knowledge collective intelligence
platform for TRaining Ethical AI Models (STREAM) to address the challenge of
aligning AI models with human moral values, and to provide ethics datasets and
knowledge bases to help promote AI models "follow good advice as naturally as a
stream follows its course". By creating a comprehensive and representative
platform that accurately mirrors the moral judgments of diverse groups
including humans and AIs, we hope to effectively portray cultural and group
variations, and capture the dynamic evolution of moral judgments over time,
which in turn will facilitate the Establishment, Evaluation, Embedding,
Embodiment, Ensemble, and Evolvement (6Es) of the moral capabilities of AI
models. Currently, STREAM has already furnished a comprehensive collection of
ethical scenarios, and amassed substantial moral judgment data annotated by
volunteers and various popular Large Language Models (LLMs), collectively
portraying the moral preferences and performances of both humans and AIs across
a range of moral contexts. This paper will outline the current structure and
construction of STREAM, explore its potential applications, and discuss its
future prospects.
</p>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05564" title="Abstract">arXiv:2310.05564</a> [<a href="/pdf/2310.05564" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Node Selection Method in Wireless Distributed Edge Storage Based  on SDN and Multi-attribute Decision Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yejin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+M">Miao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Q">Qiuxiang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+P">Peng Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The distributed edge storage system can store data collected at the edge of
the network in a decentralised manner, with low latency, high security, and
flexibility. Traditional edge-distributed storage systems only consider one
single factor, such as node capacity, when storing data, ignoring network and
storage node load conditions that affecting the system's read/write
performance. At the same time, it could be more scalable in the widely used
wireless terminal application scenarios. To tackle these challenges, this paper
proposes an innovative software-defined edge storage architecture based on SDN
(Software-Defined Networking) and SMB (Server Message Block) protocols, A data
storage node selection algorithm that integrates the network state and storage
node load state is designed based on multi-attribute decision model, and a
system prototype is realised in conjunction with 5G wireless communication
technology. Experimental results demonstrate significant improvements in the
performance of high-load write operations compared to traditional
edge-distributed storage systems. The proposed wireless distributed edge
storage system also demonstrates superior scalability and adaptability,
effectively addressing the challenge of limited system scalability and
improving compatibility with edge scenarios in mobile applications. In
addition, it results in cost savings in hardware deployment and presents a
promising advancement in edge storage technology.
</p>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05566" title="Abstract">arXiv:2310.05566</a> [<a href="/pdf/2310.05566" title="Download PDF">pdf</a>, <a href="/ps/2310.05566" title="Download PostScript">ps</a>, <a href="/format/2310.05566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aggregated f-average Neural Network for Interpretable Ensembling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vu%2C+M">Mathieu Vu</a>, 
<a href="/search/cs?searchtype=author&query=Chouzenoux%2C+E">Emilie Chouzenoux</a>, 
<a href="/search/cs?searchtype=author&query=Pesquet%2C+J">Jean-Christophe Pesquet</a>, 
<a href="/search/cs?searchtype=author&query=Ayed%2C+I+B">Ismail Ben Ayed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Ensemble learning leverages multiple models (i.e., weak learners) on a common
machine learning task to enhance prediction performance. Basic ensembling
approaches average the weak learners outputs, while more sophisticated ones
stack a machine learning model in between the weak learners outputs and the
final prediction. This work fuses both aforementioned frameworks. We introduce
an aggregated f-average (AFA) shallow neural network which models and combines
different types of averages to perform an optimal aggregation of the weak
learners predictions. We emphasise its interpretable architecture and simple
training strategy, and illustrate its good performance on the problem of
few-shot class incremental learning.
</p>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05567" title="Abstract">arXiv:2310.05567</a> [<a href="/pdf/2310.05567" title="Download PDF">pdf</a>, <a href="/format/2310.05567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collision Avoidance for Autonomous Surface Vessels using Novel  Artificial Potential Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jadhav%2C+A+K">Aditya Kailas Jadhav</a>, 
<a href="/search/cs?searchtype=author&query=Pandi%2C+A+R">Anantha Raj Pandi</a>, 
<a href="/search/cs?searchtype=author&query=Somayajula%2C+A">Abhilash Somayajula</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 30 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computational Engineering, Finance, and Science (cs.CE); Systems and Control (eess.SY)

</div>
<p class="mathjax">As the demand for transportation through waterways continues to rise, the
number of vessels plying the waters has correspondingly increased. This has
resulted in a greater number of accidents and collisions between ships, some of
which lead to significant loss of life and financial losses. Research has shown
that human error is a major factor responsible for such incidents. The maritime
industry is constantly exploring newer approaches to autonomy to mitigate this
issue. This study presents the use of novel Artificial Potential Fields (APFs)
to perform obstacle and collision avoidance in marine environments. This study
highlights the advantage of harmonic functions over traditional functions in
modeling potential fields. With a modification, the method is extended to
effectively avoid dynamic obstacles while adhering to COLREGs. Improved
performance is observed as compared to the traditional potential fields and
also against the popular velocity obstacle approach. A comprehensive
statistical analysis is also performed through Monte Carlo simulations in
different congested environments that emulate real traffic conditions to
demonstrate robustness of the approach.
</p>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05571" title="Abstract">arXiv:2310.05571</a> [<a href="/pdf/2310.05571" title="Download PDF">pdf</a>, <a href="/format/2310.05571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Noisy One-Bit Diffraction Tomography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pengwen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fannjiang%2C+A">Albert Fannjiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">A reconstruction scheme based on one-bit intensity-only measurement with a
coded aperture is shown to possess remarkable noise robustness in 3D
diffraction tomography.
</p>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05573" title="Abstract">arXiv:2310.05573</a> [<a href="/pdf/2310.05573" title="Download PDF">pdf</a>, <a href="/format/2310.05573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ODEFormer: Symbolic Regression of Dynamical Systems with Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=d%27Ascoli%2C+S">St&#xe9;phane d&#x27;Ascoli</a>, 
<a href="/search/cs?searchtype=author&query=Becker%2C+S">S&#xf6;ren Becker</a>, 
<a href="/search/cs?searchtype=author&query=Mathis%2C+A">Alexander Mathis</a>, 
<a href="/search/cs?searchtype=author&query=Schwaller%2C+P">Philippe Schwaller</a>, 
<a href="/search/cs?searchtype=author&query=Kilbertus%2C+N">Niki Kilbertus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We introduce ODEFormer, the first transformer able to infer multidimensional
ordinary differential equation (ODE) systems in symbolic form from the
observation of a single solution trajectory. We perform extensive evaluations
on two datasets: (i) the existing "Strogatz" dataset featuring two-dimensional
systems; (ii) ODEBench, a collection of one- to four-dimensional systems that
we carefully curated from the literature to provide a more holistic benchmark.
ODEFormer consistently outperforms existing methods while displaying
substantially improved robustness to noisy and irregularly sampled
observations, as well as faster inference. We release our code, model and
benchmark dataset publicly.
</p>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05588" title="Abstract">arXiv:2310.05588</a> [<a href="/pdf/2310.05588" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ride Acceptance Behaviour Investigation of Ride-sourcing Drivers Through  Agent-based Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghasemi%2C+F">Farnoud Ghasemi</a>, 
<a href="/search/cs?searchtype=author&query=Ashkrof%2C+P">Peyman Ashkrof</a>, 
<a href="/search/cs?searchtype=author&query=Kucharski%2C+R">Rafal Kucharski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Ride-sourcing platforms such as Uber and Lyft offer drivers (i.e., platform
suppliers) considerable freedom of choice in multiple aspects. At the
operational level, drivers can freely accept or decline trip requests that can
significantly impact system performance in terms of travellers' waiting time,
drivers' idle time and income. Despite the extensive research into the
supply-side operations, the behavioural aspects, particularly drivers' ride
acceptance behaviour remains so far largely unknown. To this end, we reproduce
the dynamics of a two-sided mobility platform on the road network of Delft
using an agent-based simulator. Then, we implement a ride acceptance decision
model enabling drivers to apply their acceptance strategies. Our findings
reveal that drivers who follow the decision model, on average, earn higher
income compared to drivers who randomly accept trip requests. The overall
income equality between drivers with the acceptance decision is higher and
travellers experience lower waiting time in this setting.
</p>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05589" title="Abstract">arXiv:2310.05589</a> [<a href="/pdf/2310.05589" title="Download PDF">pdf</a>, <a href="/format/2310.05589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DRIN: Dynamic Relation Interactive Network for Multimodal Entity Linking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+S">Shangyu Xing</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Fei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianbing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xinyu Dai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Multimodal Entity Linking (MEL) is a task that aims to link ambiguous
mentions within multimodal contexts to referential entities in a multimodal
knowledge base. Recent methods for MEL adopt a common framework: they first
interact and fuse the text and image to obtain representations of the mention
and entity respectively, and then compute the similarity between them to
predict the correct entity. However, these methods still suffer from two
limitations: first, as they fuse the features of text and image before
matching, they cannot fully exploit the fine-grained alignment relations
between the mention and entity. Second, their alignment is static, leading to
low performance when dealing with complex and diverse data. To address these
issues, we propose a novel framework called Dynamic Relation Interactive
Network (DRIN) for MEL tasks. DRIN explicitly models four different types of
alignment between a mention and entity and builds a dynamic Graph Convolutional
Network (GCN) to dynamically select the corresponding alignment relations for
different input samples. Experiments on two datasets show that DRIN outperforms
state-of-the-art methods by a large margin, demonstrating the effectiveness of
our approach.
</p>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05590" title="Abstract">arXiv:2310.05590</a> [<a href="/pdf/2310.05590" title="Download PDF">pdf</a>, <a href="/format/2310.05590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perceptual Artifacts Localization for Image Synthesis Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lingzhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhengjie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Barnes%2C+C">Connelly Barnes</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuqian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">He Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Amirghodsi%2C+S">Sohrab Amirghodsi</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhe Lin</a>, 
<a href="/search/cs?searchtype=author&query=Shechtman%2C+E">Eli Shechtman</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jianbo Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advancements in deep generative models have facilitated the creation
of photo-realistic images across various tasks. However, these generated images
often exhibit perceptual artifacts in specific regions, necessitating manual
correction. In this study, we present a comprehensive empirical examination of
Perceptual Artifacts Localization (PAL) spanning diverse image synthesis
endeavors. We introduce a novel dataset comprising 10,168 generated images,
each annotated with per-pixel perceptual artifact labels across ten synthesis
tasks. A segmentation model, trained on our proposed dataset, effectively
localizes artifacts across a range of tasks. Additionally, we illustrate its
proficiency in adapting to previously unseen models using minimal training
samples. We further propose an innovative zoom-in inpainting pipeline that
seamlessly rectifies perceptual artifacts in the generated images. Through our
experimental analyses, we elucidate several practical downstream applications,
such as automated artifact rectification, non-referential image quality
evaluation, and abnormal region detection in images. The dataset and code are
released.
</p>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05592" title="Abstract">arXiv:2310.05592</a> [<a href="/pdf/2310.05592" title="Download PDF">pdf</a>, <a href="/format/2310.05592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InterroLang: Exploring NLP Models and Datasets through Dialogue-based  Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feldhus%2C+N">Nils Feldhus</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianli Wang</a>, 
<a href="/search/cs?searchtype=author&query=Anikina%2C+T">Tatiana Anikina</a>, 
<a href="/search/cs?searchtype=author&query=Chopra%2C+S">Sahil Chopra</a>, 
<a href="/search/cs?searchtype=author&query=Oguz%2C+C">Cennet Oguz</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6ller%2C+S">Sebastian M&#xf6;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">While recently developed NLP explainability methods let us open the black box
in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is
an interactive tool offering a conversational interface. Such a dialogue system
can help users explore datasets and models with explanations in a
contextualized manner, e.g. via clarification or follow-up questions, and
through a natural language interface. We adapt the conversational explanation
framework TalkToModel (Slack et al., 2022) to the NLP domain, add new
NLP-specific operations such as free-text rationalization, and illustrate its
generalizability on three NLP tasks (dialogue act classification, question
answering, hate speech detection). To recognize user queries for explanations,
we evaluate fine-tuned and few-shot prompting models and implement a novel
Adapter-based approach. We then conduct two user studies on (1) the perceived
correctness and helpfulness of the dialogues, and (2) the simulatability, i.e.
how objectively helpful dialogical explanations are for humans in figuring out
the model's predicted label when it's not shown. We found rationalization and
feature attribution were helpful in explaining the model behavior. Moreover,
users could more reliably predict the model outcome based on an explanation
dialogue rather than one-off explanations.
</p>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05595" title="Abstract">arXiv:2310.05595</a> [<a href="/pdf/2310.05595" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding the Threat Landscape : ChatGPT, FraudGPT, and WormGPT in Social  Engineering Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Falade%2C+P+V">Polra Victor Falade</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 185 - 198 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Scientific Research in Computer Science,
  Engineering and Information Technology ISSN : 2456-3307 Volume 9, Issue 5
  September-October-2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In the ever-evolving realm of cybersecurity, the rise of generative AI models
like ChatGPT, FraudGPT, and WormGPT has introduced both innovative solutions
and unprecedented challenges. This research delves into the multifaceted
applications of generative AI in social engineering attacks, offering insights
into the evolving threat landscape using the blog mining technique. Generative
AI models have revolutionized the field of cyberattacks, empowering malicious
actors to craft convincing and personalized phishing lures, manipulate public
opinion through deepfakes, and exploit human cognitive biases. These models,
ChatGPT, FraudGPT, and WormGPT, have augmented existing threats and ushered in
new dimensions of risk. From phishing campaigns that mimic trusted
organizations to deepfake technology impersonating authoritative figures, we
explore how generative AI amplifies the arsenal of cybercriminals. Furthermore,
we shed light on the vulnerabilities that AI-driven social engineering
exploits, including psychological manipulation, targeted phishing, and the
crisis of authenticity. To counter these threats, we outline a range of
strategies, including traditional security measures, AI-powered security
solutions, and collaborative approaches in cybersecurity. We emphasize the
importance of staying vigilant, fostering awareness, and strengthening
regulations in the battle against AI-enhanced social engineering attacks. In an
environment characterized by the rapid evolution of AI models and a lack of
training data, defending against generative AI threats requires constant
adaptation and the collective efforts of individuals, organizations, and
governments. This research seeks to provide a comprehensive understanding of
the dynamic interplay between generative AI and social engineering attacks,
equipping stakeholders with the knowledge to navigate this intricate
cybersecurity landscape.
</p>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05597" title="Abstract">arXiv:2310.05597</a> [<a href="/pdf/2310.05597" title="Download PDF">pdf</a>, <a href="/format/2310.05597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can language models learn analogical reasoning? Investigating training  objectives and comparisons to human performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petersen%2C+M+R">Molly R. Petersen</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Plas%2C+L">Lonneke van der Plas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While analogies are a common way to evaluate word embeddings in NLP, it is
also of interest to investigate whether or not analogical reasoning is a task
in itself that can be learned. In this paper, we test several ways to learn
basic analogical reasoning, specifically focusing on analogies that are more
typical of what is used to evaluate analogical reasoning in humans than those
in commonly used NLP benchmarks. Our experiments find that models are able to
learn analogical reasoning, even with a small amount of data. We additionally
compare our models to a dataset with a human baseline, and find that after
training, models approach human performance.
</p>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05598" title="Abstract">arXiv:2310.05598</a> [<a href="/pdf/2310.05598" title="Download PDF">pdf</a>, <a href="/format/2310.05598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Prediction-Modelers and Decision-Makers: Why Fairness Requires More  Than a Fair Prediction Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scantamburlo%2C+T">Teresa Scantamburlo</a>, 
<a href="/search/cs?searchtype=author&query=Baumann%2C+J">Joachim Baumann</a>, 
<a href="/search/cs?searchtype=author&query=Heitz%2C+C">Christoph Heitz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">An implicit ambiguity in the field of prediction-based decision-making
regards the relation between the concepts of prediction and decision. Much of
the literature in the field tends to blur the boundaries between the two
concepts and often simply speaks of 'fair prediction.' In this paper, we point
out that a differentiation of these concepts is helpful when implementing
algorithmic fairness. Even if fairness properties are related to the features
of the used prediction model, what is more properly called 'fair' or 'unfair'
is a decision system, not a prediction model. This is because fairness is about
the consequences on human lives, created by a decision, not by a prediction. We
clarify the distinction between the concepts of prediction and decision and
show the different ways in which these two elements influence the final
fairness properties of a prediction-based decision system. In addition to
exploring this relationship conceptually and practically, we propose a
framework that enables a better understanding and reasoning of the conceptual
logic of creating fairness in prediction-based decision-making. In our
framework, we specify different roles, namely the 'prediction-modeler' and the
'decision-maker,' and the information required from each of them for being able
to implement fairness of the system. Our framework allows for deriving distinct
responsibilities for both roles and discussing some insights related to ethical
and legal requirements. Our contribution is twofold. First, we shift the focus
from abstract algorithmic fairness to context-dependent decision-making,
recognizing diverse actors with unique objectives and independent actions.
Second, we provide a conceptual framework that can help structure
prediction-based decision problems with respect to fairness issues, identify
responsibilities, and implement fairness governance mechanisms in real-world
scenarios.
</p>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05600" title="Abstract">arXiv:2310.05600</a> [<a href="/pdf/2310.05600" title="Download PDF">pdf</a>, <a href="/format/2310.05600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Care3D: An Active 3D Object Detection Dataset of Real Robotic-Care  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adam%2C+M+G">Michael G. Adam</a>, 
<a href="/search/cs?searchtype=author&query=Eger%2C+S">Sebastian Eger</a>, 
<a href="/search/cs?searchtype=author&query=Piccolrovazzi%2C+M">Martin Piccolrovazzi</a>, 
<a href="/search/cs?searchtype=author&query=Iskandar%2C+M">Maged Iskandar</a>, 
<a href="/search/cs?searchtype=author&query=Vogel%2C+J">Joern Vogel</a>, 
<a href="/search/cs?searchtype=author&query=Dietrich%2C+A">Alexander Dietrich</a>, 
<a href="/search/cs?searchtype=author&query=Bien%2C+S">Seongjien Bien</a>, 
<a href="/search/cs?searchtype=author&query=Skerlj%2C+J">Jon Skerlj</a>, 
<a href="/search/cs?searchtype=author&query=Naceri%2C+A">Abdeldjallil Naceri</a>, 
<a href="/search/cs?searchtype=author&query=Steinbach%2C+E">Eckehard Steinbach</a>, 
<a href="/search/cs?searchtype=author&query=Albu-Schaeffer%2C+A">Alin Albu-Schaeffer</a>, 
<a href="/search/cs?searchtype=author&query=Haddadin%2C+S">Sami Haddadin</a>, 
<a href="/search/cs?searchtype=author&query=Burgard%2C+W">Wolfram Burgard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">As labor shortage increases in the health sector, the demand for assistive
robotics grows. However, the needed test data to develop those robots is
scarce, especially for the application of active 3D object detection, where no
real data exists at all. This short paper counters this by introducing such an
annotated dataset of real environments. The captured environments represent
areas which are already in use in the field of robotic health care research. We
further provide ground truth data within one room, for assessing SLAM
algorithms running directly on a health care robot.
</p>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05605" title="Abstract">arXiv:2310.05605</a> [<a href="/pdf/2310.05605" title="Download PDF">pdf</a>, <a href="/format/2310.05605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EdgeAISim: A Toolkit for Simulation and Modelling of AI Models in Edge  Computing Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nandhakumar%2C+A+R">Aadharsh Roshan Nandhakumar</a>, 
<a href="/search/cs?searchtype=author&query=Baranwal%2C+A">Ayush Baranwal</a>, 
<a href="/search/cs?searchtype=author&query=Choudhary%2C+P">Priyanshukumar Choudhary</a>, 
<a href="/search/cs?searchtype=author&query=Golec%2C+M">Muhammed Golec</a>, 
<a href="/search/cs?searchtype=author&query=Gill%2C+S+S">Sukhpal Singh Gill</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The Preprint version is submitted to Elsevier
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">To meet next-generation IoT application demands, edge computing moves
processing power and storage closer to the network edge to minimise latency and
bandwidth utilisation. Edge computing is becoming popular as a result of these
benefits, but resource management is still challenging. Researchers are
utilising AI models to solve the challenge of resource management in edge
computing systems. However, existing simulation tools are only concerned with
typical resource management policies, not the adoption and implementation of AI
models for resource management, especially. Consequently, researchers continue
to face significant challenges, making it hard and time-consuming to use AI
models when designing novel resource management policies for edge computing
with existing simulation tools. To overcome these issues, we propose a
lightweight Python-based toolkit called EdgeAISim for the simulation and
modelling of AI models for designing resource management policies in edge
computing environments. In EdgeAISim, we extended the basic components of the
EdgeSimPy framework and developed new AI-based simulation models for task
scheduling, energy management, service migration, network flow scheduling, and
mobility support for edge computing environments. In EdgeAISim, we have
utilised advanced AI models such as Multi-Armed Bandit with Upper Confidence
Bound, Deep Q-Networks, Deep Q-Networks with Graphical Neural Network, and
ActorCritic Network to optimize power usage while efficiently managing task
migration within the edge computing environment. The performance of these
proposed models of EdgeAISim is compared with the baseline, which uses a
worst-fit algorithm-based resource management policy in different settings.
Experimental results indicate that EdgeAISim exhibits a substantial reduction
in power consumption, highlighting the compelling success of power optimization
strategies in EdgeAISim.
</p>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05615" title="Abstract">arXiv:2310.05615</a> [<a href="/pdf/2310.05615" title="Download PDF">pdf</a>, <a href="/format/2310.05615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Multi-head Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Koniusz%2C+P">Piotr Koniusz</a>, 
<a href="/search/cs?searchtype=author&query=Gedeon%2C+T">Tom Gedeon</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Liang Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In contrastive learning, two views of an original image generated by
different augmentations are considered as a positive pair whose similarity is
required to be high. Moreover, two views of two different images are considered
as a negative pair, and their similarity is encouraged to be low. Normally, a
single similarity measure given by a single projection head is used to evaluate
positive and negative sample pairs, respectively. However, due to the various
augmentation strategies and varying intra-sample similarity, augmented views
from the same image are often not similar. Moreover, due to inter-sample
similarity, augmented views of two different images may be more similar than
augmented views from the same image. As such, enforcing a high similarity for
positive pairs and a low similarity for negative pairs may not always be
achievable, and in the case of some pairs, forcing so may be detrimental to the
performance. To address this issue, we propose to use multiple projection
heads, each producing a separate set of features. Our loss function for
pre-training emerges from a solution to the maximum likelihood estimation over
head-wise posterior distributions of positive samples given observations. The
loss contains the similarity measure over positive and negative pairs, each
re-weighted by an individual adaptive temperature that is regularized to
prevent ill solutions. Our adaptive multi-head contrastive learning (AMCL) can
be applied to and experimentally improves several popular contrastive learning
methods such as SimCLR, MoCo and Barlow Twins. Such improvement is consistent
under various backbones and linear probing epoches and is more significant when
multiple augmentation methods are used.
</p>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05618" title="Abstract">arXiv:2310.05618</a> [<a href="/pdf/2310.05618" title="Download PDF">pdf</a>, <a href="/format/2310.05618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASM: Adaptive Sample Mining for In-The-Wild Facial Expression  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiao Sun</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+L">Liuwei An</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Meng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Given the similarity between facial expression categories, the presence of
compound facial expressions, and the subjectivity of annotators, facial
expression recognition (FER) datasets often suffer from ambiguity and noisy
labels. Ambiguous expressions are challenging to differentiate from expressions
with noisy labels, which hurt the robustness of FER models. Furthermore, the
difficulty of recognition varies across different expression categories,
rendering a uniform approach unfair for all expressions. In this paper, we
introduce a novel approach called Adaptive Sample Mining (ASM) to dynamically
address ambiguity and noise within each expression category. First, the
Adaptive Threshold Learning module generates two thresholds, namely the clean
and noisy thresholds, for each category. These thresholds are based on the mean
class probabilities at each training epoch. Next, the Sample Mining module
partitions the dataset into three subsets: clean, ambiguity, and noise, by
comparing the sample confidence with the clean and noisy thresholds. Finally,
the Tri-Regularization module employs a mutual learning strategy for the
ambiguity subset to enhance discrimination ability, and an unsupervised
learning strategy for the noise subset to mitigate the impact of noisy labels.
Extensive experiments prove that our method can effectively mine both ambiguity
and noise, and outperform SOTA methods on both synthetic noisy and original
datasets. The supplement material is available at
https://github.com/zzzzzzyang/ASM.
</p>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05619" title="Abstract">arXiv:2310.05619</a> [<a href="/pdf/2310.05619" title="Download PDF">pdf</a>, <a href="/format/2310.05619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Top-k Estimation Consolidates Disagreement between Feature  Attribution Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamp%2C+J">Jonathan Kamp</a>, 
<a href="/search/cs?searchtype=author&query=Beinborn%2C+L">Lisa Beinborn</a>, 
<a href="/search/cs?searchtype=author&query=Fokkens%2C+A">Antske Fokkens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Short paper accepted to EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Feature attribution scores are used for explaining the prediction of a text
classifier to users by highlighting a k number of tokens. In this work, we
propose a way to determine the number of optimal k tokens that should be
displayed from sequential properties of the attribution scores. Our approach is
dynamic across sentences, method-agnostic, and deals with sentence length bias.
We compare agreement between multiple methods and humans on an NLI task, using
fixed k and dynamic k. We find that perturbation-based methods and Vanilla
Gradient exhibit highest agreement on most method--method and method--human
agreement metrics with a static k. Their advantage over other methods
disappears with dynamic ks which mainly improve Integrated Gradient and
GradientXInput. To our knowledge, this is the first evidence that sequential
properties of attribution scores are informative for consolidating attribution
signals for human interpretation.
</p>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05620" title="Abstract">arXiv:2310.05620</a> [<a href="/pdf/2310.05620" title="Download PDF">pdf</a>, <a href="/format/2310.05620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LAiW: A Chinese Legal Large Language Models Benchmark (A Technical  Report)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yongfu Dai</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+D">Duanyu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jimin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+H">Haochen Jia</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Weiguang Han</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+W">Wei Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the emergence of numerous legal LLMs, there is currently a lack of a
comprehensive benchmark for evaluating their legal abilities. In this paper, we
propose the first Chinese Legal LLMs benchmark based on legal capabilities.
Through the collaborative efforts of legal and artificial intelligence experts,
we divide the legal capabilities of LLMs into three levels: basic legal NLP
capability, basic legal application capability, and complex legal application
capability. We have completed the first phase of evaluation, which mainly
focuses on the capability of basic legal NLP. The evaluation results show that
although some legal LLMs have better performance than their backbones, there is
still a gap compared to ChatGPT. Our benchmark can be found at URL.
</p>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05624" title="Abstract">arXiv:2310.05624</a> [<a href="/pdf/2310.05624" title="Download PDF">pdf</a>, <a href="/format/2310.05624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locality-Aware Generalizable Implicit Neural Representation}
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Doyup Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C">Chiheon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+M">Minsu Cho</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Wook-Shin Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Generalizable implicit neural representation (INR) enables a single
continuous function, i.e., a coordinate-based neural network, to represent
multiple data instances by modulating its weights or intermediate features
using latent codes. However, the expressive power of the state-of-the-art
modulation is limited due to its inability to localize and capture fine-grained
details of data entities such as specific pixels and rays. To address this
issue, we propose a novel framework for generalizable INR that combines a
transformer encoder with a locality-aware INR decoder. The transformer encoder
predicts a set of latent tokens from a data instance to encode local
information into each latent token. The locality-aware INR decoder extracts a
modulation vector by selectively aggregating the latent tokens via
cross-attention for a coordinate input and then predicts the output by
progressively decoding with coarse-to-fine modulation through multiple
frequency bandwidths. The selective token aggregation and the multi-band
feature modulation enable us to learn locality-aware representation in spatial
and spectral aspects, respectively. Our framework significantly outperforms
previous generalizable INRs and validates the usefulness of the locality-aware
latents for downstream tasks such as image generation.
</p>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05625" title="Abstract">arXiv:2310.05625</a> [<a href="/pdf/2310.05625" title="Download PDF">pdf</a>, <a href="/ps/2310.05625" title="Download PostScript">ps</a>, <a href="/format/2310.05625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximating Sparse Matrices and their Functions using Matrix-vector  products
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Park%2C+T">Taejun Park</a>, 
<a href="/search/math?searchtype=author&query=Nakatsukasa%2C+Y">Yuji Nakatsukasa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The computation of a matrix function $f(A)$ is an important task in
scientific computing appearing in machine learning, network analysis and the
solution of partial differential equations. In this work, we use only
matrix-vector products $x\mapsto Ax$ to approximate functions of sparse
matrices and matrices with similar structures such as sparse matrices $A$
themselves or matrices that have a similar decay property as matrix functions.
We show that when $A$ is a sparse matrix with an unknown sparsity pattern,
techniques from compressed sensing can be used under natural assumptions.
Moreover, if $A$ is a banded matrix then certain deterministic matrix-vector
products can efficiently recover the large entries of $f(A)$. We describe an
algorithm for each of the two cases and give error analysis based on the decay
bound for the entries of $f(A)$. We finish with numerical experiments showing
the accuracy of our algorithms.
</p>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05627" title="Abstract">arXiv:2310.05627</a> [<a href="/pdf/2310.05627" title="Download PDF">pdf</a>, <a href="/format/2310.05627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Stock Features and Global Information via Large Language  Models for Enhanced Stock Return Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yujie Ding</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+S">Shuai Jia</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Tianyi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+B">Bingcheng Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiuze Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liuliu Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dongming Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, International Joint Conferences on Artificial Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Statistical Finance (q-fin.ST)

</div>
<p class="mathjax">The remarkable achievements and rapid advancements of Large Language Models
(LLMs) such as ChatGPT and GPT-4 have showcased their immense potential in
quantitative investment. Traders can effectively leverage these LLMs to analyze
financial news and predict stock returns accurately. However, integrating LLMs
into existing quantitative models presents two primary challenges: the
insufficient utilization of semantic information embedded within LLMs and the
difficulties in aligning the latent information within LLMs with pre-existing
quantitative stock features. We propose a novel framework consisting of two
components to surmount these challenges. The first component, the Local-Global
(LG) model, introduces three distinct strategies for modeling global
information. These approaches are grounded respectively on stock features, the
capabilities of LLMs, and a hybrid method combining the two paradigms. The
second component, Self-Correlated Reinforcement Learning (SCRL), focuses on
aligning the embeddings of financial news generated by LLMs with stock features
within the same semantic space. By implementing our framework, we have
demonstrated superior performance in Rank Information Coefficient and returns,
particularly compared to models relying only on stock features in the China
A-share market.
</p>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05628" title="Abstract">arXiv:2310.05628</a> [<a href="/pdf/2310.05628" title="Download PDF">pdf</a>, <a href="/format/2310.05628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Glitter or Gold? Deriving Structured Insights from Sustainability  Reports via Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bronzini%2C+M">Marco Bronzini</a>, 
<a href="/search/cs?searchtype=author&query=Nicolini%2C+C">Carlo Nicolini</a>, 
<a href="/search/cs?searchtype=author&query=Lepri%2C+B">Bruno Lepri</a>, 
<a href="/search/cs?searchtype=author&query=Passerini%2C+A">Andrea Passerini</a>, 
<a href="/search/cs?searchtype=author&query=Staiano%2C+J">Jacopo Staiano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computational Engineering, Finance, and Science (cs.CE); Computers and Society (cs.CY)

</div>
<p class="mathjax">Over the last decade, several regulatory bodies have started requiring the
disclosure of non-financial information from publicly listed companies, in
light of the investors' increasing attention to Environmental, Social, and
Governance (ESG) issues. Such information is publicly released in a variety of
non-structured and multi-modal documentation. Hence, it is not straightforward
to aggregate and consolidate such data in a cohesive framework to further
derive insights about sustainability practices across companies and markets.
Thus, it is natural to resort to Information Extraction (IE) techniques to
provide concise, informative and actionable data to the stakeholders. Moving
beyond traditional text processing techniques, in this work we leverage Large
Language Models (LLMs), along with prominent approaches such as Retrieved
Augmented Generation and in-context learning, to extract semantically
structured information from sustainability reports. We then adopt graph-based
representations to generate meaningful statistical, similarity and correlation
analyses concerning the obtained findings, highlighting the prominent
sustainability actions undertaken across industries and discussing emerging
similarity and disclosing patterns at company, sector and region levels.
Lastly, we investigate which factual aspects impact the most on companies' ESG
scores using our findings and other company information.
</p>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05632" title="Abstract">arXiv:2310.05632</a> [<a href="/pdf/2310.05632" title="Download PDF">pdf</a>, <a href="/format/2310.05632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Binary Classification with Confidence Difference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Lei Feng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuchen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+G">Gang Niu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min-Ling Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, learning with soft labels has been shown to achieve better
performance than learning with hard labels in terms of model generalization,
calibration, and robustness. However, collecting pointwise labeling confidence
for all training examples can be challenging and time-consuming in real-world
scenarios. This paper delves into a novel weakly supervised binary
classification problem called confidence-difference (ConfDiff) classification.
Instead of pointwise labeling confidence, we are given only unlabeled data
pairs with confidence difference that specifies the difference in the
probabilities of being positive. We propose a risk-consistent approach to
tackle this problem and show that the estimation error bound achieves the
optimal convergence rate. We also introduce a risk correction approach to
mitigate overfitting problems, whose consistency and convergence rate are also
proven. Extensive experiments on benchmark data sets and a real-world
recommender system data set validate the effectiveness of our proposed
approaches in exploiting the supervision information of the confidence
difference.
</p>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05633" title="Abstract">arXiv:2310.05633</a> [<a href="/pdf/2310.05633" title="Download PDF">pdf</a>, <a href="/format/2310.05633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-order geometric integrators for the local cubic variational  Gaussian wavepacket dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fereidani%2C+R+M">Roya Moghaddasi Fereidani</a>, 
<a href="/search/math?searchtype=author&query=Van%C3%AD%C4%8Dek%2C+J+J">Ji&#x159;&#xed; JL Van&#xed;&#x10d;ek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph); Quantum Physics (quant-ph)

</div>
<p class="mathjax">Gaussian wavepacket dynamics has proven to be a useful semiclassical
approximation for quantum simulations of high-dimensional systems with low
anharmonicity. Compared to Heller's original local harmonic method, the
variational Gaussian wavepacket dynamics is more accurate, but much more
difficult to apply in practice because it requires evaluating the expectation
values of the potential energy, gradient, and Hessian. If the variational
approach is applied to the local cubic approximation of the potential, these
expectation values can be evaluated analytically, but still require the costly
third derivative of the potential. To reduce the cost of the resulting local
cubic variational Gaussian wavepacket dynamics, we describe efficient
high-order geometric integrators, which are symplectic, time-reversible, and
norm-conserving. For small time steps, they also conserve the effective energy.
We demonstrate the efficiency and geometric properties of these integrators
numerically on a multi-dimensional, nonseparable coupled Morse potential.
</p>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05634" title="Abstract">arXiv:2310.05634</a> [<a href="/pdf/2310.05634" title="Download PDF">pdf</a>, <a href="/format/2310.05634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Verifiable Generation: A Benchmark for Knowledge-aware Language  Model Attribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinze Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao2%2C+Y">Yixin Cao2</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liangming Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yubo Ma</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+A">Aixin Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Although achieving great success, Large Language Models (LLMs) usually suffer
from unreliable hallucinations. In this paper, we define a new task of
Knowledge-aware Language Model Attribution (KaLMA) that improves upon three
core concerns on conventional attributed LMs. First, we extend attribution
source from unstructured texts to Knowledge Graph (KG), whose rich structures
benefit both the attribution performance and working scenarios. Second, we
propose a new ``Conscious Incompetence" setting considering the incomplete
knowledge repository, where the model identifies the need for supporting
knowledge beyond the provided KG. Third, we propose a comprehensive automatic
evaluation metric encompassing text quality, citation quality, and text
citation alignment. To implement the above innovations, we build a dataset in
biography domain BioKaLMA via a well-designed evolutionary question generation
strategy, to control the question complexity and necessary knowledge to the
answer. For evaluation, we develop a baseline solution and demonstrate the room
for improvement in LLMs' citation generation, emphasizing the importance of
incorporating the "Conscious Incompetence" setting, and the critical role of
retrieval accuracy.
</p>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05636" title="Abstract">arXiv:2310.05636</a> [<a href="/pdf/2310.05636" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secure Expansion of Energy Storage and Transmission Lines Considering  Bundling Option Under Renewable Penetration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Moradi-Sepahvand%2C+M">Mojtaba Moradi-Sepahvand</a>, 
<a href="/search/eess?searchtype=author&query=Amraee%2C+T">Turaj Amraee</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Energy, 2023, 347, p.121414
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper presents a multi-stage expansion model for the co-planning of
transmission lines, battery energy storage (ES), and wind power plants (WPP).
High penetration of renewable energy sources (RES) is integrated into the
proposed model concerning renewable portfolio standard (RPS) policy goals. The
possibility of bundling existing transmission lines to uprate power flow
capacity is considered. Renewable energy curtailment and load shedding are
included in the model to assess the system operation more precisely. Battery ES
devices are co-planned to defer transmission expansion and renewable
management. To make the time complexity of the problem tractable and capture
the uncertainties of load and RES in an hourly resolution, a chronological
time-period clustering algorithm is used to extract the representative hours of
each planning stage. Additionally, the flexible ramp reserve is utilized to
handle the uncertainty of RES. An accelerated Benders dual decomposition (BDD)
algorithm is developed to solve the proposed model mixed-integer linear
programming (MILP) formulation. The N-1 security criterion is evaluated by
considering a designed contingency screening (CS) algorithm to identify higher
risk contingencies. The effectiveness of the proposed co-planning model is
evaluated using IEEE RTS 24-bus and IEEE 118-bus test systems.
</p>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05641" title="Abstract">arXiv:2310.05641</a> [<a href="/pdf/2310.05641" title="Download PDF">pdf</a>, <a href="/format/2310.05641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mathematical problems and solutions of the Ninth International Olympiad  in Cryptography NSUCRYPTO
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Idrisova%2C+V+A">V.A. Idrisova</a>, 
<a href="/search/cs?searchtype=author&query=Tokareva%2C+N+N">N.N. Tokareva</a>, 
<a href="/search/cs?searchtype=author&query=Gorodilova%2C+A+A">A.A. Gorodilova</a>, 
<a href="/search/cs?searchtype=author&query=Beterov%2C+I+I">I.I. Beterov</a>, 
<a href="/search/cs?searchtype=author&query=Bonich%2C+T+A">T.A. Bonich</a>, 
<a href="/search/cs?searchtype=author&query=Ishchukova%2C+E+A">E.A. Ishchukova</a>, 
<a href="/search/cs?searchtype=author&query=Kolomeec%2C+N+A">N.A. Kolomeec</a>, 
<a href="/search/cs?searchtype=author&query=Kutsenko%2C+A+V">A.V. Kutsenko</a>, 
<a href="/search/cs?searchtype=author&query=Malygina%2C+E+S">E.S. Malygina</a>, 
<a href="/search/cs?searchtype=author&query=Pankratova%2C+I+A">I.A. Pankratova</a>, 
<a href="/search/cs?searchtype=author&query=Pudovkina%2C+M+A">M.A. Pudovkina</a>, 
<a href="/search/cs?searchtype=author&query=Udovenko%2C+A+N">A.N. Udovenko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Every year the International Olympiad in Cryptography Non-Stop University
CRYPTO (NSUCRYPTO) offers mathematical problems for university and school
students and, moreover, for professionals in the area of cryptography and
computer science. The mail goal of NSUCRYPTO is to draw attention of students
and young researchers to modern cryptography and raise awareness about open
problems in the field. We present problems of NSUCRYPTO'22 and their solutions.
There are 16 problems on the following topics: ciphers, cryptosystems,
protocols, e-money and cryptocurrencies, hash functions, matrices, quantum
computing, S-boxes, etc. They vary from easy mathematical tasks that could be
solved by school students to open problems that deserve separate discussion and
study. So, in this paper, we consider several open problems on three-pass
protocols, public and private keys pairs, modifications of discrete logarithm
problem, cryptographic permutations and quantum circuits.
</p>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05642" title="Abstract">arXiv:2310.05642</a> [<a href="/pdf/2310.05642" title="Download PDF">pdf</a>, <a href="/format/2310.05642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plug n&#x27; Play: Channel Shuffle Module for Enhancing Tiny Vision  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xuwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yudong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiajun Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Vision Transformers (ViTs) have demonstrated remarkable performance in
various computer vision tasks. However, the high computational complexity
hinders ViTs' applicability on devices with limited memory and computing
resources. Although certain investigations have delved into the fusion of
convolutional layers with self-attention mechanisms to enhance the efficiency
of ViTs, there remains a knowledge gap in constructing tiny yet effective ViTs
solely based on the self-attention mechanism. Furthermore, the straightforward
strategy of reducing the feature channels in a large but outperforming ViT
often results in significant performance degradation despite improved
efficiency. To address these challenges, we propose a novel channel shuffle
module to improve tiny-size ViTs, showing the potential of pure self-attention
models in environments with constrained computing resources. Inspired by the
channel shuffle design in ShuffleNetV2 \cite{ma2018shufflenet}, our module
expands the feature channels of a tiny ViT and partitions the channels into two
groups: the \textit{Attended} and \textit{Idle} groups. Self-attention
computations are exclusively employed on the designated \textit{Attended}
group, followed by a channel shuffle operation that facilitates information
exchange between the two groups. By incorporating our module into a tiny ViT,
we can achieve superior performance while maintaining a comparable
computational complexity to the vanilla model. Specifically, our proposed
channel shuffle module consistently improves the top-1 accuracy on the
ImageNet-1K dataset for various tiny ViT models by up to 2.8\%, with the
changes in model complexity being less than 0.03 GMACs.
</p>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05643" title="Abstract">arXiv:2310.05643</a> [<a href="/pdf/2310.05643" title="Download PDF">pdf</a>, <a href="/format/2310.05643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLAID: Closing the Loop on AI &amp; Data Collection -- A Cross-Platform  Transparent Computing Middleware Framework for Smart Edge-Cloud and Digital  Biomarker Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Langer%2C+P">Patrick Langer</a>, 
<a href="/search/cs?searchtype=author&query=Fleisch%2C+E">Elgar Fleisch</a>, 
<a href="/search/cs?searchtype=author&query=Barata%2C+F">Filipe Barata</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">The increasing number of edge devices with enhanced sensing capabilities,
such as smartphones, wearables, and IoT devices equipped with sensors, holds
the potential for innovative smart-edge applications in healthcare. These
devices generate vast amounts of multimodal data, enabling the implementation
of digital biomarkers which can be leveraged by machine learning solutions to
derive insights, predict health risks, and allow personalized interventions.
Training these models requires collecting data from edge devices and
aggregating it in the cloud. To validate and verify those models, it is
essential to utilize them in real-world scenarios and subject them to testing
using data from diverse cohorts. Since some models are too computationally
expensive to be run on edge devices directly, a collaborative framework between
the edge and cloud becomes necessary. In this paper, we present CLAID, an
open-source cross-platform middleware framework based on transparent computing
compatible with Android, iOS, WearOS, Linux, macOS, and Windows. CLAID enables
logical integration of devices running different operating systems into an
edge-cloud system, facilitating communication and offloading between them, with
bindings available in different programming languages. We provide Modules for
data collection from various sensors as well as for the deployment of
machine-learning models. Furthermore, we propose a novel methodology, "ML-Model
in the Loop" for verifying deployed machine learning models, which helps to
analyze problems that may occur during the migration of models from cloud to
edge devices. We verify our framework in three different experiments and
achieve 100% sampling coverage for data collection across different sensors as
well as an equal performance of a cough detection model deployed on both
Android and iOS devices. We evaluate the memory and battery consumption of our
framework.
</p>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05644" title="Abstract">arXiv:2310.05644</a> [<a href="/pdf/2310.05644" title="Download PDF">pdf</a>, <a href="/format/2310.05644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diagnosing Catastrophe: Large parts of accuracy loss in continual  learning can be accounted for by readout misalignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anthes%2C+D">Daniel Anthes</a>, 
<a href="/search/cs?searchtype=author&query=Thorat%2C+S">Sushrut Thorat</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6nig%2C+P">Peter K&#xf6;nig</a>, 
<a href="/search/cs?searchtype=author&query=Kietzmann%2C+T+C">Tim C. Kietzmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, 1 figure; published at the 2023 Conference on Cognitive Computational Neuroscience
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Unlike primates, training artificial neural networks on changing data
distributions leads to a rapid decrease in performance on old tasks. This
phenomenon is commonly referred to as catastrophic forgetting. In this paper,
we investigate the representational changes that underlie this performance
decrease and identify three distinct processes that together account for the
phenomenon. The largest component is a misalignment between hidden
representations and readout layers. Misalignment occurs due to learning on
additional tasks and causes internal representations to shift. Representational
geometry is partially conserved under this misalignment and only a small part
of the information is irrecoverably lost. All types of representational changes
scale with the dimensionality of hidden representations. These insights have
implications for deep learning applications that need to be continuously
updated, but may also aid aligning ANN models to the rather robust biological
vision.
</p>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05648" title="Abstract">arXiv:2310.05648</a> [<a href="/pdf/2310.05648" title="Download PDF">pdf</a>, <a href="/ps/2310.05648" title="Download PostScript">ps</a>, <a href="/format/2310.05648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unifying a posteriori error analysis of five piecewise quadratic  discretisations for the biharmonic equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Carstensen%2C+C">Carsten Carstensen</a>, 
<a href="/search/math?searchtype=author&query=Gr%C3%A4%C3%9Fle%2C+B">Benedikt Gr&#xe4;&#xdf;le</a>, 
<a href="/search/math?searchtype=author&query=Nataraj%2C+N">Neela Nataraj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be published in: Journal of Numerical Mathematics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">An abstract property (H) is the key to a complete a priori error analysis in
the (discrete) energy norm for several nonstandard finite element methods in
the recent work [Lowest-order equivalent nonstandard finite element methods for
biharmonic plates, Carstensen and Nataraj, M2AN, 2022]. This paper investigates
the impact of (H) to the a posteriori error analysis and establishes known and
novel explicit residual-based a posteriori error estimates. The abstract
framework applies to Morley, two versions of discontinuous Galerkin, $C^0$
interior penalty, as well as weakly over-penalized symmetric interior penalty
schemes for the biharmonic equation with a general source term in
$H^{-2}(\Omega)$.
</p>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05649" title="Abstract">arXiv:2310.05649</a> [<a href="/pdf/2310.05649" title="Download PDF">pdf</a>, <a href="/format/2310.05649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context, Composition, Automation, and Communication -- The C2AC Roadmap  for Modeling and Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uhrmacher%2C+A">Adelinde Uhrmacher</a>, 
<a href="/search/cs?searchtype=author&query=Frazier%2C+P">Peter Frazier</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A4hnle%2C+R">Reiner H&#xe4;hnle</a>, 
<a href="/search/cs?searchtype=author&query=Kl%C3%BCgl%2C+F">Franziska Kl&#xfc;gl</a>, 
<a href="/search/cs?searchtype=author&query=Lorig%2C+F">Fabian Lorig</a>, 
<a href="/search/cs?searchtype=author&query=Lud%C3%A4scher%2C+B">Bertram Lud&#xe4;scher</a>, 
<a href="/search/cs?searchtype=author&query=Nenzi%2C+L">Laura Nenzi</a>, 
<a href="/search/cs?searchtype=author&query=Ruiz-Martin%2C+C">Cristina Ruiz-Martin</a>, 
<a href="/search/cs?searchtype=author&query=Rumpe%2C+B">Bernhard Rumpe</a>, 
<a href="/search/cs?searchtype=author&query=Szabo%2C+C">Claudia Szabo</a>, 
<a href="/search/cs?searchtype=author&query=Wainer%2C+G+A">Gabriel A. Wainer</a>, 
<a href="/search/cs?searchtype=author&query=Wilsdorf%2C+P">Pia Wilsdorf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Simulation has become, in many application areas, a sine-qua-non. Most
recently, COVID-19 has underlined the importance of simulation studies but also
limitations in current practices and methods. We identify four goals of
methodological work for addressing these limitations. The first is to provide
better support for capturing, representing, and evaluating the context of
simulation studies, including research questions, assumptions, requirements,
and activities that contributed to a simulation study and the generated
artifacts, and how this contribution took place. In addition, the composition
of simulation models and other products of simulation studies needs to be
supported beyond syntactical coherence, including aspects of semantics and
purpose, enabling their effective reuse. A higher degree of automating
simulation studies will contribute to more systematic, standardized,
qualitative simulation studies and their efficiency. Finally, it is essential
to invest increased effort into effectively communicating results and the
involved processes of simulation studies to enable their use in research and
for decision-making. These goals are not pursued independently of each other,
but they will benefit from and sometimes even rely on advances in each
subfield. In the present paper, we explore the basis and interdependencies
evident in current research and practice and delineate future research
directions based on these considerations.
</p>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05650" title="Abstract">arXiv:2310.05650</a> [<a href="/pdf/2310.05650" title="Download PDF">pdf</a>, <a href="/format/2310.05650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RAUCG: Retrieval-Augmented Unsupervised Counter Narrative Generation for  Hate Speech
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shuyu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Wenyi Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xingshu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tanga%2C+R">Rui Tanga</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haizhou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxian Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The Counter Narrative (CN) is a promising approach to combat online hate
speech (HS) without infringing on freedom of speech. In recent years, there has
been a growing interest in automatically generating CNs using natural language
generation techniques. However, current automatic CN generation methods mainly
rely on expert-authored datasets for training, which are time-consuming and
labor-intensive to acquire. Furthermore, these methods cannot directly obtain
and extend counter-knowledge from external statistics, facts, or examples. To
address these limitations, we propose Retrieval-Augmented Unsupervised Counter
Narrative Generation (RAUCG) to automatically expand external counter-knowledge
and map it into CNs in an unsupervised paradigm. Specifically, we first
introduce an SSF retrieval method to retrieve counter-knowledge from the
multiple perspectives of stance consistency, semantic overlap rate, and fitness
for HS. Then we design an energy-based decoding mechanism by quantizing
knowledge injection, countering and fluency constraints into differentiable
functions, to enable the model to build mappings from counter-knowledge to CNs
without expert-authored CN data. Lastly, we comprehensively evaluate model
performance in terms of language quality, toxicity, persuasiveness, relevance,
and success rate of countering HS, etc. Experimental results show that RAUCG
outperforms strong baselines on all metrics and exhibits stronger
generalization capabilities, achieving significant improvements of +2.0% in
relevance and +4.5% in success rate of countering metrics. Moreover, RAUCG
enabled GPT2 to outperform T0 in all metrics, despite the latter being
approximately eight times larger than the former. Warning: This paper may
contain offensive or upsetting content!
</p>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05651" title="Abstract">arXiv:2310.05651</a> [<a href="/pdf/2310.05651" title="Download PDF">pdf</a>, <a href="/format/2310.05651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FENCE: Fairplay Ensuring Network Chain Entity for Real-Time Multiple ID  Detection at Scale In Fantasy Sports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Upreti%2C+A">Akriti Upreti</a>, 
<a href="/search/cs?searchtype=author&query=Kothari%2C+K">Kartavya Kothari</a>, 
<a href="/search/cs?searchtype=author&query=Thukral%2C+U">Utkarsh Thukral</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+V">Vishal Verma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures, accepted in AIML Systems 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Dream11 takes pride in being a unique platform that enables over 190 million
fantasy sports users to demonstrate their skills and connect deeper with their
favorite sports. While managing such a scale, one issue we are faced with is
duplicate/multiple account creation in the system. This is done by some users
with the intent of abusing the platform, typically for bonus offers. The
challenge is to detect these multiple accounts before it is too late. We
propose a graph-based solution to solve this problem in which we first predict
edges/associations between users. Using the edge information we highlight
clusters of colluding multiple accounts. In this paper, we talk about our
distributed ML system which is deployed to serve and support the inferences
from our detection models. The challenge is to do this in real-time in order to
take corrective actions. A core part of this setup also involves
human-in-the-loop components for validation, feedback, and ground-truth
labeling.
</p>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05654" title="Abstract">arXiv:2310.05654</a> [<a href="/pdf/2310.05654" title="Download PDF">pdf</a>, <a href="/format/2310.05654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No Token Left Behind: Efficient Vision Transformer via Dynamic Token  Idling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xuwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Changlin Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yudong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xiaojun Chang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiajun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sen Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AJCAI2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Vision Transformers (ViTs) have demonstrated outstanding performance in
computer vision tasks, yet their high computational complexity prevents their
deployment in computing resource-constrained environments. Various token
pruning techniques have been introduced to alleviate the high computational
burden of ViTs by dynamically dropping image tokens. However, some undesirable
pruning at early stages may result in permanent loss of image information in
subsequent layers, consequently hindering model performance. To address this
problem, we propose IdleViT, a dynamic token-idle-based method that achieves an
excellent trade-off between performance and efficiency. Specifically, in each
layer, IdleViT selects a subset of the image tokens to participate in
computations while keeping the rest of the tokens idle and directly passing
them to this layer's output. By allowing the idle tokens to be re-selected in
the following layers, IdleViT mitigates the negative impact of improper pruning
in the early stages. Furthermore, inspired by the normalized graph cut, we
devise a token cut loss on the attention map as regularization to improve
IdleViT's token selection ability. Our method is simple yet effective and can
be extended to pyramid ViTs since no token is completely dropped. Extensive
experimental results on various ViT architectures have shown that IdleViT can
diminish the complexity of pretrained ViTs by up to 33\% with no more than
0.2\% accuracy decrease on ImageNet, after finetuning for only 30 epochs.
Notably, when the keep ratio is 0.5, IdleViT outperforms the state-of-the-art
EViT on DeiT-S by 0.5\% higher accuracy and even faster inference speed. The
source code is available in the supplementary material.
</p>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05657" title="Abstract">arXiv:2310.05657</a> [<a href="/pdf/2310.05657" title="Download PDF">pdf</a>, <a href="/format/2310.05657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Closer Look into Automatic Evaluation Using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiang%2C+C">Cheng-Han Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 findings (short paper). Code: <a href="https://github.com/d223302/A-Closer-Look-To-LLM-Evaluation/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Using large language models (LLMs) to evaluate text quality has recently
gained popularity. Some prior works explore the idea of using LLMs for
evaluation, while they differ in some details of the evaluation process. In
this paper, we analyze LLM evaluation (Chiang and Lee, 2023) and G-Eval (Liu et
al., 2023), and we discuss how those details in the evaluation process change
how well the ratings given by LLMs correlate with human ratings. We find that
the auto Chain-of-Thought (CoT) used in G-Eval does not always make G-Eval more
aligned with human ratings. We also show that forcing the LLM to output only a
numeric rating, as in G-Eval, is suboptimal. Last, we reveal that asking the
LLM to explain its own ratings consistently improves the correlation between
the ChatGPT and human ratings and pushes state-of-the-art (SoTA) correlations
on two meta-evaluation datasets.
</p>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05664" title="Abstract">arXiv:2310.05664</a> [<a href="/pdf/2310.05664" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViTs are Everywhere: A Comprehensive Study Showcasing Vision  Transformers in Different Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mia%2C+M+S">Md Sohag Mia</a>, 
<a href="/search/cs?searchtype=author&query=Arnob%2C+A+B+H">Abu Bakor Hayat Arnob</a>, 
<a href="/search/cs?searchtype=author&query=Naim%2B%2C+A">Abdu Naim+</a>, 
<a href="/search/cs?searchtype=author&query=Voban%2C+A+A+B">Abdullah Al Bary Voban</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+S">Md Shariful Islam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Transformer design is the de facto standard for natural language processing
tasks. The success of the transformer design in natural language processing has
lately piqued the interest of researchers in the domain of computer vision.
When compared to Convolutional Neural Networks (CNNs), Vision Transformers
(ViTs) are becoming more popular and dominant solutions for many vision
problems. Transformer-based models outperform other types of networks, such as
convolutional and recurrent neural networks, in a range of visual benchmarks.
We evaluate various vision transformer models in this work by dividing them
into distinct jobs and examining their benefits and drawbacks. ViTs can
overcome several possible difficulties with convolutional neural networks
(CNNs). The goal of this survey is to show the first use of ViTs in CV. In the
first phase, we categorize various CV applications where ViTs are appropriate.
Image classification, object identification, image segmentation, video
transformer, image denoising, and NAS are all CV applications. Our next step
will be to analyze the state-of-the-art in each area and identify the models
that are currently available. In addition, we outline numerous open research
difficulties as well as prospective research possibilities.
</p>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05666" title="Abstract">arXiv:2310.05666</a> [<a href="/pdf/2310.05666" title="Download PDF">pdf</a>, <a href="/format/2310.05666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anchor-Intermediate Detector: Decoupling and Coupling Bounding Boxes for  Accurate Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+Y">Yilong Lv</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Min Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yujie He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shaopeng Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhuzhen He</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+A">Aitao Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted 29 September, 2023; originally announced October 2023. Accepted by ICCV2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Anchor-based detectors have been continuously developed for object detection.
However, the individual anchor box makes it difficult to predict the boundary's
offset accurately. Instead of taking each bounding box as a closed individual,
we consider using multiple boxes together to get prediction boxes. To this end,
this paper proposes the \textbf{Box Decouple-Couple(BDC) strategy} in the
inference, which no longer discards the overlapping boxes, but decouples the
corner points of these boxes. Then, according to each corner's score, we couple
the corner points to select the most accurate corner pairs. To meet the BDC
strategy, a simple but novel model is designed named the
\textbf{Anchor-Intermediate Detector(AID)}, which contains two head networks,
i.e., an anchor-based head and an anchor-free \textbf{Corner-aware head}. The
corner-aware head is able to score the corners of each bounding box to
facilitate the coupling between corner points. Extensive experiments on MS COCO
show that the proposed anchor-intermediate detector respectively outperforms
their baseline RetinaNet and GFL method by $\sim$2.4 and $\sim$1.2 AP on the MS
COCO test-dev dataset without any bells and whistles. Code is available at:
https://github.com/YilongLv/AID.
</p>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05668" title="Abstract">arXiv:2310.05668</a> [<a href="/pdf/2310.05668" title="Download PDF">pdf</a>, <a href="/format/2310.05668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised  Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feiyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qing%2C+Z">Zhen Qing</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yingying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Shuiguang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yi Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+G">Guansong Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Most of current anomaly detection models assume that the normal pattern
remains same all the time. However, the normal patterns of Web services change
dramatically and frequently. The model trained on old-distribution data is
outdated after such changes. Retraining the whole model every time is
expensive. Besides, at the beginning of normal pattern changes, there is not
enough observation data from the new distribution. Retraining a large neural
network model with limited data is vulnerable to overfitting. Thus, we propose
a Light and Anti-overfitting Retraining Approach (LARA) for deep variational
auto-encoder based time series anomaly detection methods (VAEs). This work aims
to make three novel contributions: 1) the retraining process is formulated as a
convex problem and can converge at a fast rate as well as prevent overfitting;
2) designing a ruminate block, which leverages the historical data without the
need to store them; 3) mathematically proving that when fine-tuning the latent
vector and reconstructed data, the linear formations can achieve the least
adjusting errors between the ground truths and the fine-tuned ones.
<br />Moreover, we have performed many experiments to verify that retraining LARA
with even 43 time slots of data from new distribution can result in its
competitive F1 Score in comparison with the state-of-the-art anomaly detection
models trained with sufficient data. Besides, we verify its light overhead.
</p>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05670" title="Abstract">arXiv:2310.05670</a> [<a href="/pdf/2310.05670" title="Download PDF">pdf</a>, <a href="/format/2310.05670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement learning for freeform robot design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Muhan Li</a>, 
<a href="/search/cs?searchtype=author&query=Matthews%2C+D">David Matthews</a>, 
<a href="/search/cs?searchtype=author&query=Kriegman%2C+S">Sam Kriegman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Inspired by the necessity of morphological adaptation in animals, a growing
body of work has attempted to expand robot training to encompass physical
aspects of a robot's design. However, reinforcement learning methods capable of
optimizing the 3D morphology of a robot have been restricted to reorienting or
resizing the limbs of a predetermined and static topological genus. Here we
show policy gradients for designing freeform robots with arbitrary external and
internal structure. This is achieved through actions that deposit or remove
bundles of atomic building blocks to form higher-level nonparametric
macrostructures such as appendages, organs and cavities. Although results are
provided for open loop control only, we discuss how this method could be
adapted for closed loop control and sim2real transfer to physical machines in
future.
</p>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05672" title="Abstract">arXiv:2310.05672</a> [<a href="/pdf/2310.05672" title="Download PDF">pdf</a>, <a href="/format/2310.05672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-timestep models for Model-based Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benechehab%2C+A">Abdelhakim Benechehab</a>, 
<a href="/search/cs?searchtype=author&query=Paolo%2C+G">Giuseppe Paolo</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+A">Albert Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Filippone%2C+M">Maurizio Filippone</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A9gl%2C+B">Bal&#xe1;zs K&#xe9;gl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In model-based reinforcement learning (MBRL), most algorithms rely on
simulating trajectories from one-step dynamics models learned on data. A
critical challenge of this approach is the compounding of one-step prediction
errors as length of the trajectory grows. In this paper we tackle this issue by
using a multi-timestep objective to train one-step models. Our objective is a
weighted sum of a loss function (e.g., negative log-likelihood) at various
future horizons. We explore and test a range of weights profiles. We find that
exponentially decaying weights lead to models that significantly improve the
long-horizon R2 score. This improvement is particularly noticeable when the
models were evaluated on noisy data. Finally, using a soft actor-critic (SAC)
agent in pure batch reinforcement learning (RL) and iterated batch RL
scenarios, we found that our multi-timestep models outperform or match standard
one-step models. This was especially evident in a noisy variant of the
considered environment, highlighting the potential of our approach in
real-world applications.
</p>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05674" title="Abstract">arXiv:2310.05674</a> [<a href="/pdf/2310.05674" title="Download PDF">pdf</a>, <a href="/format/2310.05674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Scalable Meta Learning Practical
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choe%2C+S+K">Sang Keun Choe</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+S+V">Sanket Vaibhav Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+H">Hwijeen Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Neiswanger%2C+W">Willie Neiswanger</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+P">Pengtao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Strubell%2C+E">Emma Strubell</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E">Eric Xing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite its flexibility to learn diverse inductive biases in machine learning
programs, meta learning (i.e., learning to learn) has long been recognized to
suffer from poor scalability due to its tremendous compute/memory costs,
training instability, and a lack of efficient distributed training support. In
this work, we focus on making scalable meta learning practical by introducing
SAMA, which combines advances in both implicit differentiation algorithms and
systems. Specifically, SAMA is designed to flexibly support a broad range of
adaptive optimizers in the base level of meta learning programs, while reducing
computational burden by avoiding explicit computation of second-order gradient
information, and exploiting efficient distributed training techniques
implemented for first-order gradients. Evaluated on multiple large-scale meta
learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and
2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU
setups compared to other baseline meta learning algorithms. Furthermore, we
show that SAMA-based data optimization leads to consistent improvements in text
classification accuracy with BERT and RoBERTa large language models, and
achieves state-of-the-art results in both small- and large-scale data pruning
on image classification tasks, demonstrating the practical applicability of
scalable meta learning across language and vision domains.
</p>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05677" title="Abstract">arXiv:2310.05677</a> [<a href="/pdf/2310.05677" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Construction of stock molecular system and the popularization of Density  Functional Theory in stock market
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huajian Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Longjian Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiajian Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures, 1 table, 18 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Over the past two decades, some scholars have noticed the correlation between
quantum mechanics and finance/economy, making some novel attempts to introduce
the theoretical framework of quantum mechanics into financial and economic
research, subsequently a new research domain called quantum finance or quantum
economy was set up. In particular, some studies have made their endeavour in
the stock market, utilizing the quantum mechanical paradigm to describe the
movement of stock price. Nevertheless, the majority of researches have paid
attention to describing the motion of a single stock, and drawn an analogy
between the motion of a single stock and a one-dimensional infinite well, or
one-dimensional harmonic oscillator model, whose modality looks alike to the
one-electron Schr\"odinger equation, in which the information is solved
analytically in most cases. Hitherto, the whole stock market system composed of
all stocks and stock indexes have not been discussed. In this paper, the
concept of stock molecular system is first proposed with pioneer. The modality
of stock molecular system resembles the multi-electrons Schr\"odinger equation
with Born-Oppenheimer approximation. Similar to the interaction among all
nuclei and electrons in a molecule, the interaction exist among all stock
indexes and stocks. This paper also establish the stock-index Coulomb
potential, stock-index Coulomb potential, stock-stock Coulomb potential and
stock coulomb correlation terms by statistical theory. At length, the conceive
and feasibility of drawing upon density functional theory (DFT) to solve the
Schr\"odinger equation of stock molecular system are put forward together with
proof, ending up with experiments executed in CSI 300 index system.
</p>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05679" title="Abstract">arXiv:2310.05679</a> [<a href="/pdf/2310.05679" title="Download PDF">pdf</a>, <a href="/ps/2310.05679" title="Download PostScript">ps</a>, <a href="/format/2310.05679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A temporal weight analysis and novel nonlinear weights of weighted  essentially non-oscillatory schemes for hyperbolic conservation laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+X">Xinjuan Chen</a>, 
<a href="/search/math?searchtype=author&query=Gu%2C+J">Jiaxi Gu</a>, 
<a href="/search/math?searchtype=author&query=Jung%2C+J">Jae-Hun Jung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper we analyze the weighted essentially non-oscillatory (WENO)
schemes in the finite volume framework by examining the first step of the
explicit third-order total variation diminishing Runge-Kutta method. The
rationale for the improved performance of the finite volume WENO-M, WENO-Z and
WENO-ZR schemes over WENO-JS in the first time step is that the nonlinear
weights corresponding to large errors are adjusted to increase the accuracy of
numerical solutions. Based on this analysis, we propose novel Z-type nonlinear
weights of the finite volume WENO scheme for hyperbolic conservation laws.
Instead of taking the difference of the smoothness indicators for the global
smoothness indicator, we employ the logarithmic function with tuners to ensure
that the numerical dissipation is reduced around discontinuities while the
essentially non-oscillatory property is preserved. The proposed scheme does not
necessitate substantial extra computational expenses. Numerical examples are
presented to demonstrate the capability of the proposed WENO scheme in shock
capturing.
</p>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05680" title="Abstract">arXiv:2310.05680</a> [<a href="/pdf/2310.05680" title="Download PDF">pdf</a>, <a href="/ps/2310.05680" title="Download PostScript">ps</a>, <a href="/format/2310.05680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Argument Generation from Legal Facts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tuvey%2C+O">Oscar Tuvey</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+P">Procheta Sen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The count of pending cases has shown an exponential rise across nations
(e.g., with more than 10 million pending cases in India alone). The main issue
lies in the fact that the number of cases submitted to the law system is far
greater than the available number of legal professionals present in a country.
Given this worldwide context, the utilization of AI technology has gained
paramount importance to enhance the efficiency and speed of legal procedures.
In this study we partcularly focus on helping legal professionals in the
process of analyzing a legal case. Our specific investigation delves into
harnessing the generative capabilities of open-sourced large language models to
create arguments derived from the facts present in legal cases. Experimental
results show that the generated arguments from the best performing method have
on average 63% overlap with the benchmark set gold standard annotations.
</p>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05682" title="Abstract">arXiv:2310.05682</a> [<a href="/pdf/2310.05682" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Rainfall Variability and Water Extent of Selected Hydropower  Reservoir Using Google Earth Engine (GEE): A Case Study from Two Tropical  Countries, Sri Lanka and Vietnam
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rajakaruna%2C+P">Punsisi Rajakaruna</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Surajit Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Holmatov%2C+B">Bunyod Holmatov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This study presents a comprehensive remote sensing analysis of rainfall
patterns and selected hydropower reservoir water extent in two tropical monsoon
countries, Vietnam and Sri Lanka. The aim is to understand the relationship
between remotely sensed rainfall data and the dynamic changes (monthly) in
reservoir water extent. The analysis utilizes high-resolution optical imagery
and Sentinel-1 Synthetic Aperture Radar (SAR) data to observe and monitor water
bodies during different weather conditions, especially during the monsoon
season. The average annual rainfall for both countries is determined, and
spatiotemporal variations in monthly average rainfall are examined at regional
and reservoir basin levels using the Climate Hazards Group InfraRed
Precipitation with Station (CHIRPS) dataset from 1981 to 2022. Water extents
are derived for selected reservoirs using Sentinel-1 SAR Ground Range Detected
(GRD) images in Vietnam and Sri Lanka from 2017 to 2022. The images are
pre-processed and corrected using terrain correction and refined Lee filter. An
automated thresholding algorithm, OTSU, distinguishes water and land, taking
advantage of both VV and VH polarization data. The connected pixel count
threshold is applied to enhance result accuracy. The results indicate a clear
relationship between rainfall patterns and reservoir water extent, with
increased precipitation during the monsoon season leading to higher water
extents in the later months. This study contributes to understanding how
rainfall variability impacts reservoir water resources in tropical monsoon
regions. The preliminary findings can inform water resource management
strategies and support these countries' decision-making processes related to
hydropower generation, flood management, and irrigation.
</p>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05686" title="Abstract">arXiv:2310.05686</a> [<a href="/pdf/2310.05686" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The potential of large language models for improving probability  learning: A study on ChatGPT3.5 and first-year computer engineering students
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Udias%2C+A">Angel Udias</a>, 
<a href="/search/cs?searchtype=author&query=Alonso-Ayuso%2C+A">Antonio Alonso-Ayuso</a>, 
<a href="/search/cs?searchtype=author&query=Sanchez%2C+I">Ignacio Sanchez</a>, 
<a href="/search/cs?searchtype=author&query=Hernandez%2C+S">Sonia Hernandez</a>, 
<a href="/search/cs?searchtype=author&query=Castellanos%2C+M+E">Maria Eugenia Castellanos</a>, 
<a href="/search/cs?searchtype=author&query=Diez%2C+R+M">Raquel Montes Diez</a>, 
<a href="/search/cs?searchtype=author&query=Cano%2C+E+L">Emilio Lopez Cano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we assess the efficacy of ChatGPT (version Feb 2023), a
large-scale language model, in solving probability problems typically presented
in introductory computer engineering exams. Our study comprised a set of 23
probability exercises administered to students at Rey Juan Carlos University
(URJC) in Madrid. The responses produced by ChatGPT were evaluated by a group
of five statistics professors, who assessed them qualitatively and assigned
grades based on the same criteria used for students. Our results indicate that
ChatGPT surpasses the average student in terms of phrasing, organization, and
logical reasoning. The model's performance remained consistent for both the
Spanish and English versions of the exercises. However, ChatGPT encountered
difficulties in executing basic numerical operations. Our experiments
demonstrate that requesting ChatGPT to provide the solution in the form of an R
script proved to be an effective approach for overcoming these limitations. In
summary, our results indicate that ChatGPT surpasses the average student in
solving probability problems commonly presented in introductory computer
engineering exams. Nonetheless, the model exhibits limitations in reasoning
around certain probability concepts. The model's ability to deliver
high-quality explanations and illustrate solutions in any programming language,
coupled with its performance in solving probability exercises, suggests that
large language models have the potential to serve as learning assistants.
</p>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05687" title="Abstract">arXiv:2310.05687</a> [<a href="/pdf/2310.05687" title="Download PDF">pdf</a>, <a href="/format/2310.05687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A review of the security role of ISP mandated ONUs and ONTs in GPONs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Franke%2C+M">Max Franke</a>, 
<a href="/search/cs?searchtype=author&query=Neef%2C+S">Sebastian Neef</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Home fiber connections are largely realized by using passive optical
networks, in their most common form today relying on the GPON standard. Among
other things, this standard specifies how the first node inside of customers'
homes, the so called ONU or ONT, has to behave, and which security features
have to be supported. Currently, customers in some European countries,
including Germany, have freedom of choice between using terminal equipment
provided by the ISP or a self-selected open market device.We analyze the
security implications resulting from this freedom of choice and whether or not
ISP-mandated hardware would increase the security of the GPON. Our review
reveals that there are no differences between an ISP-mandated ONU/ONT and a
standard conforming subscriber-selected ONU/ONT that would justify the security
based recommendation of an ISP-mandated ONU/ONT.
</p>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05688" title="Abstract">arXiv:2310.05688</a> [<a href="/pdf/2310.05688" title="Download PDF">pdf</a>, <a href="/format/2310.05688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Larth: Dataset and Machine Translation for Etruscan
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vico%2C+G">Gianluca Vico</a>, 
<a href="/search/cs?searchtype=author&query=Spanakis%2C+G">Gerasimos Spanakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Etruscan is an ancient language spoken in Italy from the 7th century BC to
the 1st century AD. There are no native speakers of the language at the present
day, and its resources are scarce, as there exist only around 12,000 known
inscriptions. To the best of our knowledge, there are no publicly available
Etruscan corpora for natural language processing. Therefore, we propose a
dataset for machine translation from Etruscan to English, which contains 2891
translated examples from existing academic sources. Some examples are extracted
manually, while others are acquired in an automatic way. Along with the
dataset, we benchmark different machine translation models observing that it is
possible to achieve a BLEU score of 10.1 with a small transformer model.
Releasing the dataset can help enable future research on this language, similar
languages or other languages with scarce resources.
</p>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05689" title="Abstract">arXiv:2310.05689</a> [<a href="/pdf/2310.05689" title="Download PDF">pdf</a>, <a href="/format/2310.05689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Opinion Dynamics Model with Higher-Order Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wanyue Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhongzhi Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by IEEE Transactions on Computational Social Systems(TCSS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Opinion dynamics is a central subject of computational social science, and
various models have been developed to understand the evolution and formulation
of opinions. Existing models mainly focus on opinion dynamics on graphs that
only capture pairwise interactions between agents. In this paper, we extend the
popular Friedkin-Johnsen model for opinion dynamics on graphs to hypergraphs,
which describe higher-order interactions occurring frequently on real networks,
especially social networks. To achieve this, based on the fact that for linear
dynamics the multi-way interactions can be reduced to effective pairwise node
interactions, we propose a method to decode the group interactions encoded in
hyperedges by undirected edges or directed edges in graphs. We then show that
higher-order interactions play an important role in the opinion dynamics, since
the overall steady-state expressed opinion and polarization differ greatly from
those without group interactions. We also provide an interpretation of the
equilibrium expressed opinion from the perspective of the spanning converging
forest, based on which we design a fast sampling algorithm to approximately
evaluate the overall opinion and opinion polarization on directed weighted
graphs. Finally, we conduct experiments on real-world hypergraph datasets,
demonstrating the performance of our algorithm.
</p>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05690" title="Abstract">arXiv:2310.05690</a> [<a href="/pdf/2310.05690" title="Download PDF">pdf</a>, <a href="/format/2310.05690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Abstractive Summarization of Large Document Collections Using GPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sengjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Healey%2C+C+G">Christopher G. Healey</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This paper proposes a method of abstractive summarization designed to scale
to document collections instead of individual documents. Our approach applies a
combination of semantic clustering, document size reduction within topic
clusters, semantic chunking of a cluster's documents, GPT-based summarization
and concatenation, and a combined sentiment and text visualization of each
topic to support exploratory data analysis. Statistical comparison of our
results to existing state-of-the-art systems BART, BRIO, PEGASUS, and MoCa
using ROGUE summary scores showed statistically equivalent performance with
BART and PEGASUS on the CNN/Daily Mail test dataset, and with BART on the
Gigaword test dataset. This finding is promising since we view document
collection summarization as more challenging than individual document
summarization. We conclude with a discussion of how issues of scale are
</p>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05691" title="Abstract">arXiv:2310.05691</a> [<a href="/pdf/2310.05691" title="Download PDF">pdf</a>, <a href="/format/2310.05691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Climate-sensitive Urban Planning through Optimization of Tree Placements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schrodi%2C+S">Simon Schrodi</a>, 
<a href="/search/cs?searchtype=author&query=Briegel%2C+F">Ferdinand Briegel</a>, 
<a href="/search/cs?searchtype=author&query=Argus%2C+M">Max Argus</a>, 
<a href="/search/cs?searchtype=author&query=Christen%2C+A">Andreas Christen</a>, 
<a href="/search/cs?searchtype=author&query=Brox%2C+T">Thomas Brox</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">Climate change is increasing the intensity and frequency of many extreme
weather events, including heatwaves, which results in increased thermal
discomfort and mortality rates. While global mitigation action is undoubtedly
necessary, so is climate adaptation, e.g., through climate-sensitive urban
planning. Among the most promising strategies is harnessing the benefits of
urban trees in shading and cooling pedestrian-level environments. Our work
investigates the challenge of optimal placement of such trees. Physical
simulations can estimate the radiative and thermal impact of trees on human
thermal comfort but induce high computational costs. This rules out
optimization of tree placements over large areas and considering effects over
longer time scales. Hence, we employ neural networks to simulate the point-wise
mean radiant temperatures--a driving factor of outdoor human thermal
comfort--across various time scales, spanning from daily variations to extended
time scales of heatwave events and even decades. To optimize tree placements,
we harness the innate local effect of trees within the iterated local search
framework with tailored adaptations. We show the efficacy of our approach
across a wide spectrum of study areas and time scales. We believe that our
approach is a step towards empowering decision-makers, urban designers and
planners to proactively and effectively assess the potential of urban trees to
mitigate heat stress.
</p>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05692" title="Abstract">arXiv:2310.05692</a> [<a href="/pdf/2310.05692" title="Download PDF">pdf</a>, <a href="/format/2310.05692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Based on What We Can Control Artificial Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+C">Cheng Kang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+X">Xujing Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages,
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">How can the stability and efficiency of Artificial Neural Networks (ANNs) be
ensured through a systematic analysis method? This paper seeks to address that
query. While numerous factors can influence the learning process of ANNs,
utilizing knowledge from control systems allows us to analyze its system
function and simulate system responses. Although the complexity of most ANNs is
extremely high, we still can analyze each factor (e.g., optimiser,
hyperparameters) by simulating their system response. This new method also can
potentially benefit the development of new optimiser and learning system,
especially when discerning which components adversely affect ANNs. Controlling
ANNs can benefit from the design of optimiser and learning system, as (1) all
optimisers act as controllers, (2) all learning systems operate as control
systems with inputs and outputs, and (3) the optimiser should match the
learning system. Please find codes:
\url{https://github.com/RandomUserName2023/Control-ANNs}.
</p>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05694" title="Abstract">arXiv:2310.05694</a> [<a href="/pdf/2310.05694" title="Download PDF">pdf</a>, <a href="/format/2310.05694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Large Language Models for Healthcare: from Data, Technology,  and Applications to Accountability and Ethics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+K">Kai He</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+R">Rui Mao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qika Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+Y">Yucheng Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+X">Xiang Lan</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+M">Mengling Feng</a>, 
<a href="/search/cs?searchtype=author&query=Cambria%2C+E">Erik Cambria</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The utilization of large language models (LLMs) in the Healthcare domain has
generated both excitement and concern due to their ability to effectively
respond to freetext queries with certain professional knowledge. This survey
outlines the capabilities of the currently developed LLMs for Healthcare and
explicates their development process, with the aim of providing an overview of
the development roadmap from traditional Pretrained Language Models (PLMs) to
LLMs. Specifically, we first explore the potential of LLMs to enhance the
efficiency and effectiveness of various Healthcare applications highlighting
both the strengths and limitations. Secondly, we conduct a comparison between
the previous PLMs and the latest LLMs, as well as comparing various LLMs with
each other. Then we summarize related Healthcare training data, training
methods, optimization strategies, and usage. Finally, the unique concerns
associated with deploying LLMs in Healthcare settings are investigated,
particularly regarding fairness, accountability, transparency and ethics. Our
survey provide a comprehensive investigation from perspectives of both computer
science and Healthcare specialty. Besides the discussion about Healthcare
concerns, we supports the computer science community by compiling a collection
of open source resources, such as accessible datasets, the latest
methodologies, code implementations, and evaluation benchmarks in the Github.
Summarily, we contend that a significant paradigm shift is underway,
transitioning from PLMs to LLMs. This shift encompasses a move from
discriminative AI approaches to generative AI approaches, as well as a shift
from model-centered methodologies to datacentered methodologies.
</p>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05695" title="Abstract">arXiv:2310.05695</a> [<a href="/pdf/2310.05695" title="Download PDF">pdf</a>, <a href="/format/2310.05695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Reinforcement Learning for Temporal Pattern Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Johnson%2C+F">Faith Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Dana%2C+K">Kristin Dana</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this work, we explore the use of hierarchical reinforcement learning (HRL)
for the task of temporal sequence prediction. Using a combination of deep
learning and HRL, we develop a stock agent to predict temporal price sequences
from historical stock price data and a vehicle agent to predict steering angles
from first person, dash cam images. Our results in both domains indicate that a
type of HRL, called feudal reinforcement learning, provides significant
improvements to training speed and stability and prediction accuracy over
standard RL. A key component to this success is the multi-resolution structure
that introduces both temporal and spatial abstraction into the network
hierarchy.
</p>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05696" title="Abstract">arXiv:2310.05696</a> [<a href="/pdf/2310.05696" title="Download PDF">pdf</a>, <a href="/format/2310.05696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Protecting Sensitive Data through Federated Co-Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abourayya%2C+A">Amr Abourayya</a>, 
<a href="/search/cs?searchtype=author&query=Kleesiek%2C+J">Jens Kleesiek</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+K">Kanishka Rao</a>, 
<a href="/search/cs?searchtype=author&query=Ayday%2C+E">Erman Ayday</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+B">Bharat Rao</a>, 
<a href="/search/cs?searchtype=author&query=Webb%2C+G">Geoff Webb</a>, 
<a href="/search/cs?searchtype=author&query=Kamp%2C+M">Michael Kamp</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In many critical applications, sensitive data is inherently distributed.
Federated learning trains a model collaboratively by aggregating the parameters
of locally trained models. This avoids exposing sensitive local data. It is
possible, though, to infer upon the sensitive data from the shared model
parameters. At the same time, many types of machine learning models do not lend
themselves to parameter aggregation, such as decision trees, or rule ensembles.
It has been observed that in many applications, in particular healthcare, large
unlabeled datasets are publicly available. They can be used to exchange
information between clients by distributed distillation, i.e., co-regularizing
local training via the discrepancy between the soft predictions of each local
client on the unlabeled dataset. This, however, still discloses private
information and restricts the types of models to those trainable via
gradient-based methods. We propose to go one step further and use a form of
federated co-training, where local hard labels on the public unlabeled datasets
are shared and aggregated into a consensus label. This consensus label can be
used for local training by any supervised machine learning model. We show that
this federated co-training approach achieves a model quality comparable to both
federated learning and distributed distillation on a set of benchmark datasets
and real-world medical datasets. It improves privacy over both approaches,
protecting against common membership inference attacks to the highest degree.
Furthermore, we show that federated co-training can collaboratively train
interpretable models, such as decision trees and rule ensembles, achieving a
model quality comparable to centralized training.
</p>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05697" title="Abstract">arXiv:2310.05697</a> [<a href="/pdf/2310.05697" title="Download PDF">pdf</a>, <a href="/format/2310.05697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining recurrent and residual learning for deforestation monitoring  using multitemporal SAR images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neves%2C+C+N">Carla Nascimento Neves</a>, 
<a href="/search/cs?searchtype=author&query=Feitosa%2C+R+Q">Raul Queiroz Feitosa</a>, 
<a href="/search/cs?searchtype=author&query=Adarme%2C+M+X+O">Mabel X. Ortega Adarme</a>, 
<a href="/search/cs?searchtype=author&query=Giraldi%2C+G+A">Gilson Antonio Giraldi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 19 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With its vast expanse, exceeding that of Western Europe by twice, the Amazon
rainforest stands as the largest forest of the Earth, holding immense
importance in global climate regulation. Yet, deforestation detection from
remote sensing data in this region poses a critical challenge, often hindered
by the persistent cloud cover that obscures optical satellite data for much of
the year. Addressing this need, this paper proposes three deep-learning models
tailored for deforestation monitoring, utilizing SAR (Synthetic Aperture Radar)
multitemporal data moved by its independence on atmospheric conditions.
Specifically, the study proposes three novel recurrent fully convolutional
network architectures-namely, RRCNN-1, RRCNN-2, and RRCNN-3, crafted to enhance
the accuracy of deforestation detection. Additionally, this research explores
replacing a bitemporal with multitemporal SAR sequences, motivated by the
hypothesis that deforestation signs quickly fade in SAR images over time. A
comprehensive assessment of the proposed approaches was conducted using a
Sentinel-1 multitemporal sequence from a sample site in the Brazilian
rainforest. The experimental analysis confirmed that analyzing a sequence of
SAR images over an observation period can reveal deforestation spots
undetectable in a pair of images. Notably, experimental results underscored the
superiority of the multitemporal approach, yielding approximately a five
percent enhancement in F1-Score across all tested network architectures.
Particularly the RRCNN-1 achieved the highest accuracy and also boasted half
the processing time of its closest counterpart.
</p>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05699" title="Abstract">arXiv:2310.05699</a> [<a href="/pdf/2310.05699" title="Download PDF">pdf</a>, <a href="/format/2310.05699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uni3DETR: Unified 3D Detection Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yali Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hengshuang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shengjin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing point cloud based 3D detectors are designed for the particular
scene, either indoor or outdoor ones. Because of the substantial differences in
object distribution and point density within point clouds collected from
various environments, coupled with the intricate nature of 3D metrics, there is
still a lack of a unified network architecture that can accommodate diverse
scenes. In this paper, we propose Uni3DETR, a unified 3D detector that
addresses indoor and outdoor 3D detection within the same framework.
Specifically, we employ the detection transformer with point-voxel interaction
for object prediction, which leverages voxel features and points for
cross-attention and behaves resistant to the discrepancies from data. We then
propose the mixture of query points, which sufficiently exploits global
information for dense small-range indoor scenes and local information for
large-range sparse outdoor ones. Furthermore, our proposed decoupled IoU
provides an easy-to-optimize training target for localization by disentangling
the xy and z space. Extensive experiments validate that Uni3DETR exhibits
excellent performance consistently on both indoor and outdoor 3D detection. In
contrast to previous specialized detectors, which may perform well on some
particular datasets but suffer a substantial degradation on different scenes,
Uni3DETR demonstrates the strong generalization ability under heterogeneous
conditions (Fig. 1).
<br />Codes are available at
\href{https://github.com/zhenyuw16/Uni3DETR}{https://github.com/zhenyuw16/Uni3DETR}.
</p>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05701" title="Abstract">arXiv:2310.05701</a> [<a href="/pdf/2310.05701" title="Download PDF">pdf</a>, <a href="/format/2310.05701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physical Oscillator Model for Supercomputing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Afzal%2C+A">Ayesha Afzal</a>, 
<a href="/search/cs?searchtype=author&query=Hager%2C+G">Georg Hager</a>, 
<a href="/search/cs?searchtype=author&query=Wellein%2C+G">Gerhard Wellein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">A parallel program together with the parallel hardware it is running on is
not only a vehicle to solve numerical problems, it is also a complex system
with interesting dynamical behavior: resynchronization and desynchronization of
parallel processes, propagating phases of idleness, and the peculiar effects of
noise and system topology are just a few examples. We propose a physical
oscillator model (POM) to describe aspects of the dynamics of interacting
parallel processes. Motivated by the well-known Kuramoto Model, a process with
its regular compute-communicate cycles is modeled as an oscillator which is
coupled to other oscillators (processes) via an interaction potential. Instead
of a simple all-to-all connectivity, we employ a sparse topology matrix mapping
the communication structure and thus the inter-process dependencies of the
program onto the oscillator model and propose two interaction potentials that
are suitable for different scenarios in parallel computing: resource-scalable
and resource-bottlenecked applications. The former are not limited by a
resource bottleneck such as memory bandwidth or network contention, while the
latter are. Unlike the original Kuramoto model, which has a periodic sinusoidal
potential that is attractive for small angles, our characteristic potentials
are always attractive for large angles and only differ in the short-distance
behavior. We show that the model with appropriate potentials can mimic the
propagation of delays and the synchronizing and desynchronizing behavior of
scalable and bottlenecked parallel programs, respectively.
</p>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05703" title="Abstract">arXiv:2310.05703</a> [<a href="/pdf/2310.05703" title="Download PDF">pdf</a>, <a href="/format/2310.05703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Attribution Method for Siamese Encoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%B6ller%2C+L">Lucas M&#xf6;ller</a>, 
<a href="/search/cs?searchtype=author&query=Nikolaev%2C+D">Dmitry Nikolaev</a>, 
<a href="/search/cs?searchtype=author&query=Pad%C3%B3%2C+S">Sebastian Pad&#xf3;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite the success of Siamese encoder models such as sentence transformers
(ST), little is known about the aspects of inputs they pay attention to. A
barrier is that their predictions cannot be attributed to individual features,
as they compare two inputs rather than processing a single one. This paper
derives a local attribution method for Siamese encoders by generalizing the
principle of integrated gradients to models with multiple inputs. The solution
takes the form of feature-pair attributions, and can be reduced to a
token-token matrix for STs. Our method involves the introduction of integrated
Jacobians and inherits the advantageous formal properties of integrated
gradients: it accounts for the model's full computation graph and is guaranteed
to converge to the actual prediction. A pilot study shows that in an ST few
token-pairs can often explain large fractions of predictions, and it focuses on
nouns and verbs. For accurate predictions, it however needs to attend to the
majority of tokens and parts of speech.
</p>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05707" title="Abstract">arXiv:2310.05707</a> [<a href="/pdf/2310.05707" title="Download PDF">pdf</a>, <a href="/format/2310.05707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guiding Language Model Reasoning with Planning Tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Caccia%2C+L">Lucas Caccia</a>, 
<a href="/search/cs?searchtype=author&query=Ostapenko%2C+O">Oleksiy Ostapenko</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xingdi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Sordoni%2C+A">Alessandro Sordoni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have recently attracted considerable interest
for their ability to perform complex reasoning tasks, such as chain-of-thought
reasoning. However, most of the existing approaches to enhance this ability
rely heavily on data-driven methods, while neglecting the structural aspects of
the model's reasoning capacity. We find that while LLMs can manage individual
reasoning steps well, they struggle with maintaining consistency across an
entire reasoning chain. To solve this, we introduce 'planning tokens' at the
start of each reasoning step, serving as a guide for the model. These token
embeddings are then fine-tuned along with the rest of the model parameters. Our
approach requires a negligible increase in trainable parameters (just 0.001%)
and can be applied through either full fine-tuning or a more
parameter-efficient scheme. We demonstrate our method's effectiveness by
applying it to three different LLMs, showing notable accuracy improvements
across three math word problem datasets w.r.t. plain chain-of-thought
fine-tuning baselines.
</p>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05710" title="Abstract">arXiv:2310.05710</a> [<a href="/pdf/2310.05710" title="Download PDF">pdf</a>, <a href="/format/2310.05710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiCE -- A Data Encryption Proxy for the Cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koppenwallner%2C+J">Johannes Koppenwallner</a>, 
<a href="/search/cs?searchtype=author&query=Schikuta%2C+E">Erich Schikuta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Databases (cs.DB)

</div>
<p class="mathjax">Outsourcing a relational database to the cloud offers several benefits,
including scalability, availability, and cost-effectiveness. However, there are
concerns about the confidentiality and security of the outsourced data. A
general approach here would be to encrypt the data with a standardized
encryption algorithm and then store the data only encrypted in the cloud. The
problem with this approach, however, is that with encryption, important
properties of the data such as sorting, format or comparability, which are
essential for the functioning of database queries, are lost. One solution to
this problem is the use of (e.g. order-preserving) encryption algorithms, which
also preserve these properties in the encrypted data, thus enabling queries to
encrypted data. These algorithms range from simple algorithms like Caesar
encryption to secure algorithms like mOPE. In order to be able to use these
algorithms as easy as possible, ``DiCE'' a JDBC driver was developed, that
parses SQL queries as a proxy and transparently encrypts and decrypts these
queries. This allows to execute many queries on an encrypted database in the
cloud with (nearly) the performance as on unencrypted databases. The DiCE
driver can be used with any other JDBC driver and therefore supports a variety
of databases. The driver can be configured to support different encryption
algorithms. To keep track of the operations, the ``Dice Information Client''
has been developed to track the encryption and decryption of the driver.
Although the result of the performance analysis shows a certain overhead due to
the parsing and encryption of the SQL queries in the Dice driver, this overhead
is significantly reduced by other influencing factors such as the network and
parallel queries.
</p>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05711" title="Abstract">arXiv:2310.05711</a> [<a href="/pdf/2310.05711" title="Download PDF">pdf</a>, <a href="/format/2310.05711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expressive Quantale-valued Logics for Coalgebras: an Adjunction-based  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beohar%2C+H">Harsh Beohar</a>, 
<a href="/search/cs?searchtype=author&query=Gurke%2C+S">Sebastian Gurke</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6nig%2C+B">Barbara K&#xf6;nig</a>, 
<a href="/search/cs?searchtype=author&query=Messing%2C+K">Karla Messing</a>, 
<a href="/search/cs?searchtype=author&query=Forster%2C+J">Jonas Forster</a>, 
<a href="/search/cs?searchtype=author&query=Schr%C3%B6der%2C+L">Lutz Schr&#xf6;der</a>, 
<a href="/search/cs?searchtype=author&query=Wild%2C+P">Paul Wild</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We address the task of deriving fixpoint equations from modal logics
characterizing behavioural equivalences and metrics (summarized under the term
conformances). We rely on earlier work that obtains Hennessy-Milner theorems as
corollaries to a fixpoint preservation property along Galois connections
between suitable lattices. We instantiate this to the setting of coalgebras, in
which we spell out the compatibility property ensuring that we can derive a
behaviour function whose greatest fixpoint coincides with the logical
conformance. We then concentrate on the linear-time case, for which we study
coalgebras based on the machine functor living in Eilenberg-Moore categories, a
scenario for which we obtain a particularly simple logic and fixpoint equation.
The theory is instantiated to concrete examples, both in the branching-time
case (bisimilarity and behavioural metrics) and in the linear-time case (trace
equivalences and trace distances).
</p>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05712" title="Abstract">arXiv:2310.05712</a> [<a href="/pdf/2310.05712" title="Download PDF">pdf</a>, <a href="/format/2310.05712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Imitator Learning: Achieve Out-of-the-Box Imitation Ability in Variable  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiong-Hui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Junyin Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yi-Chen Li</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Haoran Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yu-Yan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Zhihao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Si-Hang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+A">Anqi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zongzhang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yang Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Imitation learning (IL) enables agents to mimic expert behaviors. Most
previous IL techniques focus on precisely imitating one policy through mass
demonstrations. However, in many applications, what humans require is the
ability to perform various tasks directly through a few demonstrations of
corresponding tasks, where the agent would meet many unexpected changes when
deployed. In this scenario, the agent is expected to not only imitate the
demonstration but also adapt to unforeseen environmental changes.
<br />This motivates us to propose a new topic called imitator learning (ItorL),
which aims to derive an imitator module that can on-the-fly reconstruct the
imitation policies based on very limited expert demonstrations for different
unseen tasks, without any extra adjustment. In this work, we focus on imitator
learning based on only one expert demonstration. To solve ItorL, we propose
Demo-Attention Actor-Critic (DAAC), which integrates IL into a
reinforcement-learning paradigm that can regularize policies' behaviors in
unexpected situations. Besides, for autonomous imitation policy building, we
design a demonstration-based attention architecture for imitator policy that
can effectively output imitated actions by adaptively tracing the suitable
states in demonstrations. We develop a new navigation benchmark and a robot
environment for \topic~and show that DAAC~outperforms previous imitation
methods \textit{with large margins} both on seen and unseen tasks.
</p>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05714" title="Abstract">arXiv:2310.05714</a> [<a href="/pdf/2310.05714" title="Download PDF">pdf</a>, <a href="/format/2310.05714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DecAP: Decaying Action Priors for Accelerated Learning of Torque-Based  Legged Locomotion Policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sood%2C+S">Shivam Sood</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Ge Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peizhuo Li</a>, 
<a href="/search/cs?searchtype=author&query=Sartoretti%2C+G">Guillaume Sartoretti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the IEEE International Conference on Robotics and Automation (ICRA 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Optimal Control for legged robots has gone through a paradigm shift from
position-based to torque-based control, owing to the latter's compliant and
robust nature. In parallel to this shift, the community has also turned to Deep
Reinforcement Learning (DRL) as a promising approach to directly learn
locomotion policies for complex real-life tasks. However, most end-to-end DRL
approaches still operate in position space, mainly because learning in torque
space is often sample-inefficient and does not consistently converge to natural
gaits. To address these challenges, we introduce Decaying Action Priors
(DecAP), a novel three-stage framework to learn and deploy torque policies for
legged locomotion. In the first stage, we generate our own imitation data by
training a position policy, eliminating the need for expert knowledge in
designing optimal controllers. The second stage incorporates decaying action
priors to enhance the exploration of torque-based policies aided by imitation
rewards. We show that our approach consistently outperforms imitation learning
alone and is significantly robust to the scaling of these rewards. Finally, our
third stage facilitates safe sim-to-real transfer by directly deploying our
learned torques, alongside low-gain PID control from our trained position
policy. We demonstrate the generality of our approach by training torque-based
locomotion policies for a biped, a quadruped, and a hexapod robot in
simulation, and experimentally demonstrate our learned policies on a quadruped
(Unitree Go1).
</p>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05717" title="Abstract">arXiv:2310.05717</a> [<a href="/pdf/2310.05717" title="Download PDF">pdf</a>, <a href="/format/2310.05717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STOPNet: Multiview-based 6-DoF Suction Detection for Transparent Objects  on Production Lines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuang%2C+Y">Yuxuan Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Q">Qin Han</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Danshi Li</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Q">Qiyu Dai</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Lian Ding</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+D">Dong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hanlin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">He Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review. ICRA 2024 submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this work, we present STOPNet, a framework for 6-DoF object suction
detection on production lines, with a focus on but not limited to transparent
objects, which is an important and challenging problem in robotic systems and
modern industry. Current methods requiring depth input fail on transparent
objects due to depth cameras' deficiency in sensing their geometry, while we
proposed a novel framework to reconstruct the scene on the production line
depending only on RGB input, based on multiview stereo. Compared to existing
works, our method not only reconstructs the whole 3D scene in order to obtain
high-quality 6-DoF suction poses in real time but also generalizes to novel
environments, novel arrangements and novel objects, including challenging
transparent objects, both in simulation and the real world. Extensive
experiments in simulation and the real world show that our method significantly
surpasses the baselines and has better generalizability, which caters to
practical industrial needs.
</p>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05718" title="Abstract">arXiv:2310.05718</a> [<a href="/pdf/2310.05718" title="Download PDF">pdf</a>, <a href="/format/2310.05718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational  Autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baykal%2C+G">Gulcin Baykal</a>, 
<a href="/search/cs?searchtype=author&query=Kandemir%2C+M">Melih Kandemir</a>, 
<a href="/search/cs?searchtype=author&query=Unal%2C+G">Gozde Unal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Codebook collapse is a common problem in training deep generative models with
discrete representation spaces like Vector Quantized Variational Autoencoders
(VQ-VAEs). We observe that the same problem arises for the alternatively
designed discrete variational autoencoders (dVAEs) whose encoder directly
learns a distribution over the codebook embeddings to represent the data. We
hypothesize that using the softmax function to obtain a probability
distribution causes the codebook collapse by assigning overconfident
probabilities to the best matching codebook elements. In this paper, we propose
a novel way to incorporate evidential deep learning (EDL) instead of softmax to
combat the codebook collapse problem of dVAE. We evidentially monitor the
significance of attaining the probability distribution over the codebook
embeddings, in contrast to softmax usage. Our experiments using various
datasets show that our model, called EdVAE, mitigates codebook collapse while
improving the reconstruction performance, and enhances the codebook usage
compared to dVAE and VQ-VAE based models.
</p>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05719" title="Abstract">arXiv:2310.05719</a> [<a href="/pdf/2310.05719" title="Download PDF">pdf</a>, <a href="/format/2310.05719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer Fusion with Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imfeld%2C+M">Moritz Imfeld</a> (1), 
<a href="/search/cs?searchtype=author&query=Graldi%2C+J">Jacopo Graldi</a> (1), 
<a href="/search/cs?searchtype=author&query=Giordano%2C+M">Marco Giordano</a> (1), 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+T">Thomas Hofmann</a> (1), 
<a href="/search/cs?searchtype=author&query=Anagnostidis%2C+S">Sotiris Anagnostidis</a> (1), 
<a href="/search/cs?searchtype=author&query=Singh%2C+S+P">Sidak Pal Singh</a> (1) ((1) ETH Zurich)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> M. Imfeld, J. Graldi, and M. Giordano are the first authors and contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Fusion is a technique for merging multiple independently-trained neural
networks in order to combine their capabilities. Past attempts have been
restricted to the case of fully-connected, convolutional, and residual
networks. In this paper, we present a systematic approach for fusing two or
more transformer-based networks exploiting Optimal Transport to (soft-)align
the various architectural components. We flesh out an abstraction for layer
alignment, that can generalize to arbitrary architectures -- in principle --
and we apply this to the key ingredients of Transformers such as multi-head
self-attention, layer-normalization, and residual connections, and we discuss
how to handle them via various ablation studies. Furthermore, our method allows
the fusion of models of different sizes (heterogeneous fusion), providing a new
and efficient way for compression of Transformers. The proposed approach is
evaluated on both image classification tasks via Vision Transformer and natural
language modeling tasks using BERT. Our approach consistently outperforms
vanilla fusion, and, after a surprisingly short finetuning, also outperforms
the individual converged parent models. In our analysis, we uncover intriguing
insights about the significant role of soft alignment in the case of
Transformers. Our results showcase the potential of fusing multiple
Transformers, thus compounding their expertise, in the budding paradigm of
model fusion and recombination.
</p>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05720" title="Abstract">arXiv:2310.05720</a> [<a href="/pdf/2310.05720" title="Download PDF">pdf</a>, <a href="/format/2310.05720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperLips: Hyper Control Lips with High Resolution Decoder for Talking  Face Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yaosen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Han Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xuming Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Talking face generation has a wide range of potential applications in the
field of virtual digital humans. However, rendering high-fidelity facial video
while ensuring lip synchronization is still a challenge for existing
audio-driven talking face generation approaches. To address this issue, we
propose HyperLips, a two-stage framework consisting of a hypernetwork for
controlling lips and a high-resolution decoder for rendering high-fidelity
faces.In the first stage, we construct a base face generation network that uses
the hypernetwork to control the encoding latent code of the visual face
information over audio. First, FaceEncoder is used to obtain latent code by
extracting features from the visual face information taken from the video
source containing the face frame.Then, HyperConv, which weighting parameters
are updated by HyperNet with the audio features as input, will modify the
latent code to synchronize the lip movement with the audio. Finally,
FaceDecoder will decode the modified and synchronized latent code into visual
face content. In the second stage, we obtain higher quality face videos through
a high-resolution decoder. To further improve the quality of face generation,
we trained a high-resolution decoder, HRDecoder, using face images and detected
sketches generated from the first stage as input.Extensive quantitative and
qualitative experiments show that our method outperforms state-of-the-art work
with more realistic, high-fidelity, and lip synchronization. Project page:
https://semchan.github.io/HyperLips/
</p>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05723" title="Abstract">arXiv:2310.05723</a> [<a href="/pdf/2310.05723" title="Download PDF">pdf</a>, <a href="/format/2310.05723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McInroe%2C+T">Trevor McInroe</a>, 
<a href="/search/cs?searchtype=author&query=Albrecht%2C+S+V">Stefano V. Albrecht</a>, 
<a href="/search/cs?searchtype=author&query=Storkey%2C+A">Amos Storkey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 12 figures, preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Offline pretraining with a static dataset followed by online fine-tuning
(offline-to-online, or OtO) is a paradigm that is well matched to a real-world
RL deployment process: in few real settings would one deploy an offline policy
with no test runs and tuning. In this scenario, we aim to find the
best-performing policy within a limited budget of online interactions. Previous
work in the OtO setting has focused on correcting for bias introduced by the
policy-constraint mechanisms of offline RL algorithms. Such constraints keep
the learned policy close to the behavior policy that collected the dataset, but
this unnecessarily limits policy performance if the behavior policy is far from
optimal. Instead, we forgo policy constraints and frame OtO RL as an
exploration problem: we must maximize the benefit of the online
data-collection. We study major online RL exploration paradigms, adapting them
to work well with the OtO setting. These adapted methods contribute several
strong baselines. Also, we introduce an algorithm for planning to go out of
distribution (PTGOOD), which targets online exploration in relatively
high-reward regions of the state-action space unlikely to be visited by the
behavior policy. By leveraging concepts from the Conditional Entropy
Bottleneck, PTGOOD encourages data collected online to provide new information
relevant to improving the final deployment policy. In that way the limited
interaction budget is used effectively. We show that PTGOOD significantly
improves agent returns during online fine-tuning and finds the optimal policy
in as few as 10k online steps in Walker and in as few as 50k in complex control
tasks like Humanoid. Also, we find that PTGOOD avoids the suboptimal policy
convergence that many of our baselines exhibit in several environments.
</p>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05727" title="Abstract">arXiv:2310.05727</a> [<a href="/pdf/2310.05727" title="Download PDF">pdf</a>, <a href="/format/2310.05727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Program Testing Ability of Large Language Models for Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Weimin Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yiwen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
<p class="mathjax">Recent development of large language models (LLMs) for code like CodeX and
CodeT5+ demonstrates tremendous promise in achieving code intelligence. Their
ability of synthesizing code that completes a program for performing a
pre-defined task has been intensively tested and verified on benchmark datasets
including HumanEval and MBPP. Yet, evaluation of these LLMs from more
perspectives (than just program synthesis) is also anticipated, considering
their broad scope of applications in software engineering. In this paper, we
explore the ability of LLMs for testing programs/code. By performing thorough
analyses of recent LLMs for code in program testing, we show a series of
intriguing properties of these models and demonstrate how program testing
ability of LLMs can be improved. Following recent work which utilizes generated
test cases to enhance program synthesis, we further leverage our findings in
improving the quality of the synthesized programs and show +11.77% and +4.22%
higher code pass rates on HumanEval+ comparing with the GPT-3.5-turbo baseline
and the recent state-of-the-art, respectively.
</p>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05728" title="Abstract">arXiv:2310.05728</a> [<a href="/pdf/2310.05728" title="Download PDF">pdf</a>, <a href="/format/2310.05728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hidden Permutations to the Rescue: Multi-Pass Streaming Lower Bounds for  Approximate Matchings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Assadi%2C+S">Sepehr Assadi</a>, 
<a href="/search/cs?searchtype=author&query=Sundaresan%2C+J">Janani Sundaresan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 84 pages, 19 figures; to appear in FOCS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">We prove that any semi-streaming algorithm for $(1-\epsilon)$-approximation
of maximum bipartite matching requires \[
\Omega(\frac{\log{(1/\epsilon)}}{{\log{(1/\beta)}}}) \] passes, where $\beta
\in (0,1)$ is the largest parameter so that an $n$-vertex graph with
$n^{\beta}$ edge-disjoint induced matchings of size $\Theta(n)$ exist (such
graphs are referred to as RS graphs). Currently, it is known that \[
\Omega(\frac{1}{\log\log{n}}) \leq \beta \leq
1-\Theta(\frac{\log^*{n}}{{\log{n}}}) \] and closing this huge gap between
upper and lower bounds has remained a notoriously difficult problem in
combinatorics.
<br />Under the plausible hypothesis that $\beta = \Omega(1)$, our lower bound
result provides the first pass-approximation lower bound for (small) constant
approximation of matchings in the semi-streaming model, a longstanding open
question in the graph streaming literature.
<br />Our techniques are based on analyzing communication protocols for compressing
(hidden) permutations. Prior work in this context relied on reducing such
problems to Boolean domain and analyzing them via tools like XOR Lemmas and
Fourier analysis on Boolean hypercube. In contrast, our main technical
contribution is a hardness amplification result for permutations through
concatenation in place of prior XOR Lemmas. This result is proven by analyzing
permutations directly via simple tools from group representation theory
combined with detailed information-theoretic arguments, and can be of
independent interest.
</p>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05731" title="Abstract">arXiv:2310.05731</a> [<a href="/pdf/2310.05731" title="Download PDF">pdf</a>, <a href="/format/2310.05731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Blockchain-driven Architecture for Usage Control in Solid
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Basile%2C+D">Davide Basile</a>, 
<a href="/search/cs?searchtype=author&query=Di+Ciccio%2C+C">Claudio Di Ciccio</a>, 
<a href="/search/cs?searchtype=author&query=Goretti%2C+V">Valerio Goretti</a>, 
<a href="/search/cs?searchtype=author&query=Kirrane%2C+S">Sabrina Kirrane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Decentralization initiatives like Solid enable data owners to control who has
access to their data and to stimulate innovation by creating both application
and data markets. Once data owners share their data with others, though, it is
no longer possible for them to control how their data are used. To address this
issue, we propose a usage control architecture to monitor compliance with usage
control policies. To this end, our solution relies on blockchain and trusted
execution environments. We demonstrate the potential of the architecture by
describing the various workflows needed to realize a motivating use case
scenario for data markets. Additionally, we discuss the merits of the approach
from privacy, security, integrateability, and affordability perspectives.
</p>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05732" title="Abstract">arXiv:2310.05732</a> [<a href="/pdf/2310.05732" title="Download PDF">pdf</a>, <a href="/format/2310.05732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Scheduling with a Shared Resource
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Damerius%2C+C">Christoph Damerius</a>, 
<a href="/search/cs?searchtype=author&query=Kling%2C+P">Peter Kling</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+F">Florian Schneider</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to COCOA 2023, Full Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We consider the following shared-resource scheduling problem: Given a set of
jobs $J$, for each $j\in J$ we must schedule a job-specific processing volume
of $v_j&gt;0$. A total resource of $1$ is available at any time. Jobs have a
resource requirement $r_j\in[0,1]$, and the resources assigned to them may vary
over time. However, assigning them less will cause a proportional slowdown.
<br />We consider two settings. In the first, we seek to minimize the makespan in
an online setting: The resource assignment of a job must be fixed before the
next job arrives. Here we give an optimal $e/(e-1)$-competitive algorithm with
runtime $\mathcal{O}(n \log n)$. In the second, we aim to minimize the total
completion time. We use a continuous linear programming (CLP) formulation for
the fractional total completion time and combine it with a previously known
dominance property from malleable job scheduling to obtain a lower bound on the
total completion time. We extract structural properties by considering a
geometrical representation of a CLP's primal-dual pair. We combine the CLP
schedule with a greedy schedule to obtain a $(3/2+\varepsilon)$-approximation
for this setting. This improves upon the so far best-known approximation factor
of $2$.
</p>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05733" title="Abstract">arXiv:2310.05733</a> [<a href="/pdf/2310.05733" title="Download PDF">pdf</a>, <a href="/ps/2310.05733" title="Download PostScript">ps</a>, <a href="/format/2310.05733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polyhedral approach to weighted connected matchings in general graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Samer%2C+P">Phillippe Samer</a>, 
<a href="/search/cs?searchtype=author&query=Moura%2C+P+F+S">Phablo F.S. Moura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, submitted for publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Data Structures and Algorithms (cs.DS); Combinatorics (math.CO)

</div>
<p class="mathjax">A connected matching in a graph G consists of a set of pairwise disjoint
edges whose covered vertices induce a connected subgraph of G. While finding a
connected matching of maximum cardinality is a well-solved problem, it is
NP-hard to determine an optimal connected matching in an edge-weighted graph,
even in the planar bipartite case. We present two mixed integer programming
formulations and a sophisticated branch-and-cut scheme to find weighted
connected matchings in general graphs. The formulations explore different
polyhedra associated to this problem, including strong valid inequalities both
from the matching polytope and from the connected subgraph polytope. We
conjecture that one attains a tight approximation of the convex hull of
connected matchings using our strongest formulation, and report encouraging
computational results over DIMACS Implementation Challenge benchmark instances.
The source code of the complete implementation is also made available.
</p>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05736" title="Abstract">arXiv:2310.05736</a> [<a href="/pdf/2310.05736" title="Download PDF">pdf</a>, <a href="/format/2310.05736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMLingua: Compressing Prompts for Accelerated Inference of Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Huiqiang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qianhui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chin-Yew Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuqing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+L">Lili Qiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have been applied in various applications due to
their astonishing capabilities. With advancements in technologies such as
chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed
to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of
tokens. To accelerate model inference and reduce cost, this paper presents
LLMLingua, a coarse-to-fine prompt compression method that involves a budget
controller to maintain semantic integrity under high compression ratios, a
token-level iterative compression algorithm to better model the interdependence
between compressed contents, and an instruction tuning based method for
distribution alignment between language models. We conduct experiments and
analysis over four datasets from different scenarios, i.e., GSM8K, BBH,
ShareGPT, and Arxiv-March23; showing that the proposed approach yields
state-of-the-art performance and allows for up to 20x compression with little
performance loss. Our code is available at https://aka.ms/LLMLingua.
</p>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05737" title="Abstract">arXiv:2310.05737</a> [<a href="/pdf/2310.05737" title="Download PDF">pdf</a>, <a href="/format/2310.05737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lijun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lezama%2C+J">Jos&#xe9; Lezama</a>, 
<a href="/search/cs?searchtype=author&query=Gundavarapu%2C+N+B">Nitesh B. Gundavarapu</a>, 
<a href="/search/cs?searchtype=author&query=Versari%2C+L">Luca Versari</a>, 
<a href="/search/cs?searchtype=author&query=Sohn%2C+K">Kihyuk Sohn</a>, 
<a href="/search/cs?searchtype=author&query=Minnen%2C+D">David Minnen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yong Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Agrim Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+X">Xiuye Gu</a>, 
<a href="/search/cs?searchtype=author&query=Hauptmann%2C+A+G">Alexander G. Hauptmann</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+B">Boqing Gong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Essa%2C+I">Irfan Essa</a>, 
<a href="/search/cs?searchtype=author&query=Ross%2C+D+A">David A. Ross</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Lu Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
<p class="mathjax">While Large Language Models (LLMs) are the dominant models for generative
tasks in language, they do not perform as well as diffusion models on image and
video generation. To effectively use LLMs for visual generation, one crucial
component is the visual tokenizer that maps pixel-space inputs to discrete
tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a
video tokenizer designed to generate concise and expressive tokens for both
videos and images using a common token vocabulary. Equipped with this new
tokenizer, we show that LLMs outperform diffusion models on standard image and
video generation benchmarks including ImageNet and Kinetics. In addition, we
demonstrate that our tokenizer surpasses the previously top-performing video
tokenizer on two more tasks: (1) video compression comparable to the
next-generation video codec (VCC) according to human evaluations, and (2)
learning effective representations for action recognition tasks.
</p>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05741" title="Abstract">arXiv:2310.05741</a> [<a href="/pdf/2310.05741" title="Download PDF">pdf</a>, <a href="/ps/2310.05741" title="Download PostScript">ps</a>, <a href="/format/2310.05741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fictitious Play via Finite Differences for Mean Field Games with Optimal  Stopping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shen%2C+C">Chengfeng Shen</a>, 
<a href="/search/math?searchtype=author&query=Luo%2C+Y">Yifan Luo</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Z">Zhennan Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper considers mean field games with optimal stopping time (OSMFGs)
where agents make optimal exit decisions, the coupled obstacle and
Fokker-Planck equations in such models pose challenges versus classic MFGs.
This paper proposes a generalized fictitious play algorithm that computes OSMFG
mixed equilibria by iteratively solving pure strategy systems, i.e.
approximating mixed strategies through averaging pure strategies according to a
certain updating rule. The generalized fictitious play allows for a broad
family of learning rates and the convergence to the mixed strategy equilibrium
can be rigorously justified. The algorithm also incorporates efficient finite
difference schemes of the pure strategy system, and numerical experiments
demonstrate the effectiveness of the proposed method in robustly and
efficiently computing mixed equilibria for OSMFGs.
</p>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05746" title="Abstract">arXiv:2310.05746</a> [<a href="/pdf/2310.05746" title="Download PDF">pdf</a>, <a href="/format/2310.05746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and  Execution of LLM Agents in an Auction Arena
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiangjie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Siyu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+R">Rong Ye</a>, 
<a href="/search/cs?searchtype=author&query=Majumder%2C+B+P">Bodhisattwa Prasad Majumder</a>, 
<a href="/search/cs?searchtype=author&query=Richardson%2C+K">Kyle Richardson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Can Large Language Models (LLMs) simulate human behavior in complex
environments? LLMs have recently been shown to exhibit advanced reasoning
skills but much of NLP evaluation still relies on static benchmarks. Answering
this requires evaluation environments that probe strategic reasoning in
competitive, dynamic scenarios that involve long-term planning. We introduce
AucArena, a novel simulation environment for evaluating LLMs within auctions, a
setting chosen for being highly unpredictable and involving many skills related
to resource and risk management, while also being easy to evaluate. We conduct
several controlled simulations using state-of-the-art LLMs as bidding agents.
We find that through simple prompting, LLMs do indeed demonstrate many of the
skills needed for effectively engaging in auctions (e.g., managing budget,
adhering to long-term goals and priorities), skills that we find can be
sharpened by explicitly encouraging models to be adaptive and observe
strategies in past auctions. These results are significant as they show the
potential of using LLM agents to model intricate social dynamics, especially in
competitive settings. However, we also observe considerable variability in the
capabilities of individual LLMs. Notably, even our most advanced models (GPT-4)
are occasionally surpassed by heuristic baselines and human agents,
highlighting the potential for further improvements in the design of LLM agents
and the important role that our simulation environment can play in further
testing and refining agent architectures.
</p>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05748" title="Abstract">arXiv:2310.05748</a> [<a href="/pdf/2310.05748" title="Download PDF">pdf</a>, <a href="/format/2310.05748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vem++, a c++ library to handle and play with the Virtual Element Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dassi%2C+F">F. Dassi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The Virtual Element Method (VEM) is an extension of the Finite Element Method
(FEM) used for handling polytopal meshes. This paper provides a brief
introduction to the VEM for a two-dimensional Laplacian problem. Additionally,
it highlights the differences between a VEM implementation and a FEM code,
emphasising the main challenges associated with the VEM.
<br />Furthermore, this paper presents a possible approach to address these
challenges: vem++, a c++ library specifically developed for working with VEM
discretisation. The library is designed to handle various partial differential
equations in two or three dimensions, arising from both academic and
engineering problems. Its flexible design allows for the seamless integration
of new features, such as novel polytopes' quadrature rules, solvers, and
virtual element spaces.
</p>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05751" title="Abstract">arXiv:2310.05751</a> [<a href="/pdf/2310.05751" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review of the Ethics of Artificial Intelligence and its Applications  in the United States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taiwo%2C+E">Esther Taiwo</a>, 
<a href="/search/cs?searchtype=author&query=Akinsola%2C+A">Ahmed Akinsola</a>, 
<a href="/search/cs?searchtype=author&query=Tella%2C+E">Edward Tella</a>, 
<a href="/search/cs?searchtype=author&query=Makinde%2C+K">Kolade Makinde</a>, 
<a href="/search/cs?searchtype=author&query=Akinwande%2C+M">Mayowa Akinwande</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Journal on Cybernetics &amp; Informatics (IJCI) Vol.12, No.6, December 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Volume 12, Number 6 - International Conference on Computer Science
  and Information Technology Advances (CCSITA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This study is focused on the ethics of Artificial Intelligence and its
application in the United States, the paper highlights the impact AI has in
every sector of the US economy and multiple facets of the technological space
and the resultant effect on entities spanning businesses, government, academia,
and civil society. There is a need for ethical considerations as these entities
are beginning to depend on AI for delivering various crucial tasks, which
immensely influence their operations, decision-making, and interactions with
each other. The adoption of ethical principles, guidelines, and standards of
work is therefore required throughout the entire process of AI development,
deployment, and usage to ensure responsible and ethical AI practices. Our
discussion explores eleven fundamental 'ethical principles' structured as
overarching themes. These encompass Transparency, Justice, Fairness, Equity,
Non- Maleficence, Responsibility, Accountability, Privacy, Beneficence,
Freedom, Autonomy, Trust, Dignity, Sustainability, and Solidarity. These
principles collectively serve as a guiding framework, directing the ethical
path for the responsible development, deployment, and utilization of artificial
intelligence (AI) technologies across diverse sectors and entities within the
United States. The paper also discusses the revolutionary impact of AI
applications, such as Machine Learning, and explores various approaches used to
implement AI ethics. This examination is crucial to address the growing
concerns surrounding the inherent risks associated with the widespread use of
artificial intelligence.
</p>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05752" title="Abstract">arXiv:2310.05752</a> [<a href="/pdf/2310.05752" title="Download PDF">pdf</a>, <a href="/format/2310.05752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Gulf of Interpretation: From Chart to Message and Back Again
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Knoll%2C+C">Christian Knoll</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6ller%2C+T">Torsten M&#xf6;ller</a>, 
<a href="/search/cs?searchtype=author&query=Gregory%2C+K">Kathleen Gregory</a>, 
<a href="/search/cs?searchtype=author&query=Koesten%2C+L">Laura Koesten</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 8 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Charts are used to communicate data visually, but designing an effective
chart that a broad set of people can understand is challenging. Usually, we do
not know whether a chart's intended message aligns with the message readers
perceive. In this mixed-methods study, we investigate how data journalists
encode data and how a broad audience engages with, experiences, and understands
these data visualizations. We conducted a series of workshops and interviews
with school students, university students, job seekers, designers, and senior
citizens to collect perceived messages and subjective feedback on a sample of
eight real-world charts. We analyzed these messages and compared them to the
intended message of the chart producer. Four of the collected messages from
consumers were then provided to data journalists (including the ones that
created the original charts) as a starting point to re-design the charts
accordingly. The results from our work underline the difficulty of complex
charts such as stacked bar charts and Sankey diagrams. Consumers are often
overwhelmed with the amount of data provided and are easily confused with terms
(as text) not well known. Chart producers tend to be faithful with data but are
willing to abstract further when asked to transport particular messages
visually. There are strong conventions on how to visually encode particular
information that might not be to the benefit of many consumers.
</p>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05753" title="Abstract">arXiv:2310.05753</a> [<a href="/pdf/2310.05753" title="Download PDF">pdf</a>, <a href="/format/2310.05753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-Scale OD Matrix Estimation with A Deep Learning Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zheli Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+D">Defu Lian</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Enhong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Gang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xiaomin Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages,25 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The estimation of origin-destination (OD) matrices is a crucial aspect of
Intelligent Transport Systems (ITS). It involves adjusting an initial OD matrix
by regressing the current observations like traffic counts of road sections
(e.g., using least squares). However, the OD estimation problem lacks
sufficient constraints and is mathematically underdetermined. To alleviate this
problem, some researchers incorporate a prior OD matrix as a target in the
regression to provide more structural constraints. However, this approach is
highly dependent on the existing prior matrix, which may be outdated. Others
add structural constraints through sensor data, such as vehicle trajectory and
speed, which can reflect more current structural constraints in real-time. Our
proposed method integrates deep learning and numerical optimization algorithms
to infer matrix structure and guide numerical optimization. This approach
combines the advantages of both deep learning and numerical optimization
algorithms. The neural network(NN) learns to infer structural constraints from
probe traffic flows, eliminating dependence on prior information and providing
real-time performance. Additionally, due to the generalization capability of
NN, this method is economical in engineering. We conducted tests to demonstrate
the good generalization performance of our method on a large-scale synthetic
dataset. Subsequently, we verified the stability of our method on real traffic
data. Our experiments provided confirmation of the benefits of combining NN and
numerical optimization.
</p>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05754" title="Abstract">arXiv:2310.05754</a> [<a href="/pdf/2310.05754" title="Download PDF">pdf</a>, <a href="/format/2310.05754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing the power of Neural Collapse for Transferability Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yuhe Ding</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+B">Bo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+L">Lijun Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+A">Aihua Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jian Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Transferability estimation aims to provide heuristics for quantifying how
suitable a pre-trained model is for a specific downstream task, without
fine-tuning them all. Prior studies have revealed that well-trained models
exhibit the phenomenon of Neural Collapse. Based on a widely used neural
collapse metric in existing literature, we observe a strong correlation between
the neural collapse of pre-trained models and their corresponding fine-tuned
models. Inspired by this observation, we propose a novel method termed Fair
Collapse (FaCe) for transferability estimation by comprehensively measuring the
degree of neural collapse in the pre-trained model. Typically, FaCe comprises
two different terms: the variance collapse term, which assesses the class
separation and within-class compactness, and the class fairness term, which
quantifies the fairness of the pre-trained model towards each class. We
investigate FaCe on a variety of pre-trained classification models across
different network architectures, source datasets, and training loss functions.
Results show that FaCe yields state-of-the-art performance on different tasks
including image classification, semantic segmentation, and text classification,
which demonstrate the effectiveness and generalization of our method.
</p>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05755" title="Abstract">arXiv:2310.05755</a> [<a href="/pdf/2310.05755" title="Download PDF">pdf</a>, <a href="/format/2310.05755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Concept Removal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klochkov%2C+Y">Yegor Klochkov</a>, 
<a href="/search/cs?searchtype=author&query=Ton%2C+J">Jean-Francois Ton</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+R">Ruocheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 9 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We address the problem of concept removal in deep neural networks, aiming to
learn representations that do not encode certain specified concepts (e.g.,
gender etc.) We propose a novel method based on adversarial linear classifiers
trained on a concept dataset, which helps to remove the targeted attribute
while maintaining model performance. Our approach Deep Concept Removal
incorporates adversarial probing classifiers at various layers of the network,
effectively addressing concept entanglement and improving out-of-distribution
generalization. We also introduce an implicit gradient-based technique to
tackle the challenges associated with adversarial training using linear
classifiers. We evaluate the ability to remove a concept on a set of popular
distributionally robust optimization (DRO) benchmarks with spurious
correlations, as well as out-of-distribution (OOD) generalization tasks.
</p>
</div>
</dd>
<dt><a name="item693">[693]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05757" title="Abstract">arXiv:2310.05757</a> [<a href="/pdf/2310.05757" title="Download PDF">pdf</a>, <a href="/format/2310.05757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear Correct and Smooth for Semi-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yuanhang Shao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiuwen Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph-based semi-supervised learning (GSSL) has been used successfully in
various applications. Existing methods leverage the graph structure and labeled
samples for classification. Label Propagation (LP) and Graph Neural Networks
(GNNs) both iteratively pass messages on graphs, where LP propagates node
labels through edges and GNN aggregates node features from the neighborhood.
Recently, combining LP and GNN has led to improved performance. However,
utilizing labels and features jointly in higher-order graphs has not been
explored. Therefore, we propose Nonlinear Correct and Smooth (NLCS), which
improves the existing post-processing approach by incorporating non-linearity
and higher-order representation into the residual propagation to handle
intricate node relationships effectively. Systematic evaluations show that our
method achieves remarkable average improvements of 13.71% over base prediction
and 2.16% over the state-of-the-art post-processing method on six commonly used
datasets. Comparisons and analyses show our method effectively utilizes labels
and features jointly in higher-order graphs to resolve challenging graph
relationships.
</p>
</div>
</dd>
<dt><a name="item694">[694]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05762" title="Abstract">arXiv:2310.05762</a> [<a href="/pdf/2310.05762" title="Download PDF">pdf</a>, <a href="/format/2310.05762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D tomatoes&#x27; localisation with monocular cameras using histogram filters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Magalh%C3%A3es%2C+S+C">Sandro Costa Magalh&#xe3;es</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+F+N+d">Filipe Neves dos Santos</a>, 
<a href="/search/cs?searchtype=author&query=Moreira%2C+A+P">Ant&#xf3;nio Paulo Moreira</a>, 
<a href="/search/cs?searchtype=author&query=Dias%2C+J">Jorge Dias</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Performing tasks in agriculture, such as fruit monitoring or harvesting,
requires perceiving the objects' spatial position. RGB-D cameras are limited
under open-field environments due to lightning interferences. Therefore, in
this study, we approach the use of Histogram Filters (Bayesian Discrete
Filters) to estimate the position of tomatoes in the tomato plant. Two kernel
filters were studied: the square kernel and the Gaussian kernel. The
implemented algorithm was essayed in simulation, with and without Gaussian
noise and random noise, and in a testbed at laboratory conditions. The
algorithm reported a mean absolute error lower than 10 mm in simulation and 20
mm in the testbed at laboratory conditions with an assessing distance of about
0.5 m. So, the results are viable for real environments and should be improved
at closer distances.
</p>
</div>
</dd>
<dt><a name="item695">[695]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05764" title="Abstract">arXiv:2310.05764</a> [<a href="/pdf/2310.05764" title="Download PDF">pdf</a>, <a href="/format/2310.05764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and  Binding Site Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=St%C3%A4rk%2C+H">Hannes St&#xe4;rk</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+B">Bowen Jing</a>, 
<a href="/search/cs?searchtype=author&query=Barzilay%2C+R">Regina Barzilay</a>, 
<a href="/search/cs?searchtype=author&query=Jaakkola%2C+T">Tommi Jaakkola</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review. 25 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A significant amount of protein function requires binding small molecules,
including enzymatic catalysis. As such, designing binding pockets for small
molecules has several impactful applications ranging from drug synthesis to
energy storage. Towards this goal, we first develop HarmonicFlow, an improved
generative process over 3D protein-ligand binding structures based on our
self-conditioned flow matching objective. FlowSite extends this flow model to
jointly generate a protein pocket's discrete residue types and the molecule's
binding 3D structure. We show that HarmonicFlow improves upon the
state-of-the-art generative processes for docking in simplicity, generality,
and performance. Enabled by this structure modeling, FlowSite designs binding
sites substantially better than baseline approaches and provides the first
general solution for binding site design.
</p>
</div>
</dd>
<dt><a name="item696">[696]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05765" title="Abstract">arXiv:2310.05765</a> [<a href="/pdf/2310.05765" title="Download PDF">pdf</a>, <a href="/format/2310.05765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining the simulation-to-reality gap of a wheel loader digging in  deformable terrain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aoshima%2C+K">Koji Aoshima</a>, 
<a href="/search/cs?searchtype=author&query=Servin%2C+M">Martin Servin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">We investigate how well a wheel loader simulator can replicate a real one
when performing bucket filling in a pile of gravel. The comparisons are made
using field test time series of the vehicle motion and actuation forces, loaded
mass, and total work. The vehicle was modeled as a rigid multibody system with
frictional contacts, driveline, and linear actuators. For the soil, we tested
discrete element models of different resolutions, with and without multiscale
acceleration. The spatio-temporal resolution ranged between 50-400 mm and 2-500
ms, and the computational speed was between 1/10,000 to 5 times faster than
real-time. The simulation-to-reality gap was found to be around 10% and
exhibited a weak dependence on the level of fidelity, i.e. accessible with
real-time simulation and faster. Furthermore, the sensitivity of an optimized
force feedback controller under transfer between different simulation domains
was investigated. The domain bias was observed to cause a performance reduction
of 5% despite the domain gap being about 15%.
</p>
</div>
</dd>
<dt><a name="item697">[697]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05766" title="Abstract">arXiv:2310.05766</a> [<a href="/pdf/2310.05766" title="Download PDF">pdf</a>, <a href="/format/2310.05766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FeatSense -- A Feature-based Registration Algorithm with GPU-accelerated  TSDF-Mapping Backend for NVIDIA Jetson Boards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gaal%2C+J">Julian Gaal</a>, 
<a href="/search/cs?searchtype=author&query=Wiemann%2C+T">Thomas Wiemann</a>, 
<a href="/search/cs?searchtype=author&query=Mock%2C+A">Alexander Mock</a>, 
<a href="/search/cs?searchtype=author&query=Porrmann%2C+M">Mario Porrmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper presents FeatSense, a feature-based GPU-accelerated SLAM system
for high resolution LiDARs, combined with a map generation algorithm for
real-time generation of large Truncated Signed Distance Fields (TSDFs) on
embedded hardware. FeatSense uses LiDAR point cloud features for odometry
estimation and point cloud registration. The registered point clouds are
integrated into a global Truncated Signed Distance Field (TSDF) representation.
FeatSense is intended to run on embedded systems with integrated
GPU-accelerator like NVIDIA Jetson boards. In this paper, we present a
real-time capable TSDF-SLAM system specially tailored for close coupled CPU/GPU
systems. The implementation is evaluated in various structured and unstructured
environments and benchmarked against existing reference datasets. The main
contribution of this paper is the ability to register up to 128 scan lines of
an Ouster OS1-128 LiDAR at 10Hz on a NVIDIA AGX Xavier while achieving a TSDF
map generation speedup by a factor of 100 compared to previous work on the same
power budget.
</p>
</div>
</dd>
<dt><a name="item698">[698]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05767" title="Abstract">arXiv:2310.05767</a> [<a href="/pdf/2310.05767" title="Download PDF">pdf</a>, <a href="/format/2310.05767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topological Community Detection: A Sheaf-Theoretic Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wolf%2C+A">Arne Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Monod%2C+A">Anthea Monod</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Algebraic Topology (math.AT)

</div>
<p class="mathjax">We propose a model for network community detection using topological data
analysis, a branch of modern data science that leverages theory from algebraic
topology to statistical analysis and machine learning. Specifically, we use
cellular sheaves, which relate local to global properties of various algebraic
topological constructions, to propose three new algorithms for vertex
clustering over networks to detect communities. We apply our algorithms to real
social network data in numerical experiments and obtain near optimal results in
terms of modularity. Our work is the first implementation of sheaves on real
social network data and provides a solid proof-of-concept for future work using
sheaves as tools to study complex systems captured by networks and simplicial
complexes.
</p>
</div>
</dd>
<dt><a name="item699">[699]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05768" title="Abstract">arXiv:2310.05768</a> [<a href="/pdf/2310.05768" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DANet: Enhancing Small Object Detection through an Efficient Deformable  Attention Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mia%2C+M+S">Md Sohag Mia</a>, 
<a href="/search/cs?searchtype=author&query=Voban%2C+A+A+B">Abdullah Al Bary Voban</a>, 
<a href="/search/cs?searchtype=author&query=Arnob%2C+A+B+H">Abu Bakor Hayat Arnob</a>, 
<a href="/search/cs?searchtype=author&query=Naim%2C+A">Abdu Naim</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+M+K">Md Kawsar Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+S">Md Shariful Islam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Efficient and accurate detection of small objects in manufacturing settings,
such as defects and cracks, is crucial for ensuring product quality and safety.
To address this issue, we proposed a comprehensive strategy by synergizing
Faster R-CNN with cutting-edge methods. By combining Faster R-CNN with Feature
Pyramid Network, we enable the model to efficiently handle multi-scale features
intrinsic to manufacturing environments. Additionally, Deformable Net is used
that contorts and conforms to the geometric variations of defects, bringing
precision in detecting even the minuscule and complex features. Then, we
incorporated an attention mechanism called Convolutional Block Attention Module
in each block of our base ResNet50 network to selectively emphasize informative
features and suppress less useful ones. After that we incorporated RoI Align,
replacing RoI Pooling for finer region-of-interest alignment and finally the
integration of Focal Loss effectively handles class imbalance, crucial for rare
defect occurrences. The rigorous evaluation of our model on both the NEU-DET
and Pascal VOC datasets underscores its robust performance and generalization
capabilities. On the NEU-DET dataset, our model exhibited a profound
understanding of steel defects, achieving state-of-the-art accuracy in
identifying various defects. Simultaneously, when evaluated on the Pascal VOC
dataset, our model showcases its ability to detect objects across a wide
spectrum of categories within complex and small scenes.
</p>
</div>
</dd>
<dt><a name="item700">[700]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05771" title="Abstract">arXiv:2310.05771</a> [<a href="/pdf/2310.05771" title="Download PDF">pdf</a>, <a href="/format/2310.05771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundation Models Meet Visualizations: Challenges and Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Weikai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mengchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shixia Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Computational Visual Media
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Recent studies have indicated that foundation models, such as BERT and GPT,
excel in adapting to a variety of downstream tasks. This adaptability has
established them as the dominant force in building artificial intelligence (AI)
systems. As visualization techniques intersect with these models, a new
research paradigm emerges. This paper divides these intersections into two main
areas: visualizations for foundation models (VIS4FM) and foundation models for
visualizations (FM4VIS). In VIS4FM, we explore the primary role of
visualizations in understanding, refining, and evaluating these intricate
models. This addresses the pressing need for transparency, explainability,
fairness, and robustness. Conversely, within FM4VIS, we highlight how
foundation models can be utilized to advance the visualization field itself.
The confluence of foundation models and visualizations holds great promise, but
it also comes with its own set of challenges. By highlighting these challenges
and the growing opportunities, this paper seeks to provide a starting point for
continued exploration in this promising avenue.
</p>
</div>
</dd>
<dt><a name="item701">[701]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05772" title="Abstract">arXiv:2310.05772</a> [<a href="/pdf/2310.05772" title="Download PDF">pdf</a>, <a href="/format/2310.05772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RateRL: A Framework for Developing RL-Based Rate Adaptation Algorithms  in ns-3
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Queiros%2C+R">Ruben Queiros</a>, 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+L">Luis Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Fontes%2C+H">Helder Fontes</a>, 
<a href="/search/cs?searchtype=author&query=Campos%2C+R">Rui Campos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The increasing complexity of recent Wi-Fi amendments is making the use of
traditional algorithms and heuristics unfeasible to address the Rate Adaptation
(RA) problem. This is due to the large combination of configuration parameters
along with the high variability of the wireless channel. Recently, several
works have proposed the usage of Reinforcement Learning (RL) techniques to
address the problem. However, the proposed solutions lack sufficient technical
explanation. Also, the lack of standard frameworks enabling the reproducibility
of results and the limited availability of source code, makes the fair
comparison with state of the art approaches a challenge. This paper proposes a
framework, named RateRL, that integrates state of the art libraries with the
well-known Network Simulator 3 (ns-3) to enable the implementation and
evaluation of RL-based RA algorithms. To the best of our knowledge, RateRL is
the first tool available to assist researchers during the implementation,
validation and evaluation phases of RL-based RA algorithms and enable the fair
comparison between competing algorithms.
</p>
</div>
</dd>
<dt><a name="item702">[702]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05773" title="Abstract">arXiv:2310.05773</a> [<a href="/pdf/2310.05773" title="Download PDF">pdf</a>, <a href="/format/2310.05773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory  Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Ziyao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cazenavette%2C+G">George Cazenavette</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hui Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First lossless dataset distillation method
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The ultimate goal of Dataset Distillation is to synthesize a small synthetic
dataset such that a model trained on this synthetic set will perform equally
well as a model trained on the full, real dataset. Until now, no method of
Dataset Distillation has reached this completely lossless goal, in part due to
the fact that previous methods only remain effective when the total number of
synthetic samples is extremely small. Since only so much information can be
contained in such a small number of samples, it seems that to achieve truly
loss dataset distillation, we must develop a distillation method that remains
effective as the size of the synthetic dataset grows. In this work, we present
such an algorithm and elucidate why existing methods fail to generate larger,
high-quality synthetic sets. Current state-of-the-art methods rely on
trajectory-matching, or optimizing the synthetic data to induce similar
long-term training dynamics as the real data. We empirically find that the
training stage of the trajectories we choose to match (i.e., early or late)
greatly affects the effectiveness of the distilled dataset. Specifically, early
trajectories (where the teacher network learns easy patterns) work well for a
low-cardinality synthetic set since there are fewer examples wherein to
distribute the necessary information. Conversely, late trajectories (where the
teacher network learns hard patterns) provide better signals for larger
synthetic sets since there are now enough samples to represent the necessary
complex patterns. Based on our findings, we propose to align the difficulty of
the generated patterns with the size of the synthetic dataset. In doing so, we
successfully scale trajectory matching-based methods to larger synthetic
datasets, achieving lossless dataset distillation for the very first time. Code
and distilled datasets are available at https://gzyaftermath.github.io/DATM.
</p>
</div>
</dd>
<dt><a name="item703">[703]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05776" title="Abstract">arXiv:2310.05776</a> [<a href="/pdf/2310.05776" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coordinated Expansion Planning of Transmission and Distribution Systems  Integrated with Smart Grid Technologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Moradi-Sepahvand%2C+M">Mojtaba Moradi-Sepahvand</a>, 
<a href="/search/eess?searchtype=author&query=Amraee%2C+T">Turaj Amraee</a>, 
<a href="/search/eess?searchtype=author&query=Aminifar%2C+F">Farrokh Aminifar</a>, 
<a href="/search/eess?searchtype=author&query=Akbari%2C+A">Amirhossein Akbari</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Electrical Power &amp; Energy Systems, 147,
  p.108859, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Integration of smart grid technologies in distribution systems, particularly
behind-the-meter initiatives, has a direct impact on transmission network
planning. This paper develops a coordinated expansion planning of transmission
and active distribution systems via a stochastic multistage mathematical
programming model. In the transmission level, in addition to lines, sitting and
sizing of utility-scale battery energy storage systems and wind power plants
under renewable portfolio standard policy are planned. Switchable feeders and
distributed generations are decision variables in the distribution level while
the impact of demand response programs as a sort of behind-the-meter
technologies is accommodated as well. Expansion of electric vehicle taxi
charging stations is included as a feasible option in both transmission and
distribution levels. In order to deal with short-term uncertainty of load
demand, renewable energy sources output power, and the charging pattern of
electric vehicle taxis in each station, a chronological time-period clustering
algorithm along with Monte Carlo simulation is utilized. The proposed model is
tackled by means of Benders Dual Decomposition (BDD) method. The IEEE RTS test
system (as the transmission system) along with four IEEE 33-node test feeders
(as distribution test systems) are examined to validate effectiveness of the
proposed model.
</p>
</div>
</dd>
<dt><a name="item704">[704]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05777" title="Abstract">arXiv:2310.05777</a> [<a href="/pdf/2310.05777" title="Download PDF">pdf</a>, <a href="/ps/2310.05777" title="Download PostScript">ps</a>, <a href="/format/2310.05777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unknown Truths and Unknowable Truths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Jie Fan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> pages 13
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
<p class="mathjax">Notions of unknown truths and unknowable truths are important in formal
epistemology, which are related to each other in e.g. Fitch's paradox of
knowability. Although there have been some logical research on the notion of
unknown truths and some philosophical discussion on the two notions, there
seems to be no logical research on unknowable truths. In this paper, we propose
a logic of unknowable truths, investigate the logical properties of unknown
truths and unknowable truths, which includes the similarities of the two
notions and the relationship between the two notions, and axiomatize this
logic.
</p>
</div>
</dd>
<dt><a name="item705">[705]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05779" title="Abstract">arXiv:2310.05779</a> [<a href="/pdf/2310.05779" title="Download PDF">pdf</a>, <a href="/format/2310.05779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why Should This Article Be Deleted? Transparent Stance Detection in  Multilingual Wikipedia Editor Discussions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaffee%2C+L">Lucie-Aim&#xe9;e Kaffee</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+A">Arnav Arora</a>, 
<a href="/search/cs?searchtype=author&query=Augenstein%2C+I">Isabelle Augenstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This submission has been accepted to Conference on Empirical Methods in Natural Language Processing (EMNLP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The moderation of content on online platforms is usually non-transparent. On
Wikipedia, however, this discussion is carried out publicly and the editors are
encouraged to use the content moderation policies as explanations for making
moderation decisions. Currently, only a few comments explicitly mention those
policies -- 20% of the English ones, but as few as 2% of the German and Turkish
comments. To aid in this process of understanding how content is moderated, we
construct a novel multilingual dataset of Wikipedia editor discussions along
with their reasoning in three languages. The dataset contains the stances of
the editors (keep, delete, merge, comment), along with the stated reason, and a
content moderation policy, for each edit decision. We demonstrate that stance
and corresponding reason (policy) can be predicted jointly with a high degree
of accuracy, adding transparency to the decision-making process. We release
both our joint prediction models and the multilingual content moderation
dataset for further research on automated transparent content moderation.
</p>
</div>
</dd>
<dt><a name="item706">[706]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05782" title="Abstract">arXiv:2310.05782</a> [<a href="/pdf/2310.05782" title="Download PDF">pdf</a>, <a href="/format/2310.05782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aligning Language Models with Human Preferences via a Bayesian Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiashuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haozhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shichao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjie Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the quest to advance human-centric natural language generation (NLG)
systems, ensuring alignment between NLG models and human preferences is
crucial. For this alignment, current popular methods leverage a reinforcement
learning (RL) approach with a reward model trained on feedback from humans.
However, inherent disagreements due to the subjective nature of human
preferences pose a significant challenge for training the reward model,
resulting in a deterioration of the NLG performance. To tackle this issue,
previous approaches typically rely on majority voting or averaging to
consolidate multiple inconsistent preferences into a merged one. Although
straightforward to understand and execute, such methods suffer from an
inability to capture the nuanced degrees of disaggregation among humans and may
only represent a specialized subset of individuals, thereby lacking the ability
to quantitatively disclose the universality of human preferences. To address
this challenge, this paper proposes a novel approach, which employs a Bayesian
framework to account for the distribution of disagreements among human
preferences as training a preference model, and names it as d-PM. Besides,
considering the RL strategy's inefficient and complex training process over the
training efficiency, we further propose utilizing the contrastive learning
strategy to train the NLG model with the preference scores derived from the
d-PM model. Extensive experiments on two human-centric NLG tasks, i.e.,
emotional support conversation and integrity "Rule-of-Thumb" generation, show
that our method consistently exceeds previous SOTA models in both automatic and
human evaluations.
</p>
</div>
</dd>
<dt><a name="item707">[707]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05784" title="Abstract">arXiv:2310.05784</a> [<a href="/pdf/2310.05784" title="Download PDF">pdf</a>, <a href="/format/2310.05784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Parameterised Complexity of Integer Multicommodity Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bodlaender%2C+H+L">Hans L. Bodlaender</a>, 
<a href="/search/cs?searchtype=author&query=Mannens%2C+I">Isja Mannens</a>, 
<a href="/search/cs?searchtype=author&query=Oostveen%2C+J+J">Jelle J. Oostveen</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+S">Sukanya Pandey</a>, 
<a href="/search/cs?searchtype=author&query=van+Leeuwen%2C+E+J">Erik Jan van Leeuwen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">The Integer Multicommodity Flow problem has been studied extensively in the
literature. However, from a parameterised perspective, mostly special cases,
such as the Disjoint Paths problem, have been considered. Therefore, we
investigate the parameterised complexity of the general Integer Multicommodity
Flow problem. We show that the decision version of this problem on directed
graphs for a constant number of commodities, when the capacities are given in
unary, is XNLP-complete with pathwidth as parameter and XALP-complete with
treewidth as parameter. When the capacities are given in binary, the problem is
NP-complete even for graphs of pathwidth at most 13. We give related results
for undirected graphs. These results imply that the problem is unlikely to be
fixed-parameter tractable by these parameters.
<br />In contrast, we show that the problem does become fixed-parameter tractable
when weighted tree partition width (a variant of tree partition width for edge
weighted graphs) is used as parameter.
</p>
</div>
</dd>
<dt><a name="item708">[708]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05785" title="Abstract">arXiv:2310.05785</a> [<a href="/pdf/2310.05785" title="Download PDF">pdf</a>, <a href="/format/2310.05785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint object detection and re-identification for 3D obstacle  multi-camera systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cort%C3%A9s%2C+I">Irene Cort&#xe9;s</a>, 
<a href="/search/cs?searchtype=author&query=Beltr%C3%A1n%2C+J">Jorge Beltr&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=de+la+Escalera%2C+A">Arturo de la Escalera</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa%2C+F">Fernando Garc&#xed;a</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In recent years, the field of autonomous driving has witnessed remarkable
advancements, driven by the integration of a multitude of sensors, including
cameras and LiDAR systems, in different prototypes. However, with the
proliferation of sensor data comes the pressing need for more sophisticated
information processing techniques. This research paper introduces a novel
modification to an object detection network that uses camera and lidar
information, incorporating an additional branch designed for the task of
re-identifying objects across adjacent cameras within the same vehicle while
elevating the quality of the baseline 3D object detection outcomes. The
proposed methodology employs a two-step detection pipeline: initially, an
object detection network is employed, followed by a 3D box estimator that
operates on the filtered point cloud generated from the network's detections.
Extensive experimental evaluations encompassing both 2D and 3D domains validate
the effectiveness of the proposed approach and the results underscore the
superiority of this method over traditional Non-Maximum Suppression (NMS)
techniques, with an improvement of more than 5\% in the car category in the
overlapping areas.
</p>
</div>
</dd>
<dt><a name="item709">[709]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05788" title="Abstract">arXiv:2310.05788</a> [<a href="/pdf/2310.05788" title="Download PDF">pdf</a>, <a href="/ps/2310.05788" title="Download PostScript">ps</a>, <a href="/format/2310.05788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Canonization of a random circulant graph by counting walks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verbitsky%2C+O">Oleg Verbitsky</a>, 
<a href="/search/cs?searchtype=author&query=Zhukovskii%2C+M">Maksim Zhukovskii</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">It is well known that almost all graphs are canonizable by a simple
combinatorial routine known as color refinement. With high probability, this
method assigns a unique label to each vertex of a random input graph and,
hence, it is applicable only to asymmetric graphs. The strength of
combinatorial refinement techniques becomes a subtle issue if the input graphs
are highly symmetric. We prove that the combination of color refinement with
vertex individualization produces a canonical labeling for almost all circulant
digraphs (Cayley digraphs of a cyclic group). To our best knowledge, this is
the first application of combinatorial refinement in the realm of
vertex-transitive graphs. Remarkably, we do not even need the full power of the
color refinement algorithm. We show that the canonical label of a vertex $v$
can be obtained just by counting walks of each length from $v$ to an
individualized vertex.
</p>
</div>
</dd>
<dt><a name="item710">[710]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05789" title="Abstract">arXiv:2310.05789</a> [<a href="/pdf/2310.05789" title="Download PDF">pdf</a>, <a href="/format/2310.05789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Hybrid Oversampling and Intelligent Undersampling for  Imbalanced Big Data Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vairetti%2C+C">Carla Vairetti</a>, 
<a href="/search/cs?searchtype=author&query=Assadi%2C+J+L">Jos&#xe9; Luis Assadi</a>, 
<a href="/search/cs?searchtype=author&query=Maldonado%2C+S">Sebasti&#xe1;n Maldonado</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 1 figure, submitted to Expert Systems with Applications (Elsevier)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Imbalanced classification is a well-known challenge faced by many real-world
applications. This issue occurs when the distribution of the target variable is
skewed, leading to a prediction bias toward the majority class. With the
arrival of the Big Data era, there is a pressing need for efficient solutions
to solve this problem. In this work, we present a novel resampling method
called SMOTENN that combines intelligent undersampling and oversampling using a
MapReduce framework. Both procedures are performed on the same pass over the
data, conferring efficiency to the technique. The SMOTENN method is
complemented with an efficient implementation of the neighborhoods related to
the minority samples. Our experimental results show the virtues of this
approach, outperforming alternative resampling techniques for small- and
medium-sized datasets while achieving positive results on large datasets with
reduced running times.
</p>
</div>
</dd>
<dt><a name="item711">[711]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05791" title="Abstract">arXiv:2310.05791</a> [<a href="/pdf/2310.05791" title="Download PDF">pdf</a>, <a href="/format/2310.05791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Problem-Solving Guide: Predicting the Algorithm Tags and Difficulty for  Competitive Programming Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juntae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+E">Eunjung Cho</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dongwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Na%2C+D">Dongbin Na</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The recent program development industries have required problem-solving
abilities for engineers, especially application developers. However, AI-based
education systems to help solve computer algorithm problems have not yet
attracted attention, while most big tech companies require the ability to solve
algorithm problems including Google, Meta, and Amazon. The most useful guide to
solving algorithm problems might be guessing the category (tag) of the facing
problems. Therefore, our study addresses the task of predicting the algorithm
tag as a useful tool for engineers and developers. Moreover, we also consider
predicting the difficulty levels of algorithm problems, which can be used as
useful guidance to calculate the required time to solve that problem. In this
paper, we present a real-world algorithm problem multi-task dataset, AMT, by
mainly collecting problem samples from the most famous and large competitive
programming website Codeforces. To the best of our knowledge, our proposed
dataset is the most large-scale dataset for predicting algorithm tags compared
to previous studies. Moreover, our work is the first to address predicting the
difficulty levels of algorithm problems. We present a deep learning-based novel
method for simultaneously predicting algorithm tags and the difficulty levels
of an algorithm problem given. All datasets and source codes are available at
https://github.com/sronger/PSG_Predicting_Algorithm_Tags_and_Difficulty.
</p>
</div>
</dd>
<dt><a name="item712">[712]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05793" title="Abstract">arXiv:2310.05793</a> [<a href="/pdf/2310.05793" title="Download PDF">pdf</a>, <a href="/format/2310.05793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for  Accelerated Seq2Seq Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shansan Gong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mukai Li</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiangtao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Diffusion models have gained prominence in generating high-quality sequences
of text. Nevertheless, current approaches predominantly represent discrete text
within a continuous diffusion space, which incurs substantial computational
overhead during training and results in slower sampling speeds. In this paper,
we introduce a soft absorbing state that facilitates the diffusion model in
learning to reconstruct discrete mutations based on the underlying Gaussian
space, thereby enhancing its capacity to recover conditional signals. During
the sampling phase, we employ state-of-the-art ODE solvers within the
continuous space to expedite the sampling process. Comprehensive experimental
evaluations reveal that our proposed method effectively accelerates the
training convergence by 4x and generates samples of similar quality 800x
faster, rendering it significantly closer to practical application.
\footnote{The code is released at \url{https://github.com/Shark-NLP/DiffuSeq}
</p>
</div>
</dd>
<dt><a name="item713">[713]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05794" title="Abstract">arXiv:2310.05794</a> [<a href="/pdf/2310.05794" title="Download PDF">pdf</a>, <a href="/ps/2310.05794" title="Download PostScript">ps</a>, <a href="/format/2310.05794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computation-Limited Signals: A Channel Capacity Regime Constrained by  Computational Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Queiroz%2C+S">Saulo Queiroz</a>, 
<a href="/search/cs?searchtype=author&query=Vilela%2C+J+P">Jo&#xe3;o P. Vilela</a>, 
<a href="/search/cs?searchtype=author&query=Monteiro%2C+E">Edmundo Monteiro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Computational Complexity (cs.CC); Signal Processing (eess.SP)

</div>
<p class="mathjax">In this letter, we introduce the computational-limited (comp-limited)
signals, a communication capacity regime in which the signal time computational
complexity overhead is the key constraint -- rather than power or bandwidth --
to the overall communication capacity. To relate capacity and time complexity,
we propose a novel mathematical framework that builds on concepts of
information theory and computational complexity. In particular, the algorithmic
capacity stands for the ratio between the upper-bound number of bits modulated
in a symbol and the lower-bound time complexity required to turn these bits
into a communication symbol. By setting this ratio as function of the channel
resources, we classify a given signal design as comp-limited if its algorithmic
capacity nullifies as the channel resources grow. As a use-case, we show that
an uncoded OFDM transmitter is comp-limited unless the lower-bound
computational complexity of the N-point DFT problem verifies as $\Omega(N)$,
which remains an open challenge in theoretical computer science.
</p>
</div>
</dd>
<dt><a name="item714">[714]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05797" title="Abstract">arXiv:2310.05797</a> [<a href="/pdf/2310.05797" title="Download PDF">pdf</a>, <a href="/format/2310.05797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Large Language Models Post Hoc Explainers?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kroeger%2C+N">Nicholas Kroeger</a>, 
<a href="/search/cs?searchtype=author&query=Ley%2C+D">Dan Ley</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+S">Satyapriya Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+C">Chirag Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Lakkaraju%2C+H">Himabindu Lakkaraju</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) are increasingly used as powerful tools for a
plethora of natural language processing (NLP) applications. A recent
innovation, in-context learning (ICL), enables LLMs to learn new tasks by
supplying a few examples in the prompt during inference time, thereby
eliminating the need for model fine-tuning. While LLMs have been utilized in
several applications, their applicability in explaining the behavior of other
models remains relatively unexplored. Despite the growing number of new
explanation techniques, many require white-box access to the model and/or are
computationally expensive, highlighting a need for next-generation post hoc
explainers. In this work, we present the first framework to study the
effectiveness of LLMs in explaining other predictive models. More specifically,
we propose a novel framework encompassing multiple prompting strategies: i)
Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL,
and iv) Explanation-based ICL, with varying levels of information about the
underlying ML model and the local neighborhood of the test sample. We conduct
extensive experiments with real-world benchmark datasets to demonstrate that
LLM-generated explanations perform on par with state-of-the-art post hoc
explainers using their ability to leverage ICL examples and their internal
knowledge in generating model explanations. On average, across four datasets
and two ML models, we observe that LLMs identify the most important feature
with 72.19% accuracy, opening up new frontiers in explainable artificial
intelligence (XAI) to explore LLM-based explanation frameworks.
</p>
</div>
</dd>
<dt><a name="item715">[715]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05801" title="Abstract">arXiv:2310.05801</a> [<a href="/pdf/2310.05801" title="Download PDF">pdf</a>, <a href="/format/2310.05801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An operator preconditioning perspective on training in physics-informed  machine learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De+Ryck%2C+T">Tim De Ryck</a>, 
<a href="/search/cs?searchtype=author&query=Bonnet%2C+F">Florent Bonnet</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Siddhartha Mishra</a>, 
<a href="/search/cs?searchtype=author&query=de+B%C3%A9zenac%2C+E">Emmanuel de B&#xe9;zenac</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper, we investigate the behavior of gradient descent algorithms in
physics-informed machine learning methods like PINNs, which minimize residuals
connected to partial differential equations (PDEs). Our key result is that the
difficulty in training these models is closely related to the conditioning of a
specific differential operator. This operator, in turn, is associated to the
Hermitian square of the differential operator of the underlying PDE. If this
operator is ill-conditioned, it results in slow or infeasible training.
Therefore, preconditioning this operator is crucial. We employ both rigorous
mathematical analysis and empirical evaluations to investigate various
strategies, explaining how they better condition this critical operator, and
consequently improve training.
</p>
</div>
</dd>
<dt><a name="item716">[716]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05804" title="Abstract">arXiv:2310.05804</a> [<a href="/pdf/2310.05804" title="Download PDF">pdf</a>, <a href="/format/2310.05804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Language-guided Adaptive Hyper-modality Representation for  Multimodal Sentiment Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+G">Guanghao Yin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kejun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tianshu Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)

</div>
<p class="mathjax">Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich
information from multiple sources (e.g., language, video, and audio), the
potential sentiment-irrelevant and conflicting information across modalities
may hinder the performance from being further improved. To alleviate this, we
present Adaptive Language-guided Multimodal Transformer (ALMT), which
incorporates an Adaptive Hyper-modality Learning (AHL) module to learn an
irrelevance/conflict-suppressing representation from visual and audio features
under the guidance of language features at different scales. With the obtained
hyper-modality representation, the model can obtain a complementary and joint
representation through multimodal fusion for effective MSA. In practice, ALMT
achieves state-of-the-art performance on several popular datasets (e.g., MOSI,
MOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity and
necessity of our irrelevance/conflict suppression mechanism.
</p>
</div>
</dd>
<dt><a name="item717">[717]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05807" title="Abstract">arXiv:2310.05807</a> [<a href="/pdf/2310.05807" title="Download PDF">pdf</a>, <a href="/format/2310.05807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sharing Information Between Machine Tools to Improve Surface Finish  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Clarkson%2C+D+R">Daniel R. Clarkson</a>, 
<a href="/search/cs?searchtype=author&query=Bull%2C+L+A">Lawrence A. Bull</a>, 
<a href="/search/cs?searchtype=author&query=Dardeno%2C+T+A">Tina A. Dardeno</a>, 
<a href="/search/cs?searchtype=author&query=Wickramarachchi%2C+C+T">Chandula T. Wickramarachchi</a>, 
<a href="/search/cs?searchtype=author&query=Cross%2C+E+J">Elizabeth J. Cross</a>, 
<a href="/search/cs?searchtype=author&query=Rogers%2C+T+J">Timothy J. Rogers</a>, 
<a href="/search/cs?searchtype=author&query=Worden%2C+K">Keith Worden</a>, 
<a href="/search/cs?searchtype=author&query=Dervilis%2C+N">Nikolaos Dervilis</a>, 
<a href="/search/cs?searchtype=author&query=Hughes%2C+A+J">Aidan J. Hughes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to International Workshop on Structural Health Monitoring 2023, Stanford University, California, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">At present, most surface-quality prediction methods can only perform
single-task prediction which results in under-utilised datasets, repetitive
work and increased experimental costs. To counter this, the authors propose a
Bayesian hierarchical model to predict surface-roughness measurements for a
turning machining process. The hierarchical model is compared to multiple
independent Bayesian linear regression models to showcase the benefits of
partial pooling in a machining setting with respect to prediction accuracy and
uncertainty quantification.
</p>
</div>
</dd>
<dt><a name="item718">[718]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05808" title="Abstract">arXiv:2310.05808</a> [<a href="/pdf/2310.05808" title="Download PDF">pdf</a>, <a href="/format/2310.05808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple Open-Loop Baseline for Reinforcement Learning Locomotion Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raffin%2C+A">Antonin Raffin</a>, 
<a href="/search/cs?searchtype=author&query=Sigaud%2C+O">Olivier Sigaud</a>, 
<a href="/search/cs?searchtype=author&query=Kober%2C+J">Jens Kober</a>, 
<a href="/search/cs?searchtype=author&query=Albu-Sch%C3%A4ffer%2C+A">Alin Albu-Sch&#xe4;ffer</a>, 
<a href="/search/cs?searchtype=author&query=Silv%C3%A9rio%2C+J">Jo&#xe3;o Silv&#xe9;rio</a>, 
<a href="/search/cs?searchtype=author&query=Stulp%2C+F">Freek Stulp</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> video: <a href="https://b2drop.eudat.eu/s/ykDPMM7F9KFyLgi">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In search of the simplest baseline capable of competing with Deep
Reinforcement Learning on locomotion tasks, we propose a biologically inspired
model-free open-loop strategy. Drawing upon prior knowledge and harnessing the
elegance of simple oscillators to generate periodic joint motions, it achieves
respectable performance in five different locomotion environments, with a
number of tunable parameters that is a tiny fraction of the thousands typically
required by RL algorithms. Unlike RL methods, which are prone to performance
degradation when exposed to sensor noise or failure, our open-loop oscillators
exhibit remarkable robustness due to their lack of reliance on sensors.
Furthermore, we showcase a successful transfer from simulation to reality using
an elastic quadruped, all without the need for randomization or reward
engineering.
</p>
</div>
</dd>
<dt><a name="item719">[719]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05811" title="Abstract">arXiv:2310.05811</a> [<a href="/pdf/2310.05811" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrated Expansion Planning of Electric Energy Generation,  Transmission, and Storage for Handling High Shares of Wind and Solar Power  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Moradi-Sepahvand%2C+M">Mojtaba Moradi-Sepahvand</a>, 
<a href="/search/eess?searchtype=author&query=Amraee%2C+T">Turaj Amraee</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Energy, Volume 298, Pages 117137, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, an integrated multi-period model for long term expansion
planning of electric energy transmission grid, power generation technologies,
and energy storage devices is introduced. The proposed method gives the type,
size and location of generation, transmission and storage devices to supply the
electric load demand over the planning horizon. The sitting and sizing of
Battery Energy Storage (BES) devices as flexible options is addressed to cover
the intermittency of Renewable Energy Sources (RESs), mitigate lines
congestion, and postpone the need for new transmission lines and power plants
installation. For efficient handling of RESs uncertainties, and operational
flexibility, the upward and downward Flexible Ramp Spinning Reserve (FRSR) are
modeled. Besides, the Low-Carbon Policy (LCP) is considered in the objective
function of the proposed Transmission, Generation, and Storage Expansion
Planning (TGSEP) model. A hierarchical clustering method that can preserve the
chronology of input time series throughout the planning horizon periods is
developed to capture the short-term uncertainties of load demand and RESs. The
short-term operational flexibility requirements make the joint long-term
transmission and generation planning a high computational problem. Therefore,
the Mixed-Integer Linear Programming (MILP) formulation of the model is solved
using an accelerated Benders Dual Decomposition (BDD) method. The IEEE RTS test
system is utilized to validate the effectiveness of the proposed joint
expansion planning model.
</p>
</div>
</dd>
<dt><a name="item720">[720]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05812" title="Abstract">arXiv:2310.05812</a> [<a href="/pdf/2310.05812" title="Download PDF">pdf</a>, <a href="/format/2310.05812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provably Convergent Data-Driven Convex-Nonconvex Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shumaylov%2C+Z">Zakhar Shumaylov</a>, 
<a href="/search/cs?searchtype=author&query=Budd%2C+J">Jeremy Budd</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Subhadip Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6nlieb%2C+C">Carola-Bibiane Sch&#xf6;nlieb</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages + 3 pages appendices; preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">An emerging new paradigm for solving inverse problems is via the use of deep
learning to learn a regularizer from data. This leads to high-quality results,
but often at the cost of provable guarantees. In this work, we show how
well-posedness and convergent regularization arises within the convex-nonconvex
(CNC) framework for inverse problems. We introduce a novel input weakly convex
neural network (IWCNN) construction to adapt the method of learned adversarial
regularization to the CNC framework. Empirically we show that our method
overcomes numerical issues of previous adversarial methods.
</p>
</div>
</dd>
<dt><a name="item721">[721]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05813" title="Abstract">arXiv:2310.05813</a> [<a href="/pdf/2310.05813" title="Download PDF">pdf</a>, <a href="/format/2310.05813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Audio compression-assisted feature extraction for voice replay attack  detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiangyu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yuhao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Li Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Haorui He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhizheng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Replay attack is one of the most effective and simplest voice spoofing
attacks. Detecting replay attacks is challenging, according to the Automatic
Speaker Verification Spoofing and Countermeasures Challenge 2021 (ASVspoof
2021), because they involve a loudspeaker, a microphone, and acoustic
conditions (e.g., background noise). One obstacle to detecting replay attacks
is finding robust feature representations that reflect the channel noise
information added to the replayed speech. This study proposes a feature
extraction approach that uses audio compression for assistance. Audio
compression compresses audio to preserve content and speaker information for
transmission. The missed information after decompression is expected to contain
content- and speaker-independent information (e.g., channel noise added during
the replay process). We conducted a comprehensive experiment with a few data
augmentation techniques and 3 classifiers on the ASVspoof 2021 physical access
(PA) set and confirmed the effectiveness of the proposed feature extraction
approach. To the best of our knowledge, the proposed approach achieves the
lowest EER at 22.71% on the ASVspoof 2021 PA evaluation set.
</p>
</div>
</dd>
<dt><a name="item722">[722]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05814" title="Abstract">arXiv:2310.05814</a> [<a href="/pdf/2310.05814" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning-Based Hurricane Resilient Co-planning of Transmission  Lines, Battery Energy Storages and Wind Farms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Moradi-Sepahvand%2C+M">Mojtaba Moradi-Sepahvand</a>, 
<a href="/search/eess?searchtype=author&query=Amraee%2C+T">Turaj Amraee</a>, 
<a href="/search/eess?searchtype=author&query=Gougheri%2C+S+S">Saleh Sadeghi Gougheri</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Industrial Informatics, Volume 18, Issue 3,
  pages 2120-2131, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, a multi-stage model for expansion co-planning of transmission
lines, Battery Energy Storages (BESs), and Wind Farms (WFs) is presented
considering resilience against extreme weather events. In addition to High
Voltage Alternating Current (HVAC) lines, Multi-Terminal Voltage Source
Converter (MTVSC) based High Voltage Direct Current (HVDC) lines are planned to
reduce the impact of high-risk events. To evaluate the system resilience
against hurricanes, probable hurricane speed (HS) scenarios are generated using
Monte Carlo Simulation (MCS). The Fragility Curve (FC) concept is utilized for
calculating the failure probability of lines due to extreme hurricanes. Based
on each hurricane damage, the probable scenarios are incorporated in the
proposed model. Renewable Portfolio Standard (RPS) policy is modeled to
integrate high penetration of WFs. To deal with the wind power and load demand
uncertainties, a Chronological Time-Period Clustering (CTPC) algorithm is
introduced for extracting representative hours in each planning stage. A deep
learning approach based on Bi-directional Long Short-Term Memory (B-LSTM)
networks is presented to forecast the yearly peak loads. The Mixed-Integer
Linear Programming (MILP) formulation of the proposed model is solved using a
Benders Decomposition (BD) algorithm. A modified IEEE RTS test system is used
to evaluate the proposed model effectiveness.
</p>
</div>
</dd>
<dt><a name="item723">[723]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05818" title="Abstract">arXiv:2310.05818</a> [<a href="/pdf/2310.05818" title="Download PDF">pdf</a>, <a href="/format/2310.05818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SC-Safety: A Multi-round Open-ended Question Adversarial Safety  Benchmark for Large Language Models in Chinese
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Liang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+K">Kangkang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+H">Hang Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 8 tables, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs), like ChatGPT and GPT-4, have demonstrated
remarkable abilities in natural language understanding and generation. However,
alongside their positive impact on our daily tasks, they can also produce
harmful content that negatively affects societal perceptions. To systematically
assess the safety of Chinese LLMs, we introduce SuperCLUE-Safety (SC-Safety) -
a multi-round adversarial benchmark with 4912 open-ended questions covering
more than 20 safety sub-dimensions. Adversarial human-model interactions and
conversations significantly increase the challenges compared to existing
methods. Experiments on 13 major LLMs supporting Chinese yield the following
insights: 1) Closed-source models outperform open-sourced ones in terms of
safety; 2) Models released from China demonstrate comparable safety levels to
LLMs like GPT-3.5-turbo; 3) Some smaller models with 6B-13B parameters can
compete effectively in terms of safety. By introducing SC-Safety, we aim to
promote collaborative efforts to create safer and more trustworthy LLMs. The
benchmark and findings provide guidance on model selection. Our benchmark can
be found at https://www.CLUEbenchmarks.com
</p>
</div>
</dd>
<dt><a name="item724">[724]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05821" title="Abstract">arXiv:2310.05821</a> [<a href="/pdf/2310.05821" title="Download PDF">pdf</a>, <a href="/format/2310.05821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-trained Spatial Priors on Multichannel NMF for Music Source  Separation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cabanas-Molero%2C+P">Pablo Cabanas-Molero</a>, 
<a href="/search/cs?searchtype=author&query=Munoz-Montoro%2C+A+J">Antonio J. Munoz-Montoro</a>, 
<a href="/search/cs?searchtype=author&query=Carabias-Orti%2C+J">Julio Carabias-Orti</a>, 
<a href="/search/cs?searchtype=author&query=Vera-Candeas%2C+P">Pedro Vera-Candeas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at Forum Acusticum 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper presents a novel approach to sound source separation that
leverages spatial information obtained during the recording setup. Our method
trains a spatial mixing filter using solo passages to capture information about
the room impulse response and transducer response at each sensor location. This
pre-trained filter is then integrated into a multichannel non-negative matrix
factorization (MNMF) scheme to better capture the variances of different sound
sources. The recording setup used in our experiments is the typical setup for
orchestra recordings, with a main microphone and a close "cardioid" or
"supercardioid" microphone for each section of the orchestra. This makes the
proposed method applicable to many existing recordings. Experiments on
polyphonic ensembles demonstrate the effectiveness of the proposed framework in
separating individual sound sources, improving performance compared to
conventional MNMF methods.
</p>
</div>
</dd>
<dt><a name="item725">[725]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05824" title="Abstract">arXiv:2310.05824</a> [<a href="/pdf/2310.05824" title="Download PDF">pdf</a>, <a href="/format/2310.05824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Terminology-Aware Translation with Constrained Decoding and Large  Language Model Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bogoychev%2C+N">Nikolay Bogoychev</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pinzhen Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WMT 2023 Terminology Translation Task
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Terminology correctness is important in the downstream application of machine
translation, and a prevalent way to ensure this is to inject terminology
constraints into a translation system. In our submission to the WMT 2023
terminology translation task, we adopt a translate-then-refine approach which
can be domain-independent and requires minimal manual efforts. We annotate
random source words with pseudo-terminology translations obtained from word
alignment to first train a terminology-aware model. Further, we explore two
post-processing methods. First, we use an alignment process to discover whether
a terminology constraint has been violated, and if so, we re-decode with the
violating word negatively constrained. Alternatively, we leverage a large
language model to refine a hypothesis by providing it with terminology
constraints. Results show that our terminology-aware model learns to
incorporate terminologies effectively, and the large language model refinement
process can further improve terminology recall.
</p>
</div>
</dd>
<dt><a name="item726">[726]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05825" title="Abstract">arXiv:2310.05825</a> [<a href="/pdf/2310.05825" title="Download PDF">pdf</a>, <a href="/format/2310.05825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Write What You Want: Applying Text-to-video Retrieval to Audiovisual  Archives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuchen Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Audiovisual (AV) archives, as an essential reservoir of our cultural assets,
are suffering from the issue of accessibility. The complex nature of the medium
itself made processing and interaction an open challenge still in the field of
computer vision, multimodal learning, and human-computer interaction, as well
as in culture and heritage. In recent years, with the raising of video
retrieval tasks, methods in retrieving video content with natural language
(text-to-video retrieval) gained quite some attention and have reached a
performance level where real-world application is on the horizon. Appealing as
it may sound, such methods focus on retrieving videos using plain
visual-focused descriptions of what has happened in the video and finding
videos such as instructions. It is too early to say such methods would be the
new paradigms for accessing and encoding complex video content into
high-dimensional data, but they are indeed innovative attempts and foundations
to build future exploratory interfaces for AV archives (e.g. allow users to
write stories and retrieve related snippets in the archive, or encoding video
content at high-level for visualisation). This work filled the application gap
by examining such text-to-video retrieval methods from an implementation point
of view and proposed and verified a classifier-enhanced workflow to allow
better results when dealing with in-situ queries that might have been different
from the training dataset. Such a workflow is then applied to the real-world
archive from T\'el\'evision Suisse Romande (RTS) to create a demo. At last, a
human-centred evaluation is conducted to understand whether the text-to-video
retrieval methods improve the overall experience of accessing AV archives.
</p>
</div>
</dd>
<dt><a name="item727">[727]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05828" title="Abstract">arXiv:2310.05828</a> [<a href="/pdf/2310.05828" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid AC/DC Transmission Expansion Planning Considering HVAC to HVDC  Conversion Under Renewable Penetration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Moradi-Sepahvand%2C+M">Mojtaba Moradi-Sepahvand</a>, 
<a href="/search/eess?searchtype=author&query=Amraee%2C+T">Turaj Amraee</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Power Systems, Volume 36, Issue 1, Pages
  579-591, 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, a dynamic (i.e. multi-year) hybrid model is presented for
Transmission Expansion Planning (TEP) utilizing the High Voltage Alternating
Current (HVAC) and multiterminal Voltage Sourced Converter (VSC)-based High
Voltage Direct Current (HVDC) alternatives. In addition to new HVAC and HVDC
lines, the possibility of converting existing HVAC transmission lines to HVDC
lines is considered in the proposed model. High shares of renewable resources
are integrated into the proposed hybrid AC/DC TEP model. Due to the
intermittency of renewable resources, the planning of large-scale Energy
Storage (ES) devices is considered. In order to accurately estimate the total
TEP costs and hence capturing the scenarios of load and renewable generation
uncertainty, using a clustering approach, each year of the planning horizon is
replaced with four representative days. The proposed model is formulated as a
Mixed-Integer Linear Programming (MILP) problem. Using Benders Decomposition
(BD) algorithm, the proposed model is decomposed into a Master investment
problem to handle the decision variables, and Sub-problems to check the
feasibility of master problem solution and optimize the operation and ES
investment cost. Three test systems are used as case studies to demonstrate the
effectiveness of the proposed hybrid AC/DC TEP model.
</p>
</div>
</dd>
<dt><a name="item728">[728]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05829" title="Abstract">arXiv:2310.05829</a> [<a href="/pdf/2310.05829" title="Download PDF">pdf</a>, <a href="/format/2310.05829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting the Temporal Modeling in Spatio-Temporal Predictive Learning  under A Unified View
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhangyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lirong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+J">Jun Xia</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Spatio-temporal predictive learning plays a crucial role in self-supervised
learning, with wide-ranging applications across a diverse range of fields.
Previous approaches for temporal modeling fall into two categories:
recurrent-based and recurrent-free methods. The former, while meticulously
processing frames one by one, neglect short-term spatio-temporal information
redundancies, leading to inefficiencies. The latter naively stack frames
sequentially, overlooking the inherent temporal dependencies. In this paper, we
re-examine the two dominant temporal modeling approaches within the realm of
spatio-temporal predictive learning, offering a unified perspective. Building
upon this analysis, we introduce USTEP (Unified Spatio-TEmporal Predictive
learning), an innovative framework that reconciles the recurrent-based and
recurrent-free methods by integrating both micro-temporal and macro-temporal
scales. Extensive experiments on a wide range of spatio-temporal predictive
learning demonstrate that USTEP achieves significant improvements over existing
temporal modeling approaches, thereby establishing it as a robust solution for
a wide range of spatio-temporal applications.
</p>
</div>
</dd>
<dt><a name="item729">[729]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05833" title="Abstract">arXiv:2310.05833</a> [<a href="/pdf/2310.05833" title="Download PDF">pdf</a>, <a href="/format/2310.05833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gruber%2C+S+G">Sebastian G. Gruber</a>, 
<a href="/search/cs?searchtype=author&query=Buettner%2C+F">Florian Buettner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Generative models, like large language models, are becoming increasingly
relevant in our daily lives, yet a theoretical framework to assess their
generalization behavior and uncertainty does not exist. Particularly, the
problem of uncertainty estimation is commonly solved in an ad-hoc manner and
task dependent. For example, natural language approaches cannot be transferred
to image generation. In this paper we introduce the first
bias-variance-covariance decomposition for kernel scores and their associated
entropy. We propose unbiased and consistent estimators for each quantity which
only require generated samples but not the underlying model itself. As an
application, we offer a generalization evaluation of diffusion models and
discover how mode collapse of minority groups is a contrary phenomenon to
overfitting. Further, we demonstrate that variance and predictive kernel
entropy are viable measures of uncertainty for image, audio, and language
generation. Specifically, our approach for uncertainty estimation is more
predictive of performance on CoQA and TriviaQA question answering datasets than
existing baselines and can also be applied to closed-source models.
</p>
</div>
</dd>
<dt><a name="item730">[730]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05835" title="Abstract">arXiv:2310.05835</a> [<a href="/pdf/2310.05835" title="Download PDF">pdf</a>, <a href="/format/2310.05835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Wander: an Alternative Interface for Interactive and  Serendipitous Discovery of Large AV Archives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuchen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Linyida Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Audiovisual (AV) archives are invaluable for holistically preserving the
past. Unlike other forms, AV archives can be difficult to explore. This is not
only because of its complex modality and sheer volume but also the lack of
appropriate interfaces beyond keyword search. The recent rise in text-to-video
retrieval tasks in computer science opens the gate to accessing AV content more
naturally and semantically, able to map natural language descriptive sentences
to matching videos. However, applications of this model are rarely seen. The
contribution of this work is threefold. First, working with RTS (T\'el\'evision
Suisse Romande), we identified the key blockers in a real archive for
implementing such models. We built a functioning pipeline for encoding raw
archive videos to the text-to-video feature vectors. Second, we designed and
verified a method to encode and retrieve videos using emotionally abundant
descriptions not supported in the original model. Third, we proposed an initial
prototype for immersive and interactive exploration of AV archives in a latent
space based on the previously mentioned encoding of videos.
</p>
</div>
</dd>
<dt><a name="item731">[731]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05837" title="Abstract">arXiv:2310.05837</a> [<a href="/pdf/2310.05837" title="Download PDF">pdf</a>, <a href="/format/2310.05837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Real-time Method for Inserting Virtual Objects into Neural Radiance  Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+K">Keyang Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hongzhi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+X">Xin Tong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kun Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">We present the first real-time method for inserting a rigid virtual object
into a neural radiance field, which produces realistic lighting and shadowing
effects, as well as allows interactive manipulation of the object. By
exploiting the rich information about lighting and geometry in a NeRF, our
method overcomes several challenges of object insertion in augmented reality.
For lighting estimation, we produce accurate, robust and 3D spatially-varying
incident lighting that combines the near-field lighting from NeRF and an
environment lighting to account for sources not covered by the NeRF. For
occlusion, we blend the rendered virtual object with the background scene using
an opacity map integrated from the NeRF. For shadows, with a precomputed field
of spherical signed distance field, we query the visibility term for any point
around the virtual object, and cast soft, detailed shadows onto 3D surfaces.
Compared with state-of-the-art techniques, our approach can insert virtual
object into scenes with superior fidelity, and has a great potential to be
further applied to augmented reality systems.
</p>
</div>
</dd>
<dt><a name="item732">[732]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05839" title="Abstract">arXiv:2310.05839</a> [<a href="/pdf/2310.05839" title="Download PDF">pdf</a>, <a href="/ps/2310.05839" title="Download PostScript">ps</a>, <a href="/format/2310.05839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Directed Symmetric Multicut is W[1]-hard
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Osipov%2C+G">George Osipov</a>, 
<a href="/search/cs?searchtype=author&query=Pilipczuk%2C+M">Marcin Pilipczuk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Given a directed graph $G$ and a set of vertex pairs $\{(s_1,t_1), \dots,
(s_m, t_m)\}$, the Directed Symmetric Multicut problem asks to delete the
minimum number of edges from $G$ to separate every pair $(s_i, t_i)$ into
distinct strong components. Eiben, Rambaud and Wahlstr\"om [IPEC 2022]
initiated the study of this problem parameterized by the solution size. They
gave a fixed-parameter tractable 2-approximation algorithm, and left the exact
parameterized complexity status as an open question. We answer their question
in negative, showing that Directed Symmetric Multicut is W[1]-hard.
</p>
</div>
</dd>
<dt><a name="item733">[733]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05840" title="Abstract">arXiv:2310.05840</a> [<a href="/pdf/2310.05840" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Accident Severity: An Analysis Of Factors Affecting Accident  Severity Using Random Forest Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adefabi%2C+A">Adekunle Adefabi</a>, 
<a href="/search/cs?searchtype=author&query=Olisah%2C+S">Somtobe Olisah</a>, 
<a href="/search/cs?searchtype=author&query=Obunadike%2C+C">Callistus Obunadike</a>, 
<a href="/search/cs?searchtype=author&query=Oyetubo%2C+O">Oluwatosin Oyetubo</a>, 
<a href="/search/cs?searchtype=author&query=Taiwo%2C+E">Esther Taiwo</a>, 
<a href="/search/cs?searchtype=author&query=Tella%2C+E">Edward Tella</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal on Cybernetics &amp; Informatics (IJCI) Vol.12,
  No.6, December 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Road accidents have significant economic and societal costs, with a small
number of severe accidents accounting for a large portion of these costs.
Predicting accident severity can help in the proactive approach to road safety
by identifying potential unsafe road conditions and taking well-informed
actions to reduce the number of severe accidents. This study investigates the
effectiveness of the Random Forest machine learning algorithm for predicting
the severity of an accident. The model is trained on a dataset of accident
records from a large metropolitan area and evaluated using various metrics.
Hyperparameters and feature selection are optimized to improve the model's
performance. The results show that the Random Forest model is an effective tool
for predicting accident severity with an accuracy of over 80%. The study also
identifies the top six most important variables in the model, which include
wind speed, pressure, humidity, visibility, clear conditions, and cloud cover.
The fitted model has an Area Under the Curve of 80%, a recall of 79.2%, a
precision of 97.1%, and an F1 score of 87.3%. These results suggest that the
proposed model has higher performance in explaining the target variable, which
is the accident severity class. Overall, the study provides evidence that the
Random Forest model is a viable and reliable tool for predicting accident
severity and can be used to help reduce the number of fatalities and injuries
due to road accidents in the United States
</p>
</div>
</dd>
<dt><a name="item734">[734]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05842" title="Abstract">arXiv:2310.05842</a> [<a href="/pdf/2310.05842" title="Download PDF">pdf</a>, <a href="/format/2310.05842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Angular Synchronization via Directed Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yixuan He</a>, 
<a href="/search/cs?searchtype=author&query=Reinert%2C+G">Gesine Reinert</a>, 
<a href="/search/cs?searchtype=author&query=Wipf%2C+D">David Wipf</a>, 
<a href="/search/cs?searchtype=author&query=Cucuringu%2C+M">Mihai Cucuringu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">The angular synchronization problem aims to accurately estimate (up to a
constant additive phase) a set of unknown angles $\theta_1, \dots,
\theta_n\in[0, 2\pi)$ from $m$ noisy measurements of their offsets
$\theta_i-\theta_j \;\mbox{mod} \; 2\pi.$ Applications include, for example,
sensor network localization, phase retrieval, and distributed clock
synchronization. An extension of the problem to the heterogeneous setting
(dubbed $k$-synchronization) is to estimate $k$ groups of angles
simultaneously, given noisy observations (with unknown group assignment) from
each group. Existing methods for angular synchronization usually perform poorly
in high-noise regimes, which are common in applications. In this paper, we
leverage neural networks for the angular synchronization problem, and its
heterogeneous extension, by proposing GNNSync, a theoretically-grounded
end-to-end trainable framework using directed graph neural networks. In
addition, new loss functions are devised to encode synchronization objectives.
Experimental results on extensive data sets demonstrate that GNNSync attains
competitive, and often superior, performance against a comprehensive set of
baselines for the angular synchronization problem and its extension, validating
the robustness of GNNSync even at high noise levels.
</p>
</div>
</dd>
<dt><a name="item735">[735]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05845" title="Abstract">arXiv:2310.05845</a> [<a href="/pdf/2310.05845" title="Download PDF">pdf</a>, <a href="/format/2310.05845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphLLM: Boosting Graph Reasoning Ability of Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chai%2C+Z">Ziwei Chai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianjie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Liang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kaiqiao Han</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaohai Hu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanwen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The advancement of Large Language Models (LLMs) has remarkably pushed the
boundaries towards artificial general intelligence (AGI), with their
exceptional ability on understanding diverse types of information, including
but not limited to images and audio. Despite this progress, a critical gap
remains in empowering LLMs to proficiently understand and reason on graph data.
Recent studies underscore LLMs' underwhelming performance on fundamental graph
reasoning tasks. In this paper, we endeavor to unearth the obstacles that
impede LLMs in graph reasoning, pinpointing the common practice of converting
graphs into natural language descriptions (Graph2Text) as a fundamental
bottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering
end-to-end approach that synergistically integrates graph learning models with
LLMs. This synergy equips LLMs with the ability to proficiently interpret and
reason on graph data, harnessing the superior expressive power of graph
learning models. Our empirical evaluations across four fundamental graph
reasoning tasks validate the effectiveness of GraphLLM. The results exhibit a
substantial average accuracy enhancement of 54.44%, alongside a noteworthy
context reduction of 96.45% across various graph reasoning tasks.
</p>
</div>
</dd>
<dt><a name="item736">[736]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05847" title="Abstract">arXiv:2310.05847</a> [<a href="/pdf/2310.05847" title="Download PDF">pdf</a>, <a href="/format/2310.05847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Users Indistinguishable: Attribute-wise Unlearning in Recommender  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chaochao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiaolin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yizhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhongxuan Han</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+D">Dan Meng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 31st ACM International Conference on Multimedia
  (MM '23), October 29--November 3, 2023, Ottawa, ON, Canada
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Information Retrieval (cs.IR)

</div>
<p class="mathjax">With the growing privacy concerns in recommender systems, recommendation
unlearning, i.e., forgetting the impact of specific learned targets, is getting
increasing attention. Existing studies predominantly use training data, i.e.,
model inputs, as the unlearning target. However, we find that attackers can
extract private information, i.e., gender, race, and age, from a trained model
even if it has not been explicitly encountered during training. We name this
unseen information as attribute and treat it as the unlearning target. To
protect the sensitive attribute of users, Attribute Unlearning (AU) aims to
degrade attacking performance and make target attributes indistinguishable. In
this paper, we focus on a strict but practical setting of AU, namely
Post-Training Attribute Unlearning (PoT-AU), where unlearning can only be
performed after the training of the recommendation model is completed. To
address the PoT-AU problem in recommender systems, we design a two-component
loss function that consists of i) distinguishability loss: making attribute
labels indistinguishable from attackers, and ii) regularization loss:
preventing drastic changes in the model that result in a negative impact on
recommendation performance. Specifically, we investigate two types of
distinguishability measurements, i.e., user-to-user and
distribution-to-distribution. We use the stochastic gradient descent algorithm
to optimize our proposed loss. Extensive experiments on three real-world
datasets demonstrate the effectiveness of our proposed methods.
</p>
</div>
</dd>
<dt><a name="item737">[737]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05848" title="Abstract">arXiv:2310.05848</a> [<a href="/pdf/2310.05848" title="Download PDF">pdf</a>, <a href="/format/2310.05848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FMM-Head: Enhancing Autoencoder-based ECG anomaly detection with prior  knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verardo%2C+G">Giacomo Verardo</a>, 
<a href="/search/cs?searchtype=author&query=Boman%2C+M">Magnus Boman</a>, 
<a href="/search/cs?searchtype=author&query=Bruchfeld%2C+S">Samuel Bruchfeld</a>, 
<a href="/search/cs?searchtype=author&query=Chiesa%2C+M">Marco Chiesa</a>, 
<a href="/search/cs?searchtype=author&query=Koch%2C+S">Sabine Koch</a>, 
<a href="/search/cs?searchtype=author&query=Maguire%2C+G+Q">Gerald Q. Maguire Jr.</a>, 
<a href="/search/cs?searchtype=author&query=Kostic%2C+D">Dejan Kostic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Detecting anomalies in electrocardiogram data is crucial to identifying
deviations from normal heartbeat patterns and providing timely intervention to
at-risk patients. Various AutoEncoder models (AE) have been proposed to tackle
the anomaly detection task with ML. However, these models do not consider the
specific patterns of ECG leads and are unexplainable black boxes. In contrast,
we replace the decoding part of the AE with a reconstruction head (namely,
FMM-Head) based on prior knowledge of the ECG shape. Our model consistently
achieves higher anomaly detection capabilities than state-of-the-art models, up
to 0.31 increase in area under the ROC curve (AUROC), with as little as half
the original model size and explainable extracted features. The processing time
of our model is four orders of magnitude lower than solving an optimization
problem to obtain the same parameters, thus making it suitable for real-time
ECG parameters extraction and anomaly detection.
</p>
</div>
</dd>
<dt><a name="item738">[738]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05853" title="Abstract">arXiv:2310.05853</a> [<a href="/pdf/2310.05853" title="Download PDF">pdf</a>, <a href="/format/2310.05853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Mango Mango, How to Let The Lettuce Dry Without A Spinner?&#x27;&#x27;: Exploring  User Perceptions of Using An LLM-Based Conversational Assistant Toward  Cooking Partner
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chan%2C+S">Szeyi Chan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiachen Li</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+B">Bingsheng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+A">Amama Mahmood</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chien-Ming Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jimison%2C+H">Holly Jimison</a>, 
<a href="/search/cs?searchtype=author&query=Mynatt%2C+E+D">Elizabeth D Mynatt</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dakuo Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under submission to CHI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The rapid advancement of the Large Language Model (LLM) has created numerous
potentials for integration with conversational assistants (CAs) assisting
people in their daily tasks, particularly due to their extensive flexibility.
However, users' real-world experiences interacting with these assistants remain
unexplored. In this research, we chose cooking, a complex daily task, as a
scenario to investigate people's successful and unsatisfactory experiences
while receiving assistance from an LLM-based CA, Mango Mango. We discovered
that participants value the system's ability to provide extensive information
beyond the recipe, offer customized instructions based on context, and assist
them in dynamically planning the task. However, they expect the system to be
more adaptive to oral conversation and provide more suggestive responses to
keep users actively involved. Recognizing that users began treating our LLM-CA
as a personal assistant or even a partner rather than just a recipe-reading
tool, we propose several design considerations for future development.
</p>
</div>
</dd>
<dt><a name="item739">[739]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05857" title="Abstract">arXiv:2310.05857</a> [<a href="/pdf/2310.05857" title="Download PDF">pdf</a>, <a href="/format/2310.05857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Summarization with Human Edits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zonghai Yao</a>, 
<a href="/search/cs?searchtype=author&query=Schloss%2C+B+J">Benjamin J Schloss</a>, 
<a href="/search/cs?searchtype=author&query=Selvaraj%2C+S+P">Sai P. Selvaraj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in proceedings of the Main Conference on Empirical Methods in Natural Language Processing (EMNLP) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent work has shown the promise of learning with human feedback paradigms
to produce human-determined high-quality text. Existing works use human
feedback to train large language models (LLMs) in general domain abstractive
summarization and have obtained summary quality exceeding traditional
likelihood training. In this paper, we focus on a less explored form of human
feedback -- Human Edits. We propose Sequence Alignment (un)Likelihood Training
(SALT), a novel technique to use both the human-edited and model-generated data
together in the training loop. In addition, we demonstrate simulating Human
Edits with ground truth summaries coming from existing training data --
Imitation edits, along with the model-generated summaries obtained after the
training, to reduce the need for expensive human-edit data. In our experiments,
we extend human feedback exploration from general domain summarization to
medical domain summarization. Our results demonstrate the effectiveness of SALT
to improve the summary quality with Human and Imitation Edits.
</p>
</div>
</dd>
<dt><a name="item740">[740]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05858" title="Abstract">arXiv:2310.05858</a> [<a href="/pdf/2310.05858" title="Download PDF">pdf</a>, <a href="/format/2310.05858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DSAC-T: Distributional Soft Actor-Critic with Three Refinements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jingliang Duan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+L">Liming Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jiaxin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+E">Shengbo Eben Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Reinforcement learning (RL) has proven to be highly effective in tackling
complex decision-making and control tasks. However, prevalent model-free RL
methods often face severe performance degradation due to the well-known
overestimation issue. In response to this problem, we recently introduced an
off-policy RL algorithm, called distributional soft actor-critic (DSAC or
DSAC-v1), which can effectively improve the value estimation accuracy by
learning a continuous Gaussian value distribution. Nonetheless, standard DSAC
has its own shortcomings, including occasionally unstable learning processes
and needs for task-specific reward scaling, which may hinder its overall
performance and adaptability in some special tasks. This paper further
introduces three important refinements to standard DSAC in order to address
these shortcomings. These refinements consist of critic gradient adjusting,
twin value distribution learning, and variance-based target return clipping.
The modified RL algorithm is named as DSAC with three refinements (DSAC-T or
DSAC-v2), and its performances are systematically evaluated on a diverse set of
benchmark tasks. Without any task-specific hyperparameter tuning, DSAC-T
surpasses a lot of mainstream model-free RL algorithms, including SAC, TD3,
DDPG, TRPO, and PPO, in all tested environments. Additionally, DSAC-T, unlike
its standard version, ensures a highly stable learning process and delivers
similar performance across varying reward scales.
</p>
</div>
</dd>
<dt><a name="item741">[741]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05861" title="Abstract">arXiv:2310.05861</a> [<a href="/pdf/2310.05861" title="Download PDF">pdf</a>, <a href="/format/2310.05861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rephrase, Augment, Reason: Visual Grounding of Questions for  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prasad%2C+A">Archiki Prasad</a>, 
<a href="/search/cs?searchtype=author&query=Stengel-Eskin%2C+E">Elias Stengel-Eskin</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 4 figures, Code: <a href="https://github.com/archiki/RepARe">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">An increasing number of vision-language tasks can be handled with little to
no training, i.e., in a zero and few-shot manner, by marrying large language
models (LLMs) to vision encoders, resulting in large vision-language models
(LVLMs). While this has huge upsides, such as not requiring training data or
custom architectures, how an input is presented to a LVLM can have a major
impact on zero-shot model performance. In particular, inputs phrased in an
underspecified way can result in incorrect answers due to factors like missing
visual information, complex implicit reasoning, or linguistic ambiguity.
Therefore, adding visually grounded information to the input as a preemptive
clarification should improve model performance by reducing underspecification,
e.g., by localizing objects and disambiguating references. Similarly, in the
VQA setting, changing the way questions are framed can make them easier for
models to answer. To this end, we present Rephrase, Augment and Reason
(RepARe), a gradient-free framework that extracts salient details about the
image using the underlying LVLM as a captioner and reasoner, in order to
propose modifications to the original question. We then use the LVLM's
confidence over a generated answer as an unsupervised scoring function to
select the rephrased question most likely to improve zero-shot performance.
Focusing on two visual question answering tasks, we show that RepARe can result
in a 3.85% (absolute) increase in zero-shot performance on VQAv2 and a 6.41%
point increase on A-OKVQA. Additionally, we find that using gold answers for
oracle question candidate selection achieves a substantial gain in VQA accuracy
by up to 14.41%. Through extensive analysis, we demonstrate that outputs from
RepARe increase syntactic complexity, and effectively utilize vision-language
interaction and the frozen language model in LVLMs.
</p>
</div>
</dd>
<dt><a name="item742">[742]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05862" title="Abstract">arXiv:2310.05862</a> [<a href="/pdf/2310.05862" title="Download PDF">pdf</a>, <a href="/format/2310.05862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Better Safe than Sorry: Pre-training CLIP against Targeted Data  Poisoning and Backdoor Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wenhan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jingdong Gao</a>, 
<a href="/search/cs?searchtype=author&query=Mirzasoleiman%2C+B">Baharan Mirzasoleiman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Contrastive Language-Image Pre-training (CLIP) on large image-caption
datasets has achieved remarkable success in zero-shot classification and
enabled transferability to new domains. However, CLIP is extremely more
vulnerable to targeted data poisoning and backdoor attacks, compared to
supervised learning. Perhaps surprisingly, poisoning 0.0001% of CLIP
pre-training data is enough to make targeted data poisoning attacks successful.
This is four orders of magnitude smaller than what is required to poison
supervised models. Despite this vulnerability, existing methods are very
limited in defending CLIP models during pre-training. In this work, we propose
a strong defense, SAFECLIP, to safely pre-train CLIP against targeted data
poisoning and backdoor attacks. SAFECLIP warms up the model by applying
unimodal contrastive learning (CL) on image and text modalities separately.
Then, it carefully divides the data into safe and risky subsets. SAFECLIP
trains on the risky data by applying unimodal CL to image and text modalities
separately, and trains on the safe data using the CLIP loss. By gradually
increasing the size of the safe subset during the training, SAFECLIP
effectively breaks targeted data poisoning and backdoor attacks without harming
the CLIP performance. Our extensive experiments show that SAFECLIP decrease the
attack success rate of targeted data poisoning attacks from 93.75% to 0% and
that of the backdoor attacks from 100% to 0%, without harming the CLIP
performance on various datasets.
</p>
</div>
</dd>
<dt><a name="item743">[743]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05865" title="Abstract">arXiv:2310.05865</a> [<a href="/pdf/2310.05865" title="Download PDF">pdf</a>, <a href="/format/2310.05865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Learning-Based Framework for Safe Human-Robot Collaboration with  Multiple Backup Control Barrier Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Janwani%2C+N+C">Neil C. Janwani</a>, 
<a href="/search/cs?searchtype=author&query=Da%C5%9F%2C+E">Ersin Da&#x15f;</a>, 
<a href="/search/cs?searchtype=author&query=Touma%2C+T">Thomas Touma</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+S+X">Skylar X. Wei</a>, 
<a href="/search/cs?searchtype=author&query=Molnar%2C+T+G">Tamas G. Molnar</a>, 
<a href="/search/cs?searchtype=author&query=Burdick%2C+J+W">Joel W. Burdick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Ensuring robot safety in complex environments is a difficult task due to
actuation limits, such as torque bounds. This paper presents a safety-critical
control framework that leverages learning-based switching between multiple
backup controllers to formally guarantee safety under bounded control inputs
while satisfying driver intention. By leveraging backup controllers designed to
uphold safety and input constraints, backup control barrier functions (BCBFs)
construct implicitly defined control invariance sets via a feasible quadratic
program (QP). However, BCBF performance largely depends on the design and
conservativeness of the chosen backup controller, especially in our setting of
human-driven vehicles in complex, e.g, off-road, conditions. While
conservativeness can be reduced by using multiple backup controllers,
determining when to switch is an open problem. Consequently, we develop a
broadcast scheme that estimates driver intention and integrates BCBFs with
multiple backup strategies for human-robot interaction. An LSTM classifier uses
data inputs from the robot, human, and safety algorithms to continually choose
a backup controller in real-time. We demonstrate our method's efficacy on a
dual-track robot in obstacle avoidance scenarios. Our framework guarantees
robot safety while adhering to driver intention.
</p>
</div>
</dd>
<dt><a name="item744">[744]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05867" title="Abstract">arXiv:2310.05867</a> [<a href="/pdf/2310.05867" title="Download PDF">pdf</a>, <a href="/format/2310.05867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain-wise Invariant Learning for Panoptic Scene Graph Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Li Li</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">You Qin</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+W">Wei Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuxiao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zimmermann%2C+R">Roger Zimmermann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2307.15567">arXiv:2307.15567</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Panoptic Scene Graph Generation (PSG) involves the detection of objects and
the prediction of their corresponding relationships (predicates). However, the
presence of biased predicate annotations poses a significant challenge for PSG
models, as it hinders their ability to establish a clear decision boundary
among different predicates. This issue substantially impedes the practical
utility and real-world applicability of PSG models. To address the intrinsic
bias above, we propose a novel framework to infer potentially biased
annotations by measuring the predicate prediction risks within each
subject-object pair (domain), and adaptively transfer the biased annotations to
consistent ones by learning invariant predicate representation embeddings.
Experiments show that our method significantly improves the performance of
benchmark models, achieving a new state-of-the-art performance, and shows great
generalization and effectiveness on PSG dataset.
</p>
</div>
</dd>
<dt><a name="item745">[745]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05868" title="Abstract">arXiv:2310.05868</a> [<a href="/pdf/2310.05868" title="Download PDF">pdf</a>, <a href="/format/2310.05868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bio-inspired computational memory model of the Hippocampus: an approach  to a neuromorphic spike-based Content-Addressable Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Casanueva-Morato%2C+D">Daniel Casanueva-Morato</a>, 
<a href="/search/cs?searchtype=author&query=Ayuso-Martinez%2C+A">Alvaro Ayuso-Martinez</a>, 
<a href="/search/cs?searchtype=author&query=Dominguez-Morales%2C+J+P">Juan P. Dominguez-Morales</a>, 
<a href="/search/cs?searchtype=author&query=Jimenez-Fernandez%2C+A">Angel Jimenez-Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=Jimenez-Moreno%2C+G">Gabriel Jimenez-Moreno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures, journal, Spiking Neural Network
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The brain has computational capabilities that surpass those of modern
systems, being able to solve complex problems efficiently in a simple way.
Neuromorphic engineering aims to mimic biology in order to develop new systems
capable of incorporating such capabilities. Bio-inspired learning systems
continue to be a challenge that must be solved, and much work needs to be done
in this regard. Among all brain regions, the hippocampus stands out as an
autoassociative short-term memory with the capacity to learn and recall
memories from any fragment of them. These characteristics make the hippocampus
an ideal candidate for developing bio-inspired learning systems that, in
addition, resemble content-addressable memories. Therefore, in this work we
propose a bio-inspired spiking content-addressable memory model based on the
CA3 region of the hippocampus with the ability to learn, forget and recall
memories, both orthogonal and non-orthogonal, from any fragment of them. The
model was implemented on the SpiNNaker hardware platform using Spiking Neural
Networks. A set of experiments based on functional, stress and applicability
tests were performed to demonstrate its correct functioning. This work presents
the first hardware implementation of a fully-functional bio-inspired spiking
hippocampal content-addressable memory model, paving the way for the
development of future more complex neuromorphic systems.
</p>
</div>
</dd>
<dt><a name="item746">[746]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05869" title="Abstract">arXiv:2310.05869</a> [<a href="/pdf/2310.05869" title="Download PDF">pdf</a>, <a href="/format/2310.05869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperAttention: Long-context Attention in Near-Linear Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+I">Insu Han</a>, 
<a href="/search/cs?searchtype=author&query=Jarayam%2C+R">Rajesh Jarayam</a>, 
<a href="/search/cs?searchtype=author&query=Karbasi%2C+A">Amin Karbasi</a>, 
<a href="/search/cs?searchtype=author&query=Mirrokni%2C+V">Vahab Mirrokni</a>, 
<a href="/search/cs?searchtype=author&query=Woodruff%2C+D+P">David P. Woodruff</a>, 
<a href="/search/cs?searchtype=author&query=Zandieh%2C+A">Amir Zandieh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present an approximate attention mechanism named HyperAttention to address
the computational challenges posed by the growing complexity of long contexts
used in Large Language Models (LLMs). Recent work suggests that in the
worst-case scenario, quadratic time is necessary unless the entries of the
attention matrix are bounded or the matrix has low stable rank. We introduce
two parameters which measure: (1) the max column norm in the normalized
attention matrix, and (2) the ratio of row norms in the unnormalized attention
matrix after detecting and removing large entries. We use these fine-grained
parameters to capture the hardness of the problem. Despite previous lower
bounds, we are able to achieve a linear time sampling algorithm even when the
matrix has unbounded entries or a large stable rank, provided the above
parameters are small. HyperAttention features a modular design that easily
accommodates integration of other fast low-level implementations, particularly
FlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to
identify large entries, HyperAttention outperforms existing methods, giving
significant speed improvements compared to state-of-the-art solutions like
FlashAttention. We validate the empirical performance of HyperAttention on a
variety of different long-context length datasets. For example, HyperAttention
makes the inference time of ChatGLM2 50\% faster on 32k context length while
perplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k,
with causal masking, HyperAttention offers 5-fold speedup on a single attention
layer.
</p>
</div>
</dd>
<dt><a name="item747">[747]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05871" title="Abstract">arXiv:2310.05871</a> [<a href="/pdf/2310.05871" title="Download PDF">pdf</a>, <a href="/format/2310.05871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic value alignment through preference aggregation of multiple  objectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Korecki%2C+M">Marcin Korecki</a>, 
<a href="/search/cs?searchtype=author&query=Dailisan%2C+D">Damian Dailisan</a>, 
<a href="/search/cs?searchtype=author&query=Carissimo%2C+C">Cesare Carissimo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">The development of ethical AI systems is currently geared toward setting
objective functions that align with human objectives. However, finding such
functions remains a research challenge, while in RL, setting rewards by hand is
a fairly standard approach. We present a methodology for dynamic value
alignment, where the values that are to be aligned with are dynamically
changing, using a multiple-objective approach. We apply this approach to extend
Deep $Q$-Learning to accommodate multiple objectives and evaluate this method
on a simplified two-leg intersection controlled by a switching agent.Our
approach dynamically accommodates the preferences of drivers on the system and
achieves better overall performance across three metrics (speeds, stops, and
waits) while integrating objectives that have competing or conflicting actions.
</p>
</div>
</dd>
<dt><a name="item748">[748]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05872" title="Abstract">arXiv:2310.05872</a> [<a href="/pdf/2310.05872" title="Download PDF">pdf</a>, <a href="/format/2310.05872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViCor: Bridging Visual Understanding and Commonsense Reasoning with  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kaiwen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kwonjoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Misu%2C+T">Teruhisa Misu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X+E">Xin Eric Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">In our work, we explore the synergistic capabilities of pre-trained
vision-and-language models (VLMs) and large language models (LLMs) for visual
commonsense reasoning (VCR). We categorize the problem of VCR into visual
commonsense understanding (VCU) and visual commonsense inference (VCI). For
VCU, which involves perceiving the literal visual content, pre-trained VLMs
exhibit strong cross-dataset generalization. On the other hand, in VCI, where
the goal is to infer conclusions beyond image content, VLMs face difficulties.
We find that a baseline where VLMs provide perception results (image captions)
to LLMs leads to improved performance on VCI. However, we identify a challenge
with VLMs' passive perception, which often misses crucial context information,
leading to incorrect or uncertain reasoning by LLMs. To mitigate this issue, we
suggest a collaborative approach where LLMs, when uncertain about their
reasoning, actively direct VLMs to concentrate on and gather relevant visual
elements to support potential commonsense inferences. In our method, named
ViCor, pre-trained LLMs serve as problem classifiers to analyze the problem
category, VLM commanders to leverage VLMs differently based on the problem
classification, and visual commonsense reasoners to answer the question. VLMs
will perform visual recognition and understanding. We evaluate our framework on
two VCR benchmark datasets and outperform all other methods that do not require
in-domain supervised fine-tuning.
</p>
</div>
</dd>
<dt><a name="item749">[749]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05873" title="Abstract">arXiv:2310.05873</a> [<a href="/pdf/2310.05873" title="Download PDF">pdf</a>, <a href="/format/2310.05873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geom-Erasing: Geometry-Driven Removal of Implicit Concept in Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhili Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jianhua Han</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lanqing Hong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+D">Dit-Yan Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J">James Kwok</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Fine-tuning diffusion models through personalized datasets is an acknowledged
method for improving generation quality across downstream tasks, which,
however, often inadvertently generates unintended concepts such as watermarks
and QR codes, attributed to the limitations in image sources and collecting
methods within specific downstream tasks. Existing solutions suffer from
eliminating these unintentionally learned implicit concepts, primarily due to
the dependency on the model's ability to recognize concepts that it actually
cannot discern. In this work, we introduce \methodname, a novel approach that
successfully removes the implicit concepts with either an additional accessible
classifier or detector model to encode geometric information of these concepts
into text domain. Moreover, we propose \textit{Implicit Concept}, a novel
image-text dataset imbued with three implicit concepts (\ie, watermarks, QR
codes, and text) for training and evaluation. Experimental results demonstrate
that \methodname not only identifies but also proficiently eradicates implicit
concepts, revealing a significant improvement over the existing methods. The
integration of geometric information marks a substantial progression in the
precise removal of implicit concepts in diffusion models.
</p>
</div>
</dd>
<dt><a name="item750">[750]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05876" title="Abstract">arXiv:2310.05876</a> [<a href="/pdf/2310.05876" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Systems of Concern
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Matteucci%2C+K">Kayla Matteucci</a>, 
<a href="/search/cs?searchtype=author&query=Avin%2C+S">Shahar Avin</a>, 
<a href="/search/cs?searchtype=author&query=Barez%2C+F">Fazl Barez</a>, 
<a href="/search/cs?searchtype=author&query=h%C3%89igeartaigh%2C+S+%C3%93">Se&#xe1;n &#xd3; h&#xc9;igeartaigh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 figure, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Concerns around future dangers from advanced AI often centre on systems
hypothesised to have intrinsic characteristics such as agent-like behaviour,
strategic awareness, and long-range planning. We label this cluster of
characteristics as "Property X". Most present AI systems are low in "Property
X"; however, in the absence of deliberate steering, current research directions
may rapidly lead to the emergence of highly capable AI systems that are also
high in "Property X". We argue that "Property X" characteristics are
intrinsically dangerous, and when combined with greater capabilities will
result in AI systems for which safety and control is difficult to guarantee.
Drawing on several scholars' alternative frameworks for possible AI research
trajectories, we argue that most of the proposed benefits of advanced AI can be
obtained by systems designed to minimise this property. We then propose
indicators and governance interventions to identify and limit the development
of systems with risky "Property X" characteristics.
</p>
</div>
</dd>
<dt><a name="item751">[751]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05878" title="Abstract">arXiv:2310.05878</a> [<a href="/pdf/2310.05878" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Machine Learning Approach to Predicting Single Event Upsets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Archit Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Eng%2C+C+Y">Chong Yock Eng</a>, 
<a href="/search/cs?searchtype=author&query=Wee%2C+D+L+M">Deon Lim Meng Wee</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+R+A">Rashna Analia Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Sim%2C+S+M">See Min Sim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A single event upset (SEU) is a critical soft error that occurs in
semiconductor devices on exposure to ionising particles from space
environments. SEUs cause bit flips in the memory component of semiconductors.
This creates a multitude of safety hazards as stored information becomes less
reliable. Currently, SEUs are only detected several hours after their
occurrence. CREMER, the model presented in this paper, predicts SEUs in advance
using machine learning. CREMER uses only positional data to predict SEU
occurrence, making it robust, inexpensive and scalable. Upon implementation,
the improved reliability of memory devices will create a digitally safer
environment onboard space vehicles.
</p>
</div>
</dd>
<dt><a name="item752">[752]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05881" title="Abstract">arXiv:2310.05881</a> [<a href="/pdf/2310.05881" title="Download PDF">pdf</a>, <a href="/format/2310.05881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controllable Chest X-Ray Report Generation from Longitudinal  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Serra%2C+F+D">Francesco Dalla Serra</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaoyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Deligianni%2C+F">Fani Deligianni</a>, 
<a href="/search/cs?searchtype=author&query=Dalton%2C+J">Jeffrey Dalton</a>, 
<a href="/search/cs?searchtype=author&query=O%27Neil%2C+A+Q">Alison Q O&#x27;Neil</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Radiology reports are detailed text descriptions of the content of medical
scans. Each report describes the presence/absence and location of relevant
clinical findings, commonly including comparison with prior exams of the same
patient to describe how they evolved. Radiology reporting is a time-consuming
process, and scan results are often subject to delays. One strategy to speed up
reporting is to integrate automated reporting systems, however clinical
deployment requires high accuracy and interpretability. Previous approaches to
automated radiology reporting generally do not provide the prior study as
input, precluding comparison which is required for clinical accuracy in some
types of scans, and offer only unreliable methods of interpretability.
Therefore, leveraging an existing visual input format of anatomical tokens, we
introduce two novel aspects: (1) longitudinal representation learning -- we
input the prior scan as an additional input, proposing a method to align,
concatenate and fuse the current and prior visual information into a joint
longitudinal representation which can be provided to the multimodal report
generation model; (2) sentence-anatomy dropout -- a training strategy for
controllability in which the report generator model is trained to predict only
sentences from the original report which correspond to the subset of anatomical
regions given as input. We show through in-depth experiments on the MIMIC-CXR
dataset how the proposed approach achieves state-of-the-art results while
enabling anatomy-wise controllable report generation.
</p>
</div>
</dd>
<dt><a name="item753">[753]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05882" title="Abstract">arXiv:2310.05882</a> [<a href="/pdf/2310.05882" title="Download PDF">pdf</a>, <a href="/format/2310.05882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating a VR System for Collecting Safety-Critical Vehicle-Pedestrian  Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weng%2C+E">Erica Weng</a>, 
<a href="/search/cs?searchtype=author&query=Mukoya%2C+K">Kenta Mukoya</a>, 
<a href="/search/cs?searchtype=author&query=Ramanan%2C+D">Deva Ramanan</a>, 
<a href="/search/cs?searchtype=author&query=Kitani%2C+K">Kris Kitani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In submission to CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Autonomous vehicles (AVs) require comprehensive and reliable pedestrian
trajectory data to ensure safe operation. However, obtaining data of
safety-critical scenarios such as jaywalking and near-collisions, or uncommon
agents such as children, disabled pedestrians, and vulnerable road users poses
logistical and ethical challenges. This paper evaluates a Virtual Reality (VR)
system designed to collect pedestrian trajectory and body pose data in a
controlled, low-risk environment. We substantiate the usefulness of such a
system through semi-structured interviews with professionals in the AV field,
and validate the effectiveness of the system through two empirical studies: a
first-person user evaluation involving 62 participants, and a third-person
evaluative survey involving 290 respondents. Our findings demonstrate that the
VR-based data collection system elicits realistic responses for capturing
pedestrian data in safety-critical or uncommon vehicle-pedestrian interaction
scenarios.
</p>
</div>
</dd>
<dt><a name="item754">[754]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05884" title="Abstract">arXiv:2310.05884</a> [<a href="/pdf/2310.05884" title="Download PDF">pdf</a>, <a href="/format/2310.05884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Meta-Learning Perspective on Transformers for Causal Language Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xinbo Wu</a>, 
<a href="/search/cs?searchtype=author&query=Varshney%2C+L+R">Lav R. Varshney</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">The Transformer architecture has become prominent in developing large causal
language models. However, mechanisms to explain its capabilities are not well
understood. Focused on the training process, here we establish a meta-learning
view of the Transformer architecture when trained for the causal language
modeling task, by explicating an inner optimization process that may happen
within the Transformer. Further, from within the inner optimization, we
discover and theoretically analyze a special characteristic of the norms of
learned token representations within Transformer-based causal language models.
Our analysis is supported by experiments conducted on pre-trained large
language models and real-world data.
</p>
</div>
</dd>
<dt><a name="item755">[755]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05885" title="Abstract">arXiv:2310.05885</a> [<a href="/pdf/2310.05885" title="Download PDF">pdf</a>, <a href="/format/2310.05885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DTPP: Differentiable Joint Conditional Prediction and Cost Evaluation  for Tree Policy Planning in Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhiyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Karkus%2C+P">Peter Karkus</a>, 
<a href="/search/cs?searchtype=author&query=Ivanovic%2C+B">Boris Ivanovic</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pavone%2C+M">Marco Pavone</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Chen Lv</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Motion prediction and cost evaluation are vital components in the
decision-making system of autonomous vehicles. However, existing methods often
ignore the importance of cost learning and treat them as separate modules. In
this study, we employ a tree-structured policy planner and propose a
differentiable joint training framework for both ego-conditioned prediction and
cost models, resulting in a direct improvement of the final planning
performance. For conditional prediction, we introduce a query-centric
Transformer model that performs efficient ego-conditioned motion prediction.
For planning cost, we propose a learnable context-aware cost function with
latent interaction features, facilitating differentiable joint learning. We
validate our proposed approach using the real-world nuPlan dataset and its
associated planning test platform. Our framework not only matches
state-of-the-art planning methods but outperforms other learning-based methods
in planning quality, while operating more efficiently in terms of runtime. We
show that joint training delivers significantly better performance than
separate training of the two modules. Additionally, we find that
tree-structured policy planning outperforms the conventional single-stage
planning approach.
</p>
</div>
</dd>
<dt><a name="item756">[756]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05886" title="Abstract">arXiv:2310.05886</a> [<a href="/pdf/2310.05886" title="Download PDF">pdf</a>, <a href="/format/2310.05886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Streaming Anchor Loss: Augmenting Supervision with Temporal Significance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Utkarsh">Utkarsh</a> (Oggy)
<a href="/search/cs?searchtype=author&query=Sarawgi">Sarawgi</a>, 
<a href="/search/cs?searchtype=author&query=Berkowitz%2C+J">John Berkowitz</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+V">Vineet Garg</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+A">Arnav Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+M">Minsik Cho</a>, 
<a href="/search/cs?searchtype=author&query=Buddi%2C+S+S">Sai Srujana Buddi</a>, 
<a href="/search/cs?searchtype=author&query=Adya%2C+S">Saurabh Adya</a>, 
<a href="/search/cs?searchtype=author&query=Tewfik%2C+A">Ahmed Tewfik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review for ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Streaming neural network models for fast frame-wise responses to various
speech and sensory signals are widely adopted on resource-constrained
platforms. Hence, increasing the learning capacity of such streaming models
(i.e., by adding more parameters) to improve the predictive power may not be
viable for real-world tasks. In this work, we propose a new loss, Streaming
Anchor Loss (SAL), to better utilize the given learning capacity by encouraging
the model to learn more from essential frames. More specifically, our SAL and
its focal variations dynamically modulate the frame-wise cross entropy loss
based on the importance of the corresponding frames so that a higher loss
penalty is assigned for frames within the temporal proximity of semantically
critical events. Therefore, our loss ensures that the model training focuses on
predicting the relatively rare but task-relevant frames. Experimental results
with standard lightweight convolutional and recurrent streaming networks on
three different speech based detection tasks demonstrate that SAL enables the
model to learn the overall task more effectively with improved accuracy and
latency, without any additional data, model parameters, or architectural
changes.
</p>
</div>
</dd>
<dt><a name="item757">[757]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05898" title="Abstract">arXiv:2310.05898</a> [<a href="/pdf/2310.05898" title="Download PDF">pdf</a>, <a href="/format/2310.05898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lizhang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+K">Kaizhao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Applications (stat.AP); Machine Learning (stat.ML)

</div>
<p class="mathjax">Lion (Evolved Sign Momentum), a new optimizer discovered through program
search, has shown promising results in training large AI models. It performs
comparably or favorably to AdamW but with greater memory efficiency. As we can
expect from the results of a random search program, Lion incorporates elements
from several existing algorithms, including signed momentum, decoupled weight
decay, Polak, and Nesterov momentum, but does not fit into any existing
category of theoretically grounded optimizers. Thus, even though Lion appears
to perform well as a general-purpose optimizer for a wide range of tasks, its
theoretical basis remains uncertain. This lack of theoretical clarity limits
opportunities to further enhance and expand Lion's efficacy.
<br />This work aims to demystify Lion. Based on both continuous-time and
discrete-time analysis, we demonstrate that Lion is a theoretically novel and
principled approach for minimizing a general loss function $f(x)$ while
enforcing a bound constraint $\|x\|_\infty \leq 1/\lambda$. Lion achieves this
through the incorporation of decoupled weight decay, where $\lambda$ represents
the weight decay coefficient. Our analysis is made possible by the development
of a new Lyapunov function for the Lion updates. It applies to a broader family
of Lion-$\kappa$ algorithms, where the $\text{sign}(\cdot)$ operator in Lion is
replaced by the subgradient of a convex function $\kappa$, leading to the
solution of a general composite optimization problem of $\min_x f(x) +
\kappa^*(x)$. Our findings provide valuable insights into the dynamics of Lion
and pave the way for further improvements and extensions of Lion-related
algorithms.
</p>
</div>
</dd>
<dt><a name="item758">[758]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05899" title="Abstract">arXiv:2310.05899</a> [<a href="/pdf/2310.05899" title="Download PDF">pdf</a>, <a href="/format/2310.05899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Intelligent Vehicular Networks Through Distributed Learning in  the Non-Terrestrial Networks 6G Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naseh%2C+D">David Naseh</a>, 
<a href="/search/cs?searchtype=author&query=Shinde%2C+S+S">Swapnil Sadashiv Shinde</a>, 
<a href="/search/cs?searchtype=author&query=Tarchi%2C+D">Daniele Tarchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">The forthcoming 6G-enabled Intelligent Transportation System (ITS) is set to
redefine conventional transportation networks with advanced intelligent
services and applications. These technologies, including edge computing,
Machine Learning (ML), and network softwarization, pose stringent requirements
for latency, energy efficiency, and user data security. Distributed Learning
(DL), such as Federated Learning (FL), is essential to meet these demands by
distributing the learning process at the network edge. However, traditional FL
approaches often require substantial resources for satisfactory learning
performance. In contrast, Transfer Learning (TL) and Split Learning (SL) have
shown effectiveness in enhancing learning efficiency in resource-constrained
wireless scenarios like ITS. Non-terrestrial Networks (NTNs) have recently
acquired a central place in the 6G vision, especially for boosting the
coverage, capacity, and resilience of traditional terrestrial facilities.
Air-based NTN layers, such as High Altitude Platforms (HAPs), can have added
advantages in terms of reduced transmission distances and flexible deployments
and thus can be exploited to enable intelligent solutions for latency-critical
vehicular scenarios. With this motivation, in this work, we introduce the
concept of Federated Split Transfer Learning (FSTL) in joint air-ground
networks for resource-constrained vehicular scenarios. Simulations carried out
in vehicular scenarios validate the efficacy of FSTL on HAPs in NTN,
demonstrating significant improvements in addressing the demands of ITS
applications.
</p>
</div>
</dd>
<dt><a name="item759">[759]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05904" title="Abstract">arXiv:2310.05904</a> [<a href="/pdf/2310.05904" title="Download PDF">pdf</a>, <a href="/format/2310.05904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Multi-Fidelity Impedance Tuning for Human-Robot Cooperative  Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lau%2C+E">Ethan Lau</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+V">Vaibhav Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Bopardikar%2C+S+D">Shaunak D. Bopardikar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures. Submitted to the 2024 ACC on September 29, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">We examine how a human-robot interaction (HRI) system may be designed when
input-output data from previous experiments are available. In particular, we
consider how to select an optimal impedance in the assistance design for a
cooperative manipulation task with a new operator. Due to the variability
between individuals, the design parameters that best suit one operator of the
robot may not be the best parameters for another one. However, by incorporating
historical data using a linear auto-regressive (AR-1) Gaussian process, the
search for a new operator's optimal parameters can be accelerated. We lay out a
framework for optimizing the human-robot cooperative manipulation that only
requires input-output data. We establish how the AR-1 model improves the bound
on the regret and numerically simulate a human-robot cooperative manipulation
task to show the regret improvement. Further, we show how our approach's
input-output nature provides robustness against modeling error through an
additional numerical study.
</p>
</div>
</dd>
<dt><a name="item760">[760]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05905" title="Abstract">arXiv:2310.05905</a> [<a href="/pdf/2310.05905" title="Download PDF">pdf</a>, <a href="/format/2310.05905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TAIL: Task-specific Adapters for Imitation Learning with Large  Pretrained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jesse Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Asadi%2C+K">Kavosh Asadi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Ding Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Sabach%2C+S">Shoham Sabach</a>, 
<a href="/search/cs?searchtype=author&query=Fakoor%2C+R">Rasool Fakoor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 8 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">The full potential of large pretrained models remains largely untapped in
control domains like robotics. This is mainly because of the scarcity of data
and the computational challenges associated with training or fine-tuning these
large models for such applications. Prior work mainly emphasizes effective
pretraining of large models for decision-making, with little exploration into
how to perform data-efficient continual adaptation of these models for new
tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters
for Imitation Learning), a framework for efficient adaptation to new control
tasks. Inspired by recent advancements in parameter-efficient fine-tuning in
language domains, we explore efficient fine-tuning techniques -- e.g.,
Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA) -- in TAIL to
adapt large pretrained models for new tasks with limited demonstration data.
Our extensive experiments in large-scale language-conditioned manipulation
tasks comparing prevalent parameter-efficient fine-tuning techniques and
adaptation baselines suggest that TAIL with LoRA can achieve the best
post-adaptation performance with only 1\% of the trainable parameters of full
fine-tuning, while avoiding catastrophic forgetting and preserving adaptation
plasticity in continual learning settings.
</p>
</div>
</dd>
<dt><a name="item761">[761]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05910" title="Abstract">arXiv:2310.05910</a> [<a href="/pdf/2310.05910" title="Download PDF">pdf</a>, <a href="/format/2310.05910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SALMON: Self-Alignment with Principle-Following Reward Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhiqing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yikang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qinhong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenfang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cox%2C+D">David Cox</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yiming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+C">Chuang Gan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://github.com/IBM/SALMON">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Supervised Fine-Tuning (SFT) on response demonstrations combined with
Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful
paradigm for aligning LLM-based AI agents. However, a significant limitation of
such an approach is its dependency on high-quality human annotations, making
its application to intricate tasks challenging due to difficulties in obtaining
consistent response demonstrations and in-distribution response preferences.
This paper presents a novel approach, namely SALMON (Self-ALignMent with
principle-fOllowiNg reward models), to align base language models with minimal
human supervision, using only a small set of human-defined principles, yet
achieving superior performance. Central to our approach is a
principle-following reward model. Trained on synthetic preference data, this
model can generate reward scores based on arbitrary human-defined principles.
By merely adjusting these principles during the RL training phase, we gain full
control over the preferences with the reward model, subsequently influencing
the behavior of the RL-trained policies, and eliminating the reliance on the
collection of online human preferences. Applying our method to the LLaMA-2-70b
base language model, we developed an AI assistant named Dromedary-2. With only
6 exemplars for in-context learning and 31 human-defined principles,
Dromedary-2 significantly surpasses the performance of several state-of-the-art
AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have
open-sourced the code and model weights to encourage further research into
aligning LLM-based AI agents with enhanced supervision efficiency, improved
controllability, and scalable oversight.
</p>
</div>
</dd>
<dt><a name="item762">[762]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05911" title="Abstract">arXiv:2310.05911</a> [<a href="/pdf/2310.05911" title="Download PDF">pdf</a>, <a href="/format/2310.05911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy Management in a Cooperative Energy Harvesting Wireless Sensor  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Barat%2C+A">Arghyadeep Barat</a>, 
<a href="/search/eess?searchtype=author&query=J%2C+P+K">Prabuchandran.K.J</a>, 
<a href="/search/eess?searchtype=author&query=Bhatnagar%2C+S">Shalabh Bhatnagar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we consider the problem of finding an optimal energy
management policy for a network of sensor nodes capable of harvesting their own
energy and sharing it with other nodes in the network. We formulate this
problem in the discounted cost Markov decision process framework and obtain
good energy-sharing policies using the Deep Deterministic Policy Gradient
(DDPG) algorithm. Earlier works have attempted to obtain the optimal energy
allocation policy for a single sensor and for multiple sensors arranged on a
mote with a single centralized energy buffer. Our algorithms, on the other
hand, provide optimal policies for a distributed network of sensors
individually harvesting energy and capable of sharing energy amongst
themselves. Through simulations, we illustrate that the policies obtained by
our DDPG algorithm using this enhanced network model outperform algorithms that
do not share energy or use a centralized energy buffer in the distributed
multi-nodal case.
</p>
</div>
</dd>
<dt><a name="item763">[763]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05914" title="Abstract">arXiv:2310.05914</a> [<a href="/pdf/2310.05914" title="Download PDF">pdf</a>, <a href="/format/2310.05914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NEFTune: Noisy Embeddings Improve Instruction Finetuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+N">Neel Jain</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+P">Ping-yeh Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Yuxin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Kirchenbauer%2C+J">John Kirchenbauer</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+H">Hong-Min Chu</a>, 
<a href="/search/cs?searchtype=author&query=Somepalli%2C+G">Gowthami Somepalli</a>, 
<a href="/search/cs?searchtype=author&query=Bartoldson%2C+B+R">Brian R. Bartoldson</a>, 
<a href="/search/cs?searchtype=author&query=Kailkhura%2C+B">Bhavya Kailkhura</a>, 
<a href="/search/cs?searchtype=author&query=Schwarzschild%2C+A">Avi Schwarzschild</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+A">Aniruddha Saha</a>, 
<a href="/search/cs?searchtype=author&query=Goldblum%2C+M">Micah Goldblum</a>, 
<a href="/search/cs?searchtype=author&query=Geiping%2C+J">Jonas Geiping</a>, 
<a href="/search/cs?searchtype=author&query=Goldstein%2C+T">Tom Goldstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, Code is available on Github: <a href="https://github.com/neelsjain/NEFTune">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We show that language model finetuning can be improved, sometimes
dramatically, with a simple augmentation. NEFTune adds noise to the embedding
vectors during training. Standard finetuning of LLaMA-2-7B using Alpaca
achieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings.
NEFTune also improves over strong baselines on modern instruction datasets.
Models trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8%
improvement, and with OpenPlatypus an 8% improvement. Even powerful models
further refined with RLHF such as LLaMA-2-Chat benefit from additional training
with NEFTune.
</p>
</div>
</dd>
<dt><a name="item764">[764]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05915" title="Abstract">arXiv:2310.05915</a> [<a href="/pdf/2310.05915" title="Download PDF">pdf</a>, <a href="/format/2310.05915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FireAct: Toward Language Agent Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Baian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+C">Chang Shu</a>, 
<a href="/search/cs?searchtype=author&query=Shareghi%2C+E">Ehsan Shareghi</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+N">Nigel Collier</a>, 
<a href="/search/cs?searchtype=author&query=Narasimhan%2C+K">Karthik Narasimhan</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+S">Shunyu Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code, data, and models are available at <a href="https://fireact-agent.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent efforts have augmented language models (LMs) with external tools or
environments, leading to the development of language agents that can reason and
act. However, most of these agents rely on few-shot prompting techniques with
off-the-shelf LMs. In this paper, we investigate and argue for the overlooked
direction of fine-tuning LMs to obtain language agents. Using a setup of
question answering (QA) with a Google search API, we explore a variety of base
LMs, prompting methods, fine-tuning data, and QA tasks, and find language
agents are consistently improved after fine-tuning their backbone LMs. For
example, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4
leads to a 77% HotpotQA performance increase. Furthermore, we propose FireAct,
a novel approach to fine-tuning LMs with trajectories from multiple tasks and
prompting methods, and show having more diverse fine-tuning data can further
improve agents. Along with other findings regarding scaling effects,
robustness, generalization, efficiency and cost, our work establishes
comprehensive benefits of fine-tuning LMs for agents, and provides an initial
set of experimental designs, insights, as well as open questions toward
language agent fine-tuning.
</p>
</div>
</dd>
<dt><a name="item765">[765]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05916" title="Abstract">arXiv:2310.05916</a> [<a href="/pdf/2310.05916" title="Download PDF">pdf</a>, <a href="/format/2310.05916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting CLIP&#x27;s Image Representation via Text-Based Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gandelsman%2C+Y">Yossi Gandelsman</a>, 
<a href="/search/cs?searchtype=author&query=Efros%2C+A+A">Alexei A. Efros</a>, 
<a href="/search/cs?searchtype=author&query=Steinhardt%2C+J">Jacob Steinhardt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page and code: <a href="https://yossigandelsman.github.io/clip_decomposition/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We investigate the CLIP image encoder by analyzing how individual model
components affect the final representation. We decompose the image
representation as a sum across individual image patches, model layers, and
attention heads, and use CLIP's text representation to interpret the summands.
Interpreting the attention heads, we characterize each head's role by
automatically finding text representations that span its output space, which
reveals property-specific roles for many heads (e.g. location or shape). Next,
interpreting the image patches, we uncover an emergent spatial localization
within CLIP. Finally, we use this understanding to remove spurious features
from CLIP and to create a strong zero-shot image segmenter. Our results
indicate that a scalable understanding of transformer models is attainable and
can be used to repair and improve models.
</p>
</div>
</dd>
<dt><a name="item766">[766]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05917" title="Abstract">arXiv:2310.05917</a> [<a href="/pdf/2310.05917" title="Download PDF">pdf</a>, <a href="/format/2310.05917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Drivable Avatar Clothing: Faithful Full-Body Telepresence with Dynamic  Clothing Driven by Sparse RGB-D Input
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+D">Donglai Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Prada%2C+F">Fabian Prada</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhe Cao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+K">Kaiwen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chenglei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hodgins%2C+J">Jessica Hodgins</a>, 
<a href="/search/cs?searchtype=author&query=Bagautdinov%2C+T">Timur Bagautdinov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGGRAPH Asia 2023 Conference Paper. Project website: <a href="https://xiangdonglai.github.io/www-sa23-drivable-clothing/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Clothing is an important part of human appearance but challenging to model in
photorealistic avatars. In this work we present avatars with dynamically moving
loose clothing that can be faithfully driven by sparse RGB-D inputs as well as
body and face motion. We propose a Neural Iterative Closest Point (N-ICP)
algorithm that can efficiently track the coarse garment shape given sparse
depth input. Given the coarse tracking results, the input RGB-D images are then
remapped to texel-aligned features, which are fed into the drivable avatar
models to faithfully reconstruct appearance details. We evaluate our method
against recent image-driven synthesis baselines, and conduct a comprehensive
analysis of the N-ICP algorithm. We demonstrate that our method can generalize
to a novel testing environment, while preserving the ability to produce
high-fidelity and faithful clothing dynamics and appearance.
</p>
</div>
</dd>
<dt><a name="item767">[767]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05918" title="Abstract">arXiv:2310.05918</a> [<a href="/pdf/2310.05918" title="Download PDF">pdf</a>, <a href="/format/2310.05918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grokking as Compression: A Nonlinear Complexity Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Ziqian Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Tegmark%2C+M">Max Tegmark</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">We attribute grokking, the phenomenon where generalization is much delayed
after memorization, to compression. To do so, we define linear mapping number
(LMN) to measure network complexity, which is a generalized version of linear
region number for ReLU networks. LMN can nicely characterize neural network
compression before generalization. Although the $L_2$ norm has been a popular
choice for characterizing model complexity, we argue in favor of LMN for a
number of reasons: (1) LMN can be naturally interpreted as
information/computation, while $L_2$ cannot. (2) In the compression phase, LMN
has linear relations with test losses, while $L_2$ is correlated with test
losses in a complicated nonlinear way. (3) LMN also reveals an intriguing
phenomenon of the XOR network switching between two generalization solutions,
while $L_2$ does not. Besides explaining grokking, we argue that LMN is a
promising candidate as the neural network version of the Kolmogorov complexity
since it explicitly considers local or conditioned linear computations aligned
with the nature of modern artificial neural networks.
</p>
</div>
</dd>
<dt><a name="item768">[768]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05919" title="Abstract">arXiv:2310.05919</a> [<a href="/pdf/2310.05919" title="Download PDF">pdf</a>, <a href="/format/2310.05919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-Shot Spoken Language Understanding via Joint Speech-Text Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chien%2C+C">Chung-Ming Chien</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingjiamei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chou%2C+J">Ju-Chieh Chou</a>, 
<a href="/search/cs?searchtype=author&query=Livescu%2C+K">Karen Livescu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Recent work on speech representation models jointly pre-trained with text has
demonstrated the potential of improving speech representations by encoding
speech and text in a shared space. In this paper, we leverage such shared
representations to address the persistent challenge of limited data
availability in spoken language understanding tasks. By employing a pre-trained
speech-text model, we find that models fine-tuned on text can be effectively
transferred to speech testing data. With as little as 1 hour of labeled speech
data, our proposed approach achieves comparable performance on spoken language
understanding tasks (specifically, sentiment analysis and named entity
recognition) when compared to previous methods using speech-only pre-trained
models fine-tuned on 10 times more data. Beyond the proof-of-concept study, we
also analyze the latent representations. We find that the bottom layers of
speech-text models are largely task-agnostic and align speech and text
representations into a shared space, while the top layers are more
task-specific.
</p>
</div>
</dd>
<dt><a name="item769">[769]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05920" title="Abstract">arXiv:2310.05920</a> [<a href="/pdf/2310.05920" title="Download PDF">pdf</a>, <a href="/format/2310.05920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimPLR: A Simple and Plain Transformer for Object Detection and  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Duy-Kien Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Oswald%2C+M+R">Martin R. Oswald</a>, 
<a href="/search/cs?searchtype=author&query=Snoek%2C+C+G+M">Cees G. M. Snoek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The ability to detect objects in images at varying scales has played a
pivotal role in the design of modern object detectors. Despite considerable
progress in removing handcrafted components using transformers, multi-scale
feature maps remain a key factor for their empirical success, even with a plain
backbone like the Vision Transformer (ViT). In this paper, we show that this
reliance on feature pyramids is unnecessary and a transformer-based detector
with scale-aware attention enables the plain detector `SimPLR' whose backbone
and detection head both operate on single-scale features. The plain
architecture allows SimPLR to effectively take advantages of self-supervised
learning and scaling approaches with ViTs, yielding strong performance compared
to multi-scale counterparts. We demonstrate through our experiments that when
scaling to larger backbones, SimPLR indicates better performance than
end-to-end detectors (Mask2Former) and plain-backbone detectors (ViTDet), while
consistently being faster. The code will be released.
</p>
</div>
</dd>
<dt><a name="item770">[770]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05922" title="Abstract">arXiv:2310.05922</a> [<a href="/pdf/2310.05922" title="Download PDF">pdf</a>, <a href="/format/2310.05922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video  editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cong%2C+Y">Yuren Cong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mengmeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Simon%2C+C">Christian Simon</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shoufa Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jiawei Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yanping Xie</a>, 
<a href="/search/cs?searchtype=author&query=Perez-Rua%2C+J">Juan-Manuel Perez-Rua</a>, 
<a href="/search/cs?searchtype=author&query=Rosenhahn%2C+B">Bodo Rosenhahn</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+T">Tao Xiang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Sen He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://flatten-video-editing.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Text-to-video editing aims to edit the visual appearance of a source video
conditional on textual prompts. A major challenge in this task is to ensure
that all frames in the edited video are visually consistent. Most recent works
apply advanced text-to-image diffusion models to this task by inflating 2D
spatial attention in the U-Net into spatio-temporal attention. Although
temporal context can be added through spatio-temporal attention, it may
introduce some irrelevant information for each patch and therefore cause
inconsistency in the edited video. In this paper, for the first time, we
introduce optical flow into the attention module in the diffusion model's U-Net
to address the inconsistency issue for text-to-video editing. Our method,
FLATTEN, enforces the patches on the same flow path across different frames to
attend to each other in the attention module, thus improving the visual
consistency in the edited videos. Additionally, our method is training-free and
can be seamlessly integrated into any diffusion-based text-to-video editing
methods and improve their visual consistency. Experiment results on existing
text-to-video editing benchmarks show that our proposed method achieves the new
state-of-the-art performance. In particular, our method excels in maintaining
the visual consistency in the edited videos.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Tue, 10 Oct 23</h3>
<dl>
<dt><a name="item771">[771]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09920" title="Abstract">arXiv:2211.09920</a> (cross-list from eess.IV) [<a href="/pdf/2211.09920" title="Download PDF">pdf</a>, <a href="/format/2211.09920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Deep Joint Source-Channel Coding over a Multiple Access  Channel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yilmaz%2C+S+F">Selim F. Yilmaz</a>, 
<a href="/search/eess?searchtype=author&query=Karamanli%2C+C">Can Karamanli</a>, 
<a href="/search/eess?searchtype=author&query=Gunduz%2C+D">Deniz Gunduz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IEEE International Conference on Communications (ICC) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Theory (cs.IT); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">We consider distributed image transmission over a noisy multiple access
channel (MAC) using deep joint source-channel coding (DeepJSCC). It is known
that Shannon's separation theorem holds when transmitting independent sources
over a MAC in the asymptotic infinite block length regime. However, we are
interested in the practical finite block length regime, in which case separate
source and channel coding is known to be suboptimal. We introduce a novel joint
image compression and transmission scheme, where the devices send their
compressed image representations in a non-orthogonal manner. While
non-orthogonal multiple access (NOMA) is known to achieve the capacity region,
to the best of our knowledge, non-orthogonal joint source channel coding (JSCC)
scheme for practical systems has not been studied before. Through extensive
experiments, we show significant improvements in terms of the quality of the
reconstructed images compared to orthogonal transmission employing current
DeepJSCC approaches particularly for low bandwidth ratios. We publicly share
source code to facilitate further research and reproducibility.
</p>
</div>
</dd>
<dt><a name="item772">[772]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15889" title="Abstract">arXiv:2309.15889</a> (cross-list from eess.IV) [<a href="/pdf/2309.15889" title="Download PDF">pdf</a>, <a href="/format/2309.15889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High Perceptual Quality Wireless Image Delivery with Denoising Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yilmaz%2C+S+F">Selim F. Yilmaz</a>, 
<a href="/search/eess?searchtype=author&query=Niu%2C+X">Xueyan Niu</a>, 
<a href="/search/eess?searchtype=author&query=Bai%2C+B">Bo Bai</a>, 
<a href="/search/eess?searchtype=author&query=Han%2C+W">Wei Han</a>, 
<a href="/search/eess?searchtype=author&query=Deng%2C+L">Lei Deng</a>, 
<a href="/search/eess?searchtype=author&query=Gunduz%2C+D">Deniz Gunduz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Theory (cs.IT); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">We consider the image transmission problem over a noisy wireless channel via
deep learning-based joint source-channel coding (DeepJSCC) along with a
denoising diffusion probabilistic model (DDPM) at the receiver. Specifically,
we are interested in the perception-distortion trade-off in the practical
finite block length regime, in which separate source and channel coding can be
highly suboptimal. We introduce a novel scheme that utilizes the range-null
space decomposition of the target image. We transmit the range-space of the
image after encoding and employ DDPM to progressively refine its null space
contents. Through extensive experiments, we demonstrate significant
improvements in distortion and perceptual quality of reconstructed images
compared to standard DeepJSCC and the state-of-the-art generative
learning-based method. We will publicly share our source code to facilitate
further research and reproducibility.
</p>
</div>
</dd>
<dt><a name="item773">[773]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04428" title="Abstract">arXiv:2310.04428</a> (cross-list from physics.geo-ph) [<a href="/pdf/2310.04428" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Avalanche Prediction and Dynamics using Temperature Variance , Grain  Size Variance and Flow Regimes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Sharma%2C+A">Aditya Sharma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We investigate the effects of temperature variance, grain size variation,
flow regimes, and the use of Support Vector Machines (SVMs) in avalanche
studies. The temperature variance experiments involved ice single crystals and
polycrystals, revealing that the scale-free pattern of avalanche sizes remains
consistent regardless of temperature. The dynamics of dislocations in
polycrystals were found to be independent of stress level and temperature. The
Material Point Method (MPM) was used to explore snow avalanche behavior and
identify flow regimes. The MPM accurately represented various flow patterns of
snow avalanches, although challenges remained in capturing powder clouds. SVMs
were employed for avalanche forecasting, using meteorological and snowpack
variables as input features. The selected features provided insights into
snowfall characteristics, snow accumulation, rain interaction, snowdrift
patterns, cloud dynamics, snowpack mechanics, and temperature distribution
within the snowpack. The findings contribute to a better understanding of
avalanche dynamics and offer potential improvements in avalanche trend
predictions.
</p>
</div>
</dd>
<dt><a name="item774">[774]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04430" title="Abstract">arXiv:2310.04430</a> (cross-list from physics.geo-ph) [<a href="/pdf/2310.04430" title="Download PDF">pdf</a>, <a href="/format/2310.04430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint inversion of Time-Lapse Surface Gravity and Seismic Data for  Monitoring of 3D CO$_2$ Plumes via Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Celaya%2C+A">Adrian Celaya</a>, 
<a href="/search/physics?searchtype=author&query=Araya-Polo%2C+M">Mauricio Araya-Polo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce a fully 3D, deep learning-based approach for the joint inversion
of time-lapse surface gravity and seismic data for reconstructing subsurface
density and velocity models. The target application of this proposed inversion
approach is the prediction of subsurface CO2 plumes as a complementary tool for
monitoring CO2 sequestration deployments. Our joint inversion technique
outperforms deep learning-based gravity-only and seismic-only inversion models,
achieving improved density and velocity reconstruction, accurate segmentation,
and higher R-squared coefficients. These results indicate that deep
learning-based joint inversion is an effective tool for CO$_2$ storage
monitoring. Future work will focus on validating our approach with larger
datasets, simulations with other geological storage sites, and ultimately field
data.
</p>
</div>
</dd>
<dt><a name="item775">[775]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04442" title="Abstract">arXiv:2310.04442</a> (cross-list from physics.ins-det) [<a href="/pdf/2310.04442" title="Download PDF">pdf</a>, <a href="/format/2310.04442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Optimal use of Segmentation for Sampling Calorimeters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Acosta%2C+F+T">Fernando Torales Acosta</a>, 
<a href="/search/physics?searchtype=author&query=Karki%2C+B">Bishnu Karki</a>, 
<a href="/search/physics?searchtype=author&query=Karande%2C+P">Piyush Karande</a>, 
<a href="/search/physics?searchtype=author&query=Angerami%2C+A">Aaron Angerami</a>, 
<a href="/search/physics?searchtype=author&query=Arratia%2C+M">Miguel Arratia</a>, 
<a href="/search/physics?searchtype=author&query=Barish%2C+K">Kenneth Barish</a>, 
<a href="/search/physics?searchtype=author&query=Milton%2C+R">Ryan Milton</a>, 
<a href="/search/physics?searchtype=author&query=Mor%C3%A1n%2C+S">Sebasti&#xe1;n Mor&#xe1;n</a>, 
<a href="/search/physics?searchtype=author&query=Nachman%2C+B">Benjamin Nachman</a>, 
<a href="/search/physics?searchtype=author&query=Sinha%2C+A">Anshuman Sinha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Detectors (physics.ins-det)</span>; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex); High Energy Physics - Phenomenology (hep-ph); Nuclear Experiment (nucl-ex)

</div>
<p class="mathjax">One of the key design choices of any sampling calorimeter is how fine to make
the longitudinal and transverse segmentation. To inform this choice, we study
the impact of calorimeter segmentation on energy reconstruction. To ensure that
the trends are due entirely to hardware and not to a sub-optimal use of
segmentation, we deploy deep neural networks to perform the reconstruction.
These networks make use of all available information by representing the
calorimeter as a point cloud. To demonstrate our approach, we simulate a
detector similar to the forward calorimeter system intended for use in the ePIC
detector, which will operate at the upcoming Electron Ion Collider. We find
that for the energy estimation of isolated charged pion showers, relatively
fine longitudinal segmentation is key to achieving an energy resolution that is
better than 10% across the full phase space. These results provide a valuable
benchmark for ongoing EIC detector optimizations and may also inform future
studies involving high-granularity calorimeters in other experiments at various
facilities.
</p>
</div>
</dd>
<dt><a name="item776">[776]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04457" title="Abstract">arXiv:2310.04457</a> (cross-list from math.OC) [<a href="/pdf/2310.04457" title="Download PDF">pdf</a>, <a href="/format/2310.04457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProGO: Probabilistic Global Optimizer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>, 
<a href="/search/math?searchtype=author&query=Ghosh%2C+S">Sujit Ghosh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">In the field of global optimization, many existing algorithms face challenges
posed by non-convex target functions and high computational complexity or
unavailability of gradient information. These limitations, exacerbated by
sensitivity to initial conditions, often lead to suboptimal solutions or failed
convergence. This is true even for Metaheuristic algorithms designed to
amalgamate different optimization techniques to improve their efficiency and
robustness. To address these challenges, we develop a sequence of
multidimensional integration-based methods that we show to converge to the
global optima under some mild regularity conditions. Our probabilistic approach
does not require the use of gradients and is underpinned by a mathematically
rigorous convergence framework anchored in the nuanced properties of nascent
optima distribution. In order to alleviate the problem of multidimensional
integration, we develop a latent slice sampler that enjoys a geometric rate of
convergence in generating samples from the nascent optima distribution, which
is used to approximate the global optima. The proposed Probabilistic Global
Optimizer (ProGO) provides a scalable unified framework to approximate the
global optima of any continuous function defined on a domain of arbitrary
dimension. Empirical illustrations of ProGO across a variety of popular
non-convex test functions (having finite global optima) reveal that the
proposed algorithm outperforms, by order of magnitude, many existing
state-of-the-art methods, including gradient-based, zeroth-order gradient-free,
and some Bayesian Optimization methods, in term regret value and speed of
convergence. It is, however, to be noted that our approach may not be suitable
for functions that are expensive to compute.
</p>
</div>
</dd>
<dt><a name="item777">[777]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04463" title="Abstract">arXiv:2310.04463</a> (cross-list from q-bio.BM) [<a href="/pdf/2310.04463" title="Download PDF">pdf</a>, <a href="/format/2310.04463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusing on Two Levels and Optimizing for Multiple Properties: A Novel  Approach to Generating Molecules with Desirable Properties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Guo%2C+S">Siyuan Guo</a>, 
<a href="/search/q-bio?searchtype=author&query=Guan%2C+J">Jihong Guan</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhou%2C+S">Shuigeng Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the past decade, Artificial Intelligence driven drug design and discovery
has been a hot research topic, where an important branch is molecule generation
by generative models, from GAN-based models and VAE-based models to the latest
diffusion-based models. However, most existing models pursue only the basic
properties like validity and uniqueness of the generated molecules, a few go
further to explicitly optimize one single important molecular property (e.g.
QED or PlogP), which makes most generated molecules little usefulness in
practice. In this paper, we present a novel approach to generating molecules
with desirable properties, which expands the diffusion model framework with
multiple innovative designs. The novelty is two-fold. On the one hand,
considering that the structures of molecules are complex and diverse, and
molecular properties are usually determined by some substructures (e.g.
pharmacophores), we propose to perform diffusion on two structural levels:
molecules and molecular fragments respectively, with which a mixed Gaussian
distribution is obtained for the reverse diffusion process. To get desirable
molecular fragments, we develop a novel electronic effect based fragmentation
method. On the other hand, we introduce two ways to explicitly optimize
multiple molecular properties under the diffusion model framework. First, as
potential drug molecules must be chemically valid, we optimize molecular
validity by an energy-guidance function. Second, since potential drug molecules
should be desirable in various properties, we employ a multi-objective
mechanism to optimize multiple molecular properties simultaneously. Extensive
experiments with two benchmark datasets QM9 and ZINC250k show that the
molecules generated by our proposed method have better validity, uniqueness,
novelty, Fr\'echet ChemNet Distance (FCD), QED, and PlogP than those generated
by current SOTA models.
</p>
</div>
</dd>
<dt><a name="item778">[778]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04466" title="Abstract">arXiv:2310.04466</a> (cross-list from eess.IV) [<a href="/pdf/2310.04466" title="Download PDF">pdf</a>, <a href="/format/2310.04466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HartleyMHA: Self-Attention in Frequency Domain for Resolution-Robust and  Parameter-Efficient 3D Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wong%2C+K+C+L">Ken C. L. Wong</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Hongzhi Wang</a>, 
<a href="/search/eess?searchtype=author&query=Syeda-Mahmood%2C+T">Tanveer Syeda-Mahmood</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was accepted by the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2023). arXiv admin note: text overlap with <a href="/abs/2310.03872">arXiv:2310.03872</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">With the introduction of Transformers, different attention-based models have
been proposed for image segmentation with promising results. Although
self-attention allows capturing of long-range dependencies, it suffers from a
quadratic complexity in the image size especially in 3D. To avoid the
out-of-memory error during training, input size reduction is usually required
for 3D segmentation, but the accuracy can be suboptimal when the trained models
are applied on the original image size. To address this limitation, inspired by
the Fourier neural operator (FNO), we introduce the HartleyMHA model which is
robust to training image resolution with efficient self-attention. FNO is a
deep learning framework for learning mappings between functions in partial
differential equations, which has the appealing properties of zero-shot
super-resolution and global receptive field. We modify the FNO by using the
Hartley transform with shared parameters to reduce the model size by orders of
magnitude, and this allows us to further apply self-attention in the frequency
domain for more expressive high-order feature combination with improved
efficiency. When tested on the BraTS'19 dataset, it achieved superior
robustness to training image resolution than other tested models with less than
1% of their model parameters.
</p>
</div>
</dd>
<dt><a name="item779">[779]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04481" title="Abstract">arXiv:2310.04481</a> (cross-list from eess.AS) [<a href="/pdf/2310.04481" title="Download PDF">pdf</a>, <a href="/format/2310.04481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Acoustic and linguistic representations for speech continuous emotion  recognition in call center conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Macary%2C+M">Manon Macary</a>, 
<a href="/search/eess?searchtype=author&query=Tahon%2C+M">Marie Tahon</a>, 
<a href="/search/eess?searchtype=author&query=Est%C3%A8ve%2C+Y">Yannick Est&#xe8;ve</a>, 
<a href="/search/eess?searchtype=author&query=Luzzati%2C+D">Daniel Luzzati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">The goal of our research is to automatically retrieve the satisfaction and
the frustration in real-life call-center conversations. This study focuses an
industrial application in which the customer satisfaction is continuously
tracked down to improve customer services. To compensate the lack of large
annotated emotional databases, we explore the use of pre-trained speech
representations as a form of transfer learning towards AlloSat corpus.
Moreover, several studies have pointed out that emotion can be detected not
only in speech but also in facial trait, in biological response or in textual
information. In the context of telephone conversations, we can break down the
audio information into acoustic and linguistic by using the speech signal and
its transcription. Our experiments confirms the large gain in performance
obtained with the use of pre-trained features. Surprisingly, we found that the
linguistic content is clearly the major contributor for the prediction of
satisfaction and best generalizes to unseen data. Our experiments conclude to
the definitive advantage of using CamemBERT representations, however the
benefit of the fusion of acoustic and linguistic modalities is not as obvious.
With models learnt on individual annotations, we found that fusion approaches
are more robust to the subjectivity of the annotation task. This study also
tackles the problem of performances variability and intends to estimate this
variability from different views: weights initialization, confidence intervals
and annotation subjectivity. A deep analysis on the linguistic content
investigates interpretable factors able to explain the high contribution of the
linguistic modality for this task.
</p>
</div>
</dd>
<dt><a name="item780">[780]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04536" title="Abstract">arXiv:2310.04536</a> (cross-list from q-fin.RM) [<a href="/pdf/2310.04536" title="Download PDF">pdf</a>, <a href="/format/2310.04536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Portfolio Performance Using a Novel Method for Predicting  Financial Regimes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Pomorski%2C+P">Piotr Pomorski</a>, 
<a href="/search/q-fin?searchtype=author&query=Gorse%2C+D">Denise Gorse</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Risk Management (q-fin.RM)</span>; Computational Engineering, Finance, and Science (cs.CE); Portfolio Management (q-fin.PM)

</div>
<p class="mathjax">This work extends a previous work in regime detection, which allowed trading
positions to be profitably adjusted when a new regime was detected, to ex ante
prediction of regimes, leading to substantial performance improvements over the
earlier model, over all three asset classes considered (equities, commodities,
and foreign exchange), over a test period of four years. The proposed new model
is also benchmarked over this same period against a hidden Markov model, the
most popular current model for financial regime prediction, and against an
appropriate index benchmark for each asset class, in the case of the
commodities model having a test period cost-adjusted cumulative return over
four times higher than that expected from the index. Notably, the proposed
model makes use of a contrarian trading strategy, not uncommon in the financial
industry but relatively unexplored in machine learning models. The model also
makes use of frequent short positions, something not always desirable to
investors due to issues of both financial risk and ethics; however, it is
discussed how further work could remove this reliance on shorting and allow the
construction of a long-only version of the model.
</p>
</div>
</dd>
<dt><a name="item781">[781]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04567" title="Abstract">arXiv:2310.04567</a> (cross-list from eess.AS) [<a href="/pdf/2310.04567" title="Download PDF">pdf</a>, <a href="/format/2310.04567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DPM-TSE: A Diffusion Probabilistic Model for Target Sound Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hai%2C+J">Jiarui Hai</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Helin Wang</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+D">Dongchao Yang</a>, 
<a href="/search/eess?searchtype=author&query=Thakkar%2C+K">Karan Thakkar</a>, 
<a href="/search/eess?searchtype=author&query=Chong%2C+D">Dading Chong</a>, 
<a href="/search/eess?searchtype=author&query=Dehak%2C+N">Najim Dehak</a>, 
<a href="/search/eess?searchtype=author&query=Elhilali%2C+M">Mounya Elhilali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Common target sound extraction (TSE) approaches primarily relied on
discriminative approaches in order to separate the target sound while
minimizing interference from the unwanted sources, with varying success in
separating the target from the background. This study introduces DPM-TSE, a
first generative method based on diffusion probabilistic modeling (DPM) for
target sound extraction, to achieve both cleaner target renderings as well as
improved separability from unwanted sounds. The technique also tackles common
background noise issues with DPM by introducing a correction method for noise
schedules and sample steps. This approach is evaluated using both objective and
subjective quality metrics on the FSD Kaggle 2018 dataset. The results show
that DPM-TSE has a significant improvement in perceived quality in terms of
target extraction and purity.
</p>
</div>
</dd>
<dt><a name="item782">[782]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04585" title="Abstract">arXiv:2310.04585</a> (cross-list from econ.TH) [<a href="/pdf/2310.04585" title="Download PDF">pdf</a>, <a href="/format/2310.04585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of Equal Opportunity on Statistical Discrimination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Zhu%2C+J+Y">John Y. Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">I modify the canonical statistical discrimination model of Coate and Loury
(1993) by assuming the firm's belief about an individual's unobserved class is
machine learning-generated and, therefore, contractible. This expands the
toolkit of a regulator beyond belief-free regulations like affirmative action.
Contractible beliefs make it feasible to require the firm to select a decision
policy that equalizes true positive rates across groups -- what the algorithmic
fairness literature calls equal opportunity. While affirmative action does not
necessarily end statistical discrimination, I show that imposing equal
opportunity does.
</p>
</div>
</dd>
<dt><a name="item783">[783]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04587" title="Abstract">arXiv:2310.04587</a> (cross-list from math.CT) [<a href="/pdf/2310.04587" title="Download PDF">pdf</a>, <a href="/ps/2310.04587" title="Download PostScript">ps</a>, <a href="/format/2310.04587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strongly finitary monads and multi-sorted varieties enriched in  cartesian closed concrete categories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Parker%2C+J">Jason Parker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Category Theory (math.CT)</span>; Logic in Computer Science (cs.LO); Logic (math.LO)

</div>
<p class="mathjax">It is a classical result of categorical algebra, due to Lawvere and Linton,
that finitary varieties of algebras (in the sense of Birkhoff) are dually
equivalent to finitary monads on $Set$. Recent work of Ad\'amek, Dost\'al, and
Velebil has established that analogous results also hold in certain enriched
contexts. Specifically, taking $V$ to be one of the cartesian closed categories
$\mathsf{Pos}$, $\mathsf{UltMet}$, $\omega$-$\mathsf{CPO}$, or $\mathsf{DCPO}$
of respectively posets, (extended) ultrametric spaces, $\omega$-cpos, or dcpos,
Ad\'amek, Dost\'al, and Velebil have shown that a suitable category of
$V$-enriched varieties of algebras is dually equivalent to the category of
strongly finitary $V$-monads on $V$.
<br />In this paper, we extend and generalize these results in two ways: by
allowing $V$ to be an arbitrary complete and cocomplete cartesian closed
category that is concrete over $Set$, and by also considering the multi-sorted
case. Given a set $S$ of sorts, we define a suitable notion of (finitary)
$V$-enriched $S$-sorted variety, and we say that a $V$-monad on the product
$V$-category $V^S$ is strongly finitary if its underlying $V$-endofunctor is
the left Kan extension of its restriction to a suitable full sub-$V$-category
of $V^S$. Our main result is that the category of $V$-enriched $S$-sorted
varieties is dually equivalent to the category of strongly finitary $V$-monads
on $V^S$. By taking $S$ to be a singleton and $V$ to be $\mathsf{Pos}$,
$\mathsf{UltMet}$, $\omega$-$\mathsf{CPO}$, or $\mathsf{DCPO}$, we thus recover
the aforementioned results of Ad\'amek, Dost\'al, and Velebil. We provide
several classes of examples of $V$-enriched $S$-sorted varieties, many of which
admit very concrete, syntactic formulations.
</p>
</div>
</dd>
<dt><a name="item784">[784]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04606" title="Abstract">arXiv:2310.04606</a> (cross-list from stat.ML) [<a href="/pdf/2310.04606" title="Download PDF">pdf</a>, <a href="/ps/2310.04606" title="Download PostScript">ps</a>, <a href="/format/2310.04606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Transfer Learning with Unreliable Source Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Fan%2C+J">Jianqing Fan</a>, 
<a href="/search/stat?searchtype=author&query=Gao%2C+C">Cheng Gao</a>, 
<a href="/search/stat?searchtype=author&query=Klusowski%2C+J+M">Jason M. Klusowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 86 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">This paper addresses challenges in robust transfer learning stemming from
ambiguity in Bayes classifiers and weak transferable signals between the target
and source distribution. We introduce a novel quantity called the ''ambiguity
level'' that measures the discrepancy between the target and source regression
functions, propose a simple transfer learning procedure, and establish a
general theorem that shows how this new quantity is related to the
transferability of learning in terms of risk improvements. Our proposed
''Transfer Around Boundary'' (TAB) model, with a threshold balancing the
performance of target and source data, is shown to be both efficient and
robust, improving classification while avoiding negative transfer. Moreover, we
demonstrate the effectiveness of the TAB model on non-parametric classification
and logistic regression tasks, achieving upper bounds which are optimal up to
logarithmic factors. Simulation studies lend further support to the
effectiveness of TAB. We also provide simple approaches to bound the excess
misclassification error without the need for specialized knowledge in transfer
learning.
</p>
</div>
</dd>
<dt><a name="item785">[785]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04609" title="Abstract">arXiv:2310.04609</a> (cross-list from math.PR) [<a href="/pdf/2310.04609" title="Download PDF">pdf</a>, <a href="/ps/2310.04609" title="Download PostScript">ps</a>, <a href="/format/2310.04609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kawasaki dynamics beyond the uniqueness threshold
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bauerschmidt%2C+R">Roland Bauerschmidt</a>, 
<a href="/search/math?searchtype=author&query=Bodineau%2C+T">Thierry Bodineau</a>, 
<a href="/search/math?searchtype=author&query=Dagallier%2C+B">Benoit Dagallier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Data Structures and Algorithms (cs.DS); Mathematical Physics (math-ph)

</div>
<p class="mathjax">Glauber dynamics of the Ising model on a random regular graph is known to mix
fast below the tree uniqueness threshold and exponentially slowly above it. We
show that Kawasaki dynamics of the canonical ferromagnetic Ising model on a
random $d$-regular graph mixes fast beyond the tree uniqueness threshold when
$d$ is large enough (and conjecture that it mixes fast up to the tree
reconstruction threshold for all $d\geq 3$). This result follows from a more
general spectral condition for (modified) log-Sobolev inequalities for
conservative dynamics of Ising models. The proof of this condition in fact
extends to perturbations of distributions with log-concave generating
polynomial.
</p>
</div>
</dd>
<dt><a name="item786">[786]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04622" title="Abstract">arXiv:2310.04622</a> (cross-list from cond-mat.dis-nn) [<a href="/pdf/2310.04622" title="Download PDF">pdf</a>, <a href="/format/2310.04622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FluxGAN: A Physics-Aware Generative Adversarial Network Model for  Generating Microstructures That Maintain Target Heat Flux
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Pimachev%2C+A+K">Artem K. Pimachev</a>, 
<a href="/search/cond-mat?searchtype=author&query=Settipalli%2C+M">Manoj Settipalli</a>, 
<a href="/search/cond-mat?searchtype=author&query=Neogi%2C+S">Sanghamitra Neogi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Disordered Systems and Neural Networks (cond-mat.dis-nn)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a physics-aware generative adversarial network model, FluxGAN,
capable of simultaneously generating high-quality images of large
microstructures and description of their thermal properties. During the
training phase, the model learns about the relationship between the local
structural features and the physical processes, such as the heat flux in the
microstructures, due to external temperature gradients. Once trained, the model
generates new structural and associated heat flux environments, bypassing the
computationally expensive modeling. Our model provides a cost effective and
efficient approach over conventional modeling techniques, such as the finite
element method (FEM), for describing the thermal properties of microstructures.
The conventional approach requires computational modeling that scales with the
size of the microstructure model, therefore limiting the simulation to a given
size, resolution, and complexity of the model. In contrast, the FluxGAN model
uses synthesis-by-part approach and generates arbitrary large size images at
low computational cost. We demonstrate that the model can be utilized to
generate designs of thermal sprayed coatings that satisfies target thermal
properties. Furthermore, the model is capable of generating coating
microstructures and physical processes in three-dimensional (3D) domain after
being trained on two-dimensional (2D) examples. Our approach has the potential
to transform the design and optimization of thermal sprayed coatings for
various applications, including high-temperature and long-duration operation of
gas turbines for aircraft or ground-based power generators.
</p>
</div>
</dd>
<dt><a name="item787">[787]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04630" title="Abstract">arXiv:2310.04630</a> (cross-list from eess.IV) [<a href="/pdf/2310.04630" title="Download PDF">pdf</a>, <a href="/format/2310.04630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metadata-Conditioned Generative Models to Synthesize  Anatomically-Plausible 3D Brain MRIs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Peng%2C+W">Wei Peng</a>, 
<a href="/search/eess?searchtype=author&query=Bosschieter%2C+T">Tomas Bosschieter</a>, 
<a href="/search/eess?searchtype=author&query=Ouyang%2C+J">Jiahong Ouyang</a>, 
<a href="/search/eess?searchtype=author&query=Paul%2C+R">Robert Paul</a>, 
<a href="/search/eess?searchtype=author&query=Adeli%2C+E">Ehsan Adeli</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+Q">Qingyu Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Pohl%2C+K+M">Kilian M. Pohl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Generative AI models hold great potential in creating synthetic brain MRIs
that advance neuroimaging studies by, for example, enriching data diversity.
However, the mainstay of AI research only focuses on optimizing the visual
quality (such as signal-to-noise ratio) of the synthetic MRIs while lacking
insights into their relevance to neuroscience. To gain these insights with
respect to T1-weighted MRIs, we first propose a new generative model,
BrainSynth, to synthesize metadata-conditioned (e.g., age- and sex-specific)
MRIs that achieve state-of-the-art visual quality. We then extend our
evaluation with a novel procedure to quantify anatomical plausibility, i.e.,
how well the synthetic MRIs capture macrostructural properties of brain
regions, and how accurately they encode the effects of age and sex. Results
indicate that more than half of the brain regions in our synthetic MRIs are
anatomically accurate, i.e., with a small effect size between real and
synthetic MRIs. Moreover, the anatomical plausibility varies across cortical
regions according to their geometric complexity. As is, our synthetic MRIs can
significantly improve the training of a Convolutional Neural Network to
identify accelerated aging effects in an independent study. These results
highlight the opportunities of using generative AI to aid neuroimaging research
and point to areas for further improvement.
</p>
</div>
</dd>
<dt><a name="item788">[788]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04645" title="Abstract">arXiv:2310.04645</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.04645" title="Download PDF">pdf</a>, <a href="/format/2310.04645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do self-supervised speech and language models extract similar  representations as human brain?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+P">Peili Chen</a>, 
<a href="/search/q-bio?searchtype=author&query=He%2C+L">Linyang He</a>, 
<a href="/search/q-bio?searchtype=author&query=Fu%2C+L">Li Fu</a>, 
<a href="/search/q-bio?searchtype=author&query=Fan%2C+L">Lu Fan</a>, 
<a href="/search/q-bio?searchtype=author&query=Chang%2C+E+F">Edward F. Chang</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+Y">Yuanning Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to 2024 IEEE International Conference on Acoustics, Speech and Signal Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Speech and language models trained through self-supervised learning (SSL)
demonstrate strong alignment with brain activity during speech and language
perception. However, given their distinct training modalities, it remains
unclear whether they correlate with the same neural aspects. We directly
address this question by evaluating the brain prediction performance of two
representative SSL models, Wav2Vec2.0 and GPT-2, designed for speech and
language tasks. Our findings reveal that both models accurately predict speech
responses in the auditory cortex, with a significant correlation between their
brain predictions. Notably, shared speech contextual information between
Wav2Vec2.0 and GPT-2 accounts for the majority of explained variance in brain
activity, surpassing static semantic and lower-level acoustic-phonetic
information. These results underscore the convergence of speech contextual
representations in SSL models and their alignment with the neural network
underlying speech perception, offering valuable insights into both SSL models
and the neural basis of speech and language processing.
</p>
</div>
</dd>
<dt><a name="item789">[789]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04657" title="Abstract">arXiv:2310.04657</a> (cross-list from eess.AS) [<a href="/pdf/2310.04657" title="Download PDF">pdf</a>, <a href="/format/2310.04657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spike-Triggered Contextual Biasing for End-to-End Mandarin Speech  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+K">Kaixun Huang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+A">Ao Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+B">Binbin Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+T">Tianyi Xu</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+X">Xingchen Song</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+L">Lei Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ASRU2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">The attention-based deep contextual biasing method has been demonstrated to
effectively improve the recognition performance of end-to-end automatic speech
recognition (ASR) systems on given contextual phrases. However, unlike shallow
fusion methods that directly bias the posterior of the ASR model, deep biasing
methods implicitly integrate contextual information, making it challenging to
control the degree of bias. In this study, we introduce a spike-triggered deep
biasing method that simultaneously supports both explicit and implicit bias.
Moreover, both bias approaches exhibit significant improvements and can be
cascaded with shallow fusion methods for better results. Furthermore, we
propose a context sampling enhancement strategy and improve the contextual
phrase filtering algorithm. Experiments on the public WenetSpeech Mandarin
biased-word dataset show a 32.0% relative CER reduction compared to the
baseline model, with an impressively 68.6% relative CER reduction on contextual
phrases.
</p>
</div>
</dd>
<dt><a name="item790">[790]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04677" title="Abstract">arXiv:2310.04677</a> (cross-list from eess.IV) [<a href="/pdf/2310.04677" title="Download PDF">pdf</a>, <a href="/format/2310.04677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AG-CRC: Anatomy-Guided Colorectal Cancer Segmentation in CT with  Imperfect Anatomical Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+R">Rongzhao Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Bai%2C+Z">Zhian Bai</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+R">Ruoying Yu</a>, 
<a href="/search/eess?searchtype=author&query=Pang%2C+W">Wenrao Pang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+L">Lingyun Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+L">Lifeng Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xiaofan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+H">Huan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+W">Weiguo Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Medical Image Analysis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">When delineating lesions from medical images, a human expert can always keep
in mind the anatomical structure behind the voxels. However, although
high-quality (though not perfect) anatomical information can be retrieved from
computed tomography (CT) scans with modern deep learning algorithms, it is
still an open problem how these automatically generated organ masks can assist
in addressing challenging lesion segmentation tasks, such as the segmentation
of colorectal cancer (CRC). In this paper, we develop a novel Anatomy-Guided
segmentation framework to exploit the auto-generated organ masks to aid CRC
segmentation from CT, namely AG-CRC. First, we obtain multi-organ segmentation
(MOS) masks with existing MOS models (e.g., TotalSegmentor) and further derive
a more robust organ of interest (OOI) mask that may cover most of the
colon-rectum and CRC voxels. Then, we propose an anatomy-guided training patch
sampling strategy by optimizing a heuristic gain function that considers both
the proximity of important regions (e.g., the tumor or organs of interest) and
sample diversity. Third, we design a novel self-supervised learning scheme
inspired by the topology of tubular organs like the colon to boost the model
performance further. Finally, we employ a masked loss scheme to guide the model
to focus solely on the essential learning region. We extensively evaluate the
proposed method on two CRC segmentation datasets, where substantial performance
improvement (5% to 9% in Dice) is achieved over current state-of-the-art
medical image segmentation models, and the ablation studies further evidence
the efficacy of every proposed component.
</p>
</div>
</dd>
<dt><a name="item791">[791]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04679" title="Abstract">arXiv:2310.04679</a> (cross-list from eess.IV) [<a href="/pdf/2310.04679" title="Download PDF">pdf</a>, <a href="/format/2310.04679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High Visual-Fidelity Learned Video Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+M">Meng Li</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+Y">Yibo Shi</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jing Wang</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+Y">Yunqi Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACMMM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">With the growing demand for video applications, many advanced learned video
compression methods have been developed, outperforming traditional methods in
terms of objective quality metrics such as PSNR. Existing methods primarily
focus on objective quality but tend to overlook perceptual quality. Directly
incorporating perceptual loss into a learned video compression framework is
nontrivial and raises several perceptual quality issues that need to be
addressed. In this paper, we investigated these issues in learned video
compression and propose a novel High Visual-Fidelity Learned Video Compression
framework (HVFVC). Specifically, we design a novel confidence-based feature
reconstruction method to address the issue of poor reconstruction in
newly-emerged regions, which significantly improves the visual quality of the
reconstruction. Furthermore, we present a periodic compensation loss to
mitigate the checkerboard artifacts related to deconvolution operation and
optimization. Extensive experiments have shown that the proposed HVFVC achieves
excellent perceptual quality, outperforming the latest VVC standard with only
50% required bitrate.
</p>
</div>
</dd>
<dt><a name="item792">[792]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04705" title="Abstract">arXiv:2310.04705</a> (cross-list from eess.IV) [<a href="/pdf/2310.04705" title="Download PDF">pdf</a>, <a href="/format/2310.04705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-scale MRI reconstruction via dilated ensemble networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ma%2C+W">Wendi Ma</a>, 
<a href="/search/eess?searchtype=author&query=Lorenzana%2C+M+B">Marlon Bran Lorenzana</a>, 
<a href="/search/eess?searchtype=author&query=Dai%2C+W">Wei Dai</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+H">Hongfu Sun</a>, 
<a href="/search/eess?searchtype=author&query=Chandra%2C+S+S">Shekhar S. Chandra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">As aliasing artefacts are highly structural and non-local, many MRI
reconstruction networks use pooling to enlarge filter coverage and incorporate
global context. However, this inadvertently impedes fine detail recovery as
downsampling creates a resolution bottleneck. Moreover, real and imaginary
features are commonly split into separate channels, discarding phase
information particularly important to high frequency textures. In this work, we
introduce an efficient multi-scale reconstruction network using dilated
convolutions to preserve resolution and experiment with a complex-valued
version using complex convolutions. Inspired by parallel dilated filters,
multiple receptive fields are processed simultaneously with branches that see
both large structural artefacts and fine local features. We also adopt dense
residual connections for feature aggregation to efficiently increase scale and
the deep cascade global architecture to reduce overfitting. The real-valued
version of this model outperformed common reconstruction architectures as well
as a state-of-the-art multi-scale network whilst being three times more
efficient. The complex-valued network yielded better qualitative results when
more phase information was present.
</p>
</div>
</dd>
<dt><a name="item793">[793]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04715" title="Abstract">arXiv:2310.04715</a> (cross-list from eess.AS) [<a href="/pdf/2310.04715" title="Download PDF">pdf</a>, <a href="/format/2310.04715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Exploration of Task-decoupling on Two-stage Neural Post Filter for  Real-time Personalized Acoustic Echo Cancellation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Z">Zihan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+J">Jiayao Sun</a>, 
<a href="/search/eess?searchtype=author&query=Xia%2C+X">Xianjun Xia</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Ziqian Wang</a>, 
<a href="/search/eess?searchtype=author&query=Yan%2C+X">Xiaopeng Yan</a>, 
<a href="/search/eess?searchtype=author&query=Xiao%2C+Y">Yijian Xiao</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+L">Lei Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Deep learning based techniques have been popularly adopted in acoustic echo
cancellation (AEC). Utilization of speaker representation has extended the
frontier of AEC, thus attracting many researchers' interest in personalized
acoustic echo cancellation (PAEC). Meanwhile, task-decoupling strategies are
widely adopted in speech enhancement. To further explore the task-decoupling
approach, we propose to use a two-stage task-decoupling post-filter (TDPF) in
PAEC. Furthermore, a multi-scale local-global speaker representation is applied
to improve speaker extraction in PAEC. Experimental results indicate that the
task-decoupling model can yield better performance than a single joint network.
The optimal approach is to decouple the echo cancellation from noise and
interference speech suppression. Based on the task-decoupling sequence, optimal
training strategies for the two-stage model are explored afterwards.
</p>
</div>
</dd>
<dt><a name="item794">[794]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04760" title="Abstract">arXiv:2310.04760</a> (cross-list from eess.AS) [<a href="/pdf/2310.04760" title="Download PDF">pdf</a>, <a href="/format/2310.04760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-objective Progressive Clustering for Semi-supervised Domain  Adaptation in Speaker Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Ze Li</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+Y">Yuke Lin</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+N">Ning Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Qin%2C+X">Xiaoyi Qin</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+G">Guoqing Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+H">Haiying Wu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+M">Ming Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Utilizing the pseudo-labeling algorithm with large-scale unlabeled data
becomes crucial for semi-supervised domain adaptation in speaker verification
tasks. In this paper, we propose a novel pseudo-labeling method named
Multi-objective Progressive Clustering (MoPC), specifically designed for
semi-supervised domain adaptation. Firstly, we utilize limited labeled data
from the target domain to derive domain-specific descriptors based on multiple
distinct objectives, namely within-graph denoising, intra-class denoising and
inter-class denoising. Then, the Infomap algorithm is adopted for embedding
clustering, and the descriptors are leveraged to further refine the target
domain's pseudo-labels. Moreover, to further improve the quality of pseudo
labels, we introduce the subcenter-purification and progressive-merging
strategy for label denoising. Our proposed MoPC method achieves 4.95% EER and
ranked the 1$^{st}$ place on the evaluation set of VoxSRC 2023 track 3. We also
conduct additional experiments on the FFSVC dataset and yield promising
results.
</p>
</div>
</dd>
<dt><a name="item795">[795]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04762" title="Abstract">arXiv:2310.04762</a> (cross-list from eess.IV) [<a href="/pdf/2310.04762" title="Download PDF">pdf</a>, <a href="/format/2310.04762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Low-Rank Matrix Completion via a New Sparsity-Inducing  Regularizer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zhi-Yong Wang</a>, 
<a href="/search/eess?searchtype=author&query=So%2C+H+C">Hing Cheung So</a>, 
<a href="/search/eess?searchtype=author&query=Zoubir%2C+A+M">Abdelhak M. Zoubir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper presents a novel loss function referred to as hybrid
ordinary-Welsch (HOW) and a new sparsity-inducing regularizer associated with
HOW. We theoretically show that the regularizer is quasiconvex and that the
corresponding Moreau envelope is convex. Moreover, the closed-form solution to
its Moreau envelope, namely, the proximity operator, is derived. Compared with
nonconvex regularizers like the lp-norm with 0&lt;p&lt;1 that requires iterations to
find the corresponding proximity operator, the developed regularizer has a
closed-form proximity operator. We apply our regularizer to the robust matrix
completion problem, and develop an efficient algorithm based on the alternating
direction method of multipliers. The convergence of the suggested method is
analyzed and we prove that any generated accumulation point is a stationary
point. Finally, experimental results based on synthetic and real-world datasets
demonstrate that our algorithm is superior to the state-of-the-art methods in
terms of restoration performance.
</p>
</div>
</dd>
<dt><a name="item796">[796]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04779" title="Abstract">arXiv:2310.04779</a> (cross-list from eess.IV) [<a href="/pdf/2310.04779" title="Download PDF">pdf</a>, <a href="/format/2310.04779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransCC: Transformer Network for Coronary Artery CCTA Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xu%2C+C">Chenchu Xu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+M">Meng Li</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+X">Xue Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The accurate segmentation of Coronary Computed Tomography Angiography (CCTA)
images holds substantial clinical value for the early detection and treatment
of Coronary Heart Disease (CHD). The Transformer, utilizing a self-attention
mechanism, has demonstrated commendable performance in the realm of medical
image processing. However, challenges persist in coronary segmentation tasks
due to (1) the damage to target local structures caused by fixed-size image
patch embedding, and (2) the critical role of both global and local features in
medical image segmentation tasks.To address these challenges, we propose a deep
learning framework, TransCC, that effectively amalgamates the Transformer and
convolutional neural networks for CCTA segmentation. Firstly, we introduce a
Feature Interaction Extraction (FIE) module designed to capture the
characteristics of image patches, thereby circumventing the loss of semantic
information inherent in the original method. Secondly, we devise a Multilayer
Enhanced Perceptron (MEP) to augment attention to local information within
spatial dimensions, serving as a complement to the self-attention mechanism.
Experimental results indicate that TransCC outperforms existing methods in
segmentation performance, boasting an average Dice coefficient of 0.730 and an
average Intersection over Union (IoU) of 0.582. These results underscore the
effectiveness of TransCC in CCTA image segmentation.
</p>
</div>
</dd>
<dt><a name="item797">[797]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04786" title="Abstract">arXiv:2310.04786</a> (cross-list from q-fin.RM) [<a href="/pdf/2310.04786" title="Download PDF">pdf</a>, <a href="/format/2310.04786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cyber Insurance Risk: Reporting Delays, Third-Party Cyber Events, and  Changes in Reporting Propensity -- An Analysis Using Data Breaches Published  by U.S. State Attorneys General
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Avanzi%2C+B">Benjamin Avanzi</a> (1), 
<a href="/search/q-fin?searchtype=author&query=Tan%2C+X">Xingyun Tan</a> (1), 
<a href="/search/q-fin?searchtype=author&query=Taylor%2C+G">Greg Taylor</a> (2), 
<a href="/search/q-fin?searchtype=author&query=Wong%2C+B">Bernard Wong</a> (2) ((1) University of Melbourne, (2) UNSW Sydney)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Risk Management (q-fin.RM)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">With the rise of cyber threats, cyber insurance is becoming an important
consideration for businesses. However, research on cyber insurance risk has so
far been hindered by the general lack of data, as well as limitations
underlying what limited data are available publicly. Specifically and of
particular importance to cyber insurance modelling, limitations arising from
lack of information regarding (i) delays in reporting, (ii) all businesses
affected by third-party events, and (iii) changes in reporting propensity. In
this paper, we fill this important gap by utilising an underrecognised set of
public data provided by U.S. state Attorneys General, and provide new insights
on the true scale of cyber insurance risk. These data are collected based on
mandatory reporting requirements of data breaches, and contain substantial and
detailed information. We further discuss extensively the associated
implications of our findings for cyber insurance pricing, reserving,
underwriting, and experience monitoring.
</p>
</div>
</dd>
<dt><a name="item798">[798]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04791" title="Abstract">arXiv:2310.04791</a> (cross-list from eess.AS) [<a href="/pdf/2310.04791" title="Download PDF">pdf</a>, <a href="/format/2310.04791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Diffusion Model for Target Speaker Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+T">Theodor Nguyen</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+G">Guangzhi Sun</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+X">Xianrui Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Woodland%2C+P+C">Philip C Woodland</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures, submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">We propose DiffSpEx, a generative target speaker extraction method based on
score-based generative modelling through stochastic differential equations.
DiffSpEx deploys a continuous-time stochastic diffusion process in the complex
short-time Fourier transform domain, starting from the target speaker source
and converging to a Gaussian distribution centred on the mixture of sources.
For the reverse-time process, a parametrised score function is conditioned on a
target speaker embedding to extract the target speaker from the mixture of
sources. We utilise ECAPA-TDNN target speaker embeddings and condition the
score function alternately on the SDE time embedding and the target speaker
embedding. The potential of DiffSpEx is demonstrated with the WSJ0-2mix
dataset, achieving an SI-SDR of 12.9 dB and a NISQA score of 3.56. Moreover, we
show that fine-tuning a pre-trained DiffSpEx model to a specific speaker
further improves performance, enabling personalisation in target speaker
extraction.
</p>
</div>
</dd>
<dt><a name="item799">[799]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04818" title="Abstract">arXiv:2310.04818</a> (cross-list from physics.soc-ph) [<a href="/pdf/2310.04818" title="Download PDF">pdf</a>, <a href="/format/2310.04818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How do Retail Stores Affect Pedestrian Walking Speed: An Empirical  Observation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Li%2C+D">Danrui Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures, accepted by Traffic and Granular Flow 2022. For related dataset, see <a href="https://tianchi.aliyun.com/dataset/162948">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Pedestrian studies in retail areas are critical for comfort and convenience
in transportation facility designs. But existing literature lacks detailed
empirical observations that focus on pedestrian speed variations and their
mechanisms in front of stores. This paper bridges this gap by analyzing 1193
pedestrian trajectories in front of a convenience store located in a metro
station. The results show that the store imposes a non-uniform slowing effect
on the pedestrian flow. The spatial distribution and the lower walking speed of
consumers and gazing pedestrians jointly contribute to such an effect while
avoiding behaviors between pedestrians play little role. The findings
complement the existing empirical observations and lay a foundation for
realistic pedestrian modeling in retail areas.
</p>
</div>
</dd>
<dt><a name="item800">[800]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04854" title="Abstract">arXiv:2310.04854</a> (cross-list from stat.ML) [<a href="/pdf/2310.04854" title="Download PDF">pdf</a>, <a href="/format/2310.04854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Repelling Random Walks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Reid%2C+I">Isaac Reid</a>, 
<a href="/search/stat?searchtype=author&query=Berger%2C+E">Eli Berger</a>, 
<a href="/search/stat?searchtype=author&query=Choromanski%2C+K">Krzysztof Choromanski</a>, 
<a href="/search/stat?searchtype=author&query=Weller%2C+A">Adrian Weller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a novel quasi-Monte Carlo mechanism to improve graph-based
sampling, coined repelling random walks. By inducing correlations between the
trajectories of an interacting ensemble such that their marginal transition
probabilities are unmodified, we are able to explore the graph more
efficiently, improving the concentration of statistical estimators whilst
leaving them unbiased. The mechanism has a trivial drop-in implementation. We
showcase the effectiveness of repelling random walks in a range of settings
including estimation of graph kernels, the PageRank vector and graphlet
concentrations. We provide detailed experimental evaluation and robust
theoretical guarantees. To our knowledge, repelling random walks constitute the
first rigorously studied quasi-Monte Carlo scheme correlating the directions of
walkers on a graph, inviting new research in this exciting nascent domain.
</p>
</div>
</dd>
<dt><a name="item801">[801]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04859" title="Abstract">arXiv:2310.04859</a> (cross-list from stat.ML) [<a href="/pdf/2310.04859" title="Download PDF">pdf</a>, <a href="/format/2310.04859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Graph Random Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Reid%2C+I">Isaac Reid</a>, 
<a href="/search/stat?searchtype=author&query=Choromanski%2C+K">Krzysztof Choromanski</a>, 
<a href="/search/stat?searchtype=author&query=Berger%2C+E">Eli Berger</a>, 
<a href="/search/stat?searchtype=author&query=Weller%2C+A">Adrian Weller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a novel quasi-Monte Carlo mechanism to improve graph-based
sampling, coined repelling random walks. By inducing correlations between the
trajectories of an interacting ensemble such that their marginal transition
probabilities are unmodified, we are able to explore the graph more
efficiently, improving the concentration of statistical estimators whilst
leaving them unbiased. The mechanism has a trivial drop-in implementation. We
showcase the effectiveness of repelling random walks in a range of settings
including estimation of graph kernels, the PageRank vector and graphlet
concentrations. We provide detailed experimental evaluation and robust
theoretical guarantees. To our knowledge, repelling random walks constitute the
first rigorously studied quasi-Monte Carlo scheme correlating the directions of
walkers on a graph, inviting new research in this exciting nascent domain.
</p>
</div>
</dd>
<dt><a name="item802">[802]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04871" title="Abstract">arXiv:2310.04871</a> (cross-list from eess.IV) [<a href="/pdf/2310.04871" title="Download PDF">pdf</a>, <a href="/format/2310.04871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning for Automated Mitral Regurgitation Detection from  Cardiac Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xiao%2C+K">Ke Xiao</a>, 
<a href="/search/eess?searchtype=author&query=Learned-Miller%2C+E">Erik Learned-Miller</a>, 
<a href="/search/eess?searchtype=author&query=Kalogerakis%2C+E">Evangelos Kalogerakis</a>, 
<a href="/search/eess?searchtype=author&query=Priest%2C+J">James Priest</a>, 
<a href="/search/eess?searchtype=author&query=Fiterau%2C+M">Madalina Fiterau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages including references and the appendix. 9 Figures, 2 tables. Accepted at MICCAI (Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging) 2023, Link to Springer at <a href="https://link.springer.com/chapter/10.1007/978-3-031-43990-2_23">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In: Medical Image Computing and Computer Assisted Intervention -
  MICCAI 2023. pp. 236-246 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Mitral regurgitation (MR) is a heart valve disease with potentially fatal
consequences that can only be forestalled through timely diagnosis and
treatment. Traditional diagnosis methods are expensive, labor-intensive and
require clinical expertise, posing a barrier to screening for MR. To overcome
this impediment, we propose a new semi-supervised model for MR classification
called CUSSP. CUSSP operates on cardiac imaging slices of the 4-chamber view of
the heart. It uses standard computer vision techniques and contrastive models
to learn from large amounts of unlabeled data, in conjunction with specialized
classifiers to establish the first ever automated MR classification system.
Evaluated on a test set of 179 labeled -- 154 non-MR and 25 MR -- sequences,
CUSSP attains an F1 score of 0.69 and a ROC-AUC score of 0.88, setting the
first benchmark result for this new task.
</p>
</div>
</dd>
<dt><a name="item803">[803]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04915" title="Abstract">arXiv:2310.04915</a> (cross-list from physics.comp-ph) [<a href="/pdf/2310.04915" title="Download PDF">pdf</a>, <a href="/format/2310.04915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Accelerating Diffusion-based Molecular Conformation Generation in  SE(3)-invariant Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Zhou%2C+Z">Zihan Zhou</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+R">Ruiying Liu</a>, 
<a href="/search/physics?searchtype=author&query=Yu%2C+T">Tianshu Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Diffusion-based generative models in SE(3)-invariant space have demonstrated
promising performance in molecular conformation generation, but typically
require solving stochastic differential equations (SDEs) with thousands of
update steps. Till now, it remains unclear how to effectively accelerate this
procedure explicitly in SE(3)-invariant space, which greatly hinders its wide
application in the real world. In this paper, we systematically study the
diffusion mechanism in SE(3)-invariant space via the lens of approximate errors
induced by existing methods. Thereby, we develop more precise approximate in
SE(3) in the context of projected differential equations. Theoretical analysis
is further provided as well as empirical proof relating hyper-parameters with
such errors. Altogether, we propose a novel acceleration scheme for generating
molecular conformations in SE(3)-invariant space. Experimentally, our scheme
can generate high-quality conformations with 50x--100x speedup compared to
existing methods.
</p>
</div>
</dd>
<dt><a name="item804">[804]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04916" title="Abstract">arXiv:2310.04916</a> (cross-list from math.OC) [<a href="/pdf/2310.04916" title="Download PDF">pdf</a>, <a href="/format/2310.04916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Certified Robustness via Min-Max Representations of ReLU Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Anderson%2C+B+G">Brendon G. Anderson</a>, 
<a href="/search/math?searchtype=author&query=Pfrommer%2C+S">Samuel Pfrommer</a>, 
<a href="/search/math?searchtype=author&query=Sojoudi%2C+S">Somayeh Sojoudi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Conference on Decision and Control, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The reliable deployment of neural networks in control systems requires
rigorous robustness guarantees. In this paper, we obtain tight robustness
certificates over convex attack sets for min-max representations of ReLU neural
networks by developing a convex reformulation of the nonconvex certification
problem. This is done by "lifting" the problem to an infinite-dimensional
optimization over probability measures, leveraging recent results in
distributionally robust optimization to solve for an optimal discrete
distribution, and proving that solutions of the original nonconvex problem are
generated by the discrete distribution under mild boundedness, nonredundancy,
and Slater conditions. As a consequence, optimal (worst-case) attacks against
the model may be solved for exactly. This contrasts prior state-of-the-art that
either requires expensive branch-and-bound schemes or loose relaxation
techniques. Experiments on robust control and MNIST image classification
examples highlight the benefits of our approach.
</p>
</div>
</dd>
<dt><a name="item805">[805]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04919" title="Abstract">arXiv:2310.04919</a> (cross-list from stat.ME) [<a href="/pdf/2310.04919" title="Download PDF">pdf</a>, <a href="/format/2310.04919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Conditional Prediction Function: A Novel Technique to Control False  Discovery Rate for Complex Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Shi%2C+Y">Yushu Shi</a>, 
<a href="/search/stat?searchtype=author&query=Martens%2C+M">Michael Martens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">In modern scientific research, the objective is often to identify which
variables are associated with an outcome among a large class of potential
predictors. This goal can be achieved by selecting variables in a manner that
controls the the false discovery rate (FDR), the proportion of irrelevant
predictors among the selections. Knockoff filtering is a cutting-edge approach
to variable selection that provides FDR control. Existing knockoff statistics
frequently employ linear models to assess relationships between features and
the response, but the linearity assumption is often violated in real world
applications. This may result in poor power to detect truly prognostic
variables. We introduce a knockoff statistic based on the conditional
prediction function (CPF), which can pair with state-of-art machine learning
predictive models, such as deep neural networks. The CPF statistics can capture
the nonlinear relationships between predictors and outcomes while also
accounting for correlation between features. We illustrate the capability of
the CPF statistics to provide superior power over common knockoff statistics
with continuous, categorical, and survival outcomes using repeated simulations.
Knockoff filtering with the CPF statistics is demonstrated using (1) a
residential building dataset to select predictors for the actual sales prices
and (2) the TCGA dataset to select genes that are correlated with disease
staging in lung cancer patients.
</p>
</div>
</dd>
<dt><a name="item806">[806]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04926" title="Abstract">arXiv:2310.04926</a> (cross-list from math.GR) [<a href="/pdf/2310.04926" title="Download PDF">pdf</a>, <a href="/ps/2310.04926" title="Download PostScript">ps</a>, <a href="/format/2310.04926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Further results on generalized cellular automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Castillo-Ramirez%2C+A">Alonso Castillo-Ramirez</a>, 
<a href="/search/math?searchtype=author&query=de+los+Santos+Ba%C3%B1os%2C+L">Luguis de los Santos Ba&#xf1;os</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Group Theory (math.GR)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">Given a finite set $A$ and a group homomorphism $\phi : H \to G$, a
$\phi$-cellular automaton is a function $\mathcal{T} : A^G \to A^H$ that is
continuous with respect to the prodiscrete topologies and $\phi$-equivariant in
the sense that $h \cdot \mathcal{T}(x) = \mathcal{T}( \phi(h) \cdot x)$, for
all $x \in A^G, h \in H$, where $\cdot$ denotes the shift actions of $G$ and
$H$ on $A^G$ and $A^H$, respectively. When $G=H$ and $\phi = \text{id}$, the
definition of $\text{id}$-cellular automata coincides with the classical
definition of cellular automata. The purpose of this paper is to expand the
theory of $\phi$-cellular automata by focusing on the differences and
similarities with their classical counterparts. After discussing some basic
results, we introduce the following definition: a $\phi$-cellular automaton
$\mathcal{T} : A^G \to A^H$ has the unique homomorphism property (UHP) if
$\mathcal{T}$ is not $\psi$-equivariant for any group homomorphism $\psi : H
\to G$, $\psi \neq \phi$. We show that if the difference set $\Delta(\phi,
\psi)$ is infinite, then $\mathcal{T}$ is not $\psi$-equivariant; it follows
that when $G$ is torsion-free abelian, every non-constant $\mathcal{T}$ has the
UHP. Furthermore, inspired by the theory of classical cellular automata, we
study $\phi$-cellular automata over quotient groups, as well as their
restriction and induction to subgroups and supergroups, respectively.
</p>
</div>
</dd>
<dt><a name="item807">[807]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04937" title="Abstract">arXiv:2310.04937</a> (cross-list from quant-ph) [<a href="/pdf/2310.04937" title="Download PDF">pdf</a>, <a href="/format/2310.04937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Computing and Visualization: A Disruptive Technological Change  Ahead
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Bethel%2C+E+W">E. Wes Bethel</a>, 
<a href="/search/quant-ph?searchtype=author&query=Amankwah%2C+M+G">Mercy G. Amankwah</a>, 
<a href="/search/quant-ph?searchtype=author&query=Balewski%2C+J">Jan Balewski</a>, 
<a href="/search/quant-ph?searchtype=author&query=Van+Beeumen%2C+R">Roel Van Beeumen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Camps%2C+D">Daan Camps</a>, 
<a href="/search/quant-ph?searchtype=author&query=Huang%2C+D">Daniel Huang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Perciano%2C+T">Talita Perciano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE Computer Graphics and Applications, 46(3), Nov/Dec, 2023, <a href="https://doi.org/10.1109/MCG.2023.3316932">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">The focus of this Visualization Viewpoints article is to provide some
background on Quantum Computing (QC), to explore ideas related to how
visualization helps in understanding QC, and examine how QC might be useful for
visualization with the growth and maturation of both technologies in the
future. In a quickly evolving technology landscape, QC is emerging as a
promising pathway to overcome the growth limits in classical computing. In some
cases, QC platforms offer the potential to vastly outperform the familiar
classical computer by solving problems more quickly or that may be intractable
on any known classical platform. As further performance gains for classical
computing platforms are limited by diminishing Moore's Law scaling, QC
platforms might be viewed as a potential successor to the current field of
exascale-class platforms. While present-day \qx{} hardware platforms are still
limited in scale, the field of quantum computing is robust and rapidly
advancing in terms of hardware capabilities, software environments for
developing quantum algorithms, and educational programs for training the next
generation of scientists and engineers. After a brief introduction to QC
concepts, the focus of this article is to explore the interplay between the
fields of visualization and QC. First, visualization has played a role in QC by
providing the means to show representations of the quantum state of
single-qubits in superposition states and multiple-qubits in entangled states.
Second, there are a number of ways in which the field of visual data
exploration and analysis may potentially benefit from this disruptive new
technology though there are challenges going forward.
</p>
</div>
</dd>
<dt><a name="item808">[808]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04954" title="Abstract">arXiv:2310.04954</a> (cross-list from math.OC) [<a href="/pdf/2310.04954" title="Download PDF">pdf</a>, <a href="/format/2310.04954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A framework to generate sparsity-inducing regularizers for enhanced  low-rank matrix completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+Z">Zhi-Yong Wang</a>, 
<a href="/search/math?searchtype=author&query=So%2C+H+C">Hing Cheung So</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Applying half-quadratic optimization to loss functions can yield the
corresponding regularizers, while these regularizers are usually not
sparsity-inducing regularizers (SIRs). To solve this problem, we devise a
framework to generate an SIR with closed-form proximity operator. Besides, we
specify our framework using several commonly-used loss functions, and produce
the corresponding SIRs, which are then adopted as nonconvex rank surrogates for
low-rank matrix completion. Furthermore, algorithms based on the alternating
direction method of multipliers are developed. Extensive numerical results show
the effectiveness of our methods in terms of recovery performance and runtime.
</p>
</div>
</dd>
<dt><a name="item809">[809]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04956" title="Abstract">arXiv:2310.04956</a> (cross-list from eess.SP) [<a href="/pdf/2310.04956" title="Download PDF">pdf</a>, <a href="/ps/2310.04956" title="Download PostScript">ps</a>, <a href="/format/2310.04956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Explainable Machine Learning: The Effectiveness of Reservoir  Computing in Wireless Receive Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jere%2C+S">Shashank Jere</a>, 
<a href="/search/eess?searchtype=author&query=Said%2C+K">Karim Said</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+L">Lizhong Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+L">Lingjia Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted to IEEE Military Communications Conference (MILCOM) 2023. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep learning has seen a rapid adoption in a variety of wireless
communications applications, including at the physical layer. While it has
delivered impressive performance in tasks such as channel equalization and
receive processing/symbol detection, it leaves much to be desired when it comes
to explaining this superior performance. In this work, we investigate the
specific task of channel equalization by applying a popular learning-based
technique known as Reservoir Computing (RC), which has shown superior
performance compared to conventional methods and other learning-based
approaches. Specifically, we apply the echo state network (ESN) as a channel
equalizer and provide a first principles-based signal processing understanding
of its operation. With this groundwork, we incorporate the available domain
knowledge in the form of the statistics of the wireless channel directly into
the weights of the ESN model. This paves the way for optimized initialization
of the ESN model weights, which are traditionally untrained and randomly
initialized. Finally, we show the improvement in receive processing/symbol
detection performance with this optimized initialization through simulations.
This is a first step towards explainable machine learning (XML) and assigning
practical model interpretability that can be utilized together with the
available domain knowledge to improve performance and enhance detection
reliability.
</p>
</div>
</dd>
<dt><a name="item810">[810]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04992" title="Abstract">arXiv:2310.04992</a> (cross-list from eess.IV) [<a href="/pdf/2310.04992" title="Download PDF">pdf</a>, <a href="/format/2310.04992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VisionFM: a Multi-Modal Multi-Task Vision Foundation Model for  Generalist Ophthalmic Artificial Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Qiu%2C+J">Jianing Qiu</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/eess?searchtype=author&query=Wei%2C+H">Hao Wei</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+P">Peilun Shi</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+M">Minqing Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+Y">Yunyun Sun</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+L">Lin Li</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+H">Hanruo Liu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+H">Hongyi Liu</a>, 
<a href="/search/eess?searchtype=author&query=Hou%2C+S">Simeng Hou</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+Y">Yuyang Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+X">Xuehui Shi</a>, 
<a href="/search/eess?searchtype=author&query=Xian%2C+J">Junfang Xian</a>, 
<a href="/search/eess?searchtype=author&query=Qu%2C+X">Xiaoxia Qu</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+S">Sirui Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+L">Lijie Pan</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+X">Xiaoniao Chen</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xiaojia Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+S">Shuai Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+K">Kebing Wang</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+C">Chenlong Yang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+M">Mingqiang Chen</a>, 
<a href="/search/eess?searchtype=author&query=Fan%2C+S">Sujie Fan</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+J">Jianhua Hu</a>, 
<a href="/search/eess?searchtype=author&query=Lv%2C+A">Aiguo Lv</a>, 
<a href="/search/eess?searchtype=author&query=Miao%2C+H">Hui Miao</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+L">Li Guo</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+S">Shujun Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Pei%2C+C">Cheng Pei</a>, 
<a href="/search/eess?searchtype=author&query=Fan%2C+X">Xiaojuan Fan</a>, 
<a href="/search/eess?searchtype=author&query=Lei%2C+J">Jianqin Lei</a>, 
<a href="/search/eess?searchtype=author&query=Wei%2C+T">Ting Wei</a>, 
<a href="/search/eess?searchtype=author&query=Duan%2C+J">Junguo Duan</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+C">Chun Liu</a>, 
<a href="/search/eess?searchtype=author&query=Xia%2C+X">Xiaobo Xia</a>, 
<a href="/search/eess?searchtype=author&query=Xiong%2C+S">Siqi Xiong</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Junhong Li</a>, 
<a href="/search/eess?searchtype=author&query=Lo%2C+B">Benny Lo</a>, 
<a href="/search/eess?searchtype=author&query=Tham%2C+Y+C">Yih Chung Tham</a>, 
<a href="/search/eess?searchtype=author&query=Wong%2C+T+Y">Tien Yin Wong</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+N">Ningli Wang</a>, 
<a href="/search/eess?searchtype=author&query=Yuan%2C+W">Wu Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We present VisionFM, a foundation model pre-trained with 3.4 million
ophthalmic images from 560,457 individuals, covering a broad range of
ophthalmic diseases, modalities, imaging devices, and demography. After
pre-training, VisionFM provides a foundation to foster multiple ophthalmic
artificial intelligence (AI) applications, such as disease screening and
diagnosis, disease prognosis, subclassification of disease phenotype, and
systemic biomarker and disease prediction, with each application enhanced with
expert-level intelligence and accuracy. The generalist intelligence of VisionFM
outperformed ophthalmologists with basic and intermediate levels in jointly
diagnosing 12 common ophthalmic diseases. Evaluated on a new large-scale
ophthalmic disease diagnosis benchmark database, as well as a new large-scale
segmentation and detection benchmark database, VisionFM outperformed strong
baseline deep neural networks. The ophthalmic image representations learned by
VisionFM exhibited noteworthy explainability, and demonstrated strong
generalizability to new ophthalmic modalities, disease spectrum, and imaging
devices. As a foundation model, VisionFM has a large capacity to learn from
diverse ophthalmic imaging data and disparate datasets. To be commensurate with
this capacity, in addition to the real data used for pre-training, we also
generated and leveraged synthetic ophthalmic imaging data. Experimental results
revealed that synthetic data that passed visual Turing tests, can also enhance
the representation learning capability of VisionFM, leading to substantial
performance gains on downstream ophthalmic AI tasks. Beyond the ophthalmic AI
applications developed, validated, and demonstrated in this work, substantial
further applications can be achieved in an efficient and cost-effective manner
using VisionFM as the foundation.
</p>
</div>
</dd>
<dt><a name="item811">[811]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05013" title="Abstract">arXiv:2310.05013</a> (cross-list from quant-ph) [<a href="/pdf/2310.05013" title="Download PDF">pdf</a>, <a href="/format/2310.05013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resource Efficient Boolean Function Solver on Quantum Computer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/quant-ph?searchtype=author&query=Shen%2C+H">Hanxiang Shen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Gao%2C+W">Weiguo Gao</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+Y">Yingzhou Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Nonlinear boolean equation systems play an important role in a wide range of
applications. Grover's algorithm is one of the best-known quantum search
algorithms in solving the nonlinear boolean equation system on quantum
computers. In this paper, we propose three novel techniques to improve the
efficiency under Grover's algorithm framework. A W-cycle circuit construction
introduces a recursive idea to increase the solvable number of boolean
equations given a fixed number of qubits. Then, a greedy compression technique
is proposed to reduce the oracle circuit depth. Finally, a randomized Grover's
algorithm randomly chooses a subset of equations to form a random oracle every
iteration, which further reduces the circuit depth and the number of ancilla
qubits. Numerical results on boolean quadratic equations demonstrate the
efficiency of the proposed techniques.
</p>
</div>
</dd>
<dt><a name="item812">[812]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05018" title="Abstract">arXiv:2310.05018</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2310.05018" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-in-the-loop: The future of Machine Learning in Automated Electron  Microscopy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Kalinin%2C+S+V">Sergei V. Kalinin</a>, 
<a href="/search/cond-mat?searchtype=author&query=Liu%2C+Y">Yongtao Liu</a>, 
<a href="/search/cond-mat?searchtype=author&query=Biswas%2C+A">Arpan Biswas</a>, 
<a href="/search/cond-mat?searchtype=author&query=Duscher%2C+G">Gerd Duscher</a>, 
<a href="/search/cond-mat?searchtype=author&query=Pratiush%2C+U">Utkarsh Pratiush</a>, 
<a href="/search/cond-mat?searchtype=author&query=Roccapriore%2C+K">Kevin Roccapriore</a>, 
<a href="/search/cond-mat?searchtype=author&query=Ziatdinov%2C+M">Maxim Ziatdinov</a>, 
<a href="/search/cond-mat?searchtype=author&query=Vasudevan%2C+R">Rama Vasudevan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Machine learning methods are progressively gaining acceptance in the electron
microscopy community for de-noising, semantic segmentation, and dimensionality
reduction of data post-acquisition. The introduction of the APIs by major
instrument manufacturers now allows the deployment of ML workflows in
microscopes, not only for data analytics but also for real-time decision-making
and feedback for microscope operation. However, the number of use cases for
real-time ML remains remarkably small. Here, we discuss some considerations in
designing ML-based active experiments and pose that the likely strategy for the
next several years will be human-in-the-loop automated experiments (hAE). In
this paradigm, the ML learning agent directly controls beam position and image
and spectroscopy acquisition functions, and human operator monitors experiment
progression in real- and feature space of the system and tunes the policies of
the ML agent to steer the experiment towards specific objectives.
</p>
</div>
</dd>
<dt><a name="item813">[813]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05052" title="Abstract">arXiv:2310.05052</a> (cross-list from eess.SP) [<a href="/pdf/2310.05052" title="Download PDF">pdf</a>, <a href="/format/2310.05052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing Intra- and Inter-Cell Differences: A Comprehensive Approach  to Precise Battery Lifespan Estimations across Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yuqi Li</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+S">Shun Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+Z">Ziheng Lu</a>, 
<a href="/search/eess?searchtype=author&query=Gui%2C+X">Xiaofan Gui</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+W">Wei Xu</a>, 
<a href="/search/eess?searchtype=author&query=Bian%2C+J">Jiang Bian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Battery life prediction holds significant practical value for battery
research and development. Currently, many data-driven models rely on early
electrical signals from specific target batteries to predict their lifespan. A
common shortfall is that most existing methods are developed based on specific
aging conditions, which not only limits their model's capability but also
diminishes their effectiveness in predicting degradation under varied
conditions. As a result, these models often miss out on fully benefiting from
the rich historical data available under other conditions. Here, to address
above, we introduce an approach that explicitly captures differences between
electrical signals of a target battery and a reference battery, irrespective of
their materials and aging conditions, to forecast the target battery life.
Through this inter-cell difference, we not only enhance the feature space but
also pave the way for a universal battery life prediction framework.
Remarkably, our model that combines the inter- and intra-cell differences
shines across diverse conditions, standing out in its efficiency and accuracy
using all accessible datasets. An essential application of our approach is its
capability to leverage data from older batteries effectively, enabling newer
batteries to capitalize on insights gained from past batteries. This work not
only enriches the battery data utilization strategy but also sets the stage for
smarter battery management system in the future.
</p>
</div>
</dd>
<dt><a name="item814">[814]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05059" title="Abstract">arXiv:2310.05059</a> (cross-list from math.OC) [<a href="/pdf/2310.05059" title="Download PDF">pdf</a>, <a href="/ps/2310.05059" title="Download PostScript">ps</a>, <a href="/format/2310.05059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive finite element approximation of bilinear optimal control with  fractional Laplacian
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+F">Fangyuan Wang</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Q">Qiming Wang</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Z">Zhaojie Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We investigate the application of a posteriori error estimates to a
fractional optimal control problem with pointwise control constraints.
Specifically, we address a problem in which the state equation is formulated as
an integral form of the fractional Laplacian equation, with the control
variable embedded within the state equation as a coefficient. We propose two
distinct finite element discretization approaches for an optimal control
problem. The first approach employs a fully discrete scheme where the control
variable is discretized using piecewise constant functions. The second
approach, a semi-discrete scheme, does not discretize the control variable.
Using the first-order optimality condition, the second-order optimality
condition, and a solution regularity analysis for the optimal control problem,
we devise a posteriori error estimates. We subsequently demonstrate the
reliability and efficiency of the proposed error estimators. Based on the
established error estimates framework, an adaptive refinement strategy is
developed to help achieve the optimal convergence rate. The effectiveness of
the refinement strategy is verified by numerical experiments.
</p>
</div>
</dd>
<dt><a name="item815">[815]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05078" title="Abstract">arXiv:2310.05078</a> (cross-list from eess.AS) [<a href="/pdf/2310.05078" title="Download PDF">pdf</a>, <a href="/format/2310.05078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partial Rank Similarity Minimization Method for Quality MOS Prediction  of Unseen Speech Synthesis Systems in Zero-Shot and Semi-supervised setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yadav%2C+H">Hemant Yadav</a>, 
<a href="/search/eess?searchtype=author&query=Cooper%2C+E">Erica Cooper</a>, 
<a href="/search/eess?searchtype=author&query=Yamagishi%2C+J">Junichi Yamagishi</a>, 
<a href="/search/eess?searchtype=author&query=Sitaram%2C+S">Sunayana Sitaram</a>, 
<a href="/search/eess?searchtype=author&query=Shah%2C+R+R">Rajiv Ratn Shah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">This paper introduces a novel objective function for quality mean opinion
score (MOS) prediction of unseen speech synthesis systems. The proposed
function measures the similarity of relative positions of predicted MOS values,
in a mini-batch, rather than the actual MOS values. That is the partial rank
similarity is measured (PRS) rather than the individual MOS values as with the
L1 loss. Our experiments on out-of-domain speech synthesis systems demonstrate
that the PRS outperforms L1 loss in zero-shot and semi-supervised settings,
exhibiting stronger correlation with ground truth. These findings highlight the
importance of considering rank order, as done by PRS, when training MOS
prediction models. We also argue that mean squared error and linear correlation
coefficient metrics may be unreliable for evaluating MOS prediction models. In
conclusion, PRS-trained models provide a robust framework for evaluating speech
quality and offer insights for developing high-quality speech synthesis
systems. Code and models are available at
github.com/nii-yamagishilab/partial_rank_similarity/
</p>
</div>
</dd>
<dt><a name="item816">[816]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05181" title="Abstract">arXiv:2310.05181</a> (cross-list from eess.AS) [<a href="/pdf/2310.05181" title="Download PDF">pdf</a>, <a href="/format/2310.05181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified speech and gesture synthesis using flow matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mehta%2C+S">Shivam Mehta</a>, 
<a href="/search/eess?searchtype=author&query=Tu%2C+R">Ruibo Tu</a>, 
<a href="/search/eess?searchtype=author&query=Alexanderson%2C+S">Simon Alexanderson</a>, 
<a href="/search/eess?searchtype=author&query=Beskow%2C+J">Jonas Beskow</a>, 
<a href="/search/eess?searchtype=author&query=Sz%C3%A9kely%2C+%C3%89">&#xc9;va Sz&#xe9;kely</a>, 
<a href="/search/eess?searchtype=author&query=Henter%2C+G+E">Gustav Eje Henter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure. Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Graphics (cs.GR); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">As text-to-speech technologies achieve remarkable naturalness in read-aloud
tasks, there is growing interest in multimodal synthesis of verbal and
non-verbal communicative behaviour, such as spontaneous speech and associated
body gestures. This paper presents a novel, unified architecture for jointly
synthesising speech acoustics and skeleton-based 3D gesture motion from text,
trained using optimal-transport conditional flow matching (OT-CFM). The
proposed architecture is simpler than the previous state of the art, has a
smaller memory footprint, and can capture the joint distribution of speech and
gestures, generating both modalities together in one single process. The new
training regime, meanwhile, enables better synthesis quality in much fewer
steps (network evaluations) than before. Uni- and multimodal subjective tests
demonstrate improved speech naturalness, gesture human-likeness, and
cross-modal appropriateness compared to existing benchmarks.
</p>
</div>
</dd>
<dt><a name="item817">[817]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05203" title="Abstract">arXiv:2310.05203</a> (cross-list from eess.AS) [<a href="/pdf/2310.05203" title="Download PDF">pdf</a>, <a href="/format/2310.05203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Study of Voice Conversion Models with Large-Scale Speech  and Singing Data: The T13 Systems for the Singing Voice Conversion Challenge  2023
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yamamoto%2C+R">Ryuichi Yamamoto</a>, 
<a href="/search/eess?searchtype=author&query=Yoneyama%2C+R">Reo Yoneyama</a>, 
<a href="/search/eess?searchtype=author&query=Violeta%2C+L+P">Lester Phillip Violeta</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+W">Wen-Chin Huang</a>, 
<a href="/search/eess?searchtype=author&query=Toda%2C+T">Tomoki Toda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD); Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper presents our systems (denoted as T13) for the singing voice
conversion challenge (SVCC) 2023. For both in-domain and cross-domain English
singing voice conversion (SVC) tasks (Task 1 and Task 2), we adopt a
recognition-synthesis approach with self-supervised learning-based
representation. To achieve data-efficient SVC with a limited amount of target
singer/speaker's data (150 to 160 utterances for SVCC 2023), we first train a
diffusion-based any-to-any voice conversion model using publicly available
large-scale 750 hours of speech and singing data. Then, we finetune the model
for each target singer/speaker of Task 1 and Task 2. Large-scale listening
tests conducted by SVCC 2023 show that our T13 system achieves competitive
naturalness and speaker similarity for the harder cross-domain SVC (Task 2),
which implies the generalization ability of our proposed method. Our objective
evaluation results show that using large datasets is particularly beneficial
for cross-domain SVC.
</p>
</div>
</dd>
<dt><a name="item818">[818]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05213" title="Abstract">arXiv:2310.05213</a> (cross-list from quant-ph) [<a href="/pdf/2310.05213" title="Download PDF">pdf</a>, <a href="/ps/2310.05213" title="Download PostScript">ps</a>, <a href="/format/2310.05213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Quantum Approach for Reducing Communications in Classical  Cryptographic Primitives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Zhang%2C+J">Jiayu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">How could quantum cryptography help us achieve what are not achievable in
classical cryptography? In this work we consider the following problem, which
we call succinct RSPV for classical functions (sRCF). Suppose $f$ is a function
described by a polynomial time classical Turing machine, which is public; the
client would like to sample a random $x$ as the function input and use a
protocol to send $f(x)$ to the server. What's more, (1) when the server is
malicious, what it knows in the passing space should be no more than $f(x)$;
(2) the communication should be succinct (that is, independent to the running
time of evaluating $f$). Solving this problem in classical cryptography seems
to require strong cryptographic primitives.
<br />We show that, perhaps surprisingly, it's possible to solve this problem with
quantum techniques under much weaker assumptions. By allowing for quantum
communication and computations, we give a protocol for this problem assuming
only collapsing hash functions [Unr16]. Our work conveys an interesting message
that quantum cryptography could outperform classical cryptography in a new type
of problems, that is, to reduce communications in meaningful primitives without
using heavy classical cryptographic primitives.
</p>
</div>
</dd>
<dt><a name="item819">[819]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05229" title="Abstract">arXiv:2310.05229</a> (cross-list from quant-ph) [<a href="/pdf/2310.05229" title="Download PDF">pdf</a>, <a href="/format/2310.05229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design Verification of the Quantum Control Stack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Alavi%2C+S+A">Seyed Amir Alavi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ishtiaq%2C+S">Samin Ishtiaq</a>, 
<a href="/search/quant-ph?searchtype=author&query=Johnson%2C+N">Nick Johnson</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mishra%2C+R">Rojalin Mishra</a>, 
<a href="/search/quant-ph?searchtype=author&query=Nagalakshmi%2C+D+O">Dwaraka Oruganti Nagalakshmi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Pearl%2C+A">Asher Pearl</a>, 
<a href="/search/quant-ph?searchtype=author&query=Snoeijs%2C+J">Jan Snoeijs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In DVCon Europe 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">This paper describes the verification of the classical software and hardware
stack that is used to control cold atom- and superconducting-based quantum
computing hardware. The paper serves both as an introduction to quantum
computing and to how classical device verification techniques can be employed
there. Two main challenges in building a quantum control stack are generating
precise deterministic-timing operations at the edge and scaled-out processing
in the middle layer. Both challenges are to do with a certain kind of
functional performance correctness. And, as usual, the design lives under tight
power, memory and latency constraints. The quantum control stack is a complex
interaction of algorithms, software runtimes and digital hardware. We take
inspiration from modern software approaches to engineering, such as continuous
integration and hardware automation, to quickly ship experimental features to
customers in the field.
</p>
</div>
</dd>
<dt><a name="item820">[820]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05230" title="Abstract">arXiv:2310.05230</a> (cross-list from math.OC) [<a href="/pdf/2310.05230" title="Download PDF">pdf</a>, <a href="/format/2310.05230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global Convergence of Policy Gradient Methods in Reinforcement Learning,  Games and Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cen%2C+S">Shicong Cen</a>, 
<a href="/search/math?searchtype=author&query=Chi%2C+Y">Yuejie Chi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIAG/OPT Views and News
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computer Science and Game Theory (cs.GT); Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
<p class="mathjax">Policy gradient methods, where one searches for the policy of interest by
maximizing the value functions using first-order information, become
increasingly popular for sequential decision making in reinforcement learning,
games, and control. Guaranteeing the global optimality of policy gradient
methods, however, is highly nontrivial due to nonconcavity of the value
functions. In this exposition, we highlight recent progresses in understanding
and developing policy gradient methods with global convergence guarantees,
putting an emphasis on their finite-time convergence rates with regard to
salient problem parameters.
</p>
</div>
</dd>
<dt><a name="item821">[821]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05237" title="Abstract">arXiv:2310.05237</a> (cross-list from eess.IV) [<a href="/pdf/2310.05237" title="Download PDF">pdf</a>, <a href="/format/2310.05237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Diffusion Model for Medical Image Standardization and Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Selim%2C+M">Md Selim</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Fathi%2C+F">Faraneh Fathi</a>, 
<a href="/search/eess?searchtype=author&query=Brooks%2C+M+A">Michael A. Brooks</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+G">Ge Wang</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+G">Guoqiang Yu</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+J">Jin Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Computed tomography (CT) serves as an effective tool for lung cancer
screening, diagnosis, treatment, and prognosis, providing a rich source of
features to quantify temporal and spatial tumor changes. Nonetheless, the
diversity of CT scanners and customized acquisition protocols can introduce
significant inconsistencies in texture features, even when assessing the same
patient. This variability poses a fundamental challenge for subsequent research
that relies on consistent image features. Existing CT image standardization
models predominantly utilize GAN-based supervised or semi-supervised learning,
but their performance remains limited. We present DiffusionCT, an innovative
score-based DDPM model that operates in the latent space to transform disparate
non-standard distributions into a standardized form. The architecture comprises
a U-Net-based encoder-decoder, augmented by a DDPM model integrated at the
bottleneck position. First, the encoder-decoder is trained independently,
without embedding DDPM, to capture the latent representation of the input data.
Second, the latent DDPM model is trained while keeping the encoder-decoder
parameters fixed. Finally, the decoder uses the transformed latent
representation to generate a standardized CT image, providing a more consistent
basis for downstream analysis. Empirical tests on patient CT images indicate
notable improvements in image standardization using DiffusionCT. Additionally,
the model significantly reduces image noise in SPAD images, further validating
the effectiveness of DiffusionCT for advanced imaging tasks.
</p>
</div>
</dd>
<dt><a name="item822">[822]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05271" title="Abstract">arXiv:2310.05271</a> (cross-list from eess.SP) [<a href="/pdf/2310.05271" title="Download PDF">pdf</a>, <a href="/format/2310.05271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 5G Advanced: Wireless Channel Virtualization and Resource Mapping for  Real Time Spectrum Sharing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Alqwider%2C+W">Walaa Alqwider</a>, 
<a href="/search/eess?searchtype=author&query=Abdalla%2C+A+S">Aly Sabri Abdalla</a>, 
<a href="/search/eess?searchtype=author&query=Marojevic%2C+V">Vuk Marojevic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Networking and Internet Architecture (cs.NI); Systems and Control (eess.SY)

</div>
<p class="mathjax">The coexistence between active wireless communications and passive RF
spectrum use becomes an increasingly important requirement for coordinated
spectrum access supporting critical services. The ongoing research and
technological progress are focused on effective spectrum utilization including
large-scale MIMO and energy efficient and low-power communications, innovative
spectrum use and management, and resilient spectrum sharing, just to name a
few. This paper introduces a new tool for real time spectrum sharing among
emerging cellular networks and passive RF sensing systems used for remote
sensing and radio astronomy, among others. Specifically we propose leveraging
wireless channel virtualization and propose a virtual-to-physical resource
mapping framework, mapping types, and control signaling that extends the
current 5G New Radio (NR) specifications. Our technology introduces minimal
changes to the protocol and is meant to be transparent to the end user
application. We validate the proposed technology by extending a 3GPP compliant
5G NR downlink simulator and identify further research directions where work is
needed on designing effective ways to explicitly signal the need for spectrum
or spectrum use predictions.
</p>
</div>
</dd>
<dt><a name="item823">[823]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05288" title="Abstract">arXiv:2310.05288</a> (cross-list from stat.ML) [<a href="/pdf/2310.05288" title="Download PDF">pdf</a>, <a href="/format/2310.05288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustering Three-Way Data with Outliers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Clark%2C+K+M">Katharine M. Clark</a>, 
<a href="/search/stat?searchtype=author&query=McNicholas%2C+P+D">Paul D. McNicholas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Matrix-variate distributions are a recent addition to the model-based
clustering field, thereby making it possible to analyze data in matrix form
with complex structure such as images and time series. Due to its recent
appearance, there is limited literature on matrix-variate data, with even less
on dealing with outliers in these models. An approach for clustering
matrix-variate normal data with outliers is discussed. The approach, which uses
the distribution of subset log-likelihoods, extends the OCLUST algorithm to
matrix-variate normal data and uses an iterative approach to detect and trim
outliers.
</p>
</div>
</dd>
<dt><a name="item824">[824]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05299" title="Abstract">arXiv:2310.05299</a> (cross-list from eess.IV) [<a href="/pdf/2310.05299" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Compression and Decompression Framework Based on Latent Diffusion  Model for Breast Mammography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hwang%2C+I">InChan Hwang</a>, 
<a href="/search/eess?searchtype=author&query=Woo%2C+M">MinJae Woo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages IEEE conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">This research presents a novel framework for the compression and
decompression of medical images utilizing the Latent Diffusion Model (LDM). The
LDM represents advancement over the denoising diffusion probabilistic model
(DDPM) with a potential to yield superior image quality while requiring fewer
computational resources in the image decompression process. A possible
application of LDM and Torchvision for image upscaling has been explored using
medical image data, serving as an alternative to traditional image compression
and decompression algorithms. The experimental outcomes demonstrate that this
approach surpasses a conventional file compression algorithm, and convolutional
neural network (CNN) models trained with decompressed files perform comparably
to those trained with original image files. This approach also significantly
reduces dataset size so that it can be distributed with a smaller size, and
medical images take up much less space in medical devices. The research
implications extend to noise reduction in lossy compression algorithms and
substitute for complex wavelet-based lossless algorithms.
</p>
</div>
</dd>
<dt><a name="item825">[825]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05353" title="Abstract">arXiv:2310.05353</a> (cross-list from math.CO) [<a href="/pdf/2310.05353" title="Download PDF">pdf</a>, <a href="/ps/2310.05353" title="Download PostScript">ps</a>, <a href="/format/2310.05353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity of null dynamical systems and Sauer--Shelah lemmas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gao%2C+G">Guorong Gao</a>, 
<a href="/search/math?searchtype=author&query=Ma%2C+J">Jie Ma</a>, 
<a href="/search/math?searchtype=author&query=Rong%2C+M">Mingyuan Rong</a>, 
<a href="/search/math?searchtype=author&query=Tran%2C+T">Tuan Tran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Complexity (cs.CC); Dynamical Systems (math.DS)

</div>
<p class="mathjax">The topological entropy of a topological dynamical system, introduced in a
foundational paper by Adler, Konheim and McAndrew [Trans. Am. Math. Soc.,
1965], is a nonnegative number that measures the uncertainty or disorder of the
system. Comparing with positive entropy systems, zero entropy systems are much
less understood. In order to distinguish between zero entropy systems, Huang
and Ye [Adv. Math., 2009] introduced the concept of maximal pattern entropy of
a topological dynamical system. At the heart of their analysis is a
Sauer-Shelah type lemma. In the present paper, we provide a shorter and more
conceptual proof of a strengthening of this lemma, and discuss its surprising
connection between dynamical system, combinatorics and a recent breakthrough in
communication complexity. We also improve one of the main results of Huang and
Ye on the maximal pattern entropy of zero-dimensional systems, by proving a new
Sauer-Shelah type lemma, which unifies and enhances various extremal results on
VC-dimension, Natarajan dimension and Steele dimension.
</p>
</div>
</dd>
<dt><a name="item826">[826]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05358" title="Abstract">arXiv:2310.05358</a> (cross-list from quant-ph) [<a href="/pdf/2310.05358" title="Download PDF">pdf</a>, <a href="/format/2310.05358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A family of permutationally invariant quantum codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Aydin%2C+A">Arda Aydin</a>, 
<a href="/search/quant-ph?searchtype=author&query=Alekseyev%2C+M+A">Max A. Alekseyev</a>, 
<a href="/search/quant-ph?searchtype=author&query=Barg%2C+A">Alexander Barg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">We construct a new family of permutationally invariant codes that correct $t$
Pauli errors for any $t\ge 1$. We also show that codes in the new family
correct spontaneous decay errors as well as deletion errors. In many cases the
codes in this family are shorter than the best previously known explicit
families of permutationally invariant codes both for Pauli errors, deletions,
and for the amplitude damping channel. As a separate result, we generalize the
conditions for permutationally invariant codes to correct $t$ Pauli errors from
the previously known results for $t=1$ to any number of errors. For small $t$,
these conditions can be used to construct new examples of codes by computer.
</p>
</div>
</dd>
<dt><a name="item827">[827]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05371" title="Abstract">arXiv:2310.05371</a> (cross-list from eess.IV) [<a href="/pdf/2310.05371" title="Download PDF">pdf</a>, <a href="/format/2310.05371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Prostate Cancer Diagnosis with Deep Learning: A Study using  mpMRI Segmentation and Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gavade%2C+A+B">Anil B. Gavade</a>, 
<a href="/search/eess?searchtype=author&query=Kanwal%2C+N">Neel Kanwal</a>, 
<a href="/search/eess?searchtype=author&query=Gavade%2C+P+A">Priyanka A. Gavade</a>, 
<a href="/search/eess?searchtype=author&query=Nerli%2C+R">Rajendra Nerli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CISCON-2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Prostate cancer (PCa) is a severe disease among men globally. It is important
to identify PCa early and make a precise diagnosis for effective treatment. For
PCa diagnosis, Multi-parametric magnetic resonance imaging (mpMRI) emerged as
an invaluable imaging modality that offers a precise anatomical view of the
prostate gland and its tissue structure. Deep learning (DL) models can enhance
existing clinical systems and improve patient care by locating regions of
interest for physicians. Recently, DL techniques have been employed to develop
a pipeline for segmenting and classifying different cancer types. These studies
show that DL can be used to increase diagnostic precision and give objective
results without variability. This work uses well-known DL models for the
classification and segmentation of mpMRI images to detect PCa. Our
implementation involves four pipelines; Semantic DeepSegNet with ResNet50,
DeepSegNet with recurrent neural network (RNN), U-Net with RNN, and U-Net with
a long short-term memory (LSTM). Each segmentation model is paired with a
different classifier to evaluate the performance using different metrics. The
results of our experiments show that the pipeline that uses the combination of
U-Net and the LSTM model outperforms all other combinations, excelling in both
segmentation and classification tasks
</p>
</div>
</dd>
<dt><a name="item828">[828]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05390" title="Abstract">arXiv:2310.05390</a> (cross-list from math.PR) [<a href="/pdf/2310.05390" title="Download PDF">pdf</a>, <a href="/ps/2310.05390" title="Download PostScript">ps</a>, <a href="/format/2310.05390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximation of the invariant measure for stable SDE by the  Euler-Maruyama scheme with decreasing step-sizes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+P">Peng Chen</a>, 
<a href="/search/math?searchtype=author&query=Jin%2C+X">Xinghu Jin</a>, 
<a href="/search/math?searchtype=author&query=Xiao%2C+Y">Yimin Xiao</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+L">Lihu Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Dynamical Systems (math.DS); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Let $(X_t)_{t \ge 0}$ be the solution of the stochastic differential equation
$$dX_t = b(X_t) dt+A dZ_t, \quad X_{0}=x,$$ where $b: \mathbb{R}^d \rightarrow
\mathbb R^d$ is a Lipschitz function, $A \in \mathbb R^{d \times d}$ is a
positive definite matrix, $(Z_t)_{t\geq 0}$ is a $d$-dimensional rotationally
invariant $\alpha$-stable L\'evy process with $\alpha \in (1,2)$ and
$x\in\mathbb{R}^{d}$. We use two Euler-Maruyama schemes with decreasing step
sizes $\Gamma = (\gamma_n)_{n\in \mathbb{N}}$ to approximate the invariant
measure of $(X_t)_{t \ge 0}$: one with i.i.d. $\alpha$-stable distributed
random variables as its innovations and the other with i.i.d. Pareto
distributed random variables as its innovations. We study the convergence rate
of these two approximation schemes in the Wasserstein-1 distance. For the first
scheme, when the function $b$ is Lipschitz and satisfies a certain dissipation
condition, we show that the convergence rate is $\gamma^{1/\alpha}_n$. Under an
additional assumption on the second order directional derivatives of $b$, this
convergence rate can be improved to $\gamma^{1+\frac 1
{\alpha}-\frac{1}{\kappa}}_n$ for any $\kappa \in [1,\alpha)$. For the second
scheme, when the function $b$ is twice continuously differentiable, we obtain a
convergence rate of $\gamma^{\frac{2-\alpha}{\alpha}}_n$. We show that the rate
$\gamma^{\frac{2-\alpha}{\alpha}}_n$ is optimal for the one dimensional stable
Ornstein-Uhlenbeck process. Our theorems indicate that the recent remarkable
result about the unadjusted Langevin algorithm with additive innovations can be
extended to the SDEs driven by an $\alpha$-stable L\'evy process and the
corresponding convergence rate has a similar behaviour. Compared with the
previous result, we have relaxed the second order differentiability condition
to the Lipschitz condition for the first scheme.
</p>
</div>
</dd>
<dt><a name="item829">[829]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05445" title="Abstract">arXiv:2310.05445</a> (cross-list from eess.IV) [<a href="/pdf/2310.05445" title="Download PDF">pdf</a>, <a href="/format/2310.05445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AngioMoCo: Learning-based Motion Correction in Cerebral Digital  Subtraction Angiography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Su%2C+R">Ruisheng Su</a>, 
<a href="/search/eess?searchtype=author&query=van+der+Sluijs%2C+M">Matthijs van der Sluijs</a>, 
<a href="/search/eess?searchtype=author&query=Cornelissen%2C+S">Sandra Cornelissen</a>, 
<a href="/search/eess?searchtype=author&query=van+Zwam%2C+W">Wim van Zwam</a>, 
<a href="/search/eess?searchtype=author&query=van+der+Lugt%2C+A">Aad van der Lugt</a>, 
<a href="/search/eess?searchtype=author&query=Niessen%2C+W">Wiro Niessen</a>, 
<a href="/search/eess?searchtype=author&query=Ruijters%2C+D">Danny Ruijters</a>, 
<a href="/search/eess?searchtype=author&query=van+Walsum%2C+T">Theo van Walsum</a>, 
<a href="/search/eess?searchtype=author&query=Dalca%2C+A">Adrian Dalca</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Cerebral X-ray digital subtraction angiography (DSA) is the standard imaging
technique for visualizing blood flow and guiding endovascular treatments. The
quality of DSA is often negatively impacted by body motion during acquisition,
leading to decreased diagnostic value. Time-consuming iterative methods address
motion correction based on non-rigid registration, and employ sparse key points
and non-rigidity penalties to limit vessel distortion. Recent methods alleviate
subtraction artifacts by predicting the subtracted frame from the corresponding
unsubtracted frame, but do not explicitly compensate for motion-induced
misalignment between frames. This hinders the serial evaluation of blood flow,
and often causes undesired vasculature and contrast flow alterations, leading
to impeded usability in clinical practice. To address these limitations, we
present AngioMoCo, a learning-based framework that generates motion-compensated
DSA sequences from X-ray angiography. AngioMoCo integrates contrast extraction
and motion correction, enabling differentiation between patient motion and
intensity changes caused by contrast flow. This strategy improves registration
quality while being substantially faster than iterative elastix-based methods.
We demonstrate AngioMoCo on a large national multi-center dataset (MR CLEAN
Registry) of clinically acquired angiographic images through comprehensive
qualitative and quantitative analyses. AngioMoCo produces high-quality
motion-compensated DSA, removing motion artifacts while preserving contrast
flow. Code is publicly available at https://github.com/RuishengSu/AngioMoCo.
</p>
</div>
</dd>
<dt><a name="item830">[830]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05446" title="Abstract">arXiv:2310.05446</a> (cross-list from eess.IV) [<a href="/pdf/2310.05446" title="Download PDF">pdf</a>, <a href="/format/2310.05446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RetSeg: Retention-based Colorectal Polyps Segmentation Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=ELKarazle%2C+K">Khaled ELKarazle</a>, 
<a href="/search/eess?searchtype=author&query=Raman%2C+V">Valliappan Raman</a>, 
<a href="/search/eess?searchtype=author&query=Chua%2C+C">Caslon Chua</a>, 
<a href="/search/eess?searchtype=author&query=Then%2C+P">Patrick Then</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Vision Transformers (ViTs) have revolutionized medical imaging analysis,
showcasing superior efficacy compared to conventional Convolutional Neural
Networks (CNNs) in vital tasks such as polyp classification, detection, and
segmentation. Leveraging attention mechanisms to focus on specific image
regions, ViTs exhibit contextual awareness in processing visual data,
culminating in robust and precise predictions, even for intricate medical
images. Moreover, the inherent self-attention mechanism in Transformers
accommodates varying input sizes and resolutions, granting an unprecedented
flexibility absent in traditional CNNs. However, Transformers grapple with
challenges like excessive memory usage and limited training parallelism due to
self-attention, rendering them impractical for real-time disease detection on
resource-constrained devices. In this study, we address these hurdles by
investigating the integration of the recently introduced retention mechanism
into polyp segmentation, introducing RetSeg, an encoder-decoder network
featuring multi-head retention blocks. Drawing inspiration from Retentive
Networks (RetNet), RetSeg is designed to bridge the gap between precise polyp
segmentation and resource utilization, particularly tailored for colonoscopy
images. We train and validate RetSeg for polyp segmentation employing two
publicly available datasets: Kvasir-SEG and CVC-ClinicDB. Additionally, we
showcase RetSeg's promising performance across diverse public datasets,
including CVC-ColonDB, ETIS-LaribPolypDB, CVC-300, and BKAI-IGH NeoPolyp. While
our work represents an early-stage exploration, further in-depth studies are
imperative to advance these promising findings.
</p>
</div>
</dd>
<dt><a name="item831">[831]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05466" title="Abstract">arXiv:2310.05466</a> (cross-list from math.AG) [<a href="/pdf/2310.05466" title="Download PDF">pdf</a>, <a href="/format/2310.05466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometry of the signed support of a multivariate polynomial and  Descartes&#x27; rule of signs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Telek%2C+M+L">M&#xe1;t&#xe9; L. Telek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Geometry (math.AG)</span>; Symbolic Computation (cs.SC)

</div>
<p class="mathjax">We describe conditions on the signed support, that is, on the set of the
exponent vectors and on the signs of the coefficients, of a multivariate
polynomial $f$ ensuring that the semi-algebraic set $\{ f &lt; 0 \}$ defined in
the positive orthant has at most one connected component. These results
generalize Descartes' rule of signs in the sense that they provide a bound
which is independent of the values of the coefficients and the degree of the
polynomial. Based on how the exponent vectors lie on the faces of the Newton
polytope, we give a recursive algorithm that verifies a sufficient condition
for the set $\{ f &lt; 0 \}$ to have one connected component. We apply the
algorithm to reaction networks in order to prove that the parameter region of
multistationarity of a ubiquitous network comprising phosphorylation cycles is
connected.
</p>
</div>
</dd>
<dt><a name="item832">[832]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05468" title="Abstract">arXiv:2310.05468</a> (cross-list from stat.ML) [<a href="/pdf/2310.05468" title="Download PDF">pdf</a>, <a href="/format/2310.05468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ExIFFI and EIF+: Interpretability and Enhanced Generalizability to  Extend the Extended Isolation Forest
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Arcudi%2C+A">Alessio Arcudi</a>, 
<a href="/search/stat?searchtype=author&query=Frizzo%2C+D">Davide Frizzo</a>, 
<a href="/search/stat?searchtype=author&query=Masiero%2C+C">Chiara Masiero</a>, 
<a href="/search/stat?searchtype=author&query=Susto%2C+G+A">Gian Antonio Susto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
<p class="mathjax">Anomaly detection, an essential unsupervised machine learning task, involves
identifying unusual behaviors within complex datasets and systems. While
Machine Learning algorithms and decision support systems (DSSs) offer effective
solutions for this task, simply pinpointing anomalies often falls short in
real-world applications. Users of these systems often require insight into the
underlying reasons behind predictions to facilitate Root Cause Analysis and
foster trust in the model. However, due to the unsupervised nature of anomaly
detection, creating interpretable tools is challenging. This work introduces
EIF+, an enhanced variant of Extended Isolation Forest (EIF), designed to
enhance generalization capabilities. Additionally, we present ExIFFI, a novel
approach that equips Extended Isolation Forest with interpretability features,
specifically feature rankings. Experimental results provide a comprehensive
comparative analysis of Isolation-based approaches for Anomaly Detection,
including synthetic and real dataset evaluations that demonstrate ExIFFI's
effectiveness in providing explanations. We also illustrate how ExIFFI serves
as a valid feature selection technique in unsupervised settings. To facilitate
further research and reproducibility, we also provide open-source code to
replicate the results.
</p>
</div>
</dd>
<dt><a name="item833">[833]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05478" title="Abstract">arXiv:2310.05478</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2310.05478" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grease the gears for a steady microfluidic flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Leuthner%2C+M">Moritz Leuthner</a>, 
<a href="/search/physics?searchtype=author&query=Hayden%2C+O">Oliver Hayden</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Systems and Control (eess.SY); Applied Physics (physics.app-ph)

</div>
<p class="mathjax">Pumps are indispensable for analytical applications and ensure controlled
fluid movement. Syringe pumps are among today_s most prevalent liquid delivery
systems, especially for high-pressure, stable, low-flow-rate microfluidic
applications. Due to moving mechanical parts of the assembly, regular
maintenance is essential to ensure reliable operation and flow rates. However,
lubrication of the mechanics is easily overlooked because the research focuses
on novel analytical applications rather than on the maintenance of pumps. Here,
we investigate the lubrication of the syringe pump guide rods with its effect
on the flow rate stability after regular pump cleaning from contaminations. The
guide rods of syringe pumps were thoroughly cleaned from any lubricant, and the
flow rate for specified flowrates between 5 and 30 uL/min was measured,
revealing tremendous flow rate fluctuations with a coefficient of variation
(CV) value up to 0.34. In contrast, flow rate measurements of syringe pumps
with lubricated guide rods show a five-fold smoother flow rate fluctuation
depending on the specified flow rate with CV values below 0.07. In summary, we
emphasize the awareness of lubricating moving parts of syringe pumps to achieve
constant flow rates, minimize wear, and ensure the reliable operation of, for
instance, accurate lab-on-a-chip workflows.
</p>
</div>
</dd>
<dt><a name="item834">[834]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05501" title="Abstract">arXiv:2310.05501</a> (cross-list from math.OC) [<a href="/pdf/2310.05501" title="Download PDF">pdf</a>, <a href="/format/2310.05501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inexact Newton methods with matrix approximation by sampling for  nonlinear least-squares and systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bellavia%2C+S">Stefania Bellavia</a>, 
<a href="/search/math?searchtype=author&query=Malaspina%2C+G">Greta Malaspina</a>, 
<a href="/search/math?searchtype=author&query=Morini%2C+B">Benedetta Morini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We develop and analyze stochastic inexact Gauss-Newton methods for nonlinear
least-squares problems and inexact Newton methods for nonlinear systems of
equations. Random models are formed using suitable sampling strategies for the
matrices involved in the deterministic models. The analysis of the expected
number of iterations needed in the worst case to achieve a desired level of
accuracy in the first-order optimality condition provides guidelines for
applying sampling and enforcing, with fixed probability, a suitable accuracy in
the random approximations. Results of the numerical validation of the
algorithms are presented.
</p>
</div>
</dd>
<dt><a name="item835">[835]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05526" title="Abstract">arXiv:2310.05526</a> (cross-list from math.ST) [<a href="/pdf/2310.05526" title="Download PDF">pdf</a>, <a href="/format/2310.05526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Projecting infinite time series graphs to finite marginal graphs using  number theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gerhardus%2C+A">Andreas Gerhardus</a>, 
<a href="/search/math?searchtype=author&query=Wahl%2C+J">Jonas Wahl</a>, 
<a href="/search/math?searchtype=author&query=Faltenbacher%2C+S">Sofia Faltenbacher</a>, 
<a href="/search/math?searchtype=author&query=Ninad%2C+U">Urmi Ninad</a>, 
<a href="/search/math?searchtype=author&query=Runge%2C+J">Jakob Runge</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages (including appendix), 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">In recent years, a growing number of method and application works have
adapted and applied the causal-graphical-model framework to time series data.
Many of these works employ time-resolved causal graphs that extend infinitely
into the past and future and whose edges are repetitive in time, thereby
reflecting the assumption of stationary causal relationships. However, most
results and algorithms from the causal-graphical-model framework are not
designed for infinite graphs. In this work, we develop a method for projecting
infinite time series graphs with repetitive edges to marginal graphical models
on a finite time window. These finite marginal graphs provide the answers to
$m$-separation queries with respect to the infinite graph, a task that was
previously unresolved. Moreover, we argue that these marginal graphs are useful
for causal discovery and causal effect estimation in time series, effectively
enabling to apply results developed for finite graphs to the infinite graphs.
The projection procedure relies on finding common ancestors in the
to-be-projected graph and is, by itself, not new. However, the projection
procedure has not yet been algorithmically implemented for time series graphs
since in these infinite graphs there can be infinite sets of paths that might
give rise to common ancestors. We solve the search over these possibly infinite
sets of paths by an intriguing combination of path-finding techniques for
finite directed graphs and solution theory for linear Diophantine equations. By
providing an algorithm that carries out the projection, our paper makes an
important step towards a theoretically-grounded and method-agnostic
generalization of a range of causal inference methods and results to time
series.
</p>
</div>
</dd>
<dt><a name="item836">[836]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05534" title="Abstract">arXiv:2310.05534</a> (cross-list from eess.AS) [<a href="/pdf/2310.05534" title="Download PDF">pdf</a>, <a href="/format/2310.05534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thech. Report: Genuinization of Speech waveform PMF for speaker  detection spoofing and countermeasures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lapidot%2C+I">Itshak Lapidot</a>, 
<a href="/search/eess?searchtype=author&query=Bonastre%2C+J">Jean-Francois Bonastre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">In the context of spoofing attacks in speaker recognition systems, we
observed that the waveform probability mass function (PMF) of genuine speech
differs significantly from the PMF of speech resulting from the attacks. This
is true for synthesized or converted speech as well as replayed speech. We also
noticed that this observation seems to have a significant impact on spoofing
detection performance. In this article, we propose an algorithm, denoted
genuinization, capable of reducing the waveform distribution gap between
authentic speech and spoofing speech. Our genuinization algorithm is evaluated
on ASVspoof 2019 challenge datasets, using the baseline system provided by the
challenge organization. We first assess the influence of genuinization on
spoofing performance. Using genuinization for the spoofing attacks degrades
spoofing detection performance by up to a factor of 10. Next, we integrate the
genuinization algorithm in the spoofing countermeasures and we observe a huge
spoofing detection improvement in different cases. The results of our
experiments show clearly that waveform distribution plays an important role and
must be taken into account by anti-spoofing systems.
</p>
</div>
</dd>
<dt><a name="item837">[837]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05538" title="Abstract">arXiv:2310.05538</a> (cross-list from eess.IV) [<a href="/pdf/2310.05538" title="Download PDF">pdf</a>, <a href="/format/2310.05538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> M3FPolypSegNet: Segmentation Network with Multi-frequency Feature Fusion  for Polyp Localization in Colonoscopy Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nam%2C+J">Ju-Hyeon Nam</a>, 
<a href="/search/eess?searchtype=author&query=Park%2C+S">Seo-Hyeong Park</a>, 
<a href="/search/eess?searchtype=author&query=Syazwany%2C+N+S">Nur Suriza Syazwany</a>, 
<a href="/search/eess?searchtype=author&query=Jung%2C+Y">Yerim Jung</a>, 
<a href="/search/eess?searchtype=author&query=Im%2C+Y">Yu-Han Im</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+S">Sang-Chul Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5pages. 2023 IEEE International Conference on Image Processing (ICIP). IEEE, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Polyp segmentation is crucial for preventing colorectal cancer a common type
of cancer. Deep learning has been used to segment polyps automatically, which
reduces the risk of misdiagnosis. Localizing small polyps in colonoscopy images
is challenging because of its complex characteristics, such as color,
occlusion, and various shapes of polyps. To address this challenge, a novel
frequency-based fully convolutional neural network, Multi-Frequency Feature
Fusion Polyp Segmentation Network (M3FPolypSegNet) was proposed to decompose
the input image into low/high/full-frequency components to use the
characteristics of each component. We used three independent multi-frequency
encoders to map multiple input images into a high-dimensional feature space. In
the Frequency-ASPP Scalable Attention Module (F-ASPP SAM), ASPP was applied
between each frequency component to preserve scale information. Subsequently,
scalable attention was applied to emphasize polyp regions in a high-dimensional
feature space. Finally, we designed three multi-task learning (i.e., region,
edge, and distance) in four decoder blocks to learn the structural
characteristics of the region. The proposed model outperformed various
segmentation models with performance gains of 6.92% and 7.52% on average for
all metrics on CVC-ClinicDB and BKAI-IGH-NeoPolyp, respectively.
</p>
</div>
</dd>
<dt><a name="item838">[838]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05549" title="Abstract">arXiv:2310.05549</a> (cross-list from stat.ML) [<a href="/pdf/2310.05549" title="Download PDF">pdf</a>, <a href="/format/2310.05549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Transformation Approach for Uplift Modeling with Binary Outcome
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+K">Kun Li</a>, 
<a href="/search/stat?searchtype=author&query=Tian%2C+J">Jiang Tian</a>, 
<a href="/search/stat?searchtype=author&query=Xiang%2C+X">Xiaojia Xiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Uplift modeling has been used effectively in fields such as marketing and
customer retention, to target those customers who are more likely to respond
due to the campaign or treatment. Essentially, it is a machine learning
technique that predicts the gain from performing some action with respect to
not taking it. A popular class of uplift models is the transformation approach
that redefines the target variable with the original treatment indicator. These
transformation approaches only need to train and predict the difference in
outcomes directly. The main drawback of these approaches is that in general it
does not use the information in the treatment indicator beyond the construction
of the transformed outcome and usually is not efficient. In this paper, we
design a novel transformed outcome for the case of the binary target variable
and unlock the full value of the samples with zero outcome. From a practical
perspective, our new approach is flexible and easy to use. Experimental results
on synthetic and real-world datasets obviously show that our new approach
outperforms the traditional one. At present, our new approach has already been
applied to precision marketing in a China nation-wide financial holdings group.
</p>
</div>
</dd>
<dt><a name="item839">[839]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05572" title="Abstract">arXiv:2310.05572</a> (cross-list from eess.IV) [<a href="/pdf/2310.05572" title="Download PDF">pdf</a>, <a href="/format/2310.05572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple and Robust Framework for Cross-Modality Medical Image  Segmentation applied to Vision Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bastico%2C+M">Matteo Bastico</a>, 
<a href="/search/eess?searchtype=author&query=Ryckelynck%2C+D">David Ryckelynck</a>, 
<a href="/search/eess?searchtype=author&query=Cort%C3%A9%2C+L">Laurent Cort&#xe9;</a>, 
<a href="/search/eess?searchtype=author&query=Tillier%2C+Y">Yannick Tillier</a>, 
<a href="/search/eess?searchtype=author&query=Decenci%C3%A8re%2C+E">Etienne Decenci&#xe8;re</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted in International Conference on Computer Vision Workshops (ICCVW) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">When it comes to clinical images, automatic segmentation has a wide variety
of applications and a considerable diversity of input domains, such as
different types of Magnetic Resonance Images (MRIs) and Computerized Tomography
(CT) scans. This heterogeneity is a challenge for cross-modality algorithms
that should equally perform independently of the input image type fed to them.
Often, segmentation models are trained using a single modality, preventing
generalization to other types of input data without resorting to transfer
learning techniques. Furthermore, the multi-modal or cross-modality
architectures proposed in the literature frequently require registered images,
which are not easy to collect in clinical environments, or need additional
processing steps, such as synthetic image generation. In this work, we propose
a simple framework to achieve fair image segmentation of multiple modalities
using a single conditional model that adapts its normalization layers based on
the input type, trained with non-registered interleaved mixed data. We show
that our framework outperforms other cross-modality segmentation methods, when
applied to the same 3D UNet baseline model, on the Multi-Modality Whole Heart
Segmentation Challenge. Furthermore, we define the Conditional Vision
Transformer (C-ViT) encoder, based on the proposed cross-modality framework,
and we show that it brings significant improvements to the resulting
segmentation, up to 6.87\% of Dice accuracy, with respect to its baseline
reference. The code to reproduce our experiments and the trained model weights
are available at https://github.com/matteo-bastico/MI-Seg.
</p>
</div>
</dd>
<dt><a name="item840">[840]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05629" title="Abstract">arXiv:2310.05629</a> (cross-list from eess.AS) [<a href="/pdf/2310.05629" title="Download PDF">pdf</a>, <a href="/ps/2310.05629" title="Download PostScript">ps</a>, <a href="/format/2310.05629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Super Denoise Net: Speech Super Resolution with Noise Cancellation in  Low Sampling Rate Noisy Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yang%2C+J">Junkang Yang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+H">Hongqing Liu</a>, 
<a href="/search/eess?searchtype=author&query=Gan%2C+L">Lu Gan</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Speech super-resolution (SSR) aims to predict a high resolution (HR) speech
signal from its low resolution (LR) corresponding part. Most neural SSR models
focus on producing the final result in a noise-free environment by recovering
the spectrogram of high-frequency part of the signal and concatenating it with
the original low-frequency part. Although these methods achieve high accuracy,
they become less effective when facing the real-world scenario, where
unavoidable noise is present. To address this problem, we propose a Super
Denoise Net (SDNet), a neural network for a joint task of super-resolution and
noise reduction from a low sampling rate signal. To that end, we design gated
convolution and lattice convolution blocks to enhance the repair capability and
capture information in the time-frequency axis, respectively. The experiments
show our method outperforms baseline speech denoising and SSR models on DNS
2020 no-reverb test set with higher objective and subjective scores.
</p>
</div>
</dd>
<dt><a name="item841">[841]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05638" title="Abstract">arXiv:2310.05638</a> (cross-list from eess.IV) [<a href="/pdf/2310.05638" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High Accuracy and Cost-Saving Active Learning 3D WD-UNet for Airway  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Shiyi Wang</a>, 
<a href="/search/eess?searchtype=author&query=Nan%2C+Y">Yang Nan</a>, 
<a href="/search/eess?searchtype=author&query=Walsh%2C+S">Simon Walsh</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+G">Guang Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We propose a novel Deep Active Learning (DeepAL) model-3D Wasserstein
Discriminative UNet (WD-UNet) for reducing the annotation effort of medical 3D
Computed Tomography (CT) segmentation. The proposed WD-UNet learns in a
semi-supervised way and accelerates learning convergence to meet or exceed the
prediction metrics of supervised learning models. Our method can be embedded
with different Active Learning (AL) strategies and different network
structures. The model is evaluated on 3D lung airway CT scans for medical
segmentation and show that the use of uncertainty metric, which is parametrized
as an input of query strategy, leads to more accurate prediction results than
some state-of-the-art Deep Learning (DL) supervised models, e.g.,3DUNet and 3D
CEUNet. Compared to the above supervised DL methods, our WD-UNet not only saves
the cost of annotation for radiologists but also saves computational resources.
WD-UNet uses a limited amount of annotated data (35% of the total) to achieve
better predictive metrics with a more efficient deep learning model algorithm.
</p>
</div>
</dd>
<dt><a name="item842">[842]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05639" title="Abstract">arXiv:2310.05639</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.05639" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Technocratic model of the human auditory system
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Semotiuk%2C+M+V">M. V. Semotiuk</a>, 
<a href="/search/q-bio?searchtype=author&query=Palagin%2C+A+V">A. V. Palagin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this work, we investigate the phenomenon of transverse resonance and
transverse standing waves that occur within the cochlea of living organisms. It
is demonstrated that the predisposing factor for their occurrence is the
cochlear shape, which resembles a conical acoustic tube coiled into a spiral
and exhibits non-uniformities on its internal surface. This cochlear structure
facilitates the analysis of constituent sound signals akin to a spectrum
analyzer, with a corresponding interpretation of the physical processes
occurring in the auditory system. Additionally, we conclude that the cochlear
duct's scala media, composed of a system of membranes and the organ of Corti,
functions primarily as an information collection and amplification system along
the cochlear spiral. Collectively, these findings enable the development of a
novel, highly realistic wave model of the auditory system in living organisms
based on a technocratic approach within the scientific context.
</p>
</div>
</dd>
<dt><a name="item843">[843]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05647" title="Abstract">arXiv:2310.05647</a> (cross-list from eess.IV) [<a href="/pdf/2310.05647" title="Download PDF">pdf</a>, <a href="/format/2310.05647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Manifold Structured Data Priors for Improved MR  Fingerprinting Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/eess?searchtype=author&query=Ji%2C+Y">Yuping Ji</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+Y">Yue Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 10 figures, will submit to IEEE Transactions on Medical Imaging
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Estimating tissue parameter maps with high accuracy and precision from highly
undersampled measurements presents one of the major challenges in MR
fingerprinting (MRF). Many existing works project the recovered voxel
fingerprints onto the Bloch manifold to improve reconstruction performance.
However, little research focuses on exploiting the latent manifold structure
priors among fingerprints. To fill this gap, we propose a novel MRF
reconstruction framework based on manifold structured data priors. Since it is
difficult to directly estimate the fingerprint manifold structure, we model the
tissue parameters as points on a low-dimensional parameter manifold. We reveal
that the fingerprint manifold shares the same intrinsic topology as the
parameter manifold, although being embedded in different Euclidean spaces. To
exploit the non-linear and non-local redundancies in MRF data, we divide the
MRF data into spatial patches, and the similarity measurement among data
patches can be accurately obtained using the Euclidean distance between the
corresponding patches in the parameter manifold. The measured similarity is
then used to construct the graph Laplacian operator, which represents the
fingerprint manifold structure. Thus, the fingerprint manifold structure is
introduced in the reconstruction framework by using the low-dimensional
parameter manifold. Additionally, we incorporate the locally low-rank prior in
the reconstruction framework to further utilize the local correlations within
each patch for improved reconstruction performance. We also adopt a
GPU-accelerated NUFFT library to accelerate reconstruction in non-Cartesian
sampling scenarios. Experimental results demonstrate that our method can
achieve significantly improved reconstruction performance with reduced
computational time over the state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item844">[844]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05655" title="Abstract">arXiv:2310.05655</a> (cross-list from stat.ML) [<a href="/pdf/2310.05655" title="Download PDF">pdf</a>, <a href="/format/2310.05655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal structure learning with momentum: Sampling distributions over  Markov Equivalence Classes of DAGs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Schauer%2C+M">Moritz Schauer</a>, 
<a href="/search/stat?searchtype=author&query=Wien%C3%B6bst%2C+M">Marcel Wien&#xf6;bst</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the context of inferring a Bayesian network structure (directed acyclic
graph, DAG for short), we devise a non-reversible continuous time Markov chain,
the "Causal Zig-Zag sampler", that targets a probability distribution over
classes of observationally equivalent (Markov equivalent) DAGs. The classes are
represented as completed partially directed acyclic graphs (CPDAGs). The
non-reversible Markov chain relies on the operators used in Chickering's Greedy
Equivalence Search (GES) and is endowed with a momentum variable, which
improves mixing significantly as we show empirically. The possible target
distributions include posterior distributions based on a prior over DAGs and a
Markov equivalent likelihood. We offer an efficient implementation wherein we
develop new algorithms for listing, counting, uniformly sampling, and applying
possible moves of the GES operators, all of which significantly improve upon
the state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item845">[845]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05706" title="Abstract">arXiv:2310.05706</a> (cross-list from math.LO) [<a href="/pdf/2310.05706" title="Download PDF">pdf</a>, <a href="/ps/2310.05706" title="Download PostScript">ps</a>, <a href="/format/2310.05706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extensional concepts in intensional type theory, revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kapulkin%2C+C">Chris Kapulkin</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+Y">Yufeng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages; comments welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO); Category Theory (math.CT)

</div>
<p class="mathjax">Revisiting a classic result from M. Hofmann's dissertation, we give a direct
proof of Morita equivalence, in the sense of V. Isaev, between extensional type
theory and intensional type theory extended by the principles of functional
extensionality and of uniqueness of identity proofs.
</p>
</div>
</dd>
<dt><a name="item846">[846]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05713" title="Abstract">arXiv:2310.05713</a> (cross-list from physics.comp-ph) [<a href="/pdf/2310.05713" title="Download PDF">pdf</a>, <a href="/format/2310.05713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximations of the Green&#x27;s Function in Multiple Scattering Theory for  Crystalline Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Li%2C+X">Xiaoxu Li</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+H">Huajie Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 24 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">The multiple scattering theory (MST) is a Green's function method that has
been widely used in electronic structure calculations for crystalline
disordered systems. The key property of the MST method is the scattering path
matrix (SPM) that characterizes the Green's function within a local solution
representation. This paper studies various approximations of the SPM, under the
condition that an appropriate reference is used for perturbation. In
particular, we justify the convergence of the SPM approximations with respect
to the size of scattering region and scattering length of reference, which are
the central numerical parameters to achieve a linear scaling method with MST.
We also present some numerical experiments on several typical systems to
support the theory.
</p>
</div>
</dd>
<dt><a name="item847">[847]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05725" title="Abstract">arXiv:2310.05725</a> (cross-list from stat.ML) [<a href="/pdf/2310.05725" title="Download PDF">pdf</a>, <a href="/format/2310.05725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Post-hoc Bias Scoring Is Optimal For Fair Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+W">Wenlong Chen</a>, 
<a href="/search/stat?searchtype=author&query=Klochkov%2C+Y">Yegor Klochkov</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 5 figures, 8 tables, 3 algorithms
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider a binary classification problem under group fairness constraints,
which can be one of Demographic Parity (DP), Equalized Opportunity (EOp), or
Equalized Odds (EO). We propose an explicit characterization of Bayes optimal
classifier under the fairness constraints, which turns out to be a simple
modification rule of the unconstrained classifier. Namely, we introduce a novel
instance-level measure of bias, which we call bias score, and the modification
rule is a simple linear rule on top of the finite amount of bias scores. Based
on this characterization, we develop a post-hoc approach that allows us to
adapt to fairness constraints while maintaining high accuracy. In the case of
DP and EOp constraints, the modification rule is thresholding a single bias
score, while in the case of EO constraints we are required to fit a linear
modification rule with 2 parameters. The method can also be applied for
composite group-fairness criteria, such as ones involving several sensitive
attributes. We achieve competitive or better performance compared to both
in-processing and post-processing methods across three datasets: Adult, COMPAS,
and CelebA. Unlike most post-processing methods, we do not require access to
sensitive attributes during the inference time.
</p>
</div>
</dd>
<dt><a name="item848">[848]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05742" title="Abstract">arXiv:2310.05742</a> (cross-list from stat.ML) [<a href="/pdf/2310.05742" title="Download PDF">pdf</a>, <a href="/format/2310.05742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Shape Distances on Neural Representations with Limited  Samples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Pospisil%2C+D+A">Dean A. Pospisil</a>, 
<a href="/search/stat?searchtype=author&query=Larsen%2C+B+W">Brett W. Larsen</a>, 
<a href="/search/stat?searchtype=author&query=Harvey%2C+S+E">Sarah E. Harvey</a>, 
<a href="/search/stat?searchtype=author&query=Williams%2C+A+H">Alex H. Williams</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Measuring geometric similarity between high-dimensional network
representations is a topic of longstanding interest to neuroscience and deep
learning. Although many methods have been proposed, only a few works have
rigorously analyzed their statistical efficiency or quantified estimator
uncertainty in data-limited regimes. Here, we derive upper and lower bounds on
the worst-case convergence of standard estimators of shape
distance$\unicode{x2014}$a measure of representational dissimilarity proposed
by Williams et al. (2021). These bounds reveal the challenging nature of the
problem in high-dimensional feature spaces. To overcome these challenges, we
introduce a new method-of-moments estimator with a tunable bias-variance
tradeoff. We show that this estimator achieves superior performance to standard
estimators in simulation and on neural data, particularly in high-dimensional
settings. Thus, we lay the foundation for a rigorous statistical theory for
high-dimensional shape analysis, and we contribute a new estimation method that
is well-suited to practical scientific settings.
</p>
</div>
</dd>
<dt><a name="item849">[849]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05787" title="Abstract">arXiv:2310.05787</a> (cross-list from math.PR) [<a href="/pdf/2310.05787" title="Download PDF">pdf</a>, <a href="/ps/2310.05787" title="Download PostScript">ps</a>, <a href="/format/2310.05787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact threshold for approximate ellipsoid fitting of random points
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Maillard%2C+A">Antoine Maillard</a>, 
<a href="/search/math?searchtype=author&query=Bandeira%2C+A+S">Afonso S. Bandeira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 45 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Data Structures and Algorithms (cs.DS); Statistics Theory (math.ST)

</div>
<p class="mathjax">We consider the problem $(\rm P)$ of exactly fitting an ellipsoid (centered
at $0$) to $n$ standard Gaussian random vectors in $\mathbb{R}^d$, as $n, d \to
\infty$ with $n / d^2 \to \alpha &gt; 0$. This problem is conjectured to undergo a
sharp transition: with high probability, $(\rm P)$ has a solution if $\alpha &lt;
1/4$, while $(\rm P)$ has no solutions if $\alpha &gt; 1/4$. So far, only a
trivial bound $\alpha &gt; 1/2$ is known to imply the absence of solutions, while
the sharpest results on the positive side assume $\alpha \leq \eta$ (for $\eta
&gt; 0$ a small constant) to prove that $(\rm P)$ is solvable. In this work we
study universality between this problem and a so-called "Gaussian equivalent",
for which the same transition can be rigorously analyzed. Our main results are
twofold. On the positive side, we prove that if $\alpha &lt; 1/4$, there exist an
ellipsoid fitting all the points up to a small error, and that the lengths of
its principal axes are bounded above and below. On the other hand, for $\alpha
&gt; 1/4$, we show that achieving small fitting error is not possible if the
length of the ellipsoid's shortest axis does not approach $0$ as $d \to \infty$
(and in particular there does not exist any ellipsoid fit whose shortest axis
length is bounded away from $0$ as $d \to \infty$). To the best of our
knowledge, our work is the first rigorous result characterizing the expected
phase transition in ellipsoid fitting at $\alpha = 1/4$. In a companion
non-rigorous work, the first author and D. Kunisky give a general analysis of
ellipsoid fitting using the replica method of statistical physics, which
inspired the present work.
</p>
</div>
</dd>
<dt><a name="item850">[850]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05799" title="Abstract">arXiv:2310.05799</a> (cross-list from eess.AS) [<a href="/pdf/2310.05799" title="Download PDF">pdf</a>, <a href="/format/2310.05799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The First Cadenza Signal Processing Challenge: Improving Music for Those  With a Hearing Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dabike%2C+G+R">Gerardo Roa Dabike</a>, 
<a href="/search/eess?searchtype=author&query=Bannister%2C+S">Scott Bannister</a>, 
<a href="/search/eess?searchtype=author&query=Firth%2C+J">Jennifer Firth</a>, 
<a href="/search/eess?searchtype=author&query=Graetzer%2C+S">Simone Graetzer</a>, 
<a href="/search/eess?searchtype=author&query=Vos%2C+R">Rebecca Vos</a>, 
<a href="/search/eess?searchtype=author&query=Akeroyd%2C+M+A">Michael A. Akeroyd</a>, 
<a href="/search/eess?searchtype=author&query=Barker%2C+J">Jon Barker</a>, 
<a href="/search/eess?searchtype=author&query=Cox%2C+T+J">Trevor J. Cox</a>, 
<a href="/search/eess?searchtype=author&query=Fazenda%2C+B">Bruno Fazenda</a>, 
<a href="/search/eess?searchtype=author&query=Greasley%2C+A">Alinka Greasley</a>, 
<a href="/search/eess?searchtype=author&query=Whitmer%2C+W">William Whitmer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">The Cadenza project aims to improve the audio quality of music for those who
have a hearing loss. This is being done through a series of signal processing
challenges, to foster better and more inclusive technologies. In the first
round, two common listening scenarios are considered: listening to music over
headphones, and with a hearing aid in a car. The first scenario is cast as a
demixing-remixing problem, where the music is decomposed into vocals, bass,
drums and other components. These can then be intelligently remixed in a
personalized way, to increase the audio quality for a person who has a hearing
loss. In the second scenario, music is coming from car loudspeakers, and the
music has to be enhanced to overcome the masking effect of the car noise. This
is done by taking into account the music, the hearing ability of the listener,
the hearing aid and the speed of the car. The audio quality of the submissions
will be evaluated using the Hearing Aid Audio Quality Index (HAAQI) for
objective assessment and by a panel of people with hearing loss for subjective
evaluation.
</p>
</div>
</dd>
<dt><a name="item851">[851]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05805" title="Abstract">arXiv:2310.05805</a> (cross-list from stat.ML) [<a href="/pdf/2310.05805" title="Download PDF">pdf</a>, <a href="/format/2310.05805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosted Control Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Gnecco%2C+N">Nicola Gnecco</a>, 
<a href="/search/stat?searchtype=author&query=Peters%2C+J">Jonas Peters</a>, 
<a href="/search/stat?searchtype=author&query=Engelke%2C+S">Sebastian Engelke</a>, 
<a href="/search/stat?searchtype=author&query=Pfister%2C+N">Niklas Pfister</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Modern machine learning methods and the availability of large-scale data
opened the door to accurately predict target quantities from large sets of
covariates. However, existing prediction methods can perform poorly when the
training and testing data are different, especially in the presence of hidden
confounding. While hidden confounding is well studied for causal effect
estimation (e.g., instrumental variables), this is not the case for prediction
tasks. This work aims to bridge this gap by addressing predictions under
different training and testing distributions in the presence of unobserved
confounding. In particular, we establish a novel connection between the field
of distribution generalization from machine learning, and simultaneous equation
models and control function from econometrics. Central to our contribution are
simultaneous equation models for distribution generalization (SIMDGs) which
describe the data-generating process under a set of distributional shifts.
Within this framework, we propose a strong notion of invariance for a
predictive model and compare it with existing (weaker) versions. Building on
the control function approach from instrumental variable regression, we propose
the boosted control function (BCF) as a target of inference and prove its
ability to successfully predict even in intervened versions of the underlying
SIMDG. We provide necessary and sufficient conditions for identifying the BCF
and show that it is worst-case optimal. We introduce the ControlTwicing
algorithm to estimate the BCF and analyze its predictive performance on
simulated and real world data.
</p>
</div>
</dd>
<dt><a name="item852">[852]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05816" title="Abstract">arXiv:2310.05816</a> (cross-list from math-ph) [<a href="/pdf/2310.05816" title="Download PDF">pdf</a>, <a href="/format/2310.05816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving the Transmission Problem for Open Wave-Guides, II Outgoing  Estimates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math-ph?searchtype=author&query=Epstein%2C+C+L">Charles L. Epstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article contains a completely revised portion of the original version of <a href="/abs/2302.04353">arXiv:2302.04353</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Physics (math-ph)</span>; Analysis of PDEs (math.AP); Numerical Analysis (math.NA)

</div>
<p class="mathjax">The paper continues the analysis, started in [1] (Part I,<a href="/abs/2302.04353">arXiv:2302.04353</a>),
of the model open wave-guide problem defined by 2 semi-infinite, rectangular
wave-guides meeting along a common perpendicular line. In Part I we reduce the
solution of the physical problem to a transmission problem rephrased as a
system of integral equations on the common perpendicular line. In this part we
show that solutions of the integral equations introduced in Part I have
asymptotic expansions, if the data allows it. Using these expansions we show
that the solutions to the PDE found in each half space satisfy appropriate
outgoing radiation conditions. In Part III we show that these conditions imply
uniqueness of the solution to the PDE as well as uniqueness for our system of
integral equations.
</p>
</div>
</dd>
<dt><a name="item853">[853]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05863" title="Abstract">arXiv:2310.05863</a> (cross-list from eess.AS) [<a href="/pdf/2310.05863" title="Download PDF">pdf</a>, <a href="/format/2310.05863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-grained Audio-Visual Joint Representations for Multimodal Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sun%2C+G">Guangzhi Sun</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+W">Wenyi Yu</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+C">Changli Tang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+X">Xianzhao Chen</a>, 
<a href="/search/eess?searchtype=author&query=Tan%2C+T">Tian Tan</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+L">Lu Lu</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+Z">Zejun Ma</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Sound (cs.SD)

</div>
<p class="mathjax">Audio-visual large language models (LLM) have drawn significant attention,
yet the fine-grained combination of both input streams is rather
under-explored, which is challenging but necessary for LLMs to understand
general video inputs. To this end, a fine-grained audio-visual joint
representation (FAVOR) learning framework for multimodal LLMs is proposed in
this paper, which extends a text-based LLM to simultaneously perceive speech
and audio events in the audio input stream and images or videos in the visual
input stream, at the frame level. To fuse the audio and visual feature streams
into joint representations and to align the joint space with the LLM input
embedding space, we propose a causal Q-Former structure with a causal attention
module to enhance the capture of causal relations of the audio-visual frames
across time. An audio-visual evaluation benchmark (AVEB) is also proposed which
comprises six representative single-modal tasks with five cross-modal tasks
reflecting audio-visual co-reasoning abilities. While achieving competitive
single-modal performance on audio, speech and image tasks in AVEB, FAVOR
achieved over 20% accuracy improvements on the video question-answering task
when fine-grained information or temporal causal reasoning is required. FAVOR,
in addition, demonstrated remarkable video comprehension and reasoning
abilities on tasks that are unprecedented by other multimodal LLMs. An
interactive demo of FAVOR is available at
https://github.com/the-anonymous-bs/FAVOR.git, and the training code and model
checkpoints will be released upon acceptance.
</p>
</div>
</dd>
<dt><a name="item854">[854]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05866" title="Abstract">arXiv:2310.05866</a> (cross-list from quant-ph) [<a href="/pdf/2310.05866" title="Download PDF">pdf</a>, <a href="/format/2310.05866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative quantum machine learning via denoising diffusion  probabilistic models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Zhang%2C+B">Bingzhi Zhang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Xu%2C+P">Peng Xu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+X">Xiaohui Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhuang%2C+Q">Quntao Zhuang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7+6 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep generative models are key-enabling technology to computer vision, text
generation and large language models. Denoising diffusion probabilistic models
(DDPMs) have recently gained much attention due to their ability to generate
diverse and high-quality samples in many computer vision tasks, as well as to
incorporate flexible model architectures and relatively simple training scheme.
Quantum generative models, empowered by entanglement and superposition, have
brought new insight to learning classical and quantum data. Inspired by the
classical counterpart, we propose the quantum denoising diffusion probabilistic
models (QuDDPM) to enable efficiently trainable generative learning of quantum
data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity,
while introduces multiple intermediate training tasks as interpolation between
the target distribution and noise to avoid barren plateau and guarantee
efficient training. We demonstrate QuDDPM's capability in learning correlated
quantum noise model and learning topological structure of nontrivial
distribution of quantum data.
</p>
</div>
</dd>
<dt><a name="item855">[855]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05879" title="Abstract">arXiv:2310.05879</a> (cross-list from physics.comp-ph) [<a href="/pdf/2310.05879" title="Download PDF">pdf</a>, <a href="/format/2310.05879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coarse-Graining Hamiltonian Systems Using WSINDy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Messenger%2C+D+A">Daniel A. Messenger</a>, 
<a href="/search/physics?searchtype=author&query=Burby%2C+J+W">Joshua W. Burby</a>, 
<a href="/search/physics?searchtype=author&query=Bortz%2C+D+M">David M. Bortz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Artificial Intelligence (cs.AI); Dynamical Systems (math.DS); Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
<p class="mathjax">The Weak-form Sparse Identification of Nonlinear Dynamics algorithm (WSINDy)
has been demonstrated to offer coarse-graining capabilities in the context of
interacting particle systems ( https://doi.org/10.1016/j.physd.<a href="/abs/2022.13340">2022.13340</a>6 ).
In this work we extend this capability to the problem of coarse-graining
Hamiltonian dynamics which possess approximate symmetries. Such approximate
symmetries often lead to the existence of a Hamiltonian system of reduced
dimension that may be used to efficiently capture the dynamics of the relevant
degrees of freedom. Deriving such reduced systems, or approximating them
numerically, is an ongoing challenge. We demonstrate that WSINDy can
successfully identify this reduced Hamiltonian system in the presence of large
perturbations imparted from both the inexact nature of the symmetry and
extrinsic noise. This is significant in part due to the nontrivial means by
which such systems are derived analytically. WSINDy naturally preserves the
Hamiltonian structure by restricting to a trial basis of Hamiltonian vector
fields, and the methodology is computational efficient, often requiring only a
single trajectory to learn the full reduced Hamiltonian, and avoiding forward
solves in the learning process. In this way, we argue that weak-form equation
learning is particularly well-suited for Hamiltonian coarse-graining. Using
nearly-periodic Hamiltonian systems as a prototypical class of systems with
approximate symmetries, we show that WSINDy robustly identifies the correct
leading-order reduced system of dimension $2(N-1)$ or $N$ from the original
$(2N)$-dimensional system, upon observation of the relevant degrees of freedom.
We provide physically relevant examples, namely coupled oscillator dynamics,
the H\'enon-Heiles system for stellar motion within a galaxy, and the dynamics
of charged particles.
</p>
</div>
</dd>
<dt><a name="item856">[856]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05892" title="Abstract">arXiv:2310.05892</a> (cross-list from stat.ML) [<a href="/pdf/2310.05892" title="Download PDF">pdf</a>, <a href="/ps/2310.05892" title="Download PostScript">ps</a>, <a href="/format/2310.05892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Generalization Bound of Deep Neural Networks for Dependent Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Do%2C+Q+H">Quan Huu Do</a>, 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+B+T">Binh T. Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Ho%2C+L+S+T">Lam Si Tung Ho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Existing generalization bounds for deep neural networks require data to be
independent and identically distributed (iid). This assumption may not hold in
real-life applications such as evolutionary biology, infectious disease
epidemiology, and stock price prediction. This work establishes a
generalization bound of feed-forward neural networks for non-stationary
$\phi$-mixing data.
</p>
</div>
</dd>
<dt><a name="item857">[857]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05900" title="Abstract">arXiv:2310.05900</a> (cross-list from quant-ph) [<a href="/pdf/2310.05900" title="Download PDF">pdf</a>, <a href="/format/2310.05900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Decode the Surface Code with a Recurrent, Transformer-Based  Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Bausch%2C+J">Johannes Bausch</a>, 
<a href="/search/quant-ph?searchtype=author&query=Senior%2C+A+W">Andrew W Senior</a>, 
<a href="/search/quant-ph?searchtype=author&query=Heras%2C+F+J+H">Francisco J H Heras</a>, 
<a href="/search/quant-ph?searchtype=author&query=Edlich%2C+T">Thomas Edlich</a>, 
<a href="/search/quant-ph?searchtype=author&query=Davies%2C+A">Alex Davies</a>, 
<a href="/search/quant-ph?searchtype=author&query=Newman%2C+M">Michael Newman</a>, 
<a href="/search/quant-ph?searchtype=author&query=Jones%2C+C">Cody Jones</a>, 
<a href="/search/quant-ph?searchtype=author&query=Satzinger%2C+K">Kevin Satzinger</a>, 
<a href="/search/quant-ph?searchtype=author&query=Niu%2C+M+Y">Murphy Yuezhen Niu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Blackwell%2C+S">Sam Blackwell</a>, 
<a href="/search/quant-ph?searchtype=author&query=Holland%2C+G">George Holland</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kafri%2C+D">Dvir Kafri</a>, 
<a href="/search/quant-ph?searchtype=author&query=Atalaya%2C+J">Juan Atalaya</a>, 
<a href="/search/quant-ph?searchtype=author&query=Gidney%2C+C">Craig Gidney</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hassabis%2C+D">Demis Hassabis</a>, 
<a href="/search/quant-ph?searchtype=author&query=Boixo%2C+S">Sergio Boixo</a>, 
<a href="/search/quant-ph?searchtype=author&query=Neven%2C+H">Hartmut Neven</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kohli%2C+P">Pushmeet Kohli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Quantum error-correction is a prerequisite for reliable quantum computation.
Towards this goal, we present a recurrent, transformer-based neural network
which learns to decode the surface code, the leading quantum error-correction
code. Our decoder outperforms state-of-the-art algorithmic decoders on
real-world data from Google's Sycamore quantum processor for distance 3 and 5
surface codes. On distances up to 11, the decoder maintains its advantage on
simulated data with realistic noise including cross-talk, leakage, and analog
readout signals, and sustains its accuracy far beyond the 25 cycles it was
trained on. Our work illustrates the ability of machine learning to go beyond
human-designed algorithms by learning from data directly, highlighting machine
learning as a strong contender for decoding in quantum computers.
</p>
</div>
</dd>
<dt><a name="item858">[858]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05921" title="Abstract">arXiv:2310.05921</a> (cross-list from stat.ML) [<a href="/pdf/2310.05921" title="Download PDF">pdf</a>, <a href="/format/2310.05921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformal Decision Theory: Safe Autonomous Decisions from Imperfect  Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lekeufack%2C+J">Jordan Lekeufack</a>, 
<a href="/search/stat?searchtype=author&query=Angelopoulos%2C+A+A">Anastasios A. Angelopoulos</a>, 
<a href="/search/stat?searchtype=author&query=Bajcsy%2C+A">Andrea Bajcsy</a>, 
<a href="/search/stat?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>, 
<a href="/search/stat?searchtype=author&query=Malik%2C+J">Jitendra Malik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Robotics (cs.RO); Methodology (stat.ME)

</div>
<p class="mathjax">We introduce Conformal Decision Theory, a framework for producing safe
autonomous decisions despite imperfect machine learning predictions. Examples
of such decisions are ubiquitous, from robot planning algorithms that rely on
pedestrian predictions, to calibrating autonomous manufacturing to exhibit high
throughput and low error, to the choice of trusting a nominal policy versus
switching to a safe backup policy at run-time. The decisions produced by our
algorithms are safe in the sense that they come with provable statistical
guarantees of having low risk without any assumptions on the world model
whatsoever; the observations need not be I.I.D. and can even be adversarial.
The theory extends results from conformal prediction to calibrate decisions
directly, without requiring the construction of prediction sets. Experiments
demonstrate the utility of our approach in robot motion planning around humans,
automated stock trading, and robot manufacturin
</p>
</div>
</dd>
</dl>
<h3>Replacements for Tue, 10 Oct 23</h3>
<dl>
<dt><a name="item859">[859]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1709.08774" title="Abstract">arXiv:1709.08774</a> (replaced) [<a href="/pdf/1709.08774" title="Download PDF">pdf</a>, <a href="/format/1709.08774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Design Space of Immersive Urban Analytics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu-Tian%2C+C">Chen Zhu-Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tianchen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zhigeng Pan</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Huamin Qu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yingcai Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages,11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item860">[860]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1902.05605" title="Abstract">arXiv:1902.05605</a> (replaced) [<a href="/pdf/1902.05605" title="Download PDF">pdf</a>, <a href="/format/1902.05605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater  Sample Efficiency and Simplicity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhatt%2C+A">Aditya Bhatt</a>, 
<a href="/search/cs?searchtype=author&query=Palenicek%2C+D">Daniel Palenicek</a>, 
<a href="/search/cs?searchtype=author&query=Belousov%2C+B">Boris Belousov</a>, 
<a href="/search/cs?searchtype=author&query=Argus%2C+M">Max Argus</a>, 
<a href="/search/cs?searchtype=author&query=Amiranashvili%2C+A">Artemij Amiranashvili</a>, 
<a href="/search/cs?searchtype=author&query=Brox%2C+T">Thomas Brox</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+J">Jan Peters</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item861">[861]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1903.10559" title="Abstract">arXiv:1903.10559</a> (replaced) [<a href="/pdf/1903.10559" title="Download PDF">pdf</a>, <a href="/format/1903.10559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Mode of Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pineda%2C+L+A">Luis A. Pineda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item862">[862]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1907.13538" title="Abstract">arXiv:1907.13538</a> (replaced) [<a href="/pdf/1907.13538" title="Download PDF">pdf</a>, <a href="/format/1907.13538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LassoNet: Deep Lasso-Selection of 3D Point Clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu-Tian%2C+C">Chen Zhu-Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+W">Wei Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhiguang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lingyun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Chi-Wing Fu</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Huamin Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> TVCG2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item863">[863]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1907.13550" title="Abstract">arXiv:1907.13550</a> (replaced) [<a href="/pdf/1907.13550" title="Download PDF">pdf</a>, <a href="/format/1907.13550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Automated Infographic Design: Deep Learning-based  Auto-Extraction of Extensible Timeline
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu-Tian%2C+C">Chen Zhu-Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianwen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Huamin Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, Automated Infographic Design, Deep Learning-based Approach, Timeline Infographics, Multi-task Model
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> TVCG2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item864">[864]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1909.07589" title="Abstract">arXiv:1909.07589</a> (replaced) [<a href="/pdf/1909.07589" title="Download PDF">pdf</a>, <a href="/format/1909.07589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Linear Exponential Comonad in s-finite Transition Kernels and  Probabilistic Coherent Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hamano%2C+M">Masahiro Hamano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Category Theory (math.CT)

</div>
</div>
</dd>
<dt><a name="item865">[865]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2001.00185" title="Abstract">arXiv:2001.00185</a> (replaced) [<a href="/pdf/2001.00185" title="Download PDF">pdf</a>, <a href="/format/2001.00185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New upper bounds for spherical codes and packings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sardari%2C+N+T">Naser T. Sardari</a>, 
<a href="/search/math?searchtype=author&query=Zargar%2C+M">Masoud Zargar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Math Annalen. Exposition improved, pictures added. Results unchanged
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Metric Geometry (math.MG)</span>; Information Theory (cs.IT); Number Theory (math.NT)

</div>
</div>
</dd>
<dt><a name="item866">[866]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2002.03729" title="Abstract">arXiv:2002.03729</a> (replaced) [<a href="/pdf/2002.03729" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A lightweight target detection algorithm based on Mobilenet Convolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shengquan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item867">[867]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2004.09640" title="Abstract">arXiv:2004.09640</a> (replaced) [<a href="/pdf/2004.09640" title="Download PDF">pdf</a>, <a href="/format/2004.09640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mechanism Design for Online Resource Allocation: A Unified Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xiaoqi Tan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Bo Sun</a>, 
<a href="/search/cs?searchtype=author&query=Leon-Garcia%2C+A">Alberto Leon-Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tsang%2C+D+H+K">Danny H.K. Tsang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item868">[868]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.01106" title="Abstract">arXiv:2006.01106</a> (replaced) [<a href="/pdf/2006.01106" title="Download PDF">pdf</a>, <a href="/format/2006.01106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exit Time Analysis for Approximations of Gradient Descent Trajectories  Around Saddle Points
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dixit%2C+R">Rishabh Dixit</a>, 
<a href="/search/math?searchtype=author&query=Gurbuzbalaban%2C+M">Mert Gurbuzbalaban</a>, 
<a href="/search/math?searchtype=author&query=Bajwa%2C+W+U">Waheed U. Bajwa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 70 pages; pre-print of the journal paper published in Information and Inference: A Journal of the IMA, 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Information and Inference: A Journal of the IMA, vol. 12, no. 2,
  pp. 714-786, Jun. 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item869">[869]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.09565" title="Abstract">arXiv:2006.09565</a> (replaced) [<a href="/pdf/2006.09565" title="Download PDF">pdf</a>, <a href="/format/2006.09565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mining Label Distribution Drift in Unsupervised Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peizhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zhengming Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongfu Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AJCAI'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item870">[870]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.13844" title="Abstract">arXiv:2006.13844</a> (replaced) [<a href="/e-print/2006.13844" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpolatory Projection Techniques for $\mathcal{H}_2$ Optimal  Structure-Preserving Model Order Reduction of Second-Order Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rahman%2C+M+M">Md. Motlubar Rahman</a>, 
<a href="/search/math?searchtype=author&query=Uddin%2C+M+M">M. Monir Uddin</a>, 
<a href="/search/math?searchtype=author&query=Andallah%2C+L+S">L. S. Andallah</a>, 
<a href="/search/math?searchtype=author&query=Uddin%2C+M">Mahtab Uddin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> It was a duplicate version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY); Metric Geometry (math.MG); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item871">[871]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2008.01302" title="Abstract">arXiv:2008.01302</a> (replaced) [<a href="/pdf/2008.01302" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Analysis of Deep Reinforcement Learning-enabled Freeway  Decision-making for Automated Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Teng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuyou Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+W">Wenxuan Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaolin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+M">Mingzhu Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item872">[872]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.03190" title="Abstract">arXiv:2011.03190</a> (replaced) [<a href="/pdf/2011.03190" title="Download PDF">pdf</a>, <a href="/format/2011.03190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReFloat: Low-Cost Floating-Point Processing in ReRAM for Accelerating  Iterative Linear Solvers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linghao Song</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Fan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+X">Xuehai Qian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hai Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiran Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item873">[873]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.07669" title="Abstract">arXiv:2011.07669</a> (replaced) [<a href="/pdf/2011.07669" title="Download PDF">pdf</a>, <a href="/ps/2011.07669" title="Download PostScript">ps</a>, <a href="/format/2011.07669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An exact $\sin&#x398;$ formula for matrix perturbation analysis and its  applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lyu%2C+H">He Lyu</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+R">Rongrong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item874">[874]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.14849" title="Abstract">arXiv:2011.14849</a> (replaced) [<a href="/pdf/2011.14849" title="Download PDF">pdf</a>, <a href="/ps/2011.14849" title="Download PostScript">ps</a>, <a href="/format/2011.14849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameterized algorithms for locating-dominating sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cappelle%2C+M+R">M&#xe1;rcia R. Cappelle</a>, 
<a href="/search/cs?searchtype=author&query=Gomes%2C+G+C+M">Guilherme C. M. Gomes</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+V+F+d">Vinicius F. dos Santos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 18 figures. Previous version appeared in the proceedings of LAGOS2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item875">[875]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.13633" title="Abstract">arXiv:2012.13633</a> (replaced) [<a href="/pdf/2012.13633" title="Download PDF">pdf</a>, <a href="/format/2012.13633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Road Obstacles by Erasing Them
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lis%2C+K">Krzysztof Lis</a>, 
<a href="/search/cs?searchtype=author&query=Honari%2C+S">Sina Honari</a>, 
<a href="/search/cs?searchtype=author&query=Fua%2C+P">Pascal Fua</a>, 
<a href="/search/cs?searchtype=author&query=Salzmann%2C+M">Mathieu Salzmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item876">[876]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.09446" title="Abstract">arXiv:2101.09446</a> (replaced) [<a href="/pdf/2101.09446" title="Download PDF">pdf</a>, <a href="/format/2101.09446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlabeled Principal Component Analysis and Matrix Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yunzhen Yao</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+L">Liangzu Peng</a>, 
<a href="/search/cs?searchtype=author&query=Tsakiris%2C+M+C">Manolis C. Tsakiris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item877">[877]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.04844" title="Abstract">arXiv:2104.04844</a> (replaced) [<a href="/pdf/2104.04844" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On migration to Perpetual Enterprise System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benitez%2C+M+T+C">Manuel Tomas Carrasco Benitez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item878">[878]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.05087" title="Abstract">arXiv:2104.05087</a> (replaced) [<a href="/pdf/2104.05087" title="Download PDF">pdf</a>, <a href="/format/2104.05087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from Censored and Dependent Data: The case of Linear Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Plevrakis%2C+O">Orestis Plevrakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item879">[879]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.14103" title="Abstract">arXiv:2104.14103</a> (replaced) [<a href="/pdf/2104.14103" title="Download PDF">pdf</a>, <a href="/format/2104.14103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoCone: An OmniDirectional Robot for Lane-Level Cone Placement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hartzer%2C+J">Jacob Hartzer</a>, 
<a href="/search/cs?searchtype=author&query=Saripalli%2C+S">Srikanth Saripalli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item880">[880]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.14106" title="Abstract">arXiv:2104.14106</a> (replaced) [<a href="/pdf/2104.14106" title="Download PDF">pdf</a>, <a href="/format/2104.14106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vehicular Teamwork: Collaborative localization of Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hartzer%2C+J">Jacob Hartzer</a>, 
<a href="/search/cs?searchtype=author&query=Saripalli%2C+S">Srikanth Saripalli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item881">[881]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.01512" title="Abstract">arXiv:2105.01512</a> (replaced) [<a href="/pdf/2105.01512" title="Download PDF">pdf</a>, <a href="/format/2105.01512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulation by Rounds of Letter-to-Letter Transducers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nassar%2C+A+A">Antonio Abu Nassar</a>, 
<a href="/search/cs?searchtype=author&query=Almagor%2C+S">Shaull Almagor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item882">[882]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.02605" title="Abstract">arXiv:2105.02605</a> (replaced) [<a href="/pdf/2105.02605" title="Download PDF">pdf</a>, <a href="/format/2105.02605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphFormers: GNN-nested Transformers for Representation Learning on  Textual Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Junhan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shitao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chaozhuo Li</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+D">Defu Lian</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+S">Sanjay Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Amit Singh</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Guangzhong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item883">[883]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.07329" title="Abstract">arXiv:2105.07329</a> (replaced) [<a href="/pdf/2105.07329" title="Download PDF">pdf</a>, <a href="/format/2105.07329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Spatial Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kanoria%2C+Y">Yash Kanoria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preliminary version appeared in ACM EC 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Data Structures and Algorithms (cs.DS); Theoretical Economics (econ.TH); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item884">[884]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.01810" title="Abstract">arXiv:2106.01810</a> (replaced) [<a href="/pdf/2106.01810" title="Download PDF">pdf</a>, <a href="/ps/2106.01810" title="Download PostScript">ps</a>, <a href="/format/2106.01810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defending Against Backdoor Attacks in Natural Language Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaofei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoya Li</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Y">Yuxian Meng</a>, 
<a href="/search/cs?searchtype=author&query=Ao%2C+X">Xiang Ao</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+L">Lingjuan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianwei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at AAAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item885">[885]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02329" title="Abstract">arXiv:2106.02329</a> (replaced) [<a href="/pdf/2106.02329" title="Download PDF">pdf</a>, <a href="/format/2106.02329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Switching State Space Model (DS$^3$M) for Nonlinear Time Series  Forecasting with Regime Switching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiuqin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hanqiu Peng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Ying Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item886">[886]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02933" title="Abstract">arXiv:2106.02933</a> (replaced) [<a href="/pdf/2106.02933" title="Download PDF">pdf</a>, <a href="/format/2106.02933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> k-Mixup Regularization for Deep Learning via Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Greenewald%2C+K">Kristjan Greenewald</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+A">Anming Gu</a>, 
<a href="/search/cs?searchtype=author&query=Yurochkin%2C+M">Mikhail Yurochkin</a>, 
<a href="/search/cs?searchtype=author&query=Solomon%2C+J">Justin Solomon</a>, 
<a href="/search/cs?searchtype=author&query=Chien%2C+E">Edward Chien</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item887">[887]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06682" title="Abstract">arXiv:2106.06682</a> (replaced) [<a href="/pdf/2106.06682" title="Download PDF">pdf</a>, <a href="/format/2106.06682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving PDEs on Unknown Manifolds with Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liang%2C+S">Senwei Liang</a>, 
<a href="/search/math?searchtype=author&query=Jiang%2C+S+W">Shixiao W. Jiang</a>, 
<a href="/search/math?searchtype=author&query=Harlim%2C+J">John Harlim</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+H">Haizhao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item888">[888]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.02058" title="Abstract">arXiv:2107.02058</a> (replaced) [<a href="/pdf/2107.02058" title="Download PDF">pdf</a>, <a href="/format/2107.02058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Guarantees for Multi-unit Prophet Inequalities and Online  Stochastic Knapsack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiashuo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Will Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiawei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the full version of the SODA 2022 paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item889">[889]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.02660" title="Abstract">arXiv:2107.02660</a> (replaced) [<a href="/pdf/2107.02660" title="Download PDF">pdf</a>, <a href="/ps/2107.02660" title="Download PostScript">ps</a>, <a href="/format/2107.02660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HybrUR: A Hybrid Physical-Neural Solution for Unsupervised Underwater  Image Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shuaizheng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xingyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhengxing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+M">Min Tan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Junzhi Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Image Processing, vol. 32, pp. 5004-5016,
  2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item890">[890]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.12809" title="Abstract">arXiv:2107.12809</a> (replaced) [<a href="/pdf/2107.12809" title="Download PDF">pdf</a>, <a href="/format/2107.12809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Optimisation for Sequential Experimental Design with  Applications in Additive Manufacturing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mimi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Parnell%2C+A">Andrew Parnell</a>, 
<a href="/search/cs?searchtype=author&query=Brabazon%2C+D">Dermot Brabazon</a>, 
<a href="/search/cs?searchtype=author&query=Benavoli%2C+A">Alessio Benavoli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item891">[891]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.14151" title="Abstract">arXiv:2107.14151</a> (replaced) [<a href="/pdf/2107.14151" title="Download PDF">pdf</a>, <a href="/format/2107.14151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modern Non-Linear Function-on-Function Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Rao%2C+A+R">Aniruddha Rajendra Rao</a>, 
<a href="/search/stat?searchtype=author&query=Reimherr%2C+M">Matthew Reimherr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 figures, 6 tables (including supplementary material), 16 pages (including supplementary material). arXiv admin note: text overlap with <a href="/abs/2104.09371">arXiv:2104.09371</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Statistics and Computing 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item892">[892]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.04684" title="Abstract">arXiv:2109.04684</a> (replaced) [<a href="/pdf/2109.04684" title="Download PDF">pdf</a>, <a href="/format/2109.04684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Unsupervised Anomaly Detection with Score-Guided Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zongyuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Baohua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+G">Guoqiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Longyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yanyan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yaohui Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Final version in TNNLS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item893">[893]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.04149" title="Abstract">arXiv:2110.04149</a> (replaced) [<a href="/pdf/2110.04149" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parasocial diffusion: K-pop fandoms help drive COVID-19 public health  messaging on social media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+H+H">Ho-Chun Herbert Chang</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+B">Becky Pham</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Online Social Networks and Media, 37, 100267 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item894">[894]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.06363" title="Abstract">arXiv:2110.06363</a> (replaced) [<a href="/pdf/2110.06363" title="Download PDF">pdf</a>, <a href="/format/2110.06363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Side-channel Analysis of Sensor Multiplexing for Covert Channels and  Application Profiling on Mobile Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shepherd%2C+C">Carlton Shepherd</a>, 
<a href="/search/cs?searchtype=author&query=Kalbantner%2C+J">Jan Kalbantner</a>, 
<a href="/search/cs?searchtype=author&query=Semal%2C+B">Benjamin Semal</a>, 
<a href="/search/cs?searchtype=author&query=Markantonakis%2C+K">Konstantinos Markantonakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item895">[895]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.08196" title="Abstract">arXiv:2110.08196</a> (replaced) [<a href="/pdf/2110.08196" title="Download PDF">pdf</a>, <a href="/format/2110.08196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Pebble-Relation Comonad in Finite Model Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Montacute%2C+Y">Yo&#xe0;v Montacute</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Nihil Shah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appears in Logic in Computer Science (LICS) 2022 Proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item896">[896]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.07167" title="Abstract">arXiv:2112.07167</a> (replaced) [<a href="/pdf/2112.07167" title="Download PDF">pdf</a>, <a href="/ps/2112.07167" title="Download PostScript">ps</a>, <a href="/format/2112.07167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Moderate deviation expansion for fully quantum tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Ramakrishnan%2C+N">Navneeth Ramakrishnan</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tomamichel%2C+M">Marco Tomamichel</a>, 
<a href="/search/quant-ph?searchtype=author&query=Berta%2C+M">Mario Berta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Information Theory 69(8), 5041-5059 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item897">[897]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.00288" title="Abstract">arXiv:2201.00288</a> (replaced) [<a href="/pdf/2201.00288" title="Download PDF">pdf</a>, <a href="/format/2201.00288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Community Search: A Meta-Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+S">Shuheng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+K">Kangfei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanghua Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J+X">Jeffery Xu Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICDE 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item898">[898]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.09990" title="Abstract">arXiv:2201.09990</a> (replaced) [<a href="/pdf/2201.09990" title="Download PDF">pdf</a>, <a href="/format/2201.09990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structural Properties of Optimal Fidelity Selection Policies for  Human-in-the-loop Queues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+P">Piyush Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+V">Vaibhav Srivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Performance (cs.PF); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item899">[899]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.09500" title="Abstract">arXiv:2202.09500</a> (replaced) [<a href="/pdf/2202.09500" title="Download PDF">pdf</a>, <a href="/ps/2202.09500" title="Download PostScript">ps</a>, <a href="/format/2202.09500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact Instability Margin Analysis and Minimum-Norm Strong Stabilization  -- phase change rate maximization --
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hara%2C+S">Shinji Hara</a>, 
<a href="/search/eess?searchtype=author&query=Kao%2C+C">Chung-Yao Kao</a>, 
<a href="/search/eess?searchtype=author&query=Khong%2C+S+Z">Sei Zhen Khong</a>, 
<a href="/search/eess?searchtype=author&query=Iwasaki%2C+T">Tetsuya Iwasaki</a>, 
<a href="/search/eess?searchtype=author&query=Hori%2C+Y">Yutaka Hori</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item900">[900]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.11593" title="Abstract">arXiv:2202.11593</a> (replaced) [<a href="/pdf/2202.11593" title="Download PDF">pdf</a>, <a href="/format/2202.11593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding Safe Zones of policies Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cohen%2C+L">Lee Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Mansour%2C+Y">Yishay Mansour</a>, 
<a href="/search/cs?searchtype=author&query=Moshkovitz%2C+M">Michal Moshkovitz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item901">[901]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.02680" title="Abstract">arXiv:2203.02680</a> (replaced) [<a href="/e-print/2203.02680" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language vs Speaker Change: A Comparative Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mishra%2C+J">Jagabandhu Mishra</a>, 
<a href="/search/eess?searchtype=author&query=Prasanna%2C+S+R+M">S. R. Mahadeva Prasanna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The work is substantially modified. The new version of the same will be submitted soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item902">[902]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.03525" title="Abstract">arXiv:2203.03525</a> (replaced) [<a href="/pdf/2203.03525" title="Download PDF">pdf</a>, <a href="/ps/2203.03525" title="Download PostScript">ps</a>, <a href="/format/2203.03525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cryptanalysis of some Nonabelian Group-Based Key Exchange Protocols
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tinani%2C+S">Simran Tinani</a>, 
<a href="/search/cs?searchtype=author&query=Matteotti%2C+C">Carlo Matteotti</a>, 
<a href="/search/cs?searchtype=author&query=Rosenthal%2C+J">Joachim Rosenthal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item903">[903]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.10087" title="Abstract">arXiv:2203.10087</a> (replaced) [<a href="/pdf/2203.10087" title="Download PDF">pdf</a>, <a href="/format/2203.10087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> But that&#x27;s not why: Inference adjustment by interactive prototype  revision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gerstenberger%2C+M">Michael Gerstenberger</a>, 
<a href="/search/cs?searchtype=author&query=Lapuschkin%2C+S">Sebastian Lapuschkin</a>, 
<a href="/search/cs?searchtype=author&query=Eisert%2C+P">Peter Eisert</a>, 
<a href="/search/cs?searchtype=author&query=Bosse%2C+S">Sebastian Bosse</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item904">[904]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.01500" title="Abstract">arXiv:2204.01500</a> (replaced) [<a href="/pdf/2204.01500" title="Download PDF">pdf</a>, <a href="/format/2204.01500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Which Tricks Are Important for Learning to Rank?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyzhin%2C+I">Ivan Lyzhin</a>, 
<a href="/search/cs?searchtype=author&query=Ustimenko%2C+A">Aleksei Ustimenko</a>, 
<a href="/search/cs?searchtype=author&query=Gulin%2C+A">Andrey Gulin</a>, 
<a href="/search/cs?searchtype=author&query=Prokhorenkova%2C+L">Liudmila Prokhorenkova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item905">[905]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.04145" title="Abstract">arXiv:2204.04145</a> (replaced) [<a href="/pdf/2204.04145" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constrained Bundle Adjustment for Structure From Motion Using  Uncalibrated Multi-Camera Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Debao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Elhashash%2C+M">Mostafa Elhashash</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+R">Rongjun Qin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be published in ISPRS Congress 2022
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ISPRS Annals of Photogrammetry, Remote Sensing and Spatial
  Information Sciences 52 (2022): 17-22 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item906">[906]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.06863" title="Abstract">arXiv:2204.06863</a> (replaced) [<a href="/pdf/2204.06863" title="Download PDF">pdf</a>, <a href="/format/2204.06863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ULF: Unsupervised Labeling Function Correction using Cross-Validation  for Weak Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sedova%2C+A">Anastasiia Sedova</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+B">Benjamin Roth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item907">[907]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.01306" title="Abstract">arXiv:2205.01306</a> (replaced) [<a href="/pdf/2205.01306" title="Download PDF">pdf</a>, <a href="/format/2205.01306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CANShield: Deep Learning-Based Intrusion Detection Framework for  Controller Area Networks at the Signal-Level
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahriar%2C+M+H">Md Hasan Shahriar</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Moriano%2C+P">Pablo Moriano</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+W">Wenjing Lou</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y+T">Y. Thomas Hou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 13 figures, A version of this paper is accepted by IEEE Internet of Things Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item908">[908]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.01713" title="Abstract">arXiv:2205.01713</a> (replaced) [<a href="/pdf/2205.01713" title="Download PDF">pdf</a>, <a href="/format/2205.01713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Type Checking and Set Constraint Solving to Improve Automated  Software Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cristi%C3%A1%2C+M">Maximiliano Cristi&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Rossi%2C+G">Gianfranco Rossi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item909">[909]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.02273" title="Abstract">arXiv:2205.02273</a> (replaced) [<a href="/pdf/2205.02273" title="Download PDF">pdf</a>, <a href="/format/2205.02273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Adaptive Incremental Gradient Method With Support for Non-Euclidean  Norms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Xie%2C+B">Binghui Xie</a>, 
<a href="/search/math?searchtype=author&query=Jin%2C+C">Chenhan Jin</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+K">Kaiwen Zhou</a>, 
<a href="/search/math?searchtype=author&query=Cheng%2C+J">James Cheng</a>, 
<a href="/search/math?searchtype=author&query=Meng%2C+W">Wei Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item910">[910]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.05628" title="Abstract">arXiv:2205.05628</a> (replaced) [<a href="/pdf/2205.05628" title="Download PDF">pdf</a>, <a href="/format/2205.05628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extensible Machine Learning for Encrypted Network Traffic Application  Labeling via Uncertainty Quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jorgensen%2C+S">Steven Jorgensen</a>, 
<a href="/search/cs?searchtype=author&query=Holodnak%2C+J">John Holodnak</a>, 
<a href="/search/cs?searchtype=author&query=Dempsey%2C+J">Jensen Dempsey</a>, 
<a href="/search/cs?searchtype=author&query=de+Souza%2C+K">Karla de Souza</a>, 
<a href="/search/cs?searchtype=author&query=Raghunath%2C+A">Ananditha Raghunath</a>, 
<a href="/search/cs?searchtype=author&query=Rivet%2C+V">Vernon Rivet</a>, 
<a href="/search/cs?searchtype=author&query=DeMoes%2C+N">Noah DeMoes</a>, 
<a href="/search/cs?searchtype=author&query=Alejos%2C+A">Andr&#xe9;s Alejos</a>, 
<a href="/search/cs?searchtype=author&query=Wollaber%2C+A">Allan Wollaber</a> (MIT Lincoln Laboratory)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper is 15 pages and has 10 figures. Published in IEEE Transactions on Artificial Intelligence (<a href="https://doi.org/10.1109/TAI.2023.3244168">this https URL</a>). For associated dataset, see <a href="https://www.ll.mit.edu/r-d/datasets/vpnnonvpn-network-application-traffic-dataset-vnat">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item911">[911]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.08821" title="Abstract">arXiv:2205.08821</a> (replaced) [<a href="/pdf/2205.08821" title="Download PDF">pdf</a>, <a href="/format/2205.08821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lessons Learned: Defending Against Property Inference Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stock%2C+J">Joshua Stock</a> (1), 
<a href="/search/cs?searchtype=author&query=Wettlaufer%2C+J">Jens Wettlaufer</a>, 
<a href="/search/cs?searchtype=author&query=Demmler%2C+D">Daniel Demmler</a> (1), 
<a href="/search/cs?searchtype=author&query=Federrath%2C+H">Hannes Federrath</a> (1) ((1) Universit&#xe4;t Hamburg)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 20th International Conference on Security and
  Cryptography SECRYPT (2023) 312-323
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item912">[912]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.11521" title="Abstract">arXiv:2205.11521</a> (replaced) [<a href="/pdf/2205.11521" title="Download PDF">pdf</a>, <a href="/format/2205.11521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Hours to Seconds: Towards 100x Faster Quantitative Phase Imaging  via Differentiable Microscopy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Haputhanthri%2C+U">Udith Haputhanthri</a>, 
<a href="/search/eess?searchtype=author&query=Herath%2C+K">Kithmini Herath</a>, 
<a href="/search/eess?searchtype=author&query=Hettiarachchi%2C+R">Ramith Hettiarachchi</a>, 
<a href="/search/eess?searchtype=author&query=Kariyawasam%2C+H">Hasindu Kariyawasam</a>, 
<a href="/search/eess?searchtype=author&query=Ahmad%2C+A">Azeem Ahmad</a>, 
<a href="/search/eess?searchtype=author&query=Ahluwalia%2C+B+S">Balpreet S. Ahluwalia</a>, 
<a href="/search/eess?searchtype=author&query=Edussooriya%2C+C+U+S">Chamira U. S. Edussooriya</a>, 
<a href="/search/eess?searchtype=author&query=Wadduwage%2C+D+N">Dushan N. Wadduwage</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Computational Physics (physics.comp-ph); Optics (physics.optics)

</div>
</div>
</dd>
<dt><a name="item913">[913]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.11716" title="Abstract">arXiv:2205.11716</a> (replaced) [<a href="/pdf/2205.11716" title="Download PDF">pdf</a>, <a href="/ps/2205.11716" title="Download PostScript">ps</a>, <a href="/format/2205.11716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Randomly Initialized One-Layer Neural Networks Make Data Linearly  Separable
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosal%2C+P">Promit Ghosal</a>, 
<a href="/search/cs?searchtype=author&query=Mahankali%2C+S">Srinath Mahankali</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yihang Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Probability (math.PR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item914">[914]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.13643" title="Abstract">arXiv:2205.13643</a> (replaced) [<a href="/pdf/2205.13643" title="Download PDF">pdf</a>, <a href="/format/2205.13643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable solver for time-dependent deformation problems with  contact
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zizhou Huang</a>, 
<a href="/search/cs?searchtype=author&query=Tozoni%2C+D+C">Davi Colli Tozoni</a>, 
<a href="/search/cs?searchtype=author&query=Gjoka%2C+A">Arvi Gjoka</a>, 
<a href="/search/cs?searchtype=author&query=Ferguson%2C+Z">Zachary Ferguson</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+T">Teseo Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Panozzo%2C+D">Daniele Panozzo</a>, 
<a href="/search/cs?searchtype=author&query=Zorin%2C+D">Denis Zorin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
</div>
</dd>
<dt><a name="item915">[915]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.00311" title="Abstract">arXiv:2206.00311</a> (replaced) [<a href="/pdf/2206.00311" title="Download PDF">pdf</a>, <a href="/format/2206.00311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+P">Pengyuan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chengquan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shanshan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+M">Meina Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yangliu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Liang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+K">Kun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Junyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+E">Errui Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingdong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item916">[916]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.00383" title="Abstract">arXiv:2206.00383</a> (replaced) [<a href="/pdf/2206.00383" title="Download PDF">pdf</a>, <a href="/format/2206.00383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Improvement Heuristics for Graph Combinatorial Optimization  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garmendia%2C+A+I">Andoni I. Garmendia</a>, 
<a href="/search/cs?searchtype=author&query=Ceberio%2C+J">Josu Ceberio</a>, 
<a href="/search/cs?searchtype=author&query=Mendiburu%2C+A">Alexander Mendiburu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Discrete Mathematics (cs.DM); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item917">[917]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.04890" title="Abstract">arXiv:2206.04890</a> (replaced) [<a href="/pdf/2206.04890" title="Download PDF">pdf</a>, <a href="/format/2206.04890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Counterfactual Environment Model Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiong-Hui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zheng-Mao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhihua Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenjun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenghe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yinan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hongqiu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+R">Rong-Jun Qin</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+R">Ruijin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fangsheng Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item918">[918]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.10121" title="Abstract">arXiv:2206.10121</a> (replaced) [<a href="/pdf/2206.10121" title="Download PDF">pdf</a>, <a href="/format/2206.10121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite Expression Method for Solving High-Dimensional Partial  Differential Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liang%2C+S">Senwei Liang</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+H">Haizhao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item919">[919]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.13117" title="Abstract">arXiv:2206.13117</a> (replaced) [<a href="/e-print/2206.13117" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SARNet: Semantic Augmented Registration of Large-Scale Urban Point  Clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jianwei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+D">Dong-Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Zhirong Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaopeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhanglin Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Author information changes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item920">[920]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.03361" title="Abstract">arXiv:2207.03361</a> (replaced) [<a href="/pdf/2207.03361" title="Download PDF">pdf</a>, <a href="/ps/2207.03361" title="Download PostScript">ps</a>, <a href="/format/2207.03361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prophet Inequalities via the Expected Competitive Ratio
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ezra%2C+T">Tomer Ezra</a>, 
<a href="/search/cs?searchtype=author&query=Leonardi%2C+S">Stefano Leonardi</a>, 
<a href="/search/cs?searchtype=author&query=Reiffenh%C3%A4user%2C+R">Rebecca Reiffenh&#xe4;user</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+M">Matteo Russo</a>, 
<a href="/search/cs?searchtype=author&query=Tsigonias-Dimitriadis%2C+A">Alexandros Tsigonias-Dimitriadis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item921">[921]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.05225" title="Abstract">arXiv:2207.05225</a> (replaced) [<a href="/pdf/2207.05225" title="Download PDF">pdf</a>, <a href="/format/2207.05225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Susceptibility of Continual Learning Against Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+H">Hikmat Khan</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+P+M">Pir Masoom Shah</a>, 
<a href="/search/cs?searchtype=author&query=Zaidi%2C+S+F+A">Syed Farhan Alam Zaidi</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+S+u">Saif ul Islam</a>, 
<a href="/search/cs?searchtype=author&query=Zia%2C+Q">Qasim Zia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item922">[922]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.01239" title="Abstract">arXiv:2208.01239</a> (replaced) [<a href="/pdf/2208.01239" title="Download PDF">pdf</a>, <a href="/format/2208.01239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complex matrix inversion via real matrix inversions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dai%2C+Z">Zhen Dai</a>, 
<a href="/search/math?searchtype=author&query=Lim%2C+L">Lek-Heng Lim</a>, 
<a href="/search/math?searchtype=author&query=Ye%2C+K">Ke Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item923">[923]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.01526" title="Abstract">arXiv:2208.01526</a> (replaced) [<a href="/pdf/2208.01526" title="Download PDF">pdf</a>, <a href="/format/2208.01526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multigrid reduction-in-time convergence for advection problems: A  Fourier analysis perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=De+Sterck%2C+H">H. De Sterck</a>, 
<a href="/search/math?searchtype=author&query=Friedhoff%2C+S">S. Friedhoff</a>, 
<a href="/search/math?searchtype=author&query=Krzysik%2C+O+A">O. A. Krzysik</a>, 
<a href="/search/math?searchtype=author&query=MacLachlan%2C+S+P">Scott P. MacLachlan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updates v1 with a change of LaTeX template, some minor re-wording, and small bibliographic changes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item924">[924]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.03835" title="Abstract">arXiv:2208.03835</a> (replaced) [<a href="/pdf/2208.03835" title="Download PDF">pdf</a>, <a href="/format/2208.03835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Transfer of Adversarial Robustness from Pretraining to Downstream  Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nern%2C+L+F">Laura Fee Nern</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+H">Harsh Raj</a>, 
<a href="/search/cs?searchtype=author&query=Georgi%2C+M">Maurice Georgi</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+Y">Yash Sharma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item925">[925]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.04173" title="Abstract">arXiv:2208.04173</a> (replaced) [<a href="/pdf/2208.04173" title="Download PDF">pdf</a>, <a href="/format/2208.04173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SIAD: Self-supervised Image Anomaly Detection System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+C">Chenxi Lan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+B">Bolin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuqiu Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Naiqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaowei Li</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+E">Enze Huo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bin Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 3 figures, ICCV 2023 Demo Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item926">[926]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.04441" title="Abstract">arXiv:2208.04441</a> (replaced) [<a href="/pdf/2208.04441" title="Download PDF">pdf</a>, <a href="/format/2208.04441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Txt2Img-MHN: Remote Sensing Image Generation from Text Using Modern  Hopfield Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yonghao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Weikang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ghamisi%2C+P">Pedram Ghamisi</a>, 
<a href="/search/cs?searchtype=author&query=Kopp%2C+M">Michael Kopp</a>, 
<a href="/search/cs?searchtype=author&query=Hochreiter%2C+S">Sepp Hochreiter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item927">[927]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.08288" title="Abstract">arXiv:2208.08288</a> (replaced) [<a href="/pdf/2208.08288" title="Download PDF">pdf</a>, <a href="/format/2208.08288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep learning based projection domain metal segmentation for metal  artifact reduction in cone beam computed tomography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Agrawal%2C+H">Harshit Agrawal</a>, 
<a href="/search/eess?searchtype=author&query=Hietanen%2C+A">Ari Hietanen</a>, 
<a href="/search/eess?searchtype=author&query=S%C3%A4rkk%C3%A4%2C+S">Simo S&#xe4;rkk&#xe4;</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> in IEEE Access, vol. 11, pp.00371-100382, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item928">[928]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.09287" title="Abstract">arXiv:2208.09287</a> (replaced) [<a href="/pdf/2208.09287" title="Download PDF">pdf</a>, <a href="/format/2208.09287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detect to Learn: Structure Learning with Attention and Decision Feedback  for MIMO-OFDM Receive Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xu%2C+J">Jiarui Xu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+L">Lianjun Li</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+L">Lizhong Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+L">Lingjia Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE Transactions on Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item929">[929]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.09562" title="Abstract">arXiv:2208.09562</a> (replaced) [<a href="/pdf/2208.09562" title="Download PDF">pdf</a>, <a href="/format/2208.09562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Vocabulary Multi-Label Classification with Dual-Modal Decoder on  Aligned Visual-Textual Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shichao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yikang Li</a>, 
<a href="/search/cs?searchtype=author&query=Hsiao%2C+J">Jenhao Hsiao</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+C">Chiuman Ho</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Z">Zhu Qi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item930">[930]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.09652" title="Abstract">arXiv:2208.09652</a> (replaced) [<a href="/pdf/2208.09652" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervisedly Prompting AlphaFold2 for Few-Shot Learning of Accurate  Folding Landscape and Protein Structure Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sirui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mengyun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+H">Haotian Chu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Min Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zidong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jialiang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+N">Ningxi Ni</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Diqing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y+I">Yi Isaac Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+B">Boxin Xue</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lijiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y+Q">Yi Qin Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> version 2.0; 28 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Biological Physics (physics.bio-ph)

</div>
</div>
</dd>
<dt><a name="item931">[931]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.10765" title="Abstract">arXiv:2208.10765</a> (replaced) [<a href="/pdf/2208.10765" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Low-Cost Lane-Following Algorithm for Cyber-Physical Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Archit Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Easwaran%2C+A">Arvind Easwaran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item932">[932]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.02127" title="Abstract">arXiv:2209.02127</a> (replaced) [<a href="/pdf/2209.02127" title="Download PDF">pdf</a>, <a href="/format/2209.02127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design of the topology for contrastive visual-textual alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhun Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://github.com/minogame/clip-mtob">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item933">[933]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.03434" title="Abstract">arXiv:2209.03434</a> (replaced) [<a href="/pdf/2209.03434" title="Download PDF">pdf</a>, <a href="/format/2209.03434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sporthesia: Augmenting Sports Videos Using Natural Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu-Tian%2C+C">Chen Zhu-Tian</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qisen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xiao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Beyer%2C+J">Johanna Beyer</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Haijun Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yingcai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+H">Hanspeter Pfister</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, IEEE VIS conference
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Visualization and Computer Graphics 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item934">[934]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.04861" title="Abstract">arXiv:2209.04861</a> (replaced) [<a href="/pdf/2209.04861" title="Download PDF">pdf</a>, <a href="/format/2209.04861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse Image Frequency for Long-tailed Image Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alexandridis%2C+K+P">Konstantinos Panagiotis Alexandridis</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Shan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A">Anh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jiankang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zafeiriou%2C+S">Stefanos Zafeiriou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item935">[935]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.06496" title="Abstract">arXiv:2209.06496</a> (replaced) [<a href="/pdf/2209.06496" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CCOM-HuQin: an Annotated Multimodal Chinese Fiddle Performance Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Ziya Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaobing Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Feng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 11 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions of the International Society for Music Information
  Retrieval, 2023, 6(1), 60-74
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item936">[936]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.07035" title="Abstract">arXiv:2209.07035</a> (replaced) [<a href="/pdf/2209.07035" title="Download PDF">pdf</a>, <a href="/format/2209.07035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Combinatorial Auctions for Resource Allocation with Supply Costs  and Capacity Limits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xiaoqi Tan</a>, 
<a href="/search/cs?searchtype=author&query=Leon-Garcia%2C+A">Alberto Leon-Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tsang%2C+D+H+K">Danny H.K. Tsang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2004.09640">arXiv:2004.09640</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item937">[937]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.07614" title="Abstract">arXiv:2209.07614</a> (replaced) [<a href="/pdf/2209.07614" title="Download PDF">pdf</a>, <a href="/format/2209.07614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How does Twitter account moderation work? Dynamics of account creation  and suspension on Twitter during major geopolitical events
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pierri%2C+F">Francesco Pierri</a>, 
<a href="/search/cs?searchtype=author&query=Luceri%2C+L">Luca Luceri</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Emily Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> See published version at EPJ Data Science
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPJ Data Science 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item938">[938]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.13821" title="Abstract">arXiv:2209.13821</a> (replaced) [<a href="/pdf/2209.13821" title="Download PDF">pdf</a>, <a href="/format/2209.13821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Multi Camera-IMU Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hartzer%2C+J">Jacob Hartzer</a>, 
<a href="/search/cs?searchtype=author&query=Saripalli%2C+S">Srikanth Saripalli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item939">[939]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.00571" title="Abstract">arXiv:2210.00571</a> (replaced) [<a href="/pdf/2210.00571" title="Download PDF">pdf</a>, <a href="/ps/2210.00571" title="Download PostScript">ps</a>, <a href="/format/2210.00571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond the Existential Theory of the Reals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schaefer%2C+M">Marcus Schaefer</a>, 
<a href="/search/cs?searchtype=author&query=Stefankovic%2C+D">Daniel Stefankovic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item940">[940]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01306" title="Abstract">arXiv:2210.01306</a> (replaced) [<a href="/pdf/2210.01306" title="Download PDF">pdf</a>, <a href="/format/2210.01306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Qubit Mapping Algorithm via Double-Source Optimal Routing on  Large Quantum Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Cheng%2C+C">Chin-Yi Cheng</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yang%2C+C">Chien-Yi Yang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kuo%2C+Y">Yi-Hsiang Kuo</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+R">Ren-Chu Wang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Cheng%2C+H">Hao-Chung Cheng</a>, 
<a href="/search/quant-ph?searchtype=author&query=Huang%2C+C+R">Chung-Yang Ric Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v3: restructured and more new numerical experiments added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Hardware Architecture (cs.AR); Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item941">[941]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01374" title="Abstract">arXiv:2210.01374</a> (replaced) [<a href="/pdf/2210.01374" title="Download PDF">pdf</a>, <a href="/ps/2210.01374" title="Download PostScript">ps</a>, <a href="/format/2210.01374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safety-Aware Learning-Based Control of Systems with Uncertainty  Dependent Constraints (extended version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chekan%2C+J+A">Jafar Abbaszadeh Chekan</a>, 
<a href="/search/eess?searchtype=author&query=Langbort%2C+C">Cedric Langbort</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item942">[942]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.03022" title="Abstract">arXiv:2210.03022</a> (replaced) [<a href="/pdf/2210.03022" title="Download PDF">pdf</a>, <a href="/format/2210.03022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stateful active facilitator: Coordination and Environmental  Heterogeneity in Cooperative Multi-Agent Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dianbo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+V">Vedant Shah</a>, 
<a href="/search/cs?searchtype=author&query=Boussif%2C+O">Oussama Boussif</a>, 
<a href="/search/cs?searchtype=author&query=Meo%2C+C">Cristian Meo</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+A">Anirudh Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+T">Tianmin Shu</a>, 
<a href="/search/cs?searchtype=author&query=Mozer%2C+M">Michael Mozer</a>, 
<a href="/search/cs?searchtype=author&query=Heess%2C+N">Nicolas Heess</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at ICLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item943">[943]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.03485" title="Abstract">arXiv:2210.03485</a> (replaced) [<a href="/pdf/2210.03485" title="Download PDF">pdf</a>, <a href="/format/2210.03485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient-based optimisation of the conditional-value-at-risk using the  multi-level Monte Carlo method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ganesh%2C+S">Sundar Ganesh</a>, 
<a href="/search/math?searchtype=author&query=Nobile%2C+F">Fabio Nobile</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 18 figures, 1 table, Related to <a href="/abs/2208.07252">arXiv:2208.07252</a>, Data available at <a href="https://zenodo.org/record/7193448">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item944">[944]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.05248" title="Abstract">arXiv:2210.05248</a> (replaced) [<a href="/pdf/2210.05248" title="Download PDF">pdf</a>, <a href="/format/2210.05248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised debiasing using low rank regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+G+Y">Geon Yeong Park</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+C">Chanyong Jung</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sangmin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J+C">Jong Chul Ye</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S+W">Sang Wan Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item945">[945]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.06696" title="Abstract">arXiv:2210.06696</a> (replaced) [<a href="/pdf/2210.06696" title="Download PDF">pdf</a>, <a href="/format/2210.06696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CPSAA: Accelerating Sparse Attention using Crossbar-based  Processing-In-Memory Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huize Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Hai Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Long Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+X">Xiaofei Liao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Z">Zhuohui Duan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Cong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiahong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+C">Chuanyi Gui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item946">[946]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.06719" title="Abstract">arXiv:2210.06719</a> (replaced) [<a href="/pdf/2210.06719" title="Download PDF">pdf</a>, <a href="/format/2210.06719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reward Imputation with Sketching for Contextual Batched Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+N">Ninglu Shao</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+Z">Zihua Si</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hanjing Su</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item947">[947]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.07302" title="Abstract">arXiv:2210.07302</a> (replaced) [<a href="/pdf/2210.07302" title="Download PDF">pdf</a>, <a href="/format/2210.07302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning-based Rebalancing Policies for Profit  Maximization of Relay Nodes in Payment Channel Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papadis%2C+N">Nikolaos Papadis</a>, 
<a href="/search/cs?searchtype=author&query=Tassiulas%2C+L">Leandros Tassiulas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Best Paper Award at the 4th International Conference on Mathematical Research for the Blockchain Economy (MARBLE 2023). 28 pages; minor language edits and fixes; acknowledgments added; results unchanged
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item948">[948]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.08162" title="Abstract">arXiv:2210.08162</a> (replaced) [<a href="/pdf/2210.08162" title="Download PDF">pdf</a>, <a href="/format/2210.08162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMD-DBSCAN: An Adaptive Multi-density DBSCAN for datasets of extremely  variable density
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Zhirong Ye</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yuyang Du</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yi Mao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Ziling Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at DSAA2022
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2022 IEEE 9th International Conference on Data Science and
  Advanced Analytics (DSAA). IEEE, 2022: 1-10
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item949">[949]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.09640" title="Abstract">arXiv:2210.09640</a> (replaced) [<a href="/pdf/2210.09640" title="Download PDF">pdf</a>, <a href="/format/2210.09640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustering Categorical Data: Soft Rounding k-modes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gavva%2C+S+T">Surya Teja Gavva</a>, 
<a href="/search/cs?searchtype=author&query=S.%2C+K+C">Karthik C. S.</a>, 
<a href="/search/cs?searchtype=author&query=Punna%2C+S">Sharath Punna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item950">[950]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.10567" title="Abstract">arXiv:2210.10567</a> (replaced) [<a href="/pdf/2210.10567" title="Download PDF">pdf</a>, <a href="/format/2210.10567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Margin Optimal Classification Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=D%27Onofrio%2C+F">Federico D&#x27;Onofrio</a>, 
<a href="/search/math?searchtype=author&query=Grani%2C+G">Giorgio Grani</a>, 
<a href="/search/math?searchtype=author&query=Monaci%2C+M">Marta Monaci</a>, 
<a href="/search/math?searchtype=author&query=Palagi%2C+L">Laura Palagi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item951">[951]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.17145" title="Abstract">arXiv:2210.17145</a> (replaced) [<a href="/pdf/2210.17145" title="Download PDF">pdf</a>, <a href="/format/2210.17145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probability-Dependent Gradient Decay in Large Margin Softmax
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+S">Siyuan Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Xie%2C+L">Linbo Xie</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+Y">Ying Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item952">[952]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.02143" title="Abstract">arXiv:2211.02143</a> (replaced) [<a href="/pdf/2211.02143" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sports Camera Pose Refinement Using an Evolution Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rype%C5%9B%C4%87%2C+G">Grzegorz Rype&#x15b;&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Kurzejamski%2C+G">Grzegorz Kurzejamski</a>, 
<a href="/search/cs?searchtype=author&query=Komorowski%2C+J">Jacek Komorowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference paper at 2022 IEEE Congress on Evolutionary Computation (CEC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item953">[953]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.03963" title="Abstract">arXiv:2211.03963</a> (replaced) [<a href="/pdf/2211.03963" title="Download PDF">pdf</a>, <a href="/format/2211.03963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Algorithms for $\ell_p$-Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adil%2C+D">Deeksha Adil</a>, 
<a href="/search/cs?searchtype=author&query=Kyng%2C+R">Rasmus Kyng</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+R">Richard Peng</a>, 
<a href="/search/cs?searchtype=author&query=Sachdeva%2C+S">Sushant Sachdeva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is a coherent algorithmic framework that combines and simplifies our previous works: 1. <a href="/abs/1901.06764">arXiv:1901.06764</a> 2. <a href="/abs/1907.07167">arXiv:1907.07167</a> 3. <a href="/abs/1910.10571">arXiv:1910.10571</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item954">[954]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.05238" title="Abstract">arXiv:2211.05238</a> (replaced) [<a href="/pdf/2211.05238" title="Download PDF">pdf</a>, <a href="/format/2211.05238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polarized consensus-based dynamics for optimization and sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bungert%2C+L">Leon Bungert</a>, 
<a href="/search/math?searchtype=author&query=Roith%2C+T">Tim Roith</a>, 
<a href="/search/math?searchtype=author&query=Wacker%2C+P">Philipp Wacker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added mean-field convergence theorem
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item955">[955]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.06869" title="Abstract">arXiv:2211.06869</a> (replaced) [<a href="/pdf/2211.06869" title="Download PDF">pdf</a>, <a href="/format/2211.06869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Meet Harry Potter: A Bilingual Dataset for  Aligning Dialogue Agents with Characters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+N">Nuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haiyun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Deng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhan Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Longyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item956">[956]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.07624" title="Abstract">arXiv:2211.07624</a> (replaced) [<a href="/pdf/2211.07624" title="Download PDF">pdf</a>, <a href="/format/2211.07624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Similarity Models for Depression Severity Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+A">Anxo P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Warikoo%2C+N">Neha Warikoo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kexin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Parapar%2C+J">Javier Parapar</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the EMNLP 2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item957">[957]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10897" title="Abstract">arXiv:2211.10897</a> (replaced) [<a href="/pdf/2211.10897" title="Download PDF">pdf</a>, <a href="/format/2211.10897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Best-Effort Communication Improves Performance and Scales Robustly on  Conventional Hardware
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moreno%2C+M+A">Matthew Andres Moreno</a>, 
<a href="/search/cs?searchtype=author&query=Ofria%2C+C">Charles Ofria</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item958">[958]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13175" title="Abstract">arXiv:2211.13175</a> (replaced) [<a href="/pdf/2211.13175" title="Download PDF">pdf</a>, <a href="/format/2211.13175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coordination of multiple mobile manipulators for ordered sorting of  cluttered objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahn%2C+J">Jeeho Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seabin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+C">Changjoo Nam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at iROS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item959">[959]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13974" title="Abstract">arXiv:2211.13974</a> (replaced) [<a href="/pdf/2211.13974" title="Download PDF">pdf</a>, <a href="/format/2211.13974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ILSGAN: Independent Layer Synthesis for Unsupervised  Foreground-Background Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+Q">Qiran Zou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+W+Y">Wing Yin Cheung</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+X">Xiangyang Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2023 (Oral). Code: <a href="https://github.com/qrzou/ILSGAN">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item960">[960]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13989" title="Abstract">arXiv:2211.13989</a> (replaced) [<a href="/pdf/2211.13989" title="Download PDF">pdf</a>, <a href="/format/2211.13989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HexaMesh: Scaling to Hundreds of Chiplets with an Optimized Chiplet  Arrangement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iff%2C+P">Patrick Iff</a>, 
<a href="/search/cs?searchtype=author&query=Besta%2C+M">Maciej Besta</a>, 
<a href="/search/cs?searchtype=author&query=Cavalcante%2C+M">Matheus Cavalcante</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+T">Tim Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Benini%2C+L">Luca Benini</a>, 
<a href="/search/cs?searchtype=author&query=Hoefler%2C+T">Torsten Hoefler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item961">[961]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.15072" title="Abstract">arXiv:2211.15072</a> (replaced) [<a href="/pdf/2211.15072" title="Download PDF">pdf</a>, <a href="/format/2211.15072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FaiREE: Fair Classification with Finite-Sample and Distribution-Free  Guarantee
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+P">Puheng Li</a>, 
<a href="/search/stat?searchtype=author&query=Zou%2C+J">James Zou</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+L">Linjun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 45 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item962">[962]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.15477" title="Abstract">arXiv:2211.15477</a> (replaced) [<a href="/pdf/2211.15477" title="Download PDF">pdf</a>, <a href="/format/2211.15477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On digraphs without onion star immersions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bo%C5%BCyk%2C+%C5%81">&#x141;ukasz Bo&#x17c;yk</a>, 
<a href="/search/math?searchtype=author&query=Defrain%2C+O">Oscar Defrain</a>, 
<a href="/search/math?searchtype=author&query=Okrasa%2C+K">Karolina Okrasa</a>, 
<a href="/search/math?searchtype=author&query=Pilipczuk%2C+M">Micha&#x142; Pilipczuk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item963">[963]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.15602" title="Abstract">arXiv:2211.15602</a> (replaced) [<a href="/pdf/2211.15602" title="Download PDF">pdf</a>, <a href="/ps/2211.15602" title="Download PostScript">ps</a>, <a href="/format/2211.15602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Upper Bounds for All and Max-gain Policy Iteration Algorithms on  Deterministic MDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goenka%2C+R">Ritesh Goenka</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+E">Eashan Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Khyalia%2C+S">Sushil Khyalia</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+P">Pratyush Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Wajid%2C+M+S">Mulinti Shaik Wajid</a>, 
<a href="/search/cs?searchtype=author&query=Kalyanakrishnan%2C+S">Shivaram Kalyanakrishnan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added new bounds for two state MDPs
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Computational Complexity (cs.CC); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item964">[964]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.16044" title="Abstract">arXiv:2211.16044</a> (replaced) [<a href="/pdf/2211.16044" title="Download PDF">pdf</a>, <a href="/format/2211.16044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Extraction Attack against Self-supervised Speech Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsu%2C+T">Tsu-Yuan Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chen-An Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tung-Yu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item965">[965]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.00190" title="Abstract">arXiv:2212.00190</a> (replaced) [<a href="/pdf/2212.00190" title="Download PDF">pdf</a>, <a href="/format/2212.00190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed Neural Voxels for Fast Multi-view Video Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Feng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+S">Sinan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinghang Li</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zeyue Tian</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yafei Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huaping Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item966">[966]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.04064" title="Abstract">arXiv:2212.04064</a> (replaced) [<a href="/pdf/2212.04064" title="Download PDF">pdf</a>, <a href="/format/2212.04064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CRC-Aided High-Rate Convolutional Codes With Short Blocklengths for List  Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sui%2C+W">Wenhui Sui</a>, 
<a href="/search/cs?searchtype=author&query=Towell%2C+B">Brendan Towell</a>, 
<a href="/search/cs?searchtype=author&query=Asmani%2C+A">Ava Asmani</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hengjie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Grissett%2C+H">Holden Grissett</a>, 
<a href="/search/cs?searchtype=author&query=Wesel%2C+R+D">Richard D. Wesel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2111.07929">arXiv:2111.07929</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item967">[967]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.04295" title="Abstract">arXiv:2212.04295</a> (replaced) [<a href="/pdf/2212.04295" title="Download PDF">pdf</a>, <a href="/format/2212.04295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preconditioned Chebyshev BiCG for parameterized linear systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Correnty%2C+S">Siobh&#xe1;n Correnty</a>, 
<a href="/search/math?searchtype=author&query=Jarlebring%2C+E">Elias Jarlebring</a>, 
<a href="/search/math?searchtype=author&query=Szyld%2C+D+B">Daniel B. Szyld</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item968">[968]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.08653" title="Abstract">arXiv:2212.08653</a> (replaced) [<a href="/pdf/2212.08653" title="Download PDF">pdf</a>, <a href="/format/2212.08653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attentive Mask CLIP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weiquan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yixuan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Houwen Peng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xinyang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Huiqiang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Fangyun Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Han Hu</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+L">Lili Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuqing Yang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2023, pp. 2771-2781
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item969">[969]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09289" title="Abstract">arXiv:2212.09289</a> (replaced) [<a href="/pdf/2212.09289" title="Download PDF">pdf</a>, <a href="/format/2212.09289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mining User Privacy Concern Topics from App Reviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianzhang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+J">Jinping Hua</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+N">Nan Niu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chuang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item970">[970]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10340" title="Abstract">arXiv:2212.10340</a> (replaced) [<a href="/pdf/2212.10340" title="Download PDF">pdf</a>, <a href="/format/2212.10340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly supervised training of universal visual concepts for multi-domain  semantic segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bevandi%C4%87%2C+P">Petra Bevandi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Or%C5%A1i%C4%87%2C+M">Marin Or&#x161;i&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Grubi%C5%A1i%C4%87%2C+I">Ivan Grubi&#x161;i&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0ari%C4%87%2C+J">Josip &#x160;ari&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0egvi%C4%87%2C+S">Sini&#x161;a &#x160;egvi&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 16 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item971">[971]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.11642" title="Abstract">arXiv:2212.11642</a> (replaced) [<a href="/pdf/2212.11642" title="Download PDF">pdf</a>, <a href="/format/2212.11642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predictive Coding Based Multiscale Network with Encoder-Decoder LSTM for  Video Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+C">Chaofan Ling</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+J">Junpei Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weihua Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item972">[972]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00068" title="Abstract">arXiv:2301.00068</a> (replaced) [<a href="/pdf/2301.00068" title="Download PDF">pdf</a>, <a href="/format/2301.00068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Inconsistencies of Conditionals Learned by Masked Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Young%2C+T">Tom Young</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated version; Added more experiments on "Ensemble of Conditionals"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item973">[973]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04817" title="Abstract">arXiv:2301.04817</a> (replaced) [<a href="/pdf/2301.04817" title="Download PDF">pdf</a>, <a href="/format/2301.04817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consensus in the Unknown-Participation Message-Adversary Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Losa%2C+G">Giuliano Losa</a>, 
<a href="/search/cs?searchtype=author&query=Gafni%2C+E">Eli Gafni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item974">[974]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.05500" title="Abstract">arXiv:2301.05500</a> (replaced) [<a href="/pdf/2301.05500" title="Download PDF">pdf</a>, <a href="/format/2301.05500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised  Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Z">Zengxin Qi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuehai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Ying Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lichi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item975">[975]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.07014" title="Abstract">arXiv:2301.07014</a> (replaced) [<a href="/pdf/2301.07014" title="Download PDF">pdf</a>, <a href="/format/2301.07014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dataset Distillation: A Comprehensive Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Ruonan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Songhua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinchao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by TPAMI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item976">[976]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.07270" title="Abstract">arXiv:2301.07270</a> (replaced) [<a href="/pdf/2301.07270" title="Download PDF">pdf</a>, <a href="/format/2301.07270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weighted Trace-Penalty Minimization for Full Configuration Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gao%2C+W">Weiguo Gao</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+Y">Yingzhou Li</a>, 
<a href="/search/math?searchtype=author&query=Shen%2C+H">Hanxiang Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item977">[977]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.08245" title="Abstract">arXiv:2301.08245</a> (replaced) [<a href="/pdf/2301.08245" title="Download PDF">pdf</a>, <a href="/format/2301.08245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Booster: a Benchmark for Depth from Images of Specular and Transparent  Surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramirez%2C+P+Z">Pierluigi Zama Ramirez</a>, 
<a href="/search/cs?searchtype=author&query=Costanzino%2C+A">Alex Costanzino</a>, 
<a href="/search/cs?searchtype=author&query=Tosi%2C+F">Fabio Tosi</a>, 
<a href="/search/cs?searchtype=author&query=Poggi%2C+M">Matteo Poggi</a>, 
<a href="/search/cs?searchtype=author&query=Salti%2C+S">Samuele Salti</a>, 
<a href="/search/cs?searchtype=author&query=Mattoccia%2C+S">Stefano Mattoccia</a>, 
<a href="/search/cs?searchtype=author&query=Di+Stefano%2C+L">Luigi Di Stefano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extension of the paper "Open Challenges in Deep Stereo: the Booster Dataset" presented at CVPR 2022. Accepted at TPAMI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item978">[978]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.10750" title="Abstract">arXiv:2301.10750</a> (replaced) [<a href="/pdf/2301.10750" title="Download PDF">pdf</a>, <a href="/ps/2301.10750" title="Download PostScript">ps</a>, <a href="/format/2301.10750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Out of Distribution Performance of State of Art Vision Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+S">Salman Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+W">Wonkwon Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> incomplete work - need to complete it
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item979">[979]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.10936" title="Abstract">arXiv:2301.10936</a> (replaced) [<a href="/pdf/2301.10936" title="Download PDF">pdf</a>, <a href="/format/2301.10936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PIT: Optimization of Dynamic Sparse Deep Learning Models via Permutation  Invariant Transformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Ningxin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Huiqiang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Quanlu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhenhua Han</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuqing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lingxiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chengruidong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+L">Lili Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Lidong Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item980">[980]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11975" title="Abstract">arXiv:2301.11975</a> (replaced) [<a href="/pdf/2301.11975" title="Download PDF">pdf</a>, <a href="/format/2301.11975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Byte Pair Encoding for Symbolic Music
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fradet%2C+N">Nathan Fradet</a>, 
<a href="/search/cs?searchtype=author&query=Gutowski%2C+N">Nicolas Gutowski</a>, 
<a href="/search/cs?searchtype=author&query=Chhel%2C+F">Fabien Chhel</a>, 
<a href="/search/cs?searchtype=author&query=Briot%2C+J">Jean-Pierre Briot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023, source code: <a href="https://github.com/Natooz/BPE-Symbolic-Music">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item981">[981]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12379" title="Abstract">arXiv:2301.12379</a> (replaced) [<a href="/pdf/2301.12379" title="Download PDF">pdf</a>, <a href="/format/2301.12379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedRC: Tackling Diverse Distribution Shifts Challenge in Federated  Learning by Robust Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yongxin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaoying Tang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item982">[982]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12522" title="Abstract">arXiv:2301.12522</a> (replaced) [<a href="/pdf/2301.12522" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Service Provisioning in IoT Fog-based Environment for QoS-aware  Delay-sensitive Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hashemifar%2C+S">Soroush Hashemifar</a>, 
<a href="/search/cs?searchtype=author&query=Rajabzadeh%2C+A">Amir Rajabzadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item983">[983]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12714" title="Abstract">arXiv:2301.12714</a> (replaced) [<a href="/pdf/2301.12714" title="Download PDF">pdf</a>, <a href="/format/2301.12714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Importance Weighted Actor-Critic for Optimal Conservative Offline  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanlin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Rashidinejad%2C+P">Paria Rashidinejad</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J">Jiantao Jiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item984">[984]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12842" title="Abstract">arXiv:2301.12842</a> (replaced) [<a href="/pdf/2301.12842" title="Download PDF">pdf</a>, <a href="/format/2301.12842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Preference-based Policy Optimization without Reward Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+G">Gaon An</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Junhyeok Lee</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+X">Xingdong Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Kosaka%2C+N">Norio Kosaka</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kyung-Min Kim</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H+O">Hyun Oh Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item985">[985]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13856" title="Abstract">arXiv:2301.13856</a> (replaced) [<a href="/pdf/2301.13856" title="Download PDF">pdf</a>, <a href="/format/2301.13856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simplex Random Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Reid%2C+I">Isaac Reid</a>, 
<a href="/search/stat?searchtype=author&query=Choromanski%2C+K">Krzysztof Choromanski</a>, 
<a href="/search/stat?searchtype=author&query=Likhosherstov%2C+V">Valerii Likhosherstov</a>, 
<a href="/search/stat?searchtype=author&query=Weller%2C+A">Adrian Weller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item986">[986]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02055" title="Abstract">arXiv:2302.02055</a> (replaced) [<a href="/pdf/2302.02055" title="Download PDF">pdf</a>, <a href="/format/2302.02055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Geometry and Interaction Embeddings Improve Few-Shot Molecular  Property Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fifty%2C+C">Christopher Fifty</a>, 
<a href="/search/cs?searchtype=author&query=Paggi%2C+J+M">Joseph M. Paggi</a>, 
<a href="/search/cs?searchtype=author&query=Amid%2C+E">Ehsan Amid</a>, 
<a href="/search/cs?searchtype=author&query=Leskovec%2C+J">Jure Leskovec</a>, 
<a href="/search/cs?searchtype=author&query=Dror%2C+R">Ron Dror</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item987">[987]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02743" title="Abstract">arXiv:2302.02743</a> (replaced) [<a href="/pdf/2302.02743" title="Download PDF">pdf</a>, <a href="/format/2302.02743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resolution of singularities by rational functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Herremans%2C+A">Astrid Herremans</a>, 
<a href="/search/math?searchtype=author&query=Huybrechs%2C+D">Daan Huybrechs</a>, 
<a href="/search/math?searchtype=author&query=Trefethen%2C+L+N">Lloyd N. Trefethen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item988">[988]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02995" title="Abstract">arXiv:2302.02995</a> (replaced) [<a href="/pdf/2302.02995" title="Download PDF">pdf</a>, <a href="/format/2302.02995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight bound on treedepth in terms of pathwidth and longest path
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hatzel%2C+M">Meike Hatzel</a>, 
<a href="/search/math?searchtype=author&query=Joret%2C+G">Gwena&#xeb;l Joret</a>, 
<a href="/search/math?searchtype=author&query=Micek%2C+P">Piotr Micek</a>, 
<a href="/search/math?searchtype=author&query=Pilipczuk%2C+M">Marcin Pilipczuk</a>, 
<a href="/search/math?searchtype=author&query=Ueckerdt%2C+T">Torsten Ueckerdt</a>, 
<a href="/search/math?searchtype=author&query=Walczak%2C+B">Bartosz Walczak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2: revised following referees' comments, corrects an error in v1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item989">[989]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04810" title="Abstract">arXiv:2302.04810</a> (replaced) [<a href="/pdf/2302.04810" title="Download PDF">pdf</a>, <a href="/format/2302.04810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-world Machine Learning Systems: A survey from a Data-Oriented  Architecture Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cabrera%2C+C">Christian Cabrera</a>, 
<a href="/search/cs?searchtype=author&query=Paleyes%2C+A">Andrei Paleyes</a>, 
<a href="/search/cs?searchtype=author&query=Thodoroff%2C+P">Pierre Thodoroff</a>, 
<a href="/search/cs?searchtype=author&query=Lawrence%2C+N+D">Neil D. Lawrence</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item990">[990]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04823" title="Abstract">arXiv:2302.04823</a> (replaced) [<a href="/pdf/2302.04823" title="Download PDF">pdf</a>, <a href="/format/2302.04823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Generative Adversarial Imitation Learning with Mid-level  Input Generation for Autonomous Driving on Urban Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Couto%2C+G+C+K">Gustavo Claudio Karl Couto</a>, 
<a href="/search/cs?searchtype=author&query=Antonelo%2C+E+A">Eric Aislan Antonelo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item991">[991]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06674" title="Abstract">arXiv:2302.06674</a> (replaced) [<a href="/pdf/2302.06674" title="Download PDF">pdf</a>, <a href="/format/2302.06674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PK-ICR: Persona-Knowledge Interactive Context Retrieval for Grounded  Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+M">Minsik Oh</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Joosung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoyin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item992">[992]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06831" title="Abstract">arXiv:2302.06831</a> (replaced) [<a href="/pdf/2302.06831" title="Download PDF">pdf</a>, <a href="/format/2302.06831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analytical Model of Nonlinear Fiber Propagation for General  Dual-Polarization Four-Dimensional Modulation Format
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liang%2C+Z">Zhiwei Liang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+B">Bin Chen</a>, 
<a href="/search/eess?searchtype=author&query=Lei%2C+Y">Yi Lei</a>, 
<a href="/search/eess?searchtype=author&query=Liga%2C+G">Gabriele Liga</a>, 
<a href="/search/eess?searchtype=author&query=Alvarado%2C+A">Alex Alvarado</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages,8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item993">[993]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07676" title="Abstract">arXiv:2302.07676</a> (replaced) [<a href="/pdf/2302.07676" title="Download PDF">pdf</a>, <a href="/format/2302.07676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DIVOTrack: A Novel Dataset and Baseline Method for Cross-View  Multi-Object Tracking in DIVerse Open Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+S">Shenghao Hao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Y">Yibing Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+K">Kaixun Jin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Mingli Song</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+J">Jenq-Neng Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Gaoang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IJCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item994">[994]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08817" title="Abstract">arXiv:2302.08817</a> (replaced) [<a href="/pdf/2302.08817" title="Download PDF">pdf</a>, <a href="/format/2302.08817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Response Generation for Chinese Reading Comprehension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+N">Nuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongguang Li</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+Y">Yinan Bao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baoyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item995">[995]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09207" title="Abstract">arXiv:2302.09207</a> (replaced) [<a href="/pdf/2302.09207" title="Download PDF">pdf</a>, <a href="/format/2302.09207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RETVec: Resilient and Efficient Text Vectorizer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bursztein%2C+E">Elie Bursztein</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Marina Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Vallis%2C+O">Owen Vallis</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xinyu Jia</a>, 
<a href="/search/cs?searchtype=author&query=Kurakin%2C+A">Alexey Kurakin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item996">[996]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10764" title="Abstract">arXiv:2302.10764</a> (replaced) [<a href="/pdf/2302.10764" title="Download PDF">pdf</a>, <a href="/format/2302.10764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On The Coherence of Quantitative Evaluation of Visual Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vandersmissen%2C+B">Benjamin Vandersmissen</a>, 
<a href="/search/cs?searchtype=author&query=Oramas%2C+J">Jose Oramas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item997">[997]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11197" title="Abstract">arXiv:2302.11197</a> (replaced) [<a href="/pdf/2302.11197" title="Download PDF">pdf</a>, <a href="/ps/2302.11197" title="Download PostScript">ps</a>, <a href="/format/2302.11197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantized Low-Rank Multivariate Regression with Random Dithering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+J">Junren Chen</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+Y">Yueqi Wang</a>, 
<a href="/search/stat?searchtype=author&query=Ng%2C+M+K">Michael K. Ng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Transactions on Signal Processing (publication ready version)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item998">[998]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11713" title="Abstract">arXiv:2302.11713</a> (replaced) [<a href="/pdf/2302.11713" title="Download PDF">pdf</a>, <a href="/format/2302.11713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Pre-trained Vision and Language Models Answer Visual  Information-Seeking Questions?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hexiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Luan%2C+Y">Yi Luan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haitian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Changpinyo%2C+S">Soravit Changpinyo</a>, 
<a href="/search/cs?searchtype=author&query=Ritter%2C+A">Alan Ritter</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+M">Ming-Wei Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (main conference); Our dataset and evaluation is available at <a href="https://open-vision-language.github.io/infoseek/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item999">[999]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11737" title="Abstract">arXiv:2302.11737</a> (replaced) [<a href="/pdf/2302.11737" title="Download PDF">pdf</a>, <a href="/format/2302.11737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causally Disentangled Generative Variational AutoEncoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=An%2C+S">Seunghwan An</a>, 
<a href="/search/stat?searchtype=author&query=Song%2C+K">Kyungwoo Song</a>, 
<a href="/search/stat?searchtype=author&query=Jeon%2C+J">Jong-June Jeon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1000">[1000]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.12170" title="Abstract">arXiv:2302.12170</a> (replaced) [<a href="/pdf/2302.12170" title="Download PDF">pdf</a>, <a href="/format/2302.12170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Model Crossover: Variation through Few-Shot Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meyerson%2C+E">Elliot Meyerson</a>, 
<a href="/search/cs?searchtype=author&query=Nelson%2C+M+J">Mark J. Nelson</a>, 
<a href="/search/cs?searchtype=author&query=Bradley%2C+H">Herbie Bradley</a>, 
<a href="/search/cs?searchtype=author&query=Gaier%2C+A">Adam Gaier</a>, 
<a href="/search/cs?searchtype=author&query=Moradi%2C+A">Arash Moradi</a>, 
<a href="/search/cs?searchtype=author&query=Hoover%2C+A+K">Amy K. Hoover</a>, 
<a href="/search/cs?searchtype=author&query=Lehman%2C+J">Joel Lehman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item1001">[1001]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13417" title="Abstract">arXiv:2302.13417</a> (replaced) [<a href="/pdf/2302.13417" title="Download PDF">pdf</a>, <a href="/format/2302.13417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training neural networks with structured noise improves classification  and generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Benedetti%2C+M">Marco Benedetti</a>, 
<a href="/search/cond-mat?searchtype=author&query=Ventura%2C+E">Enrico Ventura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 17 figures, main text and appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Disordered Systems and Neural Networks (cond-mat.dis-nn)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1002">[1002]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13539" title="Abstract">arXiv:2302.13539</a> (replaced) [<a href="/pdf/2302.13539" title="Download PDF">pdf</a>, <a href="/format/2302.13539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding Support Examples for In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaonan Li</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1003">[1003]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13934" title="Abstract">arXiv:2302.13934</a> (replaced) [<a href="/pdf/2302.13934" title="Download PDF">pdf</a>, <a href="/format/2302.13934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Learning under Heterogenous Distribution Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Simchowitz%2C+M">Max Simchowitz</a>, 
<a href="/search/cs?searchtype=author&query=Ajay%2C+A">Anurag Ajay</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Pulkit Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+A">Akshay Krishnamurthy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1004">[1004]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00180" title="Abstract">arXiv:2303.00180</a> (replaced) [<a href="/pdf/2303.00180" title="Download PDF">pdf</a>, <a href="/format/2303.00180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FaceRNET: a Facial Expression Intensity Estimation Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kollias%2C+D">Dimitrios Kollias</a>, 
<a href="/search/cs?searchtype=author&query=Psaroudakis%2C+A">Andreas Psaroudakis</a>, 
<a href="/search/cs?searchtype=author&query=Arsenos%2C+A">Anastasios Arsenos</a>, 
<a href="/search/cs?searchtype=author&query=Theofilou%2C+P">Paraskevi Theofilou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1005">[1005]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03192" title="Abstract">arXiv:2303.03192</a> (replaced) [<a href="/pdf/2303.03192" title="Download PDF">pdf</a>, <a href="/format/2303.03192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep symbolic regression for physics guided by units constraints: toward  the automated discovery of physical laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Tenachi%2C+W">Wassim Tenachi</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ibata%2C+R">Rodrigo Ibata</a>, 
<a href="/search/astro-ph?searchtype=author&query=Diakogiannis%2C+F+I">Foivos I. Diakogiannis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 9 figures, 11 tables. Accepted for publication at ApJ
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item1006">[1006]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03324" title="Abstract">arXiv:2303.03324</a> (replaced) [<a href="/pdf/2303.03324" title="Download PDF">pdf</a>, <a href="/format/2303.03324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time series anomaly detection with reconstruction-based state-space  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Keli Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+B">Boyu Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1007">[1007]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03476" title="Abstract">arXiv:2303.03476</a> (replaced) [<a href="/pdf/2303.03476" title="Download PDF">pdf</a>, <a href="/format/2303.03476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> iBall: Augmenting Basketball Videos with Gaze-moderated Embedded  Visualizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu-Tian%2C+C">Chen Zhu-Tian</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qisen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+J">Jiarui Shan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tica Lin</a>, 
<a href="/search/cs?searchtype=author&query=Beyer%2C+J">Johanna Beyer</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Haijun Xia</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+H">Hanspeter Pfister</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACM CHI23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1008">[1008]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04516" title="Abstract">arXiv:2303.04516</a> (replaced) [<a href="/pdf/2303.04516" title="Download PDF">pdf</a>, <a href="/format/2303.04516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-Optimal Control via Heaviside Step-Function Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pfeiffer%2C+K">Kai Pfeiffer</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+Q">Quang-Cuong Pham</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1009">[1009]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04795" title="Abstract">arXiv:2303.04795</a> (replaced) [<a href="/pdf/2303.04795" title="Download PDF">pdf</a>, <a href="/ps/2303.04795" title="Download PostScript">ps</a>, <a href="/format/2303.04795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stabilized profunctors and stable species of structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fiore%2C+M">Marcelo Fiore</a>, 
<a href="/search/cs?searchtype=author&query=Galal%2C+Z">Zeinab Galal</a>, 
<a href="/search/cs?searchtype=author&query=Paquet%2C+H">Hugo Paquet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> FSCD 2022 special issue of Logical Methods in Computer Science, minor changes (incorporated reviewers comments)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Category Theory (math.CT); Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item1010">[1010]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05118" title="Abstract">arXiv:2303.05118</a> (replaced) [<a href="/pdf/2303.05118" title="Download PDF">pdf</a>, <a href="/format/2303.05118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SLCA: Slow Learner with Classifier Alignment for Continual Learning on a  Pre-trained Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gengwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+G">Guoliang Kang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Ling Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yunchao Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023, code released
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1011">[1011]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05791" title="Abstract">arXiv:2303.05791</a> (replaced) [<a href="/pdf/2303.05791" title="Download PDF">pdf</a>, <a href="/format/2303.05791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A low-order automatic domain splitting approach for nonlinear  uncertainty mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Losacco%2C+M">Matteo Losacco</a>, 
<a href="/search/math?searchtype=author&query=Foss%C3%A0%2C+A">Alberto Foss&#xe0;</a>, 
<a href="/search/math?searchtype=author&query=Armellin%2C+R">Roberto Armellin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1012">[1012]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06305" title="Abstract">arXiv:2303.06305</a> (replaced) [<a href="/pdf/2303.06305" title="Download PDF">pdf</a>, <a href="/format/2303.06305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing Non-IID Effects in Federated Autonomous Driving with  Contrastive Divergence Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Do%2C+T">Tuong Do</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+B+X">Binh X. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H">Hien Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Tjiputra%2C+E">Erman Tjiputra</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+Q+D">Quang D. Tran</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+T">Te-Chuan Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A">Anh Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1013">[1013]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08010" title="Abstract">arXiv:2303.08010</a> (replaced) [<a href="/pdf/2303.08010" title="Download PDF">pdf</a>, <a href="/format/2303.08010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep  Ensembles are More Efficient than Single Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+G">Guoxuan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Bouganis%2C+C">Christos-Savvas Bouganis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV 2023 (camera-ready version, 9 pages)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1014">[1014]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08021" title="Abstract">arXiv:2303.08021</a> (replaced) [<a href="/pdf/2303.08021" title="Download PDF">pdf</a>, <a href="/format/2303.08021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OptBA: Optimizing Hyperparameters with the Bees Algorithm for Improved  Medical Text Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaaban%2C+M+A">Mai A. Shaaban</a>, 
<a href="/search/cs?searchtype=author&query=Kashkash%2C+M">Mariam Kashkash</a>, 
<a href="/search/cs?searchtype=author&query=Alghfeli%2C+M">Maryam Alghfeli</a>, 
<a href="/search/cs?searchtype=author&query=Ibrahim%2C+A">Adham Ibrahim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1015">[1015]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08423" title="Abstract">arXiv:2303.08423</a> (replaced) [<a href="/pdf/2303.08423" title="Download PDF">pdf</a>, <a href="/format/2303.08423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication-Efficient Design for Quantized Decentralized Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Li Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weidong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1016">[1016]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08516" title="Abstract">arXiv:2303.08516</a> (replaced) [<a href="/pdf/2303.08516" title="Download PDF">pdf</a>, <a href="/format/2303.08516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Off-Policy Learning from Observational Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frauen%2C+D">Dennis Frauen</a>, 
<a href="/search/cs?searchtype=author&query=Melnychuk%2C+V">Valentyn Melnychuk</a>, 
<a href="/search/cs?searchtype=author&query=Feuerriegel%2C+S">Stefan Feuerriegel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revised version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1017">[1017]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08991" title="Abstract">arXiv:2303.08991</a> (replaced) [<a href="/pdf/2303.08991" title="Download PDF">pdf</a>, <a href="/format/2303.08991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeltaScore: Story Evaluation with Perturbations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhuohan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Miao Li</a>, 
<a href="/search/cs?searchtype=author&query=Cohn%2C+T">Trevor Cohn</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+J+H">Jey Han Lau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 3 figures, 6 tables. Accepted to EMNLP 2023, soundness: 4 4 3, excitement: 4 4 3
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1018">[1018]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09079" title="Abstract">arXiv:2303.09079</a> (replaced) [<a href="/pdf/2303.09079" title="Download PDF">pdf</a>, <a href="/format/2303.09079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+M">Mengxin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jiaqi Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+Q">Qian Lou</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Lei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaofeng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1019">[1019]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09268" title="Abstract">arXiv:2303.09268</a> (replaced) [<a href="/pdf/2303.09268" title="Download PDF">pdf</a>, <a href="/format/2303.09268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StylerDALLE: Language-Guided Style Transfer Using a Vector-Quantized  Tokenizer of a Large-Scale Generative Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zipeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Sangineto%2C+E">Enver Sangineto</a>, 
<a href="/search/cs?searchtype=author&query=Sebe%2C+N">Nicu Sebe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1020">[1020]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09599" title="Abstract">arXiv:2303.09599</a> (replaced) [<a href="/pdf/2303.09599" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> cito: An R package for training neural networks using torch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amesoeder%2C+C">Christian Amesoeder</a>, 
<a href="/search/cs?searchtype=author&query=Hartig%2C+F">Florian Hartig</a>, 
<a href="/search/cs?searchtype=author&query=Pichler%2C+M">Maximilian Pichler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 4 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1021">[1021]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10369" title="Abstract">arXiv:2303.10369</a> (replaced) [<a href="/pdf/2303.10369" title="Download PDF">pdf</a>, <a href="/format/2303.10369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blind Multimodal Quality Assessment of Low-light Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Miaohui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhuowei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weisi Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1022">[1022]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10868" title="Abstract">arXiv:2303.10868</a> (replaced) [<a href="/pdf/2303.10868" title="Download PDF">pdf</a>, <a href="/format/2303.10868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieving Multimodal Information for Augmented Generation: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Ruochen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hailin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weishi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+F">Fangkai Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+X+L">Xuan Long Do</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+C">Chengwei Qin</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+B">Bosheng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xiaobao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Minzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1023">[1023]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11516" title="Abstract">arXiv:2303.11516</a> (replaced) [<a href="/pdf/2303.11516" title="Download PDF">pdf</a>, <a href="/format/2303.11516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear-Covariance Loss for End-to-End Learning of 6D Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fulin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yinlin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Salzmann%2C+M">Mathieu Salzmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1024">[1024]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11593" title="Abstract">arXiv:2303.11593</a> (replaced) [<a href="/pdf/2303.11593" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How does Transformer model evolve to learn diverse chemical structures?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoshikai%2C+Y">Yasuhiro Yoshikai</a>, 
<a href="/search/cs?searchtype=author&query=Mizuno%2C+T">Tadahaya Mizuno</a>, 
<a href="/search/cs?searchtype=author&query=Nemoto%2C+S">Shumpei Nemoto</a>, 
<a href="/search/cs?searchtype=author&query=Kusuhara%2C+H">Hiroyuki Kusuhara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Chemical Physics (physics.chem-ph); Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item1025">[1025]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13709" title="Abstract">arXiv:2303.13709</a> (replaced) [<a href="/pdf/2303.13709" title="Download PDF">pdf</a>, <a href="/ps/2303.13709" title="Download PostScript">ps</a>, <a href="/format/2303.13709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Isolation of regular graphs, stars and $k$-chromatic graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Borg%2C+P">Peter Borg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, minor corrections have been made, Lemma 10 and Proposition 11 have been added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item1026">[1026]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13809" title="Abstract">arXiv:2303.13809</a> (replaced) [<a href="/pdf/2303.13809" title="Download PDF">pdf</a>, <a href="/format/2303.13809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error Analysis Prompting Enables Human-Like Translation Evaluation in  Large Language Models: A Case Study on ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Q">Qingyu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+B">Baopu Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kanjian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kocmi%2C+T">Tom Kocmi</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1027">[1027]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14465" title="Abstract">arXiv:2303.14465</a> (replaced) [<a href="/pdf/2303.14465" title="Download PDF">pdf</a>, <a href="/format/2303.14465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant Similarity for Vision-Language Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+K">Kevin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chung-Ching Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhengyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanwang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lijuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV'23 (Oral); Add evaluation on MLLM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1028">[1028]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15413" title="Abstract">arXiv:2303.15413</a> (replaced) [<a href="/pdf/2303.15413" title="Download PDF">pdf</a>, <a href="/format/2303.15413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debiasing Scores and Prompts of 2D Diffusion for View-consistent  Text-to-3D Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+S">Susung Hong</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+D">Donghoon Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungryong Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023. Project Page: <a href="https://susunghong.github.io/Debiased-Score-Distillation-Sampling/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1029">[1029]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15620" title="Abstract">arXiv:2303.15620</a> (replaced) [<a href="/pdf/2303.15620" title="Download PDF">pdf</a>, <a href="/format/2303.15620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Lead Time in Fall Detection for a Planar Bipedal Robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mungai%2C+M+E">M. Eva Mungai</a>, 
<a href="/search/cs?searchtype=author&query=Grizzle%2C+J">Jessy Grizzle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> \c{opyright} 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1030">[1030]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15714" title="Abstract">arXiv:2303.15714</a> (replaced) [<a href="/pdf/2303.15714" title="Download PDF">pdf</a>, <a href="/format/2303.15714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explicit Planning Helps Language Models in Logical Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hongyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kangrui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Mo Yu</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+H">Hongyuan Mei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1031">[1031]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16254" title="Abstract">arXiv:2303.16254</a> (replaced) [<a href="/pdf/2303.16254" title="Download PDF">pdf</a>, <a href="/format/2303.16254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CryoFormer: Continuous Heterogeneous Cryo-EM Reconstruction using  Transformer-based Neural Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinhang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yifan Qin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiakai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingyi Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1032">[1032]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16372" title="Abstract">arXiv:2303.16372</a> (replaced) [<a href="/pdf/2303.16372" title="Download PDF">pdf</a>, <a href="/format/2303.16372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Query Complexity of Training Data Reconstruction in Private  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+P">Prateeti Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Lokam%2C+S">Satya Lokam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Matching upper bounds, new corollaries for DP variants
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1033">[1033]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17207" title="Abstract">arXiv:2303.17207</a> (replaced) [<a href="/pdf/2303.17207" title="Download PDF">pdf</a>, <a href="/format/2303.17207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Redundancy for UWB Anomaly Detection in Infrastructure-Free  Multi-Robot Relative Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salimpour%2C+S">Sahar Salimpour</a>, 
<a href="/search/cs?searchtype=author&query=Mor%C3%B3n%2C+P+T">Paola Torrico Mor&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xianjia Yu</a>, 
<a href="/search/cs?searchtype=author&query=Westerlund%2C+T">Tomi Westerlund</a>, 
<a href="/search/cs?searchtype=author&query=Queralta%2C+J+P">Jorge Pe&#xf1;a Queralta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1034">[1034]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17476" title="Abstract">arXiv:2303.17476</a> (replaced) [<a href="/pdf/2303.17476" title="Download PDF">pdf</a>, <a href="/format/2303.17476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable Compliant Contact Primitives for Estimation and Model  Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haninger%2C+K">Kevin Haninger</a>, 
<a href="/search/cs?searchtype=author&query=Samuel%2C+K">Kangwagye Samuel</a>, 
<a href="/search/cs?searchtype=author&query=Rozzi%2C+F">Filippo Rozzi</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Sehoon Oh</a>, 
<a href="/search/cs?searchtype=author&query=Roveda%2C+L">Loris Roveda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted ICRA24. Video available at <a href="https://youtu.be/CuCTcmn3H-o">this https URL</a> Code available at <a href="https://gitlab.cc-asp.fraunhofer.de/hanikevi/contact_mpc">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1035">[1035]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.18051" title="Abstract">arXiv:2303.18051</a> (replaced) [<a href="/pdf/2303.18051" title="Download PDF">pdf</a>, <a href="/format/2303.18051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synergistic Graph Fusion via Encoder Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Cencheng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Priebe%2C+C+E">Carey E. Priebe</a>, 
<a href="/search/cs?searchtype=author&query=Larson%2C+J">Jonathan Larson</a>, 
<a href="/search/cs?searchtype=author&query=Trinh%2C+H">Ha Trinh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1036">[1036]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00501" title="Abstract">arXiv:2304.00501</a> (replaced) [<a href="/pdf/2304.00501" title="Download PDF">pdf</a>, <a href="/format/2304.00501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Review of YOLO: From YOLOv1 and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Terven%2C+J">Juan Terven</a>, 
<a href="/search/cs?searchtype=author&query=Cordova-Esparza%2C+D">Diana Cordova-Esparza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 19 figures, 4 tables, submitted to ACM Computing Surveys. This version adds information about YOLO with transformers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1037">[1037]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00652" title="Abstract">arXiv:2304.00652</a> (replaced) [<a href="/pdf/2304.00652" title="Download PDF">pdf</a>, <a href="/format/2304.00652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meeting effectiveness and inclusiveness: large-scale measurement,  identification of key features, and prediction in real-world remote meetings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hosseinkashi%2C+Y">Yasaman Hosseinkashi</a>, 
<a href="/search/cs?searchtype=author&query=Pool%2C+J">Jamie Pool</a>, 
<a href="/search/cs?searchtype=author&query=Tankelevitch%2C+L">Lev Tankelevitch</a>, 
<a href="/search/cs?searchtype=author&query=Cutler%2C+R">Ross Cutler</a>, 
<a href="/search/cs?searchtype=author&query=Madan%2C+C">Chinmaya Madan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item1038">[1038]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01405" title="Abstract">arXiv:2304.01405</a> (replaced) [<a href="/pdf/2304.01405" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Work Avatar Face-Off: Knowledge Worker Preferences for Realism in  Meetings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Phadnis%2C+V">Vrushank Phadnis</a>, 
<a href="/search/cs?searchtype=author&query=Moore%2C+K">Kristin Moore</a>, 
<a href="/search/cs?searchtype=author&query=Franco%2C+M+G">Mar Gonzalez Franco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, accepted at ISMAR 2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1039">[1039]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01933" title="Abstract">arXiv:2304.01933</a> (replaced) [<a href="/pdf/2304.01933" title="Download PDF">pdf</a>, <a href="/format/2304.01933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhiqiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yihuai Lan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wanyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+E">Ee-Peng Lim</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Poria%2C+S">Soujanya Poria</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+R+K">Roy Ka-Wei Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023. The code of our framework can be found at <a href="https://github.com/AGI-Edgerunners/LLM-Adapters.">this https URL</a> We will keep all of the code open-source and continue to update the framework with new adapters, LLMs, and tasks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1040">[1040]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03548" title="Abstract">arXiv:2304.03548</a> (replaced) [<a href="/pdf/2304.03548" title="Download PDF">pdf</a>, <a href="/format/2304.03548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GEMINI: Controlling the Sentence-level Writing Style for Abstractive  Text Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+G">Guangsheng Bao</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+Z">Zebin Ou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP2023 main conference. 8 pages, 5 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1041">[1041]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03773" title="Abstract">arXiv:2304.03773</a> (replaced) [<a href="/pdf/2304.03773" title="Download PDF">pdf</a>, <a href="/format/2304.03773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe Explicable Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanni%2C+A">Akkamahadevi Hanni</a>, 
<a href="/search/cs?searchtype=author&query=Boateng%2C+A">Andrew Boateng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1042">[1042]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04704" title="Abstract">arXiv:2304.04704</a> (replaced) [<a href="/pdf/2304.04704" title="Download PDF">pdf</a>, <a href="/format/2304.04704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary  Visual Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shuhuai Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Aston Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shuai Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mu Li</a>, 
<a href="/search/cs?searchtype=author&query=Smola%2C+A">Alex Smola</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xu Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/amazon-science/prompt-pretraining">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1043">[1043]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04819" title="Abstract">arXiv:2304.04819</a> (replaced) [<a href="/pdf/2304.04819" title="Download PDF">pdf</a>, <a href="/format/2304.04819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recent Advancements in Machine Learning For Cybercrime Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elluri%2C+L">Lavanya Elluri</a>, 
<a href="/search/cs?searchtype=author&query=Mandalapu%2C+V">Varun Mandalapu</a>, 
<a href="/search/cs?searchtype=author&query=Vyas%2C+P">Piyush Vyas</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+N">Nirmalya Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Journal of Computer Information Systems, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1044">[1044]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05302" title="Abstract">arXiv:2304.05302</a> (replaced) [<a href="/pdf/2304.05302" title="Download PDF">pdf</a>, <a href="/format/2304.05302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RRHF: Rank Responses to Align Language Models with Human Feedback  without tears
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Hongyi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Chuanqi Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Songfang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ArXiv version For NeurIPS 2023 accepted paper: RRHF: Rank Responses to Align Language Models with Human Feedback
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1045">[1045]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05538" title="Abstract">arXiv:2304.05538</a> (replaced) [<a href="/pdf/2304.05538" title="Download PDF">pdf</a>, <a href="/format/2304.05538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of  Zoom and Spatial Biases in Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taesiri%2C+M+R">Mohammad Reza Taesiri</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+G">Giang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Habchi%2C+S">Sarra Habchi</a>, 
<a href="/search/cs?searchtype=author&query=Bezemer%2C+C">Cor-Paul Bezemer</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A">Anh Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Track on Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1046">[1046]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06054" title="Abstract">arXiv:2304.06054</a> (replaced) [<a href="/e-print/2304.06054" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Landslide Susceptibility Prediction Modeling Based on Self-Screening  Deep Learning Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Li Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lekai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Changshi Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Some contributing authors are not signed
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Geophysics (physics.geo-ph)

</div>
</div>
</dd>
<dt><a name="item1047">[1047]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06385" title="Abstract">arXiv:2304.06385</a> (replaced) [<a href="/pdf/2304.06385" title="Download PDF">pdf</a>, <a href="/format/2304.06385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransHP: Image Classification with Hierarchical Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yifan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1048">[1048]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09418" title="Abstract">arXiv:2304.09418</a> (replaced) [<a href="/pdf/2304.09418" title="Download PDF">pdf</a>, <a href="/format/2304.09418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hidden convexity in the heat, linear transport, and Euler&#x27;s rigid body  equations: A computational approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kouskiya%2C+U">Uditnarayan Kouskiya</a>, 
<a href="/search/math?searchtype=author&query=Acharya%2C+A">Amit Acharya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1049">[1049]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09423" title="Abstract">arXiv:2304.09423</a> (replaced) [<a href="/pdf/2304.09423" title="Download PDF">pdf</a>, <a href="/format/2304.09423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASM: Adaptive Skinning Model for High-Quality 3D Face Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+H">Hong Shang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+T">Tianyang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinghan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingkai Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhongqian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wei Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1050">[1050]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10151" title="Abstract">arXiv:2304.10151</a> (replaced) [<a href="/pdf/2304.10151" title="Download PDF">pdf</a>, <a href="/format/2304.10151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flexible K Nearest Neighbors Classifier: Derivation and Application for  Ion-mobility Spectrometry-based Indoor Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+P">Philipp M&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 figures, paper presented at the 2023 International Conference on Indoor Positioning and Indoor Navigation (IPIN)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1051">[1051]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10428" title="Abstract">arXiv:2304.10428</a> (replaced) [<a href="/pdf/2304.10428" title="Download PDF">pdf</a>, <a href="/format/2304.10428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-NER: Named Entity Recognition via Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaofei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoya Li</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+R">Rongbin Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoyin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1052">[1052]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13202" title="Abstract">arXiv:2304.13202</a> (replaced) [<a href="/pdf/2304.13202" title="Download PDF">pdf</a>, <a href="/format/2304.13202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernel Methods are Competitive for Operator Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Batlle%2C+P">Pau Batlle</a>, 
<a href="/search/stat?searchtype=author&query=Darcy%2C+M">Matthieu Darcy</a>, 
<a href="/search/stat?searchtype=author&query=Hosseini%2C+B">Bamdad Hosseini</a>, 
<a href="/search/stat?searchtype=author&query=Owhadi%2C+H">Houman Owhadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item1053">[1053]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14530" title="Abstract">arXiv:2304.14530</a> (replaced) [<a href="/pdf/2304.14530" title="Download PDF">pdf</a>, <a href="/format/2304.14530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating images of rare concepts using pre-trained diffusion models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Samuel%2C+D">Dvir Samuel</a>, 
<a href="/search/cs?searchtype=author&query=Ben-Ari%2C+R">Rami Ben-Ari</a>, 
<a href="/search/cs?searchtype=author&query=Raviv%2C+S">Simon Raviv</a>, 
<a href="/search/cs?searchtype=author&query=Darshan%2C+N">Nir Darshan</a>, 
<a href="/search/cs?searchtype=author&query=Chechik%2C+G">Gal Chechik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1054">[1054]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14997" title="Abstract">arXiv:2304.14997</a> (replaced) [<a href="/pdf/2304.14997" title="Download PDF">pdf</a>, <a href="/format/2304.14997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Automated Circuit Discovery for Mechanistic Interpretability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Conmy%2C+A">Arthur Conmy</a>, 
<a href="/search/cs?searchtype=author&query=Mavor-Parker%2C+A+N">Augustine N. Mavor-Parker</a>, 
<a href="/search/cs?searchtype=author&query=Lynch%2C+A">Aengus Lynch</a>, 
<a href="/search/cs?searchtype=author&query=Heimersheim%2C+S">Stefan Heimersheim</a>, 
<a href="/search/cs?searchtype=author&query=Garriga-Alonso%2C+A">Adri&#xe0; Garriga-Alonso</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a spotlight at NeurIPS 2023. We are working on an updated manuscript that fixed a bug with the very low 16H performance
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1055">[1055]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01420" title="Abstract">arXiv:2305.01420</a> (replaced) [<a href="/pdf/2305.01420" title="Download PDF">pdf</a>, <a href="/format/2305.01420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Subquadratic Bound for Online Bisection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bienkowski%2C+M">Marcin Bienkowski</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+S">Stefan Schmid</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Improved competitive ratio + a roadmap section
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item1056">[1056]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01498" title="Abstract">arXiv:2305.01498</a> (replaced) [<a href="/pdf/2305.01498" title="Download PDF">pdf</a>, <a href="/format/2305.01498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Summarizing Multiple Documents with Conversational Structure for  Meta-Review Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Miao Li</a>, 
<a href="/search/cs?searchtype=author&query=Hovy%2C+E">Eduard Hovy</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+J+H">Jey Han Lau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Long paper; Accepted to EMNLP 2023; Soundness: 3, 3, 4; Excitement: 3, 4, 4
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1057">[1057]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02034" title="Abstract">arXiv:2305.02034</a> (replaced) [<a href="/pdf/2305.02034" title="Download PDF">pdf</a>, <a href="/format/2305.02034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAMRS: Scaling-up Remote Sensing Segmentation Dataset with Segment  Anything Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Di Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bo Du</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minqiang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liangpei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023 Datasets and Benchmarks Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1058">[1058]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02526" title="Abstract">arXiv:2305.02526</a> (replaced) [<a href="/pdf/2305.02526" title="Download PDF">pdf</a>, <a href="/format/2305.02526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prefix Sorting DFAs: a Recursive Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cotumaccio%2C+N">Nicola Cotumaccio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item1059">[1059]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02722" title="Abstract">arXiv:2305.02722</a> (replaced) [<a href="/pdf/2305.02722" title="Download PDF">pdf</a>, <a href="/format/2305.02722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Avatar Knowledge Distillation: Self-ensemble Teacher Paradigm with  Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weihua Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yichen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiuyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jian Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1060">[1060]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04170" title="Abstract">arXiv:2305.04170</a> (replaced) [<a href="/pdf/2305.04170" title="Download PDF">pdf</a>, <a href="/format/2305.04170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YOLOCS: Object Detection based on Dense Channel Compression for Feature  Spatial Solidification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weisheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Linlin Shen</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Haojie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xue Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Suihan Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1061">[1061]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05004" title="Abstract">arXiv:2305.05004</a> (replaced) [<a href="/pdf/2305.05004" title="Download PDF">pdf</a>, <a href="/format/2305.05004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How WEIRD is Usable Privacy and Security Research? (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hasegawa%2C+A+A">Ayako A. Hasegawa</a>, 
<a href="/search/cs?searchtype=author&query=Inoue%2C+D">Daisuke Inoue</a>, 
<a href="/search/cs?searchtype=author&query=Akiyama%2C+M">Mitsuaki Akiyama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is the extended version of the paper presented at USENIX SECURITY 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1062">[1062]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05181" title="Abstract">arXiv:2305.05181</a> (replaced) [<a href="/pdf/2305.05181" title="Download PDF">pdf</a>, <a href="/format/2305.05181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoT: Memory-of-Thought Enables ChatGPT to Self-Improve
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaonan Li</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to appear at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1063">[1063]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05455" title="Abstract">arXiv:2305.05455</a> (replaced) [<a href="/pdf/2305.05455" title="Download PDF">pdf</a>, <a href="/format/2305.05455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ONCache: A Cache-Based Low-Overhead Container Overlay Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Shengkai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shizhen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+P">Peirui Cao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xinchi Han</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Quan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenfeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Donghai Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinbing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chenghu Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Operating Systems (cs.OS)

</div>
</div>
</dd>
<dt><a name="item1064">[1064]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05640" title="Abstract">arXiv:2305.05640</a> (replaced) [<a href="/pdf/2305.05640" title="Download PDF">pdf</a>, <a href="/format/2305.05640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representation Learning for Person or Entity-centric Knowledge Graphs:  An Application in Healthcare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Theodoropoulos%2C+C">Christos Theodoropoulos</a>, 
<a href="/search/cs?searchtype=author&query=Mulligan%2C+N">Natasha Mulligan</a>, 
<a href="/search/cs?searchtype=author&query=Stappenbeck%2C+T">Thaddeus Stappenbeck</a>, 
<a href="/search/cs?searchtype=author&query=Bettencourt-Silva%2C+J">Joao Bettencourt-Silva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted into the Twelfth International Conference on Knowledge Capture (K-CAP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1065">[1065]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06018" title="Abstract">arXiv:2305.06018</a> (replaced) [<a href="/pdf/2305.06018" title="Download PDF">pdf</a>, <a href="/format/2305.06018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TARGET: Automated Scenario Generation from Traffic Rules for Testing  Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiaohong Yao</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhi Tu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengshi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item1066">[1066]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06176" title="Abstract">arXiv:2305.06176</a> (replaced) [<a href="/pdf/2305.06176" title="Download PDF">pdf</a>, <a href="/format/2305.06176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-tuning Language Models with Generative Adversarial Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z+Z">Zhang Ze Yu</a>, 
<a href="/search/cs?searchtype=author&query=Jaw%2C+L+J">Lau Jia Jaw</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W+Q">Wong Qin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+Z">Zhang Hui</a>, 
<a href="/search/cs?searchtype=author&query=Low%2C+B+K+H">Bryan Kian Hsiang Low</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 7 figures, 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1067">[1067]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06311" title="Abstract">arXiv:2305.06311</a> (replaced) [<a href="/pdf/2305.06311" title="Download PDF">pdf</a>, <a href="/format/2305.06311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Evaluation of Attribution by Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiang Yue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boshi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziru Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huan Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1068">[1068]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06361" title="Abstract">arXiv:2305.06361</a> (replaced) [<a href="/pdf/2305.06361" title="Download PDF">pdf</a>, <a href="/format/2305.06361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Training of Multi-task Combinarotial Neural Solver with  Multi-armed Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenguang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tianshu Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1069">[1069]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07208" title="Abstract">arXiv:2305.07208</a> (replaced) [<a href="/pdf/2305.07208" title="Download PDF">pdf</a>, <a href="/format/2305.07208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Differential Privacy Work for Census Data Users
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McCartan%2C+C">Cory McCartan</a>, 
<a href="/search/cs?searchtype=author&query=Simko%2C+T">Tyler Simko</a>, 
<a href="/search/cs?searchtype=author&query=Imai%2C+K">Kosuke Imai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item1070">[1070]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08377" title="Abstract">arXiv:2305.08377</a> (replaced) [<a href="/pdf/2305.08377" title="Download PDF">pdf</a>, <a href="/format/2305.08377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Classification via Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaofei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoya Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Shangwei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoyin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1071">[1071]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08459" title="Abstract">arXiv:2305.08459</a> (replaced) [<a href="/pdf/2305.08459" title="Download PDF">pdf</a>, <a href="/format/2305.08459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Introduction to dynamical mean-field theory of randomly connected neural  networks with bidirectionally correlated couplings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Zou%2C+W">Wenxuan Zou</a>, 
<a href="/search/cond-mat?searchtype=author&query=Huang%2C+H">Haiping Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 5 figures, 44 references, revised version for SciPost Physics Lecture Notes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Disordered Systems and Neural Networks (cond-mat.dis-nn)</span>; Statistical Mechanics (cond-mat.stat-mech); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item1072">[1072]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08643" title="Abstract">arXiv:2305.08643</a> (replaced) [<a href="/pdf/2305.08643" title="Download PDF">pdf</a>, <a href="/format/2305.08643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quadratic Programming-based Reference Spreading Control for Dual-Arm  Robotic Manipulation with Planned Simultaneous Impacts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Steen%2C+J">Jari van Steen</a>, 
<a href="/search/cs?searchtype=author&query=van+den+Brandt%2C+G">Gijs van den Brandt</a>, 
<a href="/search/cs?searchtype=author&query=van+de+Wouw%2C+N">Nathan van de Wouw</a>, 
<a href="/search/cs?searchtype=author&query=Kober%2C+J">Jens Kober</a>, 
<a href="/search/cs?searchtype=author&query=Saccon%2C+A">Alessandro Saccon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 10 figures. Submitted for publication to IEEE Transactions on Robotics (T-RO) in September, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1073">[1073]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08685" title="Abstract">arXiv:2305.08685</a> (replaced) [<a href="/pdf/2305.08685" title="Download PDF">pdf</a>, <a href="/format/2305.08685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+L">Linhui Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoshan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+F">Fang Peng</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Changsheng Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transaction on Multimedia (2023), Paper page: <a href="https://ieeexplore.ieee.org/abstract/document/10269126.">this https URL</a> Code will be released at <a href="https://github.com/linhuixiao/CLIP-VG">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1074">[1074]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08732" title="Abstract">arXiv:2305.08732</a> (replaced) [<a href="/pdf/2305.08732" title="Download PDF">pdf</a>, <a href="/format/2305.08732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Rumination for Pre-trained Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yunzhi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+S">Shengyu Mao</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Chuanqi Tan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1075">[1075]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08851" title="Abstract">arXiv:2305.08851</a> (replaced) [<a href="/pdf/2305.08851" title="Download PDF">pdf</a>, <a href="/format/2305.08851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MV-Map: Offboard HD-Map Generation with Multi-view Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Ziyang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Z">Ziqi Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1076">[1076]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09549" title="Abstract">arXiv:2305.09549</a> (replaced) [<a href="/pdf/2305.09549" title="Download PDF">pdf</a>, <a href="/format/2305.09549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable Dinner Party Seating Arrangements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berriaud%2C+D">Damien Berriaud</a>, 
<a href="/search/cs?searchtype=author&query=Constantinescu%2C+A">Andrei Constantinescu</a>, 
<a href="/search/cs?searchtype=author&query=Wattenhofer%2C+R">Roger Wattenhofer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in WINE'23; preliminary version presented at COMSOC'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item1077">[1077]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10613" title="Abstract">arXiv:2305.10613</a> (replaced) [<a href="/pdf/2305.10613" title="Download PDF">pdf</a>, <a href="/format/2305.10613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Knowledge Graph Forecasting Using In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dong-Ho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ahrabian%2C+K">Kian Ahrabian</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Woojeong Jin</a>, 
<a href="/search/cs?searchtype=author&query=Morstatter%2C+F">Fred Morstatter</a>, 
<a href="/search/cs?searchtype=author&query=Pujara%2C+J">Jay Pujara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1078">[1078]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10731" title="Abstract">arXiv:2305.10731</a> (replaced) [<a href="/pdf/2305.10731" title="Download PDF">pdf</a>, <a href="/format/2305.10731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Norm Violations in Live-Stream Chat
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moon%2C+J">Jihyung Moon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dong-Ho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+H">Hyundong Cho</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Woojeong Jin</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C+Y">Chan Young Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=May%2C+J">Jonathan May</a>, 
<a href="/search/cs?searchtype=author&query=Pujara%2C+J">Jay Pujara</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sungjoon Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 8 figures, 15 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1079">[1079]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10906" title="Abstract">arXiv:2305.10906</a> (replaced) [<a href="/pdf/2305.10906" title="Download PDF">pdf</a>, <a href="/format/2305.10906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RobustFair: Adversarial Evaluation through Fairness Confusion Directed  Gradient Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuran Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Peng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+K">Kaixiang Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yanting Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1080">[1080]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11019" title="Abstract">arXiv:2305.11019</a> (replaced) [<a href="/pdf/2305.11019" title="Download PDF">pdf</a>, <a href="/format/2305.11019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Annotation-free Audio-Visual Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinxiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+C">Chen Ju</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chaofan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weidi Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera-ready version for WACV 2024; project page is <a href="https://jinxiang-liu.github.io/anno-free-AVS/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1081">[1081]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11366" title="Abstract">arXiv:2305.11366</a> (replaced) [<a href="/pdf/2305.11366" title="Download PDF">pdf</a>, <a href="/format/2305.11366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoTrial: Prompting Language Models for Clinical Trial Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Cao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jimeng Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1082">[1082]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11577" title="Abstract">arXiv:2305.11577</a> (replaced) [<a href="/pdf/2305.11577" title="Download PDF">pdf</a>, <a href="/format/2305.11577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing Text-to-Image Attention Prior for Reference-based Multi-view  Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+C">Chenjie Cao</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yunuo Cai</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Q">Qiaole Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yikai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yanwei Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We further improved our methods for multi-view synthesis. The project page is <a href="https://ewrfcas.github.io/ARCI/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1083">[1083]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12333" title="Abstract">arXiv:2305.12333</a> (replaced) [<a href="/pdf/2305.12333" title="Download PDF">pdf</a>, <a href="/format/2305.12333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRACE: Loss-Resilient Real-Time Video through Neural Codecs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yihua Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hanchen Li</a>, 
<a href="/search/cs?searchtype=author&query=Arapin%2C+A">Anton Arapin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qizheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuhan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+F+Y">Francis Y. Yan</a>, 
<a href="/search/cs?searchtype=author&query=Mazumdar%2C+A">Amrita Mazumdar</a>, 
<a href="/search/cs?searchtype=author&query=Feamster%2C+N">Nick Feamster</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junchen Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Artificial Intelligence (cs.AI); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item1084">[1084]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12530" title="Abstract">arXiv:2305.12530</a> (replaced) [<a href="/pdf/2305.12530" title="Download PDF">pdf</a>, <a href="/format/2305.12530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Family-Infant Audio Analysis Based on Unsupervised  Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jialu Li</a>, 
<a href="/search/eess?searchtype=author&query=Hasegawa-Johnson%2C+M">Mark Hasegawa-Johnson</a>, 
<a href="/search/eess?searchtype=author&query=McElwain%2C+N+L">Nancy L. McElwain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of Interspeech 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item1085">[1085]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12653" title="Abstract">arXiv:2305.12653</a> (replaced) [<a href="/pdf/2305.12653" title="Download PDF">pdf</a>, <a href="/format/2305.12653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Discrete Total Curvature with Per Triangle Normal Variation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C+H">Crane He Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computational Geometry (cs.CG); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1086">[1086]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12878" title="Abstract">arXiv:2305.12878</a> (replaced) [<a href="/pdf/2305.12878" title="Download PDF">pdf</a>, <a href="/format/2305.12878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Autoregressive Document-Level Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+G">Guangsheng Bao</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+Z">Zhiyang Teng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jianhao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP2023 Findings. Review soundness 443 and excitement 443
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1087">[1087]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12952" title="Abstract">arXiv:2305.12952</a> (replaced) [<a href="/pdf/2305.12952" title="Download PDF">pdf</a>, <a href="/ps/2305.12952" title="Download PostScript">ps</a>, <a href="/format/2305.12952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the capacity of TDMA downlink with a reconfigurable intelligent  surface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Darsena%2C+D">Donatella Darsena</a>, 
<a href="/search/cs?searchtype=author&query=Verde%2C+F">Francesco Verde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE Communications Letters. Cite as: D. Darsena and F. Verde, "On the capacity of TDMA downlink with a reconfigurable intelligent surface," in IEEE Communications Letters, 2023, doi: 10.1109/LCOMM.2023.3323094
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Communications Letters 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1088">[1088]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13225" title="Abstract">arXiv:2305.13225</a> (replaced) [<a href="/pdf/2305.13225" title="Download PDF">pdf</a>, <a href="/format/2305.13225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A  Preliminary Study on Writing Assistance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Leyang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Deng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xinting Huang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+T">Tao Fang</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+W">Wei Bi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1089">[1089]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13230" title="Abstract">arXiv:2305.13230</a> (replaced) [<a href="/pdf/2305.13230" title="Download PDF">pdf</a>, <a href="/format/2305.13230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+F">Fuzhao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wangchunshu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zangwei Zheng</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1090">[1090]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13257" title="Abstract">arXiv:2305.13257</a> (replaced) [<a href="/pdf/2305.13257" title="Download PDF">pdf</a>, <a href="/format/2305.13257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Watermarking Classification Dataset for Copyright Protection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hongsheng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1091">[1091]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13829" title="Abstract">arXiv:2305.13829</a> (replaced) [<a href="/pdf/2305.13829" title="Download PDF">pdf</a>, <a href="/format/2305.13829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from Mistakes via Interactive Study Assistant for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Danqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1092">[1092]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13993" title="Abstract">arXiv:2305.13993</a> (replaced) [<a href="/pdf/2305.13993" title="Download PDF">pdf</a>, <a href="/format/2305.13993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Condensing Multilingual Knowledge with Lightweight Language-Specific  Modules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haoran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+W">Weiting Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+S">Shuyue Stella Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunmo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>, 
<a href="/search/cs?searchtype=author&query=Koehn%2C+P">Philipp Koehn</a>, 
<a href="/search/cs?searchtype=author&query=Murray%2C+K">Kenton Murray</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the main conference of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1093">[1093]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14259" title="Abstract">arXiv:2305.14259</a> (replaced) [<a href="/pdf/2305.14259" title="Download PDF">pdf</a>, <a href="/format/2305.14259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Generate Novel Scientific Directions with Contextualized  Literature-based Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qingyun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Downey%2C+D">Doug Downey</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Hope%2C+T">Tom Hope</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages. Code and resource is available at <a href="https://github.com/EagleW/CLBD">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1094">[1094]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14282" title="Abstract">arXiv:2305.14282</a> (replaced) [<a href="/pdf/2305.14282" title="Download PDF">pdf</a>, <a href="/format/2305.14282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained  Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenda Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Danqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liangming Pan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhenqiao Song</a>, 
<a href="/search/cs?searchtype=author&query=Freitag%2C+M">Markus Freitag</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1095">[1095]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14318" title="Abstract">arXiv:2305.14318</a> (replaced) [<a href="/pdf/2305.14318" title="Download PDF">pdf</a>, <a href="/format/2305.14318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning  of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Cheng Qian</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chi Han</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+Y+R">Yi R. Fung</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yujia Qin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1096">[1096]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14493" title="Abstract">arXiv:2305.14493</a> (replaced) [<a href="/pdf/2305.14493" title="Download PDF">pdf</a>, <a href="/format/2305.14493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt position really matters in few-shot and zero-shot NLU tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Junyu Mao</a>, 
<a href="/search/cs?searchtype=author&query=Middleton%2C+S+E">Stuart E. Middleton</a>, 
<a href="/search/cs?searchtype=author&query=Niranjan%2C+M">Mahesan Niranjan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1097">[1097]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14761" title="Abstract">arXiv:2305.14761</a> (replaced) [<a href="/pdf/2305.14761" title="Download PDF">pdf</a>, <a href="/format/2305.14761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniChart: A Universal Vision-language Pretrained Model for Chart  Comprehension and Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masry%2C+A">Ahmed Masry</a>, 
<a href="/search/cs?searchtype=author&query=Kavehzadeh%2C+P">Parsa Kavehzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+X+L">Xuan Long Do</a>, 
<a href="/search/cs?searchtype=author&query=Hoque%2C+E">Enamul Hoque</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1098">[1098]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14835" title="Abstract">arXiv:2305.14835</a> (replaced) [<a href="/pdf/2305.14835" title="Download PDF">pdf</a>, <a href="/format/2305.14835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SummIt: Iterative Text Summarization via ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haopeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiawei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1099">[1099]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14951" title="Abstract">arXiv:2305.14951</a> (replaced) [<a href="/pdf/2305.14951" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DSFFNet: Dual-Side Feature Fusion Network for 3D Pose Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jue Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in Chinese language
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1100">[1100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15187" title="Abstract">arXiv:2305.15187</a> (replaced) [<a href="/pdf/2305.15187" title="Download PDF">pdf</a>, <a href="/format/2305.15187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Models Based on Cognitive Theory to Predict Human Behavior in  Traffic: A Case Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schumann%2C+J+F">Julian F. Schumann</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+A+R">Aravinda Ramakrishnan Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Kober%2C+J">Jens Kober</a>, 
<a href="/search/cs?searchtype=author&query=Markkula%2C+G">Gustav Markkula</a>, 
<a href="/search/cs?searchtype=author&query=Zgonnikov%2C+A">Arkady Zgonnikov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1101">[1101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15729" title="Abstract">arXiv:2305.15729</a> (replaced) [<a href="/pdf/2305.15729" title="Download PDF">pdf</a>, <a href="/format/2305.15729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerated K-Serial Stable Coalition for Dynamic Capture and Resource  Defense
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junfeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zili Tang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Meng Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 10 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1102">[1102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16532" title="Abstract">arXiv:2305.16532</a> (replaced) [<a href="/pdf/2305.16532" title="Download PDF">pdf</a>, <a href="/format/2305.16532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Explainer Framework for Deep Reinforcement Learning  Models Using Policy Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Samadi%2C+A">Amir Samadi</a>, 
<a href="/search/cs?searchtype=author&query=Koufos%2C+K">Konstantinos Koufos</a>, 
<a href="/search/cs?searchtype=author&query=Debattista%2C+K">Kurt Debattista</a>, 
<a href="/search/cs?searchtype=author&query=Dianati%2C+M">Mehrdad Dianati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1103">[1103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16646" title="Abstract">arXiv:2305.16646</a> (replaced) [<a href="/pdf/2305.16646" title="Download PDF">pdf</a>, <a href="/format/2305.16646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models Can Improve Event Prediction by Few-Shot Abductive  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiaoming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+S">Siqiao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kangrui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+Y">James Y. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Chenhao Tan</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+H">Hongyuan Mei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1104">[1104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16749" title="Abstract">arXiv:2305.16749</a> (replaced) [<a href="/pdf/2305.16749" title="Download PDF">pdf</a>, <a href="/format/2305.16749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diverse and Expressive Speech Prosody Prediction with Denoising  Diffusion Probabilistic Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Songxiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+M+W+Y">Max W. Y. Lam</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+C">Chao Weng</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+H">Helen Meng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of Interspeech 2023 (doi: 10.21437/Interspeech.2023-715), demo site at <a href="https://thuhcsi.github.io/interspeech2023-DiffVar/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1105">[1105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17010" title="Abstract">arXiv:2305.17010</a> (replaced) [<a href="/pdf/2305.17010" title="Download PDF">pdf</a>, <a href="/format/2305.17010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Let the Flows Tell: Solving Graph Combinatorial Optimization Problems  with GFlowNets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dinghuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Hanjun Dai</a>, 
<a href="/search/cs?searchtype=author&query=Malkin%2C+N">Nikolay Malkin</a>, 
<a href="/search/cs?searchtype=author&query=Courville%2C+A">Aaron Courville</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Ling Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023 as spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Discrete Mathematics (cs.DM); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1106">[1106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17866" title="Abstract">arXiv:2305.17866</a> (replaced) [<a href="/pdf/2305.17866" title="Download PDF">pdf</a>, <a href="/format/2305.17866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequential Condition Evolved Interaction Knowledge Graph for Traditional  Chinese Medicine Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingjin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+H+H">Hankz Hankui Zhuo</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+K">Kebing Jin</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jiamin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhimin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zhengan Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1107">[1107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18274" title="Abstract">arXiv:2305.18274</a> (replaced) [<a href="/pdf/2305.18274" title="Download PDF">pdf</a>, <a href="/format/2305.18274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstructing the Mind&#x27;s Eye: fMRI-to-Image with Contrastive Learning  and Diffusion Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scotti%2C+P+S">Paul S. Scotti</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+A">Atmadeep Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Goode%2C+J">Jimmie Goode</a>, 
<a href="/search/cs?searchtype=author&query=Shabalin%2C+S">Stepan Shabalin</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A">Alex Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+E">Ethan Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Dempster%2C+A+J">Aidan J. Dempster</a>, 
<a href="/search/cs?searchtype=author&query=Verlinde%2C+N">Nathalie Verlinde</a>, 
<a href="/search/cs?searchtype=author&query=Yundler%2C+E">Elad Yundler</a>, 
<a href="/search/cs?searchtype=author&query=Weisberg%2C+D">David Weisberg</a>, 
<a href="/search/cs?searchtype=author&query=Norman%2C+K+A">Kenneth A. Norman</a>, 
<a href="/search/cs?searchtype=author&query=Abraham%2C+T+M">Tanishq Mathew Abraham</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page at <a href="https://medarc.ai/mindeye.">this https URL</a> Code at <a href="https://github.com/MedARC-AI/fMRI-reconstruction-NSD/.">this https URL</a> Published as a conference paper at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item1108">[1108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18355" title="Abstract">arXiv:2305.18355</a> (replaced) [<a href="/pdf/2305.18355" title="Download PDF">pdf</a>, <a href="/format/2305.18355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Membership Inference Attack for the Diffusion Model by  Proximal Initialization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+F">Fei Kong</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jinhao Duan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+R">RuiPeng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Hengtao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaofeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiaoshuang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kaidi Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1109">[1109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18414" title="Abstract">arXiv:2305.18414</a> (replaced) [<a href="/pdf/2305.18414" title="Download PDF">pdf</a>, <a href="/format/2305.18414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StEik: Stabilizing the Optimization of Neural Signed Distance Functions  and Finer Shape Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huizong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuxin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Sundaramoorthi%2C+G">Ganesh Sundaramoorthi</a>, 
<a href="/search/cs?searchtype=author&query=Yezzi%2C+A">Anthony Yezzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item1110">[1110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18460" title="Abstract">arXiv:2305.18460</a> (replaced) [<a href="/pdf/2305.18460" title="Download PDF">pdf</a>, <a href="/format/2305.18460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimum Width of Leaky-ReLU Neural Networks for Uniform Universal  Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Li&#x27;ang Li</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Y">Yifei Duan</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+G">Guanghua Ji</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yongqiang Cai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fixed known mistakes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item1111">[1111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18500" title="Abstract">arXiv:2305.18500</a> (replaced) [<a href="/pdf/2305.18500" title="Download PDF">pdf</a>, <a href="/format/2305.18500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and  Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sihan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Handong Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qunbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zijia Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingzhen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinxin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1112">[1112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18507" title="Abstract">arXiv:2305.18507</a> (replaced) [<a href="/pdf/2305.18507" title="Download PDF">pdf</a>, <a href="/format/2305.18507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haotong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhouchen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muhan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1113">[1113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18543" title="Abstract">arXiv:2305.18543</a> (replaced) [<a href="/pdf/2305.18543" title="Download PDF">pdf</a>, <a href="/format/2305.18543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Lipschitz Bandits to Adversarial Corruptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yue Kang</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+T+C+M">Thomas C. M. Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1114">[1114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18829" title="Abstract">arXiv:2305.18829</a> (replaced) [<a href="/pdf/2305.18829" title="Download PDF">pdf</a>, <a href="/format/2305.18829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniScene: Multi-Camera Unified Pre-training via 3D Scene Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Min%2C+C">Chen Min</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+L">Liang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dawei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+Y">Yiming Nie</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+B">Bin Dai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1115">[1115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18907" title="Abstract">arXiv:2305.18907</a> (replaced) [<a href="/pdf/2305.18907" title="Download PDF">pdf</a>, <a href="/format/2305.18907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multitask learning for recognizing stress and depression in social media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ilias%2C+L">Loukas Ilias</a>, 
<a href="/search/cs?searchtype=author&query=Askounis%2C+D">Dimitris Askounis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Online Social Networks and Media
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1116">[1116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18980" title="Abstract">arXiv:2305.18980</a> (replaced) [<a href="/pdf/2305.18980" title="Download PDF">pdf</a>, <a href="/format/2305.18980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-modal Queried Object Detection in the Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yifan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengdan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Chaoyou Fu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Peixian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoshan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Ke Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Changsheng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1117">[1117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19187" title="Abstract">arXiv:2305.19187</a> (replaced) [<a href="/pdf/2305.19187" title="Download PDF">pdf</a>, <a href="/format/2305.19187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating with Confidence: Uncertainty Quantification for Black-box  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+S">Shubhendu Trivedi</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jimeng Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1118">[1118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00412" title="Abstract">arXiv:2306.00412</a> (replaced) [<a href="/pdf/2306.00412" title="Download PDF">pdf</a>, <a href="/ps/2306.00412" title="Download PostScript">ps</a>, <a href="/format/2306.00412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beamforming Design for IRS-and-UAV-aided Two-way Amplify-and-Forward  Relay Maritime Communication Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuehui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+F">Feng Shu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuanyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shihao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yifan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qiankun Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiangzhou Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1119">[1119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00619" title="Abstract">arXiv:2306.00619</a> (replaced) [<a href="/pdf/2306.00619" title="Download PDF">pdf</a>, <a href="/format/2306.00619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> General SIS diffusion process with indirect spreading pathways on a  hypergraph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cui%2C+S">Shaoxuan Cui</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+F">Fangzhou Liu</a>, 
<a href="/search/eess?searchtype=author&query=Jard%C3%B3n-Kojakhmetov%2C+H">Hildeberto Jard&#xf3;n-Kojakhmetov</a>, 
<a href="/search/eess?searchtype=author&query=Cao%2C+M">Ming Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1120">[1120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00977" title="Abstract">arXiv:2306.00977</a> (replaced) [<a href="/pdf/2306.00977" title="Download PDF">pdf</a>, <a href="/format/2306.00977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+Y">Yuanwen Yue</a>, 
<a href="/search/cs?searchtype=author&query=Mahadevan%2C+S">Sabarinath Mahadevan</a>, 
<a href="/search/cs?searchtype=author&query=Schult%2C+J">Jonas Schult</a>, 
<a href="/search/cs?searchtype=author&query=Engelmann%2C+F">Francis Engelmann</a>, 
<a href="/search/cs?searchtype=author&query=Leibe%2C+B">Bastian Leibe</a>, 
<a href="/search/cs?searchtype=author&query=Schindler%2C+K">Konrad Schindler</a>, 
<a href="/search/cs?searchtype=author&query=Kontogianni%2C+T">Theodora Kontogianni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://ywyue.github.io/AGILE3D">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1121">[1121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01194" title="Abstract">arXiv:2306.01194</a> (replaced) [<a href="/pdf/2306.01194" title="Download PDF">pdf</a>, <a href="/format/2306.01194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating WebRTC Video QoE Metrics Without Using Application Headers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+T">Taveesh Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Mangla%2C+T">Tarun Mangla</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Arpit Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junchen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Feamster%2C+N">Nick Feamster</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item1122">[1122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01323" title="Abstract">arXiv:2306.01323</a> (replaced) [<a href="/pdf/2306.01323" title="Download PDF">pdf</a>, <a href="/format/2306.01323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demystifying Structural Disparity in Graph Neural Networks: Can One Size  Fit All?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Haitao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhikai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wei Jin</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+H">Haoyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Neil Shah</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 54 pages, 24 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1123">[1123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01711" title="Abstract">arXiv:2306.01711</a> (replaced) [<a href="/pdf/2306.01711" title="Download PDF">pdf</a>, <a href="/format/2306.01711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OMNI: Open-endedness via Models of human Notions of Interestingness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jenny Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lehman%2C+J">Joel Lehman</a>, 
<a href="/search/cs?searchtype=author&query=Stanley%2C+K">Kenneth Stanley</a>, 
<a href="/search/cs?searchtype=author&query=Clune%2C+J">Jeff Clune</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 28 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1124">[1124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02029" title="Abstract">arXiv:2306.02029</a> (replaced) [<a href="/pdf/2306.02029" title="Download PDF">pdf</a>, <a href="/format/2306.02029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-aided Federated Reinforcement Learning for Multi-UAV Trajectory  Planning in IoT Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jichao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Esrafilian%2C+O">Omid Esrafilian</a>, 
<a href="/search/cs?searchtype=author&query=Bayerlein%2C+H">Harald Bayerlein</a>, 
<a href="/search/cs?searchtype=author&query=Gesbert%2C+D">David Gesbert</a>, 
<a href="/search/cs?searchtype=author&query=Caccamo%2C+M">Marco Caccamo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1125">[1125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02105" title="Abstract">arXiv:2306.02105</a> (replaced) [<a href="/pdf/2306.02105" title="Download PDF">pdf</a>, <a href="/format/2306.02105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapting Pretrained ASR Models to Low-resource Clinical Speech using  Epistemic Uncertainty-based Data Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dossou%2C+B+F+P">Bonaventure F. P. Dossou</a>, 
<a href="/search/cs?searchtype=author&query=Tonja%2C+A+L">Atnafu Lambebo Tonja</a>, 
<a href="/search/cs?searchtype=author&query=Emezue%2C+C+C">Chris Chinenye Emezue</a>, 
<a href="/search/cs?searchtype=author&query=Olatunji%2C+T">Tobi Olatunji</a>, 
<a href="/search/cs?searchtype=author&query=Etori%2C+N+A">Naome A Etori</a>, 
<a href="/search/cs?searchtype=author&query=Osei%2C+S">Salomey Osei</a>, 
<a href="/search/cs?searchtype=author&query=Adewumi%2C+T">Tosin Adewumi</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sahib Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1126">[1126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02270" title="Abstract">arXiv:2306.02270</a> (replaced) [<a href="/pdf/2306.02270" title="Download PDF">pdf</a>, <a href="/format/2306.02270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crypto-ransomware Detection through Quantitative API-based Behavioral  Profiling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+W">Wenjia Song</a>, 
<a href="/search/cs?searchtype=author&query=Karanam%2C+S">Sanjula Karanam</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Ya Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Jingyuan Qi</a>, 
<a href="/search/cs?searchtype=author&query=Dautenhahn%2C+N">Nathan Dautenhahn</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+N">Na Meng</a>, 
<a href="/search/cs?searchtype=author&query=Ferrari%2C+E">Elena Ferrari</a>, 
<a href="/search/cs?searchtype=author&query=Danfeng">Danfeng</a> (Daphne)Yao
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1127">[1127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02914" title="Abstract">arXiv:2306.02914</a> (replaced) [<a href="/pdf/2306.02914" title="Download PDF">pdf</a>, <a href="/format/2306.02914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Generating Code: Evaluating GPT on a Data Visualization Course
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu-Tian%2C+C">Chen Zhu-Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianwen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Troidl%2C+J">Jakob Troidl</a>, 
<a href="/search/cs?searchtype=author&query=Warchol%2C+S">Simon Warchol</a>, 
<a href="/search/cs?searchtype=author&query=Beyer%2C+J">Johanna Beyer</a>, 
<a href="/search/cs?searchtype=author&query=Gehlenborg%2C+N">Nils Gehlenborg</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+H">Hanspeter Pfister</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> vis short papge
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1128">[1128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03301" title="Abstract">arXiv:2306.03301</a> (replaced) [<a href="/pdf/2306.03301" title="Download PDF">pdf</a>, <a href="/format/2306.03301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Conditional Mutual Information for Dynamic Feature Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gadgil%2C+S">Soham Gadgil</a>, 
<a href="/search/cs?searchtype=author&query=Covert%2C+I">Ian Covert</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Su-In Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item1129">[1129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03389" title="Abstract">arXiv:2306.03389</a> (replaced) [<a href="/pdf/2306.03389" title="Download PDF">pdf</a>, <a href="/format/2306.03389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phase perturbation improves channel robustness for speech spoofing  countermeasures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zang%2C+Y">Yongyi Zang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">You Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Z">Zhiyao Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages; Proceedings of Interspeech 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1130">[1130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03516" title="Abstract">arXiv:2306.03516</a> (replaced) [<a href="/pdf/2306.03516" title="Download PDF">pdf</a>, <a href="/format/2306.03516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COPR: Consistency-Oriented Pre-Ranking for Online Advertising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhishan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jingyue Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shuguang Han</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+S">Siyuan Lou</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+X">Xiang-Rong Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Han Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuning Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+B">Bo Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1131">[1131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04893" title="Abstract">arXiv:2306.04893</a> (replaced) [<a href="/pdf/2306.04893" title="Download PDF">pdf</a>, <a href="/format/2306.04893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coping with Change: Learning Invariant and Minimum Sufficient  Representations for Fine-Grained Visual Categorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+S">Shuo Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shujian Yu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+W">Wenjin Hou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+X">Xinge You</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1132">[1132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05036" title="Abstract">arXiv:2306.05036</a> (replaced) [<a href="/pdf/2306.05036" title="Download PDF">pdf</a>, <a href="/format/2306.05036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT  and GPT-4 for Mining Insights at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oppenlaender%2C+J">Jonas Oppenlaender</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A4m%C3%A4l%C3%A4inen%2C+J">Joonas H&#xe4;m&#xe4;l&#xe4;inen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 3 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1133">[1133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05221" title="Abstract">arXiv:2306.05221</a> (replaced) [<a href="/pdf/2306.05221" title="Download PDF">pdf</a>, <a href="/format/2306.05221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Steering No-Regret Learners to Optimal Equilibria
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B+H">Brian Hu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Farina%2C+G">Gabriele Farina</a>, 
<a href="/search/cs?searchtype=author&query=Anagnostides%2C+I">Ioannis Anagnostides</a>, 
<a href="/search/cs?searchtype=author&query=Cacciamani%2C+F">Federico Cacciamani</a>, 
<a href="/search/cs?searchtype=author&query=McAleer%2C+S+M">Stephen Marcus McAleer</a>, 
<a href="/search/cs?searchtype=author&query=Haupt%2C+A+A">Andreas Alexander Haupt</a>, 
<a href="/search/cs?searchtype=author&query=Celli%2C+A">Andrea Celli</a>, 
<a href="/search/cs?searchtype=author&query=Gatti%2C+N">Nicola Gatti</a>, 
<a href="/search/cs?searchtype=author&query=Conitzer%2C+V">Vincent Conitzer</a>, 
<a href="/search/cs?searchtype=author&query=Sandholm%2C+T">Tuomas Sandholm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item1134">[1134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05246" title="Abstract">arXiv:2306.05246</a> (replaced) [<a href="/pdf/2306.05246" title="Download PDF">pdf</a>, <a href="/format/2306.05246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mesh-MLP: An all-MLP Architecture for Mesh Classification and Semantic  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Q">Qiujie Dong</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+X">Xiaoran Gong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Rui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zixiong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shuangmin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+S">Shiqing Xin</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+C">Changhe Tu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenping Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1135">[1135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05272" title="Abstract">arXiv:2306.05272</a> (replaced) [<a href="/pdf/2306.05272" title="Download PDF">pdf</a>, <a href="/format/2306.05272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Clustering via the Principle of Rate Reduction in the Age of  Pretrained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+T">Tianzhe Chu</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+S">Shengbang Tong</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+T">Tianjiao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xili Dai</a>, 
<a href="/search/cs?searchtype=author&query=Haeffele%2C+B+D">Benjamin David Haeffele</a>, 
<a href="/search/cs?searchtype=author&query=Vidal%2C+R">Ren&#xe9; Vidal</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yi Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1136">[1136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05716" title="Abstract">arXiv:2306.05716</a> (replaced) [<a href="/pdf/2306.05716" title="Download PDF">pdf</a>, <a href="/format/2306.05716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transferring Foundation Models for Generalizable Robotic Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiange Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+W">Wenhui Tan</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Chuhao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+K">Keling Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jianlong Fu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+R">Ruihua Song</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+G">Gangshan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Limin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1137">[1137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05718" title="Abstract">arXiv:2306.05718</a> (replaced) [<a href="/pdf/2306.05718" title="Download PDF">pdf</a>, <a href="/format/2306.05718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Domain-Aware Detection Head with Prompt Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haochen Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Hantao Yao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xinkai Song</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yifan Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yongwei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Ling Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunji Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1138">[1138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06102" title="Abstract">arXiv:2306.06102</a> (replaced) [<a href="/pdf/2306.06102" title="Download PDF">pdf</a>, <a href="/format/2306.06102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Backup Plan Constrained Model Predictive Control with Guaranteed  Stability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tao%2C+R">Ran Tao</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+H">Hunmin Kim</a>, 
<a href="/search/eess?searchtype=author&query=Yoon%2C+H">Hyung-Jin Yoon</a>, 
<a href="/search/eess?searchtype=author&query=Wan%2C+W">Wenbin Wan</a>, 
<a href="/search/eess?searchtype=author&query=Hovakimyan%2C+N">Naira Hovakimyan</a>, 
<a href="/search/eess?searchtype=author&query=Sha%2C+L">Lui Sha</a>, 
<a href="/search/eess?searchtype=author&query=Voulgaris%2C+P">Petros Voulgaris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1139">[1139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06599" title="Abstract">arXiv:2306.06599</a> (replaced) [<a href="/pdf/2306.06599" title="Download PDF">pdf</a>, <a href="/format/2306.06599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Imbalanced Regression: Fair Uncertainty Quantification via  Probabilistic Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1140">[1140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07207" title="Abstract">arXiv:2306.07207</a> (replaced) [<a href="/pdf/2306.07207" title="Download PDF">pdf</a>, <a href="/format/2306.07207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Valley: Video Assistant with Large Language model Enhanced abilitY
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+R">Ruipu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Ziwang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Min Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Junwei Dong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Da Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+P">Pengcheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Linmei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+M">Minghui Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhongyu Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1141">[1141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07521" title="Abstract">arXiv:2306.07521</a> (replaced) [<a href="/pdf/2306.07521" title="Download PDF">pdf</a>, <a href="/format/2306.07521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Bias and Noise Induced by the U.S. Census Bureau&#x27;s Privacy  Protection Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kenny%2C+C+T">Christopher T. Kenny</a>, 
<a href="/search/cs?searchtype=author&query=Kuriwaki%2C+S">Shiro Kuriwaki</a>, 
<a href="/search/cs?searchtype=author&query=McCartan%2C+C">Cory McCartan</a>, 
<a href="/search/cs?searchtype=author&query=Simko%2C+T">Tyler Simko</a>, 
<a href="/search/cs?searchtype=author&query=Imai%2C+K">Kosuke Imai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 6 figures, 2 tables, plus appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item1142">[1142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07684" title="Abstract">arXiv:2306.07684</a> (replaced) [<a href="/pdf/2306.07684" title="Download PDF">pdf</a>, <a href="/format/2306.07684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lookaround Optimizer: $k$ steps around, 1 step average
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiangtao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shunyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jie Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tongtian Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhengqi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Mingli Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1143">[1143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08193" title="Abstract">arXiv:2306.08193</a> (replaced) [<a href="/pdf/2306.08193" title="Download PDF">pdf</a>, <a href="/format/2306.08193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Operationalising Representation in Natural Language Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harding%2C+J">Jacqueline Harding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Forthcoming in the British Journal for the Philosophy of Science (BJPS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1144">[1144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08804" title="Abstract">arXiv:2306.08804</a> (replaced) [<a href="/pdf/2306.08804" title="Download PDF">pdf</a>, <a href="/format/2306.08804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PEACE: Cross-Platform Hate Speech Detection- A Causality-guided  Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sheth%2C+P">Paras Sheth</a>, 
<a href="/search/cs?searchtype=author&query=Kumarage%2C+T">Tharindu Kumarage</a>, 
<a href="/search/cs?searchtype=author&query=Moraffah%2C+R">Raha Moraffah</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ECML PKDD 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1145">[1145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09286" title="Abstract">arXiv:2306.09286</a> (replaced) [<a href="/pdf/2306.09286" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Source-based Over-The-Air 5G New Radio Sidelink Testbed
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elkadi%2C+M">Melissa Elkadi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Doekseong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+E">Ejaz Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Sadeghi%2C+M">Moein Sadeghi</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+A">Anh Le</a>, 
<a href="/search/cs?searchtype=author&query=Russell%2C+P">Paul Russell</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+B">Bo Ryu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 13 figures, Accepted for MILCOM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1146">[1146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09719" title="Abstract">arXiv:2306.09719</a> (replaced) [<a href="/pdf/2306.09719" title="Download PDF">pdf</a>, <a href="/format/2306.09719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pushing the Limits of ChatGPT on NLP Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaofei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+L">Linfeng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoya Li</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zhen Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+F">Fei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+L">Lingjuan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoyin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1147">[1147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10035" title="Abstract">arXiv:2306.10035</a> (replaced) [<a href="/pdf/2306.10035" title="Download PDF">pdf</a>, <a href="/format/2306.10035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized FDTD Scheme for Moving Electromagnetic Structures with  Arbitrary Space-Time Configurations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bahrami%2C+A">Amir Bahrami</a>, 
<a href="/search/cs?searchtype=author&query=Deck-L%C3%A9ger%2C+Z">Zo&#xe9;-Lise Deck-L&#xe9;ger</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Caloz%2C+C">Christophe Caloz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Classical Physics (physics.class-ph); Computational Physics (physics.comp-ph); Optics (physics.optics)

</div>
</div>
</dd>
<dt><a name="item1148">[1148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10715" title="Abstract">arXiv:2306.10715</a> (replaced) [<a href="/pdf/2306.10715" title="Download PDF">pdf</a>, <a href="/format/2306.10715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximum Entropy Heterogeneous-Agent Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiarong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yifan Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Siyi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Haobo Fu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Q">Qiang Fu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xiaojun Chang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1149">[1149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10839" title="Abstract">arXiv:2306.10839</a> (replaced) [<a href="/pdf/2306.10839" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Perspectives and Systematic Approaches for Analyzing Negative  Damping-Induced Sustained Oscillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhao%2C+C">Chongbin Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Q">Qirong Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE TPEL
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1150">[1150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11260" title="Abstract">arXiv:2306.11260</a> (replaced) [<a href="/pdf/2306.11260" title="Download PDF">pdf</a>, <a href="/format/2306.11260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Counterfactual Data Augmentation Method for Aspect-Based  Sentiment Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Dongming Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+L">Lulu Wen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhaoshu Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera-ready for ACML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1151">[1151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11891" title="Abstract">arXiv:2306.11891</a> (replaced) [<a href="/pdf/2306.11891" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vital Videos: A dataset of face videos with PPG and blood pressure  ground truths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toye%2C+P">Pieter-Jan Toye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1152">[1152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11974" title="Abstract">arXiv:2306.11974</a> (replaced) [<a href="/pdf/2306.11974" title="Download PDF">pdf</a>, <a href="/format/2306.11974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal adversarial perturbations for multiple classification tasks  with quantum classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Qiu%2C+Y">Yun-Zhong Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1153">[1153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12848" title="Abstract">arXiv:2306.12848</a> (replaced) [<a href="/pdf/2306.12848" title="Download PDF">pdf</a>, <a href="/ps/2306.12848" title="Download PostScript">ps</a>, <a href="/format/2306.12848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Direct Construction of MDS and Near-MDS Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+K+C">Kishan Chand Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+S+K">Sumit Kumar Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Samanta%2C+S">Susanta Samanta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item1154">[1154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13491" title="Abstract">arXiv:2306.13491</a> (replaced) [<a href="/pdf/2306.13491" title="Download PDF">pdf</a>, <a href="/format/2306.13491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmenting Sports Videos with VisCommentator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu-Tian%2C+C">Chen Zhu-Tian</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+S">Shuainan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+X">Xiangtong Chu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Haijun Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Huamin Qu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yingcai Wu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Visualization and Computer Graphics ( Volume:
  28, Issue: 1, January 2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1155">[1155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14066" title="Abstract">arXiv:2306.14066</a> (replaced) [<a href="/pdf/2306.14066" title="Download PDF">pdf</a>, <a href="/format/2306.14066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEEDS: Emulation of Weather Forecast Ensembles with Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lizao Li</a>, 
<a href="/search/cs?searchtype=author&query=Carver%2C+R">Rob Carver</a>, 
<a href="/search/cs?searchtype=author&query=Lopez-Gomez%2C+I">Ignacio Lopez-Gomez</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+F">Fei Sha</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+J">John Anderson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> fixed a mistake of the previous version; the paper has not been submitted to neurips 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
</div>
</dd>
<dt><a name="item1156">[1156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14208" title="Abstract">arXiv:2306.14208</a> (replaced) [<a href="/e-print/2306.14208" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PaRUS: A Virtual Reality Shopping Method Focusing on Context between  Products and Real Usage Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+W">Weitao You</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yinyu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Ziqing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yizhan Shao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Changyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhibin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lingyun Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> a mistake: the participant number of the first user study should be 24 instead of 16
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item1157">[1157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14939" title="Abstract">arXiv:2306.14939</a> (replaced) [<a href="/pdf/2306.14939" title="Download PDF">pdf</a>, <a href="/format/2306.14939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Art of Embedding Fusion: Optimizing Hate Speech Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+A">Mohammad Aflah Khan</a>, 
<a href="/search/cs?searchtype=author&query=Yadav%2C+N">Neemesh Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+M">Mohit Jain</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+S">Sanyam Goyal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a Tiny Paper at ICLR 2023, 12 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1158">[1158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15128" title="Abstract">arXiv:2306.15128</a> (replaced) [<a href="/pdf/2306.15128" title="Download PDF">pdf</a>, <a href="/format/2306.15128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIMIC: Masked Image Modeling with Image Correspondences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marathe%2C+K">Kalyani Marathe</a>, 
<a href="/search/cs?searchtype=author&query=Bigverdi%2C+M">Mahtab Bigverdi</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+N">Nishat Khan</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+T">Tuhin Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Kembhavi%2C+A">Aniruddha Kembhavi</a>, 
<a href="/search/cs?searchtype=author&query=Shapiro%2C+L+G">Linda G. Shapiro</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+R">Ranjay Krishna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1159">[1159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15521" title="Abstract">arXiv:2306.15521</a> (replaced) [<a href="/pdf/2306.15521" title="Download PDF">pdf</a>, <a href="/format/2306.15521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What a MESS: Multi-Domain Evaluation of Zero-Shot Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blumenstiel%2C+B">Benedikt Blumenstiel</a>, 
<a href="/search/cs?searchtype=author&query=Jakubik%2C+J">Johannes Jakubik</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BChne%2C+H">Hilde K&#xfc;hne</a>, 
<a href="/search/cs?searchtype=author&query=V%C3%B6ssing%2C+M">Michael V&#xf6;ssing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023 Track on Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1160">[1160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15749" title="Abstract">arXiv:2306.15749</a> (replaced) [<a href="/pdf/2306.15749" title="Download PDF">pdf</a>, <a href="/format/2306.15749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> To Spike or Not To Spike: A Digital Hardware Perspective on Deep  Learning Acceleration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ottati%2C+F">Fabrizio Ottati</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qinyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Brignone%2C+G">Giovanni Brignone</a>, 
<a href="/search/cs?searchtype=author&query=Casu%2C+M+R">Mario R. Casu</a>, 
<a href="/search/cs?searchtype=author&query=Eshraghian%2C+J+K">Jason K. Eshraghian</a>, 
<a href="/search/cs?searchtype=author&query=Lavagno%2C+L">Luciano Lavagno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE Journal on Emerging and Selected Topics in Circuits and Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1161">[1161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16241" title="Abstract">arXiv:2306.16241</a> (replaced) [<a href="/pdf/2306.16241" title="Download PDF">pdf</a>, <a href="/format/2306.16241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Focus on the Sound around You: Monaural Target Speaker Extraction via  Distance and Speaker Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiuxin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dinkel%2C+H">Heinrich Dinkel</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhiyong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junbo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yujun Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proc. INTERSPEECH 2023, 2488-2492, doi: 10.21437/Interspeech.2023-218
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1162">[1162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16361" title="Abstract">arXiv:2306.16361</a> (replaced) [<a href="/pdf/2306.16361" title="Download PDF">pdf</a>, <a href="/ps/2306.16361" title="Download PostScript">ps</a>, <a href="/format/2306.16361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond NTK with Vanilla Gradient Descent: A Mean-Field Analysis of  Neural Networks with Polynomial Width, Samples, and Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahankali%2C+A">Arvind Mahankali</a>, 
<a href="/search/cs?searchtype=author&query=Haochen%2C+J+Z">Jeff Z. Haochen</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+K">Kefan Dong</a>, 
<a href="/search/cs?searchtype=author&query=Glasgow%2C+M">Margalit Glasgow</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Tengyu Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added result on projected gradient descent with inverse-polynomial learning rate
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1163">[1163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17175" title="Abstract">arXiv:2306.17175</a> (replaced) [<a href="/pdf/2306.17175" title="Download PDF">pdf</a>, <a href="/format/2306.17175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19  Assessment in Primary Care
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mekhtieva%2C+R+L">Rakhilya Lee Mekhtieva</a>, 
<a href="/search/cs?searchtype=author&query=Forbes%2C+B">Brandon Forbes</a>, 
<a href="/search/cs?searchtype=author&query=Alrajeh%2C+D">Dalal Alrajeh</a>, 
<a href="/search/cs?searchtype=author&query=Delaney%2C+B">Brendan Delaney</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+A">Alessandra Russo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1164">[1164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00782" title="Abstract">arXiv:2307.00782</a> (replaced) [<a href="/pdf/2307.00782" title="Download PDF">pdf</a>, <a href="/format/2307.00782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph  Reading
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yujia Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaofei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xu Tan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Lei He</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Sheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Soong%2C+F+K">Frank K. Soong</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+T">Tan Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures, Proceedings of Interspeech 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1165">[1165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00819" title="Abstract">arXiv:2307.00819</a> (replaced) [<a href="/pdf/2307.00819" title="Download PDF">pdf</a>, <a href="/format/2307.00819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Data-driven Under Frequency Load Shedding Scheme in Power Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cao%2C+Q">Qianni Cao</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+C">Chen Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1166">[1166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01379" title="Abstract">arXiv:2307.01379</a> (replaced) [<a href="/pdf/2307.01379" title="Download PDF">pdf</a>, <a href="/format/2307.01379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shifting Attention to Relevance: Towards the Uncertainty Estimation of  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jinhao Duan</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zavalny%2C+A">Alex Zavalny</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Renjing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Kailkhura%2C+B">Bhavya Kailkhura</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kaidi Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1167">[1167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02688" title="Abstract">arXiv:2307.02688</a> (replaced) [<a href="/pdf/2307.02688" title="Download PDF">pdf</a>, <a href="/ps/2307.02688" title="Download PostScript">ps</a>, <a href="/format/2307.02688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Epistemic systems and Flagg and Friedman&#x27;s translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Inou%C3%A9%2C+T">Takao Inou&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item1168">[1168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03690" title="Abstract">arXiv:2307.03690</a> (replaced) [<a href="/pdf/2307.03690" title="Download PDF">pdf</a>, <a href="/format/2307.03690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Suppressing unknown disturbances to dynamical systems using machine  learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Restrepo%2C+J+G">Juan G. Restrepo</a>, 
<a href="/search/eess?searchtype=author&query=Byers%2C+C+P">Clayton P. Byers</a>, 
<a href="/search/eess?searchtype=author&query=Skardal%2C+P+S">Per Sebastian Skardal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 15 figures (including supplemental material)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1169">[1169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03864" title="Abstract">arXiv:2307.03864</a> (replaced) [<a href="/pdf/2307.03864" title="Download PDF">pdf</a>, <a href="/format/2307.03864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Do Transformers Shine in RL? Decoupling Memory from Credit  Assignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+T">Tianwei Ni</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Michel Ma</a>, 
<a href="/search/cs?searchtype=author&query=Eysenbach%2C+B">Benjamin Eysenbach</a>, 
<a href="/search/cs?searchtype=author&query=Bacon%2C+P">Pierre-Luc Bacon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1170">[1170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04754" title="Abstract">arXiv:2307.04754</a> (replaced) [<a href="/pdf/2307.04754" title="Download PDF">pdf</a>, <a href="/format/2307.04754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Action-State Dependent Dynamic Model Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cordoni%2C+F">Francesco Cordoni</a>, 
<a href="/search/cs?searchtype=author&query=Sancetta%2C+A">Alessio Sancetta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Portfolio Management (q-fin.PM); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1171">[1171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06235" title="Abstract">arXiv:2307.06235</a> (replaced) [<a href="/pdf/2307.06235" title="Download PDF">pdf</a>, <a href="/format/2307.06235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Molecular Pretraining via Modality Blending
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qiying Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yudi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+Y">Yuyan Ni</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shikun Feng</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yanyan Lan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingjing Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item1172">[1172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06714" title="Abstract">arXiv:2307.06714</a> (replaced) [<a href="/pdf/2307.06714" title="Download PDF">pdf</a>, <a href="/format/2307.06714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymptotic SEP Analysis and Optimization of Linear-Quantized Precoding  in Massive MIMO Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zheyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Junjie Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Ya-Feng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Swindlehurst%2C+A+L">A. Lee Swindlehurst</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 58 pages, 8 figures, accepted for publication in IEEE Transactions on Information Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1173">[1173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07176" title="Abstract">arXiv:2307.07176</a> (replaced) [<a href="/pdf/2307.07176" title="Download PDF">pdf</a>, <a href="/format/2307.07176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SafeDreamer: Safe Reinforcement Learning with World Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weidong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiaming Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Borong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+C">Chunhe Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1174">[1174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07909" title="Abstract">arXiv:2307.07909</a> (replaced) [<a href="/pdf/2307.07909" title="Download PDF">pdf</a>, <a href="/format/2307.07909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Imitation All You Need? Generalized Decision-Making with Dual-Phase  Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanchao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Ruijie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Vemprala%2C+S">Sai Vemprala</a>, 
<a href="/search/cs?searchtype=author&query=Bonatti%2C+R">Rogerio Bonatti</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shuhang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Madaan%2C+R">Ratnesh Madaan</a>, 
<a href="/search/cs?searchtype=author&query=Ba%2C+Z">Zhongjie Ba</a>, 
<a href="/search/cs?searchtype=author&query=Kapoor%2C+A">Ashish Kapoor</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shuang Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1175">[1175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08671" title="Abstract">arXiv:2307.08671</a> (replaced) [<a href="/pdf/2307.08671" title="Download PDF">pdf</a>, <a href="/format/2307.08671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Cross-Modal Steganography Using Neural Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+G">Gyojin Han</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dong-Jae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hur%2C+J">Jiwan Hur</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaehyun Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junmo Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICIP 2023 Oral
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1176">[1176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08721" title="Abstract">arXiv:2307.08721</a> (replaced) [<a href="/pdf/2307.08721" title="Download PDF">pdf</a>, <a href="/format/2307.08721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Where Did the President Visit Last Week? Detecting Celebrity Trips from  News Articles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+K">Kai Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+S">Shuai Ling</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+Z">Zhaoru Ke</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haipeng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICWSM 2024, 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1177">[1177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09019" title="Abstract">arXiv:2307.09019</a> (replaced) [<a href="/pdf/2307.09019" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> U-shaped Transformer: Retain High Frequency Context in Time Series  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qingkui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiqin Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1178">[1178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09052" title="Abstract">arXiv:2307.09052</a> (replaced) [<a href="/pdf/2307.09052" title="Download PDF">pdf</a>, <a href="/format/2307.09052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connections between Operator-splitting Methods and Deep Neural Networks  with Applications in Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tai%2C+X">Xue-Cheng Tai</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+R">Raymond Chan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1179">[1179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10864" title="Abstract">arXiv:2307.10864</a> (replaced) [<a href="/pdf/2307.10864" title="Download PDF">pdf</a>, <a href="/format/2307.10864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yumeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Keuper%2C+M">Margret Keuper</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Khoreva%2C+A">Anna Khoreva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at BMVC 2023 as Oral. Code: <a href="https://github.com/boschresearch/Divide-and-Bind">this https URL</a> and project page: <a href="https://sites.google.com/view/divide-and-bind">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1180">[1180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11855" title="Abstract">arXiv:2307.11855</a> (replaced) [<a href="/pdf/2307.11855" title="Download PDF">pdf</a>, <a href="/format/2307.11855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Run Time Bounds for Integer-Valued OneMax Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harder%2C+J+G">Jonathan Gadea Harder</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6tzing%2C+T">Timo K&#xf6;tzing</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Radhakrishnan%2C+A">Aishwarya Radhakrishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item1181">[1181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12270" title="Abstract">arXiv:2307.12270</a> (replaced) [<a href="/pdf/2307.12270" title="Download PDF">pdf</a>, <a href="/format/2307.12270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context Perception Parallel Decoder for Scene Text Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yongkun Du</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhineng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+C">Caiyan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+X">Xiaoting Yin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenxia Li</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yuning Du</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yu-Gang Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1182">[1182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12493" title="Abstract">arXiv:2307.12493</a> (replaced) [<a href="/pdf/2307.12493" title="Download PDF">pdf</a>, <a href="/format/2307.12493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shilin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanzhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+A+W">Adams Wai-Kin Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1183">[1183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13225" title="Abstract">arXiv:2307.13225</a> (replaced) [<a href="/pdf/2307.13225" title="Download PDF">pdf</a>, <a href="/format/2307.13225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Pairwise Dataset for GUI Conversion and Retrieval between Android  Phones and Tablets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Han Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+H">Haolan Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yujin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Di Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item1184">[1184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15337" title="Abstract">arXiv:2307.15337</a> (replaced) [<a href="/pdf/2307.15337" title="Download PDF">pdf</a>, <a href="/format/2307.15337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ning%2C+X">Xuefei Ning</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zinan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zixuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huazhong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1185">[1185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16177" title="Abstract">arXiv:2307.16177</a> (replaced) [<a href="/pdf/2307.16177" title="Download PDF">pdf</a>, <a href="/format/2307.16177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fusing VHR Post-disaster Aerial Imagery and LiDAR Data for Roof  Classification in the Caribbean
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tingzon%2C+I">Isabelle Tingzon</a>, 
<a href="/search/cs?searchtype=author&query=Cowan%2C+N+M">Nuala Margaret Cowan</a>, 
<a href="/search/cs?searchtype=author&query=Chrzanowski%2C+P">Pierre Chrzanowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023 Workshop on Artificial Intelligence for Humanitarian Assistance and Disaster Response
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1186">[1186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16230" title="Abstract">arXiv:2307.16230</a> (replaced) [<a href="/pdf/2307.16230" title="Download PDF">pdf</a>, <a href="/format/2307.16230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Private Watermark for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Aiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Leyi Pan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xuming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shu&#x27;ang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+L">Lijie Wen</a>, 
<a href="/search/cs?searchtype=author&query=King%2C+I">Irwin King</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1187">[1187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16270" title="Abstract">arXiv:2307.16270</a> (replaced) [<a href="/pdf/2307.16270" title="Download PDF">pdf</a>, <a href="/format/2307.16270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formalizing Monoidal Categories and Actions for Syntax with Binders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahrens%2C+B">Benedikt Ahrens</a>, 
<a href="/search/cs?searchtype=author&query=Matthes%2C+R">Ralph Matthes</a>, 
<a href="/search/cs?searchtype=author&query=Wullaert%2C+K">Kobe Wullaert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Abstract for a talk at CoqPL 2023, <a href="https://popl23.sigplan.org/details/CoqPL-2023-papers/7/Formalizing-Monoidal-Categories-and-Actions-for-Syntax-with-Binders">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Category Theory (math.CT)

</div>
</div>
</dd>
<dt><a name="item1188">[1188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00086" title="Abstract">arXiv:2308.00086</a> (replaced) [<a href="/pdf/2308.00086" title="Download PDF">pdf</a>, <a href="/format/2308.00086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An unsupervised machine-learning-based shock sensor for high-order  supersonic flow solvers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mateo-Gab%C3%ADn%2C+A">Andr&#xe9;s Mateo-Gab&#xed;n</a>, 
<a href="/search/cs?searchtype=author&query=Tlales%2C+K">Kenza Tlales</a>, 
<a href="/search/cs?searchtype=author&query=Valero%2C+E">Eusebio Valero</a>, 
<a href="/search/cs?searchtype=author&query=Ferrer%2C+E">Esteban Ferrer</a>, 
<a href="/search/cs?searchtype=author&query=Rubio%2C+G">Gonzalo Rubio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph); Fluid Dynamics (physics.flu-dyn)

</div>
</div>
</dd>
<dt><a name="item1189">[1189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01313" title="Abstract">arXiv:2308.01313</a> (replaced) [<a href="/pdf/2308.01313" title="Download PDF">pdf</a>, <a href="/format/2308.01313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> More Context, Less Distraction: Zero-shot Visual Classification by  Inferring and Conditioning on Contextual Attributes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bang An</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Sicheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Panaitescu-Liess%2C+M">Michael-Andrei Panaitescu-Liess</a>, 
<a href="/search/cs?searchtype=author&query=Mummadi%2C+C+K">Chaithanya Kumar Mummadi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1190">[1190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01508" title="Abstract">arXiv:2308.01508</a> (replaced) [<a href="/pdf/2308.01508" title="Download PDF">pdf</a>, <a href="/format/2308.01508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Circumventing Concept Erasure Methods For Text-to-Image Generative  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pham%2C+M">Minh Pham</a>, 
<a href="/search/cs?searchtype=author&query=Marshall%2C+K+O">Kelly O. Marshall</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+N">Niv Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+G">Govind Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Hegde%2C+C">Chinmay Hegde</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1191">[1191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01587" title="Abstract">arXiv:2308.01587</a> (replaced) [<a href="/pdf/2308.01587" title="Download PDF">pdf</a>, <a href="/format/2308.01587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistency Regularization for Generalizable Source-free Domain  Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+L">Longxiang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kai Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Chunming He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yulun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiu Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV 2023 workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1192">[1192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02287" title="Abstract">arXiv:2308.02287</a> (replaced) [<a href="/pdf/2308.02287" title="Download PDF">pdf</a>, <a href="/format/2308.02287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Frustratingly Easy Model Generalization by Dummy Risk Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Juncheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xixu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shujun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1193">[1193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02580" title="Abstract">arXiv:2308.02580</a> (replaced) [<a href="/pdf/2308.02580" title="Download PDF">pdf</a>, <a href="/format/2308.02580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gaussian-based Probabilistic Deep Supervision Network for  Noise-Resistant QoS Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziliang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaohong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Sheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Dan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Meng Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1194">[1194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02585" title="Abstract">arXiv:2308.02585</a> (replaced) [<a href="/pdf/2308.02585" title="Download PDF">pdf</a>, <a href="/format/2308.02585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PARL: A Unified Framework for Policy Alignment in Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Souradip Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Bedi%2C+A+S">Amrit Singh Bedi</a>, 
<a href="/search/cs?searchtype=author&query=Koppel%2C+A">Alec Koppel</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huazheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengdi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1195">[1195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02673" title="Abstract">arXiv:2308.02673</a> (replaced) [<a href="/pdf/2308.02673" title="Download PDF">pdf</a>, <a href="/ps/2308.02673" title="Download PostScript">ps</a>, <a href="/format/2308.02673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comments on &quot;A Linear Time Algorithm for the Optimal Discrete IRS  Beamforming&quot;
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pekcan%2C+D+K">Dogan Kutay Pekcan</a>, 
<a href="/search/cs?searchtype=author&query=Ayanoglu%2C+E">Ender Ayanoglu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item1196">[1196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02925" title="Abstract">arXiv:2308.02925</a> (replaced) [<a href="/pdf/2308.02925" title="Download PDF">pdf</a>, <a href="/format/2308.02925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConvFormer: Revisiting Transformer for Sequential User Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+J">Jianxun Lian</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Mingqi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Jiajun Fan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wanyue Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chaozhuo Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Information Retrieval (cs.IR); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1197">[1197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03272" title="Abstract">arXiv:2308.03272</a> (replaced) [<a href="/pdf/2308.03272" title="Download PDF">pdf</a>, <a href="/format/2308.03272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature-Suppressed Contrast for Self-Supervised Food Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinda Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yaohui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Linhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Jiang Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lili Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1198">[1198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04464" title="Abstract">arXiv:2308.04464</a> (replaced) [<a href="/pdf/2308.04464" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Insect-Plant Interactions Affected by Mining Operations, A  Graph Mining Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Bayat%2C+A">Ali Bayat</a>, 
<a href="/search/q-bio?searchtype=author&query=Heydari%2C+M">Mohammad Heydari</a>, 
<a href="/search/q-bio?searchtype=author&query=Albadvi%2C+A">Amir Albadvi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Populations and Evolution (q-bio.PE)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1199">[1199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04791" title="Abstract">arXiv:2308.04791</a> (replaced) [<a href="/pdf/2308.04791" title="Download PDF">pdf</a>, <a href="/format/2308.04791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PETformer: Long-term Time Series Forecasting via Placeholder-enhanced  Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Shengsheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weiwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wentai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Songbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongxiang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1200">[1200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04948" title="Abstract">arXiv:2308.04948</a> (replaced) [<a href="/pdf/2308.04948" title="Download PDF">pdf</a>, <a href="/format/2308.04948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extrapolating Large Language Models to Non-English by Aligning Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wenhao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Y">Yunzhe Lv</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Q">Qingxiu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+F">Fei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingjing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shujian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1201">[1201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06838" title="Abstract">arXiv:2308.06838</a> (replaced) [<a href="/pdf/2308.06838" title="Download PDF">pdf</a>, <a href="/format/2308.06838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing Topological Graph Neural Networks with Paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Truong%2C+Q">Quang Truong</a>, 
<a href="/search/cs?searchtype=author&query=Chin%2C+P">Peter Chin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1202">[1202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07200" title="Abstract">arXiv:2308.07200</a> (replaced) [<a href="/pdf/2308.07200" title="Download PDF">pdf</a>, <a href="/format/2308.07200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Categorical Priors for Physics-Based Character Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qingxu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">He Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+M">Mengting Lan</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Lei Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Transactions on Graphics (Proc. ACM SIGGRAPH ASIA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1203">[1203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07688" title="Abstract">arXiv:2308.07688</a> (replaced) [<a href="/pdf/2308.07688" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Network Initialization for Medical AI Models Using  Large-Scale, Unlabeled Natural Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Arasteh%2C+S+T">Soroosh Tayebi Arasteh</a>, 
<a href="/search/eess?searchtype=author&query=Misera%2C+L">Leo Misera</a>, 
<a href="/search/eess?searchtype=author&query=Kather%2C+J+N">Jakob Nikolas Kather</a>, 
<a href="/search/eess?searchtype=author&query=Truhn%2C+D">Daniel Truhn</a>, 
<a href="/search/eess?searchtype=author&query=Nebelung%2C+S">Sven Nebelung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1204">[1204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07761" title="Abstract">arXiv:2308.07761</a> (replaced) [<a href="/pdf/2308.07761" title="Download PDF">pdf</a>, <a href="/format/2308.07761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeFL: Nested Federated Learning for Heterogeneous Clients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+H">Honggu Kang</a>, 
<a href="/search/cs?searchtype=author&query=Cha%2C+S">Seohyeon Cha</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jinwoo Shin</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jongmyeong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Joonhyuk Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1205">[1205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08009" title="Abstract">arXiv:2308.08009</a> (replaced) [<a href="/pdf/2308.08009" title="Download PDF">pdf</a>, <a href="/format/2308.08009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Degrees of Freedom and Eigenfunctions of Line-of-Sight  Holographic MIMO Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruiz-Sicilia%2C+J+C">Juan Carlos Ruiz-Sicilia</a>, 
<a href="/search/cs?searchtype=author&query=Di+Renzo%2C+M">Marco Di Renzo</a>, 
<a href="/search/cs?searchtype=author&query=Migliore%2C+M+D">Marco Donald Migliore</a>, 
<a href="/search/cs?searchtype=author&query=Debbah%2C+M">Merouane Debbah</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted for journal publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1206">[1206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08159" title="Abstract">arXiv:2308.08159</a> (replaced) [<a href="/pdf/2308.08159" title="Download PDF">pdf</a>, <a href="/ps/2308.08159" title="Download PostScript">ps</a>, <a href="/format/2308.08159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resolution of Near-Field Beamforming and Its Impact on NOMA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zhiguo Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1207">[1207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08543" title="Abstract">arXiv:2308.08543</a> (replaced) [<a href="/pdf/2308.08543" title="Download PDF">pdf</a>, <a href="/format/2308.08543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InsightMapper: A Closer Look at Inner-instance Information for  Vectorized High-Definition Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhenhua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K+K">Kwan-Yee. K. Wong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hengshuang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and demo will be available at <a href="https://tonyxuqaq.github.io/InsightMapper/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1208">[1208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08574" title="Abstract">arXiv:2308.08574</a> (replaced) [<a href="/pdf/2308.08574" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing the Capabilities of Nature-inspired Feature Selection  Algorithms in Predicting Student Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trask%2C+T">Thomas Trask</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, 3 tables, submitted to ICMLA conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1209">[1209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08653" title="Abstract">arXiv:2308.08653</a> (replaced) [<a href="/pdf/2308.08653" title="Download PDF">pdf</a>, <a href="/format/2308.08653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reproducing Kernel Hilbert Space Pruning for Sparse Hyperspectral  Abundance Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rawson%2C+M+G">Michael G. Rawson</a>, 
<a href="/search/cs?searchtype=author&query=Doster%2C+T">Timothy Doster</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1210">[1210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08730" title="Abstract">arXiv:2308.08730</a> (replaced) [<a href="/pdf/2308.08730" title="Download PDF">pdf</a>, <a href="/format/2308.08730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning A Coarse-to-Fine Diffusion Transformer for Image Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qinyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jinshan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zhixun Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1211">[1211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08842" title="Abstract">arXiv:2308.08842</a> (replaced) [<a href="/pdf/2308.08842" title="Download PDF">pdf</a>, <a href="/ps/2308.08842" title="Download PostScript">ps</a>, <a href="/format/2308.08842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Introducing Divergence for Infinite Probabilistic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Finkel%2C+A">Alain Finkel</a>, 
<a href="/search/cs?searchtype=author&query=Haddad%2C+S">Serge Haddad</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+L">Lina Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages. Add more details in proofs. arXiv admin note: text overlap with <a href="/abs/2305.19564">arXiv:2305.19564</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item1212">[1212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08888" title="Abstract">arXiv:2308.08888</a> (replaced) [<a href="/pdf/2308.08888" title="Download PDF">pdf</a>, <a href="/ps/2308.08888" title="Download PostScript">ps</a>, <a href="/format/2308.08888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A low-rank algorithm for strongly damped wave equations with  visco-elastic damping and mass terms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhao%2C+Y">Yong-Liang Zhao</a>, 
<a href="/search/math?searchtype=author&query=Gu%2C+X">Xian-Ming Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1213">[1213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08896" title="Abstract">arXiv:2308.08896</a> (replaced) [<a href="/pdf/2308.08896" title="Download PDF">pdf</a>, <a href="/format/2308.08896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Resource Allocation for U-Shaped Parallel Split Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+S">Song Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+G">Guanqiao Qu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xianhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaoxia Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1214">[1214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09031" title="Abstract">arXiv:2308.09031</a> (replaced) [<a href="/pdf/2308.09031" title="Download PDF">pdf</a>, <a href="/ps/2308.09031" title="Download PostScript">ps</a>, <a href="/format/2308.09031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Properties of Intrinsic Information and Their Relation to Bound  Secrecy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khesin%2C+A+B">Andrey Boris Khesin</a>, 
<a href="/search/cs?searchtype=author&query=Tung%2C+A">Andrew Tung</a>, 
<a href="/search/cs?searchtype=author&query=Vedula%2C+K">Karthik Vedula</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Cryptography and Security (cs.CR); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item1215">[1215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10130" title="Abstract">arXiv:2308.10130</a> (replaced) [<a href="/pdf/2308.10130" title="Download PDF">pdf</a>, <a href="/format/2308.10130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Approximation of Operator-Valued Riccati Equations in Hilbert  Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cheung%2C+J">James Cheung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revision 1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1216">[1216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10425" title="Abstract">arXiv:2308.10425</a> (replaced) [<a href="/pdf/2308.10425" title="Download PDF">pdf</a>, <a href="/format/2308.10425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STAEformer: Spatio-Temporal Adaptive Embedding Makes Vanilla Transformer  SOTA for Traffic Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hangchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zheng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+R">Renhe Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jiewen Deng</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jinliang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Quanjun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xuan Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as CIKM2023 Short Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1217">[1217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10792" title="Abstract">arXiv:2308.10792</a> (replaced) [<a href="/pdf/2308.10792" title="Download PDF">pdf</a>, <a href="/format/2308.10792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instruction Tuning for Large Language Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shengyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+L">Linfeng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoya Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaofei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Runyi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoyin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A Survey paper, Pre-print
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1218">[1218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10848" title="Abstract">arXiv:2308.10848</a> (replaced) [<a href="/pdf/2308.10848" title="Download PDF">pdf</a>, <a href="/format/2308.10848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AgentVerse: Facilitating Multi-Agent Collaboration and Exploring  Emergent Behaviors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weize Chen</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yusheng Su</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+J">Jingwei Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chenfei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+C">Chi-Min Chan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Heyang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yaxi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+Y">Yi-Hsin Hung</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yujia Qin</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+X">Xin Cong</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Ruobing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review. Code at <a href="https://github.com/OpenBMB/AgentVerse/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1219">[1219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10974" title="Abstract">arXiv:2308.10974</a> (replaced) [<a href="/pdf/2308.10974" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Guinea Pig Trials&quot; Utilizing GPT: A Novel Smart Agent-Based Modeling  Approach for Studying Firm Competition and Collusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zengqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chuan Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at Conference on Information Systems and Technology (CIST) 2023. Source code is available at: <a href="https://github.com/wuzengqing001225/SABM_Pricing_Game/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computational Engineering, Finance, and Science (cs.CE); Multiagent Systems (cs.MA); General Economics (econ.GN)

</div>
</div>
</dd>
<dt><a name="item1220">[1220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11534" title="Abstract">arXiv:2308.11534</a> (replaced) [<a href="/pdf/2308.11534" title="Download PDF">pdf</a>, <a href="/format/2308.11534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PlatoLM: Teaching LLMs via a Socratic Questioning User Simulator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+C">Chuyi Kong</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yaxin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+F">Feng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benyou Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1221">[1221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12113" title="Abstract">arXiv:2308.12113</a> (replaced) [<a href="/pdf/2308.12113" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancements in Point Cloud Data Augmentation for Deep Learning: A  Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qinfeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Lei Fan</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+N">Ningxin Weng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1222">[1222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13540" title="Abstract">arXiv:2308.13540</a> (replaced) [<a href="/pdf/2308.13540" title="Download PDF">pdf</a>, <a href="/format/2308.13540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RL-LABEL: A Deep Reinforcement Learning Approach Intended for AR Label  Placement in Dynamic Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu-Tian%2C+C">Chen Zhu-Tian</a>, 
<a href="/search/cs?searchtype=author&query=Chiappalupi%2C+D">Daniele Chiappalupi</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tica Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yalong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Beyer%2C+J">Johanna Beyer</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+H">Hanspeter Pfister</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1223">[1223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13551" title="Abstract">arXiv:2308.13551</a> (replaced) [<a href="/pdf/2308.13551" title="Download PDF">pdf</a>, <a href="/format/2308.13551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dance with You: The Diversity Controllable Dancer Generation via  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+S">Siyue Yao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingjie Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bingliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fengyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junle Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruimao Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1224">[1224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13712" title="Abstract">arXiv:2308.13712</a> (replaced) [<a href="/pdf/2308.13712" title="Download PDF">pdf</a>, <a href="/format/2308.13712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Residual Denoising Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiawei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+H">Huijie Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yandong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+L">Liangqiong Qu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1225">[1225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13957" title="Abstract">arXiv:2308.13957</a> (replaced) [<a href="/pdf/2308.13957" title="Download PDF">pdf</a>, <a href="/format/2308.13957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable Weight Masks for Domain Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khanna%2C+S">Samar Khanna</a>, 
<a href="/search/cs?searchtype=author&query=Vaidyanath%2C+S">Skanda Vaidyanath</a>, 
<a href="/search/cs?searchtype=author&query=Velu%2C+A">Akash Velu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Out of Distribution Generalization in Computer Vision (OOD-CV) workshop at ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1226">[1226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14239" title="Abstract">arXiv:2308.14239</a> (replaced) [<a href="/pdf/2308.14239" title="Download PDF">pdf</a>, <a href="/format/2308.14239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Next Generation Reservoir Computing: An Efficient Quantum  Algorithm for Forecasting Quantum Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Sornsaeng%2C+A">Apimuk Sornsaeng</a>, 
<a href="/search/quant-ph?searchtype=author&query=Dangniam%2C+N">Ninnat Dangniam</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chotibut%2C+T">Thiparat Chotibut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures. v2: additional forecasting results for a chaotic quantum system
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Statistical Mechanics (cond-mat.stat-mech); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item1227">[1227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14581" title="Abstract">arXiv:2308.14581</a> (replaced) [<a href="/pdf/2308.14581" title="Download PDF">pdf</a>, <a href="/format/2308.14581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Many-valued coalgebraic logic over semi-primal varieties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurz%2C+A">Alexander Kurz</a>, 
<a href="/search/cs?searchtype=author&query=Poiger%2C+W">Wolfgang Poiger</a>, 
<a href="/search/cs?searchtype=author&query=Teheux%2C+B">Bruno Teheux</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Category Theory (math.CT); Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item1228">[1228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14900" title="Abstract">arXiv:2308.14900</a> (replaced) [<a href="/pdf/2308.14900" title="Download PDF">pdf</a>, <a href="/format/2308.14900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BIT: Bi-Level Temporal Modeling for Efficient Supervised Action  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zijia Lu</a>, 
<a href="/search/cs?searchtype=author&query=Elhamifar%2C+E">Ehsan Elhamifar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1229">[1229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15122" title="Abstract">arXiv:2308.15122</a> (replaced) [<a href="/pdf/2308.15122" title="Download PDF">pdf</a>, <a href="/format/2308.15122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpikeBERT: A Language Spikformer Learned from BERT with Knowledge  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Changze Lv</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jianhan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+C">Chenxi Gu</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zixuan Ling</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cenyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiaoqing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1230">[1230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00958" title="Abstract">arXiv:2309.00958</a> (replaced) [<a href="/pdf/2309.00958" title="Download PDF">pdf</a>, <a href="/format/2309.00958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Index-aware learning of circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garcia%2C+I+C">Idoia Cortes Garcia</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%B6rster%2C+P">Peter F&#xf6;rster</a>, 
<a href="/search/cs?searchtype=author&query=Jansen%2C+L">Lennart Jansen</a>, 
<a href="/search/cs?searchtype=author&query=Schilders%2C+W">Wil Schilders</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6ps%2C+S">Sebastian Sch&#xf6;ps</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1231">[1231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01437" title="Abstract">arXiv:2309.01437</a> (replaced) [<a href="/pdf/2309.01437" title="Download PDF">pdf</a>, <a href="/format/2309.01437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SememeASR: Boosting Performance of End-to-End Speech Recognition against  Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiaxu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Changhe Song</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+H">Helen Meng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of Interspeech
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1232">[1232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02033" title="Abstract">arXiv:2309.02033</a> (replaced) [<a href="/pdf/2309.02033" title="Download PDF">pdf</a>, <a href="/format/2309.02033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Juicer: A One-Stop Data Processing System for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Daoyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yilun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhijian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hesen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xuchen Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+C">Ce Ge</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+D">Dawei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuexiang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhaoyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jinyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+B">Bolin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 Pages. Under continuous maintenance and updating; The system, data recipes, and demos are at <a href="https://github.com/alibaba/data-juicer">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Databases (cs.DB); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1233">[1233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02110" title="Abstract">arXiv:2309.02110</a> (replaced) [<a href="/pdf/2309.02110" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and  Influence!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dilger%2C+J+P">James P. Dilger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">History and Overview (math.HO)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1234">[1234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02338" title="Abstract">arXiv:2309.02338</a> (replaced) [<a href="/pdf/2309.02338" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sustainability assessment of Low Earth Orbit (LEO) satellite broadband  mega-constellations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Osoro%2C+O+B">Ogutu B. Osoro</a>, 
<a href="/search/astro-ph?searchtype=author&query=Oughton%2C+E+J">Edward J. Oughton</a>, 
<a href="/search/astro-ph?searchtype=author&query=Wilson%2C+A+R">Andrew R. Wilson</a>, 
<a href="/search/astro-ph?searchtype=author&query=Rao%2C+A">Akhil Rao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Earth and Planetary Astrophysics (astro-ph.EP)</span>; General Economics (econ.GN); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1235">[1235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02428" title="Abstract">arXiv:2309.02428</a> (replaced) [<a href="/pdf/2309.02428" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Deep Learning Models through Tensorization: A Comprehensive  Survey and Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Helal%2C+M">Manal Helal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 8 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1236">[1236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02443" title="Abstract">arXiv:2309.02443</a> (replaced) [<a href="/pdf/2309.02443" title="Download PDF">pdf</a>, <a href="/ps/2309.02443" title="Download PostScript">ps</a>, <a href="/format/2309.02443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Choice of Sign Defining Householder Transformations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Overton%2C+M+L">Michael L. Overton</a>, 
<a href="/search/math?searchtype=author&query=Yu%2C+P">Pinze Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1237">[1237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02459" title="Abstract">arXiv:2309.02459</a> (replaced) [<a href="/pdf/2309.02459" title="Download PDF">pdf</a>, <a href="/format/2309.02459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-Only Domain Adaptation for End-to-End Speech Recognition through  Down-Sampling Acoustic Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiaxu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+W">Weinan Tong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yaoxun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Changhe Song</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Z">Zhao You</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+D">Dan Su</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+H">Helen Meng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of Interspeech. arXiv admin note: text overlap with <a href="/abs/2309.01437">arXiv:2309.01437</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1238">[1238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02654" title="Abstract">arXiv:2309.02654</a> (replaced) [<a href="/pdf/2309.02654" title="Download PDF">pdf</a>, <a href="/format/2309.02654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Resource Hallucination Prevention for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Junyu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Cao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fenglong Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1239">[1239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02868" title="Abstract">arXiv:2309.02868</a> (replaced) [<a href="/pdf/2309.02868" title="Download PDF">pdf</a>, <a href="/format/2309.02868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Asynchronous Time Series Forecasting with Contrastive  Relational Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+Z">Zhixuan Chu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Caigao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+H">Hongyan Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Minjie Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xindong Cai</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Q">Qing Cui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Longfei Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+Y">James Y Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+S">Siqiao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jun Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICDM 2023 AI4TS Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1240">[1240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02911" title="Abstract">arXiv:2309.02911</a> (replaced) [<a href="/e-print/2309.02911" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multimodal Learning Framework for Comprehensive 3D Mineral  Prospectivity Modeling with Jointly Learned Structure-Fluid Relationships
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+H">Hao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruisheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jingjie Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Upon careful review, it has come to our attention that inaccuracies exist in the formulation of the structure-fluid relationships, impacting the validity of the presented results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1241">[1241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02926" title="Abstract">arXiv:2309.02926</a> (replaced) [<a href="/pdf/2309.02926" title="Download PDF">pdf</a>, <a href="/format/2309.02926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demystifying RCE Vulnerabilities in LLM-Integrated Apps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zizhuang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+G">Guozhu Meng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuekang Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1242">[1242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03137" title="Abstract">arXiv:2309.03137</a> (replaced) [<a href="/e-print/2309.03137" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> There are only two paradoxes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Walicki%2C+M">Michal Walicki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The proof of the main Theorem 3.7 has some flaws. A minor one, which can be fixed, is the choice of \mathcal{H} as "the set of all sinkless induced subgraphs of H having finitely many ends." A more serious one is that the proof could be used to establish an untrue claim. It is not clear yet what specific mistake causes it
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item1243">[1243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03249" title="Abstract">arXiv:2309.03249</a> (replaced) [<a href="/pdf/2309.03249" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Theory Applications in Advanced Geospatial Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Surajit Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Mallick%2C+A">Archita Mallick</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+A">Anuva Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=De+Sarkar%2C+K">Kounik De Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE); Computers and Society (cs.CY); Geophysics (physics.geo-ph)

</div>
</div>
</dd>
<dt><a name="item1244">[1244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04076" title="Abstract">arXiv:2309.04076</a> (replaced) [<a href="/pdf/2309.04076" title="Download PDF">pdf</a>, <a href="/format/2309.04076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Smaller, Faster, and Greener Language Models of Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jieke Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhou Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+H+J">Hong Jin Kang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bowen Xu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junda He</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+D">David Lo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item1245">[1245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04106" title="Abstract">arXiv:2309.04106</a> (replaced) [<a href="/pdf/2309.04106" title="Download PDF">pdf</a>, <a href="/format/2309.04106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta predictive learning model of languages in neural circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chan Li</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+J">Junbin Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haiping Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 6 figures, codes are available in the main text with the link
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item1246">[1246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04204" title="Abstract">arXiv:2309.04204</a> (replaced) [<a href="/pdf/2309.04204" title="Download PDF">pdf</a>, <a href="/ps/2309.04204" title="Download PostScript">ps</a>, <a href="/format/2309.04204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task Offloading Optimization in Mobile Edge Computing under Uncertain  Processing Cycles and Intermittent Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+T">Tao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhanwei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+D">Di Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1247">[1247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04873" title="Abstract">arXiv:2309.04873</a> (replaced) [<a href="/pdf/2309.04873" title="Download PDF">pdf</a>, <a href="/ps/2309.04873" title="Download PostScript">ps</a>, <a href="/format/2309.04873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Reversible Computation to Checkpoint-Based Rollback Recovery for  Message-Passing Concurrent Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vidal%2C+G">Germ&#xe1;n Vidal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the Proceedings of the 19th International Conference on Formal Aspects of Component Software (FACS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item1248">[1248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05238" title="Abstract">arXiv:2309.05238</a> (replaced) [<a href="/pdf/2309.05238" title="Download PDF">pdf</a>, <a href="/format/2309.05238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Natural Language Queries for More Effective Systematic Review  Screening Prioritisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Scells%2C+H">Harrisen Scells</a>, 
<a href="/search/cs?searchtype=author&query=Potthast%2C+M">Martin Potthast</a>, 
<a href="/search/cs?searchtype=author&query=Koopman%2C+B">Bevan Koopman</a>, 
<a href="/search/cs?searchtype=author&query=Zuccon%2C+G">Guido Zuccon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprints for Accepted paper in SIGIR-AP-2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1249">[1249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05257" title="Abstract">arXiv:2309.05257</a> (replaced) [<a href="/pdf/2309.05257" title="Download PDF">pdf</a>, <a href="/format/2309.05257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FusionFormer: A Multi-sensory Fusion in Bird&#x27;s-Eye-View and Temporal  Consistent Transformer for 3D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chunyong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Hang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kun Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jianyun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+W">Weibo Mao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+M">Maochun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lingxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingxia Chen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Q">Qihao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kaixuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yiru Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+P">Peihan Hao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Minzhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Kaicheng Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1250">[1250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06223" title="Abstract">arXiv:2309.06223</a> (replaced) [<a href="/pdf/2309.06223" title="Download PDF">pdf</a>, <a href="/format/2309.06223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling Single-Bit-Flip Attacks on DNN Executables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yanzuo Chen</a> (1), 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhibo Liu</a> (1), 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yuanyuan Yuan</a> (1), 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Sihang Hu</a> (2), 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianxiang Li</a> (2), 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a> (1) ((1) The Hong Kong University of Science and Technology, (2) Huawei Technologies)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fix typo
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1251">[1251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06382" title="Abstract">arXiv:2309.06382</a> (replaced) [<a href="/pdf/2309.06382" title="Download PDF">pdf</a>, <a href="/format/2309.06382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensemble Mask Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luntzel%2C+J">Jonny Luntzel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1252">[1252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06681" title="Abstract">arXiv:2309.06681</a> (replaced) [<a href="/pdf/2309.06681" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A plug-and-play synthetic data deep learning for undersampled magnetic  resonance image reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xiao%2C+M">Min Xiao</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zi Wang</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+J">Jiefeng Guo</a>, 
<a href="/search/eess?searchtype=author&query=Qu%2C+X">Xiaobo Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1253">[1253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06684" title="Abstract">arXiv:2309.06684</a> (replaced) [<a href="/pdf/2309.06684" title="Download PDF">pdf</a>, <a href="/format/2309.06684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention Loss Adjusted Prioritized Experience Replay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuoying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huiping Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rizhong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1254">[1254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07124" title="Abstract">arXiv:2309.07124</a> (replaced) [<a href="/pdf/2309.07124" title="Download PDF">pdf</a>, <a href="/format/2309.07124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RAIN: Your Language Models Can Align Themselves without Finetuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Fangyun Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jinjing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongyang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1255">[1255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07405" title="Abstract">arXiv:2309.07405</a> (replaced) [<a href="/pdf/2309.07405" title="Download PDF">pdf</a>, <a href="/format/2309.07405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit  for Neural Speech Codec
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Z">Zhihao Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+K">Kai Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Siqi Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1256">[1256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07616" title="Abstract">arXiv:2309.07616</a> (replaced) [<a href="/pdf/2309.07616" title="Download PDF">pdf</a>, <a href="/format/2309.07616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Road Disease Detection based on Latent Domain Background Feature  Separation and Suppression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Juwu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jiangtao Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1257">[1257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07870" title="Abstract">arXiv:2309.07870</a> (replaced) [<a href="/pdf/2309.07870" title="Download PDF">pdf</a>, <a href="/format/2309.07870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Agents: An Open-source Framework for Autonomous Language Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wangchunshu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y+E">Yuchen Eleanor Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Long Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jialong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tiannan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+S">Shi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jintian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Ruipu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Shiding Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wentao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+P">Peng Cui</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at <a href="https://github.com/aiwaves-cn/agents">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1258">[1258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09075" title="Abstract">arXiv:2309.09075</a> (replaced) [<a href="/e-print/2309.09075" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Music Generation based on Generative Adversarial Networks with  Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Ziyi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Ruoxue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenghan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaoxuan Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> error upload
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Multimedia (cs.MM); Audio and Speech Processing (eess.AS); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1259">[1259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09550" title="Abstract">arXiv:2309.09550</a> (replaced) [<a href="/pdf/2309.09550" title="Download PDF">pdf</a>, <a href="/format/2309.09550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Reorganization of Neural Pathways for Continual Learning with  Spiking Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bing Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Feifei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+W">Wenxuan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhaoya Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xianqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Q">Qingqun Kong</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yi Zeng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1260">[1260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09770" title="Abstract">arXiv:2309.09770</a> (replaced) [<a href="/pdf/2309.09770" title="Download PDF">pdf</a>, <a href="/format/2309.09770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Data in Datathons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mougan%2C+C">Carlos Mougan</a>, 
<a href="/search/cs?searchtype=author&query=Plant%2C+R">Richard Plant</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+C">Clare Teng</a>, 
<a href="/search/cs?searchtype=author&query=Bazzi%2C+M">Marya Bazzi</a>, 
<a href="/search/cs?searchtype=author&query=Cabrejas-Egea%2C+A">Alvaro Cabrejas-Egea</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+R+S">Ryan Sze-Yin Chan</a>, 
<a href="/search/cs?searchtype=author&query=Jasin%2C+D+S">David Salvador Jasin</a>, 
<a href="/search/cs?searchtype=author&query=Stoffel%2C+M">Martin Stoffel</a>, 
<a href="/search/cs?searchtype=author&query=Whitaker%2C+K+J">Kirstie Jane Whitaker</a>, 
<a href="/search/cs?searchtype=author&query=Manser%2C+J">Jules Manser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1261">[1261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09985" title="Abstract">arXiv:2309.09985</a> (replaced) [<a href="/pdf/2309.09985" title="Download PDF">pdf</a>, <a href="/format/2309.09985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Molecular Conformation Generation via Shifting Scores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Zhou%2C+Z">Zihan Zhou</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+R">Ruiying Liu</a>, 
<a href="/search/physics?searchtype=author&query=Ying%2C+C">Chaolong Ying</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+R">Ruimao Zhang</a>, 
<a href="/search/physics?searchtype=author&query=Yu%2C+T">Tianshu Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1262">[1262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10204" title="Abstract">arXiv:2309.10204</a> (replaced) [<a href="/pdf/2309.10204" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Multiplier Based on Exponent Adder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Zhan%2C+J">Junpeng Zhan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC); Cryptography and Security (cs.CR); Quantum Algebra (math.QA)

</div>
</div>
</dd>
<dt><a name="item1263">[1263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10569" title="Abstract">arXiv:2309.10569</a> (replaced) [<a href="/pdf/2309.10569" title="Download PDF">pdf</a>, <a href="/format/2309.10569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task Graph offloading via Deep Reinforcement Learning in Mobile Edge  Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiagang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+Y">Yun Mi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1264">[1264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11059" title="Abstract">arXiv:2309.11059</a> (replaced) [<a href="/pdf/2309.11059" title="Download PDF">pdf</a>, <a href="/format/2309.11059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Complex U-Net with Conformer for Audio-Visual Speech Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ahmed%2C+S">Shafique Ahmed</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+C">Chia-Wei Chen</a>, 
<a href="/search/eess?searchtype=author&query=Ren%2C+W">Wenze Ren</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+C">Chin-Jou Li</a>, 
<a href="/search/eess?searchtype=author&query=Chu%2C+E">Ernie Chu</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+J">Jun-Cheng Chen</a>, 
<a href="/search/eess?searchtype=author&query=Hussain%2C+A">Amir Hussain</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Hsin-Min Wang</a>, 
<a href="/search/eess?searchtype=author&query=Tsao%2C+Y">Yu Tsao</a>, 
<a href="/search/eess?searchtype=author&query=Hou%2C+J">Jen-Cheng Hou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item1265">[1265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11994" title="Abstract">arXiv:2309.11994</a> (replaced) [<a href="/pdf/2309.11994" title="Download PDF">pdf</a>, <a href="/ps/2309.11994" title="Download PostScript">ps</a>, <a href="/format/2309.11994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing SAEAs with Unevaluated Solutions: A Case Study of Relation  Model for Expensive Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+H">Hao Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoqun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Aimin Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1266">[1266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12284" title="Abstract">arXiv:2309.12284</a> (replaced) [<a href="/pdf/2309.12284" title="Download PDF">pdf</a>, <a href="/format/2309.12284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MetaMath: Bootstrap Your Own Mathematical Questions for Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Longhui Yu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Weisen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Han Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jincheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J+T">James T. Kwok</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Weller%2C+A">Adrian Weller</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weiyang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report, Work in Progress. Project Page: <a href="https://meta-math.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1267">[1267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12592" title="Abstract">arXiv:2309.12592</a> (replaced) [<a href="/pdf/2309.12592" title="Download PDF">pdf</a>, <a href="/format/2309.12592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChainsFormer: A Chain Latency-aware Resource Provisioning Approach for  Microservices Cluster
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chenghao Song</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minxian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+K">Kejiang Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Huaming Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gill%2C+S+S">Sukhpal Singh Gill</a>, 
<a href="/search/cs?searchtype=author&query=Buyya%2C+R">Rajkumar Buyya</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chengzhong Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In the Proceedings of International Conference on Service Oriented
  Computing (ICSOC 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item1268">[1268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12624" title="Abstract">arXiv:2309.12624</a> (replaced) [<a href="/pdf/2309.12624" title="Download PDF">pdf</a>, <a href="/format/2309.12624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quark: A High-Performance Secure Container Runtime for Serverless  Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chenxingyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yulin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Ying Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+A">Arvind Krishnamurthy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2305.10621">arXiv:2305.10621</a>. The paper on <a href="/abs/2305.10621">arXiv:2305.10621</a> presents a detailed version of the TSoR module in Quark
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item1269">[1269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13038" title="Abstract">arXiv:2309.13038</a> (replaced) [<a href="/pdf/2309.13038" title="Download PDF">pdf</a>, <a href="/format/2309.13038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy Assessment on Reconstructed Images: Are Existing Evaluation  Metrics Faithful to Human Perception?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaoxiao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Gazagnadou%2C+N">Nidham Gazagnadou</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+V">Vivek Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+L">Lingjuan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongdong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Liang Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 9 figures and 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1270">[1270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13095" title="Abstract">arXiv:2309.13095</a> (replaced) [<a href="/pdf/2309.13095" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiple Independent DE Optimizations to Tackle Uncertainty and  Variability in Demand in Inventory Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maitra%2C+S">Sarit Maitra</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Sukanya Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+V">Vivek Mishra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, 6 tables, IEEE (ICITEE 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1271">[1271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13224" title="Abstract">arXiv:2309.13224</a> (replaced) [<a href="/pdf/2309.13224" title="Download PDF">pdf</a>, <a href="/format/2309.13224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pick Planning Strategies for Large-Scale Package Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuai Li</a>, 
<a href="/search/cs?searchtype=author&query=Keipour%2C+A">Azarakhsh Keipour</a>, 
<a href="/search/cs?searchtype=author&query=Jamieson%2C+K">Kevin Jamieson</a>, 
<a href="/search/cs?searchtype=author&query=Hudson%2C+N">Nicolas Hudson</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Sicong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Swan%2C+C">Charles Swan</a>, 
<a href="/search/cs?searchtype=author&query=Bekris%2C+K">Kostas Bekris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Learning Meets Model-based Methods for Manipulation and Grasping Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1272">[1272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13356" title="Abstract">arXiv:2309.13356</a> (replaced) [<a href="/pdf/2309.13356" title="Download PDF">pdf</a>, <a href="/format/2309.13356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probing the Moral Development of Large Language Models through Defining  Issues Test
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tanmay%2C+K">Kumar Tanmay</a>, 
<a href="/search/cs?searchtype=author&query=Khandelwal%2C+A">Aditi Khandelwal</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+U">Utkarsh Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+M">Monojit Choudhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First three authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1273">[1273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13546" title="Abstract">arXiv:2309.13546</a> (replaced) [<a href="/pdf/2309.13546" title="Download PDF">pdf</a>, <a href="/format/2309.13546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DFRD: Data-Free Robustness Distillation for Heterogeneous Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+K">Kangyang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yexuan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yunshi Lan</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Ming Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1274">[1274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13586" title="Abstract">arXiv:2309.13586</a> (replaced) [<a href="/pdf/2309.13586" title="Download PDF">pdf</a>, <a href="/format/2309.13586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task-Oriented Dexterous Grasp Synthesis via Differentiable Grasp Wrench  Boundary Estimator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiayi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jialiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">He Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In review. ICRA 2024 submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1275">[1275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13907" title="Abstract">arXiv:2309.13907</a> (replaced) [<a href="/pdf/2309.13907" title="Download PDF">pdf</a>, <a href="/format/2309.13907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HiGNN-TTS: Hierarchical Prosody Modeling with Graph Neural Networks for  Expressive Long-form TTS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Dake Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinfa Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+L">Liumeng Xue</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tao Li</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Y">Yuanjun Lv</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuepeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lei Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ASRU2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1276">[1276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14065" title="Abstract">arXiv:2309.14065</a> (replaced) [<a href="/pdf/2309.14065" title="Download PDF">pdf</a>, <a href="/format/2309.14065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile  Platform Real-Time RGB-D Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+S">Siqi Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weixi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+R">Renzhong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Shengjun Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1277">[1277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14102" title="Abstract">arXiv:2309.14102</a> (replaced) [<a href="/pdf/2309.14102" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Normalization of direct citations in publication-level networks:  Evaluation of six approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sj%C3%B6g%C3%A5rde%2C+P">Peter Sj&#xf6;g&#xe5;rde</a>, 
<a href="/search/cs?searchtype=author&query=Ahlgren%2C+P">Per Ahlgren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
</div>
</dd>
<dt><a name="item1278">[1278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14109" title="Abstract">arXiv:2309.14109</a> (replaced) [<a href="/pdf/2309.14109" title="Download PDF">pdf</a>, <a href="/format/2309.14109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Haha-Pod: An Attempt for Laughter-based Non-Verbal Speaker Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lin%2C+Y">Yuke Lin</a>, 
<a href="/search/eess?searchtype=author&query=Qin%2C+X">Xiaoyi Qin</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+N">Ning Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+G">Guoqing Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+M">Ming Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item1279">[1279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14136" title="Abstract">arXiv:2309.14136</a> (replaced) [<a href="/pdf/2309.14136" title="Download PDF">pdf</a>, <a href="/format/2309.14136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Masked Image Residual Learning for Scaling Deeper Vision Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Guoxi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Hongtao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Bors%2C+A+G">Adrian G. Bors</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1280">[1280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14235" title="Abstract">arXiv:2309.14235</a> (replaced) [<a href="/pdf/2309.14235" title="Download PDF">pdf</a>, <a href="/format/2309.14235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stackelberg Driver Model for Continual Policy Improvement in  Scenario-Based Closed-Loop Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niu%2C+H">Haoyi Niu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qimao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jianming Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1281">[1281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14717" title="Abstract">arXiv:2309.14717</a> (replaced) [<a href="/pdf/2309.14717" title="Download PDF">pdf</a>, <a href="/format/2309.14717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuhui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lingxi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+X">Xiaotao Gu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+H">Heng Chang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hengheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengsu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaopeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Qi Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1282">[1282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14970" title="Abstract">arXiv:2309.14970</a> (replaced) [<a href="/pdf/2309.14970" title="Download PDF">pdf</a>, <a href="/format/2309.14970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recurrent Hypernetworks are Surprisingly Strong in Meta-RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beck%2C+J">Jacob Beck</a>, 
<a href="/search/cs?searchtype=author&query=Vuorio%2C+R">Risto Vuorio</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zheng Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Whiteson%2C+S">Shimon Whiteson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1283">[1283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15019" title="Abstract">arXiv:2309.15019</a> (replaced) [<a href="/pdf/2309.15019" title="Download PDF">pdf</a>, <a href="/format/2309.15019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IFT: Image Fusion Transformer for Ghost-free High Dynamic Range Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hailing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+Y">Yuanyuan Xi</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jie Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hanting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Longyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhe Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1284">[1284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15112" title="Abstract">arXiv:2309.15112</a> (replaced) [<a href="/pdf/2309.15112" title="Download PDF">pdf</a>, <a href="/format/2309.15112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InternLM-XComposer: A Vision-Language Large Model for Advanced  Text-image Comprehension and Composition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiaoyi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yuhang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+L">Linke Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhiyuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+S">Shuangrui Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Songyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+H">Haodong Duan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingwen Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Conghui He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingcheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and models are available at <a href="https://github.com/InternLM/InternLM-XComposer">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1285">[1285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15490" title="Abstract">arXiv:2309.15490</a> (replaced) [<a href="/pdf/2309.15490" title="Download PDF">pdf</a>, <a href="/format/2309.15490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Survey on Deep Face Restoration: From Non-blind to Blind and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juncheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoming Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuhang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+G">Guangwei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Weihong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chia-Wen Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Face restoration, Survey, Deep learning, Non-blind/Blind, Joint restoration tasks, Facial priors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1286">[1286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15523" title="Abstract">arXiv:2309.15523</a> (replaced) [<a href="/pdf/2309.15523" title="Download PDF">pdf</a>, <a href="/format/2309.15523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Facade Parsing with Vision Transformers and Line Integration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bowen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ran Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunqin Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Nakashima%2C+Y">Yuta Nakashima</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1287">[1287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15630" title="Abstract">arXiv:2309.15630</a> (replaced) [<a href="/pdf/2309.15630" title="Download PDF">pdf</a>, <a href="/format/2309.15630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NLPBench: Evaluating Large Language Models on Solving NLP Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linxin Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jieyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Lechao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pengyuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+I">Irene Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1288">[1288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15806" title="Abstract">arXiv:2309.15806</a> (replaced) [<a href="/pdf/2309.15806" title="Download PDF">pdf</a>, <a href="/format/2309.15806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lyra: Orchestrating Dual Correction in Automated Theorem Proving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chuanyang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiankai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+H">Huajian Xin</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jianhao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yu Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tech Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1289">[1289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15857" title="Abstract">arXiv:2309.15857</a> (replaced) [<a href="/pdf/2309.15857" title="Download PDF">pdf</a>, <a href="/format/2309.15857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Image-text Multimodal Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+R">Ruifeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jingxuan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Linzhuang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bihui Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+G">Guiyong Chang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dawei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sibo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zhengbing Yao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mingjun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+L">Liping Bu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1290">[1290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16118" title="Abstract">arXiv:2309.16118</a> (replaced) [<a href="/pdf/2309.16118" title="Download PDF">pdf</a>, <a href="/format/2309.16118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable  Robotic Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yixuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuoran Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingtong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Driggs-Campbell%2C+K">Katherine Driggs-Campbell</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiajun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fei-Fei%2C+L">Li Fei-Fei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunzhu Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://robopil.github.io/d3fields/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1291">[1291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16178" title="Abstract">arXiv:2309.16178</a> (replaced) [<a href="/pdf/2309.16178" title="Download PDF">pdf</a>, <a href="/format/2309.16178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LAE-ST-MoE: Boosted Language-Aware Encoder Using Speech Translation  Auxiliary Task for E2E Code-switching ASR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+G">Guodong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuke Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuting Yang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Binbin Du</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Haoran Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1292">[1292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16375" title="Abstract">arXiv:2309.16375</a> (replaced) [<a href="/pdf/2309.16375" title="Download PDF">pdf</a>, <a href="/format/2309.16375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Review on Tree Detection Methods Using Point Cloud and  Aerial Imagery from Unmanned Aerial Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuang%2C+W">Weijie Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+H+W">Hann Woei Ho</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Ye Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Suandi%2C+S+A">Shahrel Azmin Suandi</a>, 
<a href="/search/cs?searchtype=author&query=Ismail%2C+F">Farzad Ismail</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been submitted to Computers and Electronics in Agriculture for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1293">[1293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16426" title="Abstract">arXiv:2309.16426</a> (replaced) [<a href="/pdf/2309.16426" title="Download PDF">pdf</a>, <a href="/format/2309.16426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QwenGrasp: A Usage of Large Vision-Language Model for Target-Oriented  Grasping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zonghan He</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haobin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuhui Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1294">[1294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16742" title="Abstract">arXiv:2309.16742</a> (replaced) [<a href="/pdf/2309.16742" title="Download PDF">pdf</a>, <a href="/format/2309.16742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supervised Learning Models for Early Detection of Albuminuria Risk in  Type-2 Diabetes Mellitus Patients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muharram%2C+A+P">Arief Purnama Muharram</a>, 
<a href="/search/cs?searchtype=author&query=Tahapary%2C+D+L">Dicky Levenus Tahapary</a>, 
<a href="/search/cs?searchtype=author&query=Lestari%2C+Y+D">Yeni Dwi Lestari</a>, 
<a href="/search/cs?searchtype=author&query=Sarayar%2C+R">Randy Sarayar</a>, 
<a href="/search/cs?searchtype=author&query=Dirjayanto%2C+V+J">Valerie Josephine Dirjayanto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented in the 10th International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item1295">[1295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16772" title="Abstract">arXiv:2309.16772</a> (replaced) [<a href="/pdf/2309.16772" title="Download PDF">pdf</a>, <a href="/format/2309.16772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XVO: Generalized Visual Odometry via Cross-Modal Self-Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+L">Lei Lai</a>, 
<a href="/search/cs?searchtype=author&query=Shangguan%2C+Z">Zhongkai Shangguan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jimuyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ohn-Bar%2C+E">Eshed Ohn-Bar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023, Paris <a href="https://genxvo.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1296">[1296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16940" title="Abstract">arXiv:2309.16940</a> (replaced) [<a href="/pdf/2309.16940" title="Download PDF">pdf</a>, <a href="/format/2309.16940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asynchrony-Robust Collaborative Perception via Bird&#x27;s Eye View Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+S">Sizhe Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yuxi Wei</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yue Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yifan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yiqi Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures. Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1297">[1297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16951" title="Abstract">arXiv:2309.16951</a> (replaced) [<a href="/pdf/2309.16951" title="Download PDF">pdf</a>, <a href="/format/2309.16951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Tides and Time: Machine Learning Triumph in Water Quality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+Y">Yinpu Li</a>, 
<a href="/search/stat?searchtype=author&query=Mao%2C+S">Siqi Mao</a>, 
<a href="/search/stat?searchtype=author&query=Yuan%2C+Y">Yaping Yuan</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+Z">Ziren Wang</a>, 
<a href="/search/stat?searchtype=author&query=Kang%2C+Y">Yixin Kang</a>, 
<a href="/search/stat?searchtype=author&query=Yao%2C+Y">Yuanxin Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1298">[1298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16971" title="Abstract">arXiv:2309.16971</a> (replaced) [<a href="/pdf/2309.16971" title="Download PDF">pdf</a>, <a href="/format/2309.16971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Resolution Active Learning of Fourier Neural Operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shibo Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+W">Wei Xing</a>, 
<a href="/search/cs?searchtype=author&query=Kirby%2C+M">Mike Kirby</a>, 
<a href="/search/cs?searchtype=author&query=Narayan%2C+A">Akil Narayan</a>, 
<a href="/search/cs?searchtype=author&query=Zhe%2C+S">Shandian Zhe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1299">[1299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17389" title="Abstract">arXiv:2309.17389</a> (replaced) [<a href="/pdf/2309.17389" title="Download PDF">pdf</a>, <a href="/format/2309.17389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt-based test-time real image dehazing: a novel pipeline
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zixuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zewei He</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Ziqian Lu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhe-Ming Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> update github link (<a href="https://github.com/cecret3350/PTTD-Dehazing">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1300">[1300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00068" title="Abstract">arXiv:2310.00068</a> (replaced) [<a href="/pdf/2310.00068" title="Download PDF">pdf</a>, <a href="/format/2310.00068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emotional Listener Portrait: Neural Listener Head Generation with  Emotion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Luchuan Song</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+G">Guojun Yin</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhenchao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiaoyi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenliang Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1301">[1301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00077" title="Abstract">arXiv:2310.00077</a> (replaced) [<a href="/pdf/2310.00077" title="Download PDF">pdf</a>, <a href="/format/2310.00077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing with Low Budgets: a Comparison on the Black-box Optimization  Benchmarking Suite and OpenAI Gym
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raponi%2C+E">Elena Raponi</a>, 
<a href="/search/cs?searchtype=author&query=Carraz%2C+N+R">Nathanael Rakotonirina Carraz</a>, 
<a href="/search/cs?searchtype=author&query=Rapin%2C+J">J&#xe9;r&#xe9;my Rapin</a>, 
<a href="/search/cs?searchtype=author&query=Doerr%2C+C">Carola Doerr</a>, 
<a href="/search/cs?searchtype=author&query=Teytaud%2C+O">Olivier Teytaud</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1302">[1302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00156" title="Abstract">arXiv:2310.00156</a> (replaced) [<a href="/pdf/2310.00156" title="Download PDF">pdf</a>, <a href="/format/2310.00156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Generalizable Tool-use Skills through Trajectory Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+C">Carl Qi</a>, 
<a href="/search/cs?searchtype=author&query=Shetty%2C+S">Sarthak Shetty</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xingyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Held%2C+D">David Held</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1303">[1303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00212" title="Abstract">arXiv:2310.00212</a> (replaced) [<a href="/pdf/2310.00212" title="Download PDF">pdf</a>, <a href="/format/2310.00212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for  LLM Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Banghua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruoyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Zhaojin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Ramchandran%2C+K">Kannan Ramchandran</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J">Jiantao Jiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1304">[1304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00239" title="Abstract">arXiv:2310.00239</a> (replaced) [<a href="/pdf/2310.00239" title="Download PDF">pdf</a>, <a href="/format/2310.00239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaptNet: Policy Adaptation for Physics-Based Character Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Pei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+K">Kaixiang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Andrews%2C+S">Sheldon Andrews</a>, 
<a href="/search/cs?searchtype=author&query=Kry%2C+P+G">Paul G. Kry</a>, 
<a href="/search/cs?searchtype=author&query=Neff%2C+M">Michael Neff</a>, 
<a href="/search/cs?searchtype=author&query=McGuire%2C+M">Morgan McGuire</a>, 
<a href="/search/cs?searchtype=author&query=Karamouzas%2C+I">Ioannis Karamouzas</a>, 
<a href="/search/cs?searchtype=author&query=Zordan%2C+V">Victor Zordan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGGRAPH Asia 2023. Video: <a href="https://youtu.be/WxmJSCNFb28.">this https URL</a> Website: <a href="https://motion-lab.github.io/AdaptNet">this https URL</a>, <a href="https://pei-xu.github.io/AdaptNet">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ACM Transactions on Graphics 42, 6, Article 112.1522 (December
  2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1305">[1305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00289" title="Abstract">arXiv:2310.00289</a> (replaced) [<a href="/pdf/2310.00289" title="Download PDF">pdf</a>, <a href="/format/2310.00289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pubic Symphysis-Fetal Head Segmentation Using Pure Transformer with  Bi-level Routing Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cai%2C+P">Pengzhou Cai</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+J">Jiang Lu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yanxin Li</a>, 
<a href="/search/eess?searchtype=author&query=Lan%2C+L">Libin Lan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1306">[1306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00299" title="Abstract">arXiv:2310.00299</a> (replaced) [<a href="/pdf/2310.00299" title="Download PDF">pdf</a>, <a href="/format/2310.00299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RelBERT: Embedding Relations with Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ushio%2C+A">Asahi Ushio</a>, 
<a href="/search/cs?searchtype=author&query=Camacho-Collados%2C+J">Jose Camacho-Collados</a>, 
<a href="/search/cs?searchtype=author&query=Schockaert%2C+S">Steven Schockaert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1307">[1307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00378" title="Abstract">arXiv:2310.00378</a> (replaced) [<a href="/pdf/2310.00378" title="Download PDF">pdf</a>, <a href="/format/2310.00378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Value Understanding in Language Models through  Discriminator-Critique Gap
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaowei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+F">Fengshuo Bai</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1308">[1308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00451" title="Abstract">arXiv:2310.00451</a> (replaced) [<a href="/pdf/2310.00451" title="Download PDF">pdf</a>, <a href="/format/2310.00451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Role of Neural Collapse in Meta Learning Models for Few-shot  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Medepalli%2C+S">Saaketh Medepalli</a>, 
<a href="/search/cs?searchtype=author&query=Doraiswamy%2C+N">Naren Doraiswamy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1309">[1309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00498" title="Abstract">arXiv:2310.00498</a> (replaced) [<a href="/pdf/2310.00498" title="Download PDF">pdf</a>, <a href="/format/2310.00498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Gait Generation For Walking, Soft Robotic Quadrupeds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ketchum%2C+J">Jake Ketchum</a>, 
<a href="/search/cs?searchtype=author&query=Schiffer%2C+S">Sophia Schiffer</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Muchen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Kaarthik%2C+P">Pranav Kaarthik</a>, 
<a href="/search/cs?searchtype=author&query=Truby%2C+R+L">Ryan L. Truby</a>, 
<a href="/search/cs?searchtype=author&query=Murphey%2C+T+D">Todd D. Murphey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 Pages, 6 Figures, Published at IROS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1310">[1310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00526" title="Abstract">arXiv:2310.00526</a> (replaced) [<a href="/pdf/2310.00526" title="Download PDF">pdf</a>, <a href="/format/2310.00526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Graph Neural Networks Optimal Approximation Algorithms?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yau%2C+M">Morris Yau</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+E">Eric Lu</a>, 
<a href="/search/cs?searchtype=author&query=Karalias%2C+N">Nikolaos Karalias</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jessica Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jegelka%2C+S">Stefanie Jegelka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated figure 1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item1311">[1311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00533" title="Abstract">arXiv:2310.00533</a> (replaced) [<a href="/pdf/2310.00533" title="Download PDF">pdf</a>, <a href="/format/2310.00533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SELF: Language-Driven Self-Evolution for Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jianqiao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+W">Wanjun Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenyong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yufei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+F">Fei Mi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baojun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, 6 tables. Due to the limitation "The abstract field cannot be longer than 1,920 characters", the abstract appearing here is slightly shorter than that in the PDF file
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1312">[1312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00650" title="Abstract">arXiv:2310.00650</a> (replaced) [<a href="/pdf/2310.00650" title="Download PDF">pdf</a>, <a href="/format/2310.00650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quasi-Monte Carlo for unbounded integrands with importance sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ouyang%2C+D">Du Ouyang</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+X">Xiaoqun Wang</a>, 
<a href="/search/math?searchtype=author&query=He%2C+Z">Zhijian He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1313">[1313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00672" title="Abstract">arXiv:2310.00672</a> (replaced) [<a href="/pdf/2310.00672" title="Download PDF">pdf</a>, <a href="/format/2310.00672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GeRA: Label-Efficient Geometrically Regularized Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klebe%2C+D">Dustin Klebe</a>, 
<a href="/search/cs?searchtype=author&query=Shnitzer%2C+T">Tal Shnitzer</a>, 
<a href="/search/cs?searchtype=author&query=Yurochkin%2C+M">Mikhail Yurochkin</a>, 
<a href="/search/cs?searchtype=author&query=Karlinsky%2C+L">Leonid Karlinsky</a>, 
<a href="/search/cs?searchtype=author&query=Solomon%2C+J">Justin Solomon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1314">[1314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00887" title="Abstract">arXiv:2310.00887</a> (replaced) [<a href="/pdf/2310.00887" title="Download PDF">pdf</a>, <a href="/format/2310.00887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRID: A Platform for General Robot Intelligence Development
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vemprala%2C+S">Sai Vemprala</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shuhang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shukla%2C+A">Abhinav Shukla</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+D">Dinesh Narayanan</a>, 
<a href="/search/cs?searchtype=author&query=Kapoor%2C+A">Ashish Kapoor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1315">[1315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00934" title="Abstract">arXiv:2310.00934</a> (replaced) [<a href="/pdf/2310.00934" title="Download PDF">pdf</a>, <a href="/ps/2310.00934" title="Download PostScript">ps</a>, <a href="/format/2310.00934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a control-theoretic trivialization of ABR video streaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fliess%2C+M">Michel Fliess</a>, 
<a href="/search/eess?searchtype=author&query=Join%2C+C">C&#xe9;dric Join</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Networking and Internet Architecture (cs.NI); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1316">[1316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00954" title="Abstract">arXiv:2310.00954</a> (replaced) [<a href="/pdf/2310.00954" title="Download PDF">pdf</a>, <a href="/format/2310.00954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Helpful do Novice Programmers Find the Feedback of an Automated  Repair Tool?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurniawan%2C+O">Oka Kurniawan</a>, 
<a href="/search/cs?searchtype=author&query=Poskitt%2C+C+M">Christopher M. Poskitt</a>, 
<a href="/search/cs?searchtype=author&query=Hoque%2C+I+A">Ismam Al Hoque</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+N+T+S">Norman Tiong Seng Lee</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%A9gourel%2C+C">Cyrille J&#xe9;gourel</a>, 
<a href="/search/cs?searchtype=author&query=Sockalingam%2C+N">Nachamma Sockalingam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Experience report accepted by the International Conference on Teaching, Assessment, and Learning for Engineering (TALE'23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item1317">[1317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01074" title="Abstract">arXiv:2310.01074</a> (replaced) [<a href="/pdf/2310.01074" title="Download PDF">pdf</a>, <a href="/format/2310.01074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Back to the Future: Towards Explainable Temporal Reasoning with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chenhan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jimin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ananiadou%2C+S">Sophia Ananiadou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures, code and dataset: <a href="https://github.com/chenhan97/TimeLlama">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1318">[1318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01352" title="Abstract">arXiv:2310.01352</a> (replaced) [<a href="/pdf/2310.01352" title="Download PDF">pdf</a>, <a href="/format/2310.01352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RA-DIT: Retrieval-Augmented Dual Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+X+V">Xi Victoria Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xilun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingda Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weijia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Lomeli%2C+M">Maria Lomeli</a>, 
<a href="/search/cs?searchtype=author&query=James%2C+R">Rich James</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+P">Pedro Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Kahn%2C+J">Jacob Kahn</a>, 
<a href="/search/cs?searchtype=author&query=Szilvasy%2C+G">Gergely Szilvasy</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+M">Mike Lewis</a>, 
<a href="/search/cs?searchtype=author&query=Zettlemoyer%2C+L">Luke Zettlemoyer</a>, 
<a href="/search/cs?searchtype=author&query=Yih%2C+S">Scott Yih</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1319">[1319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01412" title="Abstract">arXiv:2310.01412</a> (replaced) [<a href="/pdf/2310.01412" title="Download PDF">pdf</a>, <a href="/format/2310.01412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DriveGPT4: Interpretable End-to-end Autonomous Driving via Large  Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhenhua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yujia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K+K">Kwan-Yee. K. Wong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hengshuang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The project page is available at <a href="https://tonyxuqaq.github.io/projects/DriveGPT4/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1320">[1320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01432" title="Abstract">arXiv:2310.01432</a> (replaced) [<a href="/pdf/2310.01432" title="Download PDF">pdf</a>, <a href="/format/2310.01432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Split and Merge: Aligning Position Biases in Large Language Model based  Evaluators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaozheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+P">Pingchuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Daoyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Cuiyun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1321">[1321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01701" title="Abstract">arXiv:2310.01701</a> (replaced) [<a href="/pdf/2310.01701" title="Download PDF">pdf</a>, <a href="/format/2310.01701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transcending Domains through Text-to-Image Diffusion: A Source-Free  Approach to Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chopra%2C+S">Shivang Chopra</a>, 
<a href="/search/cs?searchtype=author&query=Kothawade%2C+S">Suraj Kothawade</a>, 
<a href="/search/cs?searchtype=author&query=Aynaou%2C+H">Houda Aynaou</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1322">[1322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01714" title="Abstract">arXiv:2310.01714</a> (replaced) [<a href="/pdf/2310.01714" title="Download PDF">pdf</a>, <a href="/format/2310.01714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models as Analogical Reasoners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yasunaga%2C+M">Michihiro Yasunaga</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yujia Li</a>, 
<a href="/search/cs?searchtype=author&query=Pasupat%2C+P">Panupong Pasupat</a>, 
<a href="/search/cs?searchtype=author&query=Leskovec%2C+J">Jure Leskovec</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+E+H">Ed H. Chi</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Denny Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1323">[1323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01769" title="Abstract">arXiv:2310.01769</a> (replaced) [<a href="/pdf/2310.01769" title="Download PDF">pdf</a>, <a href="/format/2310.01769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing:  The Curses of Symmetry and Initialization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+N">Nuoya Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Lijun Ding</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S+S">Simon S. Du</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1324">[1324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01801" title="Abstract">arXiv:2310.01801</a> (replaced) [<a href="/pdf/2310.01801" title="Download PDF">pdf</a>, <a href="/format/2310.01801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ge%2C+S">Suyu Ge</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Minjia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review; To be updated
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1325">[1325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01830" title="Abstract">arXiv:2310.01830</a> (replaced) [<a href="/pdf/2310.01830" title="Download PDF">pdf</a>, <a href="/format/2310.01830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-Generated Images as Data Source: The Dawn of Synthetic Era
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zuhao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+F">Fangneng Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kunhao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Muyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shijian Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1326">[1326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01875" title="Abstract">arXiv:2310.01875</a> (replaced) [<a href="/pdf/2310.01875" title="Download PDF">pdf</a>, <a href="/format/2310.01875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Stable Backdoor Purification through Feature Shift Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Min%2C+R">Rui Min</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zeyu Qin</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Minhao Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 paper. The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item1327">[1327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02164" title="Abstract">arXiv:2310.02164</a> (replaced) [<a href="/pdf/2310.02164" title="Download PDF">pdf</a>, <a href="/format/2310.02164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Graph Unlearning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Said%2C+A">Anwar Said</a>, 
<a href="/search/cs?searchtype=author&query=Derr%2C+T">Tyler Derr</a>, 
<a href="/search/cs?searchtype=author&query=Shabbir%2C+M">Mudassir Shabbir</a>, 
<a href="/search/cs?searchtype=author&query=Abbas%2C+W">Waseem Abbas</a>, 
<a href="/search/cs?searchtype=author&query=Koutsoukos%2C+X">Xenofon Koutsoukos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 page review paper on graph unlearning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1328">[1328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02229" title="Abstract">arXiv:2310.02229</a> (replaced) [<a href="/pdf/2310.02229" title="Download PDF">pdf</a>, <a href="/format/2310.02229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extraction of Medication and Temporal Relation from Clinical Text using  Neural Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tu%2C+H">Hangyu Tu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Lifeng Han</a>, 
<a href="/search/cs?searchtype=author&query=Nenadic%2C+G">Goran Nenadic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> working paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1329">[1329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02244" title="Abstract">arXiv:2310.02244</a> (replaced) [<a href="/pdf/2310.02244" title="Download PDF">pdf</a>, <a href="/format/2310.02244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Greg Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dingli Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hayou%2C+S">Soufiane Hayou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item1330">[1330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02519" title="Abstract">arXiv:2310.02519</a> (replaced) [<a href="/pdf/2310.02519" title="Download PDF">pdf</a>, <a href="/format/2310.02519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameterized Convex Minorant for Objective Function Approximation in  Amortized Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jinrae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Youdan Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1331">[1331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02629" title="Abstract">arXiv:2310.02629</a> (replaced) [<a href="/pdf/2310.02629" title="Download PDF">pdf</a>, <a href="/format/2310.02629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BA-MoE: Boundary-Aware Mixture-of-Experts Adapter for Code-Switching  Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Peikun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+Y">Yuhao Lian</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+H">Hongfei Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xucheng Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Naijun Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lei Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ASRU2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1332">[1332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02724" title="Abstract">arXiv:2310.02724</a> (replaced) [<a href="/pdf/2310.02724" title="Download PDF">pdf</a>, <a href="/format/2310.02724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-End Training of a Neural HMM with Label and Transition  Probabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mann%2C+D">Daniel Mann</a>, 
<a href="/search/cs?searchtype=author&query=Raissi%2C+T">Tina Raissi</a>, 
<a href="/search/cs?searchtype=author&query=Michel%2C+W">Wilfried Michel</a>, 
<a href="/search/cs?searchtype=author&query=Schl%C3%BCter%2C+R">Ralf Schl&#xfc;ter</a>, 
<a href="/search/cs?searchtype=author&query=Ney%2C+H">Hermann Ney</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for Presentation at ASRU2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1333">[1333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02854" title="Abstract">arXiv:2310.02854</a> (replaced) [<a href="/pdf/2310.02854" title="Download PDF">pdf</a>, <a href="/format/2310.02854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Domain Causal Representation Learning via Weak Distributional  Invariances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+K">Kartik Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Mansouri%2C+A">Amin Mansouri</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yixin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1334">[1334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02996" title="Abstract">arXiv:2310.02996</a> (replaced) [<a href="/pdf/2310.02996" title="Download PDF">pdf</a>, <a href="/format/2310.02996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Stochastic Dynamic Aggregative Game for Demand-Side  Management in Microgrids with Shared Battery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yadollahi%2C+S">Shahram Yadollahi</a>, 
<a href="/search/eess?searchtype=author&query=Kebriaei%2C+H">Hamed Kebriaei</a>, 
<a href="/search/eess?searchtype=author&query=Soudjani%2C+S">Sadegh Soudjani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1335">[1335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03094" title="Abstract">arXiv:2310.03094</a> (replaced) [<a href="/pdf/2310.03094" title="Download PDF">pdf</a>, <a href="/format/2310.03094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model Cascades with Mixture of Thoughts Representations  for Cost-efficient Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+M">Murong Yue</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+L">Liang Du</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Ziyu Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1336">[1336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03186" title="Abstract">arXiv:2310.03186</a> (replaced) [<a href="/pdf/2310.03186" title="Download PDF">pdf</a>, <a href="/format/2310.03186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inferring Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Raju%2C+R+V">Rajkumar Vasudeva Raju</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+Z">Zhe Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Linderman%2C+S">Scott Linderman</a>, 
<a href="/search/q-bio?searchtype=author&query=Pitkow%2C+X">Xaq Pitkow</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 4 figures and 1 supplementary figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1337">[1337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03270" title="Abstract">arXiv:2310.03270</a> (replaced) [<a href="/e-print/2310.03270" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yefei He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Weijia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+B">Bohan Zhuang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Please withdraw this paper, as required by co-authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1338">[1338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03291" title="Abstract">arXiv:2310.03291</a> (replaced) [<a href="/pdf/2310.03291" title="Download PDF">pdf</a>, <a href="/format/2310.03291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimVLG: Simple and Efficient Pretraining of Visual Language Generative  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jian%2C+Y">Yiren Jian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tingkai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Y">Yunzhe Tao</a>, 
<a href="/search/cs?searchtype=author&query=Vosoughi%2C+S">Soroush Vosoughi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1339">[1339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03334" title="Abstract">arXiv:2310.03334</a> (replaced) [<a href="/pdf/2310.03334" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Untargeted White-box Adversarial Attack with Heuristic Defence Methods  in Real-time Deep Learning based Network Intrusion Detection System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roshan%2C+K">Khushnaseeb Roshan</a>, 
<a href="/search/cs?searchtype=author&query=Zafar%2C+A">Aasim Zafar</a>, 
<a href="/search/cs?searchtype=author&query=Haque%2C+S+B+U">Sheikh Burhan Ul Haque</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1340">[1340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03388" title="Abstract">arXiv:2310.03388</a> (replaced) [<a href="/pdf/2310.03388" title="Download PDF">pdf</a>, <a href="/format/2310.03388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenPatch: a 3D patchwork for Out-Of-Distribution detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rabino%2C+P">Paolo Rabino</a>, 
<a href="/search/cs?searchtype=author&query=Alliegro%2C+A">Antonio Alliegro</a>, 
<a href="/search/cs?searchtype=author&query=Borlino%2C+F+C">Francesco Cappio Borlino</a>, 
<a href="/search/cs?searchtype=author&query=Tommasi%2C+T">Tatiana Tommasi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1341">[1341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03481" title="Abstract">arXiv:2310.03481</a> (replaced) [<a href="/pdf/2310.03481" title="Download PDF">pdf</a>, <a href="/format/2310.03481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Transformer-based Ranking for e-Commerce at Yandex
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khrylchenko%2C+K">Kirill Khrylchenko</a>, 
<a href="/search/cs?searchtype=author&query=Fritzler%2C+A">Alexander Fritzler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 figure, pre-print
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item1342">[1342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03485" title="Abstract">arXiv:2310.03485</a> (replaced) [<a href="/pdf/2310.03485" title="Download PDF">pdf</a>, <a href="/format/2310.03485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BTDNet: a Multi-Modal Approach for Brain Tumor Radiogenomic  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kollias%2C+D">Dimitrios Kollias</a>, 
<a href="/search/eess?searchtype=author&query=Vendal%2C+K">Karanjot Vendal</a>, 
<a href="/search/eess?searchtype=author&query=Gadhavi%2C+P">Priyanka Gadhavi</a>, 
<a href="/search/eess?searchtype=author&query=Russom%2C+S">Solomon Russom</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1343">[1343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03533" title="Abstract">arXiv:2310.03533</a> (replaced) [<a href="/pdf/2310.03533" title="Download PDF">pdf</a>, <a href="/format/2310.03533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Software Engineering: Survey and Open Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+A">Angela Fan</a>, 
<a href="/search/cs?searchtype=author&query=Gokkaya%2C+B">Beliz Gokkaya</a>, 
<a href="/search/cs?searchtype=author&query=Harman%2C+M">Mark Harman</a>, 
<a href="/search/cs?searchtype=author&query=Lyubarskiy%2C+M">Mitya Lyubarskiy</a>, 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+S">Shubho Sengupta</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S">Shin Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+M">Jie M. Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item1344">[1344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03602" title="Abstract">arXiv:2310.03602</a> (replaced) [<a href="/pdf/2310.03602" title="Download PDF">pdf</a>, <a href="/format/2310.03602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout  Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+C">Chuan Fang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaotao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+K">Kunming Luo</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+P">Ping Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1345">[1345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03639" title="Abstract">arXiv:2310.03639</a> (replaced) [<a href="/pdf/2310.03639" title="Download PDF">pdf</a>, <a href="/ps/2310.03639" title="Download PostScript">ps</a>, <a href="/format/2310.03639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Self-Supervised Speech Representations for Indigenous  American Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chih-Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">William Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zevallos%2C+R">Rodolfo Zevallos</a>, 
<a href="/search/cs?searchtype=author&query=Ortega%2C+J+E">John E. Ortega</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1346">[1346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03722" title="Abstract">arXiv:2310.03722</a> (replaced) [<a href="/pdf/2310.03722" title="Download PDF">pdf</a>, <a href="/format/2310.03722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anytime-valid t-tests and confidence sequences for Gaussian means with  unknown variance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+H">Hongjian Wang</a>, 
<a href="/search/math?searchtype=author&query=Ramdas%2C+A">Aaditya Ramdas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1347">[1347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03758" title="Abstract">arXiv:2310.03758</a> (replaced) [<a href="/pdf/2310.03758" title="Download PDF">pdf</a>, <a href="/format/2310.03758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Framework for Uniform Signal Recovery in Nonlinear Generative  Compressed Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+J">Junren Chen</a>, 
<a href="/search/eess?searchtype=author&query=Scarlett%2C+J">Jonathan Scarlett</a>, 
<a href="/search/eess?searchtype=author&query=Ng%2C+M+K">Michael K. Ng</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zhaoqiang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1348">[1348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03813" title="Abstract">arXiv:2310.03813</a> (replaced) [<a href="/pdf/2310.03813" title="Download PDF">pdf</a>, <a href="/format/2310.03813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accurate Cold-start Bundle Recommendation via Popularity-based  Coalescence and Curriculum Heating
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeon%2C+H">Hyunsik Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jong-eun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+J">Jeongin Yun</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+U">U Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1349">[1349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03902" title="Abstract">arXiv:2310.03902</a> (replaced) [<a href="/pdf/2310.03902" title="Download PDF">pdf</a>, <a href="/format/2310.03902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provable benefits of annealing for estimating normalizing constants:  Importance Sampling, Noise-Contrastive Estimation, and beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chehab%2C+O">Omar Chehab</a>, 
<a href="/search/stat?searchtype=author&query=Hyvarinen%2C+A">Aapo Hyvarinen</a>, 
<a href="/search/stat?searchtype=author&query=Risteski%2C+A">Andrej Risteski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1350">[1350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03906" title="Abstract">arXiv:2310.03906</a> (replaced) [<a href="/pdf/2310.03906" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PyDCM: Custom Data Center Models with Reinforcement Learning for  Sustainability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naug%2C+A">Avisek Naug</a>, 
<a href="/search/cs?searchtype=author&query=Guillen%2C+A">Antonio Guillen</a>, 
<a href="/search/cs?searchtype=author&query=Guti%C3%A9rrez%2C+R+L">Ricardo Luna Guti&#xe9;rrez</a>, 
<a href="/search/cs?searchtype=author&query=Gundecha%2C+V">Vineet Gundecha</a>, 
<a href="/search/cs?searchtype=author&query=Markovikj%2C+D">Dejan Markovikj</a>, 
<a href="/search/cs?searchtype=author&query=Kashyap%2C+L+D">Lekhapriya Dheeraj Kashyap</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+L">Lorenz Krause</a>, 
<a href="/search/cs?searchtype=author&query=Ghorbanpour%2C+S">Sahand Ghorbanpour</a>, 
<a href="/search/cs?searchtype=author&query=Mousavi%2C+S">Sajad Mousavi</a>, 
<a href="/search/cs?searchtype=author&query=Babu%2C+A+R">Ashwin Ramesh Babu</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Soumyendu Sarkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation (BuildSys '23), November 15--16, 2023, Istanbul, Turkey
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1351">[1351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03912" title="Abstract">arXiv:2310.03912</a> (replaced) [<a href="/pdf/2310.03912" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RTDK-BO: High Dimensional Bayesian Optimization with Reinforced  Transformer Deep kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shmakov%2C+A">Alexander Shmakov</a>, 
<a href="/search/cs?searchtype=author&query=Naug%2C+A">Avisek Naug</a>, 
<a href="/search/cs?searchtype=author&query=Gundecha%2C+V">Vineet Gundecha</a>, 
<a href="/search/cs?searchtype=author&query=Ghorbanpour%2C+S">Sahand Ghorbanpour</a>, 
<a href="/search/cs?searchtype=author&query=Gutierrez%2C+R+L">Ricardo Luna Gutierrez</a>, 
<a href="/search/cs?searchtype=author&query=Babu%2C+A+R">Ashwin Ramesh Babu</a>, 
<a href="/search/cs?searchtype=author&query=Guillen%2C+A">Antonio Guillen</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Soumyendu Sarkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1352">[1352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03965" title="Abstract">arXiv:2310.03965</a> (replaced) [<a href="/pdf/2310.03965" title="Download PDF">pdf</a>, <a href="/format/2310.03965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thought Propagation: An Analogical Approach to Complex Reasoning with  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Junchi Yu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ran He</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+R">Rex Ying</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1353">[1353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04015" title="Abstract">arXiv:2310.04015</a> (replaced) [<a href="/pdf/2310.04015" title="Download PDF">pdf</a>, <a href="/format/2310.04015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anonymous Learning via Look-Alike Clustering: A Precise Analysis of  Model Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Javanmard%2C+A">Adel Javanmard</a>, 
<a href="/search/cs?searchtype=author&query=Mirrokni%2C+V">Vahab Mirrokni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at the Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1354">[1354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04047" title="Abstract">arXiv:2310.04047</a> (replaced) [<a href="/pdf/2310.04047" title="Download PDF">pdf</a>, <a href="/format/2310.04047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AUTOPARLLM: GNN-Guided Automatic Code Parallelization using Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahmud%2C+Q+I">Quazi Ishtiaque Mahmud</a>, 
<a href="/search/cs?searchtype=author&query=TehraniJamsaz%2C+A">Ali TehraniJamsaz</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+H+D">Hung D Phan</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+N+K">Nesreen K. Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Jannesari%2C+A">Ali Jannesari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1355">[1355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04069" title="Abstract">arXiv:2310.04069</a> (replaced) [<a href="/pdf/2310.04069" title="Download PDF">pdf</a>, <a href="/format/2310.04069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatio-temporal flow patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kosyfaki%2C+C">Chrysanthi Kosyfaki</a>, 
<a href="/search/cs?searchtype=author&query=Mamoulis%2C+N">Nikos Mamoulis</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Reynold Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Kao%2C+B">Ben Kao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item1356">[1356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04080" title="Abstract">arXiv:2310.04080</a> (replaced) [<a href="/pdf/2310.04080" title="Download PDF">pdf</a>, <a href="/format/2310.04080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Average Networks for Monte Carlo Denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalojanov%2C+J">Javor Kalojanov</a>, 
<a href="/search/cs?searchtype=author&query=Thurston%2C+K">Kimball Thurston</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
</div>
</dd>
<dt><a name="item1357">[1357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04171" title="Abstract">arXiv:2310.04171</a> (replaced) [<a href="/pdf/2310.04171" title="Download PDF">pdf</a>, <a href="/format/2310.04171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Heehyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jinhyeok Choi</a>, 
<a href="/search/cs?searchtype=author&query=Whang%2C+J+J">Joyce Jiyoung Whang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, 3 tables. Machine Learning on Graphs (MLoG) Workshop at the 23rd IEEE International Conference on Data Mining (ICDM 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item1358">[1358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04241" title="Abstract">arXiv:2310.04241</a> (replaced) [<a href="/pdf/2310.04241" title="Download PDF">pdf</a>, <a href="/format/2310.04241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Reinforcement Learning Efficiency with Auxiliary Tasks in  Non-Visual Environments: A Comparison
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lange%2C+M">Moritz Lange</a>, 
<a href="/search/cs?searchtype=author&query=Krystiniak%2C+N">Noah Krystiniak</a>, 
<a href="/search/cs?searchtype=author&query=Engelhardt%2C+R+C">Raphael C. Engelhardt</a>, 
<a href="/search/cs?searchtype=author&query=Konen%2C+W">Wolfgang Konen</a>, 
<a href="/search/cs?searchtype=author&query=Wiskott%2C+L">Laurenz Wiskott</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at LOD 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1359">[1359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04268" title="Abstract">arXiv:2310.04268</a> (replaced) [<a href="/pdf/2310.04268" title="Download PDF">pdf</a>, <a href="/format/2310.04268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Workload-aware and Learned Z-Indexes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pai%2C+S">Sachith Pai</a>, 
<a href="/search/cs?searchtype=author&query=Mathioudakis%2C+M">Michael Mathioudakis</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanhao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fixed grammatical error in abstract
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1360">[1360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04292" title="Abstract">arXiv:2310.04292</a> (replaced) [<a href="/pdf/2310.04292" title="Download PDF">pdf</a>, <a href="/format/2310.04292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Foundational Models for Molecular Learning on Large-Scale  Multi-Task Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beaini%2C+D">Dominique Beaini</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shenyang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Cunha%2C+J+A">Joao Alex Cunha</a>, 
<a href="/search/cs?searchtype=author&query=Moisescu-Pareja%2C+G">Gabriela Moisescu-Pareja</a>, 
<a href="/search/cs?searchtype=author&query=Dymov%2C+O">Oleksandr Dymov</a>, 
<a href="/search/cs?searchtype=author&query=Maddrell-Mander%2C+S">Samuel Maddrell-Mander</a>, 
<a href="/search/cs?searchtype=author&query=McLean%2C+C">Callum McLean</a>, 
<a href="/search/cs?searchtype=author&query=Wenkel%2C+F">Frederik Wenkel</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+L">Luis M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Mohamud%2C+J+H">Jama Hussein Mohamud</a>, 
<a href="/search/cs?searchtype=author&query=Parviz%2C+A">Ali Parviz</a>, 
<a href="/search/cs?searchtype=author&query=Craig%2C+M">Michael Craig</a>, 
<a href="/search/cs?searchtype=author&query=Koziarski%2C+M">Micha&#x142; Koziarski</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiarui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhaocheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Gabellini%2C+C">Cristian Gabellini</a>, 
<a href="/search/cs?searchtype=author&query=Klaser%2C+K">Kerstin Klaser</a>, 
<a href="/search/cs?searchtype=author&query=Dean%2C+J">Josef Dean</a>, 
<a href="/search/cs?searchtype=author&query=Wognum%2C+C">Cas Wognum</a>, 
<a href="/search/cs?searchtype=author&query=Sypetkowski%2C+M">Maciej Sypetkowski</a>, 
<a href="/search/cs?searchtype=author&query=Rabusseau%2C+G">Guillaume Rabusseau</a>, 
<a href="/search/cs?searchtype=author&query=Rabbany%2C+R">Reihaneh Rabbany</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jian Tang</a>, 
<a href="/search/cs?searchtype=author&query=Morris%2C+C">Christopher Morris</a>, 
<a href="/search/cs?searchtype=author&query=Koutis%2C+I">Ioannis Koutis</a>, 
<a href="/search/cs?searchtype=author&query=Ravanelli%2C+M">Mirco Ravanelli</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+G">Guy Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Tossou%2C+P">Prudencio Tossou</a>, 
<a href="/search/cs?searchtype=author&query=Mary%2C+H">Hadrien Mary</a>, 
<a href="/search/cs?searchtype=author&query=Bois%2C+T">Therence Bois</a>, 
<a href="/search/cs?searchtype=author&query=Fitzgibbon%2C+A">Andrew Fitzgibbon</a>, 
<a href="/search/cs?searchtype=author&query=Banaszewski%2C+B">B&#x142;a&#x17c;ej Banaszewski</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+C">Chad Martin</a>, 
<a href="/search/cs?searchtype=author&query=Masters%2C+D">Dominic Masters</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1361">[1361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04315" title="Abstract">arXiv:2310.04315</a> (replaced) [<a href="/pdf/2310.04315" title="Download PDF">pdf</a>, <a href="/format/2310.04315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fostering Enterprise Conversations Around Data on Collaboration  Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyeok Kim</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+A">Arjun Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Brehmer%2C+M">Matthew Brehmer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item1362">[1362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04342" title="Abstract">arXiv:2310.04342</a> (replaced) [<a href="/pdf/2310.04342" title="Download PDF">pdf</a>, <a href="/format/2310.04342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minerva: Decentralized Collaborative Query Processing over  InterPlanetary File System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zhiyi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+B">Bowen Ding</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Q">Qianlan Bai</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuedong Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item1363">[1363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04343" title="Abstract">arXiv:2310.04343</a> (replaced) [<a href="/pdf/2310.04343" title="Download PDF">pdf</a>, <a href="/format/2310.04343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Functional Geometry Guided Protein Sequence and Backbone Structure  Co-Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhenqiao Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yunlong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wenxian Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1364">[1364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04367" title="Abstract">arXiv:2310.04367</a> (replaced) [<a href="/pdf/2310.04367" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Marketplace Price Anomaly Detection System at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sarpal%2C+A">Akshit Sarpal</a>, 
<a href="/search/stat?searchtype=author&query=Kang%2C+Q">Qiwen Kang</a>, 
<a href="/search/stat?searchtype=author&query=Huang%2C+F">Fangping Huang</a>, 
<a href="/search/stat?searchtype=author&query=Song%2C+Y">Yang Song</a>, 
<a href="/search/stat?searchtype=author&query=Wan%2C+L">Lijie Wan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1365">[1365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04414" title="Abstract">arXiv:2310.04414</a> (replaced) [<a href="/pdf/2310.04414" title="Download PDF">pdf</a>, <a href="/format/2310.04414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model  Generalization Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaoxiao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+X">Xingjian Leng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zijian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Liang Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1366">[1366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04417" title="Abstract">arXiv:2310.04417</a> (replaced) [<a href="/pdf/2310.04417" title="Download PDF">pdf</a>, <a href="/format/2310.04417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Random Feature Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Saha%2C+E">Esha Saha</a>, 
<a href="/search/stat?searchtype=author&query=Tran%2C+G">Giang Tran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 Figures, 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item771">Cross-lists</a></li>
<li><a href="#item859">Replacements</a></li>
</ul>
<small>[ total of 1366 entries:  <b>1-1366</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
