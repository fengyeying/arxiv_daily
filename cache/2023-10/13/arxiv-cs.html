<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Wed 11 Oct 23  to  Thu 12 Oct 23, announced Fri, 13 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item401">Cross-lists</a></li>
<li><a href="#item443">Replacements</a></li>
</ul>
<small>[ total of 688 entries:  <b>1-688</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Fri, 13 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07718" title="Abstract">arXiv:2310.07718</a> [<a href="/pdf/2310.07718" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long-term and Real-time High-speed Underwater Wireless Optical  Communications in Deep Sea
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jialiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sujing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Ziqi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+G">Guanjun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yonggang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shanguo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Optics (physics.optics)

</div>
<p class="mathjax">Seafloor observation network can perform all-weather, long-term, continuous,
real-time, and in-situ observation of the ocean by combing various observation
methods including cabled seafloor nodes, self-contained nodes, as well as
mobile platforms, where reliable and long-term high-speed underwater wireless
communication becomes an essential demand. Recently, underwater wireless
optical communication (UWOC) has emerged as a highly promising solution and is
rapidly becoming a research hotspot for meeting this requirement. In this
article, we demonstrate the experiment and application of high-speed UWOC
system for deep sea seafloor observation network. To the best of our knowledge
this is the first long-term real-time deep-sea UWOC link with bitrate as high
as 125 Mbps. Between 30 m distance and at a depth of 1650 m, two-way Ethernet
UWOC links are realized with 125 Mbps direction-adjustable green light link and
6.25 Mbps non-line-of-sight (NLOS) blue light link. High quality video
transmission of 8K 30 FPS and 4K 120 FPS are realized through high-speed 125
Mbps green light link, with 100% peak signal-to-noise ratio (PSNR) agreement,
showing the capability of transmitting high-quality videos lossless. The 30-day
long-term measurement results show that the BER performance of both 125 Mbps
and 6.25 Mbps links is lower than 10-5, proving the stability and reliability
of this UWOC system at depth of 1650 m. The maximum transmission distance for
the green and blue light links are estimated to be 117.7 and 128.3 m with
considering the geometry loss, which can be extended to 231.6 and 337.5 m
without geometry loss. As the first long-term and real-time UWOC system in deep
sea, we believe this demonstration can provide valuable experience for further
UWOC studies and converged ocean observation networking with cabled and
cable-less observation platforms.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07720" title="Abstract">arXiv:2310.07720</a> [<a href="/pdf/2310.07720" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parametric Leaky Tanh: A New Hybrid Activation Function for Deep  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mastromichalakis%2C+S">Stamatis Mastromichalakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages,2 figures,1 table,1 Python code snippets
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Activation functions (AFs) are crucial components of deep neural networks
(DNNs), having a significant impact on their performance. An activation
function in a DNN is typically a smooth, nonlinear function that transforms an
input signal into an output signal for the subsequent layer. In this paper, we
propose the Parametric Leaky Tanh (PLTanh), a novel hybrid activation function
designed to combine the strengths of both the Tanh and Leaky ReLU (LReLU)
activation functions. PLTanh is differentiable at all points and addresses the
'dying ReLU' problem by ensuring a non-zero gradient for negative inputs,
consistent with the behavior of LReLU. By integrating the unique advantages of
these two diverse activation functions, PLTanh facilitates the learning of more
intricate nonlinear relationships within the network. This paper presents an
empirical evaluation of PLTanh against established activation functions, namely
ReLU, LReLU, and ALReLU utilizing five diverse datasets.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07721" title="Abstract">arXiv:2310.07721</a> [<a href="/pdf/2310.07721" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing the concentration ratio of multi-faceted focusing heliostats
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Henault%2C+F">F. Henault</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optics (physics.optics)

</div>
<p class="mathjax">This technical note aims at optimizing the concentration ratio of
multi-faceted focusing heliostats implemented into a solar tower power plant.
The ideal shape of a heliostat located off-axis in the field is known to be the
local section of a fictitious parabolo\"id whose parameters are varying
continuously with the Sun angular position. We describe an optimization
procedure applicable to those heliostats. The flux densities formed at the
solar receiver and the achievable concentrating ratios are computed using an
improved convolution algorithm. It is shown that the optimized heliostat shape
can produce typical concentration gains of approximately 10%, even when the
heliostats reflect the Sun under large incidence angles.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07723" title="Abstract">arXiv:2310.07723</a> [<a href="/pdf/2310.07723" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equitable and Fair Performance Evaluation of Whale Optimization  Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hassan%2C+B+A">Bryar A. Hassan</a>, 
<a href="/search/cs?searchtype=author&query=Rashid%2C+T+A">Tarik A. Rashid</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+A">Aram Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Qader%2C+S+M">Shko M. Qader</a>, 
<a href="/search/cs?searchtype=author&query=Majidpour%2C+J">Jaffer Majidpour</a>, 
<a href="/search/cs?searchtype=author&query=Abdalla%2C+M+H">Mohmad Hussein Abdalla</a>, 
<a href="/search/cs?searchtype=author&query=Tayfor%2C+N">Noor Tayfor</a>, 
<a href="/search/cs?searchtype=author&query=Hamarashid%2C+H+K">Hozan K. Hamarashid</a>, 
<a href="/search/cs?searchtype=author&query=Sidqi%2C+H">Haval Sidqi</a>, 
<a href="/search/cs?searchtype=author&query=Noori%2C+K+A">Kaniaw A. Noori</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">It is essential that all algorithms are exhaustively, somewhat, and
intelligently evaluated. Nonetheless, evaluating the effectiveness of
optimization algorithms equitably and fairly is not an easy process for various
reasons. Choosing and initializing essential parameters, such as the size
issues of the search area for each method and the number of iterations required
to reduce the issues, might be particularly challenging. As a result, this
chapter aims to contrast the Whale Optimization Algorithm (WOA) with the most
recent algorithms on a selected set of benchmark problems with varying
benchmark function hardness scores and initial control parameters comparable
problem dimensions and search space. When solving a wide range of numerical
optimization problems with varying difficulty scores, dimensions, and search
areas, the experimental findings suggest that WOA may be statistically superior
or inferior to the preceding algorithms referencing convergence speed, running
time, and memory utilization.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07724" title="Abstract">arXiv:2310.07724</a> [<a href="/pdf/2310.07724" title="Download PDF">pdf</a>, <a href="/format/2310.07724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Forecasting as a Mid-level Representation for Avoidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hsuan-Kung Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+T">Tsung-Chih Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Ting-Ru Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chun-Wei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jou-Min Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Chun-Yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tsung-Chih Chiang, Ting-Ru Liu, Chun-Wei Huang, and Jou-Min Liu contributed equally to this work; This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The challenge of navigation in environments with dynamic objects continues to
be a central issue in the study of autonomous agents. While predictive methods
hold promise, their reliance on precise state information makes them less
practical for real-world implementation. This study presents visual forecasting
as an innovative alternative. By introducing intuitive visual cues, this
approach projects the future trajectories of dynamic objects to improve agent
perception and enable anticipatory actions. Our research explores two distinct
strategies for conveying predictive information through visual forecasting: (1)
sequences of bounding boxes, and (2) augmented paths. To validate the proposed
visual forecasting strategies, we initiate evaluations in simulated
environments using the Unity engine and then extend these evaluations to
real-world scenarios to assess both practicality and effectiveness. The results
confirm the viability of visual forecasting as a promising solution for
navigation and obstacle avoidance in dynamic environments.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07725" title="Abstract">arXiv:2310.07725</a> [<a href="/pdf/2310.07725" title="Download PDF">pdf</a>, <a href="/format/2310.07725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extreme Image Transformations Facilitate Robust Latent Object  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malik%2C+G">Girik Malik</a>, 
<a href="/search/cs?searchtype=author&query=Crowder%2C+D">Dakarai Crowder</a>, 
<a href="/search/cs?searchtype=author&query=Mingolla%2C+E">Ennio Mingolla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Adversarial attacks can affect the object recognition capabilities of
machines in wild. These can often result from spurious correlations between
input and class labels, and are prone to memorization in large networks. While
networks are expected to do automated feature selection, it is not effective at
the scale of the object. Humans, however, are able to select the minimum set of
features required to form a robust representation of an object. In this work,
we show that finetuning any pretrained off-the-shelf network with Extreme Image
Transformations (EIT) not only helps in learning a robust latent
representation, it also improves the performance of these networks against
common adversarial attacks of various intensities. Our EIT trained networks
show strong activations in the object regions even when tested with more
intense noise, showing promising generalizations across different kinds of
adversarial attacks.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07726" title="Abstract">arXiv:2310.07726</a> [<a href="/pdf/2310.07726" title="Download PDF">pdf</a>, <a href="/format/2310.07726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards the Vulnerability of Watermarking Artificial Intelligence  Generated Content
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanlin Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yifei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Shangwei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianwei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Artificial Intelligence Generated Content (AIGC) is gaining great popularity
in social media, with many commercial services available. These services
leverage advanced generative models, such as latent diffusion models and large
language models, to generate creative content (e.g., realistic images, fluent
sentences) for users. The usage of such generated content needs to be highly
regulated, as the service providers need to ensure the users do not violate the
usage policies (e.g., abuse for commercialization, generating and distributing
unsafe content).
<br />Numerous watermarking approaches have been proposed recently. However, in
this paper, we show that an adversary can easily break these watermarking
mechanisms. Specifically, we consider two possible attacks. (1) Watermark
removal: the adversary can easily erase the embedded watermark from the
generated content and then use it freely without the regulation of the service
provider. (2) Watermark forge: the adversary can create illegal content with
forged watermarks from another user, causing the service provider to make wrong
attributions. We propose WMaGi, a unified framework to achieve both attacks in
a holistic way. The key idea is to leverage a pre-trained diffusion model for
content processing, and a generative adversarial network for watermark removing
or forging. We evaluate WMaGi on different datasets and embedding setups. The
results prove that it can achieve high success rates while maintaining the
quality of the generated content. Compared with existing diffusion model-based
attacks, WMaGi is 5,050$\sim$11,000$\times$ faster.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07727" title="Abstract">arXiv:2310.07727</a> [<a href="/pdf/2310.07727" title="Download PDF">pdf</a>, <a href="/format/2310.07727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning based Systems for Crater Detection: A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tewari%2C+A">Atal Tewari</a>, 
<a href="/search/cs?searchtype=author&query=Prateek%2C+K">K Prateek</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Amrita Singh</a>, 
<a href="/search/cs?searchtype=author&query=Khanna%2C+N">Nitin Khanna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Craters are one of the most prominent features on planetary surfaces, used in
applications such as age estimation, hazard detection, and spacecraft
navigation. Crater detection is a challenging problem due to various aspects,
including complex crater characteristics such as varying sizes and shapes, data
resolution, and planetary data types. Similar to other computer vision tasks,
deep learning-based approaches have significantly impacted research on crater
detection in recent years. This survey aims to assist researchers in this field
by examining the development of deep learning-based crater detection algorithms
(CDAs). The review includes over 140 research works covering diverse crater
detection approaches, including planetary data, craters database, and
evaluation metrics. To be specific, we discuss the challenges in crater
detection due to the complex properties of the craters and survey the DL-based
CDAs by categorizing them into three parts: (a) semantic segmentation-based,
(b) object detection-based, and (c) classification-based. Additionally, we have
conducted training and testing of all the semantic segmentation-based CDAs on a
common dataset to evaluate the effectiveness of each architecture for crater
detection and its potential applications. Finally, we have provided
recommendations for potential future works.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07728" title="Abstract">arXiv:2310.07728</a> [<a href="/pdf/2310.07728" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Algorithm for the Generation of Three-Dimensional Accessibility Ramps  in Grasshopper / Rhinoceros 7
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Antonio Li</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+L">Leila Yi</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+B+Y+P">Brandon Yeo Pei Hui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Often overlooked as a component of urban development, accessibility
infrastructure is undeniably crucial in daily life. Accessibility ramps are one
of the most common types of accessibility infrastructure, and serve to benefit
not only people with mobile impairments but also able-bodied third parties.
While the necessity of accessibility ramps is acknowledged, actual
implementation fails in light of the limits of manpower required for the design
stage. In response, we present an algorithm capable of the automatic generation
of a feasible accessibility ramp based on a 3D model of the relevant
environment. Through the manual specification of initial and terminal points
within a 3D model, the algorithm uses AI search algorithms to determine the
optimal pathway connecting these points. Essential components in devising a
wheelchair-accessible ramp are encoded within the process, as evaluated by the
algorithm, including but not limited to elevation differentials, spatial
constraints, and gradient specifications. From this, the algorithm then
generates the pathway to be expanded into a full-scale, usable model of a ramp,
which then can be easily exported and transformed through inter-software
exchanges. Though some human input is still required following the generation
stage, the minimising of human resources provides significant boosts of
efficiency in the design process thus lowering the threshold for the
incorporation of accessibility features in future urban design.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07729" title="Abstract">arXiv:2310.07729</a> [<a href="/pdf/2310.07729" title="Download PDF">pdf</a>, <a href="/format/2310.07729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Aware Routing Algorithm for Mobile Ground-to-Air Charging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+B">Bill Cai</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+F">Fei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Lifeng Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">We investigate the problem of energy-constrained planning for a cooperative
system of an Unmanned Ground Vehicles (UGV) and an Unmanned Aerial Vehicle
(UAV). In scenarios where the UGV serves as a mobile base to ferry the UAV and
as a charging station to recharge the UAV, we formulate a novel
energy-constrained routing problem. To tackle this problem, we design an
energy-aware routing algorithm, aiming to minimize the overall mission duration
under the energy limitations of both vehicles. The algorithm first solves a
Traveling Salesman Problem (TSP) to generate a guided tour. Then, it employs
the Monte-Carlo Tree Search (MCTS) algorithm to refine the tour and generate
paths for the two vehicles. We evaluate the performance of our algorithm
through extensive simulations and a proof-of-concept experiment. The results
show that our algorithm consistently achieves near-optimal mission time and
maintains fast running time across a wide range of problem instances.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07730" title="Abstract">arXiv:2310.07730</a> [<a href="/pdf/2310.07730" title="Download PDF">pdf</a>, <a href="/format/2310.07730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain-Controlled Prompt Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qinglong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhengqin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuantian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaokang Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Large pre-trained vision-language models, such as CLIP, have shown remarkable
generalization capabilities across various tasks when appropriate text prompts
are provided. However, adapting these models to specialized domains, like
remote sensing images (RSIs), medical images, etc, remains unexplored and
challenging. Existing prompt learning methods often lack domain-awareness or
domain-transfer mechanisms, leading to suboptimal performance due to the
misinterpretation of specialized images in natural image patterns. To tackle
this dilemma, we proposed a Domain-Controlled Prompt Learning for the
specialized domains. Specifically, the large-scale specialized domain
foundation model (LSDM) is first introduced to provide essential specialized
domain knowledge. Using lightweight neural networks, we transfer this knowledge
into domain biases, which control both the visual and language branches to
obtain domain-adaptive prompts in a directly incorporating manner.
Simultaneously, to overcome the existing overfitting challenge, we propose a
novel noisy-adding strategy, without extra trainable parameters, to help the
model escape the suboptimal solution in a global domain oscillation manner.
Experimental results show our method achieves state-of-the-art performance in
specialized domain image recognition datasets. Our code is available at
https://anonymous.4open.science/r/DCPL-8588.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07731" title="Abstract">arXiv:2310.07731</a> [<a href="/pdf/2310.07731" title="Download PDF">pdf</a>, <a href="/format/2310.07731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Robot Task Planning to Secure Human Group Progress
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Godet%2C+R">Roland Godet</a> (ONERA and LAAS-CNRS), 
<a href="/search/cs?searchtype=author&query=Lesire%2C+C">Charles Lesire</a> (ONERA), 
<a href="/search/cs?searchtype=author&query=Bit-Monnot%2C+A">Arthur Bit-Monnot</a> (LAAS-CNRS)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings AREA 2023, <a href="/abs/2310.00333">arXiv:2310.00333</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 391, 2023, pp. 113-126
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA); Performance (cs.PF)

</div>
<p class="mathjax">Recent years have seen an increasing number of deployment of fleets of
autonomous vehicles. As the problem scales up, in terms of autonomous vehicles
number and complexity of their objectives, there is a growing need for
decision-support tooling to help the operators in controlling the fleet.
<br />In this paper, we present an automated planning system developed to assist
the operators in the CoHoMa II challenge, where a fleet of robots, remotely
controlled by a handful of operators, must explore and progress through a
potential hostile area. In this context, we use planning to provide the
operators with suggestions about the actions to consider and their allocation
to the robots.
<br />This paper especially focus on the modelling of the problem as a hierarchical
planning problem for which we use a state-of-the-art automated solver.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07736" title="Abstract">arXiv:2310.07736</a> [<a href="/pdf/2310.07736" title="Download PDF">pdf</a>, <a href="/format/2310.07736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Observatory: Characterizing Embeddings of Relational Tables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cong%2C+T">Tianji Cong</a>, 
<a href="/search/cs?searchtype=author&query=Hulsebos%2C+M">Madelon Hulsebos</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhenjie Sun</a>, 
<a href="/search/cs?searchtype=author&query=Groth%2C+P">Paul Groth</a>, 
<a href="/search/cs?searchtype=author&query=Jagadish%2C+H+V">H. V. Jagadish</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under revision
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Language models and specialized table embedding models have recently
demonstrated strong performance on many tasks over tabular data. Researchers
and practitioners are keen to leverage these models in many new application
contexts; but limited understanding of the strengths and weaknesses of these
models, and the table representations they generate, makes the process of
finding a suitable model for a given task reliant on trial and error. There is
an urgent need to gain a comprehensive understanding of these models to
minimize inefficiency and failures in downstream usage.
<br />To address this need, we propose Observatory, a formal framework to
systematically analyze embedding representations of relational tables.
Motivated both by invariants of the relational data model and by statistical
considerations regarding data distributions, we define eight primitive
properties, and corresponding measures to quantitatively characterize table
embeddings for these properties. Based on these properties, we define an
extensible framework to evaluate language and table embedding models. We
collect and synthesize a suite of datasets and use Observatory to analyze seven
such models. Our analysis provides insights into the strengths and weaknesses
of learned representations over tables. We find, for example, that some models
are sensitive to table structure such as column order, that functional
dependencies are rarely reflected in embeddings, and that specialized table
embedding models have relatively lower sample fidelity. Such insights help
researchers and practitioners better anticipate model behaviors and select
appropriate models for their downstream tasks, while guiding researchers in the
development of new models.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07739" title="Abstract">arXiv:2310.07739</a> [<a href="/pdf/2310.07739" title="Download PDF">pdf</a>, <a href="/format/2310.07739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identity Collapse? Realignment of Taiwanese Voters in the 2024  Presidential Elections on Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+H+H">Ho-Chun Herbert Chang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+S">Sunny Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The 2024 Taiwanese Presidential Election is not just a critical geopolitical
event, it also engages with long-standing debate in politics regarding the
factors that lead to the rise of new political parties and candidates. In 2021,
the Economist called Taiwan "the most dangerous place on earth" due to its
critical role in a fragile supply chain. Additionally, a four-candidate race
has emerged in a traditionally bipartisan election which begs the question: how
will voters realign given the choice of four candidates? Leveraging more than a
million posts on social media, we analyze user (predominantly Taiwanese)
discourse and engagement along the axes of national identity, issue topic, and
partisan alignment. Results reveal alternative candidates (Ko and Gou) draw
attention from the fringes rather than the center relative to national
identity, and traditional candidates derive more engagement from the
traditional media and salience to geopolitical issues. Crucially, in-group
references generate more engagement than out-group references, contrary to
Western-based studies. We discuss how the dissolution of Taiwan's single-issue
society may not just lead to more viable candidates and multi-issue discourse,
but the misalignment of national and partisan identity may heal deep-seated
partisan cleavages.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07743" title="Abstract">arXiv:2310.07743</a> [<a href="/pdf/2310.07743" title="Download PDF">pdf</a>, <a href="/format/2310.07743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PointHR: Exploring High-Resolution Architectures for 3D Point Cloud  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+H">Haibo Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Baosheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yixin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at \url{<a href="https://github.com/haibo-qiu/PointHR">this https URL</a>}
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Significant progress has been made recently in point cloud segmentation
utilizing an encoder-decoder framework, which initially encodes point clouds
into low-resolution representations and subsequently decodes high-resolution
predictions. Inspired by the success of high-resolution architectures in image
dense prediction, which always maintains a high-resolution representation
throughout the entire learning process, we consider it also highly important
for 3D dense point cloud analysis. Therefore, in this paper, we explore
high-resolution architectures for 3D point cloud segmentation. Specifically, we
generalize high-resolution architectures using a unified pipeline named
PointHR, which includes a knn-based sequence operator for feature extraction
and a differential resampling operator to efficiently communicate different
resolutions. Additionally, we propose to avoid numerous on-the-fly computations
of high-resolution architectures by pre-computing the indices for both sequence
and resampling operators. By doing so, we deliver highly competitive
high-resolution architectures while capitalizing on the benefits of
well-designed point cloud blocks without additional effort. To evaluate these
architectures for dense point cloud analysis, we conduct thorough experiments
using S3DIS and ScanNetV2 datasets, where the proposed PointHR outperforms
recent state-of-the-art methods without any bells and whistles. The source code
is available at \url{https://github.com/haibo-qiu/PointHR}.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07744" title="Abstract">arXiv:2310.07744</a> [<a href="/pdf/2310.07744" title="Download PDF">pdf</a>, <a href="/format/2310.07744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Terrain-adaptive Central Pattern Generators with Reinforcement Learning  for Hexapod Locomotion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qiyue Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yue Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shaoyuan Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Inspired by biological motion generation, central pattern generators (CPGs)
is frequently employed in legged robot locomotion control to produce natural
gait pattern with low-dimensional control signals. However, the limited
adaptability and stability over complex terrains hinder its application. To
address this issue, this paper proposes a terrain-adaptive locomotion control
method that incorporates deep reinforcement learning (DRL) framework into CPG,
where the CPG model is responsible for the generation of synchronized signals,
providing basic locomotion gait, while DRL is integrated to enhance the
adaptability of robot towards uneven terrains by adjusting the parameters of
CPG mapping functions. The experiments conducted on the hexapod robot in Isaac
Gym simulation environment demonstrated the superiority of the proposed method
in terrain-adaptability, convergence rate and reward design complexity.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07745" title="Abstract">arXiv:2310.07745</a> [<a href="/pdf/2310.07745" title="Download PDF">pdf</a>, <a href="/format/2310.07745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Palmer%2C+G">Gregory Palmer</a>, 
<a href="/search/cs?searchtype=author&query=Parry%2C+C">Chris Parry</a>, 
<a href="/search/cs?searchtype=author&query=Harrold%2C+D+J+B">Daniel J.B. Harrold</a>, 
<a href="/search/cs?searchtype=author&query=Willis%2C+C">Chris Willis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 60 pages, 14 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The rapid increase in the number of cyber-attacks in recent years raises the
need for principled methods for defending networks against malicious actors.
Deep reinforcement learning (DRL) has emerged as a promising approach for
mitigating these attacks. However, while DRL has shown much potential for
cyber-defence, numerous challenges must be overcome before DRL can be applied
to autonomous cyber-operations (ACO) at scale. Principled methods are required
for environments that confront learners with very high-dimensional state
spaces, large multi-discrete action spaces, and adversarial learning. Recent
works have reported success in solving these problems individually. There have
also been impressive engineering efforts towards solving all three for
real-time strategy games. However, applying DRL to the full ACO problem remains
an open challenge. Here, we survey the relevant DRL literature and
conceptualize an idealised ACO-DRL agent. We provide: i.) A summary of the
domain properties that define the ACO problem; ii.) A comprehensive evaluation
of the extent to which domains used for benchmarking DRL approaches are
comparable to ACO; iii.) An overview of state-of-the-art approaches for scaling
DRL to domains that confront learners with the curse of dimensionality, and;
iv.) A survey and critique of current methods for limiting the exploitability
of agents within adversarial settings from the perspective of ACO. We conclude
with open research questions that we hope will motivate future directions for
researchers and practitioners working on ACO.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07747" title="Abstract">arXiv:2310.07747</a> [<a href="/pdf/2310.07747" title="Download PDF">pdf</a>, <a href="/format/2310.07747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accountability in Offline Reinforcement Learning: Explaining Decisions  with a Corpus of Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hao Sun</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCy%C3%BCk%2C+A">Alihan H&#xfc;y&#xfc;k</a>, 
<a href="/search/cs?searchtype=author&query=Jarrett%2C+D">Daniel Jarrett</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO); Systems and Control (eess.SY)

</div>
<p class="mathjax">Learning transparent, interpretable controllers with offline data in
decision-making systems is an essential area of research due to its potential
to reduce the risk of applications in real-world systems. However, in
responsibility-sensitive settings such as healthcare, decision accountability
is of paramount importance, yet has not been adequately addressed by the
literature. This paper introduces the Accountable Offline Controller (AOC) that
employs the offline dataset as the Decision Corpus and performs accountable
control based on a tailored selection of examples, referred to as the Corpus
Subset. ABC operates effectively in low-data scenarios, can be extended to the
strictly offline imitation setting, and displays qualities of both conservation
and adaptability. We assess ABC's performance in both simulated and real-world
healthcare scenarios, emphasizing its capability to manage offline control
tasks with high levels of performance while maintaining accountability.
<br />Keywords: Interpretable Reinforcement Learning, Explainable Reinforcement
Learning, Reinforcement Learning Transparency, Offline Reinforcement Learning,
Batched Control.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07748" title="Abstract">arXiv:2310.07748</a> [<a href="/pdf/2310.07748" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implementation of Fuzzy Control Algorithm in Two-Wheeled Differential  Drive Platform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+G">Guoyi Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Designing and developing Artificial Intelligence controllers on separately
dedicated chips have many advantages. This report reviews the development of a
real-time fuzzy logic controller for optimizing locomotion control of a
two-wheeled differential drive platform using an Arduino Uno board. Based on
the Raspberry Pi board, fuzzy sets are used to optimize color recognition,
enabling the color sensor to correctly recognize color at long distances,
across a wide range of light intensity, and with high fault tolerance.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07749" title="Abstract">arXiv:2310.07749</a> [<a href="/pdf/2310.07749" title="Download PDF">pdf</a>, <a href="/format/2310.07749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+J">Jie An</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhengyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+K">Kevin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lijuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jiebo Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This work investigates a challenging task named open-domain interleaved
image-text generation, which generates interleaved texts and images following
an input query. We propose a new interleaved generation framework based on
prompting large-language models (LLMs) and pre-trained text-to-image (T2I)
models, namely OpenLEAF. In OpenLEAF, the LLM generates textual descriptions,
coordinates T2I models, creates visual prompts for generating images, and
incorporates global contexts into the T2I models. This global context improves
the entity and style consistencies of images in the interleaved generation. For
model assessment, we first propose to use large multi-modal models (LMMs) to
evaluate the entity and style consistencies of open-domain interleaved
image-text sequences. According to the LMM evaluation on our constructed
evaluation set, the proposed interleaved generation framework can generate
high-quality image-text content for various domains and applications, such as
how-to question answering, storytelling, graphical story rewriting, and
webpage/poster generation tasks. Moreover, we validate the effectiveness of the
proposed LMM evaluation technique with human assessment. We hope our proposed
framework, benchmark, and LMM evaluation could help establish the intriguing
interleaved image-text generation task.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07756" title="Abstract">arXiv:2310.07756</a> [<a href="/pdf/2310.07756" title="Download PDF">pdf</a>, <a href="/format/2310.07756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised Representation Learning From Random Data Projectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sui%2C+Y">Yi Sui</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tongzi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cresswell%2C+J+C">Jesse C. Cresswell</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+G">Ga Wu</a>, 
<a href="/search/cs?searchtype=author&query=Stein%2C+G">George Stein</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaoshi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaochen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Volkovs%2C+M">Maksims Volkovs</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Self-supervised representation learning~(SSRL) has advanced considerably by
exploiting the transformation invariance assumption under artificially designed
data augmentations. While augmentation-based SSRL algorithms push the
boundaries of performance in computer vision and natural language processing,
they are often not directly applicable to other data modalities, and can
conflict with application-specific data augmentation constraints. This paper
presents an SSRL approach that can be applied to any data modality and network
architecture because it does not rely on augmentations or masking.
Specifically, we show that high-quality data representations can be learned by
reconstructing random data projections. We evaluate the proposed approach on a
wide range of representation learning tasks that span diverse modalities and
real-world applications. We show that it outperforms multiple state-of-the-art
SSRL baselines. Due to its wide applicability and strong empirical results, we
argue that learning from randomness is a fruitful research direction worthy of
attention and further study.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07765" title="Abstract">arXiv:2310.07765</a> [<a href="/pdf/2310.07765" title="Download PDF">pdf</a>, <a href="/format/2310.07765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature Learning and Generalization in Deep Networks with Orthogonal  Weights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Day%2C+H">Hannah Day</a>, 
<a href="/search/cs?searchtype=author&query=Kahn%2C+Y">Yonatan Kahn</a>, 
<a href="/search/cs?searchtype=author&query=Roberts%2C+D+A">Daniel A. Roberts</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v1: 30+11 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; High Energy Physics - Phenomenology (hep-ph); High Energy Physics - Theory (hep-th); Machine Learning (stat.ML)

</div>
<p class="mathjax">Fully-connected deep neural networks with weights initialized from
independent Gaussian distributions can be tuned to criticality, which prevents
the exponential growth or decay of signals propagating through the network.
However, such networks still exhibit fluctuations that grow linearly with the
depth of the network, which may impair the training of networks with width
comparable to depth. We show analytically that rectangular networks with tanh
activations and weights initialized from the ensemble of orthogonal matrices
have corresponding preactivation fluctuations which are independent of depth,
to leading order in inverse width. Moreover, we demonstrate numerically that,
at initialization, all correlators involving the neural tangent kernel (NTK)
and its descendants at leading order in inverse width -- which govern the
evolution of observables during training -- saturate at a depth of $\sim 20$,
rather than growing without bound as in the case of Gaussian initializations.
We speculate that this structure preserves finite-width feature learning while
reducing overall noise, thus improving both generalization and training speed.
We provide some experimental justification by relating empirical measurements
of the NTK to the superior performance of deep nonlinear orthogonal networks
trained under full-batch gradient descent on the MNIST and CIFAR-10
classification tasks.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07771" title="Abstract">arXiv:2310.07771</a> [<a href="/pdf/2310.07771" title="Download PDF">pdf</a>, <a href="/format/2310.07771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DrivingDiffusion: Layout-Guided multi-view driving scene video  generation with latent diffusion model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaofan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xiaoqing Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the increasing popularity of autonomous driving based on the powerful
and unified bird's-eye-view (BEV) representation, a demand for high-quality and
large-scale multi-view video data with accurate annotation is urgently
required. However, such large-scale multi-view data is hard to obtain due to
expensive collection and annotation costs. To alleviate the problem, we propose
a spatial-temporal consistent diffusion framework DrivingDiffusion, to generate
realistic multi-view videos controlled by 3D layout. There are three challenges
when synthesizing multi-view videos given a 3D layout: How to keep 1)
cross-view consistency and 2) cross-frame consistency? 3) How to guarantee the
quality of the generated instances? Our DrivingDiffusion solves the problem by
cascading the multi-view single-frame image generation step, the single-view
video generation step shared by multiple cameras, and post-processing that can
handle long video generation. In the multi-view model, the consistency of
multi-view images is ensured by information exchange between adjacent cameras.
In the temporal model, we mainly query the information that needs attention in
subsequent frame generation from the multi-view images of the first frame. We
also introduce the local prompt to effectively improve the quality of generated
instances. In post-processing, we further enhance the cross-view consistency of
subsequent frames and extend the video length by employing temporal sliding
window algorithm. Without any extra cost, our model can generate large-scale
realistic multi-camera driving videos in complex urban scenes, fueling the
downstream driving tasks. The code will be made publicly available.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07779" title="Abstract">arXiv:2310.07779</a> [<a href="/pdf/2310.07779" title="Download PDF">pdf</a>, <a href="/format/2310.07779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Approval and Network Homophily as Motivators of Online Toxicity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Julie Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Luceri%2C+L">Luca Luceri</a>, 
<a href="/search/cs?searchtype=author&query=Walther%2C+J+B">Joseph B. Walther</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 10 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Online hate messaging is a pervasive issue plaguing the well-being of social
media users. This research empirically investigates a novel theory positing
that online hate may be driven primarily by the pursuit of social approval
rather than a direct desire to harm the targets. Results show that toxicity is
homophilous in users' social networks and that a user's propensity for
hostility can be predicted by their social networks. We also illustrate how
receiving greater or fewer social engagements in the form of likes, retweets,
quotes, and replies affects a user's subsequent toxicity. We establish a clear
connection between receiving social approval signals and increases in
subsequent toxicity. Being retweeted plays a particularly prominent role in
escalating toxicity. Results also show that not receiving expected levels of
social approval leads to decreased toxicity. We discuss the important
implications of our research and opportunities to combat online hate.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07780" title="Abstract">arXiv:2310.07780</a> [<a href="/pdf/2310.07780" title="Download PDF">pdf</a>, <a href="/format/2310.07780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Promoting Robustness of Randomized Smoothing: Two Cost-Effective  Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Linbo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hoang%2C+T+N">Trong Nghia Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+L+M">Lam M. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+T">Tsui-Wei Weng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Randomized smoothing has recently attracted attentions in the field of
adversarial robustness to provide provable robustness guarantees on smoothed
neural network classifiers. However, existing works show that vanilla
randomized smoothing usually does not provide good robustness performance and
often requires (re)training techniques on the base classifier in order to boost
the robustness of the resulting smoothed classifier. In this work, we propose
two cost-effective approaches to boost the robustness of randomized smoothing
while preserving its clean performance. The first approach introduces a new
robust training method AdvMacerwhich combines adversarial training and
robustness certification maximization for randomized smoothing. We show that
AdvMacer can improve the robustness performance of randomized smoothing
classifiers compared to SOTA baselines, while being 3x faster to train than
MACER baseline. The second approach introduces a post-processing method EsbRS
which greatly improves the robustness certificate based on building model
ensembles. We explore different aspects of model ensembles that has not been
studied by prior works and propose a novel design methodology to further
improve robustness of the ensemble based on our theoretical analysis.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07781" title="Abstract">arXiv:2310.07781</a> [<a href="/pdf/2310.07781" title="Download PDF">pdf</a>, <a href="/format/2310.07781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D TransUNet: Advancing Medical Image Segmentation through Vision  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jieneng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+J">Jieru Mei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xianhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yongyi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qihang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Q">Qingyue Wei</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiangde Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yutong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Adeli%2C+E">Ehsan Adeli</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lungren%2C+M">Matthew Lungren</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+L">Lei Xing</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Le Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yuille%2C+A">Alan Yuille</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuyin Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and models are available at <a href="https://github.com/Beckschen/3D-TransUNet">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Medical image segmentation plays a crucial role in advancing healthcare
systems for disease diagnosis and treatment planning. The u-shaped
architecture, popularly known as U-Net, has proven highly successful for
various medical image segmentation tasks. However, U-Net's convolution-based
operations inherently limit its ability to model long-range dependencies
effectively. To address these limitations, researchers have turned to
Transformers, renowned for their global self-attention mechanisms, as
alternative architectures. One popular network is our previous TransUNet, which
leverages Transformers' self-attention to complement U-Net's localized
information with the global context. In this paper, we extend the 2D TransUNet
architecture to a 3D network by building upon the state-of-the-art nnU-Net
architecture, and fully exploring Transformers' potential in both the encoder
and decoder design. We introduce two key components: 1) A Transformer encoder
that tokenizes image patches from a convolution neural network (CNN) feature
map, enabling the extraction of global contexts, and 2) A Transformer decoder
that adaptively refines candidate regions by utilizing cross-attention between
candidate proposals and U-Net features. Our investigations reveal that
different medical tasks benefit from distinct architectural designs. The
Transformer encoder excels in multi-organ segmentation, where the relationship
among organs is crucial. On the other hand, the Transformer decoder proves more
beneficial for dealing with small and challenging segmented targets such as
tumor segmentation. Extensive experiments showcase the significant potential of
integrating a Transformer-based encoder and decoder into the u-shaped medical
image segmentation architecture. TransUNet outperforms competitors in various
medical applications.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07782" title="Abstract">arXiv:2310.07782</a> [<a href="/pdf/2310.07782" title="Download PDF">pdf</a>, <a href="/format/2310.07782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An automated approach for improving the inference latency and energy  efficiency of pretrained CNNs by removing irrelevant pixels with focused  convolutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tung%2C+C">Caleb Tung</a>, 
<a href="/search/cs?searchtype=author&query=Eliopoulos%2C+N">Nicholas Eliopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Jajal%2C+P">Purvish Jajal</a>, 
<a href="/search/cs?searchtype=author&query=Ramshankar%2C+G">Gowri Ramshankar</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chen-Yun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Synovic%2C+N">Nicholas Synovic</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuecen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhary%2C+V">Vipin Chaudhary</a>, 
<a href="/search/cs?searchtype=author&query=Thiruvathukal%2C+G+K">George K. Thiruvathukal</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yung-Hsiang Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Computer vision often uses highly accurate Convolutional Neural Networks
(CNNs), but these deep learning models are associated with ever-increasing
energy and computation requirements. Producing more energy-efficient CNNs often
requires model training which can be cost-prohibitive. We propose a novel,
automated method to make a pretrained CNN more energy-efficient without
re-training. Given a pretrained CNN, we insert a threshold layer that filters
activations from the preceding layers to identify regions of the image that are
irrelevant, i.e. can be ignored by the following layers while maintaining
accuracy. Our modified focused convolution operation saves inference latency
(by up to 25%) and energy costs (by up to 22%) on various popular pretrained
CNNs, with little to no loss in accuracy.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07786" title="Abstract">arXiv:2310.07786</a> [<a href="/pdf/2310.07786" title="Download PDF">pdf</a>, <a href="/format/2310.07786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble  Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zheqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yueyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+X">Xu Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Van+Roy%2C+B">Benjamin Van Roy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Real-world applications of contextual bandits often exhibit non-stationarity
due to seasonality, serendipity, and evolving social trends. While a number of
non-stationary contextual bandit learning algorithms have been proposed in the
literature, they excessively explore due to a lack of prioritization for
information of enduring value, or are designed in ways that do not scale in
modern applications with high-dimensional user-specific features and large
action set, or both. In this paper, we introduce a novel non-stationary
contextual bandit algorithm that addresses these concerns. It combines a
scalable, deep-neural-network-based architecture with a carefully designed
exploration mechanism that strategically prioritizes collecting information
with the most lasting value in a non-stationary environment. Through empirical
evaluations on two real-world recommendation datasets, which exhibit pronounced
non-stationarity, we demonstrate that our approach significantly outperforms
the state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07787" title="Abstract">arXiv:2310.07787</a> [<a href="/pdf/2310.07787" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Spark Machine Learning Models to Perform Predictive Analysis on  Flight Ticket Pricing Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wong%2C+P">Philip Wong</a>, 
<a href="/search/cs?searchtype=author&query=Thant%2C+P">Phue Thant</a>, 
<a href="/search/cs?searchtype=author&query=Yadav%2C+P">Pratiksha Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Antaliya%2C+R">Ruta Antaliya</a>, 
<a href="/search/cs?searchtype=author&query=Woo%2C+J">Jongwook Woo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 13 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">This paper discusses predictive performance and processes undertaken on
flight pricing data utilizing r2(r-square) and RMSE that leverages a large
dataset, originally from Expedia.com, consisting of approximately 20 million
records or 4.68 gigabytes. The project aims to determine the best models usable
in the real world to predict airline ticket fares for non-stop flights across
the US. Therefore, good generalization capability and optimized processing
times are important measures for the model.
<br />We will discover key business insights utilizing feature importance and
discuss the process and tools used for our analysis. Four regression machine
learning algorithms were utilized: Random Forest, Gradient Boost Tree, Decision
Tree, and Factorization Machines utilizing Cross Validator and Training
Validator functions for assessing performance and generalization capability.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07788" title="Abstract">arXiv:2310.07788</a> [<a href="/pdf/2310.07788" title="Download PDF">pdf</a>, <a href="/format/2310.07788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> `Finite element approximation for the delayed generalized Burgers-Huxley  equation with weakly singular kernel: Part II Non-Conforming and DG  approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mahajan%2C+S">Sumit Mahajan</a>, 
<a href="/search/math?searchtype=author&query=Khan%2C+A">Arbaz Khan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, the numerical approximation of the generalized Burgers'-Huxley
equation (GBHE) with weakly singular kernels using non-conforming methods will
be presented. Specifically, we discuss two new formulations. The first
formulation is based on the non-conforming finite element method (NCFEM). The
other formulation is based on discontinuous Galerkin finite element methods
(DGFEM). The wellposedness results for both formulations are proved.
<br />Then, a priori error estimates for both the semi-discrete and fully-discrete
schemes are derived.
<br />Specific numerical examples, including some applications for the GBHE with
weakly singular model, are discussed to validate the theoretical results.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07793" title="Abstract">arXiv:2310.07793</a> [<a href="/pdf/2310.07793" title="Download PDF">pdf</a>, <a href="/format/2310.07793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenTKG: Generative Forecasting on Temporal Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+R">Ruotong Liao</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xu Jia</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yunpu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Tresp%2C+V">Volker Tresp</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The rapid advancements in large language models (LLMs) have ignited interest
in the temporal knowledge graph (tKG) domain, where conventional carefully
designed embedding-based and rule-based models dominate. The question remains
open of whether pre-trained LLMs can understand structured temporal relational
data and replace them as the foundation model for temporal relational
forecasting. Therefore, we bring temporal knowledge forecasting into the
generative setting. However, challenges occur in the huge chasms between
complex temporal graph data structure and sequential natural expressions LLMs
can handle, and between the enormous data sizes of tKGs and heavy computation
costs of finetuning LLMs. To address these challenges, we propose a novel
retrieval augmented generation framework that performs generative forecasting
on tKGs named GenTKG, which combines a temporal logical rule-based retrieval
strategy and lightweight parameter-efficient instruction tuning. Extensive
experiments have shown that GenTKG outperforms conventional methods of temporal
relational forecasting under low computation resources. GenTKG also highlights
remarkable transferability with exceeding performance on unseen datasets
without re-training. Our work reveals the huge potential of LLMs in the tKG
domain and opens a new frontier for generative forecasting on tKGs.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07794" title="Abstract">arXiv:2310.07794</a> [<a href="/pdf/2310.07794" title="Download PDF">pdf</a>, <a href="/format/2310.07794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory  Prediction Models for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Changhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pourkeshavarz%2C+M">Mozhgan Pourkeshavarz</a>, 
<a href="/search/cs?searchtype=author&query=Rasouli%2C+A">Amir Rasouli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Benchmarking is a common method for evaluating trajectory prediction models
for autonomous driving. Existing benchmarks rely on datasets, which are biased
towards more common scenarios, such as cruising, and distance-based metrics
that are computed by averaging over all scenarios. Following such a regiment
provides a little insight into the properties of the models both in terms of
how well they can handle different scenarios and how admissible and diverse
their outputs are. There exist a number of complementary metrics designed to
measure the admissibility and diversity of trajectories, however, they suffer
from biases, such as length of trajectories.
<br />In this paper, we propose a new benChmarking paRadIgm for evaluaTing
trajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) a
method for extracting driving scenarios at varying levels of specificity
according to the structure of the roads, models' performance, and data
properties for fine-grained ranking of prediction models; 2) A set of new
bias-free metrics for measuring diversity, by incorporating the characteristics
of a given scenario, and admissibility, by considering the structure of roads
and kinematic compliancy, motivated by real-world driving constraints. 3) Using
the proposed benchmark, we conduct extensive experimentation on a
representative set of the prediction models using the large scale Argoverse
dataset. We show that the proposed benchmark can produce a more accurate
ranking of the models and serve as a means of characterizing their behavior. We
further present ablation studies to highlight contributions of different
elements that are used to compute the proposed metrics.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07795" title="Abstract">arXiv:2310.07795</a> [<a href="/pdf/2310.07795" title="Download PDF">pdf</a>, <a href="/format/2310.07795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ontology Enrichment for Effective Fine-grained Entity Typing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+S">Siru Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiaxin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Pillai%2C+P">Pranav Pillai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Fine-grained entity typing (FET) is the task of identifying specific entity
types at a fine-grained level for entity mentions based on their contextual
information. Conventional methods for FET require extensive human annotation,
which is time-consuming and costly. Recent studies have been developing weakly
supervised or zero-shot approaches. We study the setting of zero-shot FET where
only an ontology is provided. However, most existing ontology structures lack
rich supporting information and even contain ambiguous relations, making them
ineffective in guiding FET. Recently developed language models, though
promising in various few-shot and zero-shot NLP tasks, may face challenges in
zero-shot FET due to their lack of interaction with task-specific ontology. In
this study, we propose OnEFET, where we (1) enrich each node in the ontology
structure with two types of extra information: instance information for
training sample augmentation and topic information to relate types to contexts,
and (2) develop a coarse-to-fine typing algorithm that exploits the enriched
information by training an entailment model with contrasting topics and
instance-based augmented training samples. Our experiments show that OnEFET
achieves high-quality fine-grained entity typing without human annotation,
outperforming existing zero-shot methods by a large margin and rivaling
supervised methods.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07799" title="Abstract">arXiv:2310.07799</a> [<a href="/pdf/2310.07799" title="Download PDF">pdf</a>, <a href="/format/2310.07799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges  Data Distribution Shift across EMR Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhongji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yinghao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xinyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianlong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaohe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yasha Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Liantao Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Due to the limited information about emerging diseases, symptoms are hard to
be noticed and recognized, so that the window for clinical intervention could
be ignored. An effective prognostic model is expected to assist doctors in
making right diagnosis and designing personalized treatment plan, so to
promptly prevent unfavorable outcomes. However, in the early stage of a
disease, limited data collection and clinical experiences, plus the concern out
of privacy and ethics, may result in restricted data availability for
reference, to the extent that even data labels are difficult to mark correctly.
In addition, Electronic Medical Record (EMR) data of different diseases or of
different sources of the same disease can prove to be having serious
cross-dataset feature misalignment problems, greatly mutilating the efficiency
of deep learning models. This article introduces a transfer learning method to
build a transition model from source dataset to target dataset. By way of
constraining the distribution shift of features generated in disparate domains,
domain-invariant features that are exclusively relative to downstream tasks are
captured, so to cultivate a unified domain-invariant encoder across various
task domains to achieve better feature representation. Experimental results of
several target tasks demonstrate that our proposed model outperforms competing
baseline methods and has higher rate of training convergence, especially in
dealing with limited data amount. A multitude of experiences have proven the
efficacy of our method to provide more accurate predictions concerning newly
emergent pandemics and other diseases.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07800" title="Abstract">arXiv:2310.07800</a> [<a href="/pdf/2310.07800" title="Download PDF">pdf</a>, <a href="/format/2310.07800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Attention for Few-shot Learning and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nikpour%2C+B">Bahareh Nikpour</a>, 
<a href="/search/cs?searchtype=author&query=Armanfard%2C+N">Narges Armanfard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Attention mechanisms have exhibited promising potential in enhancing learning
models by identifying salient portions of input data. This is particularly
valuable in scenarios where limited training samples are accessible due to
challenges in data collection and labeling. Drawing inspiration from human
recognition processes, we posit that an AI baseline's performance could be more
accurate and dependable if it is exposed to essential segments of raw data
rather than the entire input dataset, akin to human perception. However, the
task of selecting these informative data segments, referred to as hard
attention finding, presents a formidable challenge. In situations with few
training samples, existing studies struggle to locate such informative regions
due to the large number of training parameters that cannot be effectively
learned from the available limited samples. In this study, we introduce a novel
and practical framework for achieving explainable hard attention finding,
specifically tailored for few-shot learning scenarios, called FewXAT. Our
approach employs deep reinforcement learning to implement the concept of hard
attention, directly impacting raw input data and thus rendering the process
interpretable for human understanding. Through extensive experimentation across
various benchmark datasets, we demonstrate the efficacy of our proposed method.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07801" title="Abstract">arXiv:2310.07801</a> [<a href="/pdf/2310.07801" title="Download PDF">pdf</a>, <a href="/format/2310.07801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trajectory-aware Principal Manifold Framework for Data Augmentation and  Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+E+H">Elvis Han Cui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bingbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+W+K">Weng Kee Wong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Donghui Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME)

</div>
<p class="mathjax">Data augmentation for deep learning benefits model training, image
transformation, medical imaging analysis and many other fields. Many existing
methods generate new samples from a parametric distribution, like the Gaussian,
with little attention to generate samples along the data manifold in either the
input or feature space. In this paper, we verify that there are theoretical and
practical advantages of using the principal manifold hidden in the feature
space than the Gaussian distribution. We then propose a novel trajectory-aware
principal manifold framework to restore the manifold backbone and generate
samples along a specific trajectory. On top of the autoencoder architecture, we
further introduce an intrinsic dimension regularization term to make the
manifold more compact and enable few-shot image generation. Experimental
results show that the novel framework is able to extract more compact manifold
representation, improve classification accuracy and generate smooth
transformation among few samples.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07802" title="Abstract">arXiv:2310.07802</a> [<a href="/pdf/2310.07802" title="Download PDF">pdf</a>, <a href="/format/2310.07802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Information Bottleneck Characterization of the Understanding-Workload  Tradeoff
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sanneman%2C+L">Lindsay Sanneman</a>, 
<a href="/search/cs?searchtype=author&query=Tucker%2C+M">Mycal Tucker</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+J">Julie Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Recent advances in artificial intelligence (AI) have underscored the need for
explainable AI (XAI) to support human understanding of AI systems.
Consideration of human factors that impact explanation efficacy, such as mental
workload and human understanding, is central to effective XAI design. Existing
work in XAI has demonstrated a tradeoff between understanding and workload
induced by different types of explanations. Explaining complex concepts through
abstractions (hand-crafted groupings of related problem features) has been
shown to effectively address and balance this workload-understanding tradeoff.
In this work, we characterize the workload-understanding balance via the
Information Bottleneck method: an information-theoretic approach which
automatically generates abstractions that maximize informativeness and minimize
complexity. In particular, we establish empirical connections between workload
and complexity and between understanding and informativeness through
human-subject experiments. This empirical link between human factors and
information-theoretic concepts provides an important mathematical
characterization of the workload-understanding tradeoff which enables
user-tailored XAI design.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07803" title="Abstract">arXiv:2310.07803</a> [<a href="/pdf/2310.07803" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A general mechanism of humor: reformulating the semantic overlap
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez%2C+J">Javier Mart&#xed;nez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 8 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> HUMOR: International Journal of Humor Research, 2023, 36(4)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This article proposes a cognitive mechanism of humour of general
applicability, not restricted to verbal communication. It is indebted to
Raskin's concept of script overlap, and conforms to the incongruity-resolution
theoretical framework, but it is built on the notion of constraint, an abstract
correspondence between sets of data. Under this view, script overlap is an
outcome of a more abstractly described phenomenon, constraint overlap. The
important concept of the overlooked argument is introduced to characterise the
two overlapping constraints -- overt and covert. Their inputs and outputs are
not directly encoded in utterances, but implicated by them, and their overlap
results in another overlap at the level of the communicated utterances, that
the incongruity reveals. Our hypothesis assumes as a given that the evocation
of such constraints is a cognitive effect of the inferential process by which a
hearer interprets utterances. We base this assumption on Hofstadter's theory of
analogy-making as the essence of human thought. By substituting "stimuli" of
any kind for "utterances" in this model, we obtain a mechanism as easily
applicable to non-verbal communication -- slapstick, cartoons -- and we propose
it describes the necessary and sufficient conditions for a communicative act in
any modality to carry humour.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07805" title="Abstract">arXiv:2310.07805</a> [<a href="/pdf/2310.07805" title="Download PDF">pdf</a>, <a href="/format/2310.07805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Modeling with Phase Stochastic Bridges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianrong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiatao Gu</a>, 
<a href="/search/cs?searchtype=author&query=Dinh%2C+L">Laurent Dinh</a>, 
<a href="/search/cs?searchtype=author&query=Theodorou%2C+E+A">Evangelos A. Theodorou</a>, 
<a href="/search/cs?searchtype=author&query=Susskind%2C+J">Josh Susskind</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+S">Shuangfei Zhai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Diffusion models (DMs) represent state-of-the-art generative models for
continuous inputs. DMs work by constructing a Stochastic Differential Equation
(SDE) in the input space (ie, position space), and using a neural network to
reverse it. In this work, we introduce a novel generative modeling framework
grounded in \textbf{phase space dynamics}, where a phase space is defined as
{an augmented space encompassing both position and velocity.} Leveraging
insights from Stochastic Optimal Control, we construct a path measure in the
phase space that enables efficient sampling. {In contrast to DMs, our framework
demonstrates the capability to generate realistic data points at an early stage
of dynamics propagation.} This early prediction sets the stage for efficient
data generation by leveraging additional velocity information along the
trajectory. On standard image generation benchmarks, our model yields favorable
performance over baselines in the regime of small Number of Function
Evaluations (NFEs). Furthermore, our approach rivals the performance of
diffusion models equipped with efficient sampling techniques, underscoring its
potential as a new tool generative modeling.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07806" title="Abstract">arXiv:2310.07806</a> [<a href="/pdf/2310.07806" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Effect of Technostress on the Perceived Organizational  Commitment by Mediating Role of Individual Innovation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hessari%2C+H">Hassan Hessari</a>, 
<a href="/search/cs?searchtype=author&query=Daneshmandi%2C+F">Fatemeh Daneshmandi</a>, 
<a href="/search/cs?searchtype=author&query=Nategh%2C+T">Tahmineh Nategh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 3 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Purpose: Technology plays a pivotal role in shaping the fate of
organizations, both positively and negatively. One of its detrimental
consequences is the emergence of "Technostress," a form of destructive stress.
This paper investigates the impact of technostress on Perceived Organizational
Commitment (POC) through the lens of individual innovation. The objective is to
provide valuable insights for organizational managers, enabling them to
effectively mitigate the adverse effects of technostress within their teams.
Design/Methodology/Approach: This study utilized a questionnaire survey
conducted within an Engineering Consulting Company in Iran, with 147
individuals participating, selected according to Morgan's table. Findings: The
research findings revealed three crucial insights: (1) Technostress
significantly and negatively influences both POC and individual innovation. (2)
Individual innovation positively and significantly impacts POC. (3) Individual
innovation acts as a mediator between technostress and POC, alleviating the
negative impact of technostress on organizational commitment. Research
Implications: The study underscores the importance for managers to proactively
address technostress-related challenges and promote individual innovation
within their organizations. These efforts are vital in enhancing organizational
commitment among employees. Originality/Value: This research makes a
significant contribution to the field by illuminating the mediating role of
individual innovation in the relationship between technostress and perceived
organizational commitment. Given the close association of employees in
engineering organizations with technology, this study sheds light on the
specific challenges faced by this sector, thereby enhancing our understanding
of technostress effects in the workplace.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07807" title="Abstract">arXiv:2310.07807</a> [<a href="/pdf/2310.07807" title="Download PDF">pdf</a>, <a href="/format/2310.07807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedSym: Unleashing the Power of Entropy for Benchmarking the Algorithms  for Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kiyamousavi%2C+E">Ensiye Kiyamousavi</a>, 
<a href="/search/cs?searchtype=author&query=Kraychev%2C+B">Boris Kraychev</a>, 
<a href="/search/cs?searchtype=author&query=Koychev%2C+I">Ivan Koychev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated learning (FL) is a decentralized machine learning approach where
independent learners process data privately. Its goal is to create a robust and
accurate model by aggregating and retraining local models over multiple rounds.
However, FL faces challenges regarding data heterogeneity and model aggregation
effectiveness. In order to simulate real-world data, researchers use methods
for data partitioning that transform a dataset designated for centralized
learning into a group of sub-datasets suitable for distributed machine learning
with different data heterogeneity. In this paper, we study the currently
popular data partitioning techniques and visualize their main disadvantages:
the lack of precision in the data diversity, which leads to unreliable
heterogeneity indexes, and the inability to incrementally challenge the FL
algorithms. To resolve this problem, we propose a method that leverages entropy
and symmetry to construct 'the most challenging' and controllable data
distributions with gradual difficulty. We introduce a metric to measure data
heterogeneity among the learning agents and a transformation technique that
divides any dataset into splits with precise data diversity. Through a
comparative study, we demonstrate the superiority of our method over existing
FL data partitioning approaches, showcasing its potential to challenge model
aggregation algorithms. Experimental results indicate that our approach
gradually challenges the FL strategies, and the models trained on FedSym
distributions are more distinct.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07809" title="Abstract">arXiv:2310.07809</a> [<a href="/pdf/2310.07809" title="Download PDF">pdf</a>, <a href="/ps/2310.07809" title="Download PostScript">ps</a>, <a href="/format/2310.07809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Robustness of Mechanism Design under Total Variation Distance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Makur%2C+A">Anuran Makur</a>, 
<a href="/search/cs?searchtype=author&query=Mertzanidis%2C+M">Marios Mertzanidis</a>, 
<a href="/search/cs?searchtype=author&query=Psomas%2C+A">Alexandros Psomas</a>, 
<a href="/search/cs?searchtype=author&query=Terzoglou%2C+A">Athina Terzoglou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We study the problem of designing mechanisms when agents' valuation functions
are drawn from unknown and correlated prior distributions. In particular, we
are given a prior distribution $\D$, and we are interested in designing a
(truthful) mechanism that has good performance for all ``true distributions''
that are close to $\D$ in Total Variation (TV) distance. We show that DSIC and
BIC mechanisms in this setting are strongly robust with respect to TV distance,
for any bounded objective function $\Ocal$, extending a recent result of
Brustle et al. (\cite{Brustle2020}, EC 2020). At the heart of our result is a
fundamental duality property of total variation distance. As direct
applications of our result, we (i) demonstrate how to find approximately
revenue-optimal and approximately BIC mechanisms for weakly dependent prior
distributions; (ii) show how to find correlation-robust mechanisms when only
``noisy'' versions of marginals are accessible, extending recent results of Bei
et. al. (\cite{bei2019correlation}, SODA 2019); (iii) prove that
prophet-inequality type guarantees are preserved for correlated priors,
recovering a variant of a result of D{\"u}tting and Kesselheim
(\cite{Dutting19}, EC 2019); (iv) give a new necessary condition for a
correlated distribution to witness an infinite separation in revenue between
simple and optimal mechanisms, complementing recent results of Psomas et al.
(\cite{psomas2022infinite}, NeurIPS 2022); (v) give a new condition for simple
mechanisms to approximate revenue-optimal mechanisms for the case of a single
agent whose type is drawn from a correlated distribution that can be captured
by a Markov Random Field, complementing recent results of Cai and Oikonomou
(\cite{Cai21}, EC 2021).
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07811" title="Abstract">arXiv:2310.07811</a> [<a href="/pdf/2310.07811" title="Download PDF">pdf</a>, <a href="/ps/2310.07811" title="Download PostScript">ps</a>, <a href="/format/2310.07811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online RL in Linearly $q^&#x3c0;$-Realizable MDPs Is as Easy as in Linear  MDPs If You Learn What to Ignore
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weisz%2C+G">Gell&#xe9;rt Weisz</a>, 
<a href="/search/cs?searchtype=author&query=Gy%C3%B6rgy%2C+A">Andr&#xe1;s Gy&#xf6;rgy</a>, 
<a href="/search/cs?searchtype=author&query=Szepesv%C3%A1ri%2C+C">Csaba Szepesv&#xe1;ri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider online reinforcement learning (RL) in episodic Markov decision
processes (MDPs) under the linear $q^\pi$-realizability assumption, where it is
assumed that the action-values of all policies can be expressed as linear
functions of state-action features. This class is known to be more general than
linear MDPs, where the transition kernel and the reward function are assumed to
be linear functions of the feature vectors. As our first contribution, we show
that the difference between the two classes is the presence of states in
linearly $q^\pi$-realizable MDPs where for any policy, all the actions have
approximately equal values, and skipping over these states by following an
arbitrarily fixed policy in those states transforms the problem to a linear
MDP. Based on this observation, we derive a novel (computationally inefficient)
learning algorithm for linearly $q^\pi$-realizable MDPs that simultaneously
learns what states should be skipped over and runs another learning algorithm
on the linear MDP hidden in the problem. The method returns an
$\epsilon$-optimal policy after $\text{polylog}(H, d)/\epsilon^2$ interactions
with the MDP, where $H$ is the time horizon and $d$ is the dimension of the
feature vectors, giving the first polynomial-sample-complexity online RL
algorithm for this setting. The results are proved for the misspecified case,
where the sample complexity is shown to degrade gracefully with the
misspecification error.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07812" title="Abstract">arXiv:2310.07812</a> [<a href="/pdf/2310.07812" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Identification of Stone-Handling Behaviour in Japanese  Macaques Using LabGym Artificial Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ardoin%2C+T">Th&#xe9;o Ardoin</a>, 
<a href="/search/cs?searchtype=author&query=Sueur%2C+C">C&#xe9;dric Sueur</a> (IPHC, ANTHROPO LAB, IUF)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The latest advancements in artificial intelligence technology have opened
doors to the analysis of intricate behaviours. In light of this, ethologists
are actively exploring the potential of these innovations to streamline the
time-intensive process of behavioural analysis using video data. In the realm
of primatology, several tools have been developed for this purpose.
Nonetheless, each of these tools grapples with technical constraints that we
aim to surmount. To address these limitations, we have established a
comprehensive protocol designed to harness the capabilities of a cutting-edge
tool, LabGym. Our primary objective was to evaluate LabGym's suitability for
the analysis of primate behaviour, with a focus on Japanese macaques as our
model subjects. We have successfully developed a model that demonstrates a high
degree of accuracy in detecting Japanese macaques stone-handling behaviour. Our
behavioural analysis model was completed as per our initial expectations and
LabGym succeed to recognise stone-handling behaviour on videos. However, it is
important to note that our study's ability to draw definitive conclusions
regarding the quality of the behavioural analysis is hampered by the absence of
quantitative data within the specified timeframe. Nevertheless, our model
represents the pioneering endeavour, as far as our knowledge extends, in
leveraging LabGym for the analysis of primate behaviours. It lays the
groundwork for potential future research in this promising field.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07814" title="Abstract">arXiv:2310.07814</a> [<a href="/pdf/2310.07814" title="Download PDF">pdf</a>, <a href="/format/2310.07814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explorable Mesh Deformation Subspaces from Unstructured Generative  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maesumi%2C+A">Arman Maesumi</a>, 
<a href="/search/cs?searchtype=author&query=Guerrero%2C+P">Paul Guerrero</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+V+G">Vladimir G. Kim</a>, 
<a href="/search/cs?searchtype=author&query=Fisher%2C+M">Matthew Fisher</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+S">Siddhartha Chaudhuri</a>, 
<a href="/search/cs?searchtype=author&query=Aigerman%2C+N">Noam Aigerman</a>, 
<a href="/search/cs?searchtype=author&query=Ritchie%2C+D">Daniel Ritchie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGGRAPH Asia 2023, 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Exploring variations of 3D shapes is a time-consuming process in traditional
3D modeling tools. Deep generative models of 3D shapes often feature continuous
latent spaces that can, in principle, be used to explore potential variations
starting from a set of input shapes. In practice, doing so can be problematic:
latent spaces are high dimensional and hard to visualize, contain shapes that
are not relevant to the input shapes, and linear paths through them often lead
to sub-optimal shape transitions. Furthermore, one would ideally be able to
explore variations in the original high-quality meshes used to train the
generative model, not its lower-quality output geometry. In this paper, we
present a method to explore variations among a given set of landmark shapes by
constructing a mapping from an easily-navigable 2D exploration space to a
subspace of a pre-trained generative model. We first describe how to find a
mapping that spans the set of input landmark shapes and exhibits smooth
variations between them. We then show how to turn the variations in this
subspace into deformation fields, to transfer those variations to high-quality
meshes for the landmark shapes. Our results show that our method can produce
visually-pleasing and easily-navigable 2D exploration spaces for several
different shape categories, especially as compared to prior work on learning
deformation spaces for 3D shapes.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07815" title="Abstract">arXiv:2310.07815</a> [<a href="/pdf/2310.07815" title="Download PDF">pdf</a>, <a href="/format/2310.07815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models As Semantic Indexers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+B">Bowen Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+H">Hansi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoyin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiusi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+T">Tianxin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ruirui Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhengyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hanqing Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xianfeng Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 appendix pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Semantic identifier (ID) is an important concept in information retrieval
that aims to preserve the semantics of objects such as documents and items
inside their IDs. Previous studies typically adopt a two-stage pipeline to
learn semantic IDs by first procuring embeddings using off-the-shelf text
encoders and then deriving IDs based on the embeddings. However, each step
introduces potential information loss and there is usually an inherent mismatch
between the distribution of embeddings within the latent space produced by text
encoders and the anticipated distribution required for semantic indexing.
Nevertheless, it is non-trivial to design a method that can learn the
document's semantic representations and its hierarchical structure
simultaneously, given that semantic IDs are discrete and sequentially
structured, and the semantic supervision is deficient. In this paper, we
introduce LMINDEXER, a self-supervised framework to learn semantic IDs with a
generative language model. We tackle the challenge of sequential discrete ID by
introducing a semantic indexer capable of generating neural sequential discrete
representations with progressive training and contrastive learning. In response
to the semantic supervision deficiency, we propose to train the model with a
self-supervised document reconstruction objective. The learned semantic indexer
can facilitate various downstream tasks, such as recommendation and retrieval.
We conduct experiments on three tasks including recommendation, product search,
and document retrieval on five datasets from various domains, where LMINDEXER
outperforms competitive baselines significantly and consistently.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07818" title="Abstract">arXiv:2310.07818</a> [<a href="/pdf/2310.07818" title="Download PDF">pdf</a>, <a href="/format/2310.07818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Relationship between Analogy Identification and Sentence  Structure Encoding in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wijesiriwardene%2C+T">Thilini Wijesiriwardene</a>, 
<a href="/search/cs?searchtype=author&query=Wickramarachchi%2C+R">Ruwan Wickramarachchi</a>, 
<a href="/search/cs?searchtype=author&query=Reganti%2C+A+N">Aishwarya Naresh Reganti</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+V">Vinija Jain</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Sheth%2C+A">Amit Sheth</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Amitava Das</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Identifying analogies plays a pivotal role in human cognition and language
proficiency. In the last decade, there has been extensive research on word
analogies in the form of ``A is to B as C is to D.'' However, there is a
growing interest in analogies that involve longer text, such as sentences and
collections of sentences, which convey analogous meanings. While the current
NLP research community evaluates the ability of Large Language Models (LLMs) to
identify such analogies, the underlying reasons behind these abilities warrant
deeper investigation. Furthermore, the capability of LLMs to encode both
syntactic and semantic structures of language within their embeddings has
garnered significant attention with the surge in their utilization. In this
work, we examine the relationship between the abilities of multiple LLMs to
identify sentence analogies, and their capacity to encode syntactic and
semantic structures. Through our analysis, we find that analogy identification
ability of LLMs is positively correlated with their ability to encode syntactic
and semantic structures of sentences. Specifically, we find that the LLMs which
capture syntactic structures better, also have higher abilities in identifying
sentence analogies.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07819" title="Abstract">arXiv:2310.07819</a> [<a href="/pdf/2310.07819" title="Download PDF">pdf</a>, <a href="/format/2310.07819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faithfulness Measurable Masked Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madsen%2C+A">Andreas Madsen</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+S">Siva Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Chandar%2C+S">Sarath Chandar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A common approach to explain NLP models, is to use importance measures that
express which tokens are important for a prediction. Unfortunately, such
explanations are often wrong despite being persuasive. Therefore, it is
essential to measure their faithfulness. One such metric is if tokens are truly
important, then masking them should result in worse model performance. However,
token masking introduces out-of-distribution issues and existing solutions are
computationally expensive and employ proxy-models. Furthermore, other metrics
are very limited in scope. In this work, we propose an inherently faithfulness
measurable model that addresses these challenges. This is achieved by using a
novel fine-tuning method that incorporates masking, such that masking tokens
become in-distribution by design. This differs from existing approaches, which
are completely model-agnostic but are inapplicable in practice. We demonstrate
the generality of our approach by applying it to various tasks and validate it
using statistical in-distribution tests. Additionally, because masking is
in-distribution, importance measures which themselves use masking become more
faithful, thus our model becomes more explainable.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07820" title="Abstract">arXiv:2310.07820</a> [<a href="/pdf/2310.07820" title="Download PDF">pdf</a>, <a href="/format/2310.07820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Are Zero-Shot Time Series Forecasters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gruver%2C+N">Nate Gruver</a>, 
<a href="/search/cs?searchtype=author&query=Finzi%2C+M">Marc Finzi</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+S">Shikai Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Wilson%2C+A+G">Andrew Gordon Wilson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. Code available at: <a href="https://github.com/ngruver/llmtime">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">By encoding time series as a string of numerical digits, we can frame time
series forecasting as next-token prediction in text. Developing this approach,
we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can
surprisingly zero-shot extrapolate time series at a level comparable to or
exceeding the performance of purpose-built time series models trained on the
downstream tasks. To facilitate this performance, we propose procedures for
effectively tokenizing time series data and converting discrete distributions
over tokens into highly flexible densities over continuous values. We argue the
success of LLMs for time series stems from their ability to naturally represent
multimodal distributions, in conjunction with biases for simplicity, and
repetition, which align with the salient features in many time series, such as
repeated seasonal trends. We also show how LLMs can naturally handle missing
data without imputation through non-numerical text, accommodate textual side
information, and answer questions to help explain predictions. While we find
that increasing model size generally improves performance on time series, we
show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers,
and poor uncertainty calibration, which is likely the result of alignment
interventions such as RLHF.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07821" title="Abstract">arXiv:2310.07821</a> [<a href="/pdf/2310.07821" title="Download PDF">pdf</a>, <a href="/format/2310.07821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-autoregressive Text Editing with Copy-aware Latent Alignments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Leyang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+G">Guohong Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent work has witnessed a paradigm shift from Seq2Seq to Seq2Edit in the
field of text editing, with the aim of addressing the slow autoregressive
inference problem posed by the former. Despite promising results, Seq2Edit
approaches still face several challenges such as inflexibility in generation
and difficulty in generalizing to other languages. In this work, we propose a
novel non-autoregressive text editing method to circumvent the above issues, by
modeling the edit process with latent CTC alignments. We make a crucial
extension to CTC by introducing the copy operation into the edit space, thus
enabling more efficient management of textual overlap in editing. We conduct
extensive experiments on GEC and sentence fusion tasks, showing that our
proposed method significantly outperforms existing Seq2Edit models and achieves
similar or even better results than Seq2Seq with over $4\times$ speedup.
Moreover, it demonstrates good generalizability on German and Russian. In-depth
analyses reveal the strengths of our method in terms of the robustness under
various scenarios and generating fluent and flexible outputs.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07822" title="Abstract">arXiv:2310.07822</a> [<a href="/pdf/2310.07822" title="Download PDF">pdf</a>, <a href="/format/2310.07822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Body-mounted MR-conditional Robot for Minimally Invasive Liver  Intervention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhefeng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gunderman%2C+A+L">Anthony L. Gunderman</a>, 
<a href="/search/cs?searchtype=author&query=Wilcox%2C+S+E">Samuel E. Wilcox</a>, 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+S">Saikat Sengupta</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+A">Aiming Lu</a>, 
<a href="/search/cs?searchtype=author&query=Woodrum%2C+D">David Woodrum</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+J">Jay Shah</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yue Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 10figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">MR-guided microwave ablation (MWA) has proven effective in treating
hepatocellular carcinoma (HCC) with small-sized tumors, but the
state-of-the-art technique suffers from sub-optimal workflow due to speed and
accuracy of needle placement. This paper presents a compact body-mounted
MR-conditional robot that can operate in closed-bore MR scanners for accurate
needle guidance. The robotic platform consists of two stacked Cartesian XY
stages, each with two degrees of freedom, that facilitate needle guidance. The
robot is actuated using 3D-printed pneumatic turbines with MR-conditional bevel
gear transmission systems. Pneumatic valves and control mechatronics are
located inside the MRI control room and are connected to the robot with
pneumatic transmission lines and optical fibers. Free space experiments
indicated robot-assisted needle insertion error of 2.6$\pm$1.3 mm at an
insertion depth of 80 mm. The MR-guided phantom studies were conducted to
verify the MR-conditionality and targeting performance of the robot. Future
work will focus on the system optimization and validations in animal trials.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07824" title="Abstract">arXiv:2310.07824</a> [<a href="/pdf/2310.07824" title="Download PDF">pdf</a>, <a href="/format/2310.07824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An On-Chip Trainable Neuron Circuit for SFQ-Based Spiking Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ucpinar%2C+B+Z">Beyza Zeynep Ucpinar</a>, 
<a href="/search/cs?searchtype=author&query=Karamuftuoglu%2C+M+A">Mustafa Altay Karamuftuoglu</a>, 
<a href="/search/cs?searchtype=author&query=Razmkhah%2C+S">Sasan Razmkhah</a>, 
<a href="/search/cs?searchtype=author&query=Pedram%2C+M">Massoud Pedram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 8 figures. The work was presented in EUCAS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Superconductivity (cond-mat.supr-con)

</div>
<p class="mathjax">We present an on-chip trainable neuron circuit. Our proposed circuit suits
bio-inspired spike-based time-dependent data computation for training spiking
neural networks (SNN). The thresholds of neurons can be increased or decreased
depending on the desired application-specific spike generation rate. This
mechanism provides us with a flexible design and scalable circuit structure. We
demonstrate the trainable neuron structure under different operating scenarios.
The circuits are designed and optimized for the MIT LL SFQ5ee fabrication
process. Margin values for all parameters are above 25\% with a 3GHz throughput
for a 16-input neuron.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07826" title="Abstract">arXiv:2310.07826</a> [<a href="/pdf/2310.07826" title="Download PDF">pdf</a>, <a href="/format/2310.07826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Antarlekhaka: A Comprehensive Tool for Multi-task Natural Language  Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Terdalkar%2C+H">Hrishikesh Terdalkar</a> (1), 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+A">Arnab Bhattacharya</a> (1) ((1) Indian Institute of Technology Kanpur)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted: 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS) @ EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">One of the primary obstacles in the advancement of Natural Language
Processing (NLP) technologies for low-resource languages is the lack of
annotated datasets for training and testing machine learning models. In this
paper, we present Antarlekhaka, a tool for manual annotation of a comprehensive
set of tasks relevant to NLP. The tool is Unicode-compatible,
language-agnostic, Web-deployable and supports distributed annotation by
multiple simultaneous annotators. The system sports user-friendly interfaces
for 8 categories of annotation tasks. These, in turn, enable the annotation of
a considerably larger set of NLP tasks. The task categories include two
linguistic tasks not handled by any other tool, namely, sentence boundary
detection and deciding canonical word order, which are important tasks for text
that is in the form of poetry. We propose the idea of sequential annotation
based on small text units, where an annotator performs several tasks related to
a single text unit before proceeding to the next unit. The research
applications of the proposed mode of multi-task annotation are also discussed.
Antarlekhaka outperforms other annotation tools in objective evaluation. It has
been also used for two real-life annotation tasks on two different languages,
namely, Sanskrit and Bengali. The tool is available at
https://github.com/Antarlekhaka/code.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07830" title="Abstract">arXiv:2310.07830</a> [<a href="/pdf/2310.07830" title="Download PDF">pdf</a>, <a href="/format/2310.07830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Synthetic Data Make Large Language Models More Efficient?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gholami%2C+S">Sia Gholami</a>, 
<a href="/search/cs?searchtype=author&query=Omar%2C+M">Marwan Omar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Natural Language Processing (NLP) has undergone transformative changes with
the advent of deep learning methodologies. One challenge persistently
confronting researchers is the scarcity of high-quality, annotated datasets
that drive these models. This paper explores the nuances of synthetic data
generation in NLP, with a focal point on template-based question generation. By
assessing its advantages, including data augmentation potential and the
introduction of structured variety, we juxtapose these benefits against
inherent limitations, such as the risk of overfitting and the constraints posed
by pre-defined templates. Drawing from empirical evaluations, we demonstrate
the impact of template-based synthetic data on the performance of modern
transformer models. We conclude by emphasizing the delicate balance required
between synthetic and real-world data, and the future trajectories of
integrating synthetic data in model training pipelines. The findings aim to
guide NLP practitioners in harnessing synthetic data's potential, ensuring
optimal model performance in diverse applications.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07831" title="Abstract">arXiv:2310.07831</a> [<a href="/pdf/2310.07831" title="Download PDF">pdf</a>, <a href="/format/2310.07831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Defazio%2C+A">Aaron Defazio</a>, 
<a href="/search/cs?searchtype=author&query=Cutkosky%2C+A">Ashok Cutkosky</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+H">Harsh Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Mishchenko%2C+K">Konstantin Mishchenko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Learning rate schedules used in practice bear little resemblance to those
recommended by theory. We close much of this theory/practice gap, and as a
consequence are able to derive new problem-adaptive learning rate schedules.
Our key technical contribution is a refined analysis of learning rate schedules
for a wide class of optimization algorithms (including SGD). In contrast to
most prior works that study the convergence of the average iterate, we study
the last iterate, which is what most people use in practice. When considering
only worst-case analysis, our theory predicts that the best choice is the
linear decay schedule: a popular choice in practice that sets the stepsize
proportionally to $1 - t/T$, where $t$ is the current iteration and $T$ is the
total number of steps. To go beyond this worst-case analysis, we use the
observed gradient norms to derive schedules refined for any particular task.
These refined schedules exhibit learning rate warm-up and rapid learning rate
annealing near the end of training. Ours is the first systematic approach to
automatically yield both of these properties. We perform the most comprehensive
evaluation of learning rate schedules to date, evaluating across 10 diverse
deep learning problems, a series of LLMs, and a suite of logistic regression
problems. We validate that overall, the linear-decay schedule matches or
outperforms all commonly used default schedules including cosine annealing, and
that our schedule refinement method gives further improvements.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07837" title="Abstract">arXiv:2310.07837</a> [<a href="/pdf/2310.07837" title="Download PDF">pdf</a>, <a href="/format/2310.07837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Feature Sparsity in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+M">Mingyang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+L">Lucas Tao</a>, 
<a href="/search/cs?searchtype=author&query=Benton%2C+J">Joe Benton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent works have proposed that activations in language models can be
modelled as sparse linear combinations of vectors corresponding to features of
input text. Under this assumption, these works aimed to reconstruct feature
directions using sparse coding. We develop metrics to assess the success of
these sparse coding techniques and test the validity of the linearity and
sparsity assumptions. We show our metrics can predict the level of sparsity on
synthetic sparse linear activations, and can distinguish between sparse linear
data and several other distributions. We use our metrics to measure levels of
sparsity in several language models. We find evidence that language model
activations can be accurately modelled by sparse linear combinations of
features, significantly more so than control datasets. We also show that model
activations appear to be sparsest in the first and final layers.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07838" title="Abstract">arXiv:2310.07838</a> [<a href="/pdf/2310.07838" title="Download PDF">pdf</a>, <a href="/format/2310.07838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards the Fundamental Limits of Knowledge Transfer over Finite Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qingyue Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Banghua Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 2 figures; primary version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">We characterize the statistical efficiency of knowledge transfer through $n$
samples from a teacher to a probabilistic student classifier with input space
$\mathcal S$ over labels $\mathcal A$. We show that privileged information at
three progressive levels accelerates the transfer. At the first level, only
samples with hard labels are known, via which the maximum likelihood estimator
attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The
second level has the teacher probabilities of sampled labels available in
addition, which turns out to boost the convergence rate lower bound to
${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data
acquisition protocol, minimizing a naive adaptation of the cross-entropy loss
results in an asymptotically biased student. We overcome this limitation and
achieve the fundamental limit by using a novel empirical variant of the squared
error logit loss. The third level further equips the student with the soft
labels (complete logits) on ${\mathcal A}$ given every sampled input, thereby
provably enables the student to enjoy a rate ${|{\mathcal S}|}/{n}$ free of
$|{\mathcal A}|$. We find any Kullback-Leibler divergence minimizer to be
optimal in the last case. Numerical simulations distinguish the four learners
and corroborate our theory.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07840" title="Abstract">arXiv:2310.07840</a> [<a href="/pdf/2310.07840" title="Download PDF">pdf</a>, <a href="/format/2310.07840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Learning with Dual Model Predictive Path-Integral Control for  Interaction-Aware Autonomous Highway On-ramp Merging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Knaup%2C+J">Jacob Knaup</a>, 
<a href="/search/cs?searchtype=author&query=D%27sa%2C+J">Jovin D&#x27;sa</a>, 
<a href="/search/cs?searchtype=author&query=Chalaki%2C+B">Behdad Chalaki</a>, 
<a href="/search/cs?searchtype=author&query=Naes%2C+T">Tyler Naes</a>, 
<a href="/search/cs?searchtype=author&query=Mahjoub%2C+H+N">Hossein Nourkhiz Mahjoub</a>, 
<a href="/search/cs?searchtype=author&query=Moradi-Pari%2C+E">Ehsan Moradi-Pari</a>, 
<a href="/search/cs?searchtype=author&query=Tsiotras%2C+P">Panagiotis Tsiotras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Merging into dense highway traffic for an autonomous vehicle is a complex
decision-making task, wherein the vehicle must identify a potential gap and
coordinate with surrounding human drivers, each of whom may exhibit diverse
driving behaviors. Many existing methods consider other drivers to be dynamic
obstacles and, as a result, are incapable of capturing the full intent of the
human drivers via this passive planning. In this paper, we propose a novel dual
control framework based on Model Predictive Path-Integral control to generate
interactive trajectories. This framework incorporates a Bayesian inference
approach to actively learn the agents' parameters, i.e., other drivers' model
parameters. The proposed framework employs a sampling-based approach that is
suitable for real-time implementation through the utilization of GPUs. We
illustrate the effectiveness of our proposed methodology through comprehensive
numerical simulations conducted in both high and low-fidelity simulation
scenarios focusing on autonomous on-ramp merging.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07842" title="Abstract">arXiv:2310.07842</a> [<a href="/pdf/2310.07842" title="Download PDF">pdf</a>, <a href="/format/2310.07842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiPPeR: Diffusion-based 2D Path Planner applied on Legged Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Stamatopoulou%2C+M">Maria Stamatopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Kanoulas%2C+D">Dimitrios Kanoulas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this work, we present DiPPeR, a novel and fast 2D path planning framework
for quadrupedal locomotion, leveraging diffusion-driven techniques. Our
contributions include a scalable dataset of map images and corresponding
end-to-end trajectories, an image-conditioned diffusion planner for mobile
robots, and a training/inference pipeline employing CNNs. We validate our
approach in several mazes, as well as in real-world deployment scenarios on
Boston Dynamic's Spot and Unitree's Go1 robots. DiPPeR performs on average 70
times faster for trajectory generation against both search based and data
driven path planning algorithms with an average of 80% consistency in producing
feasible paths of various length in maps of variable size, and obstacle
structure.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07844" title="Abstract">arXiv:2310.07844</a> [<a href="/pdf/2310.07844" title="Download PDF">pdf</a>, <a href="/format/2310.07844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Saturation-Aware Angular Velocity Estimation: Extending the Robustness  of SLAM to Aggressive Motions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Desch%C3%AAnes%2C+S">Simon-Pierre Desch&#xea;nes</a>, 
<a href="/search/cs?searchtype=author&query=Baril%2C+D">Dominic Baril</a>, 
<a href="/search/cs?searchtype=author&query=Boxan%2C+M">Mat&#x11b;j Boxan</a>, 
<a href="/search/cs?searchtype=author&query=Laconte%2C+J">Johann Laconte</a>, 
<a href="/search/cs?searchtype=author&query=Gigu%C3%A8re%2C+P">Philippe Gigu&#xe8;re</a>, 
<a href="/search/cs?searchtype=author&query=Pomerleau%2C+F">Fran&#xe7;ois Pomerleau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures, submitted to the 2024 IEEE International Conference on Robotics and Automation (ICRA2024), Yokohama, Japan
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We propose a novel angular velocity estimation method to increase the
robustness of Simultaneous Localization And Mapping (SLAM) algorithms against
gyroscope saturations induced by aggressive motions. Field robotics expose
robots to various hazards, including steep terrains, landslides, and
staircases, where substantial accelerations and angular velocities can occur if
the robot loses stability and tumbles. These extreme motions can saturate
sensor measurements, especially gyroscopes, which are the first sensors to
become inoperative. While the structural integrity of the robot is at risk, the
resilience of the SLAM framework is oftentimes given little consideration.
Consequently, even if the robot is physically capable of continuing the
mission, its operation will be compromised due to a corrupted representation of
the world. Regarding this problem, we propose a way to estimate the angular
velocity using accelerometers during extreme rotations caused by tumbling. We
show that our method reduces the median localization error by 71.5 % in
translation and 65.5 % in rotation and reduces the number of SLAM failures by
73.3 % on the collected data. We also propose the Tumbling-Induced Gyroscope
Saturation (TIGS) dataset, which consists of outdoor experiments recording the
motion of a lidar subject to angular velocities four times higher than other
available datasets. The dataset is available online at
https://github.com/norlab-ulaval/Norlab_wiki/wiki/TIGS-Dataset.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07846" title="Abstract">arXiv:2310.07846</a> [<a href="/pdf/2310.07846" title="Download PDF">pdf</a>, <a href="/format/2310.07846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DAG-aware Synthesis Orchestration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingju Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+M">Mark Ren</a>, 
<a href="/search/cs?searchtype=author&query=Mishchenko%2C+A">Alan Mishchenko</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Cunxi Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">The key methodologies of modern logic synthesis techniques are conducted on
multi-level technology-independent representations such as And-Inverter-Graphs
(AIGs) of the digital logic via directed-acyclic-graph (DAGs) traversal based
structural rewriting, resubstitution, and refactoring. Existing
state-of-the-art DAG-aware logic synthesis algorithms are all designed to
perform stand-alone optimizations during a single DAG traversal. However, we
empirically identify and demonstrate that these algorithms are limited in
quality-of-results and runtime complexity due to this design concept. This work
proposes Synthesis Orchestration, which orchestrates stand-alone operations
within the single traversal of AIG. Thus, orchestration method explores more
optimization opportunities and results in better performance. Our experimental
results are comprehensively conducted on all 104 designs collected from
ISCAS'85/89/99, VTR, and EPFL benchmark suites, with consistent logic
minimization improvements over rewriting, resubstitution, refactoring, leading
to an average of 4% more node reduction with improved runtime efficiency for
the single optimization. Moreover, we evaluate orchestration as a plug-in
algorithm in resyn and resyn3 flows in ABC, which demonstrates consistent logic
minimization improvements (3.8% and 10.9% more node reduction on average). The
runtime analysis demonstrates the orchestration outperforms stand-alone
algorithms in both AIG minimization and runtime efficiency. Finally, we
integrate the orchestration into OpenROAD for end-to-end performance
evaluation. Our results demonstrate the advantages of the orchestration
optimization technique, even after technology mapping and post-routing in the
design flow have been conducted.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07847" title="Abstract">arXiv:2310.07847</a> [<a href="/pdf/2310.07847" title="Download PDF">pdf</a>, <a href="/format/2310.07847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dependency Practices for Vulnerability Mitigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jafari%2C+A+J">Abbas Javan Jafari</a>, 
<a href="/search/cs?searchtype=author&query=Costa%2C+D+E">Diego Elias Costa</a>, 
<a href="/search/cs?searchtype=author&query=Abdellatif%2C+A">Ahmad Abdellatif</a>, 
<a href="/search/cs?searchtype=author&query=Shihab%2C+E">Emad Shihab</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Relying on dependency packages accelerates software development, but it also
increases the exposure to security vulnerabilities that may be present in
dependencies. While developers have full control over which dependency packages
(and which version) they use, they have no control over the dependencies of
their dependencies. Such transitive dependencies, which often amount to a
greater number than direct dependencies, can become infected with
vulnerabilities and put software projects at risk. To mitigate this risk,
Practitioners need to select dependencies that respond quickly to
vulnerabilities to prevent the propagation of vulnerable code to their project.
To identify such dependencies, we analyze more than 450 vulnerabilities in the
npm ecosystem to understand why dependent packages remain vulnerable. We
identify over 200,000 npm packages that are infected through their dependencies
and use 9 features to build a prediction model that identifies packages that
quickly adopt the vulnerability fix and prevent further propagation of
vulnerabilities. We also study the relationship between these features and the
response speed of vulnerable packages. We complement our work with a
practitioner survey to understand the applicability of our findings. Developers
can incorporate our findings into their dependency management practices to
mitigate the impact of vulnerabilities from their dependency supply chain.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07848" title="Abstract">arXiv:2310.07848</a> [<a href="/pdf/2310.07848" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Framework for Question-Answering in Sanskrit through Automated  Construction of Knowledge Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Terdalkar%2C+H">Hrishikesh Terdalkar</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+A">Arnab Bhattacharya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at 6th International Sanskrit Computational Linguistics Symposium (ISCLS) 2019
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of the 6th International Sanskrit Computational
  Linguistics Symposium, 2019, pages 97--116, IIT Kharagpur, India. Association
  for Computational Linguistics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Sanskrit (sa\d{m}sk\d{r}ta) enjoys one of the largest and most varied
literature in the whole world. Extracting the knowledge from it, however, is a
challenging task due to multiple reasons including complexity of the language
and paucity of standard natural language processing tools. In this paper, we
target the problem of building knowledge graphs for particular types of
relationships from sa\d{m}sk\d{r}ta texts. We build a natural language
question-answering system in sa\d{m}sk\d{r}ta that uses the knowledge graph to
answer factoid questions. We design a framework for the overall system and
implement two separate instances of the system on human relationships from
mah\=abh\=arata and r\=am\=aya\d{n}a, and one instance on synonymous
relationships from bh\=avaprak\=a\'sa nigha\d{n}\d{t}u, a technical text from
\=ayurveda. We show that about 50% of the factoid questions can be answered
correctly by the system. More importantly, we analyse the shortcomings of the
system in detail for each step, and discuss the possible ways forward.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07849" title="Abstract">arXiv:2310.07849</a> [<a href="/pdf/2310.07849" title="Download PDF">pdf</a>, <a href="/format/2310.07849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthetic Data Generation with Large Language Models for Text  Classification: Potential and Limitations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuoyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hangxiao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhuoran Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+M">Ming Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The collection and curation of high-quality training data is crucial for
developing text classification models with superior performance, but it is
often associated with significant costs and time investment. Researchers have
recently explored using large language models (LLMs) to generate synthetic
datasets as an alternative approach. However, the effectiveness of the
LLM-generated synthetic data in supporting model training is inconsistent
across different classification tasks. To better understand factors that
moderate the effectiveness of the LLM-generated synthetic data, in this study,
we look into how the performance of models trained on these synthetic data may
vary with the subjectivity of classification. Our results indicate that
subjectivity, at both the task level and instance level, is negatively
associated with the performance of the model trained on synthetic data. We
conclude by discussing the implications of our work on the potential and
limitations of leveraging LLM for synthetic data generation.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07854" title="Abstract">arXiv:2310.07854</a> [<a href="/pdf/2310.07854" title="Download PDF">pdf</a>, <a href="/format/2310.07854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VaPr: Variable-Precision Tensors to Accelerate Robot Motion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsiao%2C+Y">Yu-Shun Hsiao</a>, 
<a href="/search/cs?searchtype=author&query=Hari%2C+S+K+S">Siva Kumar Sastry Hari</a>, 
<a href="/search/cs?searchtype=author&query=Sundaralingam%2C+B">Balakumar Sundaralingam</a>, 
<a href="/search/cs?searchtype=author&query=Yik%2C+J">Jason Yik</a>, 
<a href="/search/cs?searchtype=author&query=Tambe%2C+T">Thierry Tambe</a>, 
<a href="/search/cs?searchtype=author&query=Sakr%2C+C">Charbel Sakr</a>, 
<a href="/search/cs?searchtype=author&query=Keckler%2C+S+W">Stephen W. Keckler</a>, 
<a href="/search/cs?searchtype=author&query=Reddi%2C+V+J">Vijay Janapa Reddi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, 8 tables, to be published in 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">High-dimensional motion generation requires numerical precision for smooth,
collision-free solutions. Typically, double-precision or single-precision
floating-point (FP) formats are utilized. Using these for big tensors imposes a
strain on the memory bandwidth provided by the devices and alters the memory
footprint, hence limiting their applicability to low-power edge devices needed
for mobile robots. The uniform application of reduced precision can be
advantageous but severely degrades solutions. Using decreased precision data
types for important tensors, we propose to accelerate motion generation by
removing memory bottlenecks. We propose variable-precision (VaPr) search
optimization to determine the appropriate precision for large tensors from a
vast search space of approximately 4 million unique combinations for FP data
types across the tensors. To obtain the efficiency gains, we exploit existing
platform support for an out-of-the-box GPU speedup and evaluate prospective
precision converter units for GPU types that are not currently supported. Our
experimental results on 800 planning problems for the Franka Panda robot on the
MotionBenchmaker dataset across 8 environments show that a 4-bit FP format is
sufficient for the largest set of tensors in the motion generation stack. With
the software-only solution, VaPr achieves 6.3% and 6.3% speedups on average for
a significant portion of motion generation over the SOTA solution (CuRobo) on
Jetson Orin and RTX2080 Ti GPU, respectively, and 9.9%, 17.7% speedups with the
FP converter.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07855" title="Abstract">arXiv:2310.07855</a> [<a href="/pdf/2310.07855" title="Download PDF">pdf</a>, <a href="/format/2310.07855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CrIBo: Self-Supervised Learning via Cross-Image Object-Level  Bootstrapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lebailly%2C+T">Tim Lebailly</a>, 
<a href="/search/cs?searchtype=author&query=Stegm%C3%BCller%2C+T">Thomas Stegm&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Bozorgtabar%2C+B">Behzad Bozorgtabar</a>, 
<a href="/search/cs?searchtype=author&query=Thiran%2C+J">Jean-Philippe Thiran</a>, 
<a href="/search/cs?searchtype=author&query=Tuytelaars%2C+T">Tinne Tuytelaars</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Leveraging nearest neighbor retrieval for self-supervised representation
learning has proven beneficial with object-centric images. However, this
approach faces limitations when applied to scene-centric datasets, where
multiple objects within an image are only implicitly captured in the global
representation. Such global bootstrapping can lead to undesirable entanglement
of object representations. Furthermore, even object-centric datasets stand to
benefit from a finer-grained bootstrapping approach. In response to these
challenges, we introduce a novel Cross-Image Object-Level Bootstrapping method
tailored to enhance dense visual representation learning. By employing
object-level nearest neighbor bootstrapping throughout the training, CrIBo
emerges as a notably strong and adequate candidate for in-context learning,
leveraging nearest neighbor retrieval at test time. CrIBo shows
state-of-the-art performance on the latter task while being highly competitive
in more standard downstream segmentation tasks. Our code and pretrained models
will be publicly available upon acceptance.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07856" title="Abstract">arXiv:2310.07856</a> [<a href="/pdf/2310.07856" title="Download PDF">pdf</a>, <a href="/ps/2310.07856" title="Download PostScript">ps</a>, <a href="/format/2310.07856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Evaluation Metrics for Neural Test Oracle Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jiho Shin</a>, 
<a href="/search/cs?searchtype=author&query=Hemmati%2C+H">Hadi Hemmati</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+M">Moshi Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Song Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages + reference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">In this work, we revisit existing oracle generation studies plus ChatGPT to
empirically investigate the current standing of their performance in both
NLG-based and test adequacy metrics. Specifically, we train and run four
state-of-the-art test oracle generation models on five NLG-based and two test
adequacy metrics for our analysis. We apply two different correlation analyses
between these two different sets of metrics. Surprisingly, we found no
significant correlation between the NLG-based metrics and test adequacy
metrics. For instance, oracles generated from ChatGPT on the project
activemq-artemis had the highest performance on all the NLG-based metrics among
the studied NOGs, however, it had the most number of projects with a decrease
in test adequacy metrics compared to all the studied NOGs. We further conduct a
qualitative analysis to explore the reasons behind our observations, we found
that oracles with high NLG-based metrics but low test adequacy metrics tend to
have complex or multiple chained method invocations within the oracle's
parameters, making it hard for the model to generate completely, affecting the
test adequacy metrics. On the other hand, oracles with low NLG-based metrics
but high test adequacy metrics tend to have to call different assertion types
or a different method that functions similarly to the ones in the ground truth.
Overall, this work complements prior studies on test oracle generation with an
extensive performance evaluation with both NLG and test adequacy metrics and
provides guidelines for better assessment of deep learning applications in
software test generation in the future.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07857" title="Abstract">arXiv:2310.07857</a> [<a href="/pdf/2310.07857" title="Download PDF">pdf</a>, <a href="/format/2310.07857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On $(1+\varepsilon)$-Approximate Flow Sparsifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zihan Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Given a large graph $G$ with a subset $|T|=k$ of its vertices called
terminals, a quality-$q$ flow sparsifier is a small graph $G'$ that contains
$T$ and preserves all multicommodity flows that can be routed between terminals
in $T$, to within factor $q$. The problem of constructing flow sparsifiers with
good (small) quality and (small) size has been a central problem in graph
compression for decades.
<br />A natural approach of constructing $O(1)$-quality flow sparsifiers, which was
adopted in most previous constructions, is contraction. Andoni, Krauthgamer,
and Gupta constructed a sketch of size $f(k,\varepsilon)$ that stores all
feasible multicommodity flows up to a factor of $(1+\varepsilon)$, raised the
question of constructing quality-$(1+\varepsilon)$ flow sparsifiers whose size
only depends on $k,\varepsilon$ (but not the number of vertices in the input
graph $G$), and proposed a contraction-based framework towards it using their
sketch result.
<br />In this paper, we settle their question for contraction-based flow
sparsifiers, by showing that quality-$(1+\varepsilon)$ contraction-based flow
sparsifiers with size $f(\varepsilon)$ exist for all $5$-terminal graphs, but
not for all $6$-terminal graphs. Our hardness result on $6$-terminal graphs
improves upon a recent hardness result by Krauthgamer and Mosenzon on exact
(quality-$1$) flow sparsifiers, for contraction-based constructions. Our
construction and proof utilize the notion of tight spans in metric geometry,
which we believe is a powerful tool for future work.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07862" title="Abstract">arXiv:2310.07862</a> [<a href="/pdf/2310.07862" title="Download PDF">pdf</a>, <a href="/ps/2310.07862" title="Download PostScript">ps</a>, <a href="/format/2310.07862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An $\tilde&#x3a9;\big(\sqrt{\log |T|}\big)$ Lower Bound for Steiner Point  Removal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zihan Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In the Steiner point removal (SPR) problem, we are given a (weighted) graph
$G$ and a subset $T$ of its vertices called terminals, and the goal is to
compute a (weighted) graph $H$ on $T$ that is a minor of $G$, such that the
distance between every pair of terminals is preserved to within some small
multiplicative factor, that is called the stretch of $H$.
<br />It has been shown that on general graphs we can achieve stretch $O(\log |T|)$
[Filtser, 2018]. On the other hand, the best-known stretch lower bound is $8$
[Chan-Xia-Konjevod-Richa, 2006], which holds even for trees. In this work, we
show an improved lower bound of $\tilde\Omega\big(\sqrt{\log |T|}\big)$.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07871" title="Abstract">arXiv:2310.07871</a> [<a href="/pdf/2310.07871" title="Download PDF">pdf</a>, <a href="/format/2310.07871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Pretraining on Multimodal Electronic Health Records
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaochen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Junyu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Ziyi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+S">Suhan Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yuan Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fenglong Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Pretraining has proven to be a powerful technique in natural language
processing (NLP), exhibiting remarkable success in various NLP downstream
tasks. However, in the medical domain, existing pretrained models on electronic
health records (EHR) fail to capture the hierarchical nature of EHR data,
limiting their generalization capability across diverse downstream tasks using
a single pretrained model. To tackle this challenge, this paper introduces a
novel, general, and unified pretraining framework called MEDHMP, specifically
designed for hierarchically multimodal EHR data. The effectiveness of the
proposed MEDHMP is demonstrated through experimental results on eight
downstream tasks spanning three levels. Comparisons against eighteen baselines
further highlight the efficacy of our approach.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07874" title="Abstract">arXiv:2310.07874</a> [<a href="/pdf/2310.07874" title="Download PDF">pdf</a>, <a href="/format/2310.07874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Refined Mechanism Design for Approximately Structured Priors via Active  Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boutsikas%2C+C">Christos Boutsikas</a>, 
<a href="/search/cs?searchtype=author&query=Drineas%2C+P">Petros Drineas</a>, 
<a href="/search/cs?searchtype=author&query=Mertzanidis%2C+M">Marios Mertzanidis</a>, 
<a href="/search/cs?searchtype=author&query=Psomas%2C+A">Alexandros Psomas</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+P">Paritosh Verma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the problem of a revenue-maximizing seller with a large number of
items $m$ for sale to $n$ strategic bidders, whose valuations are drawn
independently from high-dimensional, unknown prior distributions. It is
well-known that optimal and even approximately-optimal mechanisms for this
setting are notoriously difficult to characterize or compute, and, even when
they can be found, are often rife with various counter-intuitive properties. In
this paper, following a model introduced recently by Cai and
Daskalakis~\cite{cai2022recommender}, we consider the case that bidders' prior
distributions can be well-approximated by a topic model. We design an active
learning component, responsible for interacting with the bidders and outputting
low-dimensional approximations of their types, and a mechanism design
component, responsible for robustifying mechanisms for the low-dimensional
model to work for the approximate types of the former component. On the active
learning front, we cast our problem in the framework of Randomized Linear
Algebra (RLA) for regression problems, allowing us to import several
breakthrough results from that line of research, and adapt them to our setting.
On the mechanism design front, we remove many restrictive assumptions of prior
work on the type of access needed to the underlying distributions and the
associated mechanisms. To the best of our knowledge, our work is the first to
formulate connections between mechanism design, and RLA for active learning of
regression problems, opening the door for further applications of randomized
linear algebra primitives to mechanism design.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07875" title="Abstract">arXiv:2310.07875</a> [<a href="/pdf/2310.07875" title="Download PDF">pdf</a>, <a href="/format/2310.07875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TabLib: A Dataset of 627M Tables with Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eggert%2C+G">Gus Eggert</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+K">Kevin Huo</a>, 
<a href="/search/cs?searchtype=author&query=Biven%2C+M">Mike Biven</a>, 
<a href="/search/cs?searchtype=author&query=Waugh%2C+J">Justin Waugh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB); Machine Learning (cs.LG)

</div>
<p class="mathjax">It is well-established that large, diverse datasets play a pivotal role in
the performance of modern AI systems for text and image modalities. However,
there are no datasets for tabular data of comparable size and diversity to
those available for text and images. Thus we present "TabLib'', a compilation
of 627 million tables totaling 69 TiB, along with 867B tokens of context.
TabLib was extracted from numerous file formats, including CSV, HTML, SQLite,
PDF, Excel, and others, sourced from GitHub and Common Crawl. The size and
diversity of TabLib offer considerable promise in the table modality,
reminiscent of the original promise of foundational datasets for text and
images, such as The Pile and LAION.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07878" title="Abstract">arXiv:2310.07878</a> [<a href="/pdf/2310.07878" title="Download PDF">pdf</a>, <a href="/format/2310.07878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coupled Scheme for Linear and Hamilton-Jacobi Equations: Theoretical and  Numerical Aspects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sahu%2C+S">Smita Sahu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">We present a comprehensive analysis of the coupled scheme introduced in
[Springer Proceedings in Mathematics \&amp; Statistics, vol 237. Springer, Cham
2018 \cite{S2018}] for linear and Hamilton-Jacobi equations. This method merges
two distinct schemes, each tailored to handle specific solution
characteristics. It offers a versatile framework for coupling various schemes,
enabling the integration of accurate methods for smooth solutions and the
treatment of discontinuities and gradient jumps. In \cite{S2018}, the emphasis
was on coupling an anti-dissipative scheme designed for discontinuous solutions
with a semi-Lagrangian scheme developed for smooth solutions. In this paper, we
rigorously establish the essential properties of the resulting coupled scheme,
especially in the linear case. To illustrate the effectiveness of this coupled
approach, we present a series of one-dimensional examples.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07879" title="Abstract">arXiv:2310.07879</a> [<a href="/pdf/2310.07879" title="Download PDF">pdf</a>, <a href="/format/2310.07879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI Privacy  Risks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hao-Ping Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yu-Ju Yang</a>, 
<a href="/search/cs?searchtype=author&query=von+Davier%2C+T+S">Thomas Serban von Davier</a>, 
<a href="/search/cs?searchtype=author&query=Forlizzi%2C+J">Jodi Forlizzi</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Sauvik Das</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Privacy is a key principle for developing ethical AI technologies, but how
does including AI technologies in products and services change privacy risks?
We constructed a taxonomy of AI privacy risks by analyzing 321 documented AI
privacy incidents. We codified how the unique capabilities and requirements of
AI technologies described in those incidents generated new privacy risks,
exacerbated known ones, or otherwise did not meaningfully alter the risk. We
present 12 high-level privacy risks that AI technologies either newly created
(e.g., exposure risks from deepfake pornography) or exacerbated (e.g.,
surveillance risks from collecting training data). One upshot of our work is
that incorporating AI technologies into a product can alter the privacy risks
it entails. Yet, current privacy-preserving AI/ML methods (e.g., federated
learning, differential privacy) only address a subset of the privacy risks
arising from the capabilities and data requirements of AI.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07881" title="Abstract">arXiv:2310.07881</a> [<a href="/pdf/2310.07881" title="Download PDF">pdf</a>, <a href="/format/2310.07881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeePref: Deep Reinforcement Learning For Video Prefetching In Content  Delivery Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alkassab%2C+N">Nawras Alkassab</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chin-Tser Huang</a>, 
<a href="/search/cs?searchtype=author&query=Botran%2C+T+L">Tania Lorido Botran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Content Delivery Networks carry the majority of Internet traffic, and the
increasing demand for video content as a major IP traffic across the Internet
highlights the importance of caching and prefetching optimization algorithms.
Prefetching aims to make data available in the cache before the requester
places its request to reduce access time and improve the Quality of Experience
on the user side. Prefetching is well investigated in operating systems,
compiler instructions, in-memory cache, local storage systems, high-speed
networks, and cloud systems. Traditional prefetching techniques are well
adapted to a particular access pattern, but fail to adapt to sudden variations
or randomization in workloads. This paper explores the use of reinforcement
learning to tackle the changes in user access patterns and automatically adapt
over time. To this end, we propose, DeePref, a Deep Reinforcement Learning
agent for online video content prefetching in Content Delivery Networks.
DeePref is a prefetcher implemented on edge networks and is agnostic to
hardware design, operating systems, and applications. Our results show that
DeePref DRQN, using a real-world dataset, achieves a 17% increase in
prefetching accuracy and a 28% increase in prefetching coverage on average
compared to baseline approaches that use video content popularity as a building
block to statically or dynamically make prefetching decisions. We also study
the possibility of transfer learning of statistical models from one edge
network into another, where unseen user requests from unknown distribution are
observed. In terms of transfer learning, the increase in prefetching accuracy
and prefetching coverage are [$30%$, $10%$], respectively. Our source code will
be available on Github.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07882" title="Abstract">arXiv:2310.07882</a> [<a href="/pdf/2310.07882" title="Download PDF">pdf</a>, <a href="/format/2310.07882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Thousand Faces of Explainable AI Along the Machine Learning Life  Cycle: Industrial Reality and Current State of Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Decker%2C+T">Thomas Decker</a>, 
<a href="/search/cs?searchtype=author&query=Gross%2C+R">Ralf Gross</a>, 
<a href="/search/cs?searchtype=author&query=Koebler%2C+A">Alexander Koebler</a>, 
<a href="/search/cs?searchtype=author&query=Lebacher%2C+M">Michael Lebacher</a>, 
<a href="/search/cs?searchtype=author&query=Schnitzer%2C+R">Ronald Schnitzer</a>, 
<a href="/search/cs?searchtype=author&query=Weber%2C+S+H">Stefan H. Weber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Human-Computer Interaction 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In this paper, we investigate the practical relevance of explainable
artificial intelligence (XAI) with a special focus on the producing industries
and relate them to the current state of academic XAI research. Our findings are
based on an extensive series of interviews regarding the role and applicability
of XAI along the Machine Learning (ML) lifecycle in current industrial practice
and its expected relevance in the future. The interviews were conducted among a
great variety of roles and key stakeholders from different industry sectors. On
top of that, we outline the state of XAI research by providing a concise review
of the relevant literature. This enables us to provide an encompassing overview
covering the opinions of the surveyed persons as well as the current state of
academic research. By comparing our interview results with the current research
approaches we reveal several discrepancies. While a multitude of different XAI
approaches exists, most of them are centered around the model evaluation phase
and data scientists. Their versatile capabilities for other stages are
currently either not sufficiently explored or not popular among practitioners.
In line with existing work, our findings also confirm that more efforts are
needed to enable also non-expert users' interpretation and understanding of
opaque AI models with existing methods and frameworks.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07884" title="Abstract">arXiv:2310.07884</a> [<a href="/pdf/2310.07884" title="Download PDF">pdf</a>, <a href="/ps/2310.07884" title="Download PostScript">ps</a>, <a href="/format/2310.07884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secretary Problems with Random Number of Candidates: How Prior  Distributional Information Helps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junhui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jaillet%2C+P">Patrick Jaillet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We study variants of the secretary problem, where $N$, the number of
candidates, is a random variable, and the decision maker wants to maximize the
probability of success -- picking the largest number among the $N$ candidates
-- using only the relative ranks of the candidates revealed so far.
<br />We consider three forms of prior information about $\mathbf p$, the
probability distribution of $N$. In the full information setting, we assume
$\mathbf p$ to be fully known. In that case, we show that single-threshold type
of strategies can achieve $1/e$-approximation to the maximum probability of
success among all possible strategies. In the upper bound setting, we assume
that $N\leq \overline{n}$ (or $\mathbb E[N]\leq \bar{\mu}$), where $\bar{n}$
(or $\bar{\mu}$) is known. In that case, we show that randomization over
single-threshold type of strategies can achieve the optimal worst case
probability of success of $\frac{1}{\log(\bar{n})}$ (or
$\frac{1}{\log(\bar{\mu})}$) asymptotically. Surprisingly, there is a
single-threshold strategy (depending on $\overline{n}$) that can succeed with
probability $2/e^2$ for all but an exponentially small fraction of
distributions supported on $[\bar{n}]$. In the sampling setting, we assume that
we have access to $m$ samples $N^{(1)},\ldots,N^{(m)}\sim_{iid} \mathbf p$. In
that case, we show that if $N\leq T$ with probability at least $1-O(\epsilon)$
for some $T\in \mathbb N$, $m\gtrsim
\frac{1}{\epsilon^2}\max(\log(\frac{1}{\epsilon}),\epsilon
\log(\frac{\log(T)}{\epsilon}))$ is enough to learn a strategy that is at least
$\epsilon$-suboptimal, and we provide a lower bound of
$\Omega(\frac{1}{\epsilon^2})$, showing that the sampling algorithm is optimal
when $\epsilon=O(\frac{1}{\log\log(T)})$.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07885" title="Abstract">arXiv:2310.07885</a> [<a href="/pdf/2310.07885" title="Download PDF">pdf</a>, <a href="/format/2310.07885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leader-Follower Neural Networks with Local Error Signals Inspired by  Complex Collectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+C">Chenzhong Yin</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Mingxi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xiongye Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinghe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Nazarian%2C+S">Shahin Nazarian</a>, 
<a href="/search/cs?searchtype=author&query=Irimia%2C+A">Andrei Irimia</a>, 
<a href="/search/cs?searchtype=author&query=Bogdan%2C+P">Paul Bogdan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The collective behavior of a network with heterogeneous, resource-limited
information processing units (e.g., group of fish, flock of birds, or network
of neurons) demonstrates high self-organization and complexity. These emergent
properties arise from simple interaction rules where certain individuals can
exhibit leadership-like behavior and influence the collective activity of the
group. Motivated by the intricacy of these collectives, we propose a neural
network (NN) architecture inspired by the rules observed in nature's collective
ensembles. This NN structure contains workers that encompass one or more
information processing units (e.g., neurons, filters, layers, or blocks of
layers). Workers are either leaders or followers, and we train a
leader-follower neural network (LFNN) by leveraging local error signals and
optionally incorporating backpropagation (BP) and global loss. We investigate
worker behavior and evaluate LFNNs through extensive experimentation. Our LFNNs
trained with local error signals achieve significantly lower error rates than
previous BP-free algorithms on MNIST and CIFAR-10 and even surpass BP-enabled
baselines. In the case of ImageNet, our LFNN-l demonstrates superior
scalability and outperforms previous BP-free algorithms by a significant
margin.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07886" title="Abstract">arXiv:2310.07886</a> [<a href="/pdf/2310.07886" title="Download PDF">pdf</a>, <a href="/format/2310.07886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Feature Types and Their Contributions for Camera Tampering  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mantini%2C+P">Pranav Mantini</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+S+K">Shishir K. Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Camera tamper detection is the ability to detect unauthorized and
unintentional alterations in surveillance cameras by analyzing the video.
Camera tampering can occur due to natural events or it can be caused
intentionally to disrupt surveillance. We cast tampering detection as a change
detection problem, and perform a review of the existing literature with
emphasis on feature types. We formulate tampering detection as a time series
analysis problem, and design experiments to study the robustness and capability
of various feature types. We compute ten features on real-world surveillance
video and apply time series analysis to ascertain their predictability, and
their capability to detect tampering. Finally, we quantify the performance of
various time series models using each feature type to detect tampering.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07888" title="Abstract">arXiv:2310.07888</a> [<a href="/pdf/2310.07888" title="Download PDF">pdf</a>, <a href="/format/2310.07888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Viability of Mobile Forms for Population Health Surveys in Low Resource  Areas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Davis%2C+A">Alexander Davis</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Aidan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Milton Chen</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+J">James Davis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE Global Humanitarian Technology Conference (GHTC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Population health surveys are an important tool to effectively allocate
limited resources in low resource communities. In such an environment, surveys
are often done by local population with pen and paper. Data thus collected is
difficult to tabulate and analyze. We conducted a series of interviews and
experiments in the Philippines to assess if mobile forms can be a viable and
more efficient survey method. We first conducted pilot interviews and found 60%
of the local surveyors actually preferred mobile forms over paper. We then
built a software that can generate mobile forms that are easy to use, capable
of working offline, and able to track key metrics such as time to complete
questions. Our mobile form was field tested in three locations in the
Philippines with 33 surveyors collecting health survey responses from 266
subjects. The percentage of surveyors preferring mobile forms increased to 76%
after just using the form a few times. The results demonstrate our mobile form
is a viable method to conduct large scale population health surveys in a low
resource environment.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07889" title="Abstract">arXiv:2310.07889</a> [<a href="/pdf/2310.07889" title="Download PDF">pdf</a>, <a href="/format/2310.07889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LangNav: Language as a Perceptual Representation for Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+B">Bowen Pan</a>, 
<a href="/search/cs?searchtype=author&query=Panda%2C+R">Rameswar Panda</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">SouYoung Jin</a>, 
<a href="/search/cs?searchtype=author&query=Feris%2C+R">Rogerio Feris</a>, 
<a href="/search/cs?searchtype=author&query=Oliva%2C+A">Aude Oliva</a>, 
<a href="/search/cs?searchtype=author&query=Isola%2C+P">Phillip Isola</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yoon Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Robotics (cs.RO)

</div>
<p class="mathjax">We explore the use of language as a perceptual representation for
vision-and-language navigation. Our approach uses off-the-shelf vision systems
(for image captioning and object detection) to convert an agent's egocentric
panoramic view at each time step into natural language descriptions. We then
finetune a pretrained language model to select an action, based on the current
view and the trajectory history, that would best fulfill the navigation
instructions. In contrast to the standard setup which adapts a pretrained
language model to work directly with continuous visual features from pretrained
vision models, our approach instead uses (discrete) language as the perceptual
representation. We explore two use cases of our language-based navigation
(LangNav) approach on the R2R vision-and-language navigation benchmark:
generating synthetic trajectories from a prompted large language model (GPT-4)
with which to finetune a smaller language model; and sim-to-real transfer where
we transfer a policy learned on a simulated environment (ALFRED) to a
real-world environment (R2R). Our approach is found to improve upon strong
baselines that rely on visual features in settings where only a few gold
trajectories (10-100) are available, demonstrating the potential of using
language as a perceptual representation for navigation tasks.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07890" title="Abstract">arXiv:2310.07890</a> [<a href="/pdf/2310.07890" title="Download PDF">pdf</a>, <a href="/format/2310.07890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cut-Cell Microstructures for Two-scale Structural Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tozoni%2C+D+C">Davi Colli Tozoni</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zizhou Huang</a>, 
<a href="/search/cs?searchtype=author&query=Panozzo%2C+D">Daniele Panozzo</a>, 
<a href="/search/cs?searchtype=author&query=Zorin%2C+D">Denis Zorin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Two-scale topology optimization, combined with the design of microstructure
families with a broad range of effective material parameters, is increasingly
widely used in many fabrication applications to achieve a target deformation
behavior for a variety of objects. The main idea of this approach is to
optimize the distribution of material properties in the object partitioned into
relatively coarse cells, and then replace each cell with microstructure
geometry that mimics these material properties. In this paper, we focus on
adapting this approach to complex shapes in situations when preserving the
shape's surface is important. Our approach extends any regular (i.e. defined on
a regular lattice grid) microstructure family to complex shapes, by enriching
it with individually optimized cut-cell tiles adapted to the geometry of the
cut-cell. We propose an automated and robust pipeline based on this approach,
and we show that the performance of the regular microstructure family is only
minimally affected by our extension while allowing its use on 2D and 3D shapes
of high complexity.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07892" title="Abstract">arXiv:2310.07892</a> [<a href="/pdf/2310.07892" title="Download PDF">pdf</a>, <a href="/format/2310.07892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASV Station Keeping under Wind Disturbances using Neural Network  Simulation Error Minimization Model Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chavez-Galaviz%2C+J">Jalil Chavez-Galaviz</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianwen Li</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhary%2C+A">Ajinkya Chaudhary</a>, 
<a href="/search/cs?searchtype=author&query=Mahmoudian%2C+N">Nina Mahmoudian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Station keeping is an essential maneuver for Autonomous Surface Vehicles
(ASVs), mainly when used in confined spaces, to carry out surveys that require
the ASV to keep its position or in collaboration with other vehicles where the
relative position has an impact over the mission. However, this maneuver can
become challenging for classic feedback controllers due to the need for an
accurate model of the ASV dynamics and the environmental disturbances. This
work proposes a Model Predictive Controller using Neural Network Simulation
Error Minimization (NNSEM-MPC) to accurately predict the dynamics of the ASV
under wind disturbances. The performance of the proposed scheme under wind
disturbances is tested and compared against other controllers in simulation,
using the Robotics Operating System (ROS) and the multipurpose simulation
environment Gazebo. A set of six tests were conducted by combining two wind
speeds (3 m/s and 6 m/s) and three wind directions (0$^\circ$, 90$^\circ$, and
180$^\circ$). The simulation results clearly show the advantage of the
NNSEM-MPC over the following methods: backstepping controller, sliding mode
controller, simplified dynamics MPC (SD-MPC), neural ordinary differential
equation MPC (NODE-MPC), and knowledge-based NODE MPC (KNODE-MPC). The proposed
NNSEM-MPC approach performs better than the rest in 4 out of the 6 test
conditions, and it is the second best in the 2 remaining test cases, reducing
the mean position and heading error by at least 31\% and 46\% respectively
across all the test cases. In terms of execution speed, the proposed NNSEM-MPC
is at least 36\% faster than the rest of the MPC controllers. The field
experiments on two different ASV platforms showed that ASVs can effectively
keep the station utilizing the proposed method, with a position error as low as
$1.68$ m and a heading error as low as $6.14^{\circ}$ within time windows of at
least $150$s.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07894" title="Abstract">arXiv:2310.07894</a> [<a href="/pdf/2310.07894" title="Download PDF">pdf</a>, <a href="/format/2310.07894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Integrators for Diffusion Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pandey%2C+K">Kushagra Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Rudolph%2C+M">Maja Rudolph</a>, 
<a href="/search/cs?searchtype=author&query=Mandt%2C+S">Stephan Mandt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">Diffusion models suffer from slow sample generation at inference time.
Therefore, developing a principled framework for fast deterministic/stochastic
sampling for a broader class of diffusion models is a promising direction. We
propose two complementary frameworks for accelerating sample generation in
pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate
integrators generalize DDIM, mapping the reverse diffusion dynamics to a more
amenable space for sampling. In contrast, splitting-based integrators, commonly
used in molecular dynamics, reduce the numerical simulation error by cleverly
alternating between numerical updates involving the data and auxiliary
variables. After extensively studying these methods empirically and
theoretically, we present a hybrid method that leads to the best-reported
performance for diffusion models in augmented spaces. Applied to Phase Space
Langevin Diffusion [Pandey &amp; Mandt, 2023] on CIFAR-10, our deterministic and
stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network
function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing
baselines, respectively. Our code and model checkpoints will be made publicly
available at \url{https://github.com/mandt-lab/PSLD}.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07895" title="Abstract">arXiv:2310.07895</a> [<a href="/pdf/2310.07895" title="Download PDF">pdf</a>, <a href="/format/2310.07895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Precise localization within the GI tract by combining classification of  CNNs and time-series analysis of HMMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Werner%2C+J">Julia Werner</a>, 
<a href="/search/cs?searchtype=author&query=Gerum%2C+C">Christoph Gerum</a>, 
<a href="/search/cs?searchtype=author&query=Reiber%2C+M">Moritz Reiber</a>, 
<a href="/search/cs?searchtype=author&query=Nick%2C+J">J&#xf6;rg Nick</a>, 
<a href="/search/cs?searchtype=author&query=Bringmann%2C+O">Oliver Bringmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at MLMI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper presents a method to efficiently classify the gastroenterologic
section of images derived from Video Capsule Endoscopy (VCE) studies by
exploring the combination of a Convolutional Neural Network (CNN) for
classification with the time-series analysis properties of a Hidden Markov
Model (HMM). It is demonstrated that successive time-series analysis identifies
and corrects errors in the CNN output. Our approach achieves an accuracy of
$98.04\%$ on the Rhode Island (RI) Gastroenterology dataset. This allows for
precise localization within the gastrointestinal (GI) tract while requiring
only approximately 1M parameters and thus, provides a method suitable for low
power devices
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07896" title="Abstract">arXiv:2310.07896</a> [<a href="/pdf/2310.07896" title="Download PDF">pdf</a>, <a href="/format/2310.07896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sridhar%2C+A">Ajay Sridhar</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+D">Dhruv Shah</a>, 
<a href="/search/cs?searchtype=author&query=Glossop%2C+C">Catherine Glossop</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page <a href="https://general-navigation-models.github.io/nomad/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Robotic learning for navigation in unfamiliar environments needs to provide
policies for both task-oriented navigation (i.e., reaching a goal that the
robot has located), and task-agnostic exploration (i.e., searching for a goal
in a novel setting). Typically, these roles are handled by separate models, for
example by using subgoal proposals, planning, or separate navigation
strategies. In this paper, we describe how we can train a single unified
diffusion policy to handle both goal-directed navigation and goal-agnostic
exploration, with the latter providing the ability to search novel
environments, and the former providing the ability to reach a user-specified
goal once it has been located. We show that this unified policy results in
better overall performance when navigating to visually indicated goals in novel
environments, as compared to approaches that use subgoal proposals from
generative models, or prior methods based on latent variable models. We
instantiate our method by using a large-scale Transformer-based policy trained
on data from multiple ground robots, with a diffusion model decoder to flexibly
handle both goal-conditioned and goal-agnostic navigation. Our experiments,
conducted on a real-world mobile robot platform, show effective navigation in
unseen environments in comparison with five alternative methods, and
demonstrate significant improvements in performance and lower collision rates,
despite utilizing smaller models than state-of-the-art approaches. For more
videos, code, and pre-trained model checkpoints, see
https://general-navigation-models.github.io/nomad/
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07898" title="Abstract">arXiv:2310.07898</a> [<a href="/pdf/2310.07898" title="Download PDF">pdf</a>, <a href="/format/2310.07898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiversion Hindsight Logging for Continuous Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garcia%2C+R">Rolando Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Dandamudi%2C+A">Anusha Dandamudi</a>, 
<a href="/search/cs?searchtype=author&query=Matute%2C+G">Gabriel Matute</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+L">Lehan Wan</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+J">Joseph Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Hellerstein%2C+J+M">Joseph M. Hellerstein</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+K">Koushik Sen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Databases (cs.DB)

</div>
<p class="mathjax">Production Machine Learning involves hosting multiple versions of models over
time, often with many model versions running at once. When model performance
does not meet expectations, Machine Learning Engineers (MLEs) debug issues by
exploring and analyzing numerous prior versions of code and training data to
identify root causes and mitigate problems. Traditional debugging and logging
tools often fall short in managing this experimental, multi-version context. To
address the challenges in this domain, novel approaches are required for
logging and log data management.
<br />FlorDB introduces Multiversion Hindsight Logging, which allows engineers to
use the most recent version's logging statements to explore past versions, even
when older versions logged different data. Log statement propagation enables
consistent injection of logging statements into past code versions, regardless
of changes to the codebase. Once log statements are propagated across code
versions, the remaining challenges in Multiversion Hindsight Logging relate to
efficiently replaying the new log statements based on checkpoints from previous
runs. Finally, a coherent user experience is required to help MLEs debug across
all versions of code and data. To this end, FlorDB presents a unified
relational model for efficient handling of historical queries, offering a
comprehensive view of the log history to simplify the exploration of past code
iterations.
<br />In sum, FlorDB provides a robust tool tailored to the specific needs of MLEs,
significantly enhancing their ability to navigate the intricate landscape of ML
experimentation.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07899" title="Abstract">arXiv:2310.07899</a> [<a href="/pdf/2310.07899" title="Download PDF">pdf</a>, <a href="/format/2310.07899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoboCLIP: One Demonstration is Enough to Learn Robot Policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sontakke%2C+S+A">Sumedh A Sontakke</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jesse Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Arnold%2C+S+M+R">S&#xe9;bastien M. R. Arnold</a>, 
<a href="/search/cs?searchtype=author&query=Pertsch%2C+K">Karl Pertsch</a>, 
<a href="/search/cs?searchtype=author&query=B%C4%B1y%C4%B1k%2C+E">Erdem B&#x131;y&#x131;k</a>, 
<a href="/search/cs?searchtype=author&query=Sadigh%2C+D">Dorsa Sadigh</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>, 
<a href="/search/cs?searchtype=author&query=Itti%2C+L">Laurent Itti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Reward specification is a notoriously difficult problem in reinforcement
learning, requiring extensive expert supervision to design robust reward
functions. Imitation learning (IL) methods attempt to circumvent these problems
by utilizing expert demonstrations but typically require a large number of
in-domain expert demonstrations. Inspired by advances in the field of
Video-and-Language Models (VLMs), we present RoboCLIP, an online imitation
learning method that uses a single demonstration (overcoming the large data
requirement) in the form of a video demonstration or a textual description of
the task to generate rewards without manual reward function design.
Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like
videos of humans solving the task for reward generation, circumventing the need
to have the same demonstration and deployment domains. RoboCLIP utilizes
pretrained VLMs without any finetuning for reward generation. Reinforcement
learning agents trained with RoboCLIP rewards demonstrate 2-3 times higher
zero-shot performance than competing imitation learning methods on downstream
robot manipulation tasks, doing so using only one video/text demonstration.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07902" title="Abstract">arXiv:2310.07902</a> [<a href="/pdf/2310.07902" title="Download PDF">pdf</a>, <a href="/format/2310.07902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unraveling the Single Tangent Space Fallacy: An Analysis and  Clarification for Applying Riemannian Geometry in Robot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaquier%2C+N">No&#xe9;mie Jaquier</a>, 
<a href="/search/cs?searchtype=author&query=Rozo%2C+L">Leonel Rozo</a>, 
<a href="/search/cs?searchtype=author&query=Asfour%2C+T">Tamim Asfour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the realm of robotics, numerous downstream robotics tasks leverage machine
learning methods for processing, modeling, or synthesizing data. Often, this
data comprises variables that inherently carry geometric constraints, such as
the unit-norm condition of quaternions representing rigid-body orientations or
the positive definiteness of stiffness and manipulability ellipsoids. Handling
such geometric constraints effectively requires the incorporation of tools from
differential geometry into the formulation of machine learning methods. In this
context, Riemannian manifolds emerge as a powerful mathematical framework to
handle such geometric constraints. Nevertheless, their recent adoption in robot
learning has been largely characterized by a mathematically-flawed
simplification, hereinafter referred to as the ``single tangent space fallacy".
This approach involves merely projecting the data of interest onto a single
tangent (Euclidean) space, over which an off-the-shelf learning algorithm is
applied. This paper provides a theoretical elucidation of various
misconceptions surrounding this approach and offers experimental evidence of
its shortcomings. Finally, it presents valuable insights to promote best
practices when employing Riemannian geometry within robot learning
applications.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07903" title="Abstract">arXiv:2310.07903</a> [<a href="/pdf/2310.07903" title="Download PDF">pdf</a>, <a href="/format/2310.07903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sorting it out in Hardware: A State-of-the-Art Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jalilvand%2C+A+H">Amir Hossein Jalilvand</a>, 
<a href="/search/cs?searchtype=author&query=Banitaba%2C+F+S">Faeze S. Banitaba</a>, 
<a href="/search/cs?searchtype=author&query=Estiri%2C+S+N">Seyedeh Newsha Estiri</a>, 
<a href="/search/cs?searchtype=author&query=Aygun%2C+S">Sercan Aygun</a>, 
<a href="/search/cs?searchtype=author&query=Najafi%2C+M+H">M. Hassan Najafi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Sorting is a fundamental operation in various applications and a traditional
research topic in computer science. Improving the performance of sorting
operations can have a significant impact on many application domains. For
high-performance sorting, much attention has been paid to hardware-based
solutions. These are often realized with application-specific integrated
circuits (ASICs) or field-programmable gate arrays (FPGAs). Recently, in-memory
sorting solutions have also been proposed to address the movement cost issue
between memory and processing units, also known as Von Neumann bottleneck. Due
to the complexity of the sorting algorithms, achieving an efficient hardware
implementation for sorting data is challenging. A large body of prior solutions
is built on compare-and-swap (CAS) units. These are categorized as
comparison-based sorting. Some recent solutions offer comparison-free sorting.
In this survey, we review the latest works in the area of hardware-based
sorting. We also discuss the recent hardware solutions for partial and stream
sorting. Finally, we will discuss some important concerns that need to be
considered in the future designs of sorting systems.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07904" title="Abstract">arXiv:2310.07904</a> [<a href="/pdf/2310.07904" title="Download PDF">pdf</a>, <a href="/format/2310.07904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Realizability Modulo Theories to Synthesis Modulo Theories Part 1:  Dynamic approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rodr%C3%ADguez%2C+A">Andoni Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Sanchez%2C+C">Cesar Sanchez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Reactive synthesis is the process of using temporal logic specifications in
LTL to generate correct controllers, but its use has been restricted to Boolean
specifications. Recently, a Boolean abstraction technique allows to translate
LTL T specifications that contain literals in theories into equi-realizable LTL
specifications. However, no synthesis procedure exists yet. In synthesis modulo
theories, the system to synthesize receives valuations of environment variables
in a first-order theory T and outputs valuations of system variables from T .
In this paper, we address how to syntheize a full controller using a
combination of the static Boolean controller obtained from the Booleanized LTL
specification together with dynamic queries to a solver that produces models of
a satisfiable existential formulae from T . This is the first method that
realizes reactive synthesis modulo theories.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07906" title="Abstract">arXiv:2310.07906</a> [<a href="/pdf/2310.07906" title="Download PDF">pdf</a>, <a href="/ps/2310.07906" title="Download PostScript">ps</a>, <a href="/format/2310.07906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Power Tracking Control of Heterogeneous Populations of TCLs with  Partially Measured States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Z">Zhenhe Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+J">Jun Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+G">Guchuan Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2305.09118">arXiv:2305.09118</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Analysis of PDEs (math.AP); Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper presents a new aggregate power tracking control scheme for
populations of thermostatically controlled loads (TCLs). The control design is
performed in the framework of partial differential equations (PDEs) based on a
late-lumping procedure without truncating the infinite-dimensional model
describing the dynamics of the TCL population. An input-output linearization
control scheme, which is independent of system parameters and uses only partial
state measurement, is derived, and a sliding model-like control is applied to
achieve finite-time input-to-state stability for tracking error dynamics. Such
a control strategy can ensure robust performance in the presence of modeling
uncertainties, while considerably reducing the communication burden in large
scale distributed systems similar to that considered in the present work. A
rigorous analysis of the closed-loop stability of the underlying PDE system was
conducted, which guaranteed the validity of the developed control scheme.
Simulation studies were performed while considering two TCL populations with a
significant difference in their size, and the results show that the developed
control scheme performs well in both cases, thereby confirming the
effectiveness of the proposed solution.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07911" title="Abstract">arXiv:2310.07911</a> [<a href="/pdf/2310.07911" title="Download PDF">pdf</a>, <a href="/format/2310.07911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pit One Against Many: Leveraging Attention-head Embeddings for  Parameter-efficient Multi-head Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+H">Huiyin Xue</a>, 
<a href="/search/cs?searchtype=author&query=Aletras%2C+N">Nikolaos Aletras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Scaling pre-trained language models has resulted in large performance gains
in various natural language processing tasks but comes with a large cost in
memory requirements. Inspired by the position embeddings in transformers, we
aim to simplify and reduce the memory footprint of the multi-head attention
(MHA) mechanism. We propose an alternative module that uses only a single
shared projection matrix and multiple head embeddings (MHE), i.e. one per head.
We empirically demonstrate that our MHE attention is substantially more memory
efficient compared to alternative attention mechanisms while achieving high
predictive performance retention ratio to vanilla MHA on several downstream
tasks. MHE attention only requires a negligible fraction of additional
parameters ($3nd$, where $n$ is the number of attention heads and $d$ the size
of the head embeddings) compared to a single-head attention, while MHA requires
$(3n^2-3n)d^2-3nd$ additional parameters.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07915" title="Abstract">arXiv:2310.07915</a> [<a href="/pdf/2310.07915" title="Download PDF">pdf</a>, <a href="/format/2310.07915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tag Your Fish in the Broken Net: A Responsible Web Framework for  Protecting Online Privacy and Copyright
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dawen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+B">Boming Xia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hoang%2C+T">Thong Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+Z">Zhenchang Xing</a>, 
<a href="/search/cs?searchtype=author&query=Staples%2C+M">Mark Staples</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Q">Qinghua Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Liming Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The World Wide Web, a ubiquitous source of information, serves as a primary
resource for countless individuals, amassing a vast amount of data from global
internet users. However, this online data, when scraped, indexed, and utilized
for activities like web crawling, search engine indexing, and, notably, AI
model training, often diverges from the original intent of its contributors.
The ascent of Generative AI has accentuated concerns surrounding data privacy
and copyright infringement. Regrettably, the web's current framework falls
short in facilitating pivotal actions like consent withdrawal or data copyright
claims. While some companies offer voluntary measures, such as crawler access
restrictions, these often remain inaccessible to individual users. To empower
online users to exercise their rights and enable companies to adhere to
regulations, this paper introduces a user-controlled consent tagging framework
for online data. It leverages the extensibility of HTTP and HTML in conjunction
with the decentralized nature of distributed ledger technology. With this
framework, users have the ability to tag their online data at the time of
transmission, and subsequently, they can track and request the withdrawal of
consent for their data from the data holders. A proof-of-concept system is
implemented, demonstrating the feasibility of the framework. This work holds
significant potential for contributing to the reinforcement of user consent,
privacy, and copyright on the modern internet and lays the groundwork for
future insights into creating a more responsible and user-centric web
ecosystem.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07916" title="Abstract">arXiv:2310.07916</a> [<a href="/pdf/2310.07916" title="Download PDF">pdf</a>, <a href="/format/2310.07916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Appearance Particle Neural Radiance Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+A">Ancheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Neural Radiance Fields (NeRFs) have shown great potential in modelling 3D
scenes. Dynamic NeRFs extend this model by capturing time-varying elements,
typically using deformation fields. The existing dynamic NeRFs employ a similar
Eulerian representation for both light radiance and deformation fields. This
leads to a close coupling of appearance and motion and lacks a physical
interpretation. In this work, we propose Dynamic Appearance Particle Neural
Radiance Field (DAP-NeRF), which introduces particle-based representation to
model the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists
of superposition of a static field and a dynamic field. The dynamic field is
quantised as a collection of {\em appearance particles}, which carries the
visual information of a small dynamic element in the scene and is equipped with
a motion model. All components, including the static field, the visual features
and motion models of the particles, are learned from monocular videos without
any prior geometric knowledge of the scene. We develop an efficient
computational framework for the particle-based model. We also construct a new
dataset to evaluate motion modelling. Experimental results show that DAP-NeRF
is an effective technique to capture not only the appearance but also the
physically meaningful motions in a 3D dynamic scene.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07917" title="Abstract">arXiv:2310.07917</a> [<a href="/pdf/2310.07917" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review of Machine Learning Techniques in Imbalanced Data and Future  Trends
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jafarigol%2C+E">Elaheh Jafarigol</a>, 
<a href="/search/cs?searchtype=author&query=Trafalis%2C+T">Theodore Trafalis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">For over two decades, detecting rare events has been a challenging task among
researchers in the data mining and machine learning domain. Real-life problems
inspire researchers to navigate and further improve data processing and
algorithmic approaches to achieve effective and computationally efficient
methods for imbalanced learning. In this paper, we have collected and reviewed
258 peer-reviewed papers from archival journals and conference papers in an
attempt to provide an in-depth review of various approaches in imbalanced
learning from technical and application perspectives. This work aims to provide
a structured review of methods used to address the problem of imbalanced data
in various domains and create a general guideline for researchers in academia
or industry who want to dive into the broad field of machine learning using
large-scale imbalanced data.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07918" title="Abstract">arXiv:2310.07918</a> [<a href="/pdf/2310.07918" title="Download PDF">pdf</a>, <a href="/format/2310.07918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextualized Policy Recovery: Modeling and Interpreting Medical  Decisions with Adaptive Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deuschel%2C+J">Jannik Deuschel</a>, 
<a href="/search/cs?searchtype=author&query=Ellington%2C+C+N">Caleb N. Ellington</a>, 
<a href="/search/cs?searchtype=author&query=Lengerich%2C+B+J">Benjamin J. Lengerich</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yingtao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Friederich%2C+P">Pascal Friederich</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E+P">Eric P. Xing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Interpretable policy learning seeks to estimate intelligible decision
policies from observed actions; however, existing models fall short by forcing
a tradeoff between accuracy and interpretability. This tradeoff limits
data-driven interpretations of human decision-making process. e.g. to audit
medical decisions for biases and suboptimal practices, we require models of
decision processes which provide concise descriptions of complex behaviors.
Fundamentally, existing approaches are burdened by this tradeoff because they
represent the underlying decision process as a universal policy, when in fact
human decisions are dynamic and can change drastically with contextual
information. Thus, we propose Contextualized Policy Recovery (CPR), which
re-frames the problem of modeling complex decision processes as a multi-task
learning problem in which complex decision policies are comprised of
context-specific policies. CPR models each context-specific policy as a linear
observation-to-action mapping, and generates new decision models
$\textit{on-demand}$ as contexts are updated with new observations. CPR is
compatible with fully offline and partially observable decision environments,
and can be tailored to incorporate any recurrent black-box model or
interpretable decision model. We assess CPR through studies on simulated and
real data, achieving state-of-the-art performance on the canonical tasks of
predicting antibiotic prescription in intensive care units ($+22\%$ AUROC vs.
previous SOTA) and predicting MRI prescription for Alzheimer's patients
($+7.7\%$ AUROC vs. previous SOTA). With this improvement in predictive
performance, CPR closes the accuracy gap between interpretable and black-box
methods for policy learning, allowing high-resolution exploration and analysis
of context-specific decision models.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07921" title="Abstract">arXiv:2310.07921</a> [<a href="/pdf/2310.07921" title="Download PDF">pdf</a>, <a href="/format/2310.07921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Large-Scale Exploratory Study of Android Sports Apps in the Google  Play Store
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chembakottu%2C+B">Bhagya Chembakottu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Heng Li</a>, 
<a href="/search/cs?searchtype=author&query=Khomh%2C+F">Foutse Khomh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Journal paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Information and Software Technology (2023): 107321
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Prior studies on mobile app analysis often analyze apps across different
categories or focus on a small set of apps within a category. These studies
either provide general insights for an entire app store which consists of
millions of apps, or provide specific insights for a small set of apps.
However, a single app category can often contain tens of thousands to hundreds
of thousands of apps. For example, according to AppBrain, there are 46,625 apps
in the "Sports" category of Google Play apps. Analyzing such a targeted
category of apps can provide more specific insights than analyzing apps across
categories while still benefiting many app developers interested in the
category. This work aims to study a large number of apps from a single category
(i.e., the sports category). We performed an empirical study on over two
thousand sports apps in the Google Play Store. We study the characteristics of
these apps (e.g., their targeted sports types and main functionalities) through
manual analysis, the topics in the user review through topic modeling, as well
as the aspects that contribute to the negative opinions of users through
analysis of user ratings and sentiment. It is concluded that analyzing a
targeted category of apps (e.g., sports apps) can provide more specific
insights than analyzing apps across different categories while still being
relevant for a large number (e.g., tens of thousands) of apps. Besides, as a
rapid-growing and competitive market, sports apps provide rich opportunities
for future research, for example, to study the integration of data science or
machine learning techniques in software applications or to study the factors
that influence the competitiveness of the apps.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07923" title="Abstract">arXiv:2310.07923</a> [<a href="/pdf/2310.07923" title="Download PDF">pdf</a>, <a href="/ps/2310.07923" title="Download PostScript">ps</a>, <a href="/format/2310.07923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Expresssive Power of Transformers with Chain of Thought
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Merrill%2C+W">William Merrill</a>, 
<a href="/search/cs?searchtype=author&query=Sabharwal%2C+A">Ashish Sabharwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9-page preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Complexity (cs.CC); Computation and Language (cs.CL); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Recent theoretical work has identified surprisingly simple reasoning
problems, such as checking if two nodes in a graph are connected or simulating
finite-state machines, that are provably unsolvable by standard transformers
that answer immediately after reading their input. However, in practice,
transformers' reasoning can be improved by allowing them to use a "chain of
thought" or "scratchpad", i.e., generate and condition on a sequence of
intermediate tokens before answering. Motivated by this, we ask: Does such
intermediate generation fundamentally extend the computational power of a
decoder-only transformer? We show that the answer is yes, but the amount of
increase depends crucially on the amount of intermediate generation. For
instance, we find that transformer decoders with a logarithmic number of
decoding steps (w.r.t. the input length) push the limits of standard
transformers only slightly, while a linear number of decoding steps adds a
clear new ability (under standard complexity conjectures): recognizing all
regular languages. Our results also imply that linear steps keep transformer
decoders within context-sensitive languages, and polynomial steps make them
recognize exactly the class of polynomial-time solvable problems -- the first
exact characterization of a type of transformers in terms of standard
complexity classes. Together, our results provide a nuanced framework for
understanding how the length of a transformer's chain of thought or scratchpad
impacts its reasoning power.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07929" title="Abstract">arXiv:2310.07929</a> [<a href="/pdf/2310.07929" title="Download PDF">pdf</a>, <a href="/format/2310.07929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crosslingual Structural Priming and the Pre-Training Dynamics of  Bilingual Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arnett%2C+C">Catherine Arnett</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+T+A">Tyler A. Chang</a>, 
<a href="/search/cs?searchtype=author&query=Michaelov%2C+J+A">James A. Michaelov</a>, 
<a href="/search/cs?searchtype=author&query=Bergen%2C+B+K">Benjamin K. Bergen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended abstract accepted to the 3rd Multilingual Representation Learning workshop at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Do multilingual language models share abstract grammatical representations
across languages, and if so, when do these develop? Following Sinclair et al.
(2022), we use structural priming to test for abstract grammatical
representations with causal effects on model outputs. We extend the approach to
a Dutch-English bilingual setting, and we evaluate a Dutch-English language
model during pre-training. We find that crosslingual structural priming effects
emerge early after exposure to the second language, with less than 1M tokens of
data in that language. We discuss implications for data contamination,
low-resource transfer, and how abstract grammatical representations emerge in
multilingual models.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07931" title="Abstract">arXiv:2310.07931</a> [<a href="/pdf/2310.07931" title="Download PDF">pdf</a>, <a href="/format/2310.07931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> D2 Pruning: Message Passing for Balancing Diversity and Difficulty in  Data Pruning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maharana%2C+A">Adyasha Maharana</a>, 
<a href="/search/cs?searchtype=author&query=Yadav%2C+P">Prateek Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages (Our code is available at <a href="https://github.com/adymaharana/d2pruning">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Analytical theories suggest that higher-quality data can lead to lower test
errors in models trained on a fixed data budget. Moreover, a model can be
trained on a lower compute budget without compromising performance if a dataset
can be stripped of its redundancies. Coreset selection (or data pruning) seeks
to select a subset of the training data so as to maximize the performance of
models trained on this subset, also referred to as coreset. There are two
dominant approaches: (1) geometry-based data selection for maximizing data
diversity in the coreset, and (2) functions that assign difficulty scores to
samples based on training dynamics. Optimizing for data diversity leads to a
coreset that is biased towards easier samples, whereas, selection by difficulty
ranking omits easy samples that are necessary for the training of deep learning
models. This demonstrates that data diversity and importance scores are two
complementary factors that need to be jointly considered during coreset
selection. We represent a dataset as an undirected graph and propose a novel
pruning algorithm, D2 Pruning, that uses forward and reverse message passing
over this dataset graph for coreset selection. D2 Pruning updates the
difficulty scores of each example by incorporating the difficulty of its
neighboring examples in the dataset graph. Then, these updated difficulty
scores direct a graph-based sampling method to select a coreset that
encapsulates both diverse and difficult regions of the dataset space. We
evaluate supervised and self-supervised versions of our method on various
vision and language datasets. Results show that D2 Pruning improves coreset
selection over previous state-of-the-art methods for up to 70% pruning rates.
Additionally, we find that using D2 Pruning for filtering large multimodal
datasets leads to increased diversity in the dataset and improved
generalization of pretrained models.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07932" title="Abstract">arXiv:2310.07932</a> [<a href="/pdf/2310.07932" title="Download PDF">pdf</a>, <a href="/format/2310.07932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Matters to You? Towards Visual Representation Alignment for Robot  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+R">Ran Tian</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenfeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>, 
<a href="/search/cs?searchtype=author&query=Malik%2C+J">Jitendra Malik</a>, 
<a href="/search/cs?searchtype=author&query=Bajcsy%2C+A">Andrea Bajcsy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">When operating in service of people, robots need to optimize rewards aligned
with end-user preferences. Since robots will rely on raw perceptual inputs like
RGB images, their rewards will inevitably use visual representations. Recently
there has been excitement in using representations from pre-trained visual
models, but key to making these work in robotics is fine-tuning, which is
typically done via proxy tasks like dynamics prediction or enforcing temporal
cycle-consistency. However, all these proxy tasks bypass the human's input on
what matters to them, exacerbating spurious correlations and ultimately leading
to robot behaviors that are misaligned with user preferences. In this work, we
propose that robots should leverage human feedback to align their visual
representations with the end-user and disentangle what matters for the task. We
propose Representation-Aligned Preference-based Learning (RAPL), a method for
solving the visual representation alignment problem and visual reward learning
problem through the lens of preference-based learning and optimal transport.
Across experiments in X-MAGICAL and in robotic manipulation, we find that
RAPL's reward consistently generates preferred robot behaviors with high sample
efficiency, and shows strong zero-shot generalization when the visual
representation is learned from a different embodiment than the robot's.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07937" title="Abstract">arXiv:2310.07937</a> [<a href="/pdf/2310.07937" title="Download PDF">pdf</a>, <a href="/format/2310.07937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bangguo Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kasaei%2C+H">Hamidreza Kasaei</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+M">Ming Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In advanced human-robot interaction tasks, visual target navigation is
crucial for autonomous robots navigating unknown environments. While numerous
approaches have been developed in the past, most are designed for single-robot
operations, which often suffer from reduced efficiency and robustness due to
environmental complexities. Furthermore, learning policies for multi-robot
collaboration are resource-intensive. To address these challenges, we propose
Co-NavGPT, an innovative framework that integrates Large Language Models (LLMs)
as a global planner for multi-robot cooperative visual target navigation.
Co-NavGPT encodes the explored environment data into prompts, enhancing LLMs'
scene comprehension. It then assigns exploration frontiers to each robot for
efficient target search. Experimental results on Habitat-Matterport 3D (HM3D)
demonstrate that Co-NavGPT surpasses existing models in success rates and
efficiency without any learning process, demonstrating the vast potential of
LLMs in multi-robot collaboration domains. The supplementary video, prompts,
and code can be accessed via the following link:
\href{https://sites.google.com/view/co-navgpt}{https://sites.google.com/view/co-navgpt}.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07940" title="Abstract">arXiv:2310.07940</a> [<a href="/pdf/2310.07940" title="Download PDF">pdf</a>, <a href="/format/2310.07940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cost-Driven Hardware-Software Co-Optimization of Machine Learning  Pipelines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Ravit Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Romaszkan%2C+W">Wojciech Romaszkan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Feiqian Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+P">Puneet Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Researchers have long touted a vision of the future enabled by a
proliferation of internet-of-things devices, including smart sensors, homes,
and cities. Increasingly, embedding intelligence in such devices involves the
use of deep neural networks. However, their storage and processing requirements
make them prohibitive for cheap, off-the-shelf platforms. Overcoming those
requirements is necessary for enabling widely-applicable smart devices. While
many ways of making models smaller and more efficient have been developed,
there is a lack of understanding of which ones are best suited for particular
scenarios. More importantly for edge platforms, those choices cannot be
analyzed in isolation from cost and user experience. In this work, we
holistically explore how quantization, model scaling, and multi-modality
interact with system components such as memory, sensors, and processors. We
perform this hardware/software co-design from the cost, latency, and
user-experience perspective, and develop a set of guidelines for optimal system
design and model deployment for the most cost-constrained platforms. We
demonstrate our approach using an end-to-end, on-device, biometric user
authentication system using a $20 ESP-EYE board.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07941" title="Abstract">arXiv:2310.07941</a> [<a href="/pdf/2310.07941" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Convolutional Network Adaptation for Cortical Classification During  Mobile Brain Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cichy%2C+B">Benjamin Cichy</a>, 
<a href="/search/cs?searchtype=author&query=Lukos%2C+J">Jamie Lukos</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+M">Mohammad Alam</a>, 
<a href="/search/cs?searchtype=author&query=Bradford%2C+J+C">J. Cortney Bradford</a>, 
<a href="/search/cs?searchtype=author&query=Wymbs%2C+N">Nicholas Wymbs</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Deep neural networks (DNN) have become increasingly utilized in
brain-computer interface (BCI) technologies with the outset goal of classifying
human physiological signals in computer-readable format. While our present
understanding of DNN usage for BCI is promising, we have little experience in
deciphering neural events from dynamic freely-mobile situations. Using an
improved version of EEGNet, our goal was to classify cognitive events from
electroencephalography (EEG) signals while subjects simultaneously walked on a
treadmill, sometimes while carrying a rucksack equivalent to 40% of their body
weight. Walking subjects simultaneously performed a visual oddball target
detection task, eliciting the P300 event-related potential (ERP), which then
served as the DNN classification target. We found the base EEGNet to reach
classification levels well above chance, with similar performance to previously
reported P300 results. We found performance to be robust to noise, with
classification similar for walking and loaded walking, with respect to standard
seated condition with minimal movement. With additional architectural search
and tuning to the EEGNet model (termed Cog-Neuro, herein; CN-EEGNet), we
reached classification accuracy of greater than 95%, similar to previously
reported state of the art levels in seated P300 tasks. To our knowledge, these
results are the first documented implementation of a DNN for the classification
of cognitive neural state during dual-task walking. The classification of one's
ongoing cognitive state during a demanding physical task establishes the
utility for BCI in complex environments.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07944" title="Abstract">arXiv:2310.07944</a> [<a href="/pdf/2310.07944" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoRepo: A general framework for multi-modal LLM-based automated  construction reporting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pu%2C+H">Hongxu Pu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xincong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+R">Runhao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Heng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Ensuring the safety, quality, and timely completion of construction projects
is paramount, with construction inspections serving as a vital instrument
towards these goals. Nevertheless, the predominantly manual approach of
present-day inspections frequently results in inefficiencies and inadequate
information management. Such methods often fall short of providing holistic,
exhaustive assessments, consequently engendering regulatory oversights and
potential safety hazards. To address this issue, this paper presents a novel
framework named AutoRepo for automated generation of construction inspection
reports. The unmanned vehicles efficiently perform construction inspections and
collect scene information, while the multimodal large language models (LLMs)
are leveraged to automatically generate the inspection reports. The framework
was applied and tested on a real-world construction site, demonstrating its
potential to expedite the inspection process, significantly reduce resource
allocation, and produce high-quality, regulatory standard-compliant inspection
reports. This research thus underscores the immense potential of multimodal
large language models in revolutionizing construction inspection practices,
signaling a significant leap forward towards a more efficient and safer
construction management paradigm.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07951" title="Abstract">arXiv:2310.07951</a> [<a href="/pdf/2310.07951" title="Download PDF">pdf</a>, <a href="/format/2310.07951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive model reduction of high-order solutions of compressible flows  via optimal transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Van+Heyningen%2C+R+L">R. Loek Van Heyningen</a>, 
<a href="/search/math?searchtype=author&query=Nguyen%2C+N+C">Ngoc Cuong Nguyen</a>, 
<a href="/search/math?searchtype=author&query=Blonigan%2C+P">Patrick Blonigan</a>, 
<a href="/search/math?searchtype=author&query=Peraire%2C+J">Jaime Peraire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The solution of conservation laws with parametrized shock waves presents
challenges for both high-order numerical methods and model reduction
techniques. We introduce an r-adaptivity scheme based on optimal transport and
apply it to develop reduced order models for compressible flows. The optimal
transport theory allows us to compute high-order r-adaptive meshes from a
starting reference mesh by solving the Monge-Ampere equation. A high-order
discretization of the conservation laws enables high-order solutions to be
computed on the resulting r-adaptive meshes. Furthermore, the Monge-Ampere
solutions contain mappings that are used to reduce the spatial locality of the
resulting solutions and make them more amenable to model reduction. We use a
non-intrusive model reduction method to construct reduced order models of both
the mesh and the solution. The procedure is demonstrated on three supersonic
and hypersonic test cases, with the hybridizable discontinuous Galerkin method
being used as the full order model.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07955" title="Abstract">arXiv:2310.07955</a> [<a href="/pdf/2310.07955" title="Download PDF">pdf</a>, <a href="/ps/2310.07955" title="Download PostScript">ps</a>, <a href="/format/2310.07955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VEM allowing small edges for the acoustic problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Amigo%2C+D">Danilo Amigo</a>, 
<a href="/search/math?searchtype=author&query=Lepe%2C+F">Felipe Lepe</a>, 
<a href="/search/math?searchtype=author&query=Rivera%2C+G">Gonzalo Rivera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper we propose and analyze a virtual element method to approximate
the natural frequencies of the acoustic eigenvalue problem with polygonal
meshes that allow the presence of small edges. With the aid of a suitable
seminorm that depends on the stabilization of the small edges method, we prove
convergence and error estimates for the eigenfrequencies and eigenfunctions of
the problem, supporting our analysis on the compact operators theory. We report
some numerical tests that allows us to assess the performance of the method and
the accuracy on the approximation.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07957" title="Abstract">arXiv:2310.07957</a> [<a href="/pdf/2310.07957" title="Download PDF">pdf</a>, <a href="/format/2310.07957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Approach Towards Autoformalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patel%2C+N">Nilay Patel</a>, 
<a href="/search/cs?searchtype=author&query=Flanigan%2C+J">Jeffrey Flanigan</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+R">Rahul Saha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review at MATHAI 2023 @ NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Verifying mathematical proofs is difficult, but can be automated with the
assistance of a computer. Autoformalization is the task of automatically
translating natural language mathematics into a formal language that can be
verified by a program. This is a challenging task, and especially for
higher-level mathematics found in research papers. Research paper mathematics
requires large amounts of background and context. In this paper, we propose an
avenue towards tackling autoformalization for research-level mathematics, by
breaking the task into easier and more approachable subtasks: unlinked
formalization (formalization with unlinked definitions and theorems), entity
linking (linking to the proper theorems and definitions), and finally adjusting
types so it passes the type checker. In addition, we present arXiv2Formal, a
benchmark dataset for unlinked formalization consisting of 50 theorems
formalized for the Lean theorem prover sampled from papers on arXiv.org. We
welcome any contributions from the community to future versions of this
dataset.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07958" title="Abstract">arXiv:2310.07958</a> [<a href="/pdf/2310.07958" title="Download PDF">pdf</a>, <a href="/format/2310.07958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Causal Deep Learning for Vulnerability Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+M">Md Mahbubur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Ceka%2C+I">Ira Ceka</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+C">Chengzhi Mao</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Saikat Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+B">Baishakhi Ray</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+W">Wei Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICSE 2024 (not camera-ready version)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">Deep learning vulnerability detection has shown promising results in recent
years. However, an important challenge that still blocks it from being very
useful in practice is that the model is not robust under perturbation and it
cannot generalize well over the out-of-distribution (OOD) data, e.g., applying
a trained model to unseen projects in real world. We hypothesize that this is
because the model learned non-robust features, e.g., variable names, that have
spurious correlations with labels. When the perturbed and OOD datasets no
longer have the same spurious features, the model prediction fails. To address
the challenge, in this paper, we introduced causality into deep learning
vulnerability detection. Our approach CausalVul consists of two phases. First,
we designed novel perturbations to discover spurious features that the model
may use to make predictions. Second, we applied the causal learning algorithms,
specifically, do-calculus, on top of existing deep learning models to
systematically remove the use of spurious features and thus promote causal
based prediction. Our results show that CausalVul consistently improved the
model accuracy, robustness and OOD performance for all the state-of-the-art
models and datasets we experimented. To the best of our knowledge, this is the
first work that introduces do calculus based causal learning to software
engineering models and shows it's indeed useful for improving the model
accuracy, robustness and generalization. Our replication package is located at
https://figshare.com/s/0ffda320dcb96c249ef2.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07962" title="Abstract">arXiv:2310.07962</a> [<a href="/pdf/2310.07962" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustering of Spell Variations for Proper Nouns Transliterated from the  other languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pawar%2C+P">Prathamesh Pawar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, published Airial Conference 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Artificial Intelligence Research in Applied Linguistics (AIRiAL)
  Conference, 18 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">One of the prominent problems with processing and operating on text data is
the non uniformity of it. Due to the change in the dialects and languages, the
caliber of translation is low. This creates a unique problem while using NLP in
text data; which is the spell variation arising from the inconsistent
translations and transliterations. This problem can also be further aggravated
by the human error arising from the various ways to write a Proper Noun from an
Indian language into its English equivalent. Translating proper nouns
originating from Indian languages can be complicated as some proper nouns are
also used as common nouns which might be taken literally. Applications of NLP
that require addresses, names and other proper nouns face this problem
frequently. We propose a method to cluster these spell variations for proper
nouns using ML techniques and mathematical similarity equations. We aimed to
use Affinity Propagation to determine relative similarity between the tokens.
The results are augmented by filtering the token-variation pair by a similarity
threshold. We were able to reduce the spell variations by a considerable
amount. This application can significantly reduce the amount of human
annotation efforts needed for data cleansing and formatting.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07966" title="Abstract">arXiv:2310.07966</a> [<a href="/pdf/2310.07966" title="Download PDF">pdf</a>, <a href="/ps/2310.07966" title="Download PostScript">ps</a>, <a href="/format/2310.07966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Singular Perturbation via Contraction Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cothren%2C+L">Liliaokeawawa Cothren</a>, 
<a href="/search/eess?searchtype=author&query=Bullo%2C+F">Francesco Bullo</a>, 
<a href="/search/eess?searchtype=author&query=Dall%27Anese%2C+E">Emiliano Dall&#x27;Anese</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been submitted to IEEE Transactions on Automatic Control
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we provide a novel contraction-theoretic approach to analyze
two-time scale systems. In our proposed framework, systems enjoy several
robustness properties, which can lead to a more complete characterization of
their behaviors. Key assumptions are the contractivity of the fast sub-system
and of the reduced model, combined with an explicit upper bound on the
time-scale parameter. For two-time scale systems subject to disturbances, we
show that the distance between solutions of the nominal system and solutions of
its reduced model is uniformly upper bounded by a function of contraction
rates, Lipschitz constants, the time-scale parameter, and the time variability
of the disturbances. We also show local contractivity of the two-time scale
system and give sufficient conditions for global contractivity. We then
consider two special cases: for autonomous nonlinear systems we obtain sharper
bounds than our general results and for linear time-invariant systems we
present novel bounds based upon log norms and induced norms. Finally, we apply
our theory to two application areas -- online feedback optimization and
Stackelberg games -- and obtain new individual tracking error bounds showing
that solutions converge to their (time-varying) optimizer and computing overall
contraction rates.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07968" title="Abstract">arXiv:2310.07968</a> [<a href="/pdf/2310.07968" title="Download PDF">pdf</a>, <a href="/format/2310.07968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Think, Act, and Ask: Open-World Interactive Personalized Robot  Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yinpei Dai</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+R">Run Peng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sikai Li</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+J">Joyce Chai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Video available at <a href="https://www.youtube.com/watch?v=QW6rMHVpxUY">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Zero-Shot Object Navigation (ZSON) enables agents to navigate towards
open-vocabulary objects in unknown environments. The existing works of ZSON
mainly focus on following individual instructions to find generic object
classes, neglecting the utilization of natural language interaction and the
complexities of identifying user-specific objects. To address these
limitations, we introduce Zero-shot Interactive Personalized Object Navigation
(ZIPON), where robots need to navigate to personalized goal objects while
engaging in conversations with users. To solve ZIPON, we propose a new
framework termed Open-woRld Interactive persOnalized Navigation (ORION), which
uses Large Language Models (LLMs) to make sequential decisions to manipulate
different modules for perception, navigation and communication. Experimental
results show that the performance of interactive agents that can leverage user
feedback exhibits significant improvement. However, obtaining a good balance
between task completion and the efficiency of navigation and interaction
remains challenging for all methods. We further provide more findings on the
impact of diverse user feedback forms on the agents' performance.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07969" title="Abstract">arXiv:2310.07969</a> [<a href="/pdf/2310.07969" title="Download PDF">pdf</a>, <a href="/format/2310.07969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CleftGAN: Adapting A Style-Based Generative Adversarial Network To  Create Images Depicting Cleft Lip Deformity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hayajneh%2C+A">Abdullah Hayajneh</a>, 
<a href="/search/cs?searchtype=author&query=Serpedin%2C+E">Erchin Serpedin</a>, 
<a href="/search/cs?searchtype=author&query=Shaqfeh%2C+M">Mohammad Shaqfeh</a>, 
<a href="/search/cs?searchtype=author&query=Glass%2C+G">Graeme Glass</a>, 
<a href="/search/cs?searchtype=author&query=Stotland%2C+M+A">Mitchell A. Stotland</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A major obstacle when attempting to train a machine learning system to
evaluate facial clefts is the scarcity of large datasets of high-quality,
ethics board-approved patient images. In response, we have built a deep
learning-based cleft lip generator designed to produce an almost unlimited
number of artificial images exhibiting high-fidelity facsimiles of cleft lip
with wide variation. We undertook a transfer learning protocol testing
different versions of StyleGAN-ADA (a generative adversarial network image
generator incorporating adaptive data augmentation (ADA)) as the base model.
Training images depicting a variety of cleft deformities were pre-processed to
adjust for rotation, scaling, color adjustment and background blurring. The ADA
modification of the primary algorithm permitted construction of our new
generative model while requiring input of a relatively small number of training
images. Adversarial training was carried out using 514 unique frontal
photographs of cleft-affected faces to adapt a pre-trained model based on
70,000 normal faces. The Frechet Inception Distance (FID) was used to measure
the similarity of the newly generated facial images to the cleft training
dataset, while Perceptual Path Length (PPL) and the novel Divergence Index of
Severity Histograms (DISH) measures were also used to assess the performance of
the image generator that we dub CleftGAN. We found that StyleGAN3 with
translation invariance (StyleGAN3-t) performed optimally as a base model.
Generated images achieved a low FID reflecting a close similarity to our
training input dataset of genuine cleft images. Low PPL and DISH measures
reflected a smooth and semantically valid interpolation of images through the
transfer learning process and a similar distribution of severity in the
training and generated images, respectively.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07970" title="Abstract">arXiv:2310.07970</a> [<a href="/pdf/2310.07970" title="Download PDF">pdf</a>, <a href="/format/2310.07970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyperparameter Adaptive Search for Surrogate Optimization: A  Self-Adjusting Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nezami%2C+N">Nazanin Nezami</a>, 
<a href="/search/cs?searchtype=author&query=Anahideh%2C+H">Hadis Anahideh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 Winter Simulation Conference (WSC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Probability (math.PR); Machine Learning (stat.ML)

</div>
<p class="mathjax">Surrogate Optimization (SO) algorithms have shown promise for optimizing
expensive black-box functions. However, their performance is heavily influenced
by hyperparameters related to sampling and surrogate fitting, which poses a
challenge to their widespread adoption. We investigate the impact of
hyperparameters on various SO algorithms and propose a Hyperparameter Adaptive
Search for SO (HASSO) approach. HASSO is not a hyperparameter tuning algorithm,
but a generic self-adjusting SO algorithm that dynamically tunes its own
hyperparameters while concurrently optimizing the primary objective function,
without requiring additional evaluations. The aim is to improve the
accessibility, effectiveness, and convergence speed of SO algorithms for
practitioners. Our approach identifies and modifies the most influential
hyperparameters specific to each problem and SO approach, reducing the need for
manual tuning without significantly increasing the computational burden.
Experimental results demonstrate the effectiveness of HASSO in enhancing the
performance of various SO algorithms across different global optimization test
problems.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07972" title="Abstract">arXiv:2310.07972</a> [<a href="/pdf/2310.07972" title="Download PDF">pdf</a>, <a href="/format/2310.07972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Diffusion via Information Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+X">Xianghao Kong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+O">Ollie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Han Li</a>, 
<a href="/search/cs?searchtype=author&query=Yogatama%2C+D">Dani Yogatama</a>, 
<a href="/search/cs?searchtype=author&query=Steeg%2C+G+V">Greg Ver Steeg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT)

</div>
<p class="mathjax">Denoising diffusion models enable conditional generation and density modeling
of complex relationships like images and text. However, the nature of the
learned relationships is opaque making it difficult to understand precisely
what relationships between words and parts of an image are captured, or to
predict the effect of an intervention. We illuminate the fine-grained
relationships learned by diffusion models by noticing a precise relationship
between diffusion and information decomposition. Exact expressions for mutual
information and conditional mutual information can be written in terms of the
denoising model. Furthermore, pointwise estimates can be easily estimated as
well, allowing us to ask questions about the relationships between specific
images and captions. Decomposing information even further to understand which
variables in a high-dimensional space carry information is a long-standing
problem. For diffusion models, we show that a natural non-negative
decomposition of mutual information emerges, allowing us to quantify
informative relationships between words and pixels in an image. We exploit
these new relations to measure the compositional understanding of diffusion
models, to do unsupervised localization of objects in images, and to measure
effects when selectively editing images through prompt interventions.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07974" title="Abstract">arXiv:2310.07974</a> [<a href="/pdf/2310.07974" title="Download PDF">pdf</a>, <a href="/format/2310.07974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causality-based Cost Allocation for Peer-to-Peer Energy Trading in  Distribution System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kim%2C+H+J">Hyun Joong Kim</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+Y+H">Yong Hyun Song</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+J">Jip Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">While peer-to-peer energy trading has the potential to harness the
capabilities of small-scale energy resources, a peer-matching process often
overlooks power grid conditions, yielding increased losses, line congestion,
and voltage problems. This imposes a great challenge on the distribution system
operator (DSO), which can eventually limit peer-to-peer energy trading. To
align the peer-matching process with the physical grid conditions, this paper
proposes a cost causality-based network cost allocation method and the
grid-aware peer-matching process. Building on the cost causality principle, the
proposed model utilizes the network cost (loss, congestion, and voltage) as a
signal to encourage peers to adjust their preferences ensuring that matches are
more in line with grid conditions, leading to enhanced social welfare.
Additionally, this paper presents mathematical proof showing the superiority of
the causality-based cost allocation over existing methods.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07975" title="Abstract">arXiv:2310.07975</a> [<a href="/pdf/2310.07975" title="Download PDF">pdf</a>, <a href="/format/2310.07975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised visual learning for analyzing firearms trafficking  activities on the Web
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Konstantakos%2C+S">Sotirios Konstantakos</a>, 
<a href="/search/cs?searchtype=author&query=Chalkiadaki%2C+D+I">Despina Ioanna Chalkiadaki</a>, 
<a href="/search/cs?searchtype=author&query=Mademlis%2C+I">Ioannis Mademlis</a>, 
<a href="/search/cs?searchtype=author&query=Chrysochoou%2C+A+A+R">Adamantia Anna Rebolledo Chrysochoou</a>, 
<a href="/search/cs?searchtype=author&query=Papadopoulos%2C+G+T">Georgios Th. Papadopoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Automated visual firearms classification from RGB images is an important
real-world task with applications in public space security, intelligence
gathering and law enforcement investigations. When applied to images massively
crawled from the World Wide Web (including social media and dark Web sites), it
can serve as an important component of systems that attempt to identify
criminal firearms trafficking networks, by analyzing Big Data from open-source
intelligence. Deep Neural Networks (DNN) are the state-of-the-art methodology
for achieving this, with Convolutional Neural Networks (CNN) being typically
employed. The common transfer learning approach consists of pretraining on a
large-scale, generic annotated dataset for whole-image classification, such as
ImageNet-1k, and then finetuning the DNN on a smaller, annotated,
task-specific, downstream dataset for visual firearms classification. Neither
Visual Transformer (ViT) neural architectures nor Self-Supervised Learning
(SSL) approaches have been so far evaluated on this critical task. SSL
essentially consists of replacing the traditional supervised pretraining
objective with an unsupervised pretext task that does not require ground-truth
labels..
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07977" title="Abstract">arXiv:2310.07977</a> [<a href="/pdf/2310.07977" title="Download PDF">pdf</a>, <a href="/ps/2310.07977" title="Download PostScript">ps</a>, <a href="/format/2310.07977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simultaneous Auctions are Approximately Revenue-Optimal for Subadditive  Bidders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziyun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jinzhao Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at FOCS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study revenue maximization in multi-item auctions, where bidders have
subadditive valuations over independent items. Providing a simple mechanism
that is approximately revenue-optimal in this setting is a major open problem
in mechanism design. In this paper, we present the first \emph{simple
mechanism} whose revenue is at least a \emph{constant fraction} of the optimal
revenue in multi-item auctions with subadditive bidders. Our mechanism is a
simultaneous auction that incorporates either a personalized entry fee or a
personalized reserve price per item. We prove that for any simultaneous auction
that satisfies c-efficiency -- a new property we propose, its revenue is at
least an $O(c)$-approximation to the optimal revenue. We further show that both
the \emph{simultaneous first-price} and the \emph{simultaneous all-pay auction}
are $1\over 2$-efficient. Providing revenue guarantees for non-truthful simple
mechanisms, e.g., simultaneous auctions, in multi-dimensional environments has
been recognized by Roughgarden et al. as an important open question. Prior to
our result, the only such revenue guarantees are due to Daskalakis et al. for
bidders who have additive valuations over independent items. Our result
significantly extends the revenue guarantees of these non-truthful simple
auctions to settings where bidders have combinatorial valuations.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07979" title="Abstract">arXiv:2310.07979</a> [<a href="/pdf/2310.07979" title="Download PDF">pdf</a>, <a href="/format/2310.07979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-SCP: Accelerating Set Cover Problems with Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shafi%2C+Z">Zohair Shafi</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+B+A">Benjamin A. Miller</a>, 
<a href="/search/cs?searchtype=author&query=Eliassi-Rad%2C+T">Tina Eliassi-Rad</a>, 
<a href="/search/cs?searchtype=author&query=Caceres%2C+R+S">Rajmonda S. Caceres</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Machine learning (ML) approaches are increasingly being used to accelerate
combinatorial optimization (CO) problems. We look specifically at the Set Cover
Problem (SCP) and propose Graph-SCP, a graph neural network method that can
augment existing optimization solvers by learning to identify a much smaller
sub-problem that contains the solution space. We evaluate the performance of
Graph-SCP on synthetic weighted and unweighted SCP instances with diverse
problem characteristics and complexities, and on instances from the OR Library,
a canonical benchmark for SCP. We show that Graph-SCP reduces the problem size
by 30-70% and achieves run time speedups up to~25x when compared to commercial
solvers (Gurobi). Given a desired optimality threshold, Graph-SCP will improve
upon it or even achieve 100% optimality. This is in contrast to fast greedy
solutions that significantly compromise solution quality to achieve guaranteed
polynomial run time. Graph-SCP can generalize to larger problem sizes and can
be used with other conventional or ML-augmented CO solvers to lead to potential
additional run time improvement.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07980" title="Abstract">arXiv:2310.07980</a> [<a href="/pdf/2310.07980" title="Download PDF">pdf</a>, <a href="/format/2310.07980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRASP: Accelerating Shortest Path Attacks via Graph Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miller%2C+Z+S+B+A">Zohair Shafi. Benjamin A. Miller</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+A">Ayan Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Eliassi-Rad%2C+T">Tina Eliassi-Rad</a>, 
<a href="/search/cs?searchtype=author&query=Caceres%2C+R+S">Rajmonda S. Caceres</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent advances in machine learning (ML) have shown promise in aiding and
accelerating classical combinatorial optimization algorithms. ML-based speed
ups that aim to learn in an end to end manner (i.e., directly output the
solution) tend to trade off run time with solution quality. Therefore,
solutions that are able to accelerate existing solvers while maintaining their
performance guarantees, are of great interest. We consider an APX-hard problem,
where an adversary aims to attack shortest paths in a graph by removing the
minimum number of edges. We propose the GRASP algorithm: Graph Attention
Accelerated Shortest Path Attack, an ML aided optimization algorithm that
achieves run times up to 10x faster, while maintaining the quality of solution
generated. GRASP uses a graph attention network to identify a smaller subgraph
containing the combinatorial solution, thus effectively reducing the input
problem size. Additionally, we demonstrate how careful representation of the
input graph, including node features that correlate well with the optimization
task, can highlight important structure in the optimization solution.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07981" title="Abstract">arXiv:2310.07981</a> [<a href="/pdf/2310.07981" title="Download PDF">pdf</a>, <a href="/format/2310.07981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning of Display Transfer Robots in Glass Flow Control  Systems: A Physical Simulation-Based Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hwajong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C">Chan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seong-Woo Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO); Systems and Control (eess.SY)

</div>
<p class="mathjax">A flow control system is a critical concept for increasing the production
capacity of manufacturing systems. To solve the scheduling optimization problem
related to the flow control with the aim of improving productivity, existing
methods depend on a heuristic design by domain human experts. Therefore, the
methods require correction, monitoring, and verification by using real
equipment. As system designs increase in complexity, the monitoring time
increases, which decreases the probability of arriving at the optimal design.
As an alternative approach to the heuristic design of flow control systems, the
use of deep reinforcement learning to solve the scheduling optimization problem
has been considered. Although the existing research on reinforcement learning
has yielded excellent performance in some areas, the applicability of the
results to actual FAB such as display and semiconductor manufacturing processes
is not evident so far. To this end, we propose a method to implement a physical
simulation environment and devise a feasible flow control system design using a
transfer robot in display manufacturing through reinforcement learning. We
present a model and parameter setting to build a virtual environment for
different display transfer robots, and training methods of reinforcement
learning on the environment to obtain an optimal scheduling of glass flow
control systems. Its feasibility was verified by using different types of
robots used in the actual process.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07983" title="Abstract">arXiv:2310.07983</a> [<a href="/pdf/2310.07983" title="Download PDF">pdf</a>, <a href="/format/2310.07983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RandCom: Random Communication Skipping Method for Decentralized  Stochastic Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Luyao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Alghunaim%2C+S+A">Sulaiman A. Alghunaim</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+K">Kun Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Condat%2C+L">Laurent Condat</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jinde Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Distributed optimization methods with random communication skips are gaining
increasing attention due to their proven benefits in accelerating communication
complexity. Nevertheless, existing research mainly focuses on centralized
communication protocols for strongly convex deterministic settings. In this
work, we provide a decentralized optimization method called RandCom, which
incorporates probabilistic local updates. We analyze the performance of RandCom
in stochastic non-convex, convex, and strongly convex settings and demonstrate
its ability to asymptotically reduce communication overhead by the probability
of communication. Additionally, we prove that RandCom achieves linear speedup
as the number of nodes increases. In stochastic strongly convex settings, we
further prove that RandCom can achieve linear speedup with network-independent
stepsizes. Moreover, we apply RandCom to federated learning and provide
positive results concerning the potential for achieving linear speedup and the
suitability of the probabilistic local update approach for non-convex settings.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07984" title="Abstract">arXiv:2310.07984</a> [<a href="/pdf/2310.07984" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Scientific Synthesis, Inference and  Explanation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yizhen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Koh%2C+H+Y">Huan Yee Koh</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+J">Jiaxin Ju</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A+T+N">Anh T.N. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=May%2C+L+T">Lauren T. May</a>, 
<a href="/search/cs?searchtype=author&query=Webb%2C+G+I">Geoffrey I. Webb</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Supplementary Information: <a href="https://drive.google.com/file/d/1KrpUpzuFTeMx6a6zl18lqdo8vV-UUa1Z/view?usp=sharing">this https URL</a> Github Repo: <a href="https://github.com/zyzisastudyreallyhardguy/LLM4SD">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Large language models are a form of artificial intelligence systems whose
primary knowledge consists of the statistical patterns, semantic relationships,
and syntactical structures of language1. Despite their limited forms of
"knowledge", these systems are adept at numerous complex tasks including
creative writing, storytelling, translation, question-answering, summarization,
and computer code generation. However, they have yet to demonstrate advanced
applications in natural science. Here we show how large language models can
perform scientific synthesis, inference, and explanation. We present a method
for using general-purpose large language models to make inferences from
scientific datasets of the form usually associated with special-purpose machine
learning algorithms. We show that the large language model can augment this
"knowledge" by synthesizing from the scientific literature. When a conventional
machine learning system is augmented with this synthesized and inferred
knowledge it can outperform the current state of the art across a range of
benchmark tasks for predicting molecular properties. This approach has the
further advantage that the large language model can explain the machine
learning system's predictions. We anticipate that our framework will open new
avenues for AI to accelerate the pace of scientific discovery.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07985" title="Abstract">arXiv:2310.07985</a> [<a href="/pdf/2310.07985" title="Download PDF">pdf</a>, <a href="/format/2310.07985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Fu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingfu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenkun Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Optimization and Control (math.OC)

</div>
<p class="mathjax">Neural combinatorial optimization (NCO) is a promising learning-based
approach for solving challenging combinatorial optimization problems without
specialized algorithm design by experts. However, most constructive NCO methods
cannot solve problems with large-scale instance sizes, which significantly
diminishes their usefulness for real-world applications. In this work, we
propose a novel Light Encoder and Heavy Decoder (LEHD) model with a strong
generalization ability to address this critical issue. The LEHD model can learn
to dynamically capture the relationships between all available nodes of varying
sizes, which is beneficial for model generalization to problems of various
scales. Moreover, we develop a data-efficient training scheme and a flexible
solution construction mechanism for the proposed LEHD model. By training on
small-scale problem instances, the LEHD model can generate nearly optimal
solutions for the Travelling Salesman Problem (TSP) and the Capacitated Vehicle
Routing Problem (CVRP) with up to 1000 nodes, and also generalizes well to
solve real-world TSPLib and CVRPLib problems. These results confirm our
proposed LEHD model can significantly improve the state-of-the-art performance
for constructive NCO. The code is available at
https://github.com/CIAM-Group/NCO_code/tree/main/single_objective/LEHD.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07987" title="Abstract">arXiv:2310.07987</a> [<a href="/pdf/2310.07987" title="Download PDF">pdf</a>, <a href="/format/2310.07987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic-Forward Relaying: A Novel Framework Towards 6G Cooperative  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Wensheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yuna Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lixin Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhu Han</a>, 
<a href="/search/cs?searchtype=author&query=Matsumoto%2C+T">Tad Matsumoto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">This letter proposes a novel relaying framework, semantic-forward (SF), for
cooperative communications towards the sixth-generation (6G) wireless networks.
The SF relay extracts and transmits the semantic features, which reduces
forwarding payload, and also improves the network robustness against intra-link
errors. Based on the theoretical basis for cooperative communications with side
information and the turbo principle, we design a joint source-channel coding
algorithm to iteratively exchange the extrinsic information for enhancing the
decoding gains at the destination. Surprisingly, simulation results indicate
that even in bad channel conditions, SF relaying can still effectively improve
the recovered information quality.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07991" title="Abstract">arXiv:2310.07991</a> [<a href="/pdf/2310.07991" title="Download PDF">pdf</a>, <a href="/format/2310.07991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting and Fixing Violations of Modification Terms in Open Source  Licenses during Forking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaifeng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yingfeng Xia</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bihuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhuotong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xin Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Open source software brings benefit to software community, but also
introduces legal risks caused by license violations, which result in serious
consequences such as lawsuits and financial losses. To mitigate legal risks,
some approaches have been proposed to identify licenses, detect license
incompatibilities and inconsistencies, and recommend licenses. As far as we
know, however, there is no prior work to understand modification terms in open
source licenses or to detect and fix violations of modification terms. To
bridge this gap, we first empirically characterize modification terms in 47
open source licenses. These licenses all require certain forms of "notice" to
describe the modifications made to the original work. Inspired by our study, we
then design LiVo to automatically detect and fix violations of modification
terms in open source licenses during forking. Our evaluation has shown the
effectiveness and efficiency of LiVo. 18 pull requests of fixing modification
term violations have received positive responses. 8 have been merged.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07995" title="Abstract">arXiv:2310.07995</a> [<a href="/pdf/2310.07995" title="Download PDF">pdf</a>, <a href="/format/2310.07995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HeightFormer: A Multilevel Interaction and Image-adaptive  Classification-regression Network for Monocular Height Estimation with Aerial  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yidan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiyu Qi</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yongqiang Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+L">Lulu Niu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yunping Ge</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Height estimation has long been a pivotal topic within measurement and remote
sensing disciplines, proving critical for endeavours such as 3D urban
modelling, MR and autonomous driving. Traditional methods utilise stereo
matching or multisensor fusion, both well-established techniques that typically
necessitate multiple images from varying perspectives and adjunct sensors like
SAR, leading to substantial deployment costs. Single image height estimation
has emerged as an attractive alternative, boasting a larger data source variety
and simpler deployment. However, current methods suffer from limitations such
as fixed receptive fields, a lack of global information interaction, leading to
noticeable instance-level height deviations. The inherent complexity of height
prediction can result in a blurry estimation of object edge depth when using
mainstream regression methods based on fixed height division. This paper
presents a comprehensive solution for monocular height estimation in remote
sensing, termed HeightFormer, combining multilevel interactions and
image-adaptive classification-regression. It features the Multilevel
Interaction Backbone (MIB) and Image-adaptive Classification-regression Height
Generator (ICG). MIB supplements the fixed sample grid in CNN of the
conventional backbone network with tokens of different interaction ranges. It
is complemented by a pixel-, patch-, and feature map-level hierarchical
interaction mechanism, designed to relay spatial geometry information across
different scales and introducing a global receptive field to enhance the
quality of instance-level height estimation. The ICG dynamically generates
height partition for each image and reframes the traditional regression task,
using a refinement from coarse to fine classification-regression that
significantly mitigates the innate ill-posedness issue and drastically improves
edge sharpness.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07996" title="Abstract">arXiv:2310.07996</a> [<a href="/pdf/2310.07996" title="Download PDF">pdf</a>, <a href="/format/2310.07996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reset It and Forget It: Relearning Last-Layer Weights Improves Continual  and Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frati%2C+L">Lapo Frati</a>, 
<a href="/search/cs?searchtype=author&query=Traft%2C+N">Neil Traft</a>, 
<a href="/search/cs?searchtype=author&query=Clune%2C+J">Jeff Clune</a>, 
<a href="/search/cs?searchtype=author&query=Cheney%2C+N">Nick Cheney</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">This work identifies a simple pre-training mechanism that leads to
representations exhibiting better continual and transfer learning. This
mechanism -- the repeated resetting of weights in the last layer, which we
nickname "zapping" -- was originally designed for a meta-continual-learning
procedure, yet we show it is surprisingly applicable in many settings beyond
both meta-learning and continual learning. In our experiments, we wish to
transfer a pre-trained image classifier to a new set of classes, in a few
shots. We show that our zapping procedure results in improved transfer accuracy
and/or more rapid adaptation in both standard fine-tuning and continual
learning settings, while being simple to implement and computationally
efficient. In many cases, we achieve performance on par with state of the art
meta-learning without needing the expensive higher-order gradients, by using a
combination of zapping and sequential learning. An intuitive explanation for
the effectiveness of this zapping procedure is that representations trained
with repeated zapping learn features that are capable of rapidly adapting to
newly initialized classifiers. Such an approach may be considered a
computationally cheaper type of, or alternative to, meta-learning rapidly
adaptable features with higher-order gradients. This adds to recent work on the
usefulness of resetting neural network parameters during training, and invites
further investigation of this mechanism.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07997" title="Abstract">arXiv:2310.07997</a> [<a href="/pdf/2310.07997" title="Download PDF">pdf</a>, <a href="/format/2310.07997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Point-NeuS: Point-Guided Neural Implicit Surface Reconstruction by  Volume Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+W">Wanjuan Su</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+W">Wenbing Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recently, learning neural implicit surface by volume rendering has been a
promising way for multi-view reconstruction. However, limited accuracy and
excessive time complexity remain bottlenecks that current methods urgently need
to overcome. To address these challenges, we propose a new method called
Point-NeuS, utilizing point-guided mechanisms to achieve accurate and efficient
reconstruction. Point modeling is organically embedded into the volume
rendering to enhance and regularize the representation of implicit surface.
Specifically, to achieve precise point guidance and noise robustness, aleatoric
uncertainty of the point cloud is modeled to capture the distribution of noise
and estimate the reliability of points. Additionally, a Neural Projection
module connecting points and images is introduced to add geometric constraints
to the Signed Distance Function (SDF). To better compensate for geometric bias
between volume rendering and point modeling, high-fidelity points are filtered
into an Implicit Displacement Network to improve the representation of SDF.
Benefiting from our effective point guidance, lightweight networks are employed
to achieve an impressive 11x speedup compared to NeuS. Extensive experiments
show that our method yields high-quality surfaces, especially for fine-grained
details and smooth regions. Moreover, it exhibits strong robustness to both
noisy and sparse data.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07998" title="Abstract">arXiv:2310.07998</a> [<a href="/pdf/2310.07998" title="Download PDF">pdf</a>, <a href="/format/2310.07998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Statistical Measure for Out-of-Distribution Detection in Data  Quality Assurance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+T">Tinghui Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Echizen%2C+I">Isao Echizen</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+Y">Yoshiki Seo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Data outside the problem domain poses significant threats to the security of
AI-based intelligent systems. Aiming to investigate the data domain and
out-of-distribution (OOD) data in AI quality management (AIQM) study, this
paper proposes to use deep learning techniques for feature representation and
develop a novel statistical measure for OOD detection. First, to extract
low-dimensional representative features distinguishing normal and OOD data, the
proposed research combines the deep auto-encoder (AE) architecture and neuron
activation status for feature engineering. Then, using local conditional
probability (LCP) in data reconstruction, a novel and superior statistical
measure is developed to calculate the score of OOD detection. Experiments and
evaluations are conducted on image benchmark datasets and an industrial
dataset. Through comparative analysis with other common statistical measures in
OOD detection, the proposed research is validated as feasible and effective in
OOD and AIQM studies.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07999" title="Abstract">arXiv:2310.07999</a> [<a href="/pdf/2310.07999" title="Download PDF">pdf</a>, <a href="/format/2310.07999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEMON: Lossless model expansion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yite Wang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jiahao Su</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hanlin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Cong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jianbo Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haibin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruoyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Scaling of deep neural networks, especially Transformers, is pivotal for
their surging performance and has further led to the emergence of sophisticated
reasoning capabilities in foundation models. Such scaling generally requires
training large models from scratch with random initialization, failing to
leverage the knowledge acquired by their smaller counterparts, which are
already resource-intensive to obtain. To tackle this inefficiency, we present
$\textbf{L}$ossl$\textbf{E}$ss $\textbf{MO}$del Expansio$\textbf{N}$ (LEMON), a
recipe to initialize scaled models using the weights of their smaller but
pre-trained counterparts. This is followed by model training with an optimized
learning rate scheduler tailored explicitly for the scaled models,
substantially reducing the training time compared to training from scratch.
Notably, LEMON is versatile, ensuring compatibility with various network
structures, including models like Vision Transformers and BERT. Our empirical
results demonstrate that LEMON reduces computational costs by 56.7% for Vision
Transformers and 33.2% for BERT when compared to training from scratch.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08004" title="Abstract">arXiv:2310.08004</a> [<a href="/pdf/2310.08004" title="Download PDF">pdf</a>, <a href="/format/2310.08004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Rational Degree of Boolean Functions and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iyer%2C+V">Vishnu Iyer</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Siddhartha Jain</a>, 
<a href="/search/cs?searchtype=author&query=Kovacs-Deak%2C+M">Matt Kovacs-Deak</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V+M">Vinayak M. Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Schaeffer%2C+L">Luke Schaeffer</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Daochen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Whitmeyer%2C+M">Michael Whitmeyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">We study a natural complexity measure of Boolean functions known as the
(exact) rational degree. For total functions $f$, it is conjectured that
$\mathrm{rdeg}(f)$ is polynomially related to $\mathrm{deg}(f)$, where
$\mathrm{deg}(f)$ is the Fourier degree. Towards this conjecture, we show that
symmetric functions have rational degree at least $\mathrm{deg}(f)/2$ and
monotone functions have rational degree at least $\sqrt{\mathrm{deg}(f)}$. We
observe that both of these lower bounds are tight. In addition, we show that
all read-once depth-$d$ Boolean formulae have rational degree at least
$\Omega(\mathrm{deg}(f)^{1/d})$. Furthermore, we show that almost every Boolean
function on $n$ variables has rational degree at least $n/2 - O(\sqrt{n})$.
<br />In contrast to total functions, we exhibit partial functions that witness
unbounded separations between rational and approximate degree, in both
directions. As a consequence, we show that for quantum computers,
post-selection and bounded-error are incomparable resources in the black-box
model.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08006" title="Abstract">arXiv:2310.08006</a> [<a href="/pdf/2310.08006" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MCPNS: A Macropixel Collocated Position and Its Neighbors Search for  Plenoptic 2.0 Video Coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Duong%2C+V">Vinh Van Duong</a>, 
<a href="/search/cs?searchtype=author&query=Huu%2C+T+N">Thuc Nguyen Huu</a>, 
<a href="/search/cs?searchtype=author&query=Yim%2C+J">Jonghoon Yim</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+B">Byeungwoo Jeon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
<p class="mathjax">Recently, it was demonstrated that a newly focused plenoptic 2.0 camera can
capture much higher spatial resolution owing to its effective light field
sampling, as compared to a traditional unfocused plenoptic 1.0 camera. However,
due to the nature difference of the optical structure between the plenoptic 1.0
and 2.0 cameras, the existing fast motion estimation (ME) method for plenoptic
1.0 videos is expected to be sub-optimal for encoding plenoptic 2.0 videos. In
this paper, we point out the main motion characteristic differences between
plenoptic 1.0 and 2.0 videos and then propose a new fast ME, called macropixel
collocated position and its neighbors search (MCPNS) for plenoptic 2.0 videos.
In detail, we propose to reduce the number of macropixel collocated position
(MCP) search candidates based on the new observation of center-biased motion
vector distribution at macropixel resolution. After that, due to large motion
deviation behavior around each MCP location in plenoptic 2.0 videos, we propose
to select a certain number of key MCP locations with the lowest matching cost
to perform the neighbors MCP search to improve the motion search accuracy.
Different from existing methods, our method can achieve better performance
without requiring prior knowledge of microlens array orientations. Our
simulation results confirmed the effectiveness of the proposed algorithm in
terms of both bitrate savings and computational costs compared to existing
methods.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08008" title="Abstract">arXiv:2310.08008</a> [<a href="/pdf/2310.08008" title="Download PDF">pdf</a>, <a href="/format/2310.08008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effects of Human Adversarial and Affable Samples on BERT  Generalizability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elangovan%2C+A">Aparna Elangovan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiayuan He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Verspoor%2C+K">Karin Verspoor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">BERT-based models have had strong performance on leaderboards, yet have been
demonstrably worse in real-world settings requiring generalization. Limited
quantities of training data is considered a key impediment to achieving
generalizability in machine learning. In this paper, we examine the impact of
training \textit{data quality}, not quantity, on a model's generalizability. We
consider two characteristics of training data: the portion of human-adversarial
(h-adversarial), i.e., sample pairs with seemingly minor differences but
different ground-truth labels, and human-affable (h-affable) training samples,
i.e., sample pairs with minor differences but the same ground-truth label. We
find that for a fixed size of training samples, as a rule of thumb, having
10-30\% h-adversarial instances improves the precision, and therefore F1, by up
to 20 points in the tasks of text classification and relation extraction.
Increasing h-adversarials beyond this range can result in performance plateaus
or even degradation.In contrast, h-affables may not contribute to a model's
generalizability and may even degrade generalization performance.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08009" title="Abstract">arXiv:2310.08009</a> [<a href="/pdf/2310.08009" title="Download PDF">pdf</a>, <a href="/format/2310.08009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual-Stream Knowledge-Preserving Hashing for Unsupervised Video  Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pandeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Hongtao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+J">Jiannan Ge</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+S">Shaobo Min</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongdong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 8 figures, ECCV 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Unsupervised video hashing usually optimizes binary codes by learning to
reconstruct input videos. Such reconstruction constraint spends much effort on
frame-level temporal context changes without focusing on video-level global
semantics that are more useful for retrieval. Hence, we address this problem by
decomposing video information into reconstruction-dependent and
semantic-dependent information, which disentangles the semantic extraction from
reconstruction constraint. Specifically, we first design a simple dual-stream
structure, including a temporal layer and a hash layer. Then, with the help of
semantic similarity knowledge obtained from self-supervision, the hash layer
learns to capture information for semantic retrieval, while the temporal layer
learns to capture the information for reconstruction. In this way, the model
naturally preserves the disentangled semantics into binary codes. Validated by
comprehensive experiments, our method consistently outperforms the
state-of-the-arts on three video benchmarks.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08011" title="Abstract">arXiv:2310.08011</a> [<a href="/pdf/2310.08011" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Study of Rarely Appeared Instructions in an Executable Binary
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Murodova%2C+N">Nozima Murodova</a>, 
<a href="/search/cs?searchtype=author&query=Koo%2C+H">Hyungjoon Koo</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> https://www.manuscriptlink.com/society/kiisc/conference/ciscw2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">An executable binary typically contains a large number of machine
instructions. Although the statistics of popular instructions is well known,
the distribution of non-popular instructions has been relatively under
explored. Our finding shows that an arbitrary group of binaries com es with
both i) a similar distribution of common machine instructions, and ii) quite a
few rarely appeared instructions (e.g., less than five occurrences) apart from
the distribution. Their infrequency may represent the signature of a code chunk
or the footprint of a binary. In this work, we investigate such rare
instructions with an in-depth analysis at the source level, clas sifying them
into four categories.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08012" title="Abstract">arXiv:2310.08012</a> [<a href="/pdf/2310.08012" title="Download PDF">pdf</a>, <a href="/format/2310.08012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoFHE: Automated Adaption of CNNs for Efficient Evaluation over FHE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ao%2C+W">Wei Ao</a>, 
<a href="/search/cs?searchtype=author&query=Boddeti%2C+V+N">Vishnu Naresh Boddeti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> USENIX Security Symposium 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Secure inference of deep convolutional neural networks (CNNs) under RNS-CKKS
involves polynomial approximation of unsupported non-linear activation
functions. However, existing approaches have three main limitations: 1)
Inflexibility: The polynomial approximation and associated homomorphic
evaluation architecture are customized manually for each CNN architecture and
do not generalize to other networks. 2) Suboptimal Approximation: Each
activation function is approximated instead of the function represented by the
CNN. 3) Restricted Design: Either high-degree or low-degree polynomial
approximations are used. The former retains high accuracy but slows down
inference due to bootstrapping operations, while the latter accelerates
ciphertext inference but compromises accuracy. To address these limitations, we
present AutoFHE, which automatically adapts standard CNNs for secure inference
under RNS-CKKS. The key idea is to adopt layerwise mixed-degree polynomial
activation functions, which are optimized jointly with the homomorphic
evaluation architecture in terms of the placement of bootstrapping operations.
The problem is modeled within a multi-objective optimization framework to
maximize accuracy and minimize the number of bootstrapping operations. AutoFHE
can be applied flexibly on any CNN architecture, and it provides diverse
solutions that span the trade-off between accuracy and latency. Experimental
evaluation over RNS-CKKS encrypted CIFAR datasets shows that AutoFHE
accelerates secure inference by $1.32\times$ to $1.8\times$ compared to methods
employing high-degree polynomials. It also improves accuracy by up to 2.56%
compared to methods using low-degree polynomials. Lastly, AutoFHE accelerates
inference and improves accuracy by $103\times$ and 3.46%, respectively,
compared to CNNs under TFHE.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08015" title="Abstract">arXiv:2310.08015</a> [<a href="/pdf/2310.08015" title="Download PDF">pdf</a>, <a href="/format/2310.08015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why Train More? Effective and Efficient Membership Inference via  Memorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jihye Choi</a>, 
<a href="/search/cs?searchtype=author&query=Tople%2C+S">Shruti Tople</a>, 
<a href="/search/cs?searchtype=author&query=Chandrasekaran%2C+V">Varun Chandrasekaran</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Somesh Jha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Membership Inference Attacks (MIAs) aim to identify specific data samples
within the private training dataset of machine learning models, leading to
serious privacy violations and other sophisticated threats. Many practical
black-box MIAs require query access to the data distribution (the same
distribution where the private data is drawn) to train shadow models. By doing
so, the adversary obtains models trained "with" or "without" samples drawn from
the distribution, and analyzes the characteristics of the samples under
consideration. The adversary is often required to train more than hundreds of
shadow models to extract the signals needed for MIAs; this becomes the
computational overhead of MIAs. In this paper, we propose that by strategically
choosing the samples, MI adversaries can maximize their attack success while
minimizing the number of shadow models. First, our motivational experiments
suggest memorization as the key property explaining disparate sample
vulnerability to MIAs. We formalize this through a theoretical bound that
connects MI advantage with memorization. Second, we show sample complexity
bounds that connect the number of shadow models needed for MIAs with
memorization. Lastly, we confirm our theoretical arguments with comprehensive
experiments; by utilizing samples with high memorization scores, the adversary
can (a) significantly improve its efficacy regardless of the MIA used, and (b)
reduce the number of shadow models by nearly two orders of magnitude compared
to state-of-the-art approaches.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08017" title="Abstract">arXiv:2310.08017</a> [<a href="/pdf/2310.08017" title="Download PDF">pdf</a>, <a href="/format/2310.08017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing Large Language Models&#x27; Empathetic Response Generation  Capabilities for Online Mental Health Counselling Support
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Loh%2C+S+B">Siyuan Brandon Loh</a>, 
<a href="/search/cs?searchtype=author&query=Raamkumar%2C+A+S">Aravind Sesagiri Raamkumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated remarkable performance across
various information-seeking and reasoning tasks. These computational systems
drive state-of-the-art dialogue systems, such as ChatGPT and Bard. They also
carry substantial promise in meeting the growing demands of mental health care,
albeit relatively unexplored. As such, this study sought to examine LLMs'
capability to generate empathetic responses in conversations that emulate those
in a mental health counselling setting. We selected five LLMs: version 3.5 and
version 4 of the Generative Pre-training (GPT), Vicuna FastChat-T5, Pathways
Language Model (PaLM) version 2, and Falcon-7B-Instruct. Based on a simple
instructional prompt, these models responded to utterances derived from the
EmpatheticDialogues (ED) dataset. Using three empathy-related metrics, we
compared their responses to those from traditional response generation dialogue
systems, which were fine-tuned on the ED dataset, along with human-generated
responses. Notably, we discovered that responses from the LLMs were remarkably
more empathetic in most scenarios. We position our findings in light of
catapulting advancements in creating empathetic conversational systems.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08019" title="Abstract">arXiv:2310.08019</a> [<a href="/pdf/2310.08019" title="Download PDF">pdf</a>, <a href="/ps/2310.08019" title="Download PostScript">ps</a>, <a href="/format/2310.08019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust 1-bit Compressed Sensing with Iterative Hard Thresholding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Matsumoto%2C+N">Namiko Matsumoto</a>, 
<a href="/search/cs?searchtype=author&query=Mazumdar%2C+A">Arya Mazumdar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to appear in ACM-SIAM Symposium on Discrete Algorithms (SODA) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Signal Processing (eess.SP); Machine Learning (stat.ML)

</div>
<p class="mathjax">In 1-bit compressed sensing, the aim is to estimate a $k$-sparse unit vector
$x\in S^{n-1}$ within an $\epsilon$ error (in $\ell_2$) from minimal number of
linear measurements that are quantized to just their signs, i.e., from
measurements of the form $y = \mathrm{Sign}(\langle a, x\rangle).$ In this
paper, we study a noisy version where a fraction of the measurements can be
flipped, potentially by an adversary. In particular, we analyze the Binary
Iterative Hard Thresholding (BIHT) algorithm, a proximal gradient descent on a
properly defined loss function used for 1-bit compressed sensing, in this noisy
setting. It is known from recent results that, with
$\tilde{O}(\frac{k}{\epsilon})$ noiseless measurements, BIHT provides an
estimate within $\epsilon$ error. This result is optimal and universal, meaning
one set of measurements work for all sparse vectors. In this paper, we show
that BIHT also provides better results than all known methods for the noisy
setting. We show that when up to $\tau$-fraction of the sign measurements are
incorrect (adversarial error), with the same number of measurements as before,
BIHT agnostically provides an estimate of $x$ within an
$\tilde{O}(\epsilon+\tau)$ error, maintaining the universality of measurements.
This establishes stability of iterative hard thresholding in the presence of
measurement error. To obtain the result, we use the restricted approximate
invertibility of Gaussian matrices, as well as a tight analysis of the
high-dimensional geometry of the adversarially corrupted measurements.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08025" title="Abstract">arXiv:2310.08025</a> [<a href="/pdf/2310.08025" title="Download PDF">pdf</a>, <a href="/format/2310.08025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visualizing Why Nondeterministic Finite-State Automata Reject
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kempinski%2C+O">Oliwia Kempinski</a>, 
<a href="/search/cs?searchtype=author&query=Moraz%C3%A1n%2C+M+T">Marco T. Moraz&#xe1;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at The 2023 Scheme and Functional Programming Workshop (<a href="/abs/cs/0101200">arXiv:cs/0101200</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Students find their first course in Formal Languages and Automata Theory
challenging. In addition to the development of formal arguments, most students
struggle to understand nondeterministic computation models. In part, the
struggle stems from the course exposing them for the first time to
nondeterminism. Often, students find it difficult to understand why a
nondeterministic machine accepts or rejects a word. Furthermore, they may feel
uncomfortable with there being multiple computations on the same input and with
a machine not consuming all of its input. This article describes a
visualization tool developed to help students understand nondeterministic
behavior. The tool is integrated into, FSM, a domain-specific language for the
Automata Theory classroom. The strategy is based on the automatic generation of
computation graphs given a machine and an input word. Unlike previous
visualization tools, the computation graphs generated reflect the structure of
the given machine's transition relation and not the structure of the
computation tree.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08026" title="Abstract">arXiv:2310.08026</a> [<a href="/pdf/2310.08026" title="Download PDF">pdf</a>, <a href="/format/2310.08026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Sharing Weights in Decoupling Feature Learning Network for UAV  RGB-Infrared Vehicle Re-Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xingyue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Jiahao Qi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bin%2C+K">Kangcheng Bin</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+P">Ping Zhong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures, 64 citations, submitted to TMM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Owing to the capacity of performing full-time target search, cross-modality
vehicle re-identification (Re-ID) based on unmanned aerial vehicle (UAV) is
gaining more attention in both video surveillance and public security. However,
this promising and innovative research has not been studied sufficiently due to
the data inadequacy issue. Meanwhile, the cross-modality discrepancy and
orientation discrepancy challenges further aggravate the difficulty of this
task. To this end, we pioneer a cross-modality vehicle Re-ID benchmark named
UAV Cross-Modality Vehicle Re-ID (UCM-VeID), containing 753 identities with
16015 RGB and 13913 infrared images. Moreover, to meet cross-modality
discrepancy and orientation discrepancy challenges, we present a hybrid weights
decoupling network (HWDNet) to learn the shared discriminative
orientation-invariant features. For the first challenge, we proposed a hybrid
weights siamese network with a well-designed weight restrainer and its
corresponding objective function to learn both modality-specific and modality
shared information. In terms of the second challenge, three effective
decoupling structures with two pretext tasks are investigated to learn
orientation-invariant feature. Comprehensive experiments are carried out to
validate the effectiveness of the proposed method. The dataset and codes will
be released at https://github.com/moonstarL/UAV-CM-VeID.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08027" title="Abstract">arXiv:2310.08027</a> [<a href="/pdf/2310.08027" title="Download PDF">pdf</a>, <a href="/format/2310.08027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Large Language Models for Multi-Modal Out-of-Distribution  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yi Dai</a>, 
<a href="/search/cs?searchtype=author&query=Lang%2C+H">Hao Lang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+K">Kaisheng Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP2023 Findings Long Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Out-of-distribution (OOD) detection is essential for reliable and trustworthy
machine learning. Recent multi-modal OOD detection leverages textual
information from in-distribution (ID) class names for visual OOD detection, yet
it currently neglects the rich contextual information of ID classes. Large
language models (LLMs) encode a wealth of world knowledge and can be prompted
to generate descriptive features for each class. Indiscriminately using such
knowledge causes catastrophic damage to OOD detection due to LLMs'
hallucinations, as is observed by our analysis. In this paper, we propose to
apply world knowledge to enhance OOD detection performance through selective
generation from LLMs. Specifically, we introduce a consistency-based
uncertainty calibration method to estimate the confidence score of each
generation. We further extract visual objects from each image to fully
capitalize on the aforementioned world knowledge. Extensive experiments
demonstrate that our method consistently outperforms the state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08031" title="Abstract">arXiv:2310.08031</a> [<a href="/pdf/2310.08031" title="Download PDF">pdf</a>, <a href="/format/2310.08031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Graph Clustering with Noisy Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Luca%2C+A+B">Artur Back de Luca</a>, 
<a href="/search/cs?searchtype=author&query=Fountoulakis%2C+K">Kimon Fountoulakis</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shenghao Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 5 figures, 14 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI); Machine Learning (stat.ML)

</div>
<p class="mathjax">The growing interest in machine learning problems over graphs with additional
node information such as texts, images, or labels has popularized methods that
require the costly operation of processing the entire graph. Yet, little effort
has been made to the development of fast local methods (i.e. without accessing
the entire graph) that extract useful information from such data. To that end,
we propose a study of local graph clustering using noisy node labels as a proxy
for additional node information. In this setting, nodes receive initial binary
labels based on cluster affiliation: 1 if they belong to the target cluster and
0 otherwise. Subsequently, a fraction of these labels is flipped. We
investigate the benefits of incorporating noisy labels for local graph
clustering. By constructing a weighted graph with such labels, we study the
performance of graph diffusion-based local clustering method on both the
original and the weighted graphs. From a theoretical perspective, we consider
recovering an unknown target cluster with a single seed node in a random graph
with independent noisy node labels. We provide sufficient conditions on the
label noise under which, with high probability, using diffusion in the weighted
graph yields a more accurate recovery of the target cluster. This approach
proves more effective than using the given labels alone or using diffusion in
the label-free original graph. Empirically, we show that reliable node labels
can be obtained with just a few samples from an attributed graph. Moreover,
utilizing these labels via diffusion in the weighted graph leads to
significantly better local clustering performance across several real-world
datasets, improving F1 scores by up to 13%.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08032" title="Abstract">arXiv:2310.08032</a> [<a href="/pdf/2310.08032" title="Download PDF">pdf</a>, <a href="/format/2310.08032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incorporating Domain Knowledge Graph into Multimodal Movie Genre  Classification with Self-Supervised Attention and Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+G">Guilin Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chuanyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yongrui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Y">Yiming Tan</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+C">Chenlong Xia</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Ye Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Multimodal movie genre classification has always been regarded as a demanding
multi-label classification task due to the diversity of multimodal data such as
posters, plot summaries, trailers and metadata. Although existing works have
made great progress in modeling and combining each modality, they still face
three issues: 1) unutilized group relations in metadata, 2) unreliable
attention allocation, and 3) indiscriminative fused features. Given that the
knowledge graph has been proven to contain rich information, we present a novel
framework that exploits the knowledge graph from various perspectives to
address the above problems. As a preparation, the metadata is processed into a
domain knowledge graph. A translate model for knowledge graph embedding is
adopted to capture the relations between entities. Firstly we retrieve the
relevant embedding from the knowledge graph by utilizing group relations in
metadata and then integrate it with other modalities. Next, we introduce an
Attention Teacher module for reliable attention allocation based on
self-supervised learning. It learns the distribution of the knowledge graph and
produces rational attention weights. Finally, a Genre-Centroid Anchored
Contrastive Learning module is proposed to strengthen the discriminative
ability of fused features. The embedding space of anchors is initialized from
the genre entities in the knowledge graph. To verify the effectiveness of our
framework, we collect a larger and more challenging dataset named MM-IMDb 2.0
compared with the MM-IMDb dataset. The experimental results on two datasets
demonstrate that our model is superior to the state-of-the-art methods. We will
release the code in the near future.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08034" title="Abstract">arXiv:2310.08034</a> [<a href="/pdf/2310.08034" title="Download PDF">pdf</a>, <a href="/format/2310.08034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Receive, Reason, and React: Drive as You Say with Large Language Models  in Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+C">Can Cui</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yunsheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wenqian Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziran Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2309.10228">arXiv:2309.10228</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">The fusion of human-centric design and artificial intelligence (AI)
capabilities has opened up new possibilities for next-generation autonomous
vehicles that go beyond transportation. These vehicles can dynamically interact
with passengers and adapt to their preferences. This paper proposes a novel
framework that leverages Large Language Models (LLMs) to enhance the
decision-making process in autonomous vehicles. By utilizing LLMs' linguistic
and contextual understanding abilities with specialized tools, we aim to
integrate the language and reasoning capabilities of LLMs into autonomous
vehicles. Our research includes experiments in HighwayEnv, a collection of
environments for autonomous driving and tactical decision-making tasks, to
explore LLMs' interpretation, interaction, and reasoning in various scenarios.
We also examine real-time personalization, demonstrating how LLMs can influence
driving behaviors based on verbal commands. Our empirical results highlight the
substantial advantages of utilizing chain-of-thought prompting, leading to
improved driving decisions, and showing the potential for LLMs to enhance
personalized driving experiences through ongoing verbal feedback. The proposed
framework aims to transform autonomous vehicle operations, offering
personalized support, transparent decision-making, and continuous learning to
enhance safety and effectiveness. We achieve user-centric, transparent, and
adaptive autonomous driving ecosystems supported by the integration of LLMs
into autonomous vehicles.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08035" title="Abstract">arXiv:2310.08035</a> [<a href="/pdf/2310.08035" title="Download PDF">pdf</a>, <a href="/format/2310.08035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BaSAL: Size Balanced Warm Start Active Learning for LiDAR Semantic  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jiarong Wei</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yancong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Caesar%2C+H">Holger Caesar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Active learning strives to reduce the need for costly data annotation, by
repeatedly querying an annotator to label the most informative samples from a
pool of unlabeled data and retraining a model from these samples. We identify
two problems with existing active learning methods for LiDAR semantic
segmentation. First, they ignore the severe class imbalance inherent in LiDAR
semantic segmentation datasets. Second, to bootstrap the active learning loop,
they train their initial model from randomly selected data samples, which leads
to low performance and is referred to as the cold start problem. To address
these problems we propose BaSAL, a size-balanced warm start active learning
model, based on the observation that each object class has a characteristic
size. By sampling object clusters according to their size, we can thus create a
size-balanced dataset that is also more class-balanced. Furthermore, in
contrast to existing information measures like entropy or CoreSet, size-based
sampling does not require an already trained model and thus can be used to
address the cold start problem. Results show that we are able to improve the
performance of the initial model by a large margin. Combining size-balanced
sampling and warm start with established information measures, our approach
achieves a comparable performance to training on the entire SemanticKITTI
dataset, despite using only 5% of the annotations, which outperforms existing
active learning methods. We also match the existing state-of-the-art in active
learning on nuScenes. Our code will be made available upon paper acceptance.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08036" title="Abstract">arXiv:2310.08036</a> [<a href="/pdf/2310.08036" title="Download PDF">pdf</a>, <a href="/format/2310.08036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ZEST: Attention-based Zero-Shot Learning for Unseen IoT Device  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Binghui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gysel%2C+P">Philipp Gysel</a>, 
<a href="/search/cs?searchtype=author&query=Divakaran%2C+D+M">Dinil Mon Divakaran</a>, 
<a href="/search/cs?searchtype=author&query=Gurusamy%2C+M">Mohan Gurusamy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent research works have proposed machine learning models for classifying
IoT devices connected to a network. However, there is still a practical
challenge of not having all devices (and hence their traffic) available during
the training of a model. This essentially means, during the operational phase,
we need to classify new devices not seen during the training phase. To address
this challenge, we propose ZEST -- a ZSL (zero-shot learning) framework based
on self-attention for classifying both seen and unseen devices. ZEST consists
of i) a self-attention based network feature extractor, termed SANE, for
extracting latent space representations of IoT traffic, ii) a generative model
that trains a decoder using latent features to generate pseudo data, and iii) a
supervised model that is trained on the generated pseudo data for classifying
devices. We carry out extensive experiments on real IoT traffic data; our
experiments demonstrate i) ZEST achieves significant improvement (in terms of
accuracy) over the baselines; ii) ZEST is able to better extract meaningful
representations than LSTM which has been commonly used for modeling network
traffic.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08037" title="Abstract">arXiv:2310.08037</a> [<a href="/pdf/2310.08037" title="Download PDF">pdf</a>, <a href="/ps/2310.08037" title="Download PostScript">ps</a>, <a href="/format/2310.08037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact Factors for Computer Science Conferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eickhoff%2C+C">Carsten Eickhoff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Communications of the ACM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">An increasing number of CS researchers are employed in academic non-CS
departments where publication output is measured in terms of journal impact
factors. To foster recognition of publications in peer-reviewed CS conference
proceedings, we analyzed more than 40,000 CS publications and computed journal
impact factors for 88 top-ranking conferences across a representative range of
fields, finding that some conferences have impact factors corresponding to
those of high-ranking journals.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08038" title="Abstract">arXiv:2310.08038</a> [<a href="/pdf/2310.08038" title="Download PDF">pdf</a>, <a href="/ps/2310.08038" title="Download PostScript">ps</a>, <a href="/format/2310.08038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual Learning via Manifold Expansion Replay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zihao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yufei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianfeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingsong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xian Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR)

</div>
<p class="mathjax">In continual learning, the learner learns multiple tasks in sequence, with
data being acquired only once for each task. Catastrophic forgetting is a major
challenge to continual learning. To reduce forgetting, some existing
rehearsal-based methods use episodic memory to replay samples of previous
tasks. However, in the process of knowledge integration when learning a new
task, this strategy also suffers from catastrophic forgetting due to an
imbalance between old and new knowledge. To address this problem, we propose a
novel replay strategy called Manifold Expansion Replay (MaER). We argue that
expanding the implicit manifold of the knowledge representation in the episodic
memory helps to improve the robustness and expressiveness of the model. To this
end, we propose a greedy strategy to keep increasing the diameter of the
implicit manifold represented by the knowledge in the buffer during memory
management. In addition, we introduce Wasserstein distance instead of cross
entropy as distillation loss to preserve previous knowledge. With extensive
experimental validation on MNIST, CIFAR10, CIFAR100, and TinyImageNet, we show
that the proposed method significantly improves the accuracy in continual
learning setup, outperforming the state of the arts.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08039" title="Abstract">arXiv:2310.08039</a> [<a href="/pdf/2310.08039" title="Download PDF">pdf</a>, <a href="/format/2310.08039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jinbo Song</a> (1), 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ruoran Huang</a> (1), 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyang Wang</a> (1), 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wei Huang</a> (1), 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qian Yu</a> (1), 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingming Chen</a> (1), 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yafei Yao</a> (1), 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Chaosheng Fan</a> (1), 
<a href="/search/cs?searchtype=author&query=Peng%2C+C">Changping Peng</a> (1), 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhangang Lin</a> (1), 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jinghe Hu</a> (1), 
<a href="/search/cs?searchtype=author&query=Shao%2C+J">Jingping Shao</a> (1) ((1) Marketing and Commercialization Center, JD.com)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 31st ACM International Conference on
  Information &amp; Knowledge Management. 2022: 4495-4499
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Industrial systems such as recommender systems and online advertising, have
been widely equipped with multi-stage architectures, which are divided into
several cascaded modules, including matching, pre-ranking, ranking and
re-ranking. As a critical bridge between matching and ranking, existing
pre-ranking approaches mainly endure sample selection bias (SSB) problem owing
to ignoring the entire-chain data dependence, resulting in sub-optimal
performances. In this paper, we rethink pre-ranking system from the perspective
of the entire sample space, and propose Entire-chain Cross-domain Models (ECM),
which leverage samples from the whole cascaded stages to effectively alleviate
SSB problem. Besides, we design a fine-grained neural structure named ECMM to
further improve the pre-ranking accuracy. Specifically, we propose a
cross-domain multi-tower neural network to comprehensively predict for each
stage result, and introduce the sub-networking routing strategy with $L0$
regularization to reduce computational costs. Evaluations on real-world
large-scale traffic logs demonstrate that our pre-ranking models outperform
SOTA methods while time consumption is maintained within an acceptable level,
which achieves better trade-off between efficiency and effectiveness.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08040" title="Abstract">arXiv:2310.08040</a> [<a href="/pdf/2310.08040" title="Download PDF">pdf</a>, <a href="/format/2310.08040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEE-OoD: Supervised Exploration For Enhanced Out-of-Distribution  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xiaoyang Song</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wenbo Sun</a>, 
<a href="/search/cs?searchtype=author&query=Nouiehed%2C+M">Maher Nouiehed</a>, 
<a href="/search/cs?searchtype=author&query=Kontar%2C+R+A">Raed Al Kontar</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+J">Judy Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Current techniques for Out-of-Distribution (OoD) detection predominantly rely
on quantifying predictive uncertainty and incorporating model regularization
during the training phase, using either real or synthetic OoD samples. However,
methods that utilize real OoD samples lack exploration and are prone to overfit
the OoD samples at hand. Whereas synthetic samples are often generated based on
features extracted from training data, rendering them less effective when the
training and OoD data are highly overlapped in the feature space. In this work,
we propose a Wasserstein-score-based generative adversarial training scheme to
enhance OoD detection accuracy, which, for the first time, performs data
augmentation and exploration simultaneously under the supervision of limited
OoD samples. Specifically, the generator explores OoD spaces and generates
synthetic OoD samples using feedback from the discriminator, while the
discriminator exploits both the observed and synthesized samples for OoD
detection using a predefined Wasserstein score. We provide theoretical
guarantees that the optimal solutions of our generative scheme are
statistically achievable through adversarial training in empirical settings. We
then demonstrate that the proposed method outperforms state-of-the-art
techniques on various computer vision datasets and exhibits superior
generalizability to unseen OoD data.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08041" title="Abstract">arXiv:2310.08041</a> [<a href="/pdf/2310.08041" title="Download PDF">pdf</a>, <a href="/format/2310.08041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+R">Ruihao Gong</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xiuying Wei</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhiwei Dong</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jianfei Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+B">Bohan Zhuang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) excel in NLP, but their demands hinder their
widespread deployment. While Quantization-Aware Training (QAT) offers a
solution, its extensive training costs make Post-Training Quantization (PTQ) a
more practical approach for LLMs. In existing studies, activation outliers in
particular channels are identified as the bottleneck to PTQ accuracy. They
propose to transform the magnitudes from activations to weights, which however
offers limited alleviation or suffers from unstable gradients, resulting in a
severe performance drop at low-bitwidth. In this paper, we propose QLLM, an
accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM
introduces an adaptive channel reassembly technique that reallocates the
magnitude of outliers to other channels, thereby mitigating their impact on the
quantization range. This is achieved by channel disassembly and channel
assembly, which first breaks down the outlier channels into several
sub-channels to ensure a more balanced distribution of activation magnitudes.
Then similar channels are merged to maintain the original channel number for
efficiency. Additionally, an adaptive strategy is designed to autonomously
determine the optimal number of sub-channels for channel disassembly. To
further compensate for the performance loss caused by quantization, we propose
an efficient tuning method that only learns a small number of low-rank weights
while freezing the pre-trained quantized model. After training, these low-rank
parameters can be fused into the frozen weights without affecting inference.
Extensive experiments on LLaMA-1 and LLaMA-2 show that QLLM can obtain accurate
quantized models efficiently. For example, QLLM quantizes the 4-bit LLaMA-2-70B
within 10 hours on a single A100-80G GPU, outperforming the previous
state-of-the-art method by 7.89% on the average accuracy across five zero-shot
tasks.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08042" title="Abstract">arXiv:2310.08042</a> [<a href="/pdf/2310.08042" title="Download PDF">pdf</a>, <a href="/format/2310.08042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> X-HRNet: Towards Lightweight Human Pose Estimation with Spatially  Unidimensional Self-Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yixuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuanhan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jingkuan Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICME 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">High-resolution representation is necessary for human pose estimation to
achieve high performance, and the ensuing problem is high computational
complexity. In particular, predominant pose estimation methods estimate human
joints by 2D single-peak heatmaps. Each 2D heatmap can be horizontally and
vertically projected to and reconstructed by a pair of 1D heat vectors.
Inspired by this observation, we introduce a lightweight and powerful
alternative, Spatially Unidimensional Self-Attention (SUSA), to the pointwise
(1x1) convolution that is the main computational bottleneck in the depthwise
separable 3c3 convolution. Our SUSA reduces the computational complexity of the
pointwise (1x1) convolution by 96% without sacrificing accuracy. Furthermore,
we use the SUSA as the main module to build our lightweight pose estimation
backbone X-HRNet, where `X' represents the estimated cross-shape attention
vectors. Extensive experiments on the COCO benchmark demonstrate the
superiority of our X-HRNet, and comprehensive ablation studies show the
effectiveness of the SUSA modules. The code is publicly available at
https://github.com/cool-xuan/x-hrnet.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08043" title="Abstract">arXiv:2310.08043</a> [<a href="/pdf/2310.08043" title="Download PDF">pdf</a>, <a href="/format/2310.08043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding and Controlling a Maze-Solving Policy Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mini%2C+U">Ulisse Mini</a>, 
<a href="/search/cs?searchtype=author&query=Grietzer%2C+P">Peli Grietzer</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+M">Mrinank Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Meek%2C+A">Austin Meek</a>, 
<a href="/search/cs?searchtype=author&query=MacDiarmid%2C+M">Monte MacDiarmid</a>, 
<a href="/search/cs?searchtype=author&query=Turner%2C+A+M">Alexander Matt Turner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">To understand the goals and goal representations of AI systems, we carefully
study a pretrained reinforcement learning policy that solves mazes by
navigating to a range of target squares. We find this network pursues multiple
context-dependent goals, and we further identify circuits within the network
that correspond to one of these goals. In particular, we identified eleven
channels that track the location of the goal. By modifying these channels,
either with hand-designed interventions or by combining forward passes, we can
partially control the policy. We show that this network contains redundant,
distributed, and retargetable goal representations, shedding light on the
nature of goal-direction in trained policy networks.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08044" title="Abstract">arXiv:2310.08044</a> [<a href="/pdf/2310.08044" title="Download PDF">pdf</a>, <a href="/format/2310.08044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EC-Depth: Exploring the consistency of self-supervised monocular depth  estimation under challenging scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+R">Ruijie Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Ziyang Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chuxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jianfeng He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianzhu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Self-supervised monocular depth estimation holds significant importance in
the fields of autonomous driving and robotics. However, existing methods are
typically designed to train and test on clear and pristine datasets,
overlooking the impact of various adverse conditions prevalent in real-world
scenarios. As a result, it is commonly observed that most self-supervised
monocular depth estimation methods struggle to perform adequately under
challenging conditions. To address this issue, we present EC-Depth, a novel
self-supervised two-stage training framework to achieve a robust depth
estimation, starting from the foundation of depth prediction consistency under
different perturbations. Leveraging the proposed perturbation-invariant depth
consistency constraint module and the consistency-based pseudo-label selection
module, our model attains accurate and consistent depth predictions in both
standard and challenging scenarios. Extensive experiments substantiate the
effectiveness of the proposed method. Moreover, our method surpasses existing
state-of-the-art methods on KITTI, KITTI-C and DrivingStereo benchmarks,
demonstrating its potential for enhancing the reliability of self-supervised
monocular depth estimation models in real-world applications.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08045" title="Abstract">arXiv:2310.08045</a> [<a href="/pdf/2310.08045" title="Download PDF">pdf</a>, <a href="/format/2310.08045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Predictive Inferential Control of Neural State-Space Models for  Autonomous Vehicle Motion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Askari%2C+I">Iman Askari</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+X">Xumein Tu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Shen Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Huazhen Fang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Model predictive control (MPC) has proven useful in enabling safe and optimal
motion planning for autonomous vehicles. In this paper, we investigate how to
achieve MPC-based motion planning when a neural state-space model represents
the vehicle dynamics. As the neural state-space model will lead to highly
complex, nonlinear and nonconvex optimization landscapes, mainstream
gradient-based MPC methods will be computationally too heavy to be a viable
solution. In a departure, we propose the idea of model predictive inferential
control (MPIC), which seeks to infer the best control decisions from the
control objectives and constraints. Following the idea, we convert the MPC
problem for motion planning into a Bayesian state estimation problem. Then, we
develop a new particle filtering/smoothing approach to perform the estimation.
This approach is implemented as banks of unscented Kalman filters/smoothers and
offers high sampling efficiency, fast computation, and estimation accuracy. We
evaluate the MPIC approach through a simulation study of autonomous driving in
different scenarios, along with an exhaustive comparison with gradient-based
MPC. The results show that the MPIC approach has considerable computational
efficiency, regardless of complex neural network architectures, and shows the
capability to solve large-scale MPC problems for neural state-space models.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08047" title="Abstract">arXiv:2310.08047</a> [<a href="/pdf/2310.08047" title="Download PDF">pdf</a>, <a href="/ps/2310.08047" title="Download PostScript">ps</a>, <a href="/format/2310.08047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Three Paths to Rational Curves with Rational Arc Length
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schr%C3%B6cker%2C+H">Hans-Peter Schr&#xf6;cker</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0%C3%ACr%2C+Z">Zbyn&#x11b;k &#x160;&#xec;r</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>; Differential Geometry (math.DG); Numerical Analysis (math.NA)

</div>
<p class="mathjax">We solve the so far open problem of constructing all spatial rational curves
with rational arc length functions. More precisely, we present three different
methods for this construction. The first method adapts a recent approach of
(Kalkan et al. 2022) to rational PH curves and requires solving a modestly
sized and well structured system of linear equations. The second constructs the
curve by imposing zero-residue conditions, thus extending ideas of previous
papers by (Farouki and Sakkalis 2019) and the authors themselves (Schr\"ocker
and \v{S}\'ir 2023). The third method generalizes the dual approach of
(Pottmann 1995) from the planar to the spatial curves. The three methods share
the same quaternion based representation in which not only the PH curve but
also its arc length function are compactly expressed. We also present a new
proof based on the quaternion polynomial factorization theory of the well known
characterization of the Pythagorean quadruples.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08049" title="Abstract">arXiv:2310.08049</a> [<a href="/pdf/2310.08049" title="Download PDF">pdf</a>, <a href="/format/2310.08049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Relationship Between Model Architecture and In-Context  Learning Ability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+I">Ivan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+N">Nan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T">Taylor Berg-Kirkpatrick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">What is the relationship between model architecture and the ability to
perform in-context learning? In this empirical study, we take the first steps
towards answering this question. In particular, we evaluate fifteen model
architectures across a suite of synthetic in-context learning tasks. The
selected architectures represent a broad range of paradigms, including
recurrent and convolution-based neural networks, transformers, and emerging
attention alternatives. We discover that all considered architectures can
perform in-context learning under certain conditions. However, contemporary
architectures are found to be the best performing, especially as task
complexity grows. Additionally, our follow-up experiments delve into various
factors that influence in-context learning. We observe varied sensitivities
among architectures with respect to hyperparameter settings. Our study of
training dynamics reveals that certain architectures exhibit a smooth,
progressive learning trajectory, while others demonstrate periods of stagnation
followed by abrupt mastery of the task. Finally, and somewhat surprisingly, we
find that several emerging attention alternatives are more robust in-context
learners than transformers; since such approaches have constant-sized memory
footprints at inference time, this result opens the future possibility of
scaling up in-context learning to vastly larger numbers of in-context examples.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08051" title="Abstract">arXiv:2310.08051</a> [<a href="/pdf/2310.08051" title="Download PDF">pdf</a>, <a href="/format/2310.08051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LGL-BCI: A Lightweight Geometric Learning Framework for Motor  Imagery-Based Brain-Computer Interfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jianchao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuzhe Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+J">Jiaqi Ge</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Q+Z">Quan Z. Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xi Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Brain-Computer Interfaces (BCIs) are a groundbreaking technology for
interacting with external devices using brain signals. Despite advancements,
electroencephalogram (EEG)-based Motor Imagery (MI) tasks face challenges like
amplitude and phase variability, and complex spatial correlations, with a need
for smaller model size and faster inference. This study introduces the LGL-BCI
framework, employing a Geometric Deep Learning Framework for EEG processing in
non-Euclidean metric spaces, particularly the Symmetric Positive Definite (SPD)
Manifold space. LGL-BCI offers robust EEG data representation and captures
spatial correlations. We propose an EEG channel selection solution via a
feature decomposition algorithm to reduce SPD matrix dimensionality, with a
lossless transformation boosting inference speed. Extensive experiments show
LGL-BCI's superior accuracy and efficiency compared to current solutions,
highlighting geometric deep learning's potential in MI-BCI applications. The
efficiency, assessed on two public EEG datasets and two real-world EEG devices,
significantly outperforms the state-of-the-art solution in accuracy ($82.54\%$
versus $62.22\%$) with fewer parameters (64.9M compared to 183.7M).
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08056" title="Abstract">arXiv:2310.08056</a> [<a href="/pdf/2310.08056" title="Download PDF">pdf</a>, <a href="/format/2310.08056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from Label Proportions: Bootstrapping Supervised Learners via  Belief Propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Havaldar%2C+S">Shreyas Havaldar</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+N">Navodita Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Sareen%2C+S">Shubhi Sareen</a>, 
<a href="/search/cs?searchtype=author&query=Shanmugam%2C+K">Karthikeyan Shanmugam</a>, 
<a href="/search/cs?searchtype=author&query=Raghuveer%2C+A">Aravindan Raghuveer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Learning from Label Proportions (LLP) is a learning problem where only
aggregate level labels are available for groups of instances, called bags,
during training, and the aim is to get the best performance at the
instance-level on the test data. This setting arises in domains like
advertising and medicine due to privacy considerations. We propose a novel
algorithmic framework for this problem that iteratively performs two main
steps. For the first step (Pseudo Labeling) in every iteration, we define a
Gibbs distribution over binary instance labels that incorporates a) covariate
information through the constraint that instances with similar covariates
should have similar labels and b) the bag level aggregated label. We then use
Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo
labels. In the second step (Embedding Refinement), we use the pseudo labels to
provide supervision for a learner that yields a better embedding. Further, we
iterate on the two steps again by using the second step's embeddings as new
covariates for the next iteration. In the final iteration, a classifier is
trained using the pseudo labels. Our algorithm displays strong gains against
several SOTA baselines (up to 15%) for the LLP Binary Classification problem on
various dataset types - tabular and Image. We achieve these improvements with
minimal computational overhead above standard supervised learning due to Belief
Propagation, for large bag sizes, even for a million samples.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08064" title="Abstract">arXiv:2310.08064</a> [<a href="/pdf/2310.08064" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Age Estimation Based on Graph Convolutional Networks and Multi-head  Attention Mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Miaomiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+C">Changwei Yao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shijin Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Age estimation technology is a part of facial recognition and has been
applied to identity authentication. This technology achieves the development
and application of a juvenile anti-addiction system by authenticating users in
the game. Convolutional Neural Network (CNN) and Transformer algorithms are
widely used in this application scenario. However, these two models cannot
flexibly extract and model features of faces with irregular shapes, and they
are ineffective in capturing key information. Furthermore, the above methods
will contain a lot of background information while extracting features, which
will interfere with the model. In consequence, it is easy to extract redundant
information from images. In this paper, a new modeling idea is proposed to
solve this problem, which can flexibly model irregular objects. The Graph
Convolutional Network (GCN) is used to extract features from irregular face
images effectively, and multi-head attention mechanisms are added to avoid
redundant features and capture key region information in the image. This model
can effectively improve the accuracy of age estimation and reduce the MAE error
value to about 3.64, which is better than the effect of today's age estimation
model, to improve the accuracy of face recognition and identity authentication.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08066" title="Abstract">arXiv:2310.08066</a> [<a href="/pdf/2310.08066" title="Download PDF">pdf</a>, <a href="/format/2310.08066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Search-and-Mix Paradigm in Approximate Nash Equilibrium Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+X">Xiaotie Deng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongchen Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hanyu Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">AI in Math deals with mathematics in a constructive manner so that reasoning
becomes automated, less laborious, and less error-prone. For algorithms, the
question becomes how to automate analyses for specific problems. For the first
time, this work provides an automatic method for approximation analysis on a
well-studied problem in theoretical computer science: computing approximate
Nash equilibria in two-player games. We observe that such algorithms can be
reformulated into a search-and-mix paradigm, which involves a search phase
followed by a mixing phase. By doing so, we are able to fully automate the
procedure of designing and analyzing the mixing phase. For example, we
illustrate how to perform our method with a program to analyze the
approximation bounds of all the algorithms in the literature. Same
approximation bounds are computed without any hand-written proof. Our automatic
method heavily relies on the LP-relaxation structure in approximate Nash
equilibria. Since many approximation algorithms and online algorithms adopt the
LP relaxation, our approach may be extended to automate the analysis of other
algorithms.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08067" title="Abstract">arXiv:2310.08067</a> [<a href="/pdf/2310.08067" title="Download PDF">pdf</a>, <a href="/format/2310.08067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GameGPT: Multi-agent Collaborative Framework for Game Development
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dake Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hanbin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+Y">Yunhao Huo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuzhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoyang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The large language model (LLM) based agents have demonstrated their capacity
to automate and expedite software development processes. In this paper, we
focus on game development and propose a multi-agent collaborative framework,
dubbed GameGPT, to automate game development. While many studies have
pinpointed hallucination as a primary roadblock for deploying LLMs in
production, we identify another concern: redundancy. Our framework presents a
series of methods to mitigate both concerns. These methods include dual
collaboration and layered approaches with several in-house lexicons, to
mitigate the hallucination and redundancy in the planning, task identification,
and implementation phases. Furthermore, a decoupling approach is also
introduced to achieve code generation with better precision.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08069" title="Abstract">arXiv:2310.08069</a> [<a href="/pdf/2310.08069" title="Download PDF">pdf</a>, <a href="/format/2310.08069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Negative Pairs in Code Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haochen Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tuan%2C+L+A">Luu Anh Tuan</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+C">Chunyan Miao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, contrastive learning has become a key component in fine-tuning code
search models for software development efficiency and effectiveness. It pulls
together positive code snippets while pushing negative samples away given
search queries. Among contrastive learning, InfoNCE is the most widely used
loss function due to its better performance. However, the following problems in
negative samples of InfoNCE may deteriorate its representation learning: 1) The
existence of false negative samples in large code corpora due to duplications.
2). The failure to explicitly differentiate between the potential relevance of
negative samples. As an example, a bubble sorting algorithm example is less
``negative'' than a file saving function for the quick sorting algorithm query.
In this paper, we tackle the above problems by proposing a simple yet effective
Soft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss
function, we apply three methods to estimate the weights of negative pairs and
show that the vanilla InfoNCE loss is a special case of Soft-InfoNCE.
Theoretically, we analyze the effects of Soft-InfoNCE on controlling the
distribution of learnt code representations and on deducing a more precise
mutual information estimation. We furthermore discuss the superiority of
proposed loss functions with other design alternatives. Extensive experiments
demonstrate the effectiveness of Soft-InfoNCE and weights estimation methods
under state-of-the-art code search models on a large-scale public dataset
consisting of six programming languages. Source code is available at
\url{https://github.com/Alex-HaochenLi/Soft-InfoNCE}.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08070" title="Abstract">arXiv:2310.08070</a> [<a href="/pdf/2310.08070" title="Download PDF">pdf</a>, <a href="/ps/2310.08070" title="Download PostScript">ps</a>, <a href="/format/2310.08070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Time-Space Lower Bounds for Constant-Pass Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+X">Xin Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Tal%2C+A">Avishay Tal</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hongxun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Junzhao Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at FOCS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">In his breakthrough paper, Raz showed that any parity learning algorithm
requires either quadratic memory or an exponential number of samples [FOCS'16,
JACM'19]. A line of work that followed extended this result to a large class of
learning problems. Until recently, all these results considered learning in the
streaming model, where each sample is drawn independently, and the learner is
allowed a single pass over the stream of samples. Garg, Raz, and Tal [CCC'19]
considered a stronger model, allowing multiple passes over the stream. In the
$2$-pass model, they showed that learning parities of size $n$ requires either
a memory of size $n^{1.5}$ or at least $2^{\sqrt{n}}$ samples. (Their result
also generalizes to other learning problems.)
<br />In this work, for any constant $q$, we prove tight memory-sample lower bounds
for any parity learning algorithm that makes $q$ passes over the stream of
samples. We show that such a learner requires either $\Omega(n^{2})$ memory
size or at least $2^{\Omega(n)}$ samples. Beyond establishing a tight lower
bound, this is the first non-trivial lower bound for $q$-pass learning for any
$q\ge 3$. Similar to prior work, our results extend to any learning problem
with many nearly-orthogonal concepts.
<br />We complement the lower bound with an upper bound, showing that parity
learning with $q$ passes can be done efficiently with $O(n^2/\log q)$ memory.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08071" title="Abstract">arXiv:2310.08071</a> [<a href="/pdf/2310.08071" title="Download PDF">pdf</a>, <a href="/format/2310.08071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Transferable Conceptual Prototypes for Interpretable  Unsupervised Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Junyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xinhong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Changsheng Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE TIP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Despite the great progress of unsupervised domain adaptation (UDA) with the
deep neural networks, current UDA models are opaque and cannot provide
promising explanations, limiting their applications in the scenarios that
require safe and controllable model decisions. At present, a surge of work
focuses on designing deep interpretable methods with adequate data annotations
and only a few methods consider the distributional shift problem. Most existing
interpretable UDA methods are post-hoc ones, which cannot facilitate the model
learning process for performance enhancement. In this paper, we propose an
inherently interpretable method, named Transferable Conceptual Prototype
Learning (TCPL), which could simultaneously interpret and improve the processes
of knowledge transfer and decision-making in UDA. To achieve this goal, we
design a hierarchically prototypical module that transfers categorical basic
concepts from the source domain to the target domain and learns domain-shared
prototypes for explaining the underlying reasoning process. With the learned
transferable prototypes, a self-predictive consistent pseudo-label strategy
that fuses confidence, predictions, and prototype information, is designed for
selecting suitable target samples for pseudo annotations and gradually
narrowing down the domain gap. Comprehensive experiments show that the proposed
method can not only provide effective and intuitive explanations but also
outperform previous state-of-the-arts.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08072" title="Abstract">arXiv:2310.08072</a> [<a href="/pdf/2310.08072" title="Download PDF">pdf</a>, <a href="/format/2310.08072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Generative Question-Answering on Synthetic Data Obtained from  an Instruct-tuned Mo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+K">Kosuke Takahashi</a>, 
<a href="/search/cs?searchtype=author&query=Omi%2C+T">Takahiro Omi</a>, 
<a href="/search/cs?searchtype=author&query=Arima%2C+K">Kosuke Arima</a>, 
<a href="/search/cs?searchtype=author&query=Ishigaki%2C+T">Tatsuya Ishigaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PACLIC 2023 short paper, 4 pages (6 pages including references), 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper presents a simple and cost-effective method for synthesizing data
to train question-answering systems. For training, fine-tuning GPT models is a
common practice in resource-rich languages like English, however, it becomes
challenging for non-English languages due to the scarcity of sufficient
question-answer (QA) pairs. Existing approaches use question and answer
generators trained on human-authored QA pairs, which involves substantial human
expenses. In contrast, we use an instruct-tuned model to generate QA pairs in a
zero-shot or few-shot manner. We conduct experiments to compare various
strategies for obtaining QA pairs from the instruct-tuned model. The results
demonstrate that a model trained on our proposed synthetic data achieves
comparable performance to a model trained on manually curated datasets, without
incurring human costs.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08073" title="Abstract">arXiv:2310.08073</a> [<a href="/pdf/2310.08073" title="Download PDF">pdf</a>, <a href="/format/2310.08073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piras%2C+G">Giorgio Piras</a>, 
<a href="/search/cs?searchtype=author&query=Pintor%2C+M">Maura Pintor</a>, 
<a href="/search/cs?searchtype=author&query=Demontis%2C+A">Ambra Demontis</a>, 
<a href="/search/cs?searchtype=author&query=Biggio%2C+B">Battista Biggio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Neural network pruning has shown to be an effective technique for reducing
the network size, trading desirable properties like generalization and
robustness to adversarial attacks for higher sparsity. Recent work has claimed
that adversarial pruning methods can produce sparse networks while also
preserving robustness to adversarial examples. In this work, we first
re-evaluate three state-of-the-art adversarial pruning methods, showing that
their robustness was indeed overestimated. We then compare pruned and dense
versions of the same models, discovering that samples on thin ice, i.e., closer
to the unpruned model's decision boundary, are typically misclassified after
pruning. We conclude by discussing how this intuition may lead to designing
more effective adversarial pruning methods in future work.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08074" title="Abstract">arXiv:2310.08074</a> [<a href="/pdf/2310.08074" title="Download PDF">pdf</a>, <a href="/ps/2310.08074" title="Download PostScript">ps</a>, <a href="/format/2310.08074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Additive one-rank hull codes over finite fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+A">Astha Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R+K">R. K. Sharma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This article explores additive codes with one-rank hull, offering key
insights and constructions. It gives a characterization of the hull of an
additive code $C$ in terms of its generator matrix and establishes a connection
between self-orthogonal elements and solutions of quadratic forms. Using
self-orthogonal elements, the existence of a one-rank hull code is
demonstrated. The article provides a precise count of self-orthogonal elements
for any duality over the finite field $\mathbb{F}_q$, particularly odd primes.
Additionally, construction methods for small-rank hull codes are introduced.
The highest possible minimum distance among additive one-rank hull codes is
denoted by $d_1[n,k]_{p^e,M}$. The value of $d_1[n,k]_{p^e,M}$ for $k=1,2$ and
$n\geq 2$ with respect to any duality $M$ over any finite field
$\mathbb{F}_{p^e}$ is determined. Also, the highest possible minimum distance
for Quaternary one-rank hull code is determined over non-symmetric dualities
for length $1\leq n\leq 10$.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08078" title="Abstract">arXiv:2310.08078</a> [<a href="/pdf/2310.08078" title="Download PDF">pdf</a>, <a href="/format/2310.08078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> To token or not to token: A Comparative Study of Text Representations  for Cross-Lingual Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+M">Md Mushfiqur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Sakib%2C+F+A">Fardin Ahsan Sakib</a>, 
<a href="/search/cs?searchtype=author&query=Faisal%2C+F">Fahim Faisal</a>, 
<a href="/search/cs?searchtype=author&query=Anastasopoulos%2C+A">Antonios Anastasopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at 3RD MULTILINGUAL REPRESENTATION LEARNING (MRL) WORKSHOP, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Choosing an appropriate tokenization scheme is often a bottleneck in
low-resource cross-lingual transfer. To understand the downstream implications
of text representation choices, we perform a comparative analysis on language
models having diverse text representation modalities including 2
segmentation-based models (\texttt{BERT}, \texttt{mBERT}), 1 image-based model
(\texttt{PIXEL}), and 1 character-level model (\texttt{CANINE}). First, we
propose a scoring Language Quotient (LQ) metric capable of providing a weighted
representation of both zero-shot and few-shot evaluation combined. Utilizing
this metric, we perform experiments comprising 19 source languages and 133
target languages on three tasks (POS tagging, Dependency parsing, and NER). Our
analysis reveals that image-based models excel in cross-lingual transfer when
languages are closely related and share visually similar scripts. However, for
tasks biased toward word meaning (POS, NER), segmentation-based models prove to
be superior. Furthermore, in dependency parsing tasks where word relationships
play a crucial role, models with their character-level focus, outperform
others. Finally, we propose a recommendation scheme based on our findings to
guide model selection according to task and language requirements.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08082" title="Abstract">arXiv:2310.08082</a> [<a href="/pdf/2310.08082" title="Download PDF">pdf</a>, <a href="/ps/2310.08082" title="Download PostScript">ps</a>, <a href="/format/2310.08082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jointly Optimized Global-Local Visual Localization of UAVs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoling Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiuniu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhiwei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenjia Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Navigation and localization of UAVs present a challenge when global
navigation satellite systems (GNSS) are disrupted and unreliable. Traditional
techniques, such as simultaneous localization and mapping (SLAM) and visual
odometry (VO), exhibit certain limitations in furnishing absolute coordinates
and mitigating error accumulation. Existing visual localization methods achieve
autonomous visual localization without error accumulation by matching with
ortho satellite images. However, doing so cannot guarantee real-time
performance due to the complex matching process. To address these challenges,
we propose a novel Global-Local Visual Localization (GLVL) network. Our GLVL
network is a two-stage visual localization approach, combining a large-scale
retrieval module that finds similar regions with the UAV flight scene, and a
fine-grained matching module that localizes the precise UAV coordinate,
enabling real-time and precise localization. The training process is jointly
optimized in an end-to-end manner to further enhance the model capability.
Experiments on six UAV flight scenes encompassing both texture-rich and
texture-sparse regions demonstrate the ability of our model to achieve the
real-time precise localization requirements of UAVs. Particularly, our method
achieves a localization error of only 2.39 meters in 0.48 seconds in a village
scene with sparse texture features.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08083" title="Abstract">arXiv:2310.08083</a> [<a href="/pdf/2310.08083" title="Download PDF">pdf</a>, <a href="/format/2310.08083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Using GUI Interaction Data to Improve Text Retrieval-based Bug  Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahmud%2C+J">Junayed Mahmud</a>, 
<a href="/search/cs?searchtype=author&query=De+Silva%2C+N">Nadeeshan De Silva</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S+A">Safwat Ali Khan</a>, 
<a href="/search/cs?searchtype=author&query=Mostafavi%2C+S+H">Seyed Hooman Mostafavi</a>, 
<a href="/search/cs?searchtype=author&query=Mansur%2C+S+H">SM Hasan Mansur</a>, 
<a href="/search/cs?searchtype=author&query=Chaparro%2C+O">Oscar Chaparro</a>, 
<a href="/search/cs?searchtype=author&query=Marcus%2C+A">Andrian Marcus</a>, 
<a href="/search/cs?searchtype=author&query=Moran%2C+K">Kevin Moran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, to appear in the Proceedings of the 46th International Conference on Software Engineering (ICSE'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">One of the most important tasks related to managing bug reports is localizing
the fault so that a fix can be applied. As such, prior work has aimed to
automate this task of bug localization by formulating it as an information
retrieval problem, where potentially buggy files are retrieved and ranked
according to their textual similarity with a given bug report. However, there
is often a notable semantic gap between the information contained in bug
reports and identifiers or natural language contained within source code files.
For user-facing software, there is currently a key source of information that
could aid in bug localization, but has not been thoroughly investigated -
information from the GUI.
<br />We investigate the hypothesis that, for end user-facing applications,
connecting information in a bug report with information from the GUI, and using
this to aid in retrieving potentially buggy files, can improve upon existing
techniques for bug localization. To examine this phenomenon, we conduct a
comprehensive empirical study that augments four baseline techniques for bug
localization with GUI interaction information from a reproduction scenario to
(i) filter out potentially irrelevant files, (ii) boost potentially relevant
files, and (iii) reformulate text-retrieval queries. To carry out our study, we
source the current largest dataset of fully-localized and reproducible real
bugs for Android apps, with corresponding bug reports, consisting of 80 bug
reports from 39 popular open-source apps. Our results illustrate that
augmenting traditional techniques with GUI information leads to a marked
increase in effectiveness across multiple metrics, including a relative
increase in Hits@10 of 13-18%. Additionally, through further analysis, we find
that our studied augmentations largely complement existing techniques.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08084" title="Abstract">arXiv:2310.08084</a> [<a href="/pdf/2310.08084" title="Download PDF">pdf</a>, <a href="/format/2310.08084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Volumetric Medical Image Segmentation via Scribble Annotations and Shape  Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qiuhui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+H">Haiying Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xinyue Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yi Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2205.06779">arXiv:2205.06779</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, weakly-supervised image segmentation using weak annotations like
scribbles has gained great attention in computer vision and medical image
analysis, since such annotations are much easier to obtain compared to
time-consuming and labor-intensive labeling at the pixel/voxel level. However,
due to a lack of structure supervision on regions of interest (ROIs), existing
scribble-based methods suffer from poor boundary localization. Furthermore,
most current methods are designed for 2D image segmentation, which do not fully
leverage the volumetric information if directly applied to each image slice. In
this paper, we propose a scribble-based volumetric image segmentation,
Scribble2D5, which tackles 3D anisotropic image segmentation and aims to its
improve boundary prediction. To achieve this, we augment a 2.5D attention UNet
with a proposed label propagation module to extend semantic information from
scribbles and use a combination of static and active boundary prediction to
learn ROI's boundary and regularize its shape. Also, we propose an optional
add-on component, which incorporates the shape prior information from unpaired
segmentation masks to further improve model accuracy. Extensive experiments on
three public datasets and one private dataset demonstrate our Scribble2D5
achieves state-of-the-art performance on volumetric image segmentation using
scribbles and shape prior if available.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08085" title="Abstract">arXiv:2310.08085</a> [<a href="/pdf/2310.08085" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Resource Clickbait Spoiling for Indonesian via Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maharani%2C+N+P+I">Ni Putu Intan Maharani</a>, 
<a href="/search/cs?searchtype=author&query=Purwarianti%2C+A">Ayu Purwarianti</a>, 
<a href="/search/cs?searchtype=author&query=Aji%2C+A+F">Alham Fikri Aji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ICAICTA 2023 (10th International Conference on Advanced Informatics: Concepts, Theory and Applications)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Clickbait spoiling aims to generate a short text to satisfy the curiosity
induced by a clickbait post. As it is a newly introduced task, the dataset is
only available in English so far. Our contributions include the construction of
manually labeled clickbait spoiling corpus in Indonesian and an evaluation on
using cross-lingual zero-shot question answering-based models to tackle
clikcbait spoiling for low-resource language like Indonesian. We utilize
selection of multilingual language models. The experimental results suggest
that XLM-RoBERTa (large) model outperforms other models for phrase and passage
spoilers, meanwhile, mDeBERTa (base) model outperforms other models for
multipart spoilers.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08088" title="Abstract">arXiv:2310.08088</a> [<a href="/pdf/2310.08088" title="Download PDF">pdf</a>, <a href="/format/2310.08088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dealing with zero-inflated data: achieving SOTA with a two-fold machine  learning approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ro%C5%BEanec%2C+J+M">Jo&#x17e;e M. Ro&#x17e;anec</a>, 
<a href="/search/cs?searchtype=author&query=Petelin%2C+G">Ga&#x161;per Petelin</a>, 
<a href="/search/cs?searchtype=author&query=Costa%2C+J">Jo&#xe3;o Costa</a>, 
<a href="/search/cs?searchtype=author&query=Bertalani%C4%8D%2C+B">Bla&#x17e; Bertalani&#x10d;</a>, 
<a href="/search/cs?searchtype=author&query=Cerar%2C+G">Gregor Cerar</a>, 
<a href="/search/cs?searchtype=author&query=Gu%C4%8Dek%2C+M">Marko Gu&#x10d;ek</a>, 
<a href="/search/cs?searchtype=author&query=Papa%2C+G">Gregor Papa</a>, 
<a href="/search/cs?searchtype=author&query=Mladeni%C4%87%2C+D">Dunja Mladeni&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In many cases, a machine learning model must learn to correctly predict a few
data points with particular values of interest in a broader range of data where
many target values are zero. Zero-inflated data can be found in diverse
scenarios, such as lumpy and intermittent demands, power consumption for home
appliances being turned on and off, impurities measurement in distillation
processes, and even airport shuttle demand prediction. The presence of zeroes
affects the models' learning and may result in poor performance. Furthermore,
zeroes also distort the metrics used to compute the model's prediction quality.
This paper showcases two real-world use cases (home appliances classification
and airport shuttle demand prediction) where a hierarchical model applied in
the context of zero-inflated data leads to excellent results. In particular,
for home appliances classification, the weighted average of Precision, Recall,
F1, and AUC ROC was increased by 27%, 34%, 49%, and 27%, respectively.
Furthermore, it is estimated that the proposed approach is also four times more
energy efficient than the SOTA approach against which it was compared to.
Two-fold models performed best in all cases when predicting airport shuttle
demand, and the difference against other models has been proven to be
statistically significant.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08089" title="Abstract">arXiv:2310.08089</a> [<a href="/pdf/2310.08089" title="Download PDF">pdf</a>, <a href="/format/2310.08089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Regularized Monotone Graphon Mean-Field Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fengzhuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+V+Y+F">Vincent Y. F. Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhuoran Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Systems and Control (eess.SY); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper studies two fundamental problems in regularized Graphon Mean-Field
Games (GMFGs). First, we establish the existence of a Nash Equilibrium (NE) of
any $\lambda$-regularized GMFG (for $\lambda\geq 0$). This result relies on
weaker conditions than those in previous works for analyzing both unregularized
GMFGs ($\lambda=0$) and $\lambda$-regularized MFGs, which are special cases of
GMFGs. Second, we propose provably efficient algorithms to learn the NE in
weakly monotone GMFGs, motivated by Lasry and Lions [2007]. Previous literature
either only analyzed continuous-time algorithms or required extra conditions to
analyze discrete-time algorithms. In contrast, we design a discrete-time
algorithm and derive its convergence rate solely under weakly monotone
conditions. Furthermore, we develop and analyze the action-value function
estimation procedure during the online learning process, which is absent from
algorithms for monotone GMFGs. This serves as a sub-module in our optimization
algorithm. The efficiency of the designed algorithm is corroborated by
empirical evaluations.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08091" title="Abstract">arXiv:2310.08091</a> [<a href="/pdf/2310.08091" title="Download PDF">pdf</a>, <a href="/format/2310.08091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discerning Temporal Difference Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jianfei Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Temporal difference learning (TD) is a foundational concept in reinforcement
learning (RL), aimed at efficiently assessing a policy's value function.
TD($\lambda$), a potent variant, incorporates a memory trace to distribute the
prediction error into the historical context. However, this approach often
neglects the significance of historical states and the relative importance of
propagating the TD error, influenced by challenges such as visitation imbalance
or outcome noise. To address this, we propose a novel TD algorithm named
discerning TD learning (DTD), which allows flexible emphasis
functions$-$predetermined or adapted during training$-$to allocate efforts
effectively across states. We establish the convergence properties of our
method within a specific class of emphasis functions and showcase its promising
potential for adaptation to deep RL contexts. Empirical results underscore that
employing a judicious emphasis function not only improves value estimation but
also expedites learning across diverse scenarios.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08092" title="Abstract">arXiv:2310.08092</a> [<a href="/pdf/2310.08092" title="Download PDF">pdf</a>, <a href="/format/2310.08092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistent123: Improve Consistency for One Image to 3D Object Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weng%2C+H">Haohan Weng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C+L+P">C. L. Philip Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> For more qualitative results, please see <a href="https://consistent-123.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Large image diffusion models enable novel view synthesis with high quality
and excellent zero-shot capability. However, such models based on
image-to-image translation have no guarantee of view consistency, limiting the
performance for downstream tasks like 3D reconstruction and image-to-3D
generation. To empower consistency, we propose Consistent123 to synthesize
novel views simultaneously by incorporating additional cross-view attention
layers and the shared self-attention mechanism. The proposed attention
mechanism improves the interaction across all synthesized views, as well as the
alignment between the condition view and novel views. In the sampling stage,
such architecture supports simultaneously generating an arbitrary number of
views while training at a fixed length. We also introduce a progressive
classifier-free guidance strategy to achieve the trade-off between texture and
geometry for synthesized object views. Qualitative and quantitative experiments
show that Consistent123 outperforms baselines in view consistency by a large
margin. Furthermore, we demonstrate a significant improvement of Consistent123
on varying downstream tasks, showing its great potential in the 3D generation
field. The project page is available at consistent-123.github.io.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08094" title="Abstract">arXiv:2310.08094</a> [<a href="/pdf/2310.08094" title="Download PDF">pdf</a>, <a href="/format/2310.08094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SingleInsert: Inserting New Concepts from a Single Image into  Text-to-Image Models for Flexible Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zijie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chaohui Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xiang Bai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://jarrentwu1031.github.io/SingleInsert-web/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent progress in text-to-image (T2I) models enables high-quality image
generation with flexible textual control. To utilize the abundant visual priors
in the off-the-shelf T2I models, a series of methods try to invert an image to
proper embedding that aligns with the semantic space of the T2I model. However,
these image-to-text (I2T) inversion methods typically need multiple source
images containing the same concept or struggle with the imbalance between
editing flexibility and visual fidelity. In this work, we point out that the
critical problem lies in the foreground-background entanglement when learning
an intended concept, and propose a simple and effective baseline for
single-image I2T inversion, named SingleInsert. SingleInsert adopts a two-stage
scheme. In the first stage, we regulate the learned embedding to concentrate on
the foreground area without being associated with the irrelevant background. In
the second stage, we finetune the T2I model for better visual resemblance and
devise a semantic loss to prevent the language drift problem. With the proposed
techniques, SingleInsert excels in single concept generation with high visual
fidelity while allowing flexible editing. Additionally, SingleInsert can
perform single-image novel view synthesis and multiple concepts composition
without requiring joint training. To facilitate evaluation, we design an
editing prompt list and introduce a metric named Editing Success Rate (ESR) for
quantitative assessment of editing flexibility. Our project page is:
https://jarrentwu1031.github.io/SingleInsert-web/
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08095" title="Abstract">arXiv:2310.08095</a> [<a href="/pdf/2310.08095" title="Download PDF">pdf</a>, <a href="/format/2310.08095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Satellite Cooperative Networks: Joint Hybrid Beamforming and User  Scheduling Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+M">Meixia Tao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaohu Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 13 figures. arXiv admin note: substantial text overlap with <a href="/abs/2301.03888">arXiv:2301.03888</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, we consider a cooperative communication network where multiple
low-Earth-orbit satellites provide services for ground users (GUs) (at the same
time and on the same frequency). The multi-satellite cooperative network has
great potential for satellite communications due to its dense configuration,
extensive coverage, and large spectral efficiency. However, the communication
and computational resources on satellites are usually restricted. Therefore,
considering the limitation of the on-board radio-frequency chains of
satellites, we first propose a hybrid beamforming method consisting of analog
beamforming for beam alignment and digital beamforming for interference
mitigation. Then, to establish appropriate connections between the satellites
and GUs, we propose a low-complexity heuristic user scheduling algorithm which
determines the connections according to the total spectral efficiency increment
of the multi-satellite cooperative network. Next, considering the intrinsic
connection between beamforming and user scheduling, a joint hybrid beamforming
and user scheduling (JHU) scheme is proposed to dramatically improve the
performance of the multi-satellite cooperative network. In addition to the
single-connection scenario, we also consider the multi-connection case using
the JHU scheme. Moreover, simulations are conducted to compare the proposed
schemes with representative baselines and to analyze the key factors
influencing the performance of the multi-satellite cooperative network.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08096" title="Abstract">arXiv:2310.08096</a> [<a href="/pdf/2310.08096" title="Download PDF">pdf</a>, <a href="/format/2310.08096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ClimateBERT-NetZero: Detecting and Assessing Net Zero and Reduction  Targets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schimanski%2C+T">Tobias Schimanski</a>, 
<a href="/search/cs?searchtype=author&query=Bingler%2C+J">Julia Bingler</a>, 
<a href="/search/cs?searchtype=author&query=Hyslop%2C+C">Camilla Hyslop</a>, 
<a href="/search/cs?searchtype=author&query=Kraus%2C+M">Mathias Kraus</a>, 
<a href="/search/cs?searchtype=author&query=Leippold%2C+M">Markus Leippold</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Public and private actors struggle to assess the vast amounts of information
about sustainability commitments made by various institutions. To address this
problem, we create a novel tool for automatically detecting corporate,
national, and regional net zero and reduction targets in three steps. First, we
introduce an expert-annotated data set with 3.5K text samples. Second, we train
and release ClimateBERT-NetZero, a natural language classifier to detect
whether a text contains a net zero or reduction target. Third, we showcase its
analysis potential with two use cases: We first demonstrate how
ClimateBERT-NetZero can be combined with conventional question-answering (Q&amp;A)
models to analyze the ambitions displayed in net zero and reduction targets.
Furthermore, we employ the ClimateBERT-NetZero model on quarterly earning call
transcripts and outline how communication patterns evolve over time. Our
experiments demonstrate promising pathways for extracting and analyzing net
zero and emission reduction targets at scale.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08097" title="Abstract">arXiv:2310.08097</a> [<a href="/pdf/2310.08097" title="Download PDF">pdf</a>, <a href="/format/2310.08097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sentinel: An Aggregation Function to Secure Decentralized Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Celdran%2C+A+H">Alberto Huertas Celdran</a>, 
<a href="/search/cs?searchtype=author&query=Baltensperger%2C+J">Janosch Baltensperger</a>, 
<a href="/search/cs?searchtype=author&query=Bertran%2C+E+T+M">Enrique Tomas Mat&#x131;nez Bertran</a>, 
<a href="/search/cs?searchtype=author&query=Bovet%2C+G">Gerome Bovet</a>, 
<a href="/search/cs?searchtype=author&query=Stiller%2C+B">Burkhard Stiller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The rapid integration of Federated Learning (FL) into networking encompasses
various aspects such as network management, quality of service, and
cybersecurity while preserving data privacy. In this context, Decentralized
Federated Learning (DFL) emerges as an innovative paradigm to train
collaborative models, addressing the single point of failure limitation.
However, the security and trustworthiness of FL and DFL are compromised by
poisoning attacks, negatively impacting its performance. Existing defense
mechanisms have been designed for centralized FL and they do not adequately
exploit the particularities of DFL. Thus, this work introduces Sentinel, a
defense strategy to counteract poisoning attacks in DFL. Sentinel leverages the
accessibility of local data and defines a three-step aggregation protocol
consisting of similarity filtering, bootstrap validation, and normalization to
safeguard against malicious model updates. Sentinel has been evaluated with
diverse datasets and various poisoning attack types and threat levels,
improving the state-of-the-art performance against both untargeted and targeted
poisoning attacks.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08099" title="Abstract">arXiv:2310.08099</a> [<a href="/pdf/2310.08099" title="Download PDF">pdf</a>, <a href="/format/2310.08099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using  Natural Language Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=K.%2C+A+K+T">Ajay Krishnan T. K.</a>, 
<a href="/search/cs?searchtype=author&query=Anoop%2C+V+S">V. S. Anoop</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Climate change's impact on human health poses unprecedented and diverse
challenges. Unless proactive measures based on solid evidence are implemented,
these threats will likely escalate and continue to endanger human well-being.
The escalating advancements in information and communication technologies have
facilitated the widespread availability and utilization of social media
platforms. Individuals utilize platforms such as Twitter and Facebook to
express their opinions, thoughts, and critiques on diverse subjects,
encompassing the pressing issue of climate change. The proliferation of climate
change-related content on social media necessitates comprehensive analysis to
glean meaningful insights. This paper employs natural language processing (NLP)
techniques to analyze climate change discourse and quantify the sentiment of
climate change-related tweets. We use ClimateBERT, a pretrained model
fine-tuned specifically for the climate change domain. The objective is to
discern the sentiment individuals express and uncover patterns in public
opinion concerning climate change. Analyzing tweet sentiments allows a deeper
comprehension of public perceptions, concerns, and emotions about this critical
global challenge. The findings from this experiment unearth valuable insights
into public sentiment and the entities associated with climate change
discourse. Policymakers, researchers, and organizations can leverage such
analyses to understand public perceptions, identify influential actors, and
devise informed strategies to address climate change challenges.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08100" title="Abstract">arXiv:2310.08100</a> [<a href="/pdf/2310.08100" title="Download PDF">pdf</a>, <a href="/format/2310.08100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Intrinsic Optimization: Intrisic Control with Model Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jianfei Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Future sequence represents the outcome after executing the action into the
environment. When driven by the information-theoretic concept of mutual
information, it seeks maximally informative consequences. Explicit outcomes may
vary across state, return, or trajectory serving different purposes such as
credit assignment or imitation learning. However, the inherent nature of
incorporating intrinsic motivation with reward maximization is often neglected.
In this work, we propose a variational approach to jointly learn the necessary
quantity for estimating the mutual information and the dynamics model,
providing a general framework for incorporating different forms of outcomes of
interest. Integrated into a policy iteration scheme, our approach guarantees
convergence to the optimal policy. While we mainly focus on theoretical
analysis, our approach opens the possibilities of leveraging intrinsic control
with model learning to enhance sample efficiency and incorporate uncertainty of
the environment into decision-making.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08101" title="Abstract">arXiv:2310.08101</a> [<a href="/pdf/2310.08101" title="Download PDF">pdf</a>, <a href="/format/2310.08101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Promptor: A Conversational and Autonomous Prompt Generation Agent for  Intelligent Text Entry Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Junxiao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Dudley%2C+J+J">John J. Dudley</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jingyao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Byrne%2C+B">Bill Byrne</a>, 
<a href="/search/cs?searchtype=author&query=Kristensson%2C+P+O">Per Ola Kristensson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Text entry is an essential task in our day-to-day digital interactions.
Numerous intelligent features have been developed to streamline this process,
making text entry more effective, efficient, and fluid. These improvements
include sentence prediction and user personalization. However, as deep
learning-based language models become the norm for these advanced features, the
necessity for data collection and model fine-tuning increases. These challenges
can be mitigated by harnessing the in-context learning capability of large
language models such as GPT-3.5. This unique feature allows the language model
to acquire new skills through prompts, eliminating the need for data collection
and fine-tuning. Consequently, large language models can learn various text
prediction techniques. We initially showed that, for a sentence prediction
task, merely prompting GPT-3.5 surpassed a GPT-2 backed system and is
comparable with a fine-tuned GPT-3.5 model, with the latter two methods
requiring costly data collection, fine-tuning and post-processing. However, the
task of prompting large language models to specialize in specific text
prediction tasks can be challenging, particularly for designers without
expertise in prompt engineering. To address this, we introduce Promptor, a
conversational prompt generation agent designed to engage proactively with
designers. Promptor can automatically generate complex prompts tailored to meet
specific needs, thus offering a solution to this challenge. We conducted a user
study involving 24 participants creating prompts for three intelligent text
entry tasks, half of the participants used Promptor while the other half
designed prompts themselves. The results show that Promptor-designed prompts
result in a 35% increase in similarity and 22% in coherence over those by
designers.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08102" title="Abstract">arXiv:2310.08102</a> [<a href="/pdf/2310.08102" title="Download PDF">pdf</a>, <a href="/format/2310.08102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QASiNa: Religious Domain Question Answering using Sirah Nabawiyah
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rizqullah%2C+M+R">Muhammad Razif Rizqullah</a> (1), 
<a href="/search/cs?searchtype=author&query=Purwarianti%2C+A">Ayu Purwarianti</a> (1), 
<a href="/search/cs?searchtype=author&query=Aji%2C+A+F">Alham Fikri Aji</a> (2) ((1) Bandung Institute of Technology, (2) Mohamed bin Zayed University of Artificial Intelligence)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 Pages. In Proceeding of 10th International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Nowadays, Question Answering (QA) tasks receive significant research focus,
particularly with the development of Large Language Model (LLM) such as Chat
GPT [1]. LLM can be applied to various domains, but it contradicts the
principles of information transmission when applied to the Islamic domain. In
Islam we strictly regulates the sources of information and who can give
interpretations or tafseer for that sources [2]. The approach used by LLM to
generate answers based on its own interpretation is similar to the concept of
tafseer, LLM is neither an Islamic expert nor a human which is not permitted in
Islam. Indonesia is the country with the largest Islamic believer population in
the world [3]. With the high influence of LLM, we need to make evaluation of
LLM in religious domain. Currently, there is only few religious QA dataset
available and none of them using Sirah Nabawiyah especially in Indonesian
Language. In this paper, we propose the Question Answering Sirah Nabawiyah
(QASiNa) dataset, a novel dataset compiled from Sirah Nabawiyah literatures in
Indonesian language. We demonstrate our dataset by using mBERT [4], XLM-R [5],
and IndoBERT [6] which fine-tuned with Indonesian translation of SQuAD v2.0
[7]. XLM-R model returned the best performance on QASiNa with EM of 61.20,
F1-Score of 75.94, and Substring Match of 70.00. We compare XLM-R performance
with Chat GPT-3.5 and GPT-4 [1]. Both Chat GPT version returned lower EM and
F1-Score with higher Substring Match, the gap of EM and Substring Match get
wider in GPT-4. The experiment indicate that Chat GPT tends to give excessive
interpretations as evidenced by its higher Substring Match scores compared to
EM and F1-Score, even after providing instruction and context. This concludes
Chat GPT is unsuitable for question answering task in religious domain
especially for Islamic religion.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08106" title="Abstract">arXiv:2310.08106</a> [<a href="/pdf/2310.08106" title="Download PDF">pdf</a>, <a href="/format/2310.08106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing  Label Bias in Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Beier Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+K">Kaihua Tang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qianru Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanwang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Foundation models like CLIP allow zero-shot transfer on various tasks without
additional training data. Yet, the zero-shot performance is less competitive
than a fully supervised one. Thus, to enhance the performance, fine-tuning and
ensembling are also commonly adopted to better fit the downstream tasks.
However, we argue that such prior work has overlooked the inherent biases in
foundation models. Due to the highly imbalanced Web-scale training set, these
foundation models are inevitably skewed toward frequent semantics, and thus the
subsequent fine-tuning or ensembling is still biased. In this study, we
systematically examine the biases in foundation models and demonstrate the
efficacy of our proposed Generalized Logit Adjustment (GLA) method. Note that
bias estimation in foundation models is challenging, as most pre-train data
cannot be explicitly accessed like in traditional long-tailed classification
tasks. To this end, GLA has an optimization-based bias estimation approach for
debiasing foundation models. As our work resolves a fundamental flaw in the
pre-training, the proposed GLA demonstrates significant improvements across a
diverse range of tasks: it achieves 1.5 pp accuracy gains on ImageNet, an large
average improvement (1.4-4.6 pp) on 11 few-shot datasets, 2.4 pp gains on
long-tailed classification. Codes are in \url{https://github.com/BeierZhu/GLA}.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08114" title="Abstract">arXiv:2310.08114</a> [<a href="/pdf/2310.08114" title="Download PDF">pdf</a>, <a href="/format/2310.08114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Modal Sensor Fusion and Object Tracking for Autonomous Racing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karle%2C+P">Phillip Karle</a>, 
<a href="/search/cs?searchtype=author&query=Fent%2C+F">Felix Fent</a>, 
<a href="/search/cs?searchtype=author&query=Huch%2C+S">Sebastian Huch</a>, 
<a href="/search/cs?searchtype=author&query=Sauerbeck%2C+F">Florian Sauerbeck</a>, 
<a href="/search/cs?searchtype=author&query=Lienkamp%2C+M">Markus Lienkamp</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Intelligent Vehicles, vol. 8, no. 7, pp.
  3871-3883, July 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Reliable detection and tracking of surrounding objects are indispensable for
comprehensive motion prediction and planning of autonomous vehicles. Due to the
limitations of individual sensors, the fusion of multiple sensor modalities is
required to improve the overall detection capabilities. Additionally, robust
motion tracking is essential for reducing the effect of sensor noise and
improving state estimation accuracy. The reliability of the autonomous vehicle
software becomes even more relevant in complex, adversarial high-speed
scenarios at the vehicle handling limits in autonomous racing. In this paper,
we present a modular multi-modal sensor fusion and tracking method for
high-speed applications. The method is based on the Extended Kalman Filter
(EKF) and is capable of fusing heterogeneous detection inputs to track
surrounding objects consistently. A novel delay compensation approach enables
to reduce the influence of the perception software latency and to output an
updated object list. It is the first fusion and tracking method validated in
high-speed real-world scenarios at the Indy Autonomous Challenge 2021 and the
Autonomous Challenge at CES (AC@CES) 2022, proving its robustness and
computational efficiency on embedded systems. It does not require any labeled
data and achieves position tracking residuals below 0.1 m. The related code is
available as open-source software at https://github.com/TUMFTM/FusionTracking.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08116" title="Abstract">arXiv:2310.08116</a> [<a href="/pdf/2310.08116" title="Download PDF">pdf</a>, <a href="/format/2310.08116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Active Measurement for Human Mesh Recovery in Close Proximity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maeda%2C+T">Takahiro Maeda</a>, 
<a href="/search/cs?searchtype=author&query=Takeshita%2C+K">Keisuke Takeshita</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+K">Kazuhito Tanaka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">For safe and sophisticated physical human-robot interactions (pHRI), a robot
needs to estimate the accurate body pose or mesh of the target person. However,
in these pHRI scenarios, the robot cannot fully observe the target person's
body with equipped cameras because the target person is usually close to the
robot. This leads to severe truncation and occlusions, and results in poor
accuracy of human pose estimation. For better accuracy of human pose estimation
or mesh recovery on this limited information from cameras, we propose an active
measurement and sensor fusion framework of the equipped cameras and other
sensors such as touch sensors and 2D LiDAR. These touch and LiDAR sensing are
obtained attendantly through pHRI without additional costs. These sensor
measurements are sparse but reliable and informative cues for human mesh
recovery. In our active measurement process, camera viewpoints and sensor
placements are optimized based on the uncertainty of the estimated pose, which
is closely related to the truncated or occluded areas. In our sensor fusion
process, we fuse the sensor measurements to the camera-based estimated pose by
minimizing the distance between the estimated mesh and measured positions. Our
method is agnostic to robot configurations. Experiments were conducted using
the Toyota Human Support Robot, which has a camera, 2D LiDAR, and a touch
sensor on the robot arm. Our proposed method demonstrated the superiority in
the human pose estimation accuracy on the quantitative comparison. Furthermore,
our proposed method reliably estimated the pose of the target person in
practical settings such as target people occluded by a blanket and standing aid
with the robot arm.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08117" title="Abstract">arXiv:2310.08117</a> [<a href="/pdf/2310.08117" title="Download PDF">pdf</a>, <a href="/format/2310.08117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DUSA: Decoupled Unsupervised Sim2Real Adaptation for  Vehicle-to-Everything Collaborative Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+X">Xianghao Kong</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wentao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinrang Jia</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yifeng Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Runsheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Si Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Vehicle-to-Everything (V2X) collaborative perception is crucial for
autonomous driving. However, achieving high-precision V2X perception requires a
significant amount of annotated real-world data, which can always be expensive
and hard to acquire. Simulated data have raised much attention since they can
be massively produced at an extremely low cost. Nevertheless, the significant
domain gap between simulated and real-world data, including differences in
sensor type, reflectance patterns, and road surroundings, often leads to poor
performance of models trained on simulated data when evaluated on real-world
data. In addition, there remains a domain gap between real-world collaborative
agents, e.g. different types of sensors may be installed on autonomous vehicles
and roadside infrastructures with different extrinsics, further increasing the
difficulty of sim2real generalization. To take full advantage of simulated
data, we present a new unsupervised sim2real domain adaptation method for V2X
collaborative detection named Decoupled Unsupervised Sim2Real Adaptation
(DUSA). Our new method decouples the V2X collaborative sim2real domain
adaptation problem into two sub-problems: sim2real adaptation and inter-agent
adaptation. For sim2real adaptation, we design a Location-adaptive Sim2Real
Adapter (LSA) module to adaptively aggregate features from critical locations
of the feature map and align the features between simulated data and real-world
data via a sim/real discriminator on the aggregated global feature. For
inter-agent adaptation, we further devise a Confidence-aware Inter-agent
Adapter (CIA) module to align the fine-grained features from heterogeneous
agents under the guidance of agent-wise confidence maps. Experiments
demonstrate the effectiveness of the proposed DUSA approach on unsupervised
sim2real adaptation from the simulated V2XSet dataset to the real-world
DAIR-V2X-C dataset.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08118" title="Abstract">arXiv:2310.08118</a> [<a href="/pdf/2310.08118" title="Download PDF">pdf</a>, <a href="/format/2310.08118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models Really Improve by Self-critiquing Their Own  Plans?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Valmeekam%2C+K">Karthik Valmeekam</a>, 
<a href="/search/cs?searchtype=author&query=Marquez%2C+M">Matthew Marquez</a>, 
<a href="/search/cs?searchtype=author&query=Kambhampati%2C+S">Subbarao Kambhampati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">There have been widespread claims about Large Language Models (LLMs) being
able to successfully verify or self-critique their candidate solutions in
reasoning problems in an iterative mode. Intrigued by those claims, in this
paper we set out to investigate the verification/self-critiquing abilities of
large language models in the context of planning. We evaluate a planning system
that employs LLMs for both plan generation and verification. We assess the
verifier LLM's performance against ground-truth verification, the impact of
self-critiquing on plan generation, and the influence of varying feedback
levels on system performance. Using GPT-4, a state-of-the-art LLM, for both
generation and verification, our findings reveal that self-critiquing appears
to diminish plan generation performance, especially when compared to systems
with external, sound verifiers and the LLM verifiers in that system produce a
notable number of false positives, compromising the system's reliability.
Additionally, the nature of feedback, whether binary or detailed, showed
minimal impact on plan generation. Collectively, our results cast doubt on the
effectiveness of LLMs in a self-critiquing, iterative framework for planning
tasks.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08122" title="Abstract">arXiv:2310.08122</a> [<a href="/pdf/2310.08122" title="Download PDF">pdf</a>, <a href="/format/2310.08122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Core-sets for Fair and Diverse Data Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahabadi%2C+S">Sepideh Mahabadi</a>, 
<a href="/search/cs?searchtype=author&query=Trajanovski%2C+S">Stojan Trajanovski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study core-set construction algorithms for the task of Diversity
Maximization under fairness/partition constraint. Given a set of points $P$ in
a metric space partitioned into $m$ groups, and given $k_1,\ldots,k_m$, the
goal of this problem is to pick $k_i$ points from each group $i$ such that the
overall diversity of the $k=\sum_i k_i$ picked points is maximized. We consider
two natural diversity measures: sum-of-pairwise distances and
sum-of-nearest-neighbor distances, and show improved core-set construction
algorithms with respect to these measures. More precisely, we show the first
constant factor core-set w.r.t. sum-of-pairwise distances whose size is
independent of the size of the dataset and the aspect ratio. Second, we show
the first core-set w.r.t. the sum-of-nearest-neighbor distances. Finally, we
run several experiments showing the effectiveness of our core-set approach. In
particular, we apply constrained diversity maximization to summarize a set of
timed messages that takes into account the messages' recency. Specifically, the
summary should include more recent messages compared to older ones. This is a
real task in one of the largest communication platforms, affecting the
experience of hundreds of millions daily active users. By utilizing our
core-set method for this task, we achieve a 100x speed-up while losing the
diversity by only a few percent. Moreover, our approach allows us to improve
the space usage of the algorithm in the streaming setting.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08123" title="Abstract">arXiv:2310.08123</a> [<a href="/pdf/2310.08123" title="Download PDF">pdf</a>, <a href="/format/2310.08123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Who Wrote it and Why? Prompting Large-Language Models for Authorship  Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hung%2C+C">Chia-Yu Hung</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhiqiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yujia Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+R+K">Roy Ka-Wei Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages,1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Authorship verification (AV) is a fundamental task in natural language
processing (NLP) and computational linguistics, with applications in forensic
analysis, plagiarism detection, and identification of deceptive content.
Existing AV techniques, including traditional stylometric and deep learning
approaches, face limitations in terms of data requirements and lack of
explainability. To address these limitations, this paper proposes PromptAV, a
novel technique that leverages Large-Language Models (LLMs) for AV by providing
step-by-step stylometric explanation prompts. PromptAV outperforms
state-of-the-art baselines, operates effectively with limited training data,
and enhances interpretability through intuitive explanations, showcasing its
potential as an effective and interpretable solution for the AV task.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08126" title="Abstract">arXiv:2310.08126</a> [<a href="/pdf/2310.08126" title="Download PDF">pdf</a>, <a href="/format/2310.08126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel Newton method for inverse elastic scattering problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chang%2C+Y">Yan Chang</a>, 
<a href="/search/math?searchtype=author&query=Guo%2C+Y">Yukun Guo</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+H">Hongyu Liu</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+D">Deyue Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This work is concerned with an inverse elastic scattering problem of
identifying the unknown rigid obstacle embedded in an open space filled with a
homogeneous and isotropic elastic medium. A Newton-type iteration method
relying on the boundary condition is designed to identify the boundary curve of
the obstacle. Based on the Helmholtz decomposition and the Fourier-Bessel
expansion, we explicitly derive the approximate scattered field and its
derivative on each iterative curve. Rigorous mathematical justifications for
the proposed method are provided. Numerical examples are presented to verify
the effectiveness of the proposed method.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08129" title="Abstract">arXiv:2310.08129</a> [<a href="/pdf/2310.08129" title="Download PDF">pdf</a>, <a href="/format/2310.08129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tailored Visions: Enhancing Text-to-Image Generation with Personalized  Prompt Rewriting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zijie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lichao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+F">Fangsheng Weng</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Lili Pan</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Z">Zhenzhong Lan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose a novel perspective of viewing large pretrained models as search
engines, thereby enabling the repurposing of techniques previously used to
enhance search engine performance. As an illustration, we employ a personalized
query rewriting technique in the realm of text-to-image generation. Despite
significant progress in the field, it is still challenging to create
personalized visual representations that align closely with the desires and
preferences of individual users. This process requires users to articulate
their ideas in words that are both comprehensible to the models and accurately
capture their vision, posing difficulties for many users. In this paper, we
tackle this challenge by leveraging historical user interactions with the
system to enhance user prompts. We propose a novel approach that involves
rewriting user prompts based a new large-scale text-to-image dataset with over
300k prompts from 3115 users. Our rewriting model enhances the expressiveness
and alignment of user prompts with their intended visual outputs. Experimental
results demonstrate the superiority of our methods over baseline approaches, as
evidenced in our new offline evaluation method and online tests. Our approach
opens up exciting possibilities of applying more search engine techniques to
build truly personalized large pretrained models.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08130" title="Abstract">arXiv:2310.08130</a> [<a href="/pdf/2310.08130" title="Download PDF">pdf</a>, <a href="/format/2310.08130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-grained Conversational Decoding via Isotropic and Proximal Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuxuan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Han Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qiling Xu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linqi Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in EMNLP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">General-purpose text decoding approaches are usually adopted for dialogue
response generation. Although the quality of the generated responses can be
improved with dialogue-specific encoding methods, conversational decoding
methods are still under-explored. Inspired by \citet{wu2023learning} that a
good dialogue feature space should follow the rules of locality and isotropy,
we present a fine-grained conversational decoding method, termed
\textit{isotropic and proximal search (IPS)}. Our method is designed to
generate the semantic-concentrated response, while still maintaining
informativeness and discrimination against the context. Experiments show that
our approach outperforms existing decoding strategies in the dialogue field
across both automatic and human evaluation metrics. More in-depth analyses
further confirm the effectiveness of our approach.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08132" title="Abstract">arXiv:2310.08132</a> [<a href="/pdf/2310.08132" title="Download PDF">pdf</a>, <a href="/format/2310.08132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Relevance of Phoneme Duration Variability of Synthesized Training  Data for Automatic Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rossenbach%2C+N">Nick Rossenbach</a>, 
<a href="/search/cs?searchtype=author&query=Hilmes%2C+B">Benedikt Hilmes</a>, 
<a href="/search/cs?searchtype=author&query=Schl%C3%BCter%2C+R">Ralf Schl&#xfc;ter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Synthetic data generated by text-to-speech (TTS) systems can be used to
improve automatic speech recognition (ASR) systems in low-resource or domain
mismatch tasks. It has been shown that TTS-generated outputs still do not have
the same qualities as real data. In this work we focus on the temporal
structure of synthetic data and its relation to ASR training. By using a novel
oracle setup we show how much the degradation of synthetic data quality is
influenced by duration modeling in non-autoregressive (NAR) TTS. To get
reference phoneme durations we use two common alignment methods, a hidden
Markov Gaussian-mixture model (HMM-GMM) aligner and a neural connectionist
temporal classification (CTC) aligner. Using a simple algorithm based on random
walks we shift phoneme duration distributions of the TTS system closer to real
durations, resulting in an improvement of an ASR system using synthetic data in
a semi-supervised setting.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08133" title="Abstract">arXiv:2310.08133</a> [<a href="/pdf/2310.08133" title="Download PDF">pdf</a>, <a href="/format/2310.08133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi Level Dense Layer Neural Network Model for Housing Price  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wijaya%2C+R">Robert Wijaya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Predicting the price of a house remains a challenging issue that needs to be
addressed. Research has attempted to establish a model with different methods
and algorithms to predict the housing price, from the traditional hedonic model
to a neural network algorithm. However, many existing algorithms in the
literature are proposed without any finetuning and customization in the model.
In this paper, the author attempted to propose a novel neural network-based
model to improve the performance of housing price prediction. Inspired by the
modular neural network, the proposed model consists of a three-level neural
network that is capable to process information in parallel. The author compared
several state-of-the-art algorithms available in the literature on the Boston
housing dataset to evaluate the effectiveness of the proposed model. The
results show that the proposed model provides better accuracy and outperforms
existing algorithms in different evaluation metrics. The code for the
implementation is available
https://github.com/wijayarobert/MultiLevelDenseLayerNN
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08137" title="Abstract">arXiv:2310.08137</a> [<a href="/pdf/2310.08137" title="Download PDF">pdf</a>, <a href="/format/2310.08137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Explanations for Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhendong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Miliou%2C+I">Ioanna Miliou</a>, 
<a href="/search/cs?searchtype=author&query=Samsten%2C+I">Isak Samsten</a>, 
<a href="/search/cs?searchtype=author&query=Papapetrou%2C+P">Panagiotis Papapetrou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures. Accepted by ICDM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Among recent developments in time series forecasting methods, deep
forecasting models have gained popularity as they can utilize hidden feature
patterns in time series to improve forecasting performance. Nevertheless, the
majority of current deep forecasting models are opaque, hence making it
challenging to interpret the results. While counterfactual explanations have
been extensively employed as a post-hoc approach for explaining classification
models, their application to forecasting models still remains underexplored. In
this paper, we formulate the novel problem of counterfactual generation for
time series forecasting, and propose an algorithm, called ForecastCF, that
solves the problem by applying gradient-based perturbations to the original
time series. ForecastCF guides the perturbations by applying constraints to the
forecasted values to obtain desired prediction outcomes. We experimentally
evaluate ForecastCF using four state-of-the-art deep model architectures and
compare to two baselines. Our results show that ForecastCF outperforms the
baseline in terms of counterfactual validity and data manifold closeness.
Overall, our findings suggest that ForecastCF can generate meaningful and
relevant counterfactual explanations for various forecasting tasks.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08138" title="Abstract">arXiv:2310.08138</a> [<a href="/pdf/2310.08138" title="Download PDF">pdf</a>, <a href="/format/2310.08138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Scale Spatial-Temporal Recurrent Networks for Traffic Flow  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haiyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chunjiang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Detian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Traffic flow prediction is one of the most fundamental tasks of intelligent
transportation systems. The complex and dynamic spatial-temporal dependencies
make the traffic flow prediction quite challenging. Although existing
spatial-temporal graph neural networks hold prominent, they often encounter
challenges such as (1) ignoring the fixed graph that limits the predictive
performance of the model, (2) insufficiently capturing complex spatial-temporal
dependencies simultaneously, and (3) lacking attention to spatial-temporal
information at different time lengths. In this paper, we propose a Multi-Scale
Spatial-Temporal Recurrent Network for traffic flow prediction, namely MSSTRN,
which consists of two different recurrent neural networks: the single-step gate
recurrent unit and the multi-step gate recurrent unit to fully capture the
complex spatial-temporal information in the traffic data under different time
windows. Moreover, we propose a spatial-temporal synchronous attention
mechanism that integrates adaptive position graph convolutions into the
self-attention mechanism to achieve synchronous capture of spatial-temporal
dependencies. We conducted extensive experiments on four real traffic datasets
and demonstrated that our model achieves the best prediction accuracy with
non-trivial margins compared to all the twenty baseline methods.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08139" title="Abstract">arXiv:2310.08139</a> [<a href="/pdf/2310.08139" title="Download PDF">pdf</a>, <a href="/format/2310.08139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DualAug: Exploiting Additional Heavy Augmentation with OOD Data  Rejection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zehao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yiwen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qizhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Guanglei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+W">Wangmeng Zuo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Data augmentation is a dominant method for reducing model overfitting and
improving generalization. Most existing data augmentation methods tend to find
a compromise in augmenting the data, \textit{i.e.}, increasing the amplitude of
augmentation carefully to avoid degrading some data too much and doing harm to
the model performance. We delve into the relationship between data augmentation
and model performance, revealing that the performance drop with heavy
augmentation comes from the presence of out-of-distribution (OOD) data.
Nonetheless, as the same data transformation has different effects for
different training samples, even for heavy augmentation, there remains part of
in-distribution data which is beneficial to model training. Based on the
observation, we propose a novel data augmentation method, named
\textbf{DualAug}, to keep the augmentation in distribution as much as possible
at a reasonable time and computational cost. We design a data mixing strategy
to fuse augmented data from both the basic- and the heavy-augmentation
branches. Extensive experiments on supervised image classification benchmarks
show that DualAug improve various automated data augmentation method. Moreover,
the experiments on semi-supervised learning and contrastive self-supervised
learning demonstrate that our DualAug can also improve related method. Code is
available at
\href{https://github.com/shuguang99/DualAug}{https://github.com/shuguang99/DualAug}.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08140" title="Abstract">arXiv:2310.08140</a> [<a href="/pdf/2310.08140" title="Download PDF">pdf</a>, <a href="/format/2310.08140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CODY: A graph-based framework for the analysis of COnversation DYnamics  in online social networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziegler%2C+J">John Ziegler</a>, 
<a href="/search/cs?searchtype=author&query=Kneissl%2C+F">Fabian Kneissl</a>, 
<a href="/search/cs?searchtype=author&query=Gertz%2C+M">Michael Gertz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Conversations are an integral part of online social media, and gaining
insights into these conversations is of significant value for many commercial
as well as academic use cases. From a computational perspective, however,
analyzing conversation data is complex, and numerous aspects must be
considered. Next to the structure of conversations, the discussed content - as
well as their dynamics - have to be taken into account. Still, most existing
modelling and analysis approaches focus only on one of these aspects and, in
particular, lack the capability to investigate the temporal evolution of a
conversation. To address these shortcomings, in this work, we present CODY, a
content-aware, graph-based framework to study the dynamics of online
conversations along multiple dimensions. Its capabilities are extensively
demonstrated by conducting three experiments based on a large conversation
dataset from the German political Twittersphere. First, the posting activity
across the lifetime of conversations is examined. We find that posting activity
follows an exponential saturation pattern. Based on this activity model, we
develop a volume-based sampling method to study conversation dynamics using
temporal network snapshots. In a second experiment, we focus on the evolution
of a conversation's structure and leverage a novel metric, the temporal Wiener
index, for that. Results indicate that as conversations progress, a
conversation's structure tends to be less sprawling and more centered around
the original seed post. Furthermore, focusing on the dynamics of content in
conversations, the evolution of hashtag usage within conversations is studied.
Initially used hashtags do not necessarily keep their dominant prevalence
throughout the lifetime of a conversation. Instead, various "hashtag hijacking"
scenarios are found.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08142" title="Abstract">arXiv:2310.08142</a> [<a href="/pdf/2310.08142" title="Download PDF">pdf</a>, <a href="/format/2310.08142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-Grained Annotation for Face Anti-Spoofing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yunde Jia</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuwei Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Face anti-spoofing plays a critical role in safeguarding facial recognition
systems against presentation attacks. While existing deep learning methods show
promising results, they still suffer from the lack of fine-grained annotations,
which lead models to learn task-irrelevant or unfaithful features. In this
paper, we propose a fine-grained annotation method for face anti-spoofing.
Specifically, we first leverage the Segment Anything Model (SAM) to obtain
pixel-wise segmentation masks by utilizing face landmarks as point prompts. The
face landmarks provide segmentation semantics, which segments the face into
regions. We then adopt these regions as masks and assemble them into three
separate annotation maps: spoof, living, and background maps. Finally, we
combine three separate maps into a three-channel map as annotations for model
training. Furthermore, we introduce the Multi-Channel Region Exchange
Augmentation (MCREA) to diversify training data and reduce overfitting.
Experimental results demonstrate that our method outperforms existing
state-of-the-art approaches in both intra-dataset and cross-dataset
evaluations.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08143" title="Abstract">arXiv:2310.08143</a> [<a href="/pdf/2310.08143" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Deep Learning Framework for Spatiotemporal Ultrasound Localization  Microscopy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Milecki%2C+L">L&#xe9;o Milecki</a>, 
<a href="/search/cs?searchtype=author&query=Por%C3%A9e%2C+J">Jonathan Por&#xe9;e</a>, 
<a href="/search/cs?searchtype=author&query=Belgharbi%2C+H">Hatim Belgharbi</a>, 
<a href="/search/cs?searchtype=author&query=Bourquin%2C+C">Chlo&#xe9; Bourquin</a>, 
<a href="/search/cs?searchtype=author&query=Damseh%2C+R">Rafat Damseh</a>, 
<a href="/search/cs?searchtype=author&query=Delafontaine-Martel%2C+P">Patrick Delafontaine-Martel</a>, 
<a href="/search/cs?searchtype=author&query=Lesage%2C+F">Fr&#xe9;d&#xe9;ric Lesage</a>, 
<a href="/search/cs?searchtype=author&query=Gasse%2C+M">Maxime Gasse</a>, 
<a href="/search/cs?searchtype=author&query=Provost%2C+J">Jean Provost</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Copyright 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Medical Imaging (Volume: 40, Issue: 5, May
  2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Ultrasound Localization Microscopy can resolve the microvascular bed down to
a few micrometers. To achieve such performance microbubble contrast agents must
perfuse the entire microvascular network. Microbubbles are then located
individually and tracked over time to sample individual vessels, typically over
hundreds of thousands of images. To overcome the fundamental limit of
diffraction and achieve a dense reconstruction of the network, low microbubble
concentrations must be used, which lead to acquisitions lasting several
minutes. Conventional processing pipelines are currently unable to deal with
interference from multiple nearby microbubbles, further reducing achievable
concentrations. This work overcomes this problem by proposing a Deep Learning
approach to recover dense vascular networks from ultrasound acquisitions with
high microbubble concentrations. A realistic mouse brain microvascular network,
segmented from 2-photon microscopy, was used to train a three-dimensional
convolutional neural network based on a V-net architecture. Ultrasound data
sets from multiple microbubbles flowing through the microvascular network were
simulated and used as ground truth to train the 3D CNN to track microbubbles.
The 3D-CNN approach was validated in silico using a subset of the data and in
vivo on a rat brain acquisition. In silico, the CNN reconstructed vascular
networks with higher precision (81%) than a conventional ULM framework (70%).
In vivo, the CNN could resolve micro vessels as small as 10 $\mu$m with an
increase in resolution when compared against a conventional approach.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08147" title="Abstract">arXiv:2310.08147</a> [<a href="/pdf/2310.08147" title="Download PDF">pdf</a>, <a href="/format/2310.08147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Client Selection of Federated Learning under Device and Data  Heterogeneity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shuaijun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tavallaie%2C+O">Omid Tavallaie</a>, 
<a href="/search/cs?searchtype=author&query=Hambali%2C+M+H">Michael Henri Hambali</a>, 
<a href="/search/cs?searchtype=author&query=Zandavi%2C+S+M">Seid Miad Zandavi</a>, 
<a href="/search/cs?searchtype=author&query=Haddadi%2C+H">Hamed Haddadi</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Song Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zomaya%2C+A+Y">Albert Y. Zomaya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Federated learning (FL) is a promising distributed learning framework
designed for privacy-aware applications of resource-constrained devices.
Without sharing data, FL trains a model on each device locally and builds the
global model on the server by aggregating the trained models. To reduce the
communication overhead, only a portion of client devices participate in each
round of training. Random selection is the most common way of selecting client
devices for training data in a round of FL. However, random client selection
uses distributed data and computational resources inefficiently, as it does not
take into account the hardware specifications and data distribution among
clients. This paper proposes FedGRA, an adaptive fair client selection
algorithm designed for FL applications with unbalanced, non-Identically and
Independently Distributed (IID) data running on client devices with
heterogeneous computing resources. FedGRA dynamically adjusts the set of
selected clients at each round of training based on clients' trained models and
their available computational resources. To find an optimal solution, we model
the client selection problem of FL as a multi-objective optimization by using
Grey Relational Analysis (GRA) theory. To examine the performance of our
proposed method, we implement our contribution on Amazon Web Services (AWS) by
using 50 Elastic Compute Cloud (EC2) instances with 4 different hardware
configurations. The evaluation results reveal that our contribution improves
convergence significantly and reduces the average client's waiting time
compared to state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08148" title="Abstract">arXiv:2310.08148</a> [<a href="/pdf/2310.08148" title="Download PDF">pdf</a>, <a href="/format/2310.08148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-Set Knowledge-Based Visual Question Answering with Inference Paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gan%2C+J">Jingru Gan</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xinzhe Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuhui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qingming Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Given an image and an associated textual question, the purpose of
Knowledge-Based Visual Question Answering (KB-VQA) is to provide a correct
answer to the question with the aid of external knowledge bases. Prior KB-VQA
models are usually formulated as a retriever-classifier framework, where a
pre-trained retriever extracts textual or visual information from knowledge
graphs and then makes a prediction among the candidates. Despite promising
progress, there are two drawbacks with existing models. Firstly, modeling
question-answering as multi-class classification limits the answer space to a
preset corpus and lacks the ability of flexible reasoning. Secondly, the
classifier merely consider "what is the answer" without "how to get the
answer", which cannot ground the answer to explicit reasoning paths. In this
paper, we confront the challenge of \emph{explainable open-set} KB-VQA, where
the system is required to answer questions with entities at wild and retain an
explainable reasoning path. To resolve the aforementioned issues, we propose a
new retriever-ranker paradigm of KB-VQA, Graph pATH rankER (GATHER for
brevity). Specifically, it contains graph constructing, pruning, and path-level
ranking, which not only retrieves accurate answers but also provides inference
paths that explain the reasoning process. To comprehensively evaluate our
model, we reformulate the benchmark dataset OK-VQA with manually corrected
entity-level annotations and release it as ConceptVQA. Extensive experiments on
real-world questions demonstrate that our framework is not only able to perform
open-set question answering across the whole knowledge base but provide
explicit reasoning path.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08152" title="Abstract">arXiv:2310.08152</a> [<a href="/pdf/2310.08152" title="Download PDF">pdf</a>, <a href="/format/2310.08152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context Compression for Auto-regressive Transformers with Sentinel  Tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Siyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Q">Qi Jia</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K+Q">Kenny Q. Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The quadratic complexity of the attention module makes it gradually become
the bulk of compute in Transformer-based LLMs during generation. Moreover, the
excessive key-value cache that arises when dealing with long inputs also brings
severe issues on memory footprint and inference latency. In this work, we
propose a plug-and-play approach that is able to incrementally compress the
intermediate activation of a specified span of tokens into compact ones,
thereby reducing both memory and computational cost when processing subsequent
context. Experiments on both in-domain language modeling and zero-shot
open-ended document generation demonstrate the advantage of our approach over
sparse attention baselines in terms of fluency, n-gram matching, and semantic
similarity. At last, we comprehensively profile the benefit of context
compression on improving the system throughout. Code is available at
https://github.com/DRSY/KV_Compression.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08153" title="Abstract">arXiv:2310.08153</a> [<a href="/pdf/2310.08153" title="Download PDF">pdf</a>, <a href="/format/2310.08153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Evaluation of Automated Tools for Side-Channel  Vulnerabilities Detection in Cryptographic Libraries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geimer%2C+A">Antoine Geimer</a>, 
<a href="/search/cs?searchtype=author&query=Vergnolle%2C+M">Math&#xe9;o Vergnolle</a>, 
<a href="/search/cs?searchtype=author&query=Recoules%2C+F">Fr&#xe9;d&#xe9;ric Recoules</a>, 
<a href="/search/cs?searchtype=author&query=Daniel%2C+L">Lesly-Ann Daniel</a>, 
<a href="/search/cs?searchtype=author&query=Bardin%2C+S">S&#xe9;bastien Bardin</a>, 
<a href="/search/cs?searchtype=author&query=Maurice%2C+C">Cl&#xe9;mentine Maurice</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, Accepted to ACM CCS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">To protect cryptographic implementations from side-channel vulnerabilities,
developers must adopt constant-time programming practices. As these can be
error-prone, many side-channel detection tools have been proposed. Despite
this, such vulnerabilities are still manually found in cryptographic libraries.
While a recent paper by Jancar et al. shows that developers rarely perform
side-channel detection, it is unclear if existing detection tools could have
found these vulnerabilities in the first place. To answer this question, we
surveyed the literature to build a classification of 34 side-channel detection
frameworks. The classification we offer compares multiple criteria, including
the methods used, the scalability of the analysis or the threat model
considered. We then built a unified common benchmark of representative
cryptographic operations on a selection of 5 promising detection tools. This
benchmark allows us to better compare the capabilities of each tool, and the
scalability of their analysis. Additionally, we offer a classification of
recently published side-channel vulnerabilities. We then test each of the
selected tools on benchmarks reproducing a subset of these vulnerabilities as
well as the context in which they appear. We find that existing tools can
struggle to find vulnerabilities for a variety of reasons, mainly the lack of
support for SIMD instructions, implicit flows, and internal secret generation.
Based on our findings, we develop a set of recommendations for the research
community and cryptographic library developers, with the goal to improve the
effectiveness of side-channel detection tools.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08157" title="Abstract">arXiv:2310.08157</a> [<a href="/pdf/2310.08157" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MCRepair: Multi-Chunk Program Repair via Patch Optimization with Buggy  Block
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jisung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Byeongjung Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> This is the revised manuscript of the conference paper published
  in the 38th ACM/SIGAPP Symposium on Applied Computing (SAC 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Automated program repair (APR) is a technology that identifies and repairs
bugs automatically. However, repairing multi-chunk bugs remains a long-standing
and challenging problem because an APR technique must consider dependencies and
then reduce the large patch space. In addition, little is known about how to
combine individual candidate patches even though multi-chunk bugs require
combinations. Therefore, we propose a novel APR technique called multi-code
repair (MCRepair), which applies a buggy block, patch optimization, and
CodeBERT to target multi-chunk bugs. A buggy block is a novel method that binds
buggy chunks into a multi-buggy chunk and preprocesses the chunk with its buggy
contexts for patch space reduction and dependency problems. Patch optimization
is a novel strategy that effectively combines the generated candidate patches
with patch space reduction. In addition, CodeBERT, a BERT for source code
datasets, is fine-tuned to address the lack of datasets and out-of-vocabulary
problems. We conducted several experiments to evaluate our approach on six
project modules of Defects4J. In the experiments using Defects4J, MCRepair
repaired 65 bugs, including 21 multi-chunk bugs. Moreover, it fixed 18 unique
bugs, including eight multi-chunk bugs, and improved 40 to 250 percent
performance than the baselines.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08163" title="Abstract">arXiv:2310.08163</a> [<a href="/pdf/2310.08163" title="Download PDF">pdf</a>, <a href="/format/2310.08163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Decentralized IDentifiers with Proof of Membership to Enable  Trust in IoT Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pino%2C+A">Alessandro Pino</a>, 
<a href="/search/cs?searchtype=author&query=Margaria%2C+D">Davide Margaria</a>, 
<a href="/search/cs?searchtype=author&query=Vesco%2C+A">Andrea Vesco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The Self-Sovereign Identity (SSI) is a decentralized paradigm enabling full
control over the data used to build and prove the identity. In Internet of
Things networks with security requirements, the Self-Sovereign Identity can
play a key role and bring benefits with respect to centralized identity
solutions. The challenge is to make the SSI compatible with resource-constraint
IoT networks. In line with this objective, the paper proposes and discusses an
alternative (mutual) authentication process for IoT nodes under the same
administration domain. The main idea is to combine the Decentralized IDentifier
(DID)-based verification of private key ownership with the verification of a
proof that the DID belongs to an evolving trusted set. The solution is built
around the proof of membership notion. The paper analyzes two membership
solutions, a novel solution designed by the Authors based on Merkle trees and a
second one based on the adaptation of Boneh, Boyen and Shacham (BBS) group
signature scheme. The paper concludes with a performance estimation and a
comparative analysis.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08164" title="Abstract">arXiv:2310.08164</a> [<a href="/pdf/2310.08164" title="Download PDF">pdf</a>, <a href="/format/2310.08164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse  Autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marks%2C+L">Luke Marks</a>, 
<a href="/search/cs?searchtype=author&query=Abdullah%2C+A">Amir Abdullah</a>, 
<a href="/search/cs?searchtype=author&query=Mendez%2C+L">Luna Mendez</a>, 
<a href="/search/cs?searchtype=author&query=Arike%2C+R">Rauno Arike</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P">Philip Torr</a>, 
<a href="/search/cs?searchtype=author&query=Barez%2C+F">Fazl Barez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large language models (LLMs) aligned to human preferences via reinforcement
learning from human feedback (RLHF) underpin many commercial applications.
However, how RLHF impacts LLM internals remains opaque. We propose a novel
method to interpret learned reward functions in RLHF-tuned LLMs using sparse
autoencoders. Our approach trains autoencoder sets on activations from a base
LLM and its RLHF-tuned version. By comparing autoencoder hidden spaces, we
identify unique features that reflect the accuracy of the learned reward model.
To quantify this, we construct a scenario where the tuned LLM learns
token-reward mappings to maximize reward. This is the first application of
sparse autoencoders for interpreting learned rewards and broadly inspecting
reward learning in LLMs. Our method provides an abstract approximation of
reward integrity. This presents a promising technique for ensuring alignment
between specified objectives and model behaviors.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08166" title="Abstract">arXiv:2310.08166</a> [<a href="/pdf/2310.08166" title="Download PDF">pdf</a>, <a href="/format/2310.08166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ziya-VL: Bilingual Large Vision-Language Model via Multi-Task  Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Junyu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dixiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaojun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xinyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+R">Ruyi Gan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yan Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pingjian Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent advancements enlarge the capabilities of large language models (LLMs)
in zero-shot image-to-text generation and understanding by integrating
multi-modal inputs. However, such success is typically limited to English
scenarios due to the lack of large-scale and high-quality non-English
multi-modal resources, making it extremely difficult to establish competitive
counterparts in other languages. In this paper, we introduce the Ziya-VL
series, a set of bilingual large-scale vision-language models (LVLMs) designed
to incorporate visual semantics into LLM for multi-modal dialogue. Composed of
Ziya-VL-Base and Ziya-VL-Chat, our models adopt the Querying Transformer from
BLIP-2, further exploring the assistance of optimization schemes such as
instruction tuning, multi-stage training and low-rank adaptation module for
visual-language alignment. In addition, we stimulate the understanding ability
of GPT-4 in multi-modal scenarios, translating our gathered English image-text
datasets into Chinese and generating instruction-response through the
in-context learning method. The experiment results demonstrate that compared to
the existing LVLMs, Ziya-VL achieves competitive performance across a wide
range of English-only tasks including zero-shot image-text retrieval, image
captioning, and visual question answering. The evaluation leaderboard accessed
by GPT-4 also indicates that our models possess satisfactory image-text
understanding and generation capabilities in Chinese multi-modal scenario
dialogues. Code, demo and models are available at
~\url{https://huggingface.co/IDEA-CCNL/Ziya-BLIP2-14B-Visual-v1}.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08167" title="Abstract">arXiv:2310.08167</a> [<a href="/pdf/2310.08167" title="Download PDF">pdf</a>, <a href="/format/2310.08167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiclass Classification of Policy Documents with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gunes%2C+E">Erkan Gunes</a>, 
<a href="/search/cs?searchtype=author&query=Florczak%2C+C+K">Christoffer Koch Florczak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Classifying policy documents into policy issue topics has been a long-time
effort in political science and communication disciplines. Efforts to automate
text classification processes for social science research purposes have so far
achieved remarkable results, but there is still a large room for progress. In
this work, we test the prediction performance of an alternative strategy, which
requires human involvement much less than full manual coding. We use the GPT
3.5 and GPT 4 models of the OpenAI, which are pre-trained instruction-tuned
Large Language Models (LLM), to classify congressional bills and congressional
hearings into Comparative Agendas Project's 21 major policy issue topics. We
propose three use-case scenarios and estimate overall accuracies ranging from
%58-83 depending on scenario and GPT model employed. The three scenarios aims
at minimal, moderate, and major human interference, respectively. Overall, our
results point towards the insufficiency of complete reliance on GPT with
minimal human intervention, an increasing accuracy along with the human effort
exerted, and a surprisingly high accuracy achieved in the most humanly
demanding use-case. However, the superior use-case achieved the %83 accuracy on
the %65 of the data in which the two models agreed, suggesting that a similar
approach to ours can be relatively easily implemented and allow for mostly
automated coding of a majority of a given dataset. This could free up resources
allowing manual human coding of the remaining %35 of the data to achieve an
overall higher level of accuracy while reducing costs significantly.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08170" title="Abstract">arXiv:2310.08170</a> [<a href="/pdf/2310.08170" title="Download PDF">pdf</a>, <a href="/format/2310.08170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simplicity Level Estimate (SLE): A Learned Reference-Less Metric for  Sentence Simplification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cripwell%2C+L">Liam Cripwell</a>, 
<a href="/search/cs?searchtype=author&query=Legrand%2C+J">Jo&#xeb;l Legrand</a>, 
<a href="/search/cs?searchtype=author&query=Gardent%2C+C">Claire Gardent</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Main Conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automatic evaluation for sentence simplification remains a challenging
problem. Most popular evaluation metrics require multiple high-quality
references -- something not readily available for simplification -- which makes
it difficult to test performance on unseen domains. Furthermore, most existing
metrics conflate simplicity with correlated attributes such as fluency or
meaning preservation. We propose a new learned evaluation metric (SLE) which
focuses on simplicity, outperforming almost all existing metrics in terms of
correlation with human judgements.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08172" title="Abstract">arXiv:2310.08172</a> [<a href="/pdf/2310.08172" title="Download PDF">pdf</a>, <a href="/format/2310.08172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Cognitive Knowledge Structure of Large Language Models: An  Educational Diagnostic Assessment Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jifan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanzi Li</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Lei Hou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023 (Short Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have not only exhibited exceptional performance
across various tasks, but also demonstrated sparks of intelligence. Recent
studies have focused on assessing their capabilities on human exams and
revealed their impressive competence in different domains. However, cognitive
research on the overall knowledge structure of LLMs is still lacking. In this
paper, based on educational diagnostic assessment method, we conduct an
evaluation using MoocRadar, a meticulously annotated human test dataset based
on Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain
insights of their cognitive capabilities. This research emphasizes the
significance of investigating LLMs' knowledge and understanding the disparate
cognitive patterns of LLMs. By shedding light on models' knowledge, researchers
can advance development and utilization of LLMs in a more informed and
effective manner.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08176" title="Abstract">arXiv:2310.08176</a> [<a href="/pdf/2310.08176" title="Download PDF">pdf</a>, <a href="/format/2310.08176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Infinite Width Graph Neural Networks for Node Regression/ Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cobanoglu%2C+Y">Yunus Cobanoglu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 Pages, 2 Figures (with subfigures)o, multiple tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This work analyzes Graph Neural Networks, a generalization of Fully-Connected
Deep Neural Nets on Graph structured data, when their width, that is the number
of nodes in each fullyconnected layer is increasing to infinity. Infinite Width
Neural Networks are connecting Deep Learning to Gaussian Processes and Kernels,
both Machine Learning Frameworks with long traditions and extensive theoretical
foundations. Gaussian Processes and Kernels have much less hyperparameters then
Neural Networks and can be used for uncertainty estimation, making them more
user friendly for applications. This works extends the increasing amount of
research connecting Gaussian Processes and Kernels to Neural Networks. The
Kernel and Gaussian Process closed forms are derived for a variety of
architectures, namely the standard Graph Neural Network, the Graph Neural
Network with Skip-Concatenate Connections and the Graph Attention Neural
Network. All architectures are evaluated on a variety of datasets on the task
of transductive Node Regression and Classification. Additionally, a Spectral
Sparsification method known as Effective Resistance is used to improve runtime
and memory requirements. Extending the setting to inductive graph learning
tasks (Graph Regression/ Classification) is straightforward and is briefly
discussed in 3.5.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08177" title="Abstract">arXiv:2310.08177</a> [<a href="/pdf/2310.08177" title="Download PDF">pdf</a>, <a href="/format/2310.08177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Floris%2C+G">Giuseppe Floris</a>, 
<a href="/search/cs?searchtype=author&query=Mura%2C+R">Raffaele Mura</a>, 
<a href="/search/cs?searchtype=author&query=Scionis%2C+L">Luca Scionis</a>, 
<a href="/search/cs?searchtype=author&query=Piras%2C+G">Giorgio Piras</a>, 
<a href="/search/cs?searchtype=author&query=Pintor%2C+M">Maura Pintor</a>, 
<a href="/search/cs?searchtype=author&query=Demontis%2C+A">Ambra Demontis</a>, 
<a href="/search/cs?searchtype=author&query=Biggio%2C+B">Battista Biggio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ESANN23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Evaluating the adversarial robustness of machine learning models using
gradient-based attacks is challenging. In this work, we show that
hyperparameter optimization can improve fast minimum-norm attacks by automating
the selection of the loss function, the optimizer and the step-size scheduler,
along with the corresponding hyperparameters. Our extensive evaluation
involving several robust models demonstrates the improved efficacy of fast
minimum-norm attacks when hyper-up with hyperparameter optimization. We release
our open-source code at https://github.com/pralab/HO-FMN.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08182" title="Abstract">arXiv:2310.08182</a> [<a href="/pdf/2310.08182" title="Download PDF">pdf</a>, <a href="/format/2310.08182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness  Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+S">Shengzhao Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Kamnoedboon%2C+P">Porawit Kamnoedboon</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">WeiWei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> UnderSubmission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The lack of standardized robustness metrics and the widespread reliance on
numerous unrelated benchmark datasets for testing have created a gap between
academically validated robust models and their often problematic practical
adoption. To address this, we introduce XIMAGENET-12, an explainable benchmark
dataset with over 200K images and 15,600 manual semantic annotations. Covering
12 categories from ImageNet to represent objects commonly encountered in
practical life and simulating six diverse scenarios, including overexposure,
blurring, color changing, etc., we further propose a novel robustness criterion
that extends beyond model generation ability assessment. This benchmark
dataset, along with related code, is available at
https://sites.google.com/view/ximagenet-12/home. Researchers and practitioners
can leverage this resource to evaluate the robustness of their visual models
under challenging conditions and ultimately benefit from the demands of
practical computer vision systems.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08184" title="Abstract">arXiv:2310.08184</a> [<a href="/pdf/2310.08184" title="Download PDF">pdf</a>, <a href="/format/2310.08184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learn From Model Beyond Fine-Tuning: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Hongling Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+A">Anke Tang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Han Hu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bo Du</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Foundation models (FM) have demonstrated remarkable performance across a wide
range of tasks (especially in the fields of natural language processing and
computer vision), primarily attributed to their ability to comprehend
instructions and access extensive, high-quality data. This not only showcases
their current effectiveness but also sets a promising trajectory towards the
development of artificial general intelligence. Unfortunately, due to multiple
constraints, the raw data of the model used for large model training are often
inaccessible, so the use of end-to-end models for downstream tasks has become a
new research trend, which we call Learn From Model (LFM) in this article. LFM
focuses on the research, modification, and design of FM based on the model
interface, so as to better understand the model structure and weights (in a
black box environment), and to generalize the model to downstream tasks. The
study of LFM techniques can be broadly categorized into five major areas: model
tuning, model distillation, model reuse, meta learning and model editing. Each
category encompasses a repertoire of methods and strategies that aim to enhance
the capabilities and performance of FM. This paper gives a comprehensive review
of the current methods based on FM from the perspective of LFM, in order to
help readers better understand the current research status and ideas. To
conclude, we summarize the survey by highlighting several critical areas for
future exploration and addressing open issues that require further attention
from the research community. The relevant papers we investigated in this
article can be accessed at
&lt;https://github.com/ruthless-man/Awesome-Learn-from-Model&gt;.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08185" title="Abstract">arXiv:2310.08185</a> [<a href="/pdf/2310.08185" title="Download PDF">pdf</a>, <a href="/format/2310.08185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form  Narrative Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+W">Wang You</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wenshan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yaobo Liang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+S">Shaoguang Mao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chenfei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+M">Maosong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yuzhe Cai</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yiduo Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+N">Nan Duan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Plan-and-Write is a common hierarchical approach in long-form narrative text
generation, which first creates a plan to guide the narrative writing.
Following this approach, several studies rely on simply prompting large
language models for planning, which often yields suboptimal results. In this
paper, we propose a new framework called Evaluation-guided Iterative Plan
Extraction for long-form narrative text generation (EIPE-text), which extracts
plans from the corpus of narratives and utilizes the extracted plans to
construct a better planner. EIPE-text has three stages: plan extraction,
learning, and inference. In the plan extraction stage, it iteratively extracts
and improves plans from the narrative corpus and constructs a plan corpus. We
propose a question answer (QA) based evaluation mechanism to automatically
evaluate the plans and generate detailed plan refinement instructions to guide
the iterative improvement. In the learning stage, we build a better planner by
fine-tuning with the plan corpus or in-context learning with examples in the
plan corpus. Finally, we leverage a hierarchical approach to generate long-form
narratives. We evaluate the effectiveness of EIPE-text in the domains of novels
and storytelling. Both GPT-4-based evaluations and human evaluations
demonstrate that our method can generate more coherent and relevant long-form
narratives. Our code will be released in the future.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08187" title="Abstract">arXiv:2310.08187</a> [<a href="/pdf/2310.08187" title="Download PDF">pdf</a>, <a href="/format/2310.08187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Question Generation in Bengali
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M">Mahmud Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+L">Labiba Islam</a>, 
<a href="/search/cs?searchtype=author&query=Ruma%2C+J+F">Jannatul Ferdous Ruma</a>, 
<a href="/search/cs?searchtype=author&query=Mayeesha%2C+T+T">Tasmiah Tahsin Mayeesha</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+R+M">Rashedur M. Rahman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages including references, 4 figures and 3 tables. Accepted in the Proceedings of the Workshop on Multimodal, Multilingual Natural Language Generation and Multilingual WebNLG Challenge (MM-NLG 2023)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the Workshop on Multimodal, Multilingual Natural
  Language Generation and Multilingual WebNLG Challenge (MM-NLG 2023), 2023,
  10-19
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The task of Visual Question Generation (VQG) is to generate human-like
questions relevant to the given image. As VQG is an emerging research field,
existing works tend to focus only on resource-rich language such as English due
to the availability of datasets. In this paper, we propose the first Bengali
Visual Question Generation task and develop a novel transformer-based
encoder-decoder architecture that generates questions in Bengali when given an
image. We propose multiple variants of models - (i) image-only: baseline model
of generating questions from images without additional information, (ii)
image-category and image-answer-category: guided VQG where we condition the
model to generate questions based on the answer and the category of expected
question. These models are trained and evaluated on the translated VQAv2.0
dataset. Our quantitative and qualitative results establish the first state of
the art models for VQG task in Bengali and demonstrate that our models are
capable of generating grammatically correct and relevant questions. Our
quantitative results show that our image-cat model achieves a BLUE-1 score of
33.12 and BLEU-3 score of 7.56 which is the highest of the other two variants.
We also perform a human evaluation to assess the quality of the generation
tasks. Human evaluation suggests that image-cat model is capable of generating
goal-driven and attribute-specific questions and also stays relevant to the
corresponding image.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08192" title="Abstract">arXiv:2310.08192</a> [<a href="/pdf/2310.08192" title="Download PDF">pdf</a>, <a href="/format/2310.08192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Slip Detection and Surface Prediction Through Bio-Inspired Tactile  Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shepherd%2C+D+R">Dexter R. Shepherd</a>, 
<a href="/search/cs?searchtype=author&query=Husbands%2C+P">Phil Husbands</a>, 
<a href="/search/cs?searchtype=author&query=Philippides%2C+A">Andy Philippides</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+C">Chris Johnson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages + references, under review for ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">High resolution tactile sensing has great potential in autonomous mobile
robotics, particularly for legged robots. One particular area where it has
significant promise is the traversal of challenging, varied terrain. Depending
on whether an environment is slippery, soft, hard or dry, a robot must adapt
its method of locomotion accordingly. Currently many multi-legged robots, such
as Boston Dynamic's Spot robot, have preset gaits for different surface types,
but struggle over terrains where the surface type changes frequently. Being
able to automatically detect changes within an environment would allow a robot
to autonomously adjust its method of locomotion to better suit conditions,
without requiring a human user to manually set the change in surface type. In
this paper we report on the first detailed investigation of the properties of a
particular bio-inspired tactile sensor, the TacTip, to test its suitability for
this kind of automatic detection of surface conditions. We explored different
processing techniques and a regression model, using a custom made rig for data
collection to determine how a robot could sense directional and general force
on the sensor in a variety of conditions. This allowed us to successfully
demonstrate how the sensor can be used to distinguish between soft, hard, dry
and (wet) slippery surfaces. We further explored a neural model to classify
specific surface textures. Pin movement (the movement of optical markers within
the sensor) was key to sensing this information, and all models relied on some
form of temporal information. Our final trained models could successfully
determine the direction the sensor is heading in, the amount of force acting on
it, and determine differences in the surface texture such as Lego vs smooth
hard surface, or concrete vs smooth hard surface.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08194" title="Abstract">arXiv:2310.08194</a> [<a href="/pdf/2310.08194" title="Download PDF">pdf</a>, <a href="/format/2310.08194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Free-Riding in Multi-Issue Decisions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lackner%2C+M">Martin Lackner</a>, 
<a href="/search/cs?searchtype=author&query=Maly%2C+J">Jan Maly</a>, 
<a href="/search/cs?searchtype=author&query=Nardi%2C+O">Oliviero Nardi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an extended version of the publication with the same name in the proceedings of AAMAS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Voting in multi-issue domains allows for compromise outcomes that satisfy all
voters to some extent. Such fairness considerations, however, open the
possibility of a special form of manipulation: free-riding. By untruthfully
opposing a popular opinion in one issue, voters can receive increased
consideration in other issues. We study under which conditions this is
possible. Additionally, we study free-riding from a computational and
experimental point of view. Our results show that free-riding in multi-issue
domains is largely unavoidable, but comes at a non-negligible individual risk
for voters. Thus, the allure of free-riding is smaller than one could
intuitively assume.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08198" title="Abstract">arXiv:2310.08198</a> [<a href="/pdf/2310.08198" title="Download PDF">pdf</a>, <a href="/format/2310.08198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Traditional DoE: Deep Reinforcement Learning for Optimizing  Experiments in Model Identification of Battery Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Budan%2C+G">Gokhan Budan</a>, 
<a href="/search/cs?searchtype=author&query=Damiani%2C+F">Francesca Damiani</a>, 
<a href="/search/cs?searchtype=author&query=Kurtulus%2C+C">Can Kurtulus</a>, 
<a href="/search/cs?searchtype=author&query=Ure%2C+N+K">N. Kemal Ure</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Model identification of battery dynamics is a central problem in energy
research; many energy management systems and design processes rely on accurate
battery models for efficiency optimization. The standard methodology for
battery modelling is traditional design of experiments (DoE), where the battery
dynamics are excited with many different current profiles and the measured
outputs are used to estimate the system dynamics. However, although it is
possible to obtain useful models with the traditional approach, the process is
time consuming and expensive because of the need to sweep many different
current-profile configurations. In the present work, a novel DoE approach is
developed based on deep reinforcement learning, which alters the configuration
of the experiments on the fly based on the statistics of past experiments.
Instead of sticking to a library of predefined current profiles, the proposed
approach modifies the current profiles dynamically by updating the output space
covered by past measurements, hence only the current profiles that are
informative for future experiments are applied. Simulations and real
experiments are used to show that the proposed approach gives models that are
as accurate as those obtained with traditional DoE but by using 85\% less
resources.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08204" title="Abstract">arXiv:2310.08204</a> [<a href="/pdf/2310.08204" title="Download PDF">pdf</a>, <a href="/format/2310.08204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lifelong Audio-video Masked Autoencoder with Forget-robust Localized  Alignments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jaewoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jaehong Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Wonjae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yunji Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, project page: <a href="https://g-jwlee.github.io/FLAVA/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a lifelong audio-video masked autoencoder that continually learns
the multimodal representations from a video stream containing audio-video
pairs, while its distribution continually shifts over time. Specifically, we
propose two novel ideas to tackle the problem: (1) Localized Alignment: We
introduce a small trainable multimodal encoder that predicts the audio and
video tokens that are well-aligned with each other. This allows the model to
learn only the highly correlated audiovisual patches with accurate multimodal
relationships. (2) Forget-robust multimodal patch selection: We compare the
relative importance of each audio-video patch between the current and past data
pair to mitigate unintended drift of the previously learned audio-video
representations. Our proposed method, FLAVA (Forget-robust Localized
Audio-Video Alignment), therefore, captures the complex relationships between
the audio and video modalities during training on a sequence of pre-training
tasks while alleviating the forgetting of learned audiovisual correlations. Our
experiments validate that FLAVA outperforms the state-of-the-art continual
learning methods on several benchmark datasets under continual audio-video
representation learning scenarios.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08205" title="Abstract">arXiv:2310.08205</a> [<a href="/pdf/2310.08205" title="Download PDF">pdf</a>, <a href="/format/2310.08205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LiveVV: Human-Centered Live Volumetric Video Streaming System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+K">Kaiyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yongting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kaiying Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junhua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haowen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yili Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fangxin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Volumetric video has emerged as a prominent medium within the realm of
eXtended Reality (XR) with the advancements in computer graphics and depth
capture hardware. Users can fully immersive themselves in volumetric video with
the ability to switch their viewport in six degree-of-freedom (DOF), including
three rotational dimensions (yaw, pitch, roll) and three translational
dimensions (X, Y, Z). Different from traditional 2D videos that are composed of
pixel matrices, volumetric videos employ point clouds, meshes, or voxels to
represent a volumetric scene, resulting in significantly larger data sizes.
While previous works have successfully achieved volumetric video streaming in
video-on-demand scenarios, the live streaming of volumetric video remains an
unresolved challenge due to the limited network bandwidth and stringent latency
constraints. In this paper, we for the first time propose a holistic live
volumetric video streaming system, LiveVV, which achieves multi-view capture,
scene segmentation \&amp; reuse, adaptive transmission, and rendering. LiveVV
contains multiple lightweight volumetric video capture modules that are capable
of being deployed without prior preparation. To reduce bandwidth consumption,
LiveVV processes static and dynamic volumetric content separately by reusing
static data with low disparity and decimating data with low visual saliency.
Besides, to deal with network fluctuation, LiveVV integrates a volumetric video
adaptive bitrate streaming algorithm (VABR) to enable fluent playback with the
maximum quality of experience. Extensive real-world experiment shows that
LiveVV can achieve live volumetric video streaming at a frame rate of 24 fps
with a latency of less than 350ms.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08206" title="Abstract">arXiv:2310.08206</a> [<a href="/pdf/2310.08206" title="Download PDF">pdf</a>, <a href="/format/2310.08206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long-Tailed Classification Based on Coarse-Grained Leading Forest and  Multi-Center Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinye Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Ji Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is another research work to apply leading tree structure along with deep learning architecture
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Long-tailed(LT) classification is an unavoidable and challenging problem in
the real world. Most of the existing long-tailed classification methods focus
only on solving the inter-class imbalance in which there are more samples in
the head class than in the tail class, while ignoring the intra-lass imbalance
in which the number of samples of the head attribute within the same class is
much larger than the number of samples of the tail attribute. The deviation in
the model is caused by both of these factors, and due to the fact that
attributes are implicit in most datasets and the combination of attributes is
very complex, the intra-class imbalance is more difficult to handle. For this
purpose, we proposed a long-tailed classification framework, known as
\textbf{\textsc{Cognisance}}, which is founded on Coarse-Grained Leading Forest
(CLF) and Multi-Center Loss (MCL), aiming to build a multi-granularity joint
solution model by means of invariant feature learning. In this method, we
designed an unsupervised learning method, i.e., CLF, to better characterize the
distribution of attributes within a class. Depending on the distribution of
attributes, we can flexibly construct sampling strategies suitable for
different environments. In addition, we introduce a new metric learning loss
(MCL), which aims to gradually eliminate confusing attributes during the
feature learning process. More importantly, this approach does not depend on a
specific model structure and can be integrated with existing LT methods as an
independent component. We have conducted extensive experiments and our approach
has state-of-the-art performance in both existing benchmarks ImageNet-GLT and
MSCOCO-GLT, and can improve the performance of existing LT methods. Our codes
are available on GitHub: \url{https://github.com/jinyery/cognisance}
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08210" title="Abstract">arXiv:2310.08210</a> [<a href="/pdf/2310.08210" title="Download PDF">pdf</a>, <a href="/format/2310.08210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLExtract: Recovering Highly Corrupted DVB/GSE Satellite Stream with  Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lin%2C+M">Minghao Lin</a>, 
<a href="/search/eess?searchtype=author&query=Cheng%2C+M">Minghao Cheng</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+D">Dongsheng Luo</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yueqi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SpaceSec'23, 11 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Since satellite systems are playing an increasingly important role in our
civilization, their security and privacy weaknesses are more and more
concerned. For example, prior work demonstrates that the communication channel
between maritime VSAT and ground segment can be eavesdropped on using
consumer-grade equipment. The stream decoder GSExtract developed in this prior
work performs well for most packets but shows incapacity for corrupted streams.
We discovered that such stream corruption commonly exists in not only Europe
and North Atlantic areas but also Asian areas. In our experiment, using
GSExtract, we are only able to decode 2.1\% satellite streams we eavesdropped
on in Asia.
<br />Therefore, in this work, we propose to use a contrastive learning technique
with data augmentation to decode and recover such highly corrupted streams.
Rather than rely on critical information in corrupted streams to search for
headers and perform decoding, contrastive learning directly learns the features
of packet headers at different protocol layers and identifies them in a stream
sequence. By filtering them out, we can extract the innermost data payload for
further analysis. Our evaluation shows that this new approach can successfully
recover 71-99\% eavesdropped data hundreds of times faster speed than
GSExtract. Besides, the effectiveness of our approach is not largely damaged
when stream corruption becomes more severe.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08213" title="Abstract">arXiv:2310.08213</a> [<a href="/pdf/2310.08213" title="Download PDF">pdf</a>, <a href="/format/2310.08213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Universal Scheme for Partitioned Dynamic Shortest Path Index
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengxuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xinjie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Trajcevski%2C+G">Goce Trajcevski</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaofang Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Graph partitioning is a common solution to scale up the graph algorithms, and
shortest path (SP) computation is one of them. However, the existing solutions
typically have a fixed partition method with a fixed path index and fixed
partition structure, so it is unclear how the partition method and path index
influence the pathfinding performance. Moreover, few studies have explored the
index maintenance of partitioned SP (PSP) on dynamic graphs. To provide a
deeper insight into the dynamic PSP indexes, we systematically deliberate on
the existing works and propose a universal scheme to analyze this problem
theoretically. Specifically, we first propose two novel partitioned index
strategies and one optimization to improve index construction, query answering,
or index maintenance of PSP index. Then we propose a path-oriented graph
partitioning classification criteria for easier partition method selection.
After that, we re-couple the dimensions in our scheme (partitioned index
strategy, path index, and partition structure) to propose five new partitioned
SP indexes that are more efficient either in the query or update on different
networks. Finally, we demonstrate the effectiveness of our new indexes by
comparing them with state-of-the-art PSP indexes through comprehensive
evaluations.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08215" title="Abstract">arXiv:2310.08215</a> [<a href="/pdf/2310.08215" title="Download PDF">pdf</a>, <a href="/format/2310.08215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trustworthy Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mucs%C3%A1nyi%2C+B">B&#xe1;lint Mucs&#xe1;nyi</a>, 
<a href="/search/cs?searchtype=author&query=Kirchhof%2C+M">Michael Kirchhof</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+E">Elisa Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Rubinstein%2C+A">Alexander Rubinstein</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S+J">Seong Joon Oh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 373 pages, textbook at the University of T\"ubingen
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As machine learning technology gets applied to actual products and solutions,
new challenges have emerged. Models unexpectedly fail to generalize to small
changes in the distribution, tend to be confident on novel data they have never
seen, or cannot communicate the rationale behind their decisions effectively
with the end users. Collectively, we face a trustworthiness issue with the
current machine learning technology. This textbook on Trustworthy Machine
Learning (TML) covers a theoretical and technical background of four key topics
in TML: Out-of-Distribution Generalization, Explainability, Uncertainty
Quantification, and Evaluation of Trustworthiness. We discuss important
classical and contemporary research papers of the aforementioned fields and
uncover and connect their underlying intuitions. The book evolved from the
homonymous course at the University of T\"ubingen, first offered in the Winter
Semester of 2022/23. It is meant to be a stand-alone product accompanied by
code snippets and various pointers to further sources on topics of TML. The
dedicated website of the book is https://trustworthyml.io/.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08217" title="Abstract">arXiv:2310.08217</a> [<a href="/pdf/2310.08217" title="Download PDF">pdf</a>, <a href="/format/2310.08217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge  Retention and Promotion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vijayan%2C+P">Preetha Vijayan</a>, 
<a href="/search/cs?searchtype=author&query=Bhat%2C+P">Prashant Bhat</a>, 
<a href="/search/cs?searchtype=author&query=Arani%2C+E">Elahe Arani</a>, 
<a href="/search/cs?searchtype=author&query=Zonooz%2C+B">Bahram Zonooz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Continual learning (CL) has remained a persistent challenge for deep neural
networks due to catastrophic forgetting (CF) of previously learned tasks.
Several techniques such as weight regularization, experience rehearsal, and
parameter isolation have been proposed to alleviate CF. Despite their relative
success, these research directions have predominantly remained orthogonal and
suffer from several shortcomings, while missing out on the advantages of
competing strategies. On the contrary, the brain continually learns,
accommodates, and transfers knowledge across tasks by simultaneously leveraging
several neurophysiological processes, including neurogenesis, active
forgetting, neuromodulation, metaplasticity, experience rehearsal, and
context-dependent gating, rarely resulting in CF. Inspired by how the brain
exploits multiple mechanisms concurrently, we propose TriRE, a novel CL
paradigm that encompasses retaining the most prominent neurons for each task,
revising and solidifying the extracted knowledge of current and past tasks, and
actively promoting less active neurons for subsequent tasks through rewinding
and relearning. Across CL settings, TriRE significantly reduces task
interference and surpasses different CL approaches considered in isolation.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08218" title="Abstract">arXiv:2310.08218</a> [<a href="/pdf/2310.08218" title="Download PDF">pdf</a>, <a href="/format/2310.08218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence of Arbitrary Lagrangian-Eulerian Second-order Projection  Method for the Stokes Equations on an Evolving Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rao%2C+Q">Qiqi Rao</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+J">Jilu Wang</a>, 
<a href="/search/math?searchtype=author&query=Xie%2C+Y">Yupei Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The numerical solution of the Stokes equations on an evolving domain with a
moving boundary is studied based on the arbitrary Lagrangian-Eulerian finite
element method and a second-order projection method along the trajectories of
the evolving mesh for decoupling the unknown solutions of velocity and
pressure. The error of the semidiscrete arbitrary Lagrangian-Eulerian method is
shown to be $O(h^{r+1})$ for the Taylor--Hood finite elements of degree $r\ge
2$, using Nitsche's duality argument adapted to an evolving mesh, by proving
that the material derivative and the Stokes--Ritz projection commute up to
terms which have optimal-order convergence in the $L^2$ norm. Additionally, the
error of the fully discrete finite element method, with a second-order
projection method along the trajectories of the evolving mesh, is shown to be
$O(\ln(1/\tau+1)\tau^{2}+\ln(1/h+1)h^{r+1})$ in the discrete $L^\infty(0,T;
L^2)$ norm using newly developed energy techniques and backward parabolic
duality arguments that are applicable to the Stokes equations with an evolving
mesh. To maintain consistency between the notations of the numerical scheme in
a moving domain and those in a fixed domain, we introduce the equivalence class
of finite element spaces across time levels. Numerical examples are provided to
support the theoretical analysis and to illustrate the performance of the
method in simulating Navier--Stokes flow in a domain with a rotating propeller.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08221" title="Abstract">arXiv:2310.08221</a> [<a href="/pdf/2310.08221" title="Download PDF">pdf</a>, <a href="/format/2310.08221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimCKP: Simple Contrastive Learning of Keyphrase Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+M">Minseok Choi</a>, 
<a href="/search/cs?searchtype=author&query=Gwak%2C+C">Chaeheon Gwak</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S+H">Si Hyeong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choo%2C+J">Jaegul Choo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Keyphrase generation (KG) aims to generate a set of summarizing words or
phrases given a source document, while keyphrase extraction (KE) aims to
identify them from the text. Because the search space is much smaller in KE, it
is often combined with KG to predict keyphrases that may or may not exist in
the corresponding document. However, current unified approaches adopt sequence
labeling and maximization-based generation that primarily operate at a token
level, falling short in observing and scoring keyphrases as a whole. In this
work, we propose SimCKP, a simple contrastive learning framework that consists
of two stages: 1) An extractor-generator that extracts keyphrases by learning
context-aware phrase-level representations in a contrastive manner while also
generating keyphrases that do not appear in the document; 2) A reranker that
adapts scores for each generated phrase by likewise aligning their
representations with the corresponding document. Experimental results on
multiple benchmark datasets demonstrate the effectiveness of our proposed
approach, which outperforms the state-of-the-art models by a significant
margin.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08222" title="Abstract">arXiv:2310.08222</a> [<a href="/pdf/2310.08222" title="Download PDF">pdf</a>, <a href="/format/2310.08222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structural analysis of Hindi online handwritten characters for character  recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Anand Sharma</a> (MIET, Meerut), 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+A+G">A. G. Ramakrishnan</a> (IISc, Bengaluru)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 36 jpg figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Direction properties of online strokes are used to analyze them in terms of
homogeneous regions or sub-strokes with points satisfying common geometric
properties. Such sub-strokes are called sub-units. These properties are used to
extract sub-units from Hindi ideal online characters. These properties along
with some heuristics are used to extract sub-units from Hindi online
handwritten characters.\\ A method is developed to extract point stroke,
clockwise curve stroke, counter-clockwise curve stroke and loop stroke segments
as sub-units from Hindi online handwritten characters. These extracted
sub-units are close in structure to the sub-units of the corresponding Hindi
online ideal characters.\\ Importance of local representation of online
handwritten characters in terms of sub-units is assessed by training a
classifier with sub-unit level local and character level global features
extracted from characters for character recognition. The classifier has the
recognition accuracy of 93.5\% on the testing set. This accuracy is the highest
when compared with that of the classifiers trained only with global features
extracted from characters in the same training set and evaluated on the same
testing set.\\ Sub-unit extraction algorithm and the sub-unit based character
classifier are tested on Hindi online handwritten character dataset. This
dataset consists of samples from 96 different characters. There are 12832 and
2821 samples in the training and testing sets, respectively.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08224" title="Abstract">arXiv:2310.08224</a> [<a href="/pdf/2310.08224" title="Download PDF">pdf</a>, <a href="/format/2310.08224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergence of Latent Binary Encoding in Deep Neural Network Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sbail%C3%B2%2C+L">Luigi Sbail&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Ghiringhelli%2C+L">Luca Ghiringhelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We observe the emergence of binary encoding within the latent space of
deep-neural-network classifiers. Such binary encoding is induced by introducing
a linear penultimate layer, which is equipped during training with a loss
function that grows as $\exp(\vec{x}^2)$, where $\vec{x}$ are the coordinates
in the latent space. The phenomenon we describe represents a specific instance
of a well-documented occurrence known as \textit{neural collapse}, which arises
in the terminal phase of training and entails the collapse of latent class
means to the vertices of a simplex equiangular tight frame (ETF). We show that
binary encoding accelerates convergence toward the simplex ETF and enhances
classification accuracy.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08226" title="Abstract">arXiv:2310.08226</a> [<a href="/pdf/2310.08226" title="Download PDF">pdf</a>, <a href="/ps/2310.08226" title="Download PostScript">ps</a>, <a href="/format/2310.08226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluate PAC codes via Efficient Estimation on Weight Distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+J">Junhua You</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shaohua Wu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yajing Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In this letter, we introduce an efficient method for estimating weight
distributions of polar codes and polarization-adjusted convolutional (PAC)
codes. Based on a recursive algorithm of computing the weight enumerating
functions of polar cosets, this method focuses on two key objectives:
accurately determining the number of low-weight codewords and quickly
approximating the distribution of high-weight codewords. Simulation results
demonstrate that this hybrid method maintains competitively low complexity
while effectively achieving the objectives.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08230" title="Abstract">arXiv:2310.08230</a> [<a href="/pdf/2310.08230" title="Download PDF">pdf</a>, <a href="/format/2310.08230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Discrete Optimisation for Geometrically Consistent 3D Shape  Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roetzer%2C+P">Paul Roetzer</a>, 
<a href="/search/cs?searchtype=author&query=Abbas%2C+A">Ahmed Abbas</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+D">Dongliang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Bernard%2C+F">Florian Bernard</a>, 
<a href="/search/cs?searchtype=author&query=Swoboda%2C+P">Paul Swoboda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paul Roetzer and Ahmed Abbas contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this work we propose to combine the advantages of learning-based and
combinatorial formalisms for 3D shape matching. While learning-based shape
matching solutions lead to state-of-the-art matching performance, they do not
ensure geometric consistency, so that obtained matchings are locally unsmooth.
On the contrary, axiomatic methods allow to take geometric consistency into
account by explicitly constraining the space of valid matchings. However,
existing axiomatic formalisms are impractical since they do not scale to
practically relevant problem sizes, or they require user input for the
initialisation of non-convex optimisation problems. In this work we aim to
close this gap by proposing a novel combinatorial solver that combines a unique
set of favourable properties: our approach is (i) initialisation free, (ii)
massively parallelisable powered by a quasi-Newton method, (iii) provides
optimality gaps, and (iv) delivers decreased runtime and globally optimal
results for many instances.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08232" title="Abstract">arXiv:2310.08232</a> [<a href="/pdf/2310.08232" title="Download PDF">pdf</a>, <a href="/format/2310.08232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models are Universal Embedders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zehan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanzhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+D">Dingkun Long</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+P">Pengjun Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Meishan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the large language model (LLM) revolution, embedding is a key component of
various systems. For example, it is used to retrieve knowledge or memories for
LLMs, to build content moderation filters, etc. As such cases span from English
to other natural or programming languages, from retrieval to classification and
beyond, it is desirable to build a unified embedding model rather than
dedicated ones for each scenario. In this work, we make an initial step towards
this goal, demonstrating that multiple languages (both natural and programming)
pre-trained transformer decoders can embed universally when finetuned on
limited English data. We provide a comprehensive practice with thorough
evaluations. On English MTEB, our models achieve competitive performance on
different embedding tasks by minimal training data. On other benchmarks, such
as multilingual classification and code search, our models (without any
supervision) perform comparably to, or even surpass heavily supervised
baselines and/or APIs. These results provide evidence of a promising path
towards building powerful unified embedders that can be applied across tasks
and languages.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08233" title="Abstract">arXiv:2310.08233</a> [<a href="/pdf/2310.08233" title="Download PDF">pdf</a>, <a href="/format/2310.08233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of Time Step Frequency on the Realism of Robotic Manipulation  Simulation for Objects of Different Scales
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ta%2C+M+Q">Minh Q. Ta</a>, 
<a href="/search/cs?searchtype=author&query=Dinkel%2C+H">Holly Dinkel</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Rashid%2C+H">Hameed Abdul-Rashid</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yangfei Dai</a>, 
<a href="/search/cs?searchtype=author&query=Myers%2C+J">Jessica Myers</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+J">Junyi Geng</a>, 
<a href="/search/cs?searchtype=author&query=Bretl%2C+T">Timothy Bretl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, 3 figures, Best Poster Finalist at the 2023 Robotics and AI in Future Factory Workshop at the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Video presentation [<a href="https://www.youtube.com/watch?v=JOXrBpMmI0A">this https URL</a>]. Robotics and AI in Future Factory workshop [<a href="https://sites.google.com/view/robot-ai-future-factory/">this https URL</a>]
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This work evaluates the impact of time step frequency and component scale on
robotic manipulation simulation accuracy. Increasing the time step frequency
for small-scale objects is shown to improve simulation accuracy. This
simulation, demonstrating pre-assembly part picking for two object geometries,
serves as a starting point for discussing how to improve Sim2Real transfer in
robotic assembly processes.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08234" title="Abstract">arXiv:2310.08234</a> [<a href="/pdf/2310.08234" title="Download PDF">pdf</a>, <a href="/format/2310.08234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CiRA: An Open-Source Python Package for Automated Generation of Test  Case Descriptions from Natural Language Requirements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frattini%2C+J">Julian Frattini</a>, 
<a href="/search/cs?searchtype=author&query=Fischbach%2C+J">Jannik Fischbach</a>, 
<a href="/search/cs?searchtype=author&query=Bauer%2C+A">Andreas Bauer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tenth International Workshop on Artificial Intelligence and Requirements Engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Deriving acceptance tests from high-level, natural language requirements that
achieve full coverage is a major manual challenge at the interface between
requirements engineering and testing. Conditional requirements (e.g., "If A or
B then C.") imply causal relationships which - when extracted - allow to
generate these acceptance tests automatically. This paper presents a tool from
the CiRA (Causality In Requirements Artifacts) initiative, which automatically
processes conditional natural language requirements and generates a minimal set
of test case descriptions achieving full coverage. We evaluate the tool on a
publicly available data set of 61 requirements from the requirements
specification of the German Corona-Warn-App. The tool infers the correct test
variables in 84.5% and correct variable configurations in 92.3% of all cases,
which corroborates the feasibility of our approach.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08235" title="Abstract">arXiv:2310.08235</a> [<a href="/pdf/2310.08235" title="Download PDF">pdf</a>, <a href="/format/2310.08235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GROOT: Learning to Follow Instructions by Watching Gameplay Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+S">Shaofei Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bowei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaojian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Anji Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yitao Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the problem of building a controller that can follow open-ended
instructions in open-world environments. We propose to follow reference videos
as instructions, which offer expressive goal specifications while eliminating
the need for expensive text-gameplay annotations. A new learning framework is
derived to allow learning such instruction-following controllers from gameplay
videos while producing a video instruction encoder that induces a structured
goal space. We implement our agent GROOT in a simple yet effective
encoder-decoder architecture based on causal transformers. We evaluate GROOT
against open-world counterparts and human players on a proposed Minecraft
SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the
human-machine gap as well as exhibiting a 70% winning rate over the best
generalist agent baseline. Qualitative analysis of the induced goal space
further demonstrates some interesting emergent properties, including the goal
composition and complex gameplay behavior synthesis. Code and video can be
found on the website https://craftjarvis-groot.github.io.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08240" title="Abstract">arXiv:2310.08240</a> [<a href="/pdf/2310.08240" title="Download PDF">pdf</a>, <a href="/ps/2310.08240" title="Download PostScript">ps</a>, <a href="/format/2310.08240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Who Said That? Benchmarking Social Media AI Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+W">Wanyun Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Linqiu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianle Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+S">Shuyang Cai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">AI-generated text has proliferated across various online platforms, offering
both transformative prospects and posing significant risks related to
misinformation and manipulation. Addressing these challenges, this paper
introduces SAID (Social media AI Detection), a novel benchmark developed to
assess AI-text detection models' capabilities in real social media platforms.
It incorporates real AI-generate text from popular social media platforms like
Zhihu and Quora. Unlike existing benchmarks, SAID deals with content that
reflects the sophisticated strategies employed by real AI users on the Internet
which may evade detection or gain visibility, providing a more realistic and
challenging evaluation landscape. A notable finding of our study, based on the
Zhihu dataset, reveals that annotators can distinguish between AI-generated and
human-generated texts with an average accuracy rate of 96.5%. This finding
necessitates a re-evaluation of human capability in recognizing AI-generated
text in today's widely AI-influenced environment. Furthermore, we present a new
user-oriented AI-text detection challenge focusing on the practicality and
effectiveness of identifying AI-generated text based on user information and
multiple responses. The experimental results demonstrate that conducting
detection tasks on actual social media platforms proves to be more challenging
compared to traditional simulated AI-text detection, resulting in a decreased
accuracy. On the other hand, user-oriented AI-generated text detection
significantly improve the accuracy of detection.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08241" title="Abstract">arXiv:2310.08241</a> [<a href="/pdf/2310.08241" title="Download PDF">pdf</a>, <a href="/format/2310.08241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Riemann Problem Method for the Kapila Model of Compressible  Multiphase Flows I: Temporal-Spatial Coupling Finite Volume Scheme
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Du%2C+Z">Zhifang Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A second-order accurate and robust numerical scheme is developed for the
Kapila model to simulate compressible multiphase flows. The scheme is
formulated within the temporal-spatial coupling framework with the generalized
Riemann problem (GRP) solver applied as the cornerstone. The use of the GRP
solver enhances the capability of the resulting scheme to handle the stiffness
of the Kapila model in two ways. Firstly, in addition to Riemann solutions, the
time derivatives of flow variables at cell interfaces are obtained by the GRP
solver. The coupled values, i.e. Riemann solutions and time derivatives, lead
to a straightforward approximation to the velocity divergence at the next time
level, enabling a semi-implicit time discretization to the volume fraction
equation. Secondly, the use of time derivatives enables numerical fluxes to
comprehensively account for the effect of the source term, which includes
interactions between phases. The robustness of the resulting numerical scheme
is therefore further improved. Several challenging numerical experiments are
conducted to demonstrate the performance of the proposed finite volume scheme.
In particular, a test case with a nonlinear smooth solution is designed to
verify the numerical accuracy.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08242" title="Abstract">arXiv:2310.08242</a> [<a href="/pdf/2310.08242" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond the Pandemic: Transforming Software Development with the IJARS  Model for Wellbeing and Resilience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Russo%2C+D">Daniel Russo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 1 Figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">This article delves into the lessons learned, highlighting the importance of
developer wellbeing. We introduce the Integrated Job Demands-Resources and
Self-Determination Model (IJARS) for a comprehensive understanding of
pandemic-era productivity. Emphasizing Agile values, mental health initiatives,
and learning from disruptions, we advocate for reshaped workplaces that
prioritize work-life balance and hybrid models, preparing for future
challenges. This guidance aims for a resilient and adaptive future, turning
adversity into opportunity.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08243" title="Abstract">arXiv:2310.08243</a> [<a href="/pdf/2310.08243" title="Download PDF">pdf</a>, <a href="/format/2310.08243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Twin-Width Parameterized by the Feedback Edge Number
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balab%C3%A1n%2C+J">Jakub Balab&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Ganian%2C+R">Robert Ganian</a>, 
<a href="/search/cs?searchtype=author&query=Rocton%2C+M">Mathis Rocton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">The problem of whether and how one can compute the twin-width of a graph --
along with an accompanying contraction sequence -- lies at the forefront of the
area of algorithmic model theory. While significant effort has been aimed at
obtaining a fixed-parameter approximation for the problem when parameterized by
twin-width, here we approach the question from a different perspective and
consider whether one can obtain (near-)optimal contraction sequences under a
larger parameterization, notably the feedback edge number $k$. As our main
contributions, under this parameterization we obtain (1) a linear bikernel for
the problem of either computing a $2$-contraction sequence or determining that
none exists and (2) an approximate fixed-parameter algorithm which computes an
$\ell$-contraction sequence (for an arbitrary specified $\ell$) or determines
that the twin-width of the input graph is at least $\ell$. These algorithmic
results rely on newly obtained insights into the structure of optimal
contraction sequences, and as a byproduct of these we also slightly tighten the
bound on the twin-width of graphs with small feedback edge number.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08246" title="Abstract">arXiv:2310.08246</a> [<a href="/pdf/2310.08246" title="Download PDF">pdf</a>, <a href="/ps/2310.08246" title="Download PostScript">ps</a>, <a href="/format/2310.08246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Precoding Design for Adjacent Integrated Sensing and  Communication Base Stations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wangjun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhiqing Wei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhiyong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Ping Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>

</div>
<p class="mathjax">Integrated sensing and communication (ISAC) base stations can provide
communication and wide range sensing information for vehicles via downlink (DL)
transmission, thus enhancing vehicle driving safety. One major challenge for
realizing high performance communication and sensing is how to deal with the DL
mutual interference among adjacent ISAC base stations, which includes not only
communication related interference, but also radar sensing related
interference. In this paper, we establish a DL mutual interference model of
adjacent ISAC base stations, and analyze the relationship for mutual
interference channels between communications and radar sensing. To improve the
sensing and communication performance, we propose a collaborative precoding
design for coordinated adjacent base stations to mitigate the mutual
interference under the transmit power constraint and constant modulus
constraint, which is formulated as a non-convex optimization problem. We first
relax the problem into a convex programming by omitting the rank constraint,
and propose a joint optimization algorithm to solve the problem. We furthermore
propose a sequential optimization algorithm, which divides the collaborative
precoding design problem into four subproblems and finds the optimum via a
gradient descent algorithm. Finally, we evaluate the collaborative precoding
design algorithms by considering sensing and communication performance via
numerical results.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08247" title="Abstract">arXiv:2310.08247</a> [<a href="/pdf/2310.08247" title="Download PDF">pdf</a>, <a href="/format/2310.08247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging DevOps for Scientific Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nuyujukian%2C+P">Paul Nuyujukian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Critical goals of scientific computing are to increase scientific rigor,
reproducibility, and transparency while keeping up with ever-increasing
computational demands. This work presents an integrated framework well-suited
for data processing and analysis spanning individual, on-premises, and cloud
environments. This framework leverages three well-established DevOps tools: 1)
Git repositories linked to 2) CI/CD engines operating on 3) containers. It
supports the full life-cycle of scientific data workflows with minimal friction
between stages--including solutions for researchers who generate data. This is
achieved by leveraging a single container that supports local, interactive user
sessions and deployment in HPC or Kubernetes clusters. Combined with Git
repositories integrated with CI/CD, this approach enables decentralized data
pipelines across multiple, arbitrary computational environments. This framework
has been successfully deployed and validated within our research group,
spanning experimental acquisition systems and computational clusters with
open-source, purpose-built GitLab CI/CD executors for slurm and Google
Kubernetes Engine Autopilot. Taken together, this framework can increase the
rigor, reproducibility, and transparency of compute-dependent scientific
research.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08248" title="Abstract">arXiv:2310.08248</a> [<a href="/pdf/2310.08248" title="Download PDF">pdf</a>, <a href="/format/2310.08248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visualizing a Nondeterministic to Deterministic Finite-State Machine  Transformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Minic%2C+T">Tijana Minic</a>, 
<a href="/search/cs?searchtype=author&query=Moraz%C3%A1n%2C+M+T">Marco T. Moraz&#xe1;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at The 2023 Scheme and Functional Programming Workshop (<a href="/abs/cs/0101200">arXiv:cs/0101200</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">The transformation of a nondeterministic finite-state automaton into a
deterministic finite-state automaton is an integral part of any course on
formal languages and automata theory. For some students, understanding this
transformation is challenging. Common problems encountered include not
comprehending how the states of the deterministic finite-state automaton are
determined and not comprehending the role that all the edges of the
nondeterministic finite-state automaton have in the deterministic finite-state
automaton's construction. To aid students in understanding, transformation
visualization tools have been developed. Although useful in helping students,
these tools do not properly illustrate the relationship between the states of
the deterministic finite-state automaton and the edges of the nondeterministic
finite-state automaton. This article presents a novel interactive visualization
tool to illustrate the transformation that highlights this relationship and
that is integrated into the FSM programming language. In addition, the
implementation of the visualization is sketched.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08252" title="Abstract">arXiv:2310.08252</a> [<a href="/pdf/2310.08252" title="Download PDF">pdf</a>, <a href="/format/2310.08252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zeyuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hongshu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiacheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenrui Li</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+G">Guojun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yue-Jiao Gong</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yining Ma</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhiguang Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NuerIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Recently, Meta-Black-Box Optimization with Reinforcement Learning
(MetaBBO-RL) has showcased the power of leveraging RL at the meta-level to
mitigate manual fine-tuning of low-level black-box optimizers. However, this
field is hindered by the lack of a unified benchmark. To fill this gap, we
introduce MetaBox, the first benchmark platform expressly tailored for
developing and evaluating MetaBBO-RL methods. MetaBox offers a flexible
algorithmic template that allows users to effortlessly implement their unique
designs within the platform. Moreover, it provides a broad spectrum of over 300
problem instances, collected from synthetic to realistic scenarios, and an
extensive library of 19 baseline methods, including both traditional black-box
optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three
standardized performance metrics, enabling a more thorough assessment of the
methods. In a bid to illustrate the utility of MetaBox for facilitating
rigorous evaluation and in-depth analysis, we carry out a wide-ranging
benchmarking study on existing MetaBBO-RL methods. Our MetaBox is open-source
and accessible at: https://github.com/GMC-DRL/MetaBox.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08255" title="Abstract">arXiv:2310.08255</a> [<a href="/pdf/2310.08255" title="Download PDF">pdf</a>, <a href="/format/2310.08255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distilling from Vision-Language Models for Improved OOD Generalization  in Vision Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Addepalli%2C+S">Sravanti Addepalli</a>, 
<a href="/search/cs?searchtype=author&query=Asokan%2C+A+R">Ashish Ramayee Asokan</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+L">Lakshay Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Babu%2C+R+V">R. Venkatesh Babu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/val-iisc/VL2V-ADiP.git">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Vision-Language Models (VLMs) such as CLIP are trained on large amounts of
image-text pairs, resulting in remarkable generalization across several data
distributions. The prohibitively expensive training and data
collection/curation costs of these models make them valuable Intellectual
Property (IP) for organizations. This motivates a vendor-client paradigm, where
a vendor trains a large-scale VLM and grants only input-output access to
clients on a pay-per-query basis in a black-box setting. The client aims to
minimize inference cost by distilling the VLM to a student model using the
limited available task-specific data, and further deploying this student model
in the downstream application. While naive distillation largely improves the
In-Domain (ID) accuracy of the student, it fails to transfer the superior
out-of-distribution (OOD) generalization of the VLM teacher using the limited
available labeled images. To mitigate this, we propose Vision-Language to
Vision-Align, Distill, Predict (VL2V-ADiP), which first aligns the vision and
language modalities of the teacher model with the vision modality of a
pre-trained student model, and further distills the aligned VLM embeddings to
the student. This maximally retains the pre-trained features of the student,
while also incorporating the rich representations of the VLM image encoder and
the superior generalization of the text embeddings. The proposed approach
achieves state-of-the-art results on the standard Domain Generalization
benchmarks in a black-box teacher setting, and also when weights of the VLM are
accessible.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08256" title="Abstract">arXiv:2310.08256</a> [<a href="/pdf/2310.08256" title="Download PDF">pdf</a>, <a href="/format/2310.08256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of Co-occurrence on Factual Knowledge of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+C">Cheongwoong Kang</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaesik Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) often make factually incorrect responses despite
their success in various applications. In this paper, we hypothesize that
relying heavily on simple co-occurrence statistics of the pre-training corpora
is one of the main factors that cause factual errors. Our results reveal that
LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently
co-occurred words over the correct answer. Consequently, LLMs struggle to
recall facts whose subject and object rarely co-occur in the pre-training
dataset although they are seen during finetuning. We show that co-occurrence
bias remains despite scaling up model sizes or finetuning. Therefore, we
suggest finetuning on a debiased dataset to mitigate the bias by filtering out
biased samples whose subject-object co-occurrence count is high. Although
debiased finetuning allows LLMs to memorize rare facts in the training set, it
is not effective in recalling rare facts unseen during finetuning. Further
research in mitigation will help build reliable language models by preventing
potential errors. The code is available at
\url{https://github.com/CheongWoong/impact_of_cooccurrence}.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08259" title="Abstract">arXiv:2310.08259</a> [<a href="/pdf/2310.08259" title="Download PDF">pdf</a>, <a href="/format/2310.08259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Invisible Threats: Backdoor Attack in OCR Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Conti%2C+M">Mauro Conti</a>, 
<a href="/search/cs?searchtype=author&query=Farronato%2C+N">Nicola Farronato</a>, 
<a href="/search/cs?searchtype=author&query=Koffas%2C+S">Stefanos Koffas</a>, 
<a href="/search/cs?searchtype=author&query=Pajola%2C+L">Luca Pajola</a>, 
<a href="/search/cs?searchtype=author&query=Picek%2C+S">Stjepan Picek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Optical Character Recognition (OCR) is a widely used tool to extract text
from scanned documents. Today, the state-of-the-art is achieved by exploiting
deep neural networks. However, the cost of this performance is paid at the
price of system vulnerability. For instance, in backdoor attacks, attackers
compromise the training phase by inserting a backdoor in the victim's model
that will be activated at testing time by specific patterns while leaving the
overall model performance intact. This work proposes a backdoor attack for OCR
resulting in the injection of non-readable characters from malicious input
images. This simple but effective attack exposes the state-of-the-art OCR
weakness, making the extracted text correct to human eyes but simultaneously
unusable for the NLP application that uses OCR as a preprocessing step.
Experimental results show that the attacked models successfully output
non-readable characters for around 90% of the poisoned instances without
harming their performance for the remaining instances.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08261" title="Abstract">arXiv:2310.08261</a> [<a href="/pdf/2310.08261" title="Download PDF">pdf</a>, <a href="/format/2310.08261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for  Multi-Modal 3D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Ziying Song</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Haiyue Wei</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+L">Lin Bai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+C">Caiyan Jia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">LiDAR and cameras are complementary sensors for 3D object detection in
autonomous driving. However, it is challenging to explore the unnatural
interaction between point clouds and images, and the critical factor is how to
conduct feature alignment of heterogeneous modalities. Currently, many methods
achieve feature alignment by projection calibration only, without considering
the problem of coordinate conversion accuracy errors between sensors, leading
to sub-optimal performance. In this paper, we present GraphAlign, a more
accurate feature alignment strategy for 3D object detection by graph matching.
Specifically, we fuse image features from a semantic segmentation encoder in
the image branch and point cloud features from a 3D Sparse CNN in the LiDAR
branch. To save computation, we construct the nearest neighbor relationship by
calculating Euclidean distance within the subspaces that are divided into the
point cloud features. Through the projection calibration between the image and
point cloud, we project the nearest neighbors of point cloud features onto the
image features. Then by matching the nearest neighbors with a single point
cloud to multiple images, we search for a more appropriate feature alignment.
In addition, we provide a self-attention module to enhance the weights of
significant relations to fine-tune the feature alignment between heterogeneous
modalities. Extensive experiments on nuScenes benchmark demonstrate the
effectiveness and efficiency of our GraphAlign.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08262" title="Abstract">arXiv:2310.08262</a> [<a href="/html/2310.08262" title="Download HTML">html</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proceedings of The 2023 Scheme and Functional Programming Workshop
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moraz%C3%A1n%2C+M+T">Marco T. Moraz&#xe1;n</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">This volume contains the articles presented at The 2023 Scheme and Functional
Programming Workshop in Seattle, Washington on September 9, 2023. The program
committee reviewed the articles using current academic standards and selected
four articles for presentation. These proceedings are considered non-archival
and the authors are free to submit revised versions of their articles to other
venues for archival publication.
<br />Program Committee: Leif Andersen, Northeastern University; Mark Friedman
<br />Leilani Gilpin, University of California, Santa Cruz; Jason Hemann, Seton
Hall University
<br />Julia Lawall, Inria; Joe Gibbs Politz, University of California at San Diego;
Marco T Moraz\'an (Chair), Seton Hall University
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08263" title="Abstract">arXiv:2310.08263</a> [<a href="/pdf/2310.08263" title="Download PDF">pdf</a>, <a href="/format/2310.08263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrated Sensing and Communication enabled Sensing Base Station:  System Design, Beamforming, Interference Cancellation and Performance  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jiang%2C+W">Wangjun Jiang</a>, 
<a href="/search/math?searchtype=author&query=Wei%2C+Z">Zhiqing Wei</a>, 
<a href="/search/math?searchtype=author&query=Feng%2C+Z">Zhiyong Feng</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+X">Xu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper studies the sensing base station (SBS) that has great potential to
improve the safety of vehicles and pedestrians on roads. It can detect the
targets on the road with communication signals using the integrated sensing and
communication (ISAC) technique. Compared with vehicle-mounted radar, SBS has a
better sensing field due to its higher deployment position, which can help
solve the problem of sensing blind areas. In this paper, key technologies of
SBS are studied, including the beamforming algorithm, beam scanning scheme, and
interference cancellation algorithm. To transmit and receive ISAC signals
simultaneously, a double-coupling antenna array is applied. The free detection
beam and directional communication beam are proposed for joint communication
and sensing to meet the requirements of beamwidth and pointing directions. The
joint time-space-frequency domain division multiple access algorithm is
proposed to cancel the interference of SBS, including multiuser interference
and duplex interference between sensing and communication. Finally, the sensing
and communication performance of SBS under the industrial scientific medical
power limitation is analyzed and simulated. Simulation results show that the
communication rate of SBS can reach over 100 Mbps and the range of sensing and
communication can reach about 500 m.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08267" title="Abstract">arXiv:2310.08267</a> [<a href="/pdf/2310.08267" title="Download PDF">pdf</a>, <a href="/format/2310.08267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimizing Sensor Allocation Cost for Crowdsensing On-street Parking  Availability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pang%2C+B">Boyu Pang</a>, 
<a href="/search/eess?searchtype=author&query=Liao%2C+R">Ruizhi Liao</a>, 
<a href="/search/eess?searchtype=author&query=Ye%2C+Y">Yinyu Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In recent years, innovative roadside parking vacancy crowdsensing solutions
have emerged as a cost-effective alternative to traditional methods, which can
significantly reduce sensor installation and maintenance expenses. This
crowdsensing scheme relies on vehicles equipped with sensors, such as buses and
taxis, roaming around urban streets to detect on-street parking availability.
Therefore, the accuracy of this scheme strongly depends on the vehicles' routes
and the frequency of their passage through parking spots. This paper presents
an integer programming-based optimal sensor allocation model to ensure the
detection accuracy of the scheme while using the minimum number of sensing kits
or probing vehicles. Moreover, a customized heuristic algorithm is proposed to
hasten the solution process. Numerical simulations using the street dataset
from San Francisco confirm the model's ability to reduce probing vehicle usage
while ensuring detection accuracy. Thus, our approach represents an effective
means of optimizing roadside parking detection in a crowdsensing way.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08270" title="Abstract">arXiv:2310.08270</a> [<a href="/pdf/2310.08270" title="Download PDF">pdf</a>, <a href="/format/2310.08270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hilbert Space Embedding-based Trajectory Optimization for Multi-Modal  Uncertain Obstacle Trajectory Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+B">Basant Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Aditya Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+K+M">K.Madhava Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A+K">Arun Kumar Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Safe autonomous driving critically depends on how well the ego-vehicle can
predict the trajectories of neighboring vehicles. To this end, several
trajectory prediction algorithms have been presented in the existing
literature. Many of these approaches output a multi-modal distribution of
obstacle trajectories instead of a single deterministic prediction to account
for the underlying uncertainty. However, existing planners cannot handle the
multi-modality based on just sample-level information of the predictions. With
this motivation, this paper proposes a trajectory optimizer that can leverage
the distributional aspects of the prediction in a computationally tractable and
sample-efficient manner. Our optimizer can work with arbitrarily complex
distributions and thus can be used with output distribution represented as a
deep neural network. The core of our approach is built on embedding
distribution in Reproducing Kernel Hilbert Space (RKHS), which we leverage in
two ways. First, we propose an RKHS embedding approach to select probable
samples from the obstacle trajectory distribution. Second, we rephrase
chance-constrained optimization as distribution matching in RKHS and propose a
novel sampling-based optimizer for its solution. We validate our approach with
hand-crafted and neural network-based predictors trained on real-world datasets
and show improvement over the existing stochastic optimization approaches in
safety metrics.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08271" title="Abstract">arXiv:2310.08271</a> [<a href="/pdf/2310.08271" title="Download PDF">pdf</a>, <a href="/format/2310.08271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variant Codes Based on A Special Polynomial Ring and Their Fast  Computations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Leilei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y+S">Yunghsiang S. Han</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jiasheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhongpei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">In this paper, we propose two new classes of binary array codes, termed
V-ETBR and V-ESIP codes, by reformulating and generalizing the variant
technique of deriving the well-known generalized row-diagonal parity~(RDP)
codes from shortened independent parity~(IP) codes. The V-ETBR and V-ESIP codes
are both based on binary parity-check matrices and are essentially variants of
two classes of codes over a special polynomial ring (termed ETBR and ESIP codes
in this paper).
<br />To explore the conditions that make the variant codes binary Maximum Distance
Separable~(MDS) array codes that achieve optimal storage efficiency, this paper
derives the connections between V-ETBR/V-ESIP codes and ETBR/ESIP codes. These
connections are beneficial for constructing various forms of the variant codes.
By utilizing these connections, this paper also explicitly presents the
constructions of V-ETBR and V-ESIP MDS array codes with any number of parity
columns $r$, along with their fast syndrome computations. In terms of
construction, all proposed MDS array codes have an exponentially growing total
number of data columns with respect to the column size, while alternative codes
have that only with linear order. In terms of computation, the proposed
syndrome computations make the corresponding encoding/decoding asymptotically
require $\lfloor \lg r \rfloor+1$ XOR~(exclusive OR) operations per data bit,
when the total number of data columns approaches infinity. This is also the
lowest known asymptotic complexity in MDS codes.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08274" title="Abstract">arXiv:2310.08274</a> [<a href="/pdf/2310.08274" title="Download PDF">pdf</a>, <a href="/format/2310.08274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Von Neumann Stability Analysis for Multi-level Multi-step Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Neelan%2C+A+G">Arun Govind Neelan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages and 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Spectral Theory (math.SP)

</div>
<p class="mathjax">Von Neumann stability analysis, a well-known Fourier-based method, is a
widely used technique for assessing stability in numerical computations.
However, as noted in "Numerical Solution of Partial Differential Equations:
Finite Difference Methods" by Smith (1985, pp. 67-68), this approach faces
limitations when applied to multi-level methods employing schemes with more
than two levels. In this study, we aim to extend the applicability of Von
Neumann stability analysis to multi-level methods. An alternative method
closely related to Von Neumann stability analysis is the Approximate Dispersion
Relation (ADR) analysis. In this work, we not only explore ADR analysis but
also introduce various ADR analysis variants while examining their inherent
limitations so that other researchers can improve the analysis before using
that in their work. Furthermore, we propose an innovative strategy for reducing
dissipation, optimizing it through the use of an evolutionary algorithm. Our
findings demonstrate that our proposed method yields minimal errors when
compared to other advection equation schemes, both in one and two spatial
dimensions.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08275" title="Abstract">arXiv:2310.08275</a> [<a href="/pdf/2310.08275" title="Download PDF">pdf</a>, <a href="/format/2310.08275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing the Power of LLM to Support Binary Taint Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Puzhuo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chengnian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yaowen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+X">Xuan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+C">Chuan Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuncheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Limin Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages,5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">This paper proposes LATTE, the first static binary taint analysis that is
powered by a large language model (LLM). LATTE is superior to the state of the
art (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fully
automated while prior static binary taint analyzers need rely on human
expertise to manually customize taint propagation rules and vulnerability
inspection rules. Second, LATTE is significantly effective in vulnerability
detection, demonstrated by our comprehensive evaluations. For example, LATTE
has found 37 new bugs in real-world firmware which the baselines failed to
find, and 7 of them have been assigned CVE numbers. Lastly, LATTE incurs
remarkably low engineering cost, making it a cost-efficient and scalable
solution for security researchers and practitioners. We strongly believe that
LATTE opens up a new direction to harness the recent advance in LLMs to improve
vulnerability analysis for binary programs.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08276" title="Abstract">arXiv:2310.08276</a> [<a href="/pdf/2310.08276" title="Download PDF">pdf</a>, <a href="/format/2310.08276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direction-Oriented Visual-semantic Embedding Model for Remote Sensing  Image-text Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qing Ma</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jiancheng Pan</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+C">Cong Bai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Image-text retrieval has developed rapidly in recent years. However, it is
still a challenge in remote sensing due to visual-semantic imbalance, which
leads to incorrect matching of non-semantic visual and textual features. To
solve this problem, we propose a novel Direction-Oriented Visual-semantic
Embedding Model (DOVE) to mine the relationship between vision and language.
Concretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the
distance between the final visual and textual embeddings in the latent semantic
space, oriented by regional visual features. Meanwhile, a lightweight Digging
Text Genome Assistant (DTGA) is designed to expand the range of tractable
textual representation and enhance global word-level semantic connections using
less attention operations. Ultimately, we exploit a global visual-semantic
constraint to reduce single visual dependency and serve as an external
constraint for the final visual and textual representations. The effectiveness
and superiority of our method are verified by extensive experiments including
parameter evaluation, quantitative comparison, ablation studies and visual
analysis, on two benchmark datasets, RSICD and RSITMD.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08278" title="Abstract">arXiv:2310.08278</a> [<a href="/pdf/2310.08278" title="Download PDF">pdf</a>, <a href="/format/2310.08278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lag-Llama: Towards Foundation Models for Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rasul%2C+K">Kashif Rasul</a>, 
<a href="/search/cs?searchtype=author&query=Ashok%2C+A">Arjun Ashok</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+A+R">Andrew Robert Williams</a>, 
<a href="/search/cs?searchtype=author&query=Khorasani%2C+A">Arian Khorasani</a>, 
<a href="/search/cs?searchtype=author&query=Adamopoulos%2C+G">George Adamopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Bhagwatkar%2C+R">Rishika Bhagwatkar</a>, 
<a href="/search/cs?searchtype=author&query=Bilo%C5%A1%2C+M">Marin Bilo&#x161;</a>, 
<a href="/search/cs?searchtype=author&query=Ghonia%2C+H">Hena Ghonia</a>, 
<a href="/search/cs?searchtype=author&query=Hassen%2C+N+V">Nadhir Vincent Hassen</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+A">Anderson Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Sahil Garg</a>, 
<a href="/search/cs?searchtype=author&query=Drouin%2C+A">Alexandre Drouin</a>, 
<a href="/search/cs?searchtype=author&query=Chapados%2C+N">Nicolas Chapados</a>, 
<a href="/search/cs?searchtype=author&query=Nevmyvaka%2C+Y">Yuriy Nevmyvaka</a>, 
<a href="/search/cs?searchtype=author&query=Rish%2C+I">Irina Rish</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Aiming to build foundation models for time-series forecasting and study their
scaling behavior, we present here our work-in-progress on Lag-Llama, a
general-purpose univariate probabilistic time-series forecasting model trained
on a large collection of time-series data. The model shows good zero-shot
prediction capabilities on unseen "out-of-distribution" time-series datasets,
outperforming supervised baselines. We use smoothly broken power-laws to fit
and predict model scaling behavior. The open source code is made available at
https://github.com/kashif/pytorch-transformer-ts.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08279" title="Abstract">arXiv:2310.08279</a> [<a href="/pdf/2310.08279" title="Download PDF">pdf</a>, <a href="/format/2310.08279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+L">Li Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce
and infer missing connections within knowledge graphs. Text-based approaches,
like SimKGC, have outperformed graph embedding methods, showcasing the promise
of inductive KGC. However, the efficacy of text-based methods hinges on the
quality of entity textual descriptions. In this paper, we identify the key
issue of whether large language models (LLMs) can generate effective text. To
mitigate hallucination in LLM-generated text in this paper, we introduce a
constraint-based prompt that utilizes the entity and its textual description as
contextual constraints to enhance data quality. Our Constrained-Prompt
Knowledge Graph Completion (CP-KGC) method demonstrates effective inference
under low resource computing conditions and surpasses prior results on the
WN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC
tasks and provides new directions for future research.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08280" title="Abstract">arXiv:2310.08280</a> [<a href="/pdf/2310.08280" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Odia Braille Literacy: The Influence of Speed on Error  Reduction and Enhanced Comprehension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parida%2C+M">Monnie Parida</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+M">Manjira Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+A">Anupam Basu</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+P">Pabitra Mitra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 Pages, Paper accepted in Diversity and Inclusion track at CODS-COMAD 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This study aims to conduct an extensive detailed analysis of the Odia Braille
reading comprehension among students with visual disability. Specifically, the
study explores their reading speed and hand or finger movements. The study also
aims to investigate any comprehension difficulties and reading errors they may
encounter. Six students from the 9th and 10th grades, aged between 14 and 16,
participated in the study. We observed participants hand movements to
understand how reading errors were connected to hand movement and identify the
students reading difficulties. We also evaluated the participants Odia Braille
reading skills, including their reading speed (in words per minute), errors,
and comprehension. The average speed of Odia Braille reader is 17.64wpm.
According to the study, there was a noticeable correlation between reading
speed and reading errors. As reading speed decreased, the number of reading
errors tended to increase. Moreover, the study established a link between
reduced Braille reading errors and improved reading comprehension. In contrast,
the study found that better comprehension was associated with increased reading
speed. The researchers concluded with some interesting findings about preferred
Braille reading patterns. These findings have important theoretical,
developmental, and methodological implications for instruction.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08282" title="Abstract">arXiv:2310.08282</a> [<a href="/pdf/2310.08282" title="Download PDF">pdf</a>, <a href="/format/2310.08282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data driven modeling of self-similar dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tao%2C+R">Ruyi Tao</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+N">Ningning Tao</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yizhuang You</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages,4 figures,1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistical Mechanics (cond-mat.stat-mech)

</div>
<p class="mathjax">Multiscale modeling of complex systems is crucial for understanding their
intricacies. Data-driven multiscale modeling has emerged as a promising
approach to tackle challenges associated with complex systems. On the other
hand, self-similarity is prevalent in complex systems, hinting that large-scale
complex systems can be modeled at a reduced cost. In this paper, we introduce a
multiscale neural network framework that incorporates self-similarity as prior
knowledge, facilitating the modeling of self-similar dynamical systems. For
deterministic dynamics, our framework can discern whether the dynamics are
self-similar. For uncertain dynamics, it can compare and determine which
parameter set is closer to self-similarity. The framework allows us to extract
scale-invariant kernels from the dynamics for modeling at any scale. Moreover,
our method can identify the power law exponents in self-similar systems.
Preliminary tests on the Ising model yielded critical exponents consistent with
theoretical expectations, providing valuable insights for addressing critical
phase transitions in non-equilibrium systems.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08289" title="Abstract">arXiv:2310.08289</a> [<a href="/pdf/2310.08289" title="Download PDF">pdf</a>, <a href="/format/2310.08289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximization of minimum rate in MIMO OFDM RIS-assisted Broadcast  Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soleymani%2C+M">Mohammad Soleymani</a>, 
<a href="/search/cs?searchtype=author&query=Santamaria%2C+I">Ignacio Santamaria</a>, 
<a href="/search/cs?searchtype=author&query=Sezgin%2C+A">Aydin Sezgin</a>, 
<a href="/search/cs?searchtype=author&query=Jorswieck%2C+E">Eduard Jorswieck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE CAMSAP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Reconfigurable intelligent surface (RIS) is a promising technology to enhance
the spectral efficiency of wireless communication systems. By optimizing the
RIS elements, the performance of the overall system can be improved. Yet, in
contrast to single-carrier systems, in multi-carrier systems, it is not
possible to independently optimize RIS elements at each sub-carrier, which may
reduce the benefits of RIS in multi-user orthogonal frequency division
multiplexing (OFDM) systems. To this end, we investigate the effectiveness of
RIS in multiple-input, multiple-output (MIMO) OFDM broadcast channels (BC). We
formulate and solve a joint precoding and RIS optimization problem. We show
that RIS can significantly improve the system performance even when the number
of RIS elements per sub-band is very low.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08291" title="Abstract">arXiv:2310.08291</a> [<a href="/pdf/2310.08291" title="Download PDF">pdf</a>, <a href="/format/2310.08291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expanding the Vocabulary of BERT for Knowledge Base Construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Dong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Celebi%2C+R">Remzi Celebi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Knowledge base construction entails acquiring structured information to
create a knowledge base of factual and relational data, facilitating question
answering, information retrieval, and semantic understanding. The challenge
called "Knowledge Base Construction from Pretrained Language Models" at
International Semantic Web Conference 2023 defines tasks focused on
constructing knowledge base using language model. Our focus was on Track 1 of
the challenge, where the parameters are constrained to a maximum of 1 billion,
and the inclusion of entity descriptions within the prompt is prohibited.
<br />Although the masked language model offers sufficient flexibility to extend
its vocabulary, it is not inherently designed for multi-token prediction. To
address this, we present Vocabulary Expandable BERT for knowledge base
construction, which expand the language model's vocabulary while preserving
semantic embeddings for newly added words. We adopt task-specific
re-pre-training on masked language model to further enhance the language model.
<br />Through experimentation, the results show the effectiveness of our
approaches. Our framework achieves F1 score of 0.323 on the hidden test set and
0.362 on the validation set, both data set is provided by the challenge.
Notably, our framework adopts a lightweight language model (BERT-base, 0.13
billion parameters) and surpasses the model using prompts directly on large
language model (Chatgpt-3, 175 billion parameters). Besides, Token-Recode
achieves comparable performances as Re-pretrain. This research advances
language understanding models by enabling the direct embedding of multi-token
entities, signifying a substantial step forward in link prediction task in
knowledge graph and metadata completion in data management.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08295" title="Abstract">arXiv:2310.08295</a> [<a href="/pdf/2310.08295" title="Download PDF">pdf</a>, <a href="/format/2310.08295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> If our aim is to build morality into an artificial agent, how might we  begin to go about doing so?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seeamber%2C+R">Reneira Seeamber</a>, 
<a href="/search/cs?searchtype=author&query=Badea%2C+C">Cosmin Badea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 1 figure,
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Intelligent Systems. 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">As Artificial Intelligence (AI) becomes pervasive in most fields, from
healthcare to autonomous driving, it is essential that we find successful ways
of building morality into our machines, especially for decision-making.
However, the question of what it means to be moral is still debated,
particularly in the context of AI. In this paper, we highlight the different
aspects that should be considered when building moral agents, including the
most relevant moral paradigms and challenges. We also discuss the top-down and
bottom-up approaches to design and the role of emotion and sentience in
morality. We then propose solutions including a hybrid approach to design and a
hierarchical approach to combining moral paradigms. We emphasize how governance
and policy are becoming ever more critical in AI Ethics and in ensuring that
the tasks we set for moral agents are attainable, that ethical behavior is
achieved, and that we obtain good AI.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08298" title="Abstract">arXiv:2310.08298</a> [<a href="/pdf/2310.08298" title="Download PDF">pdf</a>, <a href="/format/2310.08298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MProto: Multi-Prototype Network with Denoised Optimal Transport for  Distantly Supervised Named Entity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shuhui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yongliang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zeqi Tan</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+W">Wenqi Ren</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jietian Guo</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+S">Shiliang Pu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Weiming Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP-2023, camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Distantly supervised named entity recognition (DS-NER) aims to locate entity
mentions and classify their types with only knowledge bases or gazetteers and
unlabeled corpus. However, distant annotations are noisy and degrade the
performance of NER models. In this paper, we propose a noise-robust prototype
network named MProto for the DS-NER task. Different from previous
prototype-based NER methods, MProto represents each entity type with multiple
prototypes to characterize the intra-class variance among entity
representations. To optimize the classifier, each token should be assigned an
appropriate ground-truth prototype and we consider such token-prototype
assignment as an optimal transport (OT) problem. Furthermore, to mitigate the
noise from incomplete labeling, we propose a novel denoised optimal transport
(DOT) algorithm. Specifically, we utilize the assignment result between Other
class tokens and all prototypes to distinguish unlabeled entity tokens from
true negatives. Experiments on several DS-NER benchmarks demonstrate that our
MProto achieves state-of-the-art performance. The source code is now available
on Github.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08303" title="Abstract">arXiv:2310.08303</a> [<a href="/pdf/2310.08303" title="Download PDF">pdf</a>, <a href="/format/2310.08303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Variational Auto-encoder based Audio-Visual Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yuxin Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+M">Mochu Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yiran Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yuchao Dai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV2023,Project page(https://npucvr.github.io/MMVAE-AVS),Code(<a href="https://github.com/OpenNLPLab/MMVAE-AVS">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We propose an Explicit Conditional Multimodal Variational Auto-Encoder
(ECMVAE) for audio-visual segmentation (AVS), aiming to segment sound sources
in the video sequence. Existing AVS methods focus on implicit feature fusion
strategies, where models are trained to fit the discrete samples in the
dataset. With a limited and less diverse dataset, the resulting performance is
usually unsatisfactory. In contrast, we address this problem from an effective
representation learning perspective, aiming to model the contribution of each
modality explicitly. Specifically, we find that audio contains critical
category information of the sound producers, and visual data provides candidate
sound producer(s). Their shared information corresponds to the target sound
producer(s) shown in the visual data. In this case, cross-modal shared
representation learning is especially important for AVS. To achieve this, our
ECMVAE factorizes the representations of each modality with a modality-shared
representation and a modality-specific representation. An orthogonality
constraint is applied between the shared and specific representations to
maintain the exclusive attribute of the factorized latent code. Further, a
mutual information maximization regularizer is introduced to achieve extensive
exploration of each modality. Quantitative and qualitative evaluations on the
AVSBench demonstrate the effectiveness of our approach, leading to a new
state-of-the-art for AVS, with a 3.84 mIOU performance leap on the challenging
MS3 subset for multiple sound source segmentation.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08304" title="Abstract">arXiv:2310.08304</a> [<a href="/pdf/2310.08304" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CHIP: Contrastive Hierarchical Image Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mittal%2C+A">Arpit Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Jhaveri%2C+H">Harshil Jhaveri</a>, 
<a href="/search/cs?searchtype=author&query=Mallick%2C+S">Swapnil Mallick</a>, 
<a href="/search/cs?searchtype=author&query=Ajmera%2C+A">Abhishek Ajmera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Few-shot object classification is the task of classifying objects in an image
with limited number of examples as supervision. We propose a one-shot/few-shot
classification model that can classify an object of any unseen class into a
relatively general category in an hierarchically based classification. Our
model uses a three-level hierarchical contrastive loss based ResNet152
classifier for classifying an object based on its features extracted from Image
embedding, not used during the training phase. For our experimentation, we have
used a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal
classes for training our model and created our own dataset of unseen classes
for evaluating our trained model. Our model provides satisfactory results in
classifying the unknown objects into a generic category which has been later
discussed in greater detail.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08308" title="Abstract">arXiv:2310.08308</a> [<a href="/pdf/2310.08308" title="Download PDF">pdf</a>, <a href="/format/2310.08308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multicriteria Optimization of Lower Limb Exoskeleton Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ibrayev%2C+S">Sayat Ibrayev</a>, 
<a href="/search/cs?searchtype=author&query=Ibrayeva%2C+A">Arman Ibrayeva</a>, 
<a href="/search/cs?searchtype=author&query=Rakhmatullina%2C+A">Ayaulym Rakhmatullina</a>, 
<a href="/search/cs?searchtype=author&query=Ibrayeva%2C+A">Aizhan Ibrayeva</a>, 
<a href="/search/cs?searchtype=author&query=Amanov%2C+B">Bekzat Amanov</a>, 
<a href="/search/cs?searchtype=author&query=Imanbayeva%2C+N">Nurbibi Imanbayeva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Typical leg exoskeletons employ open-loop kinematic chains with motors placed
directly on movable joints; while this design offers flexibility, it leads to
increased costs and heightened control complexity due to the high number of
degrees of freedom. The use of heavy servo-motors to handle torque in active
joints results in complex and bulky designs, as highlighted in existing
literature. In this study, we introduced a novel synthesis method with
analytical solutions provided for synthesizing lower-limb exoskeleton.
Additionally, we have incorporated multicriteria optimization by six designing
criteria. As a result, we offer several mechanisms, comprising only six links,
well-suited to the human anatomical structure, exhibit superior trajectory
accuracy, efficient force transmission, satisfactory step height, and having
internal transfer segment of the foot.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08309" title="Abstract">arXiv:2310.08309</a> [<a href="/pdf/2310.08309" title="Download PDF">pdf</a>, <a href="/format/2310.08309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Not All Demonstration Examples are Equally Beneficial: Reweighting  Demonstration Examples for In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhe Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+D">Damai Dai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peiyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Z">Zhifang Sui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have recently gained the In-Context Learning
(ICL) ability with the models scaling up, allowing them to quickly adapt to
downstream tasks with only a few demonstration examples prepended in the input
sequence. Nonetheless, the current practice of ICL treats all demonstration
examples equally, which still warrants improvement, as the quality of examples
is usually uneven. In this paper, we investigate how to determine approximately
optimal weights for demonstration examples and how to apply them during ICL. To
assess the quality of weights in the absence of additional validation data, we
design a masked self-prediction (MSP) score that exhibits a strong correlation
with the final ICL performance. To expedite the weight-searching process, we
discretize the continuous weight space and adopt beam search. With
approximately optimal weights obtained, we further propose two strategies to
apply them to demonstrations at different model positions. Experimental results
on 8 text classification tasks show that our approach outperforms conventional
ICL by a large margin. Our code are publicly available at
https:github.com/Zhe-Young/WICL.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08312" title="Abstract">arXiv:2310.08312</a> [<a href="/pdf/2310.08312" title="Download PDF">pdf</a>, <a href="/format/2310.08312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GePSAn: Generative Procedure Step Anticipation in Cooking Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdelsalam%2C+M+A">Mohamed Ashraf Abdelsalam</a>, 
<a href="/search/cs?searchtype=author&query=Rangrej%2C+S+B">Samrudhdhi B. Rangrej</a>, 
<a href="/search/cs?searchtype=author&query=Hadji%2C+I">Isma Hadji</a>, 
<a href="/search/cs?searchtype=author&query=Dvornik%2C+N">Nikita Dvornik</a>, 
<a href="/search/cs?searchtype=author&query=Derpanis%2C+K+G">Konstantinos G. Derpanis</a>, 
<a href="/search/cs?searchtype=author&query=Fazly%2C+A">Afsaneh Fazly</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> published at ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the problem of future step anticipation in procedural videos. Given
a video of an ongoing procedural activity, we predict a plausible next
procedure step described in rich natural language. While most previous work
focus on the problem of data scarcity in procedural video datasets, another
core challenge of future anticipation is how to account for multiple plausible
future realizations in natural settings. This problem has been largely
overlooked in previous work. To address this challenge, we frame future step
prediction as modelling the distribution of all possible candidates for the
next step. Specifically, we design a generative model that takes a series of
video clips as input, and generates multiple plausible and diverse candidates
(in natural language) for the next step. Following previous work, we side-step
the video annotation scarcity by pretraining our model on a large text-based
corpus of procedural activities, and then transfer the model to the video
domain. Our experiments, both in textual and video domains, show that our model
captures diversity in the next step prediction and generates multiple plausible
future predictions. Moreover, our model establishes new state-of-the-art
results on YouCookII, where it outperforms existing baselines on the next step
anticipation. Finally, we also show that our model can successfully transfer
from text to the video domain zero-shot, ie, without fine-tuning or adaptation,
and produces good-quality future step predictions from video.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08314" title="Abstract">arXiv:2310.08314</a> [<a href="/pdf/2310.08314" title="Download PDF">pdf</a>, <a href="/format/2310.08314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information Design for Congestion Games with Unknown Demand
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Griesbach%2C+S+M">Svenja M. Griesbach</a>, 
<a href="/search/cs?searchtype=author&query=Hoefer%2C+M">Martin Hoefer</a>, 
<a href="/search/cs?searchtype=author&query=Klimm%2C+M">Max Klimm</a>, 
<a href="/search/cs?searchtype=author&query=Koglin%2C+T">Tim Koglin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2205.09823">arXiv:2205.09823</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study a novel approach to information design in the standard traffic model
of network congestion games. It captures the natural condition that the demand
is unknown to the users of the network. A principal (e.g., a mobility service)
commits to a signaling strategy, observes the realized demand and sends a
(public) signal to agents (i.e., users of the network). Based on the induced
belief about the demand, the users then form an equilibrium. We consider the
algorithmic goal of the principal: Compute a signaling scheme that minimizes
the expected total cost of the induced equilibrium. We concentrate on
single-commodity networks and affine cost functions, for which we obtain the
following results. First, we devise a fully polynomial-time approximation
scheme (FPTAS) for the case that the demand can only take two values. It relies
on several structural properties of the cost of the induced equilibrium as a
function of the updated belief about the distribution of demands. We show that
this function is piecewise linear for any number of demands, and monotonic for
two demands. Second, we give a complete characterization of the graph
structures for which it is optimal to fully reveal the information about the
realized demand. This signaling scheme turns out to be optimal for all cost
functions and probability distributions over demands if and only if the graph
is series-parallel. Third, we propose an algorithm that computes the optimal
signaling scheme for any number of demands whose time complexity is polynomial
in the number of supports that occur in a Wardrop equilibrium for some demand.
Finally, we conduct a computational study that tests this algorithm on
real-world instances.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08316" title="Abstract">arXiv:2310.08316</a> [<a href="/pdf/2310.08316" title="Download PDF">pdf</a>, <a href="/format/2310.08316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extended target tracking utilizing machine-learning software -- with  applications to animal classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malmstr%C3%B6m%2C+M">Magnus Malmstr&#xf6;m</a>, 
<a href="/search/cs?searchtype=author&query=Kullberg%2C+A">Anton Kullberg</a>, 
<a href="/search/cs?searchtype=author&query=Skog%2C+I">Isaac Skog</a>, 
<a href="/search/cs?searchtype=author&query=Axehill%2C+D">Daniel Axehill</a>, 
<a href="/search/cs?searchtype=author&query=Gustafsson%2C+F">Fredrik Gustafsson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper considers the problem of detecting and tracking objects in a
sequence of images. The problem is formulated in a filtering framework, using
the output of object-detection algorithms as measurements. An extension to the
filtering formulation is proposed that incorporates class information from the
previous frame to robustify the classification, even if the object-detection
algorithm outputs an incorrect prediction. Further, the properties of the
object-detection algorithm are exploited to quantify the uncertainty of the
bounding box detection in each frame. The complete filtering method is
evaluated on camera trap images of the four large Swedish carnivores, bear,
lynx, wolf, and wolverine. The experiments show that the class tracking
formulation leads to a more robust classification.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08319" title="Abstract">arXiv:2310.08319</a> [<a href="/pdf/2310.08319" title="Download PDF">pdf</a>, <a href="/format/2310.08319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-Tuning LLaMA for Multi-Stage Text Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xueguang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+N">Nan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jimmy Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The effectiveness of multi-stage text retrieval has been solidly demonstrated
since before the era of pre-trained language models. However, most existing
studies utilize models that predate recent advances in large language models
(LLMs). This study seeks to explore potential improvements that
state-of-the-art LLMs can bring. We conduct a comprehensive study, fine-tuning
the latest LLaMA model both as a dense retriever (RepLLaMA) and as a pointwise
reranker (RankLLaMA) for both passage retrieval and document retrieval using
the MS MARCO datasets. Our findings demonstrate that the effectiveness of large
language models indeed surpasses that of smaller models. Additionally, since
LLMs can inherently handle longer contexts, they can represent entire documents
holistically, obviating the need for traditional segmenting and pooling
strategies. Furthermore, evaluations on BEIR demonstrate that our
RepLLaMA-RankLLaMA pipeline exhibits strong zero-shot effectiveness. Model
checkpoints from this study are available on HuggingFace.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08320" title="Abstract">arXiv:2310.08320</a> [<a href="/pdf/2310.08320" title="Download PDF">pdf</a>, <a href="/format/2310.08320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defending Our Privacy With Backdoors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hintersdorf%2C+D">Dominik Hintersdorf</a>, 
<a href="/search/cs?searchtype=author&query=Struppek%2C+L">Lukas Struppek</a>, 
<a href="/search/cs?searchtype=author&query=Neider%2C+D">Daniel Neider</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The proliferation of large AI models trained on uncurated, often sensitive
web-scraped data has raised significant privacy concerns. One of the concerns
is that adversaries can extract information about the training data using
privacy attacks. Unfortunately, the task of removing specific information from
the models without sacrificing performance is not straightforward and has
proven to be challenging. We propose a rather easy yet effective defense based
on backdoor attacks to remove private information such as names of individuals
from models, and focus in this work on text encoders. Specifically, through
strategic insertion of backdoors, we align the embeddings of sensitive phrases
with those of neutral terms-"a person" instead of the person's name. Our
empirical results demonstrate the effectiveness of our backdoor-based defense
on CLIP by assessing its performance using a specialized privacy attack for
zero-shot classifiers. Our approach provides not only a new "dual-use"
perspective on backdoor attacks, but also presents a promising avenue to
enhance the privacy of individuals within models trained on uncurated
web-scraped data.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08323" title="Abstract">arXiv:2310.08323</a> [<a href="/pdf/2310.08323" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robo Sapiens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ash%2C+C">Chaim Ash</a>, 
<a href="/search/cs?searchtype=author&query=Hans%2C+A">Amelia Hans</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Computer Science (cs.OH)</span>

</div>
<p class="mathjax">This paper proposes a new method of natural language acquisition for robots
that does not require the conversion of speech to text. Folks'Talks employs
voice2voice technology that enables a robot to understand the meaning of what
it is told and to have the ability to learn and understand new languages -
inclusive of accent, dialect, and physiological differences. To do this, sound
processing and computer vision are incorporated to give the robot a sense of
spatiotemporal causality. The "language model" we are proposing equips a robot
to imitate a natural speaker's conversational behavior by thinking contextually
and articulating its surroundings.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08326" title="Abstract">arXiv:2310.08326</a> [<a href="/pdf/2310.08326" title="Download PDF">pdf</a>, <a href="/format/2310.08326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NSM4D: Neural Scene Model Based Online 4D Point Cloud Sequence  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuhao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuoyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yunze Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+L">Li Yi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Understanding 4D point cloud sequences online is of significant practical
value in various scenarios such as VR/AR, robotics, and autonomous driving. The
key goal is to continuously analyze the geometry and dynamics of a 3D scene as
unstructured and redundant point cloud sequences arrive. And the main challenge
is to effectively model the long-term history while keeping computational costs
manageable. To tackle these challenges, we introduce a generic online 4D
perception paradigm called NSM4D. NSM4D serves as a plug-and-play strategy that
can be adapted to existing 4D backbones, significantly enhancing their online
perception capabilities for both indoor and outdoor scenarios. To efficiently
capture the redundant 4D history, we propose a neural scene model that
factorizes geometry and motion information by constructing geometry tokens
separately storing geometry and motion features. Exploiting the history becomes
as straightforward as querying the neural scene model. As the sequence
progresses, the neural scene model dynamically deforms to align with new
observations, effectively providing the historical context and updating itself
with the new observations. By employing token representation, NSM4D also
exhibits robustness to low-level sensor noise and maintains a compact size
through a geometric sampling scheme. We integrate NSM4D with state-of-the-art
4D perception backbones, demonstrating significant improvements on various
online perception benchmarks in indoor and outdoor settings. Notably, we
achieve a 9.6% accuracy improvement for HOI4D online action segmentation and a
3.4% mIoU improvement for SemanticKITTI online semantic segmentation.
Furthermore, we show that NSM4D inherently offers excellent scalability to
longer sequences beyond the training set, which is crucial for real-world
applications.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08327" title="Abstract">arXiv:2310.08327</a> [<a href="/pdf/2310.08327" title="Download PDF">pdf</a>, <a href="/format/2310.08327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Z3-Noodler: An Automata-based String Solver
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu-Fang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chocholat%C3%BD%2C+D">David Chocholat&#xfd;</a>, 
<a href="/search/cs?searchtype=author&query=Havlena%2C+V">Vojt&#x11b;ch Havlena</a>, 
<a href="/search/cs?searchtype=author&query=Hol%C3%ADk%2C+L">Luk&#xe1;&#x161; Hol&#xed;k</a>, 
<a href="/search/cs?searchtype=author&query=Leng%C3%A1l%2C+O">Ond&#x159;ej Leng&#xe1;l</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%AD%C4%8D%2C+J">Juraj S&#xed;&#x10d;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">Z3-Noodler is a fork of Z3 that replaces its string theory solver with a
custom solver implementing the recently introduced stabilization-based
algorithm for solving word equations with regular constraints. An extensive
experimental evaluation shows that Z3-Noodler is a fully-fledged solver that
can compete with state-of-the-art solvers, surpassing them by far on many
benchmarks. Moreover, it is often complementary to other solvers, making it a
suitable choice as a candidate to a solver portfolio.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08328" title="Abstract">arXiv:2310.08328</a> [<a href="/pdf/2310.08328" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transport-Hub-Aware Spatial-Temporal Adaptive Graph Transformer for  Traffic Flow Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bailong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Zhizhen Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuefei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures. arXiv admin note: text overlap with <a href="/abs/2301.07945">arXiv:2301.07945</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">As a core technology of Intelligent Transportation System (ITS), traffic flow
prediction has a wide range of applications. Traffic flow data are
spatial-temporal, which are not only correlated to spatial locations in road
networks, but also vary with temporal time indices. Existing methods have
solved the challenges in traffic flow prediction partly, focusing on modeling
spatial-temporal dependencies effectively, while not all intrinsic properties
of traffic flow data are utilized fully. Besides, there are very few attempts
at incremental learning of spatial-temporal data mining, and few previous works
can be easily transferred to the traffic flow prediction task. Motivated by the
challenge of incremental learning methods for traffic flow prediction and the
underutilization of intrinsic properties of road networks, we propose a
Transport-Hub-aware Spatial-Temporal adaptive graph transFormer (H-STFormer)
for traffic flow prediction. Specifically, we first design a novel spatial
self-attention module to capture the dynamic spatial dependencies. Three graph
masking matrices are integrated into spatial self-attentions to highlight both
short- and long-term dependences. Additionally, we employ a temporal
self-attention module to detect dynamic temporal patterns in the traffic flow
data. Finally, we design an extra spatial-temporal knowledge distillation
module for incremental learning of traffic flow prediction tasks. Through
extensive experiments, we show the effectiveness of H-STFormer in normal and
incremental traffic flow prediction tasks. The code is available at
https://github.com/Fantasy-Shaw/H-STFormer.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08332" title="Abstract">arXiv:2310.08332</a> [<a href="/pdf/2310.08332" title="Download PDF">pdf</a>, <a href="/format/2310.08332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-Time Neural BRDF with Spherically Distributed Primitives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dou%2C+Y">Yishun Dou</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qiaoqiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+B">Bingbing Ni</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yugang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+J">Junxiang Ke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose a novel compact and efficient neural BRDF offering highly
versatile material representation, yet with very-light memory and neural
computation consumption towards achieving real-time rendering. The results in
Figure 1, rendered at full HD resolution on a current desktop machine, show
that our system achieves real-time rendering with a wide variety of
appearances, which is approached by the following two designs. On the one hand,
noting that bidirectional reflectance is distributed in a very sparse
high-dimensional subspace, we propose to project the BRDF into two
low-dimensional components, i.e., two hemisphere feature-grids for incoming and
outgoing directions, respectively. On the other hand, learnable neural
reflectance primitives are distributed on our highly-tailored spherical surface
grid, which offer informative features for each component and alleviate the
conventional heavy feature learning network to a much smaller one, leading to
very fast evaluation. These primitives are centrally stored in a codebook and
can be shared across multiple grids and even across materials, based on the
low-cost indices stored in material-specific spherical surface grids. Our
neural BRDF, which is agnostic to the material, provides a unified framework
that can represent a variety of materials in consistent manner. Comprehensive
experimental results on measured BRDF compression, Monte Carlo simulated BRDF
acceleration, and extension to spatially varying effect demonstrate the
superior quality and generalizability achieved by the proposed scheme.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08335" title="Abstract">arXiv:2310.08335</a> [<a href="/pdf/2310.08335" title="Download PDF">pdf</a>, <a href="/format/2310.08335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 2SFGL: A Simple And Robust Protocol For Graph-Based Fraud Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zhirui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guangzhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhaoning Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yang Bian</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Z">Zhongyuan Lai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Financial crime detection using graph learning improves financial safety and
efficiency. However, criminals may commit financial crimes across different
institutions to avoid detection, which increases the difficulty of detection
for financial institutions which use local data for graph learning. As most
financial institutions are subject to strict regulations in regards to data
privacy protection, the training data is often isolated and conventional
learning technology cannot handle the problem. Federated learning (FL) allows
multiple institutions to train a model without revealing their datasets to each
other, hence ensuring data privacy protection. In this paper, we proposes a
novel two-stage approach to federated graph learning (2SFGL): The first stage
of 2SFGL involves the virtual fusion of multiparty graphs, and the second
involves model training and inference on the virtual graph. We evaluate our
framework on a conventional fraud detection task based on the
FraudAmazonDataset and FraudYelpDataset. Experimental results show that
integrating and applying a GCN (Graph Convolutional Network) with our 2SFGL
framework to the same task results in a 17.6\%-30.2\% increase in performance
on several typical metrics compared to the case only using FedAvg, while
integrating GraphSAGE with 2SFGL results in a 6\%-16.2\% increase in
performance compared to the case only using FedAvg. We conclude that our
proposed framework is a robust and simple protocol which can be simply
integrated to pre-existing graph-based fraud detection methods.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08337" title="Abstract">arXiv:2310.08337</a> [<a href="/pdf/2310.08337" title="Download PDF">pdf</a>, <a href="/format/2310.08337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bartosh%2C+G">Grigory Bartosh</a>, 
<a href="/search/cs?searchtype=author&query=Vetrov%2C+D">Dmitry Vetrov</a>, 
<a href="/search/cs?searchtype=author&query=Naesseth%2C+C+A">Christian A. Naesseth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Diffusion models have shown remarkable performance on many generative tasks.
Despite recent success, most diffusion models are restricted in that they only
allow linear transformation of the data distribution. In contrast, broader
family of transformations can potentially help train generative distributions
more efficiently, simplifying the reverse process and closing the gap between
the true negative log-likelihood and the variational approximation. In this
paper, we present Neural Diffusion Models (NDMs), a generalization of
conventional diffusion models that enables defining and learning time-dependent
non-linear transformations of data. We show how to optimise NDMs using a
variational bound in a simulation-free setting. Moreover, we derive a
time-continuous formulation of NDMs, which allows fast and reliable inference
using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the
utility of NDMs with learnable transformations through experiments on standard
image generation benchmarks, including CIFAR-10, downsampled versions of
ImageNet and CelebA-HQ. NDMs outperform conventional diffusion models in terms
of likelihood and produce high-quality samples.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08339" title="Abstract">arXiv:2310.08339</a> [<a href="/pdf/2310.08339" title="Download PDF">pdf</a>, <a href="/format/2310.08339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Generic Software Framework for Distributed Topological Analysis  Pipelines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guillou%2C+E+L">Eve Le Guillou</a>, 
<a href="/search/cs?searchtype=author&query=Will%2C+M">Michael Will</a>, 
<a href="/search/cs?searchtype=author&query=Guillou%2C+P">Pierre Guillou</a>, 
<a href="/search/cs?searchtype=author&query=Lukasczyk%2C+J">Jonas Lukasczyk</a>, 
<a href="/search/cs?searchtype=author&query=Fortin%2C+P">Pierre Fortin</a>, 
<a href="/search/cs?searchtype=author&query=Garth%2C+C">Christoph Garth</a>, 
<a href="/search/cs?searchtype=author&query=Tierny%2C+J">Julien Tierny</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Computational Geometry (cs.CG); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Mathematical Software (cs.MS)

</div>
<p class="mathjax">This system paper presents a software framework for the support of
topological analysis pipelines in a distributed-memory model. While several
recent papers introduced topology-based approaches for distributed-memory
environments, these were reporting experiments obtained with tailored,
mono-algorithm implementations. In contrast, we describe in this paper a
general-purpose, generic framework for topological analysis pipelines, i.e. a
sequence of topological algorithms interacting together, possibly on distinct
numbers of processes. Specifically, we instantiated our framework with the MPI
model, within the Topology ToolKit (TTK). While developing this framework, we
faced several algorithmic and software engineering challenges, which we
document in this paper. We provide a taxonomy for the distributed-memory
topological algorithms supported by TTK, depending on their communication needs
and provide examples of hybrid MPI+thread parallelizations. Detailed
performance analyses show that parallel efficiencies range from $20\%$ to
$80\%$ (depending on the algorithms), and that the MPI-specific preconditioning
introduced by our framework induces a negligible computation time overhead. We
illustrate the new distributed-memory capabilities of TTK with an example of
advanced analysis pipeline, combining multiple algorithms, run on the largest
publicly available dataset we have found (120 billion vertices) on a standard
cluster with 64 nodes (for a total of 1,536 cores). Finally, we provide a
roadmap for the completion of TTK's MPI extension, along with generic
recommendations for each algorithm communication category.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08342" title="Abstract">arXiv:2310.08342</a> [<a href="/pdf/2310.08342" title="Download PDF">pdf</a>, <a href="/format/2310.08342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discontinuous Galerkin for the heterodimer model of prion dynamics in  Parkinson&#x27;s disease
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Antonietti%2C+P+F">Paola F. Antonietti</a>, 
<a href="/search/math?searchtype=author&query=Bonizzoni%2C+F">Francesca Bonizzoni</a>, 
<a href="/search/math?searchtype=author&query=Corti%2C+M">Mattia Corti</a>, 
<a href="/search/math?searchtype=author&query=Dall%27Olio%2C+A">Agnese Dall&#x27;Olio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Neurodegenerative diseases have a significant global impact affecting
millions of individuals worldwide. Some of them, known as proteinopathies, are
characterized by the accumulation and propagation of toxic proteins, known as
prions. Alzheimer's and Parkinson's diseases are relevant of protheinopathies.
Mathematical models of prion dynamics play a crucial role in understanding
disease progression and could be of help to potential interventions. This
article focuses on the heterodimer model: a system of two partial differential
equations that describe the evolution of healthy and misfolded proteins. In
particular, we propose a space discretization based on a Discontinuous Galerkin
method on polygonal/polyhedral grids, which provides flexibility in handling
meshes of complex brain geometries. Concerning the semi-discrete formulation we
prove stability and a-priori error estimates. Next, we adopt a
$\vartheta$-method scheme for time discretization. Some convergence tests are
performed to confirm the theoretical bounds and the ability of the method to
approximate travelling wave solutions. The proposed scheme is also tested to
simulate the spread of $\alpha$-synuclein in a realistic test case of
Parkinson's disease in a two-dimensional sagittal brain section geometry
reconstructed from medical images.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08344" title="Abstract">arXiv:2310.08344</a> [<a href="/pdf/2310.08344" title="Download PDF">pdf</a>, <a href="/ps/2310.08344" title="Download PostScript">ps</a>, <a href="/format/2310.08344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LeXInt: GPU-accelerated Exponential Integrators package
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Deka%2C+P+J">Pranab J. Deka</a>, 
<a href="/search/math?searchtype=author&query=Moriggl%2C+A">Alexander Moriggl</a>, 
<a href="/search/math?searchtype=author&query=Einkemmer%2C+L">Lukas Einkemmer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at <a href="https://github.com/Pranab-JD/LeXInt">this https URL</a>; comments welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Mathematical Physics (math-ph); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">We present an open-source CUDA-based package that consists of a compilation
of exponential integrators where the action of the matrix exponential or the
$\varphi_l$ functions on a vector is approximated using the method of
polynomial interpolation at Leja points. Using a couple of test examples on an
NVIDIA A100 GPU, we show that one can achieve significant speedups using CUDA
over the corresponding CPU code. LeXInt, written in a modular format,
facilitates easy integration into any existing software package, and can be
used for temporal integration of any differential equation.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08348" title="Abstract">arXiv:2310.08348</a> [<a href="/pdf/2310.08348" title="Download PDF">pdf</a>, <a href="/format/2310.08348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LightZero: A Unified Benchmark for Monte Carlo Tree Search in General  Sequential Decision Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niu%2C+Y">Yazhe Niu</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+Y">Yuan Pu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhenjie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xueyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jiyuan Ren</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shuai Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongsheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Building agents based on tree-search planning capabilities with learned
models has achieved remarkable success in classic decision-making problems,
such as Go and Atari. However, it has been deemed challenging or even
infeasible to extend Monte Carlo Tree Search (MCTS) based algorithms to diverse
real-world applications, especially when these environments involve complex
action spaces and significant simulation costs, or inherent stochasticity. In
this work, we introduce LightZero, the first unified benchmark for deploying
MCTS/MuZero in general sequential decision scenarios. Specificially, we
summarize the most critical challenges in designing a general MCTS-style
decision-making solver, then decompose the tightly-coupled algorithm and system
design of tree-search RL methods into distinct sub-modules. By incorporating
more appropriate exploration and optimization strategies, we can significantly
enhance these sub-modules and construct powerful LightZero agents to tackle
tasks across a wide range of domains, such as board games, Atari, MuJoCo,
MiniGrid and GoBigger. Detailed benchmark results reveal the significant
potential of such methods in building scalable and efficient decision
intelligence. The code is available as part of OpenDILab at
https://github.com/opendilab/LightZero.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08349" title="Abstract">arXiv:2310.08349</a> [<a href="/pdf/2310.08349" title="Download PDF">pdf</a>, <a href="/format/2310.08349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performativity and Prospective Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zezulka%2C+S">Sebastian Zezulka</a>, 
<a href="/search/cs?searchtype=author&query=Genin%2C+K">Konstantin Genin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Deploying an algorithmically informed policy is a significant intervention in
the structure of society. As is increasingly acknowledged, predictive
algorithms have performative effects: using them can shift the distribution of
social outcomes away from the one on which the algorithms were trained.
Algorithmic fairness research is usually motivated by the worry that these
performative effects will exacerbate the structural inequalities that gave rise
to the training data. However, standard retrospective fairness methodologies
are ill-suited to predict these effects. They impose static fairness
constraints that hold after the predictive algorithm is trained, but before it
is deployed and, therefore, before performative effects have had a chance to
kick in. However, satisfying static fairness criteria after training is not
sufficient to avoid exacerbating inequality after deployment. Addressing the
fundamental worry that motivates algorithmic fairness requires explicitly
comparing the change in relevant structural inequalities before and after
deployment. We propose a prospective methodology for estimating this
post-deployment change from pre-deployment data and knowledge about the
algorithmic policy. That requires a strategy for distinguishing between, and
accounting for, different kinds of performative effects. In this paper, we
focus on the algorithmic effect on the causally downstream outcome variable.
Throughout, we are guided by an application from public administration: the use
of algorithms to (1) predict who among the recently unemployed will stay
unemployed for the long term and (2) targeting them with labor market programs.
We illustrate our proposal by showing how to predict whether such policies will
exacerbate gender inequalities in the labor market.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08350" title="Abstract">arXiv:2310.08350</a> [<a href="/pdf/2310.08350" title="Download PDF">pdf</a>, <a href="/format/2310.08350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALPHA: Attention-based Long-horizon Pathfinding in Highly-structured  Areas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+C">Chengyang He</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianze Yang</a>, 
<a href="/search/cs?searchtype=author&query=Duhan%2C+T">Tanishq Duhan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yutong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sartoretti%2C+G">Guillaume Sartoretti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the IEEE International Conference on Robotics and Automation (ICRA 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The multi-agent pathfinding (MAPF) problem seeks collision-free paths for a
team of agents from their current positions to their pre-set goals in a known
environment, and is an essential problem found at the core of many logistics,
transportation, and general robotics applications. Existing learning-based MAPF
approaches typically only let each agent make decisions based on a limited
field-of-view (FOV) around its position, as a natural means to fix the input
dimensions of its policy network. However, this often makes policies
short-sighted, since agents lack the ability to perceive and plan for
obstacles/agents beyond their FOV. To address this challenge, we propose ALPHA,
a new framework combining the use of ground truth proximal (local) information
and fuzzy distal (global) information to let agents sequence local decisions
based on the full current state of the system, and avoid such myopicity. We
further allow agents to make short-term predictions about each others' paths,
as a means to reason about each others' path intentions, thereby enhancing the
level of cooperation among agents at the whole system level. Our neural
structure relies on a Graph Transformer architecture to allow agents to
selectively combine these different sources of information and reason about
their inter-dependencies at different spatial scales. Our simulation
experiments demonstrate that ALPHA outperforms both globally-guided MAPF
solvers and communication-learning based ones, showcasing its potential towards
scalability in realistic deployments.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08352" title="Abstract">arXiv:2310.08352</a> [<a href="/pdf/2310.08352" title="Download PDF">pdf</a>, <a href="/format/2310.08352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral deferred correction methods for second-order problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Akramov%2C+I">Ikrom Akramov</a>, 
<a href="/search/math?searchtype=author&query=G%C3%B6tschel%2C+S">Sebastian G&#xf6;tschel</a>, 
<a href="/search/math?searchtype=author&query=Minion%2C+M">Michael Minion</a>, 
<a href="/search/math?searchtype=author&query=Ruprecht%2C+D">Daniel Ruprecht</a>, 
<a href="/search/math?searchtype=author&query=Speck%2C+R">Robert Speck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Spectral deferred corrections (SDC) are a class of iterative methods for the
numerical solution of ordinary differential equations. SDC can be interpreted
as a Picard iteration to solve a fully implicit collocation problem,
preconditioned with a low-order method. It has been widely studied for
first-order problems, using explicit, implicit or implicit-explicit Euler and
other low-order methods as preconditioner. For first-order problems, SDC
achieves arbitrary order of accuracy and possesses good stability properties.
While numerical results for SDC applied to the second-order Lorentz equations
exist, no theoretical results are available for SDC applied to second-order
problems.
<br />We present an analysis of the convergence and stability properties of SDC
using velocity-Verlet as the base method for general second-order initial value
problems. Our analysis proves that the order of convergence depends on whether
the force in the system depends on the velocity. We also demonstrate that the
SDC iteration is stable under certain conditions. Finally, we show that SDC can
be computationally more efficient than a simple Picard iteration or a
fourth-order Runge-Kutta-Nystr\"om method.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08356" title="Abstract">arXiv:2310.08356</a> [<a href="/pdf/2310.08356" title="Download PDF">pdf</a>, <a href="/format/2310.08356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A new local and explicit kinetic method for linear and non-linear  convection-diffusion problems with finite kinetic speeds: I. One-dimensional  case
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wissocq%2C+G">Gauthier Wissocq</a>, 
<a href="/search/math?searchtype=author&query=Abgrall%2C+R">R&#xe9;mi Abgrall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose a numerical approach, of the BGK kinetic type, that is able to
approximate with a given, but arbitrary, order of accuracy the solution of
linear and non-linear convection-diffusion type problems: scalar
advection-diffusion, non-linear scalar problems of this type and the
compressible Navier-Stokes equations. Our kinetic model can use \emph{finite}
advection speeds that are independent of the relaxation parameter, and the time
step does not suffer from a parabolic constraint. Having finite speeds is in
contrast with many of the previous works about this kind of approach, and we
explain why this is possible: paraphrasing more or less
\cite{golse:hal-00859451}, the convection-diffusion like PDE is not a limit of
the BGK equation, but a correction of the same PDE without the parabolic term
at the second order in the relaxation parameter that is interpreted as Knudsen
number. We then show that introducing a matrix collision instead of the
well-known BGK relaxation makes it possible to target a desired
convection-diffusion system.
<br />Several numerical examples, ranging from a simple pure diffusion model to the
compressible Navier-Stokes equations illustrate our approach
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08358" title="Abstract">arXiv:2310.08358</a> [<a href="/pdf/2310.08358" title="Download PDF">pdf</a>, <a href="/format/2310.08358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Demystifying the Generalization Behaviors When Neural Collapse  Emerges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Peifeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qianqian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yibo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+P">Peisong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+H">Huiyang Shao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhiyong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ghanem%2C+B">Bernard Ghanem</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qingming Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 6 figures. arXiv admin note: substantial text overlap with <a href="/abs/2304.08914">arXiv:2304.08914</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Neural Collapse (NC) is a well-known phenomenon of deep neural networks in
the terminal phase of training (TPT). It is characterized by the collapse of
features and classifier into a symmetrical structure, known as simplex
equiangular tight frame (ETF). While there have been extensive studies on
optimization characteristics showing the global optimality of neural collapse,
little research has been done on the generalization behaviors during the
occurrence of NC. Particularly, the important phenomenon of generalization
improvement during TPT has been remaining in an empirical observation and
lacking rigorous theoretical explanation. In this paper, we establish the
connection between the minimization of CE and a multi-class SVM during TPT, and
then derive a multi-class margin generalization bound, which provides a
theoretical explanation for why continuing training can still lead to accuracy
improvement on test set, even after the train accuracy has reached 100%.
Additionally, our further theoretical results indicate that different alignment
between labels and features in a simplex ETF can result in varying degrees of
generalization improvement, despite all models reaching NC and demonstrating
similar optimization performance on train set. We refer to this newly
discovered property as "non-conservative generalization". In experiments, we
also provide empirical observations to verify the indications suggested by our
theoretical results.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08361" title="Abstract">arXiv:2310.08361</a> [<a href="/pdf/2310.08361" title="Download PDF">pdf</a>, <a href="/format/2310.08361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Would you trust a vehicle merging into your lane? Subjective evaluation  of negotiating behaviour in a congested merging scenario
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goto%2C+A">Akinobu Goto</a>, 
<a href="/search/cs?searchtype=author&query=Eder%2C+K">Kerstin Eder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 12 figures, accepted by the 2024 16th IEEE/SICE International Symposium on System Integration
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Aiming for a society where humans and automated vehicles can coexist
cooperatively, understanding what constitutes cooperative and trustworthy
behaviour is essential to designing automated vehicle controllers that enable
the integration of highly automated vehicles into the real world. This study
investigates how merging vehicles can gain trust from human-driven vehicles in
a congested merging situation that requires explicit and implicit
communication. Specifically, this study examines how the different behaviours
of merging vehicles in the preparatory phase of the merge affect perceived
trust from the perspective of the host vehicle in the mainstream lane. The
findings suggest that transparent longitudinal positioning could improve the
chance of successful merging, and cooperative deceleration during merging
preparation could enhance the trust perceived by the host vehicle. Furthermore,
the results reveal that, in time-sensitive situations where the merging vehicle
approaches a lane closing point, prompt and decisive action of the merging
vehicle encourages establishing trust with the host vehicle; any delay or
hesitation can result in a lower level of trust. The results can provide
valuable insights towards developing collaborative automated vehicles that
improve safety and efficiency in real-world traffic situations that involve
humans.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08362" title="Abstract">arXiv:2310.08362</a> [<a href="/pdf/2310.08362" title="Download PDF">pdf</a>, <a href="/format/2310.08362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Value Alignment in Normative Multi-Agent System: An Evolutionary  Optimisation Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Riad%2C+M">Maha Riad</a>, 
<a href="/search/cs?searchtype=author&query=de+Carvalho%2C+V">Vinicius de Carvalho</a>, 
<a href="/search/cs?searchtype=author&query=Golpayegani%2C+F">Fatemeh Golpayegani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in MODeM 2023 Workshop at ECAI 2023. arXiv admin note: substantial text overlap with <a href="/abs/2305.07366">arXiv:2305.07366</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Value-alignment in normative multi-agent systems is used to promote a certain
value and to ensure the consistent behaviour of agents in autonomous
intelligent systems with human values. However, the current literature is
limited to the incorporation of effective norms for single-value alignment with
no consideration of agents' heterogeneity and the requirement of simultaneous
promotion and alignment of multiple values. This research proposes a
multi-value promotion model that uses multi-objective evolutionary algorithms
and decentralised reasoning to produce the optimum parametric set of norms that
is aligned with multiple simultaneous values of heterogeneous agents and the
system. To understand various aspects of this complex problem, several
evolutionary algorithms were used to find a set of optimised norm parameters
considering two toy tax scenarios with two and five values are considered. The
results are analysed from different perspectives to show the impact of a
selected evolutionary algorithm on the solution, and the importance of
understanding the relation between values when prioritising them.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08364" title="Abstract">arXiv:2310.08364</a> [<a href="/pdf/2310.08364" title="Download PDF">pdf</a>, <a href="/format/2310.08364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Map2Schedule: An End-to-End Link Scheduling Method for Urban V2V  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lihao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haijian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Parasuraman%2C+R">Ramviyas Parasuraman</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yinghui Ye</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R+Q">Rose Qingyang Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to IEEE conference for future publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Urban vehicle-to-vehicle (V2V) link scheduling with shared spectrum is a
challenging problem. Its main goal is to find the scheduling policy that can
maximize system performance (usually the sum capacity of each link or their
energy efficiency). Given that each link can experience interference from all
other active links, the scheduling becomes a combinatorial integer programming
problem and generally does not scale well with the number of V2V pairs.
Moreover, link scheduling requires accurate channel state information (CSI),
which is very difficult to estimate with good accuracy under high vehicle
mobility. In this paper, we propose an end-to-end urban V2V link scheduling
method called Map2Schedule, which can directly generate V2V scheduling policy
from the city map and vehicle locations. Map2Schedule delivers comparable
performance to the physical-model-based methods in urban settings while
maintaining low computation complexity. This enhanced performance is achieved
by machine learning (ML) technologies. Specifically, we first deploy the
convolutional neural network (CNN) model to estimate the CSI from street layout
and vehicle locations and then apply the graph embedding model for optimal
scheduling policy. The results show that the proposed method can achieve high
accuracy with much lower overhead and latency.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08365" title="Abstract">arXiv:2310.08365</a> [<a href="/pdf/2310.08365" title="Download PDF">pdf</a>, <a href="/format/2310.08365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Large Language Models to Knowledge Graphs for Biomarker Discovery  in Cancer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karim%2C+M+R">Md. Rezaul Karim</a>, 
<a href="/search/cs?searchtype=author&query=Comet%2C+L+M">Lina Molinas Comet</a>, 
<a href="/search/cs?searchtype=author&query=Shajalal%2C+M">Md Shajalal</a>, 
<a href="/search/cs?searchtype=author&query=Beyan%2C+O">Oya Beyan</a>, 
<a href="/search/cs?searchtype=author&query=Rebholz-Schuhmann%2C+D">Dietrich Rebholz-Schuhmann</a>, 
<a href="/search/cs?searchtype=author&query=Decker%2C+S">Stefan Decker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2302.04737">arXiv:2302.04737</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Domain experts often rely on up-to-date knowledge for apprehending and
disseminating specific biological processes that help them design strategies to
develop prevention and therapeutic decision-making. A challenging scenario for
artificial intelligence (AI) is using biomedical data (e.g., texts, imaging,
omics, and clinical) to provide diagnosis and treatment recommendations for
cancerous conditions. Data and knowledge about cancer, drugs, genes, proteins,
and their mechanism is spread across structured (knowledge bases (KBs)) and
unstructured (e.g., scientific articles) sources. A large-scale knowledge graph
(KG) can be constructed by integrating these data, followed by extracting facts
about semantically interrelated entities and relations. Such KGs not only allow
exploration and question answering (QA) but also allow domain experts to deduce
new knowledge. However, exploring and querying large-scale KGs is tedious for
non-domain users due to a lack of understanding of the underlying data assets
and semantic technologies. In this paper, we develop a domain KG to leverage
cancer-specific biomarker discovery and interactive QA. For this, a domain
ontology called OncoNet Ontology (ONO) is developed to enable semantic
reasoning for validating gene-disease relations. The KG is then enriched by
harmonizing the ONO, controlled vocabularies, and additional biomedical
concepts from scientific articles by employing BioBERT- and SciBERT-based
information extraction (IE) methods. Further, since the biomedical domain is
evolving, where new findings often replace old ones, without employing
up-to-date findings, there is a high chance an AI system exhibits concept drift
while providing diagnosis and treatment. Therefore, we finetuned the KG using
large language models (LLMs) based on more recent articles and KBs that might
not have been seen by the named entity recognition models.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08367" title="Abstract">arXiv:2310.08367</a> [<a href="/pdf/2310.08367" title="Download PDF">pdf</a>, <a href="/format/2310.08367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MCU: A Task-centric Framework for Open-ended Agent Evaluation in  Minecraft
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haowei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jianzhu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yitao Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">To pursue the goal of creating an open-ended agent in Minecraft, an
open-ended game environment with unlimited possibilities, this paper introduces
a task-centric framework named MCU for Minecraft agent evaluation. The MCU
framework leverages the concept of atom tasks as fundamental building blocks,
enabling the generation of diverse or even arbitrary tasks. Within the MCU
framework, each task is measured with six distinct difficulty scores (time
consumption, operational effort, planning complexity, intricacy, creativity,
novelty). These scores offer a multi-dimensional assessment of a task from
different angles, and thus can reveal an agent's capability on specific facets.
The difficulty scores also serve as the feature of each task, which creates a
meaningful task space and unveils the relationship between tasks. For efficient
evaluation of Minecraft agents employing the MCU framework, we maintain a
unified benchmark, namely SkillForge, which comprises representative tasks with
diverse categories and difficulty distribution. We also provide convenient
filters for users to select tasks to assess specific capabilities of agents. We
show that MCU has the high expressivity to cover all tasks used in recent
literature on Minecraft agent, and underscores the need for advancements in
areas such as creativity, precise control, and out-of-distribution
generalization under the goal of open-ended Minecraft agent development.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08368" title="Abstract">arXiv:2310.08368</a> [<a href="/pdf/2310.08368" title="Download PDF">pdf</a>, <a href="/format/2310.08368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mapping Memes to Words for Multimodal Hateful Meme Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Burbi%2C+G">Giovanni Burbi</a>, 
<a href="/search/cs?searchtype=author&query=Baldrati%2C+A">Alberto Baldrati</a>, 
<a href="/search/cs?searchtype=author&query=Agnolucci%2C+L">Lorenzo Agnolucci</a>, 
<a href="/search/cs?searchtype=author&query=Bertini%2C+M">Marco Bertini</a>, 
<a href="/search/cs?searchtype=author&query=Del+Bimbo%2C+A">Alberto Del Bimbo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV2023 CLVL Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multimodal image-text memes are prevalent on the internet, serving as a
unique form of communication that combines visual and textual elements to
convey humor, ideas, or emotions. However, some memes take a malicious turn,
promoting hateful content and perpetuating discrimination. Detecting hateful
memes within this multimodal context is a challenging task that requires
understanding the intertwined meaning of text and images. In this work, we
address this issue by proposing a novel approach named ISSUES for multimodal
hateful meme classification. ISSUES leverages a pre-trained CLIP
vision-language model and the textual inversion technique to effectively
capture the multimodal semantic content of the memes. The experiments show that
our method achieves state-of-the-art results on the Hateful Memes Challenge and
HarMeme datasets. The code and the pre-trained models are publicly available at
https://github.com/miccunifi/ISSUES.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08369" title="Abstract">arXiv:2310.08369</a> [<a href="/pdf/2310.08369" title="Download PDF">pdf</a>, <a href="/ps/2310.08369" title="Download PostScript">ps</a>, <a href="/format/2310.08369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Search Method for Large Polarization Kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trofimiuk%2C+G">Grigorii Trofimiuk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE Transactions on Communications. The source code is available at <a href="https://github.com/gtrofimiuk/KernelBruteforcer.">this https URL</a> arXiv admin note: text overlap with <a href="/abs/2101.10269">arXiv:2101.10269</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">A novel search method for large polarization kernels is proposed. The
algorithm produces a kernel with given partial distances by employing the
depth-first search combined with the computation of coset leaders weight tables
and sufficient conditions of code non-equivalence. Using the proposed method,
we improved all existing lower bounds on the maximum error exponent for kernels
of size from 17 to 29.
<br />We also obtained kernels which admit low complexity processing by the
recently proposed recursive trellis algorithm. Numerical results demonstrate
the advantage of polar codes with the obtained kernels compared with shortened
polar codes and polar codes with small kernels.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08370" title="Abstract">arXiv:2310.08370</a> [<a href="/pdf/2310.08370" title="Download PDF">pdf</a>, <a href="/format/2310.08370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniPAD: A Universal Pre-training Paradigm for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Honghui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sha Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Di Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaoyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Haoyi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tong He</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Shixiang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hengshuang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Q">Qibo Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Binbin Lin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiaofei He</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the context of autonomous driving, the significance of effective feature
learning is widely acknowledged. While conventional 3D self-supervised
pre-training methods have shown widespread success, most methods follow the
ideas originally designed for 2D images. In this paper, we present UniPAD, a
novel self-supervised learning paradigm applying 3D volumetric differentiable
rendering. UniPAD implicitly encodes 3D space, facilitating the reconstruction
of continuous 3D shape structures and the intricate appearance characteristics
of their 2D projections. The flexibility of our method enables seamless
integration into both 2D and 3D frameworks, enabling a more holistic
comprehension of the scenes. We manifest the feasibility and effectiveness of
UniPAD by conducting extensive experiments on various downstream 3D tasks. Our
method significantly improves lidar-, camera-, and lidar-camera-based baseline
by 9.1, 7.7, and 6.9 NDS, respectively. Notably, our pre-training pipeline
achieves 73.2 NDS for 3D object detection and 79.4 mIoU for 3D semantic
segmentation on the nuScenes validation set, achieving state-of-the-art results
in comparison with previous methods. The code will be available at
https://github.com/Nightmare-n/UniPAD.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08371" title="Abstract">arXiv:2310.08371</a> [<a href="/pdf/2310.08371" title="Download PDF">pdf</a>, <a href="/format/2310.08371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Worst-Case Morphs using Wasserstein ALI and Improved MIPGAN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kelly%2C+U+M">Una M. Kelly</a>, 
<a href="/search/cs?searchtype=author&query=Nauta%2C+M">Meike Nauta</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Spreeuwers%2C+L+J">Luuk J. Spreeuwers</a>, 
<a href="/search/cs?searchtype=author&query=Veldhuis%2C+R+N+J">Raymond N. J. Veldhuis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">A lot of progress has been made in the last years on using Generative
Adversarial Networks (GAN) to create realistic images. However, to be able
reconstruct images or to generate images using real data as input, an Encoder
is needed that reverses the mapping from the GAN's latent space to image space.
This means that three networks are needed: an Encoder, a Decoder (called
Generator in a normal GAN) and a Discriminator. These three networks can be
trained from scratch simultaneously (Adversarially Learned Inference), or
alternatively an Encoder network can be trained that maps images into the
latent space of a \textit{pretrained} GAN model (Inverse GAN). In the latter
case, the networks are trained consecutively, so the Encoder has to make do
with whatever model the Decoder learned during GAN training. Training three
networks simultaneously is more unstable and therefore more challenging, but it
is possible that the Encoder and Decoder benefit from interacting with each
other during training. We compare the two different approaches and discuss
whether it is worth the extra effort to train all three networks
simultaneously.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08372" title="Abstract">arXiv:2310.08372</a> [<a href="/pdf/2310.08372" title="Download PDF">pdf</a>, <a href="/format/2310.08372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Factual Consistency for Knowledge-Grounded Dialogue Systems  via Knowledge Enhancement and Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+B">Boyang Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongru Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+F">Fei Mi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yasheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kam-Fai Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pretrained language models (PLMs) based knowledge-grounded dialogue systems
are prone to generate responses that are factually inconsistent with the
provided knowledge source. In such inconsistent responses, the dialogue models
fail to accurately express the external knowledge they rely upon. Inspired by
previous work which identified that feed-forward networks (FFNs) within
Transformers are responsible for factual knowledge expressions, we investigate
two methods to efficiently improve the factual expression capability {of FFNs}
by knowledge enhancement and alignment respectively. We first propose
\textsc{K-Dial}, which {explicitly} introduces {extended FFNs in Transformers
to enhance factual knowledge expressions} given the specific patterns of
knowledge-grounded dialogue inputs. Additionally, we apply the reinforcement
learning for factual consistency (RLFC) method to implicitly adjust FFNs'
expressions in responses by aligning with gold knowledge for the factual
consistency preference. To comprehensively assess the factual consistency and
dialogue quality of responses, we employ extensive automatic measures and human
evaluations including sophisticated fine-grained NLI-based metrics.
Experimental results on WoW and CMU\_DoG datasets demonstrate that our methods
efficiently enhance the ability of the FFN module to convey factual knowledge,
validating the efficacy of improving factual consistency for knowledge-grounded
dialogue systems.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08373" title="Abstract">arXiv:2310.08373</a> [<a href="/pdf/2310.08373" title="Download PDF">pdf</a>, <a href="/format/2310.08373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chrono: A Peer-to-Peer Network with Verifiable Causality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yiqing%2C+M+H">Michael Hu Yiqing</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Guangda Sun</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+A">Arun Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+A">Akasha Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jialin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Logical clocks are a fundamental tool to establish causal ordering of events
in a distributed system. They have been used as the building block in weakly
consistent storage systems, causally ordered broadcast, distributed snapshots,
deadlock detection, and distributed system debugging. However, prior logical
clock constructs fail to work in a permissionless setting with Byzantine
participants. In this work, we introduce Chrono, a novel logical clock system
that targets an open and decentralized network. Chrono introduces a new logical
clock construct, the Decaying Onion Bloom Clock (DOBC), that scales
independently to the size of the network. To tolerate Byzantine behaviors,
Chrono leverages non-uniform incrementally verifiable computation (IVC) to
efficiently prove and verify the construction of DOBC clocks. We have applied
Chrono to build two decentralized applications, a weakly consistent key-value
store and an anti-censorship social network, demonstrating the power of
scalable, verifiable causality in a decentralized network.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08377" title="Abstract">arXiv:2310.08377</a> [<a href="/pdf/2310.08377" title="Download PDF">pdf</a>, <a href="/format/2310.08377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Not Marginalize Mechanisms, Rather Consolidate!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Willig%2C+M">Moritz Willig</a> (1), 
<a href="/search/cs?searchtype=author&query=Ze%C4%8Devi%C4%87%2C+M">Matej Ze&#x10d;evi&#x107;</a> (1), 
<a href="/search/cs?searchtype=author&query=Dhami%2C+D+S">Devendra Singh Dhami</a> (4), 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a> (1,2,3) (Technical University of Darmstadt, (2) Hessian Center for AI, (3) German Research Center for AI (4) Eindhoven University of Technology)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Structural causal models (SCMs) are a powerful tool for understanding the
complex causal relationships that underlie many real-world systems. As these
systems grow in size, the number of variables and complexity of interactions
between them does, too. Thus, becoming convoluted and difficult to analyze.
This is particularly true in the context of machine learning and artificial
intelligence, where an ever increasing amount of data demands for new methods
to simplify and compress large scale SCM. While methods for marginalizing and
abstracting SCM already exist today, they may destroy the causality of the
marginalized model. To alleviate this, we introduce the concept of
consolidating causal mechanisms to transform large-scale SCM while preserving
consistent interventional behaviour. We show consolidation is a powerful method
for simplifying SCM, discuss reduction of computational complexity and give a
perspective on generalizing abilities of consolidated SCM.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08381" title="Abstract">arXiv:2310.08381</a> [<a href="/pdf/2310.08381" title="Download PDF">pdf</a>, <a href="/format/2310.08381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoVP: An Automated Visual Prompting Framework and Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsao%2C+H">Hsi-Ai Tsao</a>, 
<a href="/search/cs?searchtype=author&query=Hsiung%2C+L">Lei Hsiung</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+T">Tsung-Yi Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. The code is available at <a href="https://github.com/IBM/AutoVP">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach
to adapting pre-trained vision models to solve various downstream
image-classification tasks. However, there has hitherto been little systematic
study of the design space of VP and no clear benchmark for evaluating its
performance. To bridge this gap, we propose AutoVP, an end-to-end expandable
framework for automating VP design choices, along with 12 downstream
image-classification tasks that can serve as a holistic VP-performance
benchmark. Our design space covers 1) the joint optimization of the prompts; 2)
the selection of pre-trained models, including image classifiers and text-image
encoders; and 3) model output mapping strategies, including nonparametric and
trainable label mapping. Our extensive experimental results show that AutoVP
outperforms the best-known current VP methods by a substantial margin, having
up to 6.7% improvement in accuracy; and attains a maximum performance increase
of 27.5% compared to linear-probing (LP) baseline. AutoVP thus makes a two-fold
contribution: serving both as an efficient tool for hyperparameter tuning on VP
design choices, and as a comprehensive benchmark that can reasonably be
expected to accelerate VP's development. The source code is available at
https://github.com/IBM/AutoVP.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08383" title="Abstract">arXiv:2310.08383</a> [<a href="/pdf/2310.08383" title="Download PDF">pdf</a>, <a href="/format/2310.08383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstructing Materials Tetrahedron: Challenges in Materials  Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hira%2C+K">Kausik Hira</a>, 
<a href="/search/cs?searchtype=author&query=Zaki%2C+M">Mohd Zaki</a>, 
<a href="/search/cs?searchtype=author&query=Sheth%2C+D">Dhruvil Sheth</a>, 
<a href="/search/cs?searchtype=author&query=Mausam">Mausam</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+N+M+A">N M Anoop Krishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Materials Science (cond-mat.mtrl-sci)

</div>
<p class="mathjax">Discovery of new materials has a documented history of propelling human
progress for centuries and more. The behaviour of a material is a function of
its composition, structure, and properties, which further depend on its
processing and testing conditions. Recent developments in deep learning and
natural language processing have enabled information extraction at scale from
published literature such as peer-reviewed publications, books, and patents.
However, this information is spread in multiple formats, such as tables, text,
and images, and with little or no uniformity in reporting style giving rise to
several machine learning challenges. Here, we discuss, quantify, and document
these outstanding challenges in automated information extraction (IE) from
materials science literature towards the creation of a large materials science
knowledge base. Specifically, we focus on IE from text and tables and outline
several challenges with examples. We hope the present work inspires researchers
to address the challenges in a coherent fashion, providing to fillip to IE for
the materials knowledge base.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08384" title="Abstract">arXiv:2310.08384</a> [<a href="/pdf/2310.08384" title="Download PDF">pdf</a>, <a href="/format/2310.08384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Running Time Analysis of Interactive Multi-objective  Evolutionary Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+T">Tianhao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+C">Chao Bian</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chao Qian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Evolutionary algorithms (EAs) are widely used for multi-objective
optimization due to their population-based nature. Traditional multi-objective
EAs (MOEAs) generate a large set of solutions to approximate the Pareto front,
leaving a decision maker (DM) with the task of selecting a preferred solution.
However, this process can be inefficient and time-consuming, especially when
there are many objectives or the subjective preferences of DM is known. To
address this issue, interactive MOEAs (iMOEAs) combine decision making into the
optimization process, i.e., update the population with the help of the DM. In
contrast to their wide applications, there has existed only two pieces of
theoretical works on iMOEAs, which only considered interactive variants of the
two simple single-objective algorithms, RLS and (1+1)-EA. This paper provides
the first running time analysis (the essential theoretical aspect of EAs) for
practical iMOEAs. Specifically, we prove that the expected running time of the
well-developed interactive NSGA-II (called R-NSGA-II) for solving the OneMinMax
and OneJumpZeroJump problems is $O(n \log n)$ and $O(n^k)$, respectively, which
are all asymptotically faster than the traditional NSGA-II. Meanwhile, we
present a variant of OneMinMax, and prove that R-NSGA-II can be exponentially
slower than NSGA-II. These results provide theoretical justification for the
effectiveness of iMOEAs while identifying situations where they may fail.
Experiments are also conducted to validate the theoretical results.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08387" title="Abstract">arXiv:2310.08387</a> [<a href="/pdf/2310.08387" title="Download PDF">pdf</a>, <a href="/format/2310.08387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MeanAP-Guided Reinforced Active Learning for Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Zhixuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xingyu Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Rui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Active learning presents a promising avenue for training high-performance
models with minimal labeled data, achieved by judiciously selecting the most
informative instances to label and incorporating them into the task learner.
Despite notable advancements in active learning for image recognition, metrics
devised or learned to gauge the information gain of data, crucial for query
strategy design, do not consistently align with task model performance metrics,
such as Mean Average Precision (MeanAP) in object detection tasks. This paper
introduces MeanAP-Guided Reinforced Active Learning for Object Detection
(MAGRAL), a novel approach that directly utilizes the MeanAP metric of the task
model to devise a sampling strategy employing a reinforcement learning-based
sampling agent. Built upon LSTM architecture, the agent efficiently explores
and selects subsequent training instances, and optimizes the process through
policy gradient with MeanAP serving as reward. Recognizing the time-intensive
nature of MeanAP computation at each step, we propose fast look-up tables to
expedite agent training. We assess MAGRAL's efficacy across popular benchmarks,
PASCAL VOC and MS COCO, utilizing different backbone architectures. Empirical
findings substantiate MAGRAL's superiority over recent state-of-the-art
methods, showcasing substantial performance gains. MAGRAL establishes a robust
baseline for reinforced active object detection, signifying its potential in
advancing the field.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08390" title="Abstract">arXiv:2310.08390</a> [<a href="/pdf/2310.08390" title="Download PDF">pdf</a>, <a href="/format/2310.08390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyp-UML: Hyperbolic Image Retrieval with Uncertainty-aware Metric  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shiyang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zongxuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lin Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Metric learning plays a critical role in training image retrieval and
classification. It is also a key algorithm in representation learning, e.g.,
for feature learning and its alignment in metric space. Hyperbolic embedding
has been recently developed, compared to the conventional Euclidean embedding
in most of the previously developed models, and can be more effective in
representing the hierarchical data structure. Second, uncertainty
estimation/measurement is a long-lasting challenge in artificial intelligence.
Successful uncertainty estimation can improve a machine learning model's
performance, robustness, and security. In Hyperbolic space, uncertainty
measurement is at least with equivalent, if not more, critical importance. In
this paper, we develop a Hyperbolic image embedding with uncertainty-aware
metric learning for image retrieval. We call our method Hyp-UML: Hyperbolic
Uncertainty-aware Metric Learning. Our contribution are threefold: we propose
an image embedding algorithm based on Hyperbolic space, with their
corresponding uncertainty value; we propose two types of uncertainty-aware
metric learning, for the popular Contrastive learning and conventional
margin-based metric learning, respectively. We perform extensive experimental
validations to prove that the proposed algorithm can achieve state-of-the-art
results among related methods. The comprehensive ablation study validates the
effectiveness of each component of the proposed algorithm.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08392" title="Abstract">arXiv:2310.08392</a> [<a href="/pdf/2310.08392" title="Download PDF">pdf</a>, <a href="/format/2310.08392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Introducing a Deep Neural Network-based Model Predictive Control  Framework for Rapid Controller Implementation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gordon%2C+D+C">David C. Gordon</a>, 
<a href="/search/eess?searchtype=author&query=Winkler%2C+A">Alexander Winkler</a>, 
<a href="/search/eess?searchtype=author&query=Bedei%2C+J">Julian Bedei</a>, 
<a href="/search/eess?searchtype=author&query=Schaber%2C+P">Patrick Schaber</a>, 
<a href="/search/eess?searchtype=author&query=Andert%2C+J">Jakob Andert</a>, 
<a href="/search/eess?searchtype=author&query=Koch%2C+C+R">Charles R. Koch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to 2024 American Control Conference (ACC), July 8-12, 2024 in Toronto, Canada. ACC is the annual conference of the American Automatic Control Council (AACC), the U.S. national member organization of the International Federation for Automatic Control (IFAC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Model Predictive Control (MPC) provides an optimal control solution based on
a cost function while allowing for the implementation of process constraints.
As a model-based optimal control technique, the performance of MPC strongly
depends on the model used where a trade-off between model computation time and
prediction performance exists. One solution is the integration of MPC with a
machine learning (ML) based process model which are quick to evaluate online.
This work presents the experimental implementation of a deep neural network
(DNN) based nonlinear MPC for Homogeneous Charge Compression Ignition (HCCI)
combustion control. The DNN model consists of a Long Short-Term Memory (LSTM)
network surrounded by fully connected layers which was trained using
experimental engine data and showed acceptable prediction performance with
under 5% error for all outputs. Using this model, the MPC is designed to track
the Indicated Mean Effective Pressure (IMEP) and combustion phasing
trajectories, while minimizing several parameters. Using the acados software
package to enable the real-time implementation of the MPC on an ARM Cortex A72,
the optimization calculations are completed within 1.4 ms. The external A72
processor is integrated with the prototyping engine controller using a UDP
connection allowing for rapid experimental deployment of the NMPC. The IMEP
trajectory following of the developed controller was excellent, with a
root-mean-square error of 0.133 bar, in addition to observing process
constraints.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08394" title="Abstract">arXiv:2310.08394</a> [<a href="/pdf/2310.08394" title="Download PDF">pdf</a>, <a href="/format/2310.08394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Better Evaluation of Instruction-Following: A Case-Study in  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Skopek%2C+O">Ondrej Skopek</a>, 
<a href="/search/cs?searchtype=author&query=Aralikatte%2C+R">Rahul Aralikatte</a>, 
<a href="/search/cs?searchtype=author&query=Gooding%2C+S">Sian Gooding</a>, 
<a href="/search/cs?searchtype=author&query=Carbune%2C+V">Victor Carbune</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CoNLL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite recent advances, evaluating how well large language models (LLMs)
follow user instructions remains an open problem. While evaluation methods of
language models have seen a rise in prompt-based approaches, limited work on
the correctness of these methods has been conducted. In this work, we perform a
meta-evaluation of a variety of metrics to quantify how accurately they measure
the instruction-following abilities of LLMs. Our investigation is performed on
grounded query-based summarization by collecting a new short-form, real-world
dataset riSum, containing $300$ document-instruction pairs with $3$ answers
each. All $900$ answers are rated by $3$ human annotators. Using riSum, we
analyze agreement between evaluation methods and human judgment. Finally, we
propose new LLM-based reference-free evaluation methods that improve upon
established baselines and perform on-par with costly reference-based metrics
which require high-quality summaries.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08395" title="Abstract">arXiv:2310.08395</a> [<a href="/pdf/2310.08395" title="Download PDF">pdf</a>, <a href="/format/2310.08395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Large Language Models with Chain-of-Thought for Few-Shot  Knowledge Base Question Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuanyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanlun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+W">Weining Qian</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yunshi Lan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The task of Question Generation over Knowledge Bases (KBQG) aims to convert a
logical form into a natural language question. For the sake of expensive cost
of large-scale question annotation, the methods of KBQG under low-resource
scenarios urgently need to be developed. However, current methods heavily rely
on annotated data for fine-tuning, which is not well-suited for few-shot
question generation. The emergence of Large Language Models (LLMs) has shown
their impressive generalization ability in few-shot tasks. Inspired by
Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for
reasoning, we formulate KBQG task as a reasoning problem, where the generation
of a complete question is splitted into a series of sub-question generation.
Our proposed prompting method KQG-CoT first retrieves supportive logical forms
from the unlabeled data pool taking account of the characteristics of the
logical form. Then, we write a prompt to explicit the reasoning chain of
generating complicated questions based on the selected demonstrations. To
further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the
logical forms by their complexity. We conduct extensive experiments over three
public KBQG datasets. The results demonstrate that our prompting method
consistently outperforms other prompting baselines on the evaluated datasets.
Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of
the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4,
METEOR, and ROUGE-L, respectively.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08396" title="Abstract">arXiv:2310.08396</a> [<a href="/pdf/2310.08396" title="Download PDF">pdf</a>, <a href="/format/2310.08396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-Aware Planning for Heterogeneous Robot Teams using Dynamic  Topological Graphs and Mixed-Integer Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dimmig%2C+C+A">Cora A. Dimmig</a>, 
<a href="/search/cs?searchtype=author&query=Wolfe%2C+K+C">Kevin C. Wolfe</a>, 
<a href="/search/cs?searchtype=author&query=Kobilarov%2C+M">Marin Kobilarov</a>, 
<a href="/search/cs?searchtype=author&query=Moore%2C+J">Joseph Moore</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Planning under uncertainty is a fundamental challenge in robotics. For
multi-robot teams, the challenge is further exacerbated, since the planning
problem can quickly become computationally intractable as the number of robots
increase. In this paper, we propose a novel approach for planning under
uncertainty using heterogeneous multi-robot teams. In particular, we leverage
the notion of a dynamic topological graph and mixed-integer programming to
generate multi-robot plans that deploy fast scout team members to reduce
uncertainty about the environment. We test our approach in a number of
representative scenarios where the robot team must move through an environment
while minimizing detection in the presence of uncertain observer positions. We
demonstrate that our approach is sufficiently computationally tractable for
real-time re-planning in changing environments, can improve performance in the
presence of imperfect information, and can be adjusted to accommodate different
risk profiles.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08398" title="Abstract">arXiv:2310.08398</a> [<a href="/pdf/2310.08398" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Design and Development of an ArUco Markers-Based Quantitative  Surface Tactile Sensor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kara%2C+O+C">Ozdemir Can Kara</a>, 
<a href="/search/cs?searchtype=author&query=Everson%2C+C">Charles Everson</a>, 
<a href="/search/cs?searchtype=author&query=Alambeigi%2C+F">Farshid Alambeigi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this paper, with the goal of quantifying the qualitative image outputs of
a Vision-based Tactile Sensor (VTS), we present the design, fabrication, and
characterization of a novel Quantitative Surface Tactile Sensor (called QS-TS).
QS-TS directly estimates the sensor's gel layer deformation in real-time
enabling safe and autonomous tactile manipulation and servoing of delicate
objects using robotic manipulators. The core of the proposed sensor is the
utilization of miniature 1.5 mm x 1.5 mm synthetic square markers with inner
binary patterns and a broad black border, called ArUco Markers. Each ArUco
marker can provide real-time camera pose estimation that, in our design, is
used as a quantitative measure for obtaining deformation of the QS-TS gel
layer. Moreover, thanks to the use of ArUco markers, we propose a unique
fabrication procedure that mitigates various challenges associated with the
fabrication of the existing marker-based VTSs and offers an intuitive and
less-arduous method for the construction of the VTS. Remarkably, the proposed
fabrication facilitates the integration and adherence of markers with the gel
layer to robustly and reliably obtain a quantitative measure of deformation in
real-time regardless of the orientation of ArUco Markers. The performance and
efficacy of the proposed QS-TS in estimating the deformation of the sensor's
gel layer were experimentally evaluated and verified. Results demonstrate the
phenomenal performance of the QS-TS in estimating the deformation of the gel
layer with a relative error of &lt;5%.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08401" title="Abstract">arXiv:2310.08401</a> [<a href="/pdf/2310.08401" title="Download PDF">pdf</a>, <a href="/format/2310.08401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance/power assessment of CNN packages on embedded automotive  platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Burgio%2C+P">Paolo Burgio</a>, 
<a href="/search/cs?searchtype=author&query=Brilli%2C+G">Gianluca Brilli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages; 17 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI); Performance (cs.PF)

</div>
<p class="mathjax">The rise of power-efficient embedded computers based on highly-parallel
accelerators opens a number of opportunities and challenges for researchers and
engineers, and paved the way to the era of edge computing. At the same time,
advances in embedded AI for object detection and categorization such as YOLO,
GoogleNet and AlexNet reached an unprecedented level of accuracy (mean-Average
Precision - mAP) and performance (Frames-Per-Second - FPS). Today, edge
computers based on heterogeneous many-core systems are a predominant choice to
deploy such systems in industry 4.0, wearable devices, and - our focus -
autonomous driving systems. In these latter systems, engineers struggle to make
reduced automotive power and size budgets co-exist with the accuracy and
performance targets requested by autonomous driving. We aim at validating the
effectiveness and efficiency of most recent networks on state-of-the-art
platforms with embedded commercial-off-the-shelf System-on-Chips, such as
Xavier AGX, Tegra X2 and Nano for NVIDIA and XCZU9EG and XCZU3EG of the Zynq
UltraScale+ family, for the Xilinx counterpart. Our work aims at supporting
engineers in choosing the most appropriate CNN package and computing system for
their designs, and deriving guidelines for adequately sizing their systems.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08403" title="Abstract">arXiv:2310.08403</a> [<a href="/pdf/2310.08403" title="Download PDF">pdf</a>, <a href="/format/2310.08403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vault: Decentralized Storage Made Durable
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Guangda Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yiqing%2C+M+H">Michael Hu Yiqing</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+A">Arun Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+A">Akasha Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jialin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">The lack of centralized control, combined with highly dynamic adversarial
behaviors, makes data durability a challenge in decentralized storage systems.
In this work, we introduce a new storage system, Vault, that offers strong data
durability guarantees in a fully decentralized, permission-less setting. Vault
leverages the rateless property of erasure code to encode each data object into
an infinite stream of encoding fragments. To ensure durability in the presence
of dynamic Byzantine behaviors and targeted attacks, an infinite sequence of
storage nodes are randomly selected to store encoding fragments. Encoding
generation and candidate selection are fully decentralized: When necessary,
Vault nodes use a gossip protocol and a publically verifiable selection proof
to determine new fragments. Simulations and large-scale EC2 experiments
demonstrate that Vault provides close-to-ideal mean-time-to-data-loss (MTTDL)
with low storage redundancy, scales to more than 10,000 nodes, and attains
performance comparable to IPFS
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08406" title="Abstract">arXiv:2310.08406</a> [<a href="/pdf/2310.08406" title="Download PDF">pdf</a>, <a href="/format/2310.08406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tightening Bounds on Probabilities of Causation By Merging Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sani%2C+N">Numair Sani</a>, 
<a href="/search/cs?searchtype=author&query=Mastakouri%2C+A+A">Atalanti A. Mastakouri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Statistics Theory (math.ST)

</div>
<p class="mathjax">Probabilities of Causation (PoC) play a fundamental role in decision-making
in law, health care and public policy. Nevertheless, their point identification
is challenging, requiring strong assumptions, in the absence of which only
bounds can be derived. Existing work to further tighten these bounds by
leveraging extra information either provides numerical bounds, symbolic bounds
for fixed dimensionality, or requires access to multiple datasets that contain
the same treatment and outcome variables. However, in many clinical,
epidemiological and public policy applications, there exist external datasets
that examine the effect of different treatments on the same outcome variable,
or study the association between covariates and the outcome variable. These
external datasets cannot be used in conjunction with the aforementioned bounds,
since the former may entail different treatment assignment mechanisms, or even
obey different causal structures. Here, we provide symbolic bounds on the PoC
for this challenging scenario. We focus on combining either two randomized
experiments studying different treatments, or a randomized experiment and an
observational study, assuming causal sufficiency. Our symbolic bounds work for
arbitrary dimensionality of covariates and treatment, and we discuss the
conditions under which these bounds are tighter than existing bounds in
literature. Finally, our bounds parameterize the difference in treatment
assignment mechanism across datasets, allowing the mechanisms to vary across
datasets while still allowing causal information to be transferred from the
external dataset to the target dataset.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08412" title="Abstract">arXiv:2310.08412</a> [<a href="/pdf/2310.08412" title="Download PDF">pdf</a>, <a href="/format/2310.08412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Sound and Complete Refinement Relation for Non-reducible Modal  Transition Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Basile%2C+D">Davide Basile</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">Modal Transition Systems (MTS) are a well-known formalism that extend
Labelled Transition Systems (LTS) with the possibility of specifying necessary
and permitted behaviour. Whenever two MTS are not in modal refinement
relationship, it could still be the case that the set of implementations of one
MTS is included in the set of implementations of the other. The challenge of
devising an alternative notion of modal refinement that is both sound and
complete with respect to the set of implementations, without disregarding
valuable implementations, remains open. In this paper, we address this
challenge. We introduce a subset of MTS called Non-reducible Modal Transition
Systems (NMTS), together with a novel refinement relation for NMTS. We show
that NMTS refinement is sound and also complete with respect to its set of
implementations. We illustrate through examples how the additional constraints
imposed by NMTS are necessary for achieving completeness. Furthermore, we
discuss a property holding for NMTS whose implementations are
non-deterministic. We show that any implementation obtained through modal
refinement but disregarded by NMTS refinement is violating this property.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08413" title="Abstract">arXiv:2310.08413</a> [<a href="/pdf/2310.08413" title="Download PDF">pdf</a>, <a href="/format/2310.08413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Control-Based Planning over Probability Mass Function Measurements via  Robust Linear Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kermanshah%2C+M">Mehdi Kermanshah</a>, 
<a href="/search/eess?searchtype=author&query=Belta%2C+C">Calin Belta</a>, 
<a href="/search/eess?searchtype=author&query=Tron%2C+R">Roberto Tron</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">We propose an approach to synthesize linear feedback controllers for linear
systems in polygonal environments. Our method focuses on designing a robust
controller that can account for uncertainty in measurements. Its inputs are
provided by a perception module that generates probability mass functions
(PMFs) for predefined landmarks in the environment, such as distinguishable
geometric features. We formulate an optimization problem with Control Lyapunov
Function (CLF) and Control Barrier Function (CBF) constraints to derive a
stable and safe controller. Using the strong duality of linear programs (LPs)
and robust optimization, we convert the optimization problem to a linear
program that can be efficiently solved offline. At a high level, our approach
partially combines perception, planning, and real-time control into a single
design problem. An additional advantage of our method is the ability to produce
controllers capable of exhibiting nonlinear behavior while relying solely on an
offline LP for control synthesis.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08418" title="Abstract">arXiv:2310.08418</a> [<a href="/pdf/2310.08418" title="Download PDF">pdf</a>, <a href="/ps/2310.08418" title="Download PostScript">ps</a>, <a href="/format/2310.08418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserved Aggregate Thermal Dynamic Model of Buildings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hou%2C+Z">Zeyin Hou</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+S">Shuai Lu</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Y">Yijun Xu</a>, 
<a href="/search/eess?searchtype=author&query=Qiu%2C+H">Haifeng Qiu</a>, 
<a href="/search/eess?searchtype=author&query=Gu%2C+W">Wei Gu</a>, 
<a href="/search/eess?searchtype=author&query=Dong%2C+Z">Zhaoyang Dong</a>, 
<a href="/search/eess?searchtype=author&query=Ding%2C+S">Shixing Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The thermal inertia of buildings brings considerable flexibility to the
heating and cooling load, which is known to be a promising demand response
resource. The aggregate model that can describe the thermal dynamics of the
building cluster is an important interference for energy systems to exploit its
intrinsic thermal inertia. However, the private information of users, such as
the indoor temperature and heating/cooling power, needs to be collected in the
parameter estimation procedure to obtain the aggregate model, causing severe
privacy concerns. In light of this, we propose a novel privacy-preserved
parameter estimation approach to infer the aggregate model for the thermal
dynamics of the building cluster for the first time. Using it, the parameters
of the aggregate thermal dynamic model (ATDM) can be obtained by the load
aggregator without accessing the individual's privacy information. More
specifically, this method not only exploits the block coordinate descent (BCD)
method to resolve its non-convexity in the estimation but investigates the
transformation-based encryption (TE) associated with its secure aggregation
protocol (SAP) techniques to realize privacy-preserved computation. Its
capability of preserving privacy is also theoretically proven. Finally,
simulation results using real-world data demonstrate the accuracy and
privacy-preserved performance of our proposed method.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08419" title="Abstract">arXiv:2310.08419</a> [<a href="/pdf/2310.08419" title="Download PDF">pdf</a>, <a href="/format/2310.08419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jailbreaking Black Box Large Language Models in Twenty Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chao%2C+P">Patrick Chao</a>, 
<a href="/search/cs?searchtype=author&query=Robey%2C+A">Alexander Robey</a>, 
<a href="/search/cs?searchtype=author&query=Dobriban%2C+E">Edgar Dobriban</a>, 
<a href="/search/cs?searchtype=author&query=Hassani%2C+H">Hamed Hassani</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+G+J">George J. Pappas</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+E">Eric Wong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">There is growing interest in ensuring that large language models (LLMs) align
with human values. However, the alignment of such models is vulnerable to
adversarial jailbreaks, which coax LLMs into overriding their safety
guardrails. The identification of these vulnerabilities is therefore
instrumental in understanding inherent weaknesses and preventing future misuse.
To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an
algorithm that generates semantic jailbreaks with only black-box access to an
LLM. PAIR -- which is inspired by social engineering attacks -- uses an
attacker LLM to automatically generate jailbreaks for a separate targeted LLM
without human intervention. In this way, the attacker LLM iteratively queries
the target LLM to update and refine a candidate jailbreak. Empirically, PAIR
often requires fewer than twenty queries to produce a jailbreak, which is
orders of magnitude more efficient than existing algorithms. PAIR also achieves
competitive jailbreaking success rates and transferability on open and
closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08420" title="Abstract">arXiv:2310.08420</a> [<a href="/pdf/2310.08420" title="Download PDF">pdf</a>, <a href="/format/2310.08420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Attention-Prompted Prediction and Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+S">Siyi Gu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+B">Bo Pan</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+G">Guangji Bai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaofeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Explanation(attention)-guided learning is a method that enhances a model's
predictive power by incorporating human understanding during the training
phase. While attention-guided learning has shown promising results, it often
involves time-consuming and computationally expensive model retraining. To
address this issue, we introduce the attention-prompted prediction technique,
which enables direct prediction guided by the attention prompt without the need
for model retraining. However, this approach presents several challenges,
including: 1) How to incorporate the visual attention prompt into the model's
decision-making process and leverage it for future predictions even in the
absence of a prompt? and 2) How to handle the incomplete information from the
visual attention prompt? To tackle these challenges, we propose a novel
framework called Visual Attention-Prompted Prediction and Learning, which
seamlessly integrates visual attention prompts into the model's decision-making
process and adapts to images both with and without attention prompts for
prediction. To address the incomplete information of the visual attention
prompt, we introduce a perturbation-based attention map modification method.
Additionally, we propose an optimization-based mask aggregation method with a
new weight learning function for adaptive perturbed annotation aggregation in
the attention map modification process. Our overall framework is designed to
learn in an attention-prompt guided multi-task manner to enhance future
predictions even for samples without attention prompts and trained in an
alternating manner for better convergence. Extensive experiments conducted on
two datasets demonstrate the effectiveness of our proposed framework in
enhancing predictions for samples, both with and without provided prompts.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08421" title="Abstract">arXiv:2310.08421</a> [<a href="/pdf/2310.08421" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;SegLoc&quot;: Study on Novel Visual Self-supervised Learning Scheme (Segment  Localization) Tailored for Dense Prediction Tasks of Security Inspection  X-ray Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Halat%2C+S">Shervin Halat</a>, 
<a href="/search/cs?searchtype=author&query=Rahmati%2C+M">Mohammad Rahmati</a>, 
<a href="/search/cs?searchtype=author&query=Nazerfard%2C+E">Ehsan Nazerfard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Lately, remarkable advancements of artificial intelligence have been
attributed to the integration of self-supervised learning scheme. Despite
impressive achievements within NLP, yet SSL in computer vision has not been
able to stay on track comparatively. Recently, integration of contrastive
learning on top of existing SSL models has established considerable progress in
computer vision through which visual SSL models have outperformed their
supervised counterparts. Nevertheless, most of these improvements were limited
to classification tasks, and also, few works have been dedicated to evaluation
of SSL models in real-world scenarios of computer vision, while the majority of
works are centered around datasets containing class-wise portrait images, most
notably, ImageNet. Consequently, in this work, we have considered dense
prediction task of semantic segmentation in security inspection x-ray images to
evaluate our proposed model Segmentation Localization. Based upon the model
Instance Localization, our model SegLoc has managed to address one of the most
challenging downsides of contrastive learning, i.e., false negative pairs of
query embeddings. In order to do so, in contrast to baseline model InsLoc, our
pretraining dataset is synthesized by cropping, transforming, then pasting
already labeled segments from an available labeled dataset, foregrounds, onto
instances of an unlabeled dataset, backgrounds. In our case, PIDray and SIXray
datasets are considered as labeled and unlabeled datasets, respectively.
Moreover, we fully harness labels by avoiding false negative pairs through
implementing the idea, one queue per class, in MoCo-v2 whereby negative pairs
corresponding to each query are extracted from its corresponding queue within
the memory bank. Our approach has outperformed random initialization by 3% to
6%, while having underperformed supervised initialization.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08425" title="Abstract">arXiv:2310.08425</a> [<a href="/pdf/2310.08425" title="Download PDF">pdf</a>, <a href="/format/2310.08425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Non-convex Learning for Multi-layer Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Hanpu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cheng-Long Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Z">Zihang Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+Y">Yiming Ying</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Di Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper focuses on the problem of Differentially Private Stochastic
Optimization for (multi-layer) fully connected neural networks with a single
output node. In the first part, we examine cases with no hidden nodes,
specifically focusing on Generalized Linear Models (GLMs). We investigate the
well-specific model where the random noise possesses a zero mean, and the link
function is both bounded and Lipschitz continuous. We propose several
algorithms and our analysis demonstrates the feasibility of achieving an excess
population risk that remains invariant to the data dimension. We also delve
into the scenario involving the ReLU link function, and our findings mirror
those of the bounded link function. We conclude this section by contrasting
well-specified and misspecified models, using ReLU regression as a
representative example.
<br />In the second part of the paper, we extend our ideas to two-layer neural
networks with sigmoid or ReLU activation functions in the well-specified model.
In the third part, we study the theoretical guarantees of DP-SGD in Abadi et
al. (2016) for fully connected multi-layer neural networks. By utilizing recent
advances in Neural Tangent Kernel theory, we provide the first excess
population risk when both the sample size and the width of the network are
sufficiently large. Additionally, we discuss the role of some parameters in
DP-SGD regarding their utility, both theoretically and empirically.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08429" title="Abstract">arXiv:2310.08429</a> [<a href="/pdf/2310.08429" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Data Augmentation for Rotational Invariance in Convolutional  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quiroga%2C+F+M">Facundo Manuel Quiroga</a>, 
<a href="/search/cs?searchtype=author&query=Ronchetti%2C+F">Franco Ronchetti</a>, 
<a href="/search/cs?searchtype=author&query=Lanzarini%2C+L">Laura Lanzarini</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez-Bariviera%2C+A">Aurelio Fernandez-Bariviera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Convolutional Neural Networks (CNN) offer state of the art performance in
various computer vision tasks. Many of those tasks require different subtypes
of affine invariances (scale, rotational, translational) to image
transformations. Convolutional layers are translation equivariant by design,
but in their basic form lack invariances. In this work we investigate how best
to include rotational invariance in a CNN for image classification. Our
experiments show that networks trained with data augmentation alone can
classify rotated images nearly as well as in the normal unrotated case; this
increase in representational power comes only at the cost of training time. We
also compare data augmentation versus two modified CNN models for achieving
rotational invariance or equivariance, Spatial Transformer Networks and Group
Equivariant CNNs, finding no significant accuracy increase with these
specialized methods. In the case of data augmented networks, we also analyze
which layers help the network to encode the rotational invariance, which is
important for understanding its limitations and how to best retrain a network
with data augmentation to achieve invariance to rotation.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08430" title="Abstract">arXiv:2310.08430</a> [<a href="/pdf/2310.08430" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing of Soil Erosion Risk Through Geoinformation Sciences and  Remote Sensing -- A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Filchev%2C+L">Lachezar Filchev</a>, 
<a href="/search/cs?searchtype=author&query=Kolev%2C+V">Vasil Kolev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Chapter 21 (pages 54)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Rai, P.K., Singh, P., Mishra, V.N. (eds), Recent Technologies for
  Disaster Management and Risk Reduction, Earth and Environmental Sciences
  Library, Springer, 2021.
  https://link.springer.com/chapter/10.1007/978-3-030-76116-5_21
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Data Analysis, Statistics and Probability (physics.data-an); Geophysics (physics.geo-ph)

</div>
<p class="mathjax">During past decades a marked manifestation of widespread erosion phenomena
was studied worldwide. Global conservation community has launched campaigns at
local, regional and continental level in developing countries for preservation
of soil resources in order not only to stop or mitigate human impact on nature
but also to improve life in rural areas introducing new approaches for soil
cultivation. After the adoption of Sustainable Development Goals of UNs and
launching several world initiatives such as the Land Degradation Neutrality
(LDN) the world came to realize the very importance of the soil resources on
which the biosphere relies for its existence. The main goal of the chapter is
to review different types and structures erosion models as well as their
applications. Several methods using spatial analysis capabilities of geographic
information systems (GIS) are in operation for soil erosion risk assessment,
such as Universal Soil Loss Equation (USLE), Revised Universal Soil Loss
Equation (RUSLE) in operation worldwide and in the USA and MESALES model. These
and more models are being discussed in the present work alongside more
experimental models and methods for assessing soil erosion risk such as
Artificial Intelligence (AI), Machine and Deep Learning, etc. At the end of
this work, a prospectus for the future development of soil erosion risk
assessment is drawn.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08431" title="Abstract">arXiv:2310.08431</a> [<a href="/pdf/2310.08431" title="Download PDF">pdf</a>, <a href="/format/2310.08431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Sampling in Hierarchical Exponential-family Energy-based Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xingsi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Si Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Bayesian brain theory suggests that the brain employs generative models to
understand the external world. The sampling-based perspective posits that the
brain infers the posterior distribution through samples of stochastic neuronal
responses. Additionally, the brain continually updates its generative model to
approach the true distribution of the external world. In this study, we
introduce the Hierarchical Exponential-family Energy-based (HEE) model, which
captures the dynamics of inference and learning. In the HEE model, we decompose
the partition function into individual layers and leverage a group of neurons
with shorter time constants to sample the gradient of the decomposed
normalization term. This allows our model to estimate the partition function
and perform inference simultaneously, circumventing the negative phase
encountered in conventional energy-based models (EBMs). As a result, the
learning process is localized both in time and space, and the model is easy to
converge. To match the brain's rapid computation, we demonstrate that neural
adaptation can serve as a momentum term, significantly accelerating the
inference process. On natural image datasets, our model exhibits
representations akin to those observed in the biological visual system.
Furthermore, for the machine learning community, our model can generate
observations through joint or marginal generation. We show that marginal
generation outperforms joint generation and achieves performance on par with
other EBMs.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08433" title="Abstract">arXiv:2310.08433</a> [<a href="/pdf/2310.08433" title="Download PDF">pdf</a>, <a href="/format/2310.08433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative  Writing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez-Rodr%C3%ADguez%2C+C">Carlos G&#xf3;mez-Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+P">Paul Williams</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">We evaluate a range of recent LLMs on English creative writing, a challenging
and complex task that requires imagination, coherence, and style. We use a
difficult, open-ended scenario chosen to avoid training data reuse: an epic
narration of a single combat between Ignatius J. Reilly, the protagonist of the
Pulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl,
a prehistoric flying reptile. We ask several LLMs and humans to write such a
story and conduct a human evalution involving various criteria such as fluency,
coherence, originality, humor, and style. Our results show that some
state-of-the-art commercial LLMs match or slightly outperform our writers in
most dimensions; whereas open-source LLMs lag behind. Humans retain an edge in
creativity, while humor shows a binary divide between LLMs that can handle it
comparably to humans and those that fail at it. We discuss the implications and
limitations of our study and suggest directions for future research.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08435" title="Abstract">arXiv:2310.08435</a> [<a href="/pdf/2310.08435" title="Download PDF">pdf</a>, <a href="/format/2310.08435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MUN-FRL: A Visual Inertial LiDAR Dataset for Aerial Autonomous  Navigation and Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thalagala%2C+R+G">Ravindu G. Thalagala</a>, 
<a href="/search/cs?searchtype=author&query=Gunawardena%2C+S+M">Sahan M. Gunawardena</a>, 
<a href="/search/cs?searchtype=author&query=De+Silva%2C+O">Oscar De Silva</a>, 
<a href="/search/cs?searchtype=author&query=Jayasiri%2C+A">Awantha Jayasiri</a>, 
<a href="/search/cs?searchtype=author&query=Gubbels%2C+A">Arthur Gubbels</a>, 
<a href="/search/cs?searchtype=author&query=Mann%2C+G+K+I">George K.I Mann</a>, 
<a href="/search/cs?searchtype=author&query=Gosine%2C+R+G">Raymond G. Gosine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">This paper presents a unique outdoor aerial visual-inertial-LiDAR dataset
captured using a multi-sensor payload to promote the global navigation
satellite system (GNSS)-denied navigation research. The dataset features flight
distances ranging from 300m to 5km, collected using a DJI M600 hexacopter drone
and the National Research Council (NRC) Bell 412 Advanced Systems Research
Aircraft (ASRA). The dataset consists of hardware synchronized monocular
images, IMU measurements, 3D LiDAR point-clouds, and high-precision real-time
kinematic (RTK)-GNSS based ground truth. Ten datasets were collected as ROS
bags over 100 mins of outdoor environment footage ranging from urban areas,
highways, hillsides, prairies, and waterfronts. The datasets were collected to
facilitate the development of visual-inertial-LiDAR odometry and mapping
algorithms, visual-inertial navigation algorithms, object detection,
segmentation, and landing zone detection algorithms based upon real-world drone
and full-scale helicopter data. All the datasets contain raw sensor
measurements, hardware timestamps, and spatio-temporally aligned ground truth.
The intrinsic and extrinsic calibrations of the sensors are also provided along
with raw calibration datasets. A performance summary of state-of-the-art
methods applied on the datasets is also provided.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08437" title="Abstract">arXiv:2310.08437</a> [<a href="/pdf/2310.08437" title="Download PDF">pdf</a>, <a href="/format/2310.08437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cold Start Latency in Serverless Computing: A Systematic Review,  Taxonomy, and Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golec%2C+M">Muhammed Golec</a>, 
<a href="/search/cs?searchtype=author&query=Walia%2C+G+K">Guneet Kaur Walia</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+M">Mohit Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Cuadrado%2C+F">Felix Cuadrado</a>, 
<a href="/search/cs?searchtype=author&query=Gill%2C+S+S">Sukhpal Singh Gill</a>, 
<a href="/search/cs?searchtype=author&query=Uhlig%2C+S">Steve Uhlig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 Pages, 16 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Recently, academics and the corporate sector have paid attention to
serverless computing, which enables dynamic scalability and an economic model.
In serverless computing, users pay only for the time they actually spend using
the resources. Although zero scaling optimises cost and resource utilisation,
it is the fundamental reason for the serverless cold start problem. Various
academic and corporate sector studies are being conducted to tackle the cold
start problem, which has large research challenges. To study the "cold start"
problem in serverless computing, this article provides a comprehensive
literature overview of recent research. In addition, we present a detailed
taxonomy of several approaches to addressing the issue of cold start latency in
serverless computing. Several academic and industrial organisations have
proposed methods for cutting down the cold start time and cold start frequency,
and this taxonomy is being used to explore these methods. There are several
categories in which a current study on cold start latency is organised: caching
and application-level optimization-based solutions, as well as AI/ML-based
solutions. We have analysed the current methods and grouped them into
categories based on their commonalities and features. Finally, we conclude with
a review of current challenges and possible future research directions.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08442" title="Abstract">arXiv:2310.08442</a> [<a href="/pdf/2310.08442" title="Download PDF">pdf</a>, <a href="/format/2310.08442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debias the Training of Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Man Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongsheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Feng Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> University of Science and Technology of China, Alibaba Group, The Chinese University of Hong Kong
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Diffusion models have demonstrated compelling generation quality by
optimizing the variational lower bound through a simple denoising score
matching loss. In this paper, we provide theoretical evidence that the
prevailing practice of using a constant loss weight strategy in diffusion
models leads to biased estimation during the training phase. Simply optimizing
the denoising network to predict Gaussian noise with constant weighting may
hinder precise estimations of original images. To address the issue, we propose
an elegant and effective weighting strategy grounded in the theoretically
unbiased principle. Moreover, we conduct a comprehensive and systematic
exploration to dissect the inherent bias problem deriving from constant
weighting loss from the perspectives of its existence, impact and reasons.
These analyses are expected to advance our understanding and demystify the
inner workings of diffusion models. Through empirical evaluation, we
demonstrate that our proposed debiased estimation method significantly enhances
sample quality without the reliance on complex techniques, and exhibits
improved efficiency compared to the baseline method both in training and
sampling processes.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08444" title="Abstract">arXiv:2310.08444</a> [<a href="/pdf/2310.08444" title="Download PDF">pdf</a>, <a href="/format/2310.08444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Essentially non-hourglass and non-tensile-instability SPH elastic  dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuaihao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Louren%C3%A7o%2C+S+D+N">S&#xe9;rgio D.N. Louren&#xe7;o</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Dong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiangyu Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Since the tension instability was discovered in updated Lagrangian smoothed
particle hydrodynamics (ULSPH) at the end of the 20th century, researchers have
made considerable efforts to suppress its occurrence. However, up to the
present day, this problem has not been fundamentally resolved. In this paper,
the concept of hourglass modes is firstly introduced into ULSPH, and the
inherent causes of tension instability in elastic dynamics are clarified based
on this brand-new perspective. Specifically, we present an essentially
non-hourglass formulation by decomposing the shear acceleration with the
Laplacian operator, and a comprehensive set of challenging benchmark cases for
elastic dynamics is used to showcase that our method can completely eliminate
tensile instability by resolving hourglass modes. The present results reveal
the true origin of tension instability and challenge the traditional
understanding of its sources, i.e., hourglass modes are the real culprit behind
inducing this instability in tension zones rather that the tension itself.
Furthermore, a time integration scheme known as dual-criteria time stepping is
adopted into the simulation of solids for the first time, to significantly
enhance computational efficiency.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08445" title="Abstract">arXiv:2310.08445</a> [<a href="/pdf/2310.08445" title="Download PDF">pdf</a>, <a href="/format/2310.08445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Risk-informed Resilience Planning of Transmission Systems Against Ice  Storms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hu%2C+C">Chenxi Hu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yujia Li</a>, 
<a href="/search/eess?searchtype=author&query=Hou%2C+Y">Yunhe Hou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper proposed a risk-informed resilience planning model for
transmission systems against ice storms. Recognizing the pivotal role of
predictive information, the model quantifies and integrates the associated
uncertainty to optimize resource allocation in anticipation of extreme events.
Considering the changes in operation objectives prior to and during ice storms,
the model adopts a dual-objective approach to strike a balance between economic
and resiliency objectives across both normal and emergency operational
conditions. To accurately model the uncertain contingencies under ice storms,
the model encapsulates the decision-dependent uncertainty (DDU) introduced by
investment decisions and various exogenous uncertainties related to extreme
events. The model is formulated as a stochastic mixed-integer linear problem.
The first stage makes line hardening decisions, as well as the optimal sitting
and sizing of energy storage. The second stage evaluates the risk-informed
operation costs by taking into account both the normal operation prior to the
events and the emergent operation during the events, respectively. Case studies
demonstrate that predictive information can fortify resilience enhancement by
facilitating judicious investment decision-making and optimizing the
utilization of dispatchable resources during risk-informed operations. The
findings highlight the essentiality and efficacy of incorporating predictive
information during planning and operation.The model also provides valuable
insights for grid operators and planners in making risk-informed decisions on
infrastructure investments and operational strategies to better prepare for and
respond to future extreme weather events.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08446" title="Abstract">arXiv:2310.08446</a> [<a href="/pdf/2310.08446" title="Download PDF">pdf</a>, <a href="/format/2310.08446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Multi-Modal Reasoning via Model Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiangyan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Rongxue Li</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+W">Wei Ji</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The reasoning capabilities of LLM (Large Language Model) are widely
acknowledged in recent research, inspiring studies on tool learning and
autonomous agents. LLM serves as the "brain" of agent, orchestrating multiple
tools for collaborative multi-step task solving. Unlike methods invoking tools
like calculators or weather APIs for straightforward tasks, multi-modal agents
excel by integrating diverse AI models for complex challenges. However, current
multi-modal agents neglect the significance of model selection: they primarily
focus on the planning and execution phases, and will only invoke predefined
task-specific models for each subtask, making the execution fragile. Meanwhile,
other traditional model selection methods are either incompatible with or
suboptimal for the multi-modal agent scenarios, due to ignorance of
dependencies among subtasks arising by multi-step reasoning.
<br />To this end, we identify the key challenges therein and propose the
$\textit{M}^3$ framework as a plug-in with negligible runtime overhead at
test-time. This framework improves model selection and bolsters the robustness
of multi-modal agents in multi-step reasoning. In the absence of suitable
benchmarks, we create MS-GQA, a new dataset specifically designed to
investigate the model selection challenge in multi-modal agents. Our
experiments reveal that our framework enables dynamic model selection,
considering both user inputs and subtask dependencies, thereby robustifying the
overall reasoning process. Our code and benchmark:
https://github.com/LINs-lab/M3.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08447" title="Abstract">arXiv:2310.08447</a> [<a href="/pdf/2310.08447" title="Download PDF">pdf</a>, <a href="/ps/2310.08447" title="Download PostScript">ps</a>, <a href="/format/2310.08447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite sections: stability, spectral pollution and asymptotics of  condition numbers and pseudospectra
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lindner%2C+M">Marko Lindner</a>, 
<a href="/search/math?searchtype=author&query=Schmeckpeper%2C+D">Dennis Schmeckpeper</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Functional Analysis (math.FA); Spectral Theory (math.SP)

</div>
<p class="mathjax">The stability of an approximating sequence $(A_n)$ for an operator $A$
usually requires, besides invertibility of $A$, the invertibility of further
operators, say $B, C, \dots$, that are well-associated to the sequence $(A_n)$.
We study this set, $\{A,B,C,\dots\}$, of so-called stability indicators of
$(A_n)$ and connect it to the asymptotics of $\|A_n\|$, $\|A_n^{-1}\|$ and
$\kappa(A_n)=\|A_n\|\|A_n^{-1}\|$ as well as to spectral pollution by showing
that $\limsup {\rm Spec}_\varepsilon A_n= {\rm Spec}_\varepsilon A\cup{\rm
Spec}_\varepsilon B\cup{\rm Spec}_\varepsilon C\cup\dots$. We further specify,
for each of $\|A_n\|$, $\|A_n^{-1}\|$, $\kappa(A_n)$ and ${\rm
Spec}_\varepsilon A_n$, under which conditions even convergence applies.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08450" title="Abstract">arXiv:2310.08450</a> [<a href="/pdf/2310.08450" title="Download PDF">pdf</a>, <a href="/format/2310.08450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monotone discretizations of levelset convex geometric PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Calder%2C+J">Jeff Calder</a>, 
<a href="/search/math?searchtype=author&query=Lee%2C+W">Wonjun Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages including references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We introduce a novel algorithm that converges to level-set convex viscosity
solutions of high-dimensional Hamilton-Jacobi equations. The algorithm is
applicable to a broad class of curvature motion PDEs, as well as a recently
developed Hamilton-Jacobi equation for the Tukey depth, which is a statistical
depth measure of data points. A main contribution of our work is a new monotone
scheme for approximating the direction of the gradient, which allows for
monotone discretizations of pure partial derivatives in the direction of, and
orthogonal to, the gradient. We provide a convergence analysis of the algorithm
on both regular Cartesian grids and unstructured point clouds in any dimension
and present numerical experiments that demonstrate the effectiveness of the
algorithm in approximating solutions of the affine flow in two dimensions and
the Tukey depth measure of high-dimensional datasets such as MNIST and
FashionMNIST.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08451" title="Abstract">arXiv:2310.08451</a> [<a href="/pdf/2310.08451" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proving the Potential of Skeleton Based Action Recognition to Automate  the Analysis of Manual Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berger%2C+M">Marlin Berger</a>, 
<a href="/search/cs?searchtype=author&query=Cloppenburg%2C+F">Frederik Cloppenburg</a>, 
<a href="/search/cs?searchtype=author&query=Eufinger%2C+J">Jens Eufinger</a>, 
<a href="/search/cs?searchtype=author&query=Gries%2C+T">Thomas Gries</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 6 figures. Find peer-reviewed version in Proceedings of IntelliSys 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In manufacturing sectors such as textiles and electronics, manual processes
are a fundamental part of production. The analysis and monitoring of the
processes is necessary for efficient production design. Traditional methods for
analyzing manual processes are complex, expensive, and inflexible. Compared to
established approaches such as Methods-Time-Measurement (MTM), machine learning
(ML) methods promise: Higher flexibility, self-sufficient &amp; permanent use,
lower costs. In this work, based on a video stream, the current motion class in
a manual assembly process is detected. With information on the current motion,
Key-Performance-Indicators (KPIs) can be derived easily. A skeleton-based
action recognition approach is taken, as this field recently shows major
success in machine vision tasks. For skeleton-based action recognition in
manual assembly, no sufficient pre-work could be found. Therefore, a ML
pipeline is developed, to enable extensive research on different (pre-)
processing methods and neural nets. Suitable well generalizing approaches are
found, proving the potential of ML to enhance analyzation of manual processes.
Models detect the current motion, performed by an operator in manual assembly,
but the results can be transferred to all kinds of manual processes.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08453" title="Abstract">arXiv:2310.08453</a> [<a href="/pdf/2310.08453" title="Download PDF">pdf</a>, <a href="/format/2310.08453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Lead-vehicle Kinematics For Rear-end Crash Scenario Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Flannagan%2C+C">Carol Flannagan</a>, 
<a href="/search/cs?searchtype=author&query=Sander%2C+U">Ulrich Sander</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A4rgman%2C+J">Jonas B&#xe4;rgman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The use of virtual safety assessment as the primary method for evaluating
vehicle safety technologies has emphasized the importance of crash scenario
generation. One of the most common crash types is the rear-end crash, which
involves a lead vehicle and a following vehicle. Most studies have focused on
the following vehicle, assuming that the lead vehicle maintains a constant
acceleration/deceleration before the crash. However, there is no evidence for
this premise in the literature. This study aims to address this knowledge gap
by thoroughly analyzing and modeling the lead vehicle's behavior as a first
step in generating rear-end crash scenarios. Accordingly, the study employed a
piecewise linear model to parameterize the speed profiles of lead vehicles,
utilizing two rear-end pre-crash/near-crash datasets. These datasets were
merged and categorized into multiple sub-datasets; for each one, a multivariate
distribution was constructed to represent the corresponding parameters.
Subsequently, a synthetic dataset was generated using these distribution models
and validated by comparison with the original combined dataset. The results
highlight diverse lead-vehicle speed patterns, indicating that a more accurate
model, such as the proposed piecewise linear model, is required instead of the
conventional constant acceleration/deceleration model. Crashes generated with
the proposed models accurately match crash data across the full severity range,
surpassing existing lead-vehicle kinematics models in both severity range and
accuracy. By providing more realistic speed profiles for the lead vehicle, the
model developed in the study contributes to creating realistic rear-end crash
scenarios and reconstructing real-life crashes.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08454" title="Abstract">arXiv:2310.08454</a> [<a href="/pdf/2310.08454" title="Download PDF">pdf</a>, <a href="/ps/2310.08454" title="Download PostScript">ps</a>, <a href="/format/2310.08454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster Ascending Auctions via Polymatroid Sum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eickhoff%2C+K">Katharina Eickhoff</a>, 
<a href="/search/cs?searchtype=author&query=Peis%2C+B">Britta Peis</a>, 
<a href="/search/cs?searchtype=author&query=Rieken%2C+N">Niklas Rieken</a>, 
<a href="/search/cs?searchtype=author&query=Koch%2C+L+V">Laura Vargas Koch</a>, 
<a href="/search/cs?searchtype=author&query=V%C3%A9gh%2C+L+A">L&#xe1;szl&#xf3; A. V&#xe9;gh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We consider ascending auctions for finding Walrasian equilibria in markets
with indivisible items and gross substitutes valuation functions. Each price
increase step in the auction algorithm requires finding an inclusion-wise
minimal maximal overdemanded set at the current prices. This can be formulated
as a submodular function minimization problem. We observe that minimizing this
submodular function corresponds to a polymatroid sum problem, and using this
viewpoint, we give a fast and simple push-relabel algorithm for finding the
minimal maximal overdemanded set. This improves on the previously best running
time of Murota, Shioura and Yang (ISAAC 2013). Our algorithm is an adaptation
of the push-relabel framework by Frank and Mikl\'os (JJIAM, 2012) to the
particular setting. We obtain a further improvement for the special case of
unit-supplies. Furthermore, we show that for gross substitutes valuations, the
component-wise minimal competitive prices are the same as the minimal Walrasian
prices. This enables us to derive monotonicity properties of the Walrasian
prices. Namely, we show that the minimal Walrasian prices can only increase if
supply decreases, or demand increases.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08455" title="Abstract">arXiv:2310.08455</a> [<a href="/pdf/2310.08455" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metrics for popularity bias in dynamic recommender systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Braun%2C+V">Valentijn Braun</a>, 
<a href="/search/cs?searchtype=author&query=Bhaumik%2C+D">Debarati Bhaumik</a>, 
<a href="/search/cs?searchtype=author&query=Dey%2C+D">Diptish Dey</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Albeit the widespread application of recommender systems (RecSys) in our
daily lives, rather limited research has been done on quantifying unfairness
and biases present in such systems. Prior work largely focuses on determining
whether a RecSys is discriminating or not but does not compute the amount of
bias present in these systems. Biased recommendations may lead to decisions
that can potentially have adverse effects on individuals, sensitive user
groups, and society. Hence, it is important to quantify these biases for fair
and safe commercial applications of these systems. This paper focuses on
quantifying popularity bias that stems directly from the output of RecSys
models, leading to over recommendation of popular items that are likely to be
misaligned with user preferences. Four metrics to quantify popularity bias in
RescSys over time in dynamic setting across different sensitive user groups
have been proposed. These metrics have been demonstrated for four collaborative
filtering based RecSys algorithms trained on two commonly used benchmark
datasets in the literature. Results obtained show that the metrics proposed
provide a comprehensive understanding of growing disparities in treatment
between sensitive groups over time when used conjointly.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08459" title="Abstract">arXiv:2310.08459</a> [<a href="/pdf/2310.08459" title="Download PDF">pdf</a>, <a href="/format/2310.08459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Heterogeneous Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+R">Runxue Bao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yiming Sun</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yuhe Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Z">Zhi-Hong Mao</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Ye Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The application of transfer learning, an approach utilizing knowledge from a
source domain to enhance model performance in a target domain, has seen a
tremendous rise in recent years, underpinning many real-world scenarios. The
key to its success lies in the shared common knowledge between the domains, a
prerequisite in most transfer learning methodologies. These methods typically
presuppose identical feature spaces and label spaces in both domains, known as
homogeneous transfer learning, which, however, is not always a practical
assumption. Oftentimes, the source and target domains vary in feature spaces,
data distributions, and label spaces, making it challenging or costly to secure
source domain data with identical feature and label spaces as the target
domain. Arbitrary elimination of these differences is not always feasible or
optimal. Thus, heterogeneous transfer learning, acknowledging and dealing with
such disparities, has emerged as a promising approach for a variety of tasks.
Despite the existence of a survey in 2017 on this topic, the fast-paced
advances post-2017 necessitate an updated, in-depth review. We therefore
present a comprehensive survey of recent developments in heterogeneous transfer
learning methods, offering a systematic guide for future research. Our paper
reviews methodologies for diverse learning scenarios, discusses the limitations
of current studies, and covers various application contexts, including Natural
Language Processing, Computer Vision, Multimodality, and Biomedicine, to foster
a deeper understanding and spur future research.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08461" title="Abstract">arXiv:2310.08461</a> [<a href="/pdf/2310.08461" title="Download PDF">pdf</a>, <a href="/format/2310.08461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DistillSpec: Improving Speculative Decoding via Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yongchao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+K">Kaifeng Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Rawat%2C+A+S">Ankit Singh Rawat</a>, 
<a href="/search/cs?searchtype=author&query=Menon%2C+A+K">Aditya Krishna Menon</a>, 
<a href="/search/cs?searchtype=author&query=Rostamizadeh%2C+A">Afshin Rostamizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sanjiv Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Kagy%2C+J">Jean-Fran&#xe7;ois Kagy</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+R">Rishabh Agarwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Speculative decoding (SD) accelerates large language model inference by
employing a faster draft model for generating multiple tokens, which are then
verified in parallel by the larger target model, resulting in the text
generated according to the target model distribution. However, identifying a
compact draft model that is well-aligned with the target model is challenging.
To tackle this issue, we propose DistillSpec that uses knowledge distillation
to better align the draft model with the target model, before applying SD.
DistillSpec makes two key design choices, which we demonstrate via systematic
study to be crucial to improving the draft and target alignment: utilizing
on-policy data generation from the draft model, and tailoring the divergence
function to the task and decoding strategy. Notably, DistillSpec yields
impressive 10 - 45% speedups over standard SD on a range of standard
benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine
DistillSpec with lossy SD to achieve fine-grained control over the latency vs.
task performance trade-off. Finally, in practical scenarios with models of
varying sizes, first using distillation to boost the performance of the target
model and then applying DistillSpec to train a well-aligned draft model can
reduce decoding latency by 6-10x with minimal performance drop, compared to
standard decoding without distillation.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08465" title="Abstract">arXiv:2310.08465</a> [<a href="/pdf/2310.08465" title="Download PDF">pdf</a>, <a href="/format/2310.08465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MotionDirector: Motion Customization of Text-to-Video Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Rui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuchao Gu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J+Z">Jay Zhangjie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D+J">David Junhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiawei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Weijia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Keppo%2C+J">Jussi Keppo</a>, 
<a href="/search/cs?searchtype=author&query=Shou%2C+M+Z">Mike Zheng Shou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://showlab.github.io/MotionDirector/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Large-scale pre-trained diffusion models have exhibited remarkable
capabilities in diverse video generations. Given a set of video clips of the
same motion concept, the task of Motion Customization is to adapt existing
text-to-video diffusion models to generate videos with this motion. For
example, generating a video with a car moving in a prescribed manner under
specific camera movements to make a movie, or a video illustrating how a bear
would lift weights to inspire creators. Adaptation methods have been developed
for customizing appearance like subject or style, yet unexplored for motion. It
is straightforward to extend mainstream adaption methods for motion
customization, including full model tuning, parameter-efficient tuning of
additional layers, and Low-Rank Adaptions (LoRAs). However, the motion concept
learned by these methods is often coupled with the limited appearances in the
training videos, making it difficult to generalize the customized motion to
other appearances. To overcome this challenge, we propose MotionDirector, with
a dual-path LoRAs architecture to decouple the learning of appearance and
motion. Further, we design a novel appearance-debiased temporal loss to
mitigate the influence of appearance on the temporal training objective.
Experimental results show the proposed method can generate videos of diverse
appearances for the customized motions. Our method also supports various
downstream applications, such as the mixing of different videos with their
appearance and motion respectively, and animating a single image with
customized motions. Our code and model weights will be released.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08470" title="Abstract">arXiv:2310.08470</a> [<a href="/pdf/2310.08470" title="Download PDF">pdf</a>, <a href="/format/2310.08470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategies and impact of learning curve estimation for CNN-based image  classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Didyk%2C+L">Laura Didyk</a>, 
<a href="/search/cs?searchtype=author&query=Yarish%2C+B">Brayden Yarish</a>, 
<a href="/search/cs?searchtype=author&query=Beck%2C+M+A">Michael A. Beck</a>, 
<a href="/search/cs?searchtype=author&query=Bidinosti%2C+C+P">Christopher P. Bidinosti</a>, 
<a href="/search/cs?searchtype=author&query=Henry%2C+C+J">Christopher J. Henry</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Learning curves are a measure for how the performance of machine learning
models improves given a certain volume of training data. Over a wide variety of
applications and models it was observed that learning curves follow -- to a
large extent -- a power law behavior. This makes the performance of different
models for a given task somewhat predictable and opens the opportunity to
reduce the training time for practitioners, who are exploring the space of
possible models and hyperparameters for the problem at hand. By estimating the
learning curve of a model from training on small subsets of data only the best
models need to be considered for training on the full dataset. How to choose
subset sizes and how often to sample models on these to obtain estimates is
however not researched. Given that the goal is to reduce overall training time
strategies are needed that sample the performance in a time-efficient way and
yet leads to accurate learning curve estimates. In this paper we formulate the
framework for these strategies and propose several strategies. Further we
evaluate the strategies for simulated learning curves and in experiments with
popular datasets and models for image classification tasks.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08471" title="Abstract">arXiv:2310.08471</a> [<a href="/pdf/2310.08471" title="Download PDF">pdf</a>, <a href="/format/2310.08471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WinSyn: A High Resolution Testbed for Synthetic Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kelly%2C+T">Tom Kelly</a>, 
<a href="/search/cs?searchtype=author&query=Femiani%2C+J">John Femiani</a>, 
<a href="/search/cs?searchtype=author&query=Wonka%2C+P">Peter Wonka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">We present WinSyn, a dataset consisting of high-resolution photographs and
renderings of 3D models as a testbed for synthetic-to-real research. The
dataset consists of 75,739 high-resolution photographs of building windows,
including traditional and modern designs, captured globally. These include
89,318 cropped subimages of windows, of which 9,002 are semantically labeled.
Further, we present our domain-matched photorealistic procedural model which
enables experimentation over a variety of parameter distributions and
engineering approaches. Our procedural model provides a second corresponding
dataset of 21,290 synthetic images. This jointly developed dataset is designed
to facilitate research in the field of synthetic-to-real learning and synthetic
data generation. WinSyn allows experimentation into the factors that make it
challenging for synthetic data to compete with real-world data. We perform
ablations using our synthetic model to identify the salient rendering,
materials, and geometric factors pertinent to accuracy within the labeling
task. We chose windows as a benchmark because they exhibit a large variability
of geometry and materials in their design, making them ideal to study synthetic
data generation in a constrained setting. We argue that the dataset is a
crucial step to enable future research in synthetic data generation for deep
learning.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08473" title="Abstract">arXiv:2310.08473</a> [<a href="/pdf/2310.08473" title="Download PDF">pdf</a>, <a href="/format/2310.08473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No-Regret Learning and Equilibrium Computation in Quantum Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Wayne Lin</a>, 
<a href="/search/cs?searchtype=author&query=Piliouras%2C+G">Georgios Piliouras</a>, 
<a href="/search/cs?searchtype=author&query=Sim%2C+R">Ryann Sim</a>, 
<a href="/search/cs?searchtype=author&query=Varvitsiotis%2C+A">Antonios Varvitsiotis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">As quantum processors advance, the emergence of large-scale decentralized
systems involving interacting quantum-enabled agents is on the horizon. Recent
research efforts have explored quantum versions of Nash and correlated
equilibria as solution concepts of strategic quantum interactions, but these
approaches did not directly connect to decentralized adaptive setups where
agents possess limited information. This paper delves into the dynamics of
quantum-enabled agents within decentralized systems that employ no-regret
algorithms to update their behaviors over time. Specifically, we investigate
two-player quantum zero-sum games and polymatrix quantum zero-sum games,
showing that no-regret algorithms converge to separable quantum Nash equilibria
in time-average. In the case of general multi-player quantum games, our work
leads to a novel solution concept, (separable) quantum coarse correlated
equilibria (QCCE), as the convergent outcome of the time-averaged behavior
no-regret algorithms, offering a natural solution concept for decentralized
quantum systems. Finally, we show that computing QCCEs can be formulated as a
semidefinite program and establish the existence of entangled (i.e.,
non-separable) QCCEs, which cannot be approached via the current paradigm of
no-regret learning.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08475" title="Abstract">arXiv:2310.08475</a> [<a href="/pdf/2310.08475" title="Download PDF">pdf</a>, <a href="/format/2310.08475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can We Edit Multimodal Large Language Models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Siyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+B">Bozhong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qingbin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">In this paper, we focus on editing Multimodal Large Language Models (MLLMs).
Compared to editing single-modal LLMs, multimodal model editing is more
challenging, which demands a higher level of scrutiny and careful consideration
in the editing process. To facilitate research in this area, we construct a new
benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite
of innovative metrics for evaluation. We conduct comprehensive experiments
involving various model editing baselines and analyze the impact of editing
different components for multimodal LLMs. Empirically, we notice that previous
baselines can implement editing multimodal LLMs to some extent, but the effect
is still barely satisfactory, indicating the potential difficulty of this task.
We hope that our work can provide the NLP community with insights\footnote{Code
and dataset are available in https://github.com/zjunlp/EasyEdit.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08483" title="Abstract">arXiv:2310.08483</a> [<a href="/pdf/2310.08483" title="Download PDF">pdf</a>, <a href="/format/2310.08483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Humans Behind Online Misinformation: An Observational  Study Through the Lens of the COVID-19 Pandemic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chandra%2C+M">Mohit Chandra</a>, 
<a href="/search/cs?searchtype=author&query=Mattapalli%2C+A">Anush Mattapalli</a>, 
<a href="/search/cs?searchtype=author&query=De+Choudhury%2C+M">Munmun De Choudhury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The proliferation of online misinformation has emerged as one of the biggest
threats to society. Considerable efforts have focused on building
misinformation detection models, still the perils of misinformation remain
abound. Mitigating online misinformation and its ramifications requires a
holistic approach that encompasses not only an understanding of its intricate
landscape in relation to the complex issue and topic-rich information ecosystem
online, but also the psychological drivers of individuals behind it. Adopting a
time series analytic technique and robust causal inference-based design, we
conduct a large-scale observational study analyzing over 32 million COVID-19
tweets and 16 million historical timeline tweets. We focus on understanding the
behavior and psychology of users disseminating misinformation during COVID-19
and its relationship with the historical inclinations towards sharing
misinformation on Non-COVID topics before the pandemic. Our analysis
underscores the intricacies inherent to cross-topic misinformation, and
highlights that users' historical inclination toward sharing misinformation is
positively associated with their present behavior pertaining to misinformation
sharing on emergent topics and beyond. This work may serve as a valuable
foundation for designing user-centric inoculation strategies and
ecologically-grounded agile interventions for effectively tackling online
misinformation.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08487" title="Abstract">arXiv:2310.08487</a> [<a href="/pdf/2310.08487" title="Download PDF">pdf</a>, <a href="/format/2310.08487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphextQA: A Benchmark for Evaluating Graph-Enhanced Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yuanchun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+R">Ruotong Liao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhen Han</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yunpu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Tresp%2C+V">Volker Tresp</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While multi-modal models have successfully integrated information from image,
video, and audio modalities, integrating graph modality into large language
models (LLMs) remains unexplored. This discrepancy largely stems from the
inherent divergence between structured graph data and unstructured text data.
Incorporating graph knowledge provides a reliable source of information,
enabling potential solutions to address issues in text generation, e.g.,
hallucination, and lack of domain knowledge. To evaluate the integration of
graph knowledge into language models, a dedicated dataset is needed. However,
there is currently no benchmark dataset specifically designed for multimodal
graph-language models. To address this gap, we propose GraphextQA, a question
answering dataset with paired subgraphs, retrieved from Wikidata, to facilitate
the evaluation and future development of graph-language models. Additionally,
we introduce a baseline model called CrossGNN, which conditions answer
generation on the paired graphs by cross-attending question-aware graph
features at decoding. The proposed dataset is designed to evaluate
graph-language models' ability to understand graphs and make use of it for
answer generation. We perform experiments with language-only models and the
proposed graph-language model to validate the usefulness of the paired graphs
and to demonstrate the difficulty of the task.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08488" title="Abstract">arXiv:2310.08488</a> [<a href="/pdf/2310.08488" title="Download PDF">pdf</a>, <a href="/format/2310.08488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Community Consensus: Converging Locally despite Adversaries and  Heterogeneous Connectivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gava%2C+C">Cristina Gava</a>, 
<a href="/search/cs?searchtype=author&query=Vekassy%2C+A">Aron Vekassy</a>, 
<a href="/search/cs?searchtype=author&query=Cavorsi%2C+M">Matthew Cavorsi</a>, 
<a href="/search/cs?searchtype=author&query=Gil%2C+S">Stephanie Gil</a>, 
<a href="/search/cs?searchtype=author&query=Mallmann-Trenn%2C+F">Frederik Mallmann-Trenn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">We introduce the concept of community consensus in the presence of malicious
agents using a well-known median-based consensus algorithm. We consider
networks that have multiple well-connected regions that we term communities,
characterized by specific robustness and minimum degree properties. Prior work
derives conditions on properties that are necessary and sufficient for
achieving global consensus in a network. This however, requires the minimum
degree of the network graph to be proportional to the number of malicious
agents in the network, which is not very practical in large networks. In this
work we present a natural generalization of this previous result. We
characterize cases when although global consensus is not reached, some subsets
of agents $V_i$ will still converge to the same values $\mathcal{M}_i$ among
themselves. We define more relaxed requirements for this new type of consensus
to be reached in terms of the number $k$ of edges connecting an agent in a
community to agents external to the community, and the number of malicious
agents in each community.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08491" title="Abstract">arXiv:2310.08491</a> [<a href="/pdf/2310.08491" title="Download PDF">pdf</a>, <a href="/format/2310.08491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prometheus: Inducing Fine-grained Evaluation Capability in Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungone Kim</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jamin Shin</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+Y">Yejin Cho</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+J">Joel Jang</a>, 
<a href="/search/cs?searchtype=author&query=Longpre%2C+S">Shayne Longpre</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hwaran Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Sangdoo Yun</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+S">Seongjin Shin</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungdong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Thorne%2C+J">James Thorne</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, using a powerful proprietary Large Language Model (LLM) (e.g.,
GPT-4) as an evaluator for long-form responses has become the de facto
standard. However, for practitioners with large-scale evaluation tasks and
custom criteria in consideration (e.g., child-readability), using proprietary
LLMs as an evaluator is unreliable due to the closed-source nature,
uncontrolled versioning, and prohibitive costs. In this work, we propose
Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation
capabilities when the appropriate reference materials (reference answer, score
rubric) are accompanied. We first construct the Feedback Collection, a new
dataset that consists of 1K fine-grained score rubrics, 20K instructions, and
100K responses and language feedback generated by GPT-4. Using the Feedback
Collection, we train Prometheus, a 13B evaluator LLM that can assess any given
long-form text based on customized score rubric provided by the user.
Experimental results show that Prometheus scores a Pearson correlation of 0.897
with human evaluators when evaluating with 45 customized score rubrics, which
is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392).
Furthermore, measuring correlation with GPT-4 with 1222 customized score
rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask
Eval) shows similar trends, bolstering Prometheus's capability as an evaluator
LLM. Lastly, Prometheus achieves the highest accuracy on two human preference
benchmarks (HHH Alignment &amp; MT Bench Human Judgment) compared to open-sourced
reward models explicitly trained on human preference datasets, highlighting its
potential as an universal reward model. We open-source our code, dataset, and
model at https://github.com/kaistAI/Prometheus.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08494" title="Abstract">arXiv:2310.08494</a> [<a href="/pdf/2310.08494" title="Download PDF">pdf</a>, <a href="/format/2310.08494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Experience-based TAMP Framework for Foliated Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jiaming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+S+R">Shrutheesh R. Iyer</a>, 
<a href="/search/cs?searchtype=author&query=Christensen%2C+H+I">Henrik I. Christensen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Due to their complexity, foliated structure problems often pose intricate
challenges to task and motion planning in robotics manipulation. To counter
this, our study presents the ``Foliated Repetition Roadmap.'' This roadmap
assists task and motion planners by transforming the complex foliated structure
problem into a more accessible graph format. By leveraging query experiences
from different foliated manifolds, our framework can dynamically and
efficiently update this graph. The refined graph can generate distribution
sets, optimizing motion planning performance in foliated structure problems. In
our paper, we lay down the theoretical groundwork and illustrate its practical
applications through real-world examples.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08496" title="Abstract">arXiv:2310.08496</a> [<a href="/pdf/2310.08496" title="Download PDF">pdf</a>, <a href="/format/2310.08496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Uncertainty-based Retrieval Framework for Ancient Chinese CWS and  POS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pengyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhichen Ren</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the Second Workshop on Language Technologies for
  Historical and Ancient Languages, 2022, 164-168
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automatic analysis for modern Chinese has greatly improved the accuracy of
text mining in related fields, but the study of ancient Chinese is still
relatively rare. Ancient text division and lexical annotation are important
parts of classical literature comprehension, and previous studies have tried to
construct auxiliary dictionary and other fused knowledge to improve the
performance. In this paper, we propose a framework for ancient Chinese Word
Segmentation and Part-of-Speech Tagging that makes a twofold effort: on the one
hand, we try to capture the wordhood semantics; on the other hand, we
re-predict the uncertain samples of baseline model by introducing external
knowledge. The performance of our architecture outperforms pre-trained BERT
with CRF and existing tools such as Jiayan.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08497" title="Abstract">arXiv:2310.08497</a> [<a href="/pdf/2310.08497" title="Download PDF">pdf</a>, <a href="/format/2310.08497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of time and note duration tokenizations on deep learning symbolic  music modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fradet%2C+N">Nathan Fradet</a>, 
<a href="/search/cs?searchtype=author&query=Gutowski%2C+N">Nicolas Gutowski</a>, 
<a href="/search/cs?searchtype=author&query=Chhel%2C+F">Fabien Chhel</a>, 
<a href="/search/cs?searchtype=author&query=Briot%2C+J">Jean-Pierre Briot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ISMIR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Symbolic music is widely used in various deep learning tasks, including
generation, transcription, synthesis, and Music Information Retrieval (MIR). It
is mostly employed with discrete models like Transformers, which require music
to be tokenized, i.e., formatted into sequences of distinct elements called
tokens. Tokenization can be performed in different ways. As Transformer can
struggle at reasoning, but capture more easily explicit information, it is
important to study how the way the information is represented for such model
impact their performances. In this work, we analyze the common tokenization
methods and experiment with time and note duration representations. We compare
the performances of these two impactful criteria on several tasks, including
composer and emotion classification, music generation, and sequence
representation learning. We demonstrate that explicit information leads to
better results depending on the task.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08501" title="Abstract">arXiv:2310.08501</a> [<a href="/pdf/2310.08501" title="Download PDF">pdf</a>, <a href="/format/2310.08501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Learning of Object-Centric Embeddings for Cell Instance  Segmentation in Microscopy Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wolf%2C+S">Steffen Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Lalit%2C+M">Manan Lalit</a>, 
<a href="/search/cs?searchtype=author&query=Westmacott%2C+H">Henry Westmacott</a>, 
<a href="/search/cs?searchtype=author&query=McDole%2C+K">Katie McDole</a>, 
<a href="/search/cs?searchtype=author&query=Funke%2C+J">Jan Funke</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2023, pages 21263-21272
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Segmentation of objects in microscopy images is required for many biomedical
applications. We introduce object-centric embeddings (OCEs), which embed image
patches such that the spatial offsets between patches cropped from the same
object are preserved. Those learnt embeddings can be used to delineate
individual objects and thus obtain instance segmentations. Here, we show
theoretically that, under assumptions commonly found in microscopy images, OCEs
can be learnt through a self-supervised task that predicts the spatial offset
between image patches. Together, this forms an unsupervised cell instance
segmentation method which we evaluate on nine diverse large-scale microscopy
datasets. Segmentations obtained with our method lead to substantially improved
results, compared to state-of-the-art baselines on six out of nine datasets,
and perform on par on the remaining three datasets. If ground-truth annotations
are available, our method serves as an excellent starting point for supervised
training, reducing the required amount of ground-truth needed by one order of
magnitude, thus substantially increasing the practical applicability of our
method. Source code is available at https://github.com/funkelab/cellulus.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08507" title="Abstract">arXiv:2310.08507</a> [<a href="/pdf/2310.08507" title="Download PDF">pdf</a>, <a href="/format/2310.08507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Yuga: Automatically Detecting Lifetime Annotation Bugs in the Rust  Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nitin%2C+V">Vikram Nitin</a>, 
<a href="/search/cs?searchtype=author&query=Mulhern%2C+A">Anne Mulhern</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Sanjay Arora</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+B">Baishakhi Ray</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The Rust programming language is becoming increasingly popular among systems
programmers due to its efficient performance and robust memory safety
guarantees. Rust employs an ownership model to ensure this guarantee by
allowing each value to be owned by only one identifier at a time. Additionally,
it introduces the concept of borrowing and lifetimes to enable other variables
to borrow the values under certain conditions temporarily. Despite its
benefits, security vulnerabilities have been reported in Rust projects, often
attributed to the use of "unsafe" Rust code. These vulnerabilities, in part,
arise from incorrect lifetime annotations on function signatures. However,
existing tools fail to detect these bugs, primarily because such bugs are rare,
challenging to detect through dynamic analysis, and require explicit memory
models. To overcome these limitations, first, we characterize incorrect
lifetime annotations as a source of memory safety bugs and leverage this
understanding to devise a novel static analysis tool, Yuga, to detect potential
lifetime annotation bugs. Yuga uses a multi-phase analysis approach, starting
with a quick pattern-matching algorithm to identify potential buggy components
and then conducting a flow and field-sensitive alias analysis to confirm the
bugs. We also curate new datasets of lifetime annotation bugs. Yuga
successfully detects bugs with good precision on these datasets, and we make
the code and datasets publicly available for review.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08511" title="Abstract">arXiv:2310.08511</a> [<a href="/pdf/2310.08511" title="Download PDF">pdf</a>, <a href="/format/2310.08511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HoneyBee: Progressive Instruction Finetuning of Large Language Models  for Materials Science
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yu Song</a>, 
<a href="/search/cs?searchtype=author&query=Miret%2C+S">Santiago Miret</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We propose an instruction-based process for trustworthy data curation in
materials science (MatSci-Instruct), which we then apply to finetune a
LLaMa-based language model targeted for materials science (HoneyBee).
MatSci-Instruct helps alleviate the scarcity of relevant, high-quality
materials science textual data available in the open literature, and HoneyBee
is the first billion-parameter language model specialized to materials science.
In MatSci-Instruct we improve the trustworthiness of generated data by
prompting multiple commercially available large language models for generation
with an Instructor module (e.g. Chat-GPT) and verification from an independent
Verifier module (e.g. Claude). Using MatSci-Instruct, we construct a dataset of
multiple tasks and measure the quality of our dataset along multiple
dimensions, including accuracy against known facts, relevance to materials
science, as well as completeness and reasonableness of the data. Moreover, we
iteratively generate more targeted instructions and instruction-data in a
finetuning-evaluation-feedback loop leading to progressively better performance
for our finetuned HoneyBee models. Our evaluation on the MatSci-NLP benchmark
shows HoneyBee's outperformance of existing language models on materials
science tasks and iterative improvement in successive stages of
instruction-data refinement. We study the quality of HoneyBee's language
modeling through automatic evaluation and analyze case studies to further
understand the model's capabilities and limitations. Our code and relevant
datasets are publicly available at
\url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-HoneyBee}.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08513" title="Abstract">arXiv:2310.08513</a> [<a href="/pdf/2310.08513" title="Download PDF">pdf</a>, <a href="/format/2310.08513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How connectivity structure shapes rich and lazy learning in neural  circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y+H">Yuhan Helena Liu</a>, 
<a href="/search/cs?searchtype=author&query=Baratin%2C+A">Aristide Baratin</a>, 
<a href="/search/cs?searchtype=author&query=Cornford%2C+J">Jonathan Cornford</a>, 
<a href="/search/cs?searchtype=author&query=Mihalas%2C+S">Stefan Mihalas</a>, 
<a href="/search/cs?searchtype=author&query=Shea-Brown%2C+E">Eric Shea-Brown</a>, 
<a href="/search/cs?searchtype=author&query=Lajoie%2C+G">Guillaume Lajoie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">In theoretical neuroscience, recent work leverages deep learning tools to
explore how some network attributes critically influence its learning dynamics.
Notably, initial weight distributions with small (resp. large) variance may
yield a rich (resp. lazy) regime, where significant (resp. minor) changes to
network states and representation are observed over the course of learning.
However, in biology, neural circuit connectivity generally has a low-rank
structure and therefore differs markedly from the random initializations
generally used for these studies. As such, here we investigate how the
structure of the initial weights, in particular their effective rank,
influences the network learning regime. Through both empirical and theoretical
analyses, we discover that high-rank initializations typically yield smaller
network changes indicative of lazier learning, a finding we also confirm with
experimentally-driven initial connectivity in recurrent neural networks.
Conversely, low-rank initialization biases learning towards richer learning.
Importantly, however, as an exception to this rule, we find lazier learning can
still occur with a low-rank initialization that aligns with task and data
statistics. Our research highlights the pivotal role of initial weight
structures in shaping learning regimes, with implications for metabolic costs
of plasticity and risks of catastrophic forgetting.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08517" title="Abstract">arXiv:2310.08517</a> [<a href="/pdf/2310.08517" title="Download PDF">pdf</a>, <a href="/format/2310.08517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A linear proof language for second-order intuitionistic linear logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%C3%ADaz-Caro%2C+A">Alejandro D&#xed;az-Caro</a>, 
<a href="/search/cs?searchtype=author&query=Dowek%2C+G">Gilles Dowek</a>, 
<a href="/search/cs?searchtype=author&query=Ivnisky%2C+M">Malena Ivnisky</a>, 
<a href="/search/cs?searchtype=author&query=Malherbe%2C+O">Octavio Malherbe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages + appendix. arXiv admin note: text overlap with <a href="/abs/2201.11221">arXiv:2201.11221</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We present a polymorphic linear lambda-calculus as a proof language for
second-order intuitionistic linear logic. The calculus includes addition and
scalar multiplication, enabling the proof of a linearity result at the
syntactic level.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08523" title="Abstract">arXiv:2310.08523</a> [<a href="/pdf/2310.08523" title="Download PDF">pdf</a>, <a href="/format/2310.08523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-augmented Preference Learning from Natural Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+I">Inwon Kang</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+S">Sikai Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+T">Tyler Ho</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jui-Chien Lin</a>, 
<a href="/search/cs?searchtype=author&query=Mohsin%2C+F">Farhad Mohsin</a>, 
<a href="/search/cs?searchtype=author&query=Seneviratne%2C+O">Oshani Seneviratne</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Lirong Xia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Finding preferences expressed in natural language is an important but
challenging task. State-of-the-art(SotA) methods leverage transformer-based
models such as BERT, RoBERTa, etc. and graph neural architectures such as graph
attention networks. Since Large Language Models (LLMs) are equipped to deal
with larger context lengths and have much larger model sizes than the
transformer-based model, we investigate their ability to classify comparative
text directly. This work aims to serve as a first step towards using LLMs for
the CPC task. We design and conduct a set of experiments that format the
classification task into an input prompt for the LLM and a methodology to get a
fixed-format response that can be automatically evaluated. Comparing
performances with existing methods, we see that pre-trained LLMs are able to
outperform the previous SotA models with no fine-tuning involved. Our results
show that the LLMs can consistently outperform the SotA when the target text is
large -- i.e. composed of multiple sentences --, and are still comparable to
the SotA performance in shorter text. We also find that few-shot learning
yields better performance than zero-shot learning.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08528" title="Abstract">arXiv:2310.08528</a> [<a href="/pdf/2310.08528" title="Download PDF">pdf</a>, <a href="/format/2310.08528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+G">Guanjun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+T">Taoran Yi</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+J">Jiemin Fang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lingxi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaopeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Qi Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinggang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress. Project page: <a href="https://guanjunwu.github.io/4dgs/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Representing and rendering dynamic scenes has been an important but
challenging task. Especially, to accurately model complex motions, high
efficiency is usually hard to maintain. We introduce the 4D Gaussian Splatting
(4D-GS) to achieve real-time dynamic scene rendering while also enjoying high
training and storage efficiency. An efficient deformation field is constructed
to model both Gaussian motions and shape deformations. Different adjacent
Gaussians are connected via a HexPlane to produce more accurate position and
shape deformations. Our 4D-GS method achieves real-time rendering under high
resolutions, 70 FPS at a 800$\times$800 resolution on an RTX 3090 GPU, while
maintaining comparable or higher quality than previous state-of-the-art
methods. More demos and code are available at
https://guanjunwu.github.io/4dgs/.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08529" title="Abstract">arXiv:2310.08529</a> [<a href="/pdf/2310.08529" title="Download PDF">pdf</a>, <a href="/format/2310.08529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with  Point Cloud Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+T">Taoran Yi</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+J">Jiemin Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+G">Guanjun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lingxi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaopeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Qi Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinggang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress. Project page: <a href="https://taoranyi.com/gaussiandreamer/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">In recent times, the generation of 3D assets from text prompts has shown
impressive results. Both 2D and 3D diffusion models can generate decent 3D
objects based on prompts. 3D diffusion models have good 3D consistency, but
their quality and generalization are limited as trainable 3D data is expensive
and hard to obtain. 2D diffusion models enjoy strong abilities of
generalization and fine generation, but the 3D consistency is hard to
guarantee. This paper attempts to bridge the power from the two types of
diffusion models via the recent explicit and efficient 3D Gaussian splatting
representation. A fast 3D generation framework, named as \name, is proposed,
where the 3D diffusion model provides point cloud priors for initialization and
the 2D diffusion model enriches the geometry and appearance. Operations of
noisy point growing and color perturbation are introduced to enhance the
initialized Gaussians. Our \name can generate a high-quality 3D instance within
25 minutes on one GPU, much faster than previous methods, while the generated
instances can be directly rendered in real time. Demos and code are available
at https://taoranyi.com/gaussiandreamer/.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08530" title="Abstract">arXiv:2310.08530</a> [<a href="/pdf/2310.08530" title="Download PDF">pdf</a>, <a href="/format/2310.08530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniPose: Detecting Any Keypoints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+A">Ailing Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruimao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This work proposes a unified framework called UniPose to detect keypoints of
any articulated (e.g., human and animal), rigid, and soft objects via visual or
textual prompts for fine-grained vision understanding and manipulation.
Keypoint is a structure-aware, pixel-level, and compact representation of any
object, especially articulated objects. Existing fine-grained promptable tasks
mainly focus on object instance detection and segmentation but often fail to
identify fine-grained granularity and structured information of image and
instance, such as eyes, leg, paw, etc. Meanwhile, prompt-based keypoint
detection is still under-explored. To bridge the gap, we make the first attempt
to develop an end-to-end prompt-based keypoint detection framework called
UniPose to detect keypoints of any objects. As keypoint detection tasks are
unified in this framework, we can leverage 13 keypoint detection datasets with
338 keypoints across 1,237 categories over 400K instances to train a generic
keypoint detection model. UniPose can effectively align text-to-keypoint and
image-to-keypoint due to the mutual enhancement of textual and visual prompts
based on the cross-modality contrastive learning optimization objectives. Our
experimental results show that UniPose has strong fine-grained localization and
generalization abilities across image styles, categories, and poses. Based on
UniPose as a generalist keypoint detector, we hope it could serve fine-grained
visual perception, understanding, and generation.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08532" title="Abstract">arXiv:2310.08532</a> [<a href="/pdf/2310.08532" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Platform for generating medical datasets for machine learning in public  health
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andreychenko%2C+A">Anna Andreychenko</a>, 
<a href="/search/cs?searchtype=author&query=Korzhuk%2C+V">Viktoriia Korzhuk</a>, 
<a href="/search/cs?searchtype=author&query=Kondratenko%2C+S">Stanislav Kondratenko</a>, 
<a href="/search/cs?searchtype=author&query=Cheraneva%2C+P">Polina Cheraneva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Currently, there are many difficulties regarding the interoperability of
medical data and related population data sources. These complications get in
the way of the generation of high-quality data sets at city, region and
national levels. Moreover, the collection of datasets within large medical
centers is feasible due to own IT departments whereas the collection of raw
medical data from multiple organizations is a more complicated process. In
these circumstances, the most appropriate option is to develop digital products
based on microservice architecture. Because of this approach, it is possible to
ensure the multimodality of the system, the flexibility of the interface and
the internal system approach, when interconnected elements behave as a whole,
demonstrating behavior different from the behavior when working independently.
These conditions allow, in turn, to ensure the maximum number and
representativeness of the resulting data sets. This paper demonstrates a
concept of the platform for a sustainable generation of quality and reliable
sets of multimodal medical data. It collects data from different external
sources, harmonizes it using a special service, anonymizes harmonized data, and
labels processed data. The proposed system aims to be a promising solution to
the improvement of medical data quality for machine learning.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08534" title="Abstract">arXiv:2310.08534</a> [<a href="/pdf/2310.08534" title="Download PDF">pdf</a>, <a href="/format/2310.08534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Animating Street View
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shan%2C+M">Mengyi Shan</a>, 
<a href="/search/cs?searchtype=author&query=Curless%2C+B">Brian Curless</a>, 
<a href="/search/cs?searchtype=author&query=Kemelmacher-Shlizerman%2C+I">Ira Kemelmacher-Shlizerman</a>, 
<a href="/search/cs?searchtype=author&query=Seitz%2C+S">Steve Seitz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGGRAPH Asia 2023 Conference Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">We present a system that automatically brings street view imagery to life by
populating it with naturally behaving, animated pedestrians and vehicles. Our
approach is to remove existing people and vehicles from the input image, insert
moving objects with proper scale, angle, motion, and appearance, plan paths and
traffic behavior, as well as render the scene with plausible occlusion and
shadowing effects. The system achieves these by reconstructing the still image
street scene, simulating crowd behavior, and rendering with consistent
lighting, visibility, occlusions, and shadows. We demonstrate results on a
diverse range of street scenes including regular still images and panoramas.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08535" title="Abstract">arXiv:2310.08535</a> [<a href="/pdf/2310.08535" title="Download PDF">pdf</a>, <a href="/format/2310.08535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formally Specifying the High-Level Behavior of LLM-Based Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Crouse%2C+M">Maxwell Crouse</a>, 
<a href="/search/cs?searchtype=author&query=Abdelaziz%2C+I">Ibrahim Abdelaziz</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+K">Kinjal Basu</a>, 
<a href="/search/cs?searchtype=author&query=Dan%2C+S">Soham Dan</a>, 
<a href="/search/cs?searchtype=author&query=Kumaravel%2C+S">Sadhana Kumaravel</a>, 
<a href="/search/cs?searchtype=author&query=Fokoue%2C+A">Achille Fokoue</a>, 
<a href="/search/cs?searchtype=author&query=Kapanipathi%2C+P">Pavan Kapanipathi</a>, 
<a href="/search/cs?searchtype=author&query=Lastras%2C+L">Luis Lastras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">LLM-based agents have recently emerged as promising tools for solving
challenging problems without the need for task-specific finetuned models that
can be expensive to procure. Currently, the design and implementation of such
agents is ad hoc, as the wide variety of tasks that LLM-based agents may be
applied to naturally means there can be no one-size-fits-all approach to agent
design. In this work we aim to alleviate the difficulty of designing and
implementing new agents by proposing a minimalistic, high-level generation
framework that simplifies the process of building agents. The framework we
introduce allows the user to specify desired agent behaviors in Linear Temporal
Logic (LTL). The declarative LTL specification is then used to construct a
constrained decoder that guarantees the LLM will produce an output exhibiting
the desired behavior. By designing our framework in this way, we obtain several
benefits, including the ability to enforce complex agent behavior, the ability
to formally validate prompt examples, and the ability to seamlessly incorporate
content-focused logical constraints into generation. In particular, our
declarative approach, in which the desired behavior is simply described without
concern for how it should be implemented or enforced, enables rapid design,
implementation and experimentation with different LLM-based agents. We
demonstrate how the proposed framework can be used to implement recent
LLM-based agents, and show how the guardrails our approach provides can lead to
improvements in agent performance. In addition, we release our code for general
use.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08537" title="Abstract">arXiv:2310.08537</a> [<a href="/pdf/2310.08537" title="Download PDF">pdf</a>, <a href="/format/2310.08537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XAI Benchmark for Visual Explanation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+S">Siyi Gu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">James Song</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+B">Bo Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The rise of deep learning algorithms has led to significant advancements in
computer vision tasks, but their "black box" nature has raised concerns
regarding interpretability. Explainable AI (XAI) has emerged as a critical area
of research aiming to open this "black box", and shed light on the
decision-making process of AI models. Visual explanations, as a subset of
Explainable Artificial Intelligence (XAI), provide intuitive insights into the
decision-making processes of AI models handling visual data by highlighting
influential areas in an input image. Despite extensive research conducted on
visual explanations, most evaluations are model-centered since the availability
of corresponding real-world datasets with ground truth explanations is scarce
in the context of image data. To bridge this gap, we introduce an XAI Benchmark
comprising a dataset collection from diverse topics that provide both class
labels and corresponding explanation annotations for images. We have processed
data from diverse domains to align with our unified visual explanation
framework. We introduce a comprehensive Visual Explanation pipeline, which
integrates data loading, preprocessing, experimental setup, and model
evaluation processes. This structure enables researchers to conduct fair
comparisons of various visual explanation techniques. In addition, we provide a
comprehensive review of over 10 evaluation methods for visual explanation to
assist researchers in effectively utilizing our dataset collection. To further
assess the performance of existing visual explanation methods, we conduct
experiments on selected datasets using various model-centered and ground
truth-centered evaluation metrics. We envision this benchmark could facilitate
the advancement of visual explanation models. The XAI dataset collection and
easy-to-use code for evaluation are publicly accessible at
https://xaidataset.github.io.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08538" title="Abstract">arXiv:2310.08538</a> [<a href="/pdf/2310.08538" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image2PCI -- A Multitask Learning Framework for Estimating Pavement  Condition Indices Directly from Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Owor%2C+N+J">Neema Jakisa Owor</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hang Du</a>, 
<a href="/search/cs?searchtype=author&query=Daud%2C+A">Abdulateef Daud</a>, 
<a href="/search/cs?searchtype=author&query=Aboah%2C+A">Armstrong Aboah</a>, 
<a href="/search/cs?searchtype=author&query=Adu-Gyamfi%2C+Y">Yaw Adu-Gyamfi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The Pavement Condition Index (PCI) is a widely used metric for evaluating
pavement performance based on the type, extent and severity of distresses
detected on a pavement surface. In recent times, significant progress has been
made in utilizing deep-learning approaches to automate PCI estimation process.
However, the current approaches rely on at least two separate models to
estimate PCI values -- one model dedicated to determining the type and extent
and another for estimating their severity. This approach presents several
challenges, including complexities, high computational resource demands, and
maintenance burdens that necessitate careful consideration and resolution. To
overcome these challenges, the current study develops a unified multi-tasking
model that predicts the PCI directly from a top-down pavement image. The
proposed architecture is a multi-task model composed of one encoder for feature
extraction and four decoders to handle specific tasks: two detection heads, one
segmentation head and one PCI estimation head. By multitasking, we are able to
extract features from the detection and segmentation heads for automatically
estimating the PCI directly from the images. The model performs very well on
our benchmarked and open pavement distress dataset that is annotated for
multitask learning (the first of its kind). To our best knowledge, this is the
first work that can estimate PCI directly from an image at real time speeds
while maintaining excellent accuracy on all related tasks for crack detection
and segmentation.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08540" title="Abstract">arXiv:2310.08540</a> [<a href="/pdf/2310.08540" title="Download PDF">pdf</a>, <a href="/format/2310.08540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do pretrained Transformers Really Learn In-context by Gradient Descent?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Lingfeng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+A">Aayush Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Khashabi%2C+D">Daniel Khashabi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)?
Several recent works draw analogies between the dynamics of GD and the emergent
behavior of ICL in large language models. However, these works make assumptions
far from the realistic natural language setting in which language models are
trained. Such discrepancies between theory and practice, therefore, necessitate
further investigation to validate their applicability.
<br />We start by highlighting the weaknesses in prior works that construct
Transformer weights to simulate gradient descent. Their experiments with
training Transformers on ICL objective, inconsistencies in the order
sensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity
to parameter changes are some examples of a mismatch from the real-world
setting.
<br />Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural
setting. We conduct comprehensive empirical analyses on language models
pretrained on natural data (LLaMa-7B). Our comparisons on various performance
metrics highlight the inconsistent behavior of ICL and GD as a function of
various factors such as datasets, models, and number of demonstrations. We
observe that ICL and GD adapt the output distribution of language models
differently. These results indicate that the equivalence between ICL and GD is
an open hypothesis, requires nuanced considerations and calls for further
studies.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08541" title="Abstract">arXiv:2310.08541</a> [<a href="/pdf/2310.08541" title="Download PDF">pdf</a>, <a href="/format/2310.08541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic  Image Design and Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhengyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+K">Kevin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chung-Ching Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lijuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page at <a href="https://idea2img.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We introduce ``Idea to Image,'' a system that enables multimodal iterative
self-refinement with GPT-4V(ision) for automatic image design and generation.
Humans can quickly identify the characteristics of different text-to-image
(T2I) models via iterative explorations. This enables them to efficiently
convert their high-level generation ideas into effective T2I prompts that can
produce good images. We investigate if systems based on large multimodal models
(LMMs) can develop analogous multimodal self-refinement abilities that enable
exploring unknown models or environments via self-refining tries. Idea2Img
cyclically generates revised T2I prompts to synthesize draft images, and
provides directional feedback for prompt revision, both conditioned on its
memory of the probed T2I model's characteristics. The iterative self-refinement
brings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Img
can process input ideas with interleaved image-text sequences, follow ideas
with design instructions, and generate images of better semantic and visual
qualities. The user preference study validates the efficacy of multimodal
iterative self-refinement on automatic image design and generation.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08543" title="Abstract">arXiv:2310.08543</a> [<a href="/pdf/2310.08543" title="Download PDF">pdf</a>, <a href="/format/2310.08543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NetDiffusion: Network Data Augmentation Through Protocol-Constrained  Traffic Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shinan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gember-Jacobson%2C+A">Aaron Gember-Jacobson</a>, 
<a href="/search/cs?searchtype=author&query=Bhagoji%2C+A+N">Arjun Nitin Bhagoji</a>, 
<a href="/search/cs?searchtype=author&query=Schmitt%2C+P">Paul Schmitt</a>, 
<a href="/search/cs?searchtype=author&query=Bronzino%2C+F">Francesco Bronzino</a>, 
<a href="/search/cs?searchtype=author&query=Feamster%2C+N">Nick Feamster</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Datasets of labeled network traces are essential for a multitude of machine
learning (ML) tasks in networking, yet their availability is hindered by
privacy and maintenance concerns, such as data staleness. To overcome this
limitation, synthetic network traces can often augment existing datasets.
Unfortunately, current synthetic trace generation methods, which typically
produce only aggregated flow statistics or a few selected packet attributes, do
not always suffice, especially when model training relies on having features
that are only available from packet traces. This shortfall manifests in both
insufficient statistical resemblance to real traces and suboptimal performance
on ML tasks when employed for data augmentation. In this paper, we apply
diffusion models to generate high-resolution synthetic network traffic traces.
We present NetDiffusion, a tool that uses a finely-tuned, controlled variant of
a Stable Diffusion model to generate synthetic network traffic that is high
fidelity and conforms to protocol specifications. Our evaluation demonstrates
that packet captures generated from NetDiffusion can achieve higher statistical
similarity to real data and improved ML model performance than current
state-of-the-art approaches (e.g., GAN-based approaches). Furthermore, our
synthetic traces are compatible with common network analysis tools and support
a myriad of network tasks, suggesting that NetDiffusion can serve a broader
spectrum of network analysis and testing tasks, extending beyond ML-centric
applications.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08548" title="Abstract">arXiv:2310.08548</a> [<a href="/pdf/2310.08548" title="Download PDF">pdf</a>, <a href="/ps/2310.08548" title="Download PostScript">ps</a>, <a href="/format/2310.08548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stronger Coreset Bounds for Kernel Density Estimators via Chaining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bozzai%2C+R">Rainie Bozzai</a>, 
<a href="/search/cs?searchtype=author&query=Rothvoss%2C+T">Thomas Rothvoss</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Geometry (cs.CG); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We apply the discrepancy method and a chaining approach to give improved
bounds on the coreset complexity of a wide class of kernel functions. Our
results give randomized polynomial time algorithms to produce coresets of size
$O\big(\frac{\sqrt{d}}{\varepsilon}\sqrt{\log\log \frac{1}{\varepsilon}}\big)$
for the Gaussian and Laplacian kernels in the case that the data set is
uniformly bounded, an improvement that was not possible with previous
techniques. We also obtain coresets of size
$O\big(\frac{1}{\varepsilon}\sqrt{\log\log \frac{1}{\varepsilon}}\big)$ for the
Laplacian kernel for $d$ constant. Finally, we give the best known bounds of
$O\big(\frac{\sqrt{d}}{\varepsilon}\sqrt{\log(2\max\{1,\alpha\})}\big)$ on the
coreset complexity of the exponential, Hellinger, and JS Kernels, where
$1/\alpha$ is the bandwidth parameter of the kernel.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08549" title="Abstract">arXiv:2310.08549</a> [<a href="/pdf/2310.08549" title="Download PDF">pdf</a>, <a href="/format/2310.08549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Episodic Curriculum for Transformer Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+L+X">Lucy Xiaoyang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yunfan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Grigsby%2C+J">Jake Grigsby</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L+%22">Linxi &quot;Jim&quot; Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuke Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in NeurIPS 2023; The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the
learning efficiency and generalization of Transformer agents. Central to CEC is
the placement of cross-episodic experiences into a Transformer's context, which
forms the basis of a curriculum. By sequentially structuring online learning
trials and mixed-quality demonstrations, CEC constructs curricula that
encapsulate learning progression and proficiency increase across episodes. Such
synergy combined with the potent pattern recognition capabilities of
Transformer models delivers a powerful cross-episodic attention mechanism. The
effectiveness of CEC is demonstrated under two representative scenarios: one
involving multi-task reinforcement learning with discrete control, such as in
DeepMind Lab, where the curriculum captures the learning progression in both
individual and progressively complex settings; and the other involving
imitation learning with mixed-quality data for continuous control, as seen in
RoboMimic, where the curriculum captures the improvement in demonstrators'
expertise. In all instances, policies resulting from CEC exhibit superior
performance and strong generalization. Code is open-sourced at
https://cec-agent.github.io/ to facilitate research on Transformer agent
learning.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08553" title="Abstract">arXiv:2310.08553</a> [<a href="/pdf/2310.08553" title="Download PDF">pdf</a>, <a href="/format/2310.08553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transmission line dynamics on inverter-dominated grids: analysis and  simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Colon-Reyes%2C+G+E">Gabriel E. Colon-Reyes</a>, 
<a href="/search/eess?searchtype=author&query=Kravis%2C+R">Ruth Kravis</a>, 
<a href="/search/eess?searchtype=author&query=Sharma%2C+S">Sunash Sharma</a>, 
<a href="/search/eess?searchtype=author&query=Callaway%2C+D">Duncan Callaway</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this work, we study fast time scale dynamic interactions between
inverters, synchronous machines, and transmission lines. The overlapping time
scales between inverter controls and electromagnetic phenomena idfentified in
recent years has necessitated a re-evaluation of assumptions made in power
system dynamics studies. We utilize an open-source modeling platform to perform
both small signal stability and dynamic time domain studies of networks
containing inverters, machines, and loads. We use transmission line models of
varying fidelity, including models with multiple segments and frequency
dependence. Our results indicate that, for the cases we study, line dynamics
are not important for characterizing small signal stability. However, while in
many cases the high fidelity line models are unnecessary for dynamic
simulations, there are some cases in which simpler models omit dynamics that
could be important to the operation of inner inverter control loops.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08558" title="Abstract">arXiv:2310.08558</a> [<a href="/pdf/2310.08558" title="Download PDF">pdf</a>, <a href="/format/2310.08558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate  Exploration Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mark%2C+M+S">Max Sobol Mark</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Archit Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Tajwar%2C+F">Fahim Tajwar</a>, 
<a href="/search/cs?searchtype=author&query=Rafailov%2C+R">Rafael Rafailov</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">It is desirable for policies to optimistically explore new states and
behaviors during online reinforcement learning (RL) or fine-tuning, especially
when prior offline data does not provide enough state coverage. However,
exploration bonuses can bias the learned policy, and our experiments find that
naive, yet standard use of such bonuses can fail to recover a performant
policy. Concurrently, pessimistic training in offline RL has enabled recovery
of performant policies from static datasets. Can we leverage offline RL to
recover better policies from online interaction? We make a simple observation
that a policy can be trained from scratch on all interaction data with
pessimistic objectives, thereby decoupling the policies used for data
collection and for evaluation. Specifically, we propose offline retraining, a
policy extraction step at the end of online fine-tuning in our
Offline-to-Online-to-Offline (OOO) framework for reinforcement learning (RL).
An optimistic (exploration) policy is used to interact with the environment,
and a separate pessimistic (exploitation) policy is trained on all the observed
data for evaluation. Such decoupling can reduce any bias from online
interaction (intrinsic rewards, primacy bias) in the evaluation policy, and can
allow more exploratory behaviors during online interaction which in turn can
generate better data for exploitation. OOO is complementary to several
offline-to-online RL and online RL methods, and improves their average
performance by 14% to 26% in our fine-tuning experiments, achieves
state-of-the-art performance on several environments in the D4RL benchmarks,
and improves online RL performance by 165% on two OpenAI gym environments.
Further, OOO can enable fine-tuning from incomplete offline datasets where
prior methods can fail to recover a performant policy. Implementation:
https://github.com/MaxSobolMark/OOO
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08559" title="Abstract">arXiv:2310.08559</a> [<a href="/pdf/2310.08559" title="Download PDF">pdf</a>, <a href="/format/2310.08559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of  Language Models with Hypothesis Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+L">Linlu Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Liwei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Ximing Lu</a>, 
<a href="/search/cs?searchtype=author&query=Sclar%2C+M">Melanie Sclar</a>, 
<a href="/search/cs?searchtype=author&query=Pyatkin%2C+V">Valentina Pyatkin</a>, 
<a href="/search/cs?searchtype=author&query=Bhagavatula%2C+C">Chandra Bhagavatula</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bailin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Dziri%2C+N">Nouha Dziri</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiang Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The ability to derive underlying principles from a handful of observations
and then generalize to novel situations -- known as inductive reasoning -- is
central to human intelligence. Prior work suggests that language models (LMs)
often fall short on inductive reasoning, despite achieving impressive success
on research benchmarks. In this work, we conduct a systematic study of the
inductive reasoning capabilities of LMs through iterative hypothesis
refinement, a technique that more closely mirrors the human inductive process
than standard input-output prompting. Iterative hypothesis refinement employs a
three-step process: proposing, selecting, and refining hypotheses in the form
of textual rules. By examining the intermediate rules, we observe that LMs are
phenomenal hypothesis proposers (i.e., generating candidate rules), and when
coupled with a (task-specific) symbolic interpreter that is able to
systematically filter the proposed set of rules, this hybrid approach achieves
strong results across inductive reasoning benchmarks that require inducing
causal relations, language-like instructions, and symbolic concepts. However,
they also behave as puzzling inductive reasoners, showing notable performance
gaps in rule induction (i.e., identifying plausible rules) and rule application
(i.e., applying proposed rules to instances), suggesting that LMs are proposing
hypotheses without being able to actually apply the rules. Through empirical
and human analyses, we further reveal several discrepancies between the
inductive reasoning processes of LMs and humans, shedding light on both the
potentials and limitations of using LMs in inductive reasoning tasks.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08560" title="Abstract">arXiv:2310.08560</a> [<a href="/pdf/2310.08560" title="Download PDF">pdf</a>, <a href="/format/2310.08560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MemGPT: Towards LLMs as Operating Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Packer%2C+C">Charles Packer</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+V">Vivian Fang</a>, 
<a href="/search/cs?searchtype=author&query=Patil%2C+S+G">Shishir G. Patil</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+K">Kevin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wooders%2C+S">Sarah Wooders</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+J+E">Joseph E. Gonzalez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and data available at <a href="https://memgpt.ai">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Large language models (LLMs) have revolutionized AI, but are constrained by
limited context windows, hindering their utility in tasks like extended
conversations and document analysis. To enable using context beyond limited
context windows, we propose virtual context management, a technique drawing
inspiration from hierarchical memory systems in traditional operating systems
that provide the appearance of large memory resources through data movement
between fast and slow memory. Using this technique, we introduce MemGPT
(Memory-GPT), a system that intelligently manages different memory tiers in
order to effectively provide extended context within the LLM's limited context
window, and utilizes interrupts to manage control flow between itself and the
user. We evaluate our OS-inspired design in two domains where the limited
context windows of modern LLMs severely handicaps their performance: document
analysis, where MemGPT is able to analyze large documents that far exceed the
underlying LLM's context window, and multi-session chat, where MemGPT can
create conversational agents that remember, reflect, and evolve dynamically
through long-term interactions with their users. We release MemGPT code and
data for our experiments at https://memgpt.ai.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08565" title="Abstract">arXiv:2310.08565</a> [<a href="/pdf/2310.08565" title="Download PDF">pdf</a>, <a href="/format/2310.08565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Security Considerations in AI-Robotics: A Survey of Current Methods,  Challenges, and Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neupane%2C+S">Subash Neupane</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+S">Shaswata Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+I+A">Ivan A. Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Swayamjit Saha</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+S">Sudip Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingdao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pillai%2C+N">Nisha Pillai</a>, 
<a href="/search/cs?searchtype=author&query=Rahimi%2C+S">Shahram Rahimi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Robotics and Artificial Intelligence (AI) have been inextricably intertwined
since their inception. Today, AI-Robotics systems have become an integral part
of our daily lives, from robotic vacuum cleaners to semi-autonomous cars. These
systems are built upon three fundamental architectural elements: perception,
navigation and planning, and control. However, while the integration of
AI-Robotics systems has enhanced the quality our lives, it has also presented a
serious problem - these systems are vulnerable to security attacks. The
physical components, algorithms, and data that make up AI-Robotics systems can
be exploited by malicious actors, potentially leading to dire consequences.
Motivated by the need to address the security concerns in AI-Robotics systems,
this paper presents a comprehensive survey and taxonomy across three
dimensions: attack surfaces, ethical and legal concerns, and Human-Robot
Interaction (HRI) security. Our goal is to provide users, developers and other
stakeholders with a holistic understanding of these areas to enhance the
overall AI-Robotics system security. We begin by surveying potential attack
surfaces and provide mitigating defensive strategies. We then delve into
ethical issues, such as dependency and psychological impact, as well as the
legal concerns regarding accountability for these systems. Besides, emerging
trends such as HRI are discussed, considering privacy, integrity, safety,
trustworthiness, and explainability concerns. Finally, we present our vision
for future research directions in this dynamic and promising field.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08566" title="Abstract">arXiv:2310.08566</a> [<a href="/pdf/2310.08566" title="Download PDF">pdf</a>, <a href="/format/2310.08566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers as Decision Makers: Provable In-Context Reinforcement  Learning via Supervised Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Licong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yu Bai</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+S">Song Mei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">Large transformer models pretrained on offline reinforcement learning
datasets have demonstrated remarkable in-context reinforcement learning (ICRL)
capabilities, where they can make good decisions when prompted with interaction
trajectories from unseen environments. However, when and how transformers can
be trained to perform ICRL have not been theoretically well-understood. In
particular, it is unclear which reinforcement-learning algorithms transformers
can perform in context, and how distribution mismatch in offline training data
affects the learned algorithms. This paper provides a theoretical framework
that analyzes supervised pretraining for ICRL. This includes two recently
proposed training methods -- algorithm distillation and decision-pretrained
transformers. First, assuming model realizability, we prove the
supervised-pretrained transformer will imitate the conditional expectation of
the expert algorithm given the observed trajectory. The generalization error
will scale with model capacity and a distribution divergence factor between the
expert and offline algorithms. Second, we show transformers with ReLU attention
can efficiently approximate near-optimal online reinforcement learning
algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and
UCB-VI for tabular Markov decision processes. This provides the first
quantitative analysis of the ICRL capabilities of transformers pretrained from
offline trajectories.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08568" title="Abstract">arXiv:2310.08568</a> [<a href="/pdf/2310.08568" title="Download PDF">pdf</a>, <a href="/format/2310.08568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Placement Optimization of Substitutable Products
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Housni%2C+O+E">Omar El Housni</a>, 
<a href="/search/cs?searchtype=author&query=Udwani%2C+R">Rajan Udwani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Strategic product placement can have a strong influence on customer purchase
behavior in physical stores as well as online platforms. Motivated by this, we
consider the problem of optimizing the placement of substitutable products in
designated display locations to maximize the expected revenue of the seller. We
model the customer behavior as a two-stage process: first, the customer visits
a subset of display locations according to a browsing distribution; second, the
customer chooses at most one product from the displayed products at those
locations according to a choice model. Our goal is to design a general
algorithm that can select and place the products optimally for any browsing
distribution and choice model, and we call this the Placement problem. We give
a randomized algorithm that utilizes an $\alpha$-approximate algorithm for
cardinality constrained assortment optimization and outputs a
$\frac{\Theta(\alpha)}{\log m}$-approximate solution (in expectation) for
Placement with $m$ display locations, i.e., our algorithm outputs a solution
with value at least $\frac{\Omega(\alpha)}{\log m}$ factor of the optimal and
this is tight in the worst case. We also give algorithms with stronger
guarantees in some special cases. In particular, we give a deterministic
$\frac{\Omega(1)}{\log m}$-approximation algorithm for the Markov choice model,
and a tight $(1-1/e)$-approximation algorithm for the problem when products
have identical prices.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08569" title="Abstract">arXiv:2310.08569</a> [<a href="/pdf/2310.08569" title="Download PDF">pdf</a>, <a href="/format/2310.08569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Lightweight Calibrated Simulation Enabling Efficient Offline Learning  for Optimal Control of Real Buildings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goldfeder%2C+J">Judah Goldfeder</a>, 
<a href="/search/cs?searchtype=author&query=Sipple%2C+J">John Sipple</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computational Engineering, Finance, and Science (cs.CE); Signal Processing (eess.SP)

</div>
<p class="mathjax">Modern commercial Heating, Ventilation, and Air Conditioning (HVAC) devices
form a complex and interconnected thermodynamic system with the building and
outside weather conditions, and current setpoint control policies are not fully
optimized for minimizing energy use and carbon emission. Given a suitable
training environment, a Reinforcement Learning (RL) model is able to improve
upon these policies, but training such a model, especially in a way that scales
to thousands of buildings, presents many real world challenges. We propose a
novel simulation-based approach, where a customized simulator is used to train
the agent for each building. Our open-source simulator (available online:
https://github.com/google/sbsim) is lightweight and calibrated via telemetry
from the building to reach a higher level of fidelity. On a two-story, 68,000
square foot building, with 127 devices, we were able to calibrate our simulator
to have just over half a degree of drift from the real world over a six-hour
interval. This approach is an important step toward having a real-world RL
control system that can be scaled to many buildings, allowing for greater
efficiency and resulting in reduced energy consumption and carbon emissions.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08571" title="Abstract">arXiv:2310.08571</a> [<a href="/pdf/2310.08571" title="Download PDF">pdf</a>, <a href="/format/2310.08571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dubi%C5%84ski%2C+J">Jan Dubi&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Pawlak%2C+S">Stanis&#x142;aw Pawlak</a>, 
<a href="/search/cs?searchtype=author&query=Boenisch%2C+F">Franziska Boenisch</a>, 
<a href="/search/cs?searchtype=author&query=Trzci%C5%84ski%2C+T">Tomasz Trzci&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Dziedzic%2C+A">Adam Dziedzic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Machine Learning as a Service (MLaaS) APIs provide ready-to-use and
high-utility encoders that generate vector representations for given inputs.
Since these encoders are very costly to train, they become lucrative targets
for model stealing attacks during which an adversary leverages query access to
the API to replicate the encoder locally at a fraction of the original training
costs. We propose Bucks for Buckets (B4B), the first active defense that
prevents stealing while the attack is happening without degrading
representation quality for legitimate API users. Our defense relies on the
observation that the representations returned to adversaries who try to steal
the encoder's functionality cover a significantly larger fraction of the
embedding space than representations of legitimate users who utilize the
encoder to solve a particular downstream task.vB4B leverages this to adaptively
adjust the utility of the returned representations according to a user's
coverage of the embedding space. To prevent adaptive adversaries from eluding
our defense by simply creating multiple user accounts (sybils), B4B also
individually transforms each user's representations. This prevents the
adversary from directly aggregating representations over multiple accounts to
create their stolen encoder copy. Our active defense opens a new path towards
securely sharing and democratizing encoders over public APIs.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08573" title="Abstract">arXiv:2310.08573</a> [<a href="/pdf/2310.08573" title="Download PDF">pdf</a>, <a href="/format/2310.08573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PolyTask: Learning Unified Policies through Behavior Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haldar%2C+S">Siddhant Haldar</a>, 
<a href="/search/cs?searchtype=author&query=Pinto%2C+L">Lerrel Pinto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Unified models capable of solving a wide variety of tasks have gained
traction in vision and NLP due to their ability to share regularities and
structures across tasks, which improves individual task performance and reduces
computational footprint. However, the impact of such models remains limited in
embodied learning problems, which present unique challenges due to
interactivity, sample inefficiency, and sequential task presentation. In this
work, we present PolyTask, a novel method for learning a single unified model
that can solve various embodied tasks through a 'learn then distill' mechanism.
In the 'learn' step, PolyTask leverages a few demonstrations for each task to
train task-specific policies. Then, in the 'distill' step, task-specific
policies are distilled into a single policy using a new distillation method
called Behavior Distillation. Given a unified policy, individual task behavior
can be extracted through conditioning variables. PolyTask is designed to be
conceptually simple while being able to leverage well-established algorithms in
RL to enable interactivity, a handful of expert demonstrations to allow for
sample efficiency, and preventing interactive access to tasks during
distillation to enable lifelong learning. Experiments across three simulated
environment suites and a real-robot suite show that PolyTask outperforms prior
state-of-the-art approaches in multi-task and lifelong learning settings by
significant margins.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08574" title="Abstract">arXiv:2310.08574</a> [<a href="/pdf/2310.08574" title="Download PDF">pdf</a>, <a href="/format/2310.08574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jigsaw: Supporting Designers in Prototyping Multimodal Applications by  Assembling AI Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+D+C">David Chuan-En Lin</a>, 
<a href="/search/cs?searchtype=author&query=Martelaro%2C+N">Nikolas Martelaro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Webpage: <a href="https://preview.jigsaw.to">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advancements in AI foundation models have made it possible for them to
be utilized off-the-shelf for creative tasks, including ideating design
concepts or generating visual prototypes. However, integrating these models
into the creative process can be challenging as they often exist as standalone
applications tailored to specific tasks. To address this challenge, we
introduce Jigsaw, a prototype system that employs puzzle pieces as metaphors to
represent foundation models. Jigsaw allows designers to combine different
foundation model capabilities across various modalities by assembling
compatible puzzle pieces. To inform the design of Jigsaw, we interviewed ten
designers and distilled design goals. In a user study, we showed that Jigsaw
enhanced designers' understanding of available foundation model capabilities,
provided guidance on combining capabilities across different modalities and
tasks, and served as a canvas to support design exploration, prototyping, and
documentation.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08576" title="Abstract">arXiv:2310.08576</a> [<a href="/pdf/2310.08576" title="Download PDF">pdf</a>, <a href="/format/2310.08576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Act from Actionless Videos through Dense Correspondences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+P">Po-Chen Ko</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Jiayuan Mao</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yilun Du</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shao-Hua Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tenenbaum%2C+J+B">Joshua B. Tenenbaum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://flow-diffusion.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this work, we present an approach to construct a video-based robot policy
capable of reliably executing diverse tasks across different robots and
environments from few video demonstrations without using any action
annotations. Our method leverages images as a task-agnostic representation,
encoding both the state and action information, and text as a general
representation for specifying robot goals. By synthesizing videos that
``hallucinate'' robot executing actions and in combination with dense
correspondences between frames, our approach can infer the closed-formed action
to execute to an environment without the need of any explicit action labels.
This unique capability allows us to train the policy solely based on RGB videos
and deploy learned policies to various robotic tasks. We demonstrate the
efficacy of our approach in learning policies on table-top manipulation and
navigation tasks. Additionally, we contribute an open-source framework for
efficient video modeling, enabling the training of high-fidelity policy models
with four GPUs within a single day.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08577" title="Abstract">arXiv:2310.08577</a> [<a href="/pdf/2310.08577" title="Download PDF">pdf</a>, <a href="/format/2310.08577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Data-Type Understanding does not emerge from Scaling  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Udandarao%2C+V">Vishaal Udandarao</a>, 
<a href="/search/cs?searchtype=author&query=Burg%2C+M+F">Max F. Burg</a>, 
<a href="/search/cs?searchtype=author&query=Albanie%2C+S">Samuel Albanie</a>, 
<a href="/search/cs?searchtype=author&query=Bethge%2C+M">Matthias Bethge</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08579" title="Abstract">arXiv:2310.08579</a> [<a href="/pdf/2310.08579" title="Download PDF">pdf</a>, <a href="/format/2310.08579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperHuman: Hyper-Realistic Human Generation with Latent Structural  Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jian Ren</a>, 
<a href="/search/cs?searchtype=author&query=Siarohin%2C+A">Aliaksandr Siarohin</a>, 
<a href="/search/cs?searchtype=author&query=Skorokhodov%2C+I">Ivan Skorokhodov</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xihui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tulyakov%2C+S">Sergey Tulyakov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://snap-research.github.io/HyperHuman/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Despite significant advances in large-scale text-to-image models, achieving
hyper-realistic human image generation remains a desirable yet unsolved task.
Existing models like Stable Diffusion and DALL-E 2 tend to generate human
images with incoherent parts or unnatural poses. To tackle these challenges,
our key insight is that human image is inherently structural over multiple
granularities, from the coarse-level body skeleton to fine-grained spatial
geometry. Therefore, capturing such correlations between the explicit
appearance and latent structure in one model is essential to generate coherent
and natural human images. To this end, we propose a unified framework,
HyperHuman, that generates in-the-wild human images of high realism and diverse
layouts. Specifically, 1) we first build a large-scale human-centric dataset,
named HumanVerse, which consists of 340M images with comprehensive annotations
like human pose, depth, and surface normal. 2) Next, we propose a Latent
Structural Diffusion Model that simultaneously denoises the depth and surface
normal along with the synthesized RGB image. Our model enforces the joint
learning of image appearance, spatial relationship, and geometry in a unified
network, where each branch in the model complements to each other with both
structural awareness and textural richness. 3) Finally, to further boost the
visual quality, we propose a Structure-Guided Refiner to compose the predicted
conditions for more detailed generation of higher resolution. Extensive
experiments demonstrate that our framework yields the state-of-the-art
performance, generating hyper-realistic human images under diverse scenarios.
Project Page: https://snap-research.github.io/HyperHuman/
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08580" title="Abstract">arXiv:2310.08580</a> [<a href="/pdf/2310.08580" title="Download PDF">pdf</a>, <a href="/format/2310.08580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OmniControl: Control Any Joint at Any Time for Human Motion Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yiming Xie</a>, 
<a href="/search/cs?searchtype=author&query=Jampani%2C+V">Varun Jampani</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+L">Lei Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+D">Deqing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Huaizu Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://neu-vi.github.io/omnicontrol/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">We present a novel approach named OmniControl for incorporating flexible
spatial control signals into a text-conditioned human motion generation model
based on the diffusion process. Unlike previous methods that can only control
the pelvis trajectory, OmniControl can incorporate flexible spatial control
signals over different joints at different times with only one model.
Specifically, we propose analytic spatial guidance that ensures the generated
motion can tightly conform to the input control signals. At the same time,
realism guidance is introduced to refine all the joints to generate more
coherent motion. Both the spatial and realism guidance are essential and they
are highly complementary for balancing control accuracy and motion realism. By
combining them, OmniControl generates motions that are realistic, coherent, and
consistent with the spatial constraints. Experiments on HumanML3D and KIT-ML
datasets show that OmniControl not only achieves significant improvement over
state-of-the-art methods on pelvis control but also shows promising results
when incorporating the constraints over other joints.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08581" title="Abstract">arXiv:2310.08581</a> [<a href="/pdf/2310.08581" title="Download PDF">pdf</a>, <a href="/format/2310.08581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Visual Decomposer: Long-Horizon Manipulation Made Easy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zichen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunshuang Li</a>, 
<a href="/search/cs?searchtype=author&query=Bastani%2C+O">Osbert Bastani</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Abhishek Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Jayaraman%2C+D">Dinesh Jayaraman</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y+J">Yecheng Jason Ma</a>, 
<a href="/search/cs?searchtype=author&query=Weihs%2C+L">Luca Weihs</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Real-world robotic tasks stretch over extended horizons and encompass
multiple stages. Learning long-horizon manipulation tasks, however, is a
long-standing challenge, and demands decomposing the overarching task into
several manageable subtasks to facilitate policy learning and generalization to
unseen tasks. Prior task decomposition methods require task-specific knowledge,
are computationally intensive, and cannot readily be applied to new tasks. To
address these shortcomings, we propose Universal Visual Decomposer (UVD), an
off-the-shelf task decomposition method for visual long horizon manipulation
using pre-trained visual representations designed for robotic control. At a
high level, UVD discovers subgoals by detecting phase shifts in the embedding
space of the pre-trained representation. Operating purely on visual
demonstrations without auxiliary information, UVD can effectively extract
visual subgoals embedded in the videos, while incurring zero additional
training cost on top of standard visuomotor policy training. Goal-conditioned
policies learned with UVD-discovered subgoals exhibit significantly improved
compositional generalization at test time to unseen tasks. Furthermore,
UVD-discovered subgoals can be used to construct goal-based reward shaping that
jump-starts temporally extended exploration for reinforcement learning. We
extensively evaluate UVD on both simulation and real-world tasks, and in all
cases, UVD substantially outperforms baselines across imitation and
reinforcement learning settings on in-domain and out-of-domain task sequences
alike, validating the clear advantage of automated visual task decomposition
within the simple, compact UVD framework.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08582" title="Abstract">arXiv:2310.08582</a> [<a href="/pdf/2310.08582" title="Download PDF">pdf</a>, <a href="/format/2310.08582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tree-Planner: Efficient Close-loop Task Planning with Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+M">Mengkang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+Y">Yao Mu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xinmiao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Mingyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shiguang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wenqi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qiguang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">This paper studies close-loop task planning, which refers to the process of
generating a sequence of skills (a plan) to accomplish a specific goal while
adapting the plan based on real-time observations. Recently, prompting Large
Language Models (LLMs) to generate actions iteratively has become a prevalent
paradigm due to its superior performance and user-friendliness. However, this
paradigm is plagued by two inefficiencies: high token consumption and redundant
error correction, both of which hinder its scalability for large-scale testing
and applications. To address these issues, we propose Tree-Planner, which
reframes task planning with LLMs into three distinct phases: plan sampling,
action tree construction, and grounded deciding. Tree-Planner starts by using
an LLM to sample a set of potential plans before execution, followed by the
aggregation of them to form an action tree. Finally, the LLM performs a
top-down decision-making process on the tree, taking into account real-time
environmental information. Experiments show that Tree-Planner achieves
state-of-the-art performance while maintaining high efficiency. By decomposing
LLM queries into a single plan-sampling call and multiple grounded-deciding
calls, a considerable part of the prompt are less likely to be repeatedly
consumed. As a result, token consumption is reduced by 92.2% compared to the
previously best-performing model. Additionally, by enabling backtracking on the
action tree as needed, the correction process becomes more flexible, leading to
a 40.5% decrease in error corrections. Project page:
https://tree-planner.github.io/
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08583" title="Abstract">arXiv:2310.08583</a> [<a href="/pdf/2310.08583" title="Download PDF">pdf</a>, <a href="/format/2310.08583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Fatigued Movements for Virtual Character Animation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheema%2C+N">Noshaba Cheema</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Rui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N+H">Nam Hee Kim</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A4m%C3%A4l%C3%A4inen%2C+P">Perttu H&#xe4;m&#xe4;l&#xe4;inen</a>, 
<a href="/search/cs?searchtype=author&query=Golyanik%2C+V">Vladislav Golyanik</a>, 
<a href="/search/cs?searchtype=author&query=Habermann%2C+M">Marc Habermann</a>, 
<a href="/search/cs?searchtype=author&query=Theobalt%2C+C">Christian Theobalt</a>, 
<a href="/search/cs?searchtype=author&query=Slusallek%2C+P">Philipp Slusallek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 22 figures. To be published in ACM SIGGRAPH Asia Conference Papers 2023. ACM ISBN 979-8-4007-0315-7/23/12
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ACM SIGGRAPH Asia Conference Papers 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Virtual character animation and movement synthesis have advanced rapidly
during recent years, especially through a combination of extensive motion
capture datasets and machine learning. A remaining challenge is interactively
simulating characters that fatigue when performing extended motions, which is
indispensable for the realism of generated animations. However, capturing such
movements is problematic, as performing movements like backflips with fatigued
variations up to exhaustion raises capture cost and risk of injury.
Surprisingly, little research has been done on faithful fatigue modeling. To
address this, we propose a deep reinforcement learning-based approach, which --
for the first time in literature -- generates control policies for full-body
physically simulated agents aware of cumulative fatigue. For this, we first
leverage Generative Adversarial Imitation Learning (GAIL) to learn an expert
policy for the skill; Second, we learn a fatigue policy by limiting the
generated constant torque bounds based on endurance time to non-linear, state-
and time-dependent limits in the joint-actuation space using a
Three-Compartment Controller (3CC) model. Our results demonstrate that agents
can adapt to different fatigue and rest rates interactively, and discover
realistic recovery strategies without the need for any captured data of
fatigued movement.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08584" title="Abstract">arXiv:2310.08584</a> [<a href="/pdf/2310.08584" title="Download PDF">pdf</a>, <a href="/format/2310.08584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is ImageNet worth 1 video? Learning strong image encoders from 1 long  unlabelled video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Venkataramanan%2C+S">Shashanka Venkataramanan</a>, 
<a href="/search/cs?searchtype=author&query=Rizve%2C+M+N">Mamshad Nayeem Rizve</a>, 
<a href="/search/cs?searchtype=author&query=Carreira%2C+J">Jo&#xe3;o Carreira</a>, 
<a href="/search/cs?searchtype=author&query=Asano%2C+Y+M">Yuki M. Asano</a>, 
<a href="/search/cs?searchtype=author&query=Avrithis%2C+Y">Yannis Avrithis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Self-supervised learning has unlocked the potential of scaling up pretraining
to billions of images, since annotation is unnecessary. But are we making the
best use of data? How more economical can we be? In this work, we attempt to
answer this question by making two contributions. First, we investigate
first-person videos and introduce a "Walking Tours" dataset. These videos are
high-resolution, hours-long, captured in a single uninterrupted take, depicting
a large number of objects and actions with natural scene transitions. They are
unlabeled and uncurated, thus realistic for self-supervision and comparable
with human learning.
<br />Second, we introduce a novel self-supervised image pretraining method
tailored for learning from continuous videos. Existing methods typically adapt
image-based pretraining approaches to incorporate more frames. Instead, we
advocate a "tracking to learn to recognize" approach. Our method called DoRA,
leads to attention maps that Discover and tRAck objects over time in an
end-to-end manner, using transformer cross-attention. We derive multiple views
from the tracks and use them in a classical self-supervised distillation loss.
Using our novel approach, a single Walking Tours video remarkably becomes a
strong competitor to ImageNet for several image and video downstream tasks.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08585" title="Abstract">arXiv:2310.08585</a> [<a href="/pdf/2310.08585" title="Download PDF">pdf</a>, <a href="/format/2310.08585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Im4D: High-Fidelity and Real-Time Novel View Synthesis for Dynamic  Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haotong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">Sida Peng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tao Xie</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xingyi He</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+H">Hujun Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaowei Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGGRAPH Asia 2023; Project page: <a href="https://zju3dv.github.io/im4d">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper aims to tackle the challenge of dynamic view synthesis from
multi-view videos. The key observation is that while previous grid-based
methods offer consistent rendering, they fall short in capturing appearance
details of a complex dynamic scene, a domain where multi-view image-based
rendering methods demonstrate the opposite properties. To combine the best of
two worlds, we introduce Im4D, a hybrid scene representation that consists of a
grid-based geometry representation and a multi-view image-based appearance
representation. Specifically, the dynamic geometry is encoded as a 4D density
function composed of spatiotemporal feature planes and a small MLP network,
which globally models the scene structure and facilitates the rendering
consistency. We represent the scene appearance by the original multi-view
videos and a network that learns to predict the color of a 3D point from image
features, instead of memorizing detailed appearance totally with networks,
thereby naturally making the learning of networks easier. Our method is
evaluated on five dynamic view synthesis datasets including DyNeRF, ZJU-MoCap,
NHR, DNA-Rendering and ENeRF-Outdoor datasets. The results show that Im4D
exhibits state-of-the-art performance in rendering quality and can be trained
efficiently, while realizing real-time rendering with a speed of 79.8 FPS for
512x512 images, on a single RTX 3090 GPU.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08586" title="Abstract">arXiv:2310.08586</a> [<a href="/pdf/2310.08586" title="Download PDF">pdf</a>, <a href="/format/2310.08586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PonderV2: Pave the Way for 3D Foundataion Model with A Universal  Pre-training Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Haoyi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Honghui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaoyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Di Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sha Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xianglong He</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tong He</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hengshuang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chunhua Shen</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2301.00157">arXiv:2301.00157</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In contrast to numerous NLP and 2D computer vision foundational models, the
learning of a robust and highly generalized 3D foundational model poses
considerably greater challenges. This is primarily due to the inherent data
variability and the diversity of downstream tasks. In this paper, we introduce
a comprehensive 3D pre-training framework designed to facilitate the
acquisition of efficient 3D representations, thereby establishing a pathway to
3D foundational models. Motivated by the fact that informative 3D features
should be able to encode rich geometry and appearance cues that can be utilized
to render realistic images, we propose a novel universal paradigm to learn
point cloud representations by differentiable neural rendering, serving as a
bridge between 3D and 2D worlds. We train a point cloud encoder within a
devised volumetric neural renderer by comparing the rendered images with the
real images. Notably, our approach demonstrates the seamless integration of the
learned 3D encoder into diverse downstream tasks. These tasks encompass not
only high-level challenges such as 3D detection and segmentation but also
low-level objectives like 3D reconstruction and image synthesis, spanning both
indoor and outdoor scenarios. Besides, we also illustrate the capability of
pre-training a 2D backbone using the proposed universal methodology, surpassing
conventional pre-training methods by a large margin. For the first time,
\sexyname achieves state-of-the-art performance on 11 indoor and outdoor
benchmarks. The consistent improvements in various settings imply the
effectiveness of the proposed method. Code and models will be made available at
https://github.com/Pointcept/Pointcept.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08587" title="Abstract">arXiv:2310.08587</a> [<a href="/pdf/2310.08587" title="Download PDF">pdf</a>, <a href="/format/2310.08587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Generalized Dynamic Novel View Synthesis from Monocular Videos  Possible Today?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiaoming Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Colburn%2C+A">Alex Colburn</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fangchang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Bautista%2C+M+A">Miguel Angel Bautista</a>, 
<a href="/search/cs?searchtype=author&query=Susskind%2C+J+M">Joshua M. Susskind</a>, 
<a href="/search/cs?searchtype=author&query=Schwing%2C+A+G">Alexander G. Schwing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://xiaoming-zhao.github.io/projects/pgdvs">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Rendering scenes observed in a monocular video from novel viewpoints is a
challenging problem. For static scenes the community has studied both
scene-specific optimization techniques, which optimize on every test scene, and
generalized techniques, which only run a deep net forward pass on a test scene.
In contrast, for dynamic scenes, scene-specific optimization techniques exist,
but, to our best knowledge, there is currently no generalized method for
dynamic novel view synthesis from a given monocular video. To answer whether
generalized dynamic novel view synthesis from monocular videos is possible
today, we establish an analysis framework based on existing techniques and work
toward the generalized approach. We find a pseudo-generalized process without
scene-specific appearance optimization is possible, but geometrically and
temporally consistent depth estimates are needed. Despite no scene-specific
appearance optimization, the pseudo-generalized approach improves upon some
scene-specific methods.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08588" title="Abstract">arXiv:2310.08588</a> [<a href="/pdf/2310.08588" title="Download PDF">pdf</a>, <a href="/format/2310.08588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Octopus: Embodied Vision-Language Programmer from Environmental Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingkang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuhao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chencheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+H">Haoran Tan</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jiamu Kang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuanhan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kaiyang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://choiszt.github.io/Octopus/">this https URL</a>, Codebase: <a href="https://github.com/dongyh20/Octopus">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Large vision-language models (VLMs) have achieved substantial progress in
multimodal perception and reasoning. Furthermore, when seamlessly integrated
into an embodied agent, it signifies a crucial stride towards the creation of
autonomous and context-aware systems capable of formulating plans and executing
commands with precision. In this paper, we introduce Octopus, a novel VLM
designed to proficiently decipher an agent's vision and textual task objectives
and to formulate intricate action sequences and generate executable code. Our
design allows the agent to adeptly handle a wide spectrum of tasks, ranging
from mundane daily chores in simulators to sophisticated interactions in
complex video games. Octopus is trained by leveraging GPT-4 to control an
explorative agent to generate training data, i.e., action blueprints and the
corresponding executable code, within our experimental environment called
OctoVerse. We also collect the feedback that allows the enhanced training
scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a
series of experiments, we illuminate Octopus's functionality and present
compelling results, and the proposed RLEF turns out to refine the agent's
decision-making. By open-sourcing our model architecture, simulator, and
dataset, we aspire to ignite further innovation and foster collaborative
applications within the broader embodied AI community.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Fri, 13 Oct 23</h3>
<dl>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13542" title="Abstract">arXiv:2301.13542</a> (cross-list from math.OC) [<a href="/pdf/2301.13542" title="Download PDF">pdf</a>, <a href="/ps/2301.13542" title="Download PostScript">ps</a>, <a href="/format/2301.13542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theoretical aspects in penalty hyperparameters optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Esposito%2C+F">Flavia Esposito</a>, 
<a href="/search/math?searchtype=author&query=Selicato%2C+L">Laura Selicato</a>, 
<a href="/search/math?searchtype=author&query=Sportelli%2C+C">Caterina Sportelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Annals of Mathematics and Artificial Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Learning processes are useful methodologies able to improve knowledge of real
phenomena. These are often dependent on hyperparameters, variables set before
the training process and regulating the learning procedure. Hyperparameters
optimization problem is an open issue in learning approaches since it can
strongly affect any real data analysis. They are usually selected using
Grid-Search or Cross Validation techniques. No automatic tuning procedure
exists especially if we focus on an unsupervised learning scenario. This study
aims to assess some theoretical considerations for tuning penalty
hyperparameters in optimization problems. It considers a bi-level formulation
tuning problem in an unsupervised context, by using Gradient-based methods.
Suitable conditions for the existence of a minimizer in an infinite-dimensional
Hilbert space are outlined, together with some theoretical results, applicable
in all those situations when it is unnecessary or not possible obtaining an
exact minimizer. An iterative algorithmic strategy is considered, equipped with
a stopping criterion via Ekeland's variational principle.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12852" title="Abstract">arXiv:2309.12852</a> (cross-list from math.OC) [<a href="/pdf/2309.12852" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensemble Differential Evolution with Simulation-Based Hybridization and  Self-Adaptation for Inventory Management Under Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Maitra%2C+S">Sarit Maitra</a>, 
<a href="/search/math?searchtype=author&query=Mishra%2C+V">Vivek Mishra</a>, 
<a href="/search/math?searchtype=author&query=Kundu%2C+S">Sukanya Kundu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 6 figures, AsiaSIM 2023 (Springer)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Mathematical Software (cs.MS); Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
<p class="mathjax">This study proposes an Ensemble Differential Evolution with Simula-tion-Based
Hybridization and Self-Adaptation (EDESH-SA) approach for inven-tory management
(IM) under uncertainty. In this study, DE with multiple runs is combined with a
simulation-based hybridization method that includes a self-adaptive mechanism
that dynamically alters mutation and crossover rates based on the success or
failure of each iteration. Due to its adaptability, the algorithm is able to
handle the complexity and uncertainty present in IM. Utilizing Monte Carlo
Simulation (MCS), the continuous review (CR) inventory strategy is ex-amined
while accounting for stochasticity and various demand scenarios. This
simulation-based approach enables a realistic assessment of the proposed
algo-rithm's applicability in resolving the challenges faced by IM in practical
settings. The empirical findings demonstrate the potential of the proposed
method to im-prove the financial performance of IM and optimize large search
spaces. The study makes use of performance testing with the Ackley function and
Sensitivity Analysis with Perturbations to investigate how changes in variables
affect the objective value. This analysis provides valuable insights into the
behavior and robustness of the algorithm.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06960" title="Abstract">arXiv:2310.06960</a> (cross-list from cond-mat.stat-mech) [<a href="/pdf/2310.06960" title="Download PDF">pdf</a>, <a href="/format/2310.06960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jaynes Machine: The universal microstructure of deep neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Venkatasubramanian%2C+V">Venkat Venkatasubramanian</a>, 
<a href="/search/cond-mat?searchtype=author&query=Sanjeevrajan%2C+N">N. Sanjeevrajan</a>, 
<a href="/search/cond-mat?searchtype=author&query=Khandekar%2C+M">Manasi Khandekar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Mechanics (cond-mat.stat-mech)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">We present a novel theory of the microstructure of deep neural networks.
Using a theoretical framework called statistical teleodynamics, which is a
conceptual synthesis of statistical thermodynamics and potential game theory,
we predict that all highly connected layers of deep neural networks have a
universal microstructure of connection strengths that is distributed
lognormally ($LN({\mu}, {\sigma})$). Furthermore, under ideal conditions, the
theory predicts that ${\mu}$ and ${\sigma}$ are the same for all layers in all
networks. This is shown to be the result of an arbitrage equilibrium where all
connections compete and contribute the same effective utility towards the
minimization of the overall loss function. These surprising predictions are
shown to be supported by empirical data from six large-scale deep neural
networks in real life. We also discuss how these results can be exploited to
reduce the amount of data, time, and computational resources needed to train
large deep neural networks.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07740" title="Abstract">arXiv:2310.07740</a> (cross-list from astro-ph.IM) [<a href="/pdf/2310.07740" title="Download PDF">pdf</a>, <a href="/format/2310.07740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spiral-Elliptical automated galaxy morphology classification from  telescope images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Baumstark%2C+M+J">Matthew J. Baumstark</a>, 
<a href="/search/astro-ph?searchtype=author&query=Vinci%2C+G">Giuseppe Vinci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Astrophysics of Galaxies (astro-ph.GA); Machine Learning (cs.LG); Applications (stat.AP)

</div>
<p class="mathjax">The classification of galaxy morphologies is an important step in the
investigation of theories of hierarchical structure formation. While human
expert visual classification remains quite effective and accurate, it cannot
keep up with the massive influx of data from emerging sky surveys. A variety of
approaches have been proposed to classify large numbers of galaxies; these
approaches include crowdsourced visual classification, and automated and
computational methods, such as machine learning methods based on designed
morphology statistics and deep learning. In this work, we develop two novel
galaxy morphology statistics, descent average and descent variance, which can
be efficiently extracted from telescope galaxy images. We further propose
simplified versions of the existing image statistics concentration, asymmetry,
and clumpiness, which have been widely used in the literature of galaxy
morphologies. We utilize the galaxy image data from the Sloan Digital Sky
Survey to demonstrate the effective performance of our proposed image
statistics at accurately detecting spiral and elliptical galaxies when used as
features of a random forest classifier.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07852" title="Abstract">arXiv:2310.07852</a> (cross-list from stat.ML) [<a href="/pdf/2310.07852" title="Download PDF">pdf</a>, <a href="/format/2310.07852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Computational Complexity of Private High-dimensional Model  Selection via the Exponential Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Roy%2C+S">Saptarshi Roy</a>, 
<a href="/search/stat?searchtype=author&query=Tewari%2C+A">Ambuj Tewari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO); Methodology (stat.ME)

</div>
<p class="mathjax">We consider the problem of model selection in a high-dimensional sparse
linear regression model under the differential privacy framework. In
particular, we consider the problem of differentially private best subset
selection and study its utility guarantee. We adopt the well-known exponential
mechanism for selecting the best model, and under a certain margin condition,
we establish its strong model recovery property. However, the exponential
search space of the exponential mechanism poses a serious computational
bottleneck. To overcome this challenge, we propose a Metropolis-Hastings
algorithm for the sampling step and establish its polynomial mixing time to its
stationary distribution in the problem parameters $n,p$, and $s$. Furthermore,
we also establish approximate differential privacy for the final estimates of
the Metropolis-Hastings random walk using its mixing property. Finally, we also
perform some illustrative simulations that echo the theoretical findings of our
main results.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07858" title="Abstract">arXiv:2310.07858</a> (cross-list from quant-ph) [<a href="/pdf/2310.07858" title="Download PDF">pdf</a>, <a href="/format/2310.07858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QArchSearch: A Scalable Quantum Architecture Search Package
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Kulshrestha%2C+A">Ankit Kulshrestha</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lykov%2C+D">Danylo Lykov</a>, 
<a href="/search/quant-ph?searchtype=author&query=Safro%2C+I">Ilya Safro</a>, 
<a href="/search/quant-ph?searchtype=author&query=Alexeev%2C+Y">Yuri Alexeev</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Published in Workshops of The International Conference on High
  Performance Computing, Network, Storage, and Analysis, SC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The current era of quantum computing has yielded several algorithms that
promise high computational efficiency. While the algorithms are sound in theory
and can provide potentially exponential speedup, there is little guidance on
how to design proper quantum circuits to realize the appropriate unitary
transformation to be applied to the input quantum state. In this paper, we
present \texttt{QArchSearch}, an AI based quantum architecture search package
with the \texttt{QTensor} library as a backend that provides a principled and
automated approach to finding the best model given a task and input quantum
state. We show that the search package is able to efficiently scale the search
to large quantum circuits and enables the exploration of more complex models
for different quantum applications. \texttt{QArchSearch} runs at scale and high
efficiency on high-performance computing systems using a two-level
parallelization scheme on both CPUs and GPUs, which has been demonstrated on
the Polaris supercomputer.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07861" title="Abstract">arXiv:2310.07861</a> (cross-list from math.AP) [<a href="/pdf/2310.07861" title="Download PDF">pdf</a>, <a href="/format/2310.07861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-isothermal nonlocal phase-field models with a double-obstacle  potential
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Burkovska%2C+O">Olena Burkovska</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Phase-field models are a popular choice in computational physics to describe
complex dynamics of substances with multiple phases and are widely used in
various applications. We present nonlocal non-isothermal phase-field models of
Cahn-Hilliard and Allen-Cahn types involving a nonsmooth double-well obstacle
potential. Mathematically, in a weak form, the model translates to a system of
variational inequalities coupled to a temperature evolution equation. We
demonstrate that under certain conditions and with a careful choice of the
nonlocal operator one can obtain a model that allows for sharp interfaces in
the solution that evolve in time, which is a desirable property in many
applications. This can be contrasted to the diffuse-interface local models that
can not resolve sharp interfaces. We present the well-posedness analysis of the
models, discuss an appropriate numerical discretization scheme, and supplement
our findings with several numerical experiments.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07865" title="Abstract">arXiv:2310.07865</a> (cross-list from math.OC) [<a href="/pdf/2310.07865" title="Download PDF">pdf</a>, <a href="/ps/2310.07865" title="Download PostScript">ps</a>, <a href="/format/2310.07865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Specter (and Spectra) of Miner Extractable Value
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Angeris%2C+G">Guillermo Angeris</a>, 
<a href="/search/math?searchtype=author&query=Chitra%2C+T">Tarun Chitra</a>, 
<a href="/search/math?searchtype=author&query=Diamonds%2C+T">Theo Diamonds</a>, 
<a href="/search/math?searchtype=author&query=Kulkarni%2C+K">Kshitij Kulkarni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computer Science and Game Theory (cs.GT); Combinatorics (math.CO); Computational Finance (q-fin.CP)

</div>
<p class="mathjax">Miner extractable value (MEV) refers to any excess value that a transaction
validator can realize by manipulating the ordering of transactions. In this
work, we introduce a simple theoretical definition of the 'cost of MEV', prove
some basic properties, and show that the definition is useful via a number of
examples. In a variety of settings, this definition is related to the
'smoothness' of a function over the symmetric group. From this definition and
some basic observations, we recover a number of results from the literature.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07867" title="Abstract">arXiv:2310.07867</a> (cross-list from econ.TH) [<a href="/pdf/2310.07867" title="Download PDF">pdf</a>, <a href="/format/2310.07867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cheap Talking Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Condorelli%2C+D">Daniele Condorelli</a>, 
<a href="/search/econ?searchtype=author&query=Furlan%2C+M">Massimiliano Furlan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We simulate behaviour of independent reinforcement learning algorithms
playing the Crawford and Sobel (1982) game of strategic information
transmission. We show that a sender and a receiver training together converge
to strategies close to the exante optimal equilibrium of the game. Hence,
communication takes place to the largest extent predicted by Nash equilibrium
given the degree of conflict of interest between agents. The conclusion is
shown to be robust to alternative specifications of the hyperparameters and of
the game. We discuss implications for theories of equilibrium selection in
information transmission games, for work on emerging communication among
algorithms in computer science and for the economics of collusions in markets
populated by artificially intelligent agents.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07870" title="Abstract">arXiv:2310.07870</a> (cross-list from math.OC) [<a href="/pdf/2310.07870" title="Download PDF">pdf</a>, <a href="/format/2310.07870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical planning-scheduling-control -- Optimality surrogates and  derivative-free optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=van+de+Berg%2C+D">Damien van de Berg</a>, 
<a href="/search/math?searchtype=author&query=Shah%2C+N">Nilay Shah</a>, 
<a href="/search/math?searchtype=author&query=del+Rio-Chanona%2C+E+A">Ehecatl Antonio del Rio-Chanona</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Planning, scheduling, and control typically constitute separate
decision-making units within chemical companies. Traditionally, their
integration is modelled sequentially, but recent efforts prioritize lower-level
feasibility and optimality, leading to large-scale, potentially multi-level,
hierarchical formulations. Data-driven techniques, like optimality surrogates
or derivative-free optimization, become essential in addressing ensuing
tractability challenges. We demonstrate a step-by-step workflow to find a
tractable solution to a tri-level formulation of a multi-site, multi-product
planning-scheduling-control case study. We discuss solution
tractability-accuracy trade-offs and scaling properties for both methods.
Despite individual improvements over conventional heuristics, both approaches
present drawbacks. Consequently, we synthesize our findings into a methodology
combining their strengths. Our approach remains agnostic to the level-specific
formulations when the linking variables are identified and retains the
heuristic sequential solution as fallback option. We advance the field by
leveraging parallelization, hyperparameter tuning, and a combination of off-
and on-line computation, to find tractable solutions to more accurate
multi-level formulations.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07887" title="Abstract">arXiv:2310.07887</a> (cross-list from eess.IV) [<a href="/pdf/2310.07887" title="Download PDF">pdf</a>, <a href="/format/2310.07887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Structured Noise Removal with Variational Lossy Autoencoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Salmon%2C+B">Benjamin Salmon</a>, 
<a href="/search/eess?searchtype=author&query=Krull%2C+A">Alexander Krull</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Most unsupervised denoising methods are based on the assumption that imaging
noise is either pixel-independent, i.e., spatially uncorrelated, or
signal-independent, i.e., purely additive. However, in practice many imaging
setups, especially in microscopy, suffer from a combination of signal-dependent
noise (e.g. Poisson shot noise) and axis-aligned correlated noise (e.g. stripe
shaped scanning or readout artifacts). In this paper, we present the first
unsupervised deep learning-based denoiser that can remove this type of noise
without access to any clean images or a noise model. Unlike self-supervised
techniques, our method does not rely on removing pixels by masking or
subsampling so can utilize all available information. We implement a
Variational Autoencoder (VAE) with a specially designed autoregressive decoder
capable of modelling the noise component of an image but incapable of
independently modelling the underlying clean signal component. As a
consequence, our VAE's encoder learns to encode only underlying clean signal
content and to discard imaging noise. We also propose an additional decoder for
mapping the encoder's latent variables back into image space, thereby sampling
denoised images. Experimental results demonstrate that our approach surpasses
existing methods for self- and unsupervised image denoising while being robust
with respect to the size of the autoregressive receptive field. Code for this
project can be found at https://github.com/krulllab/DVLAE.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07891" title="Abstract">arXiv:2310.07891</a> (cross-list from stat.ML) [<a href="/pdf/2310.07891" title="Download PDF">pdf</a>, <a href="/format/2310.07891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Theory of Non-Linear Feature Learning with One Gradient Step in  Two-Layer Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Moniri%2C+B">Behrad Moniri</a>, 
<a href="/search/stat?searchtype=author&query=Lee%2C+D">Donghwan Lee</a>, 
<a href="/search/stat?searchtype=author&query=Hassani%2C+H">Hamed Hassani</a>, 
<a href="/search/stat?searchtype=author&query=Dobriban%2C+E">Edgar Dobriban</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Feature learning is thought to be one of the fundamental reasons for the
success of deep neural networks. It is rigorously known that in two-layer
fully-connected neural networks under certain conditions, one step of gradient
descent on the first layer followed by ridge regression on the second layer can
lead to feature learning; characterized by the appearance of a separated
rank-one component -- spike -- in the spectrum of the feature matrix. However,
with a constant gradient descent step size, this spike only carries information
from the linear component of the target function and therefore learning
non-linear components is impossible. We show that with a learning rate that
grows with the sample size, such training in fact introduces multiple rank-one
components, each corresponding to a specific polynomial feature. We further
prove that the limiting large-dimensional and large sample training and test
errors of the updated neural networks are fully characterized by these spikes.
By precisely analyzing the improvement in the loss, we demonstrate that these
non-linear features can enhance learning.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07908" title="Abstract">arXiv:2310.07908</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.07908" title="Download PDF">pdf</a>, <a href="/format/2310.07908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recurrent networks recognize patterns with low-dimensional oscillations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Murray%2C+K+T">Keith T. Murray</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">This study proposes a novel dynamical mechanism for pattern recognition
discovered by interpreting a recurrent neural network (RNN) trained on a simple
task inspired by the SET card game. We interpreted the trained RNN as
recognizing patterns via phase shifts in a low-dimensional limit cycle in a
manner analogous to transitions in a finite state automaton (FSA). We further
validated this interpretation by handcrafting a simple oscillatory model that
reproduces the dynamics of the trained RNN. Our findings not only suggest of a
potential dynamical mechanism capable of pattern recognition, but also suggest
of a potential neural implementation of FSA. Above all, this work contributes
to the growing discourse on deep learning model interpretability.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07925" title="Abstract">arXiv:2310.07925</a> (cross-list from math.OC) [<a href="/pdf/2310.07925" title="Download PDF">pdf</a>, <a href="/format/2310.07925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> First-Order Dynamic Optimization for Streaming Convex Costs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rostami%2C+M">M. Rostami</a>, 
<a href="/search/math?searchtype=author&query=Moradian%2C+H">H. Moradian</a>, 
<a href="/search/math?searchtype=author&query=Kia%2C+S+S">S. S. Kia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper proposes a set of novel optimization algorithms for solving a
class of convex optimization problems with time-varying streaming cost
function. We develop an approach to track the optimal solution with a bounded
error. Unlike the existing results, our algorithm is executed only by using the
first-order derivatives of the cost function which makes it computationally
efficient for optimization with time-varying cost function. We compare our
algorithms to the gradient descent algorithm and show why gradient descent is
not an effective solution for optimization problems with time-varying cost.
Several examples including solving a model predictive control problem cast as a
convex optimization problem with a streaming time-varying cost function
demonstrate our results.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07927" title="Abstract">arXiv:2310.07927</a> (cross-list from cond-mat.stat-mech) [<a href="/pdf/2310.07927" title="Download PDF">pdf</a>, <a href="/format/2310.07927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced sampling of Crystal Nucleation with Graph Representation Learnt  Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Zou%2C+Z">Ziyue Zou</a>, 
<a href="/search/cond-mat?searchtype=author&query=Tiwary%2C+P">Pratyush Tiwary</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Mechanics (cond-mat.stat-mech)</span>; Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this study, we present a graph neural network-based learning approach
using an autoencoder setup to derive low-dimensional variables from features
observed in experimental crystal structures. These variables are then biased in
enhanced sampling to observe state-to-state transitions and reliable
thermodynamic weights. Our approach uses simple convolution and pooling
methods. To verify the effectiveness of our protocol, we examined the
nucleation of various allotropes and polymorphs of iron and glycine from their
molten states. Our graph latent variables when biased in well-tempered
metadynamics consistently show transitions between states and achieve accurate
free energy calculations in agreement with experiments, both of which are
indicators of dependable sampling. This underscores the strength and promise of
our graph neural net variables for improved sampling. The protocol shown here
should be applicable for other systems and with other sampling methods.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07971" title="Abstract">arXiv:2310.07971</a> (cross-list from math.AT) [<a href="/pdf/2310.07971" title="Download PDF">pdf</a>, <a href="/ps/2310.07971" title="Download PostScript">ps</a>, <a href="/format/2310.07971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interval Decompositions for Persistence Modules of Free Abelian Groups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Luo%2C+J">Jiajie Luo</a>, 
<a href="/search/math?searchtype=author&query=Henselman-Petrusek%2C+G">Gregory Henselman-Petrusek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages. This is a working draft
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Topology (math.AT)</span>; Computational Geometry (cs.CG); Category Theory (math.CT)

</div>
<p class="mathjax">The study of persistence rests largely on the result that any
finitely-indexed persistence module of finite-dimensional vector spaces admits
an interval decomposition -- that is, a decomposition as a direct sum of
interval modules.
<br />This result fails if we replace vector spaces with modules over more general
coefficient rings. For example, not every persistence module of
finitely-generated free abelian groups admits an interval decomposition.
Nevertheless, many interesting examples of such persistence modules have been
empirically observed to decompose into intervals.
<br />Due to the prevalence of these modules in applied and theoretical settings,
it is important to understand the conditions under which interval decomposition
is possible. We provide a necessary and sufficient condition, and a
polynomial-time algorithm to either (a) compute an interval decomposition of a
persistence module of free abelian groups, or (b) certify that no such
decomposition exists. This complements earlier work, which characterizes
filtered topological spaces whose persistence diagrams are independent of the
choice of ground field.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07990" title="Abstract">arXiv:2310.07990</a> (cross-list from q-bio.GN) [<a href="/pdf/2310.07990" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-View Variational Autoencoder for Missing Value Imputation in  Untargeted Metabolomics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/q-bio?searchtype=author&query=Su%2C+K">Kuan-Jui Su</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+C">Chong Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=Cao%2C+X">Xuewei Cao</a>, 
<a href="/search/q-bio?searchtype=author&query=Sha%2C+Q">Qiuying Sha</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+W">Wu Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Luo%2C+Z">Zhe Luo</a>, 
<a href="/search/q-bio?searchtype=author&query=Qin%2C+T">Tian Qin</a>, 
<a href="/search/q-bio?searchtype=author&query=Qiu%2C+C">Chuan Qiu</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhao%2C+L+J">Lan Juan Zhao</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+A">Anqi Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Jiang%2C+L">Lindong Jiang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Shen%2C+H">Hui Shen</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhou%2C+W">Weihua Zhou</a>, 
<a href="/search/q-bio?searchtype=author&query=Deng%2C+H">Hong-Wen Deng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG); Applications (stat.AP)

</div>
<p class="mathjax">Background: Missing data is a common challenge in mass spectrometry-based
metabolomics, which can lead to biased and incomplete analyses. The integration
of whole-genome sequencing (WGS) data with metabolomics data has emerged as a
promising approach to enhance the accuracy of data imputation in metabolomics
studies. Method: In this study, we propose a novel method that leverages the
information from WGS data and reference metabolites to impute unknown
metabolites. Our approach utilizes a multi-view variational autoencoder to
jointly model the burden score, polygenetic risk score (PGS), and linkage
disequilibrium (LD) pruned single nucleotide polymorphisms (SNPs) for feature
extraction and missing metabolomics data imputation. By learning the latent
representations of both omics data, our method can effectively impute missing
metabolomics values based on genomic information. Results: We evaluate the
performance of our method on empirical metabolomics datasets with missing
values and demonstrate its superiority compared to conventional imputation
techniques. Using 35 template metabolites derived burden scores, PGS and
LD-pruned SNPs, the proposed methods achieved r2-scores &gt; 0.01 for 71.55% of
metabolites. Conclusion: The integration of WGS data in metabolomics imputation
not only improves data completeness but also enhances downstream analyses,
paving the way for more comprehensive and accurate investigations of metabolic
pathways and disease associations. Our findings offer valuable insights into
the potential benefits of utilizing WGS data for metabolomics data imputation
and underscore the importance of leveraging multi-modal data integration in
precision medicine research.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08002" title="Abstract">arXiv:2310.08002</a> (cross-list from eess.IV) [<a href="/pdf/2310.08002" title="Download PDF">pdf</a>, <a href="/format/2310.08002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MLP-AMDC: An MLP Architecture for Adaptive-Mask-based Dual-Camera  snapshot hyperspectral imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cai%2C+Z">Zeyu Cai</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+C">Can Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+X">Xunhao Chen</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+S">Shanghuan Liu</a>, 
<a href="/search/eess?searchtype=author&query=Jin%2C+C">Chengqian Jin</a>, 
<a href="/search/eess?searchtype=author&query=Da%2C+F">Feipeng Da</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2308.01541">arXiv:2308.01541</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Coded Aperture Snapshot Spectral Imaging (CASSI) system has great advantages
over traditional methods in dynamically acquiring Hyper-Spectral Image (HSI),
but there are the following problems. 1) Traditional mask relies on random
patterns or analytical design, both of which limit the performance improvement
of CASSI. 2) Existing high-quality reconstruction algorithms are slow in
reconstruction and can only reconstruct scene information offline. To address
the above two problems, this paper designs the AMDC-CASSI system, introducing
RGB camera with CASSI based on Adaptive-Mask as multimodal input to improve the
reconstruction quality. The existing SOTA reconstruction schemes are based on
transformer, but the operation of self-attention pulls down the operation
efficiency of the network. In order to improve the inference speed of the
reconstruction network, this paper proposes An MLP Architecture for
Adaptive-Mask-based Dual-Camera (MLP-AMDC) to replace the transformer structure
of the network. Numerous experiments have shown that MLP performs no less well
than transformer-based structures for HSI reconstruction, while MLP greatly
improves the network inference speed and has less number of parameters and
operations, our method has a 8 db improvement over SOTA and at least a 5-fold
improvement in reconstruction speed. (https://github.com/caizeyu1992/MLP-AMDC.)
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08061" title="Abstract">arXiv:2310.08061</a> (cross-list from q-bio.BM) [<a href="/pdf/2310.08061" title="Download PDF">pdf</a>, <a href="/format/2310.08061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ETDock: A Novel Equivariant Transformer for Protein-Ligand Docking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Yi%2C+Y">Yiqiang Yi</a>, 
<a href="/search/q-bio?searchtype=author&query=Wan%2C+X">Xu Wan</a>, 
<a href="/search/q-bio?searchtype=author&query=Bian%2C+Y">Yatao Bian</a>, 
<a href="/search/q-bio?searchtype=author&query=Ou-Yang%2C+L">Le Ou-Yang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhao%2C+P">Peilin Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Predicting the docking between proteins and ligands is a crucial and
challenging task for drug discovery. However, traditional docking methods
mainly rely on scoring functions, and deep learning-based docking approaches
usually neglect the 3D spatial information of proteins and ligands, as well as
the graph-level features of ligands, which limits their performance. To address
these limitations, we propose an equivariant transformer neural network for
protein-ligand docking pose prediction. Our approach involves the fusion of
ligand graph-level features by feature processing, followed by the learning of
ligand and protein representations using our proposed TAMformer module.
Additionally, we employ an iterative optimization approach based on the
predicted distance matrix to generate refined ligand poses. The experimental
results on real datasets show that our model can achieve state-of-the-art
performance.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08068" title="Abstract">arXiv:2310.08068</a> (cross-list from eess.IV) [<a href="/pdf/2310.08068" title="Download PDF">pdf</a>, <a href="/format/2310.08068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Frequency-Aware Re-Parameterization for Over-Fitting Based Image  Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ye%2C+Y">Yun Ye</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+Y">Yanjie Pan</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Q">Qually Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+M">Ming Lu</a>, 
<a href="/search/eess?searchtype=author&query=Fang%2C+X">Xiaoran Fang</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+B">Beryl Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be published at ICIP 2023, this version fixed a mistake in Eq. (1) in the proceeding version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Over-fitting-based image compression requires weights compactness for
compression and fast convergence for practical use, posing challenges for deep
convolutional neural networks (CNNs) based methods. This paper presents a
simple re-parameterization method to train CNNs with reduced weights storage
and accelerated convergence. The convolution kernels are re-parameterized as a
weighted sum of discrete cosine transform (DCT) kernels enabling direct
optimization in the frequency domain. Combined with L1 regularization, the
proposed method surpasses vanilla convolutions by achieving a significantly
improved rate-distortion with low computational cost. The proposed method is
verified with extensive experiments of over-fitting-based image restoration on
various datasets, achieving up to -46.12% BD-rate on top of HEIF with only 200
iterations.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08080" title="Abstract">arXiv:2310.08080</a> (cross-list from eess.IV) [<a href="/pdf/2310.08080" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RT-SRTS: Angle-Agnostic Real-Time Simultaneous 3D Reconstruction and  Tumor Segmentation from Single X-Ray Projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhu%2C+M">Miao Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Fu%2C+Q">Qiming Fu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+M">Mengxi Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+B">Bojian Li</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+X">Xiaoyan Luo</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+F">Fugen Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Radiotherapy is one of the primary treatment methods for tumors, but the
organ movement caused by respiratory motion limits its accuracy. Recently, 3D
imaging from single X-ray projection receives extensive attentions as a
promising way to address this issue. However, current methods can only
reconstruct 3D image without direct location of the tumor and are only
validated for fixed-angle imaging, which fails to fully meet the requirement of
motion control in radiotherapy. In this study, we propose a novel imaging
method RT-SRTS which integrates 3D imaging and tumor segmentation into one
network based on the multi-task learning (MTL) and achieves real-time
simultaneous 3D reconstruction and tumor segmentation from single X-ray
projection at any angle. Futhermore, we propose the attention enhanced
calibrator (AEC) and uncertain-region elaboration (URE) modules to aid feature
extraction and improve segmentation accuracy. We evaluated the proposed method
on ten patient cases and compared it with two state-of-the-art methods. Our
approach not only delivered superior 3D reconstruction but also demonstrated
commendable tumor segmentation results. The simultaneous reconstruction and
segmentation could be completed in approximately 70 ms, significantly faster
than the required time threshold for real-time tumor tracking. The efficacy of
both AEC and URE was also validated through ablation studies.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08087" title="Abstract">arXiv:2310.08087</a> (cross-list from eess.SP) [<a href="/pdf/2310.08087" title="Download PDF">pdf</a>, <a href="/format/2310.08087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Carbon Tracking Model for Federated Learning: Impact of Quantization  and Sparsification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Barbieri%2C+L">Luca Barbieri</a>, 
<a href="/search/eess?searchtype=author&query=Savazzi%2C+S">Stefano Savazzi</a>, 
<a href="/search/eess?searchtype=author&query=Kianoush%2C+S">Sanaz Kianoush</a>, 
<a href="/search/eess?searchtype=author&query=Nicoli%2C+M">Monica Nicoli</a>, 
<a href="/search/eess?searchtype=author&query=Serio%2C+L">Luigi Serio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted for presentation at IEEE CAMAD 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Federated Learning (FL) methods adopt efficient communication technologies to
distribute machine learning tasks across edge devices, reducing the overhead in
terms of data storage and computational complexity compared to centralized
solutions. Rather than moving large data volumes from producers (sensors,
machines) to energy-hungry data centers, raising environmental concerns due to
resource demands, FL provides an alternative solution to mitigate the energy
demands of several learning tasks while enabling new Artificial Intelligence of
Things (AIoT) applications. This paper proposes a framework for real-time
monitoring of the energy and carbon footprint impacts of FL systems. The carbon
tracking tool is evaluated for consensus (fully decentralized) and classical FL
policies. For the first time, we present a quantitative evaluation of different
computationally and communication efficient FL methods from the perspectives of
energy consumption and carbon equivalent emissions, suggesting also general
guidelines for energy-efficient design. Results indicate that consensus-driven
FL implementations should be preferred for limiting carbon emissions when the
energy efficiency of the communication is low (i.e., &lt; 25 Kbit/Joule). Besides,
quantization and sparsification operations are shown to strike a balance
between learning performances and energy consumption, leading to sustainable FL
designs.
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08104" title="Abstract">arXiv:2310.08104</a> (cross-list from eess.AS) [<a href="/pdf/2310.08104" title="Download PDF">pdf</a>, <a href="/format/2310.08104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and  Textually Described Voices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Baas%2C+M">Matthew Baas</a>, 
<a href="/search/eess?searchtype=author&query=Kamper%2C+H">Herman Kamper</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 figure, 5 tables. Accepted at SACAIR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
<p class="mathjax">Voice conversion aims to convert source speech into a target voice using
recordings of the target speaker as a reference. Newer models are producing
increasingly realistic output. But what happens when models are fed with
non-standard data, such as speech from a user with a speech impairment? We
investigate how a recent voice conversion model performs on non-standard
downstream voice conversion tasks. We use a simple but robust approach called
k-nearest neighbors voice conversion (kNN-VC). We look at four non-standard
applications: stuttered voice conversion, cross-lingual voice conversion,
musical instrument conversion, and text-to-voice conversion. The latter
involves converting to a target voice specified through a text description,
e.g. "a young man with a high-pitched voice". Compared to an established
baseline, we find that kNN-VC retains high performance in stuttered and
cross-lingual voice conversion. Results are more mixed for the musical
instrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some
instruments like drums but not on others. Nevertheless, this shows that voice
conversion models - and kNN-VC in particular - are increasingly applicable in a
range of non-standard downstream tasks. But there are still limitations when
samples are very far from the training distribution. Code, samples, trained
models: https://rf5.github.io/sacair2023-knnvc-demo/.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08109" title="Abstract">arXiv:2310.08109</a> (cross-list from physics.geo-ph) [<a href="/pdf/2310.08109" title="Download PDF">pdf</a>, <a href="/format/2310.08109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overview of Physics-Informed Machine Learning Inversion of Geophysical  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Schuster%2C+G+T">Gerard T. Schuster</a>, 
<a href="/search/physics?searchtype=author&query=Feng%2C+S">Shihang Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We review four types of algorithms for physics-informed machine learning
(PIML) inversion of geophysical data. The unifying equation is given by the
joint objective function $\epsilon$:
<br />\begin{eqnarray} \epsilon^{||-PIML}&amp;=&amp;\lambda_1 \overbrace{||{\bf
W}^{ML}({\bf H}_{{\bf w}} {\bf d}^{obs}-{\bf m})||^2}^{NN} + \lambda_2
\overbrace{{||{\bf W}^{FWI}({\bf L} {\bf m}-{\bf d}^{obs})||^2}}^{FWI} ~+
\nonumber\\ \nonumber\\ &amp;&amp; + ~~Regularizer, \label{PIML.eq120}
\end{eqnarray}where the optimal model ${\bf m}^*$ and weights $\bf w^*$
minimize $\epsilon$. Here, The matrix weights are given by the boldface symbol
$\bf W$, and full waveform inversion (FWI) is typically computed using a
finite-difference solution of the wave equation, where $\bf L$ represents the
forward modeling operation of the wave equation as a function of the model $\bf
m$. Also, a fully-connected neural network (NN) is used to compute the model
${\bf H_w}{\bf d}^{obs} \approx \bf m$ from the observed input data ${\bf
d}^{obs}$. The selection of weights $\lambda_i$ and the NN operations determine
one of four different PIML algorithms.
<br />PIML offers potential advantages over standard FWI through its enhanced
ability to avoid local minima and the option to locally train the inversion
operator, minimizing the requirement for extensive training data for global
applicability. However, the effectiveness of PIML relies on the similarity
between the test and trained data. Nevertheless, a possible strategy to
overcome this limitation involves initial pretraining of a PIML architecture
with data from a broader region, followed by fine-tuning for specific data-a
method reminiscent of the way large language models are pretrained and adapted
for various tasks.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08150" title="Abstract">arXiv:2310.08150</a> (cross-list from math.ST) [<a href="/pdf/2310.08150" title="Download PDF">pdf</a>, <a href="/ps/2310.08150" title="Download PostScript">ps</a>, <a href="/format/2310.08150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Extreme Value Asymptotics of Projected Sample Covariances in High  Dimensions with Applications in Finance and Convolutional Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Steland%2C+A">Ansgar Steland</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Maximum-type statistics of certain functions of the sample covariance matrix
of high-dimensional vector time series are studied to statistically confirm or
reject the null hypothesis that a data set has been collected under normal
conditions. The approach generalizes the case of the maximal deviation of the
sample autocovariances function from its assumed values. Within a linear time
series framework it is shown that Gumbel-type extreme value asymptotics holds
true. As applications we discuss long-only mimimal-variance portfolio
optimization and subportfolio analysis with respect to idiosyncratic risks, ETF
index tracking by sparse tracking portfolios, convolutional deep learners for
image analysis and the analysis of array-of-sensors data.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08165" title="Abstract">arXiv:2310.08165</a> (cross-list from eess.IV) [<a href="/pdf/2310.08165" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COVID-19 Detection Using Swin Transformer Approach from Computed  Tomography Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Morani%2C+K">Kenan Morani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">The accurate and efficient diagnosis of COVID-19 is of paramount importance,
particularly in the context of large-scale medical imaging datasets. In this
preprint paper, we propose a novel approach for COVID-19 diagnosis using CT
images that leverages the power of Swin Transformer models, state-of-the-art
solutions in computer vision tasks. Our method includes a systematic approach
for patient-level predictions, where individual CT slices are classified as
COVID-19 or non-COVID, and the patient's overall diagnosis is determined
through majority voting. The application of the Swin Transformer in this
context results in patient-level predictions that demonstrate exceptional
diagnostic accuracy. In terms of evaluation metrics, our approach consistently
outperforms the baseline, as well as numerous competing methods, showcasing its
effectiveness in COVID-19 diagnosis. The macro F1 score achieved by our model
exceeds the baseline and offers a robust solution for accurate diagnosis.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08209" title="Abstract">arXiv:2310.08209</a> (cross-list from stat.ML) [<a href="/pdf/2310.08209" title="Download PDF">pdf</a>, <a href="/format/2310.08209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformal inference for regression on Riemannian Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cholaquidis%2C+A">Alejandro Cholaquidis</a>, 
<a href="/search/stat?searchtype=author&query=Gamboa%2C+F">Fabrice Gamboa</a>, 
<a href="/search/stat?searchtype=author&query=Moreno%2C+L">Leonardo Moreno</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Regression on manifolds, and, more broadly, statistics on manifolds, has
garnered significant importance in recent years due to the vast number of
applications for this type of data. Circular data is a classic example, but so
is data in the space of covariance matrices, data on the Grassmannian manifold
obtained as a result of principal component analysis, among many others. In
this work we investigate prediction sets for regression scenarios when the
response variable, denoted by $Y$, resides in a manifold, and the covariable,
denoted by X, lies in Euclidean space. This extends the concepts delineated in
[Lei and Wasserman, 2014] to this novel context. Aligning with traditional
principles in conformal inference, these prediction sets are distribution-free,
indicating that no specific assumptions are imposed on the joint distribution
of $(X, Y)$, and they maintain a non-parametric character. We prove the
asymptotic almost sure convergence of the empirical version of these regions on
the manifold to their population counterparts. The efficiency of this method is
shown through a comprehensive simulation study and an analysis involving
real-world data.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08225" title="Abstract">arXiv:2310.08225</a> (cross-list from eess.AS) [<a href="/pdf/2310.08225" title="Download PDF">pdf</a>, <a href="/format/2310.08225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Word Error Rate Estimation Using Self-Supervised Representations  For Speech And Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Park%2C+C">Chanho Park</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+C">Chengsong Lu</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+M">Mingjie Chen</a>, 
<a href="/search/eess?searchtype=author&query=Hain%2C+T">Thomas Hain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
<p class="mathjax">The quality of automatic speech recognition (ASR) is typically measured by
word error rate (WER). WER estimation is a task aiming to predict the WER of an
ASR system, given a speech utterance and a transcription. This task has gained
increasing attention while advanced ASR systems are trained on large amounts of
data. In this case, WER estimation becomes necessary in many scenarios, for
example, selecting training data with unknown transcription quality or
estimating the testing performance of an ASR system without ground truth
transcriptions. Facing large amounts of data, the computation efficiency of a
WER estimator becomes essential in practical applications. However, previous
works usually did not consider it as a priority. In this paper, a Fast WER
estimator (Fe-WER) using self-supervised learning representation (SSLR) is
introduced. The estimator is built upon SSLR aggregated by average pooling. The
results show that Fe-WER outperformed the e-WER3 baseline relatively by 19.69%
and 7.16% on Ted-Lium3 in both evaluation metrics of root mean square error and
Pearson correlation coefficient, respectively. Moreover, the estimation
weighted by duration was 10.43% when the target was 10.88%. Lastly, the
inference speed was about 4x in terms of a real-time factor.
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08227" title="Abstract">arXiv:2310.08227</a> (cross-list from math.PR) [<a href="/pdf/2310.08227" title="Download PDF">pdf</a>, <a href="/ps/2310.08227" title="Download PostScript">ps</a>, <a href="/format/2310.08227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic limit behaviors of numerical discretizations for  time-homogeneous Markov processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+C">Chuchu Chen</a>, 
<a href="/search/math?searchtype=author&query=Dang%2C+T">Tonghe Dang</a>, 
<a href="/search/math?searchtype=author&query=Hong%2C+J">Jialin Hong</a>, 
<a href="/search/math?searchtype=author&query=Song%2C+G">Guoting Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">In order to give quantitative estimates for approximating the ergodic limit,
we investigate probabilistic limit behaviors of time-averaging estimators of
numerical discretizations for a class of time-homogeneous Markov processes, by
studying the corresponding strong law of large numbers and the central limit
theorem. Verifiable general sufficient conditions are proposed to ensure these
limit behaviors, which are related to the properties of strong mixing and
strong convergence for numerical discretizations of Markov processes. Our
results hold for test functionals with lower regularity compared with existing
results, and the analysis does not require the existence of the Poisson
equation associated with the underlying Markov process. Notably, our results
are applicable to numerical discretizations for a large class of stochastic
systems, including stochastic ordinary differential equations, infinite
dimensional stochastic evolution equations, and stochastic functional
differential equations.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08237" title="Abstract">arXiv:2310.08237</a> (cross-list from stat.ML) [<a href="/pdf/2310.08237" title="Download PDF">pdf</a>, <a href="/format/2310.08237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Feng%2C+X">Xingdong Feng</a>, 
<a href="/search/stat?searchtype=author&query=He%2C+X">Xin He</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+C">Caixing Wang</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+C">Chao Wang</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+J">Jingnan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Poster to appear in Thirty-seventh Conference on Neural Information Processing Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">Covariate shift occurs prevalently in practice, where the input distributions
of the source and target data are substantially different. Despite its
practical importance in various learning problems, most of the existing methods
only focus on some specific learning tasks and are not well validated
theoretically and numerically. To tackle this problem, we propose a unified
analysis of general nonparametric methods in a reproducing kernel Hilbert space
(RKHS) under covariate shift. Our theoretical results are established for a
general loss belonging to a rich loss function family, which includes many
commonly used methods as special cases, such as mean regression, quantile
regression, likelihood-based classification, and margin-based classification.
Two types of covariate shift problems are the focus of this paper and the sharp
convergence rates are established for a general loss function to provide a
unified theoretical analysis, which concurs with the optimal results in
literature where the squared loss is used. Extensive numerical studies on
synthetic and real examples confirm our theoretical findings and further
illustrate the effectiveness of our proposed method.
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08277" title="Abstract">arXiv:2310.08277</a> (cross-list from eess.AS) [<a href="/pdf/2310.08277" title="Download PDF">pdf</a>, <a href="/format/2310.08277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Single Speech Enhancement Model Unifying Dereverberation, Denoising,  Speaker Counting, Separation, and Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Saijo%2C+K">Kohei Saijo</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+W">Wangyou Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zhong-Qiu Wang</a>, 
<a href="/search/eess?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>, 
<a href="/search/eess?searchtype=author&query=Kobayashi%2C+T">Tetsunori Kobayashi</a>, 
<a href="/search/eess?searchtype=author&query=Ogawa%2C+T">Tetsuji Ogawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, 2 tables, accepted by ASRU2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">We propose a multi-task universal speech enhancement (MUSE) model that can
perform five speech enhancement (SE) tasks: dereverberation, denoising, speech
separation (SS), target speaker extraction (TSE), and speaker counting. This is
achieved by integrating two modules into an SE model: 1) an internal separation
module that does both speaker counting and separation; and 2) a TSE module that
extracts the target speech from the internal separation outputs using target
speaker cues. The model is trained to perform TSE if the target speaker cue is
given and SS otherwise. By training the model to remove noise and
reverberation, we allow the model to tackle the five tasks mentioned above with
a single model, which has not been accomplished yet. Evaluation results
demonstrate that the proposed MUSE model can successfully handle multiple tasks
with a single model.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08285" title="Abstract">arXiv:2310.08285</a> (cross-list from econ.GN) [<a href="/pdf/2310.08285" title="Download PDF">pdf</a>, <a href="/format/2310.08285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How would mobility-as-a-service (MaaS) platform survive as an  intermediary? From the viewpoint of stability in many-to-many matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Yao%2C+R">Rui Yao</a>, 
<a href="/search/econ?searchtype=author&query=Zhang%2C+K">Kenan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Mobility-as-a-service (MaaS) provides seamless door-to-door trips by
integrating different transport modes. Although many MaaS platforms have
emerged in recent years, most of them remain at a limited integration level.
This study investigates the assignment and pricing problem for a MaaS platform
as an intermediary in a multi-modal transportation network, which purchases
capacity from service operators and sells multi-modal trips to travelers. The
analysis framework of many-to-many stable matching is adopted to decompose the
joint design problem and to derive the stability condition such that both
operators and travelers are willing to participate in the MaaS system. To
maximize the flexibility in route choice and remove boundaries between modes,
we design an origin-destination pricing scheme for MaaS trips. On the supply
side, we propose a wholesale purchase price for service capacity. Accordingly,
the assignment problem is reformulated and solved as a bi-level program, where
MaaS travelers make multi-modal trips to minimize their travel costs meanwhile
interacting with non-MaaS travelers in the multi-modal transport system. We
prove that, under the proposed pricing scheme, there always exists a stable
outcome to the overall many-to-many matching problem. Further, given an optimal
assignment and under some mild conditions, a unique optimal pricing scheme is
ensured. Numerical experiments conducted on the extended Sioux Falls network
also demonstrate that the proposed MaaS system could create a win-win-win
situation -- the MaaS platform is profitable and both traveler welfare and
transit operator revenues increase from a baseline scenario without MaaS.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08287" title="Abstract">arXiv:2310.08287</a> (cross-list from stat.ML) [<a href="/pdf/2310.08287" title="Download PDF">pdf</a>, <a href="/format/2310.08287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Laurent%2C+O">Olivier Laurent</a>, 
<a href="/search/stat?searchtype=author&query=Aldea%2C+E">Emanuel Aldea</a>, 
<a href="/search/stat?searchtype=author&query=Franchi%2C+G">Gianni Franchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The distribution of the weights of modern deep neural networks (DNNs) -
crucial for uncertainty quantification and robustness - is an eminently complex
object due to its extremely high dimensionality. This paper proposes one of the
first large-scale explorations of the posterior distribution of deep Bayesian
Neural Networks (BNNs), expanding its study to real-world vision tasks and
architectures. Specifically, we investigate the optimal approach for
approximating the posterior, analyze the connection between posterior quality
and uncertainty quantification, delve into the impact of modes on the
posterior, and explore methods for visualizing the posterior. Moreover, we
uncover weight-space symmetries as a critical aspect for understanding the
posterior. To this extent, we develop an in-depth assessment of the impact of
both permutation and scaling symmetries that tend to obfuscate the Bayesian
posterior. While the first type of transformation is known for duplicating
modes, we explore the relationship between the latter and L2 regularization,
challenging previous misconceptions. Finally, to help the community improve our
understanding of the Bayesian posterior, we will shortly release the first
large-scale checkpoint dataset, including thousands of real-world models and
our codes.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08292" title="Abstract">arXiv:2310.08292</a> (cross-list from eess.SP) [<a href="/pdf/2310.08292" title="Download PDF">pdf</a>, <a href="/format/2310.08292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concealed Electronic Countermeasures of Radar Signal with Adversarial  Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ma%2C+R">Ruinan Ma</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+C">Canjie Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+M">Mingfeng Lu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yunjie Li</a>, 
<a href="/search/eess?searchtype=author&query=Tan%2C+Y">Yu-an Tan</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+R">Ruibin Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Tao%2C+R">Ran Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Electronic countermeasures involving radar signals are an important aspect of
modern warfare. Traditional electronic countermeasures techniques typically add
large-scale interference signals to ensure interference effects, which can lead
to attacks being too obvious. In recent years, AI-based attack methods have
emerged that can effectively solve this problem, but the attack scenarios are
currently limited to time domain radar signal classification. In this paper, we
focus on the time-frequency images classification scenario of radar signals. We
first propose an attack pipeline under the time-frequency images scenario and
DITIMI-FGSM attack algorithm with high transferability. Then, we propose
STFT-based time domain signal attack(STDS) algorithm to solve the problem of
non-invertibility in time-frequency analysis, thus obtaining the time-domain
representation of the interference signal. A large number of experiments show
that our attack pipeline is feasible and the proposed attack method has a high
success rate.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08331" title="Abstract">arXiv:2310.08331</a> (cross-list from stat.ML) [<a href="/pdf/2310.08331" title="Download PDF">pdf</a>, <a href="/format/2310.08331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of multi-armed bandit strategies on deep recurrent reinforcement  learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zangirolami%2C+V">Valentina Zangirolami</a>, 
<a href="/search/stat?searchtype=author&query=Borrotti%2C+M">Matteo Borrotti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Incomplete knowledge of the environment leads an agent to make decisions
under uncertainty. One of the major dilemmas in Reinforcement Learning (RL)
where an autonomous agent has to balance two contrasting needs in making its
decisions is: exploiting the current knowledge of the environment to maximize
the cumulative reward as well as exploring actions that allow improving the
knowledge of the environment, hopefully leading to higher reward values
(exploration-exploitation trade-off). Concurrently, another relevant issue
regards the full observability of the states, which may not be assumed in all
applications. Such as when only 2D images are considered as input in a RL
approach used for finding the optimal action within a 3D simulation
environment. In this work, we address these issues by deploying and testing
several techniques to balance exploration and exploitation trade-off on
partially observable systems for predicting steering wheels in autonomous
driving scenario. More precisely, the final aim is to investigate the effects
of using both stochastic and deterministic multi-armed bandit strategies
coupled with a Deep Recurrent Q-Network. Additionally, we adapted and evaluated
the impact of an innovative method to improve the learning phase of the
underlying Convolutional Recurrent Neural Network. We aim to show that adaptive
stochastic methods for exploration better approximate the trade-off between
exploration and exploitation as, in general, Softmax and Max-Boltzmann
strategies are able to outperform epsilon-greedy techniques.
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08338" title="Abstract">arXiv:2310.08338</a> (cross-list from eess.AS) [<a href="/pdf/2310.08338" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A cry for help: Early detection of brain injury in newborns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Onu%2C+C+C">Charles C. Onu</a>, 
<a href="/search/eess?searchtype=author&query=Latremouille%2C+S">Samantha Latremouille</a>, 
<a href="/search/eess?searchtype=author&query=Gorin%2C+A">Arsenii Gorin</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Junhao Wang</a>, 
<a href="/search/eess?searchtype=author&query=Ekwochi%2C+U">Uchenna Ekwochi</a>, 
<a href="/search/eess?searchtype=author&query=Ubuane%2C+P+O">Peter O. Ubuane</a>, 
<a href="/search/eess?searchtype=author&query=Kehinde%2C+O+A">Omolara A. Kehinde</a>, 
<a href="/search/eess?searchtype=author&query=Salisu%2C+M+A">Muhammad A. Salisu</a>, 
<a href="/search/eess?searchtype=author&query=Briggs%2C+D">Datonye Briggs</a>, 
<a href="/search/eess?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/eess?searchtype=author&query=Precup%2C+D">Doina Precup</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Since the 1960s, neonatal clinicians have known that newborns suffering from
certain neurological conditions exhibit altered crying patterns such as the
high-pitched cry in birth asphyxia. Despite an annual burden of over 1.5
million infant deaths and disabilities, early detection of neonatal brain
injuries due to asphyxia remains a challenge, particularly in developing
countries where the majority of births are not attended by a trained physician.
Here we report on the first inter-continental clinical study to demonstrate
that neonatal brain injury can be reliably determined from recorded infant
cries using an AI algorithm we call Roseline. Previous and recent work has been
limited by the lack of a large, high-quality clinical database of cry
recordings, constraining the application of state-of-the-art machine learning.
We develop a new training methodology for audio-based pathology detection
models and evaluate this system on a large database of newborn cry sounds
acquired from geographically diverse settings -- 5 hospitals across 3
continents. Our system extracts interpretable acoustic biomarkers that support
clinical decisions and is able to accurately detect neurological injury from
newborns' cries with an AUC of 92.5% (88.7% sensitivity at 80% specificity).
Cry-based neurological monitoring opens the door for low-cost, easy-to-use,
non-invasive and contact-free screening of at-risk babies, especially when
integrated into simple devices like smartphones or neonatal ICU monitors. This
would provide a reliable tool where there are no alternatives, but also curtail
the need to regularly exert newborns to physically-exhausting or
radiation-exposing assessments such as brain CT scans. This work sets the stage
for embracing the infant cry as a vital sign and indicates the potential of
AI-driven sound monitoring for the future of affordable healthcare.
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08391" title="Abstract">arXiv:2310.08391</a> (cross-list from stat.ML) [<a href="/pdf/2310.08391" title="Download PDF">pdf</a>, <a href="/format/2310.08391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Many Pretraining Tasks Are Needed for In-Context Learning of Linear  Regression?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wu%2C+J">Jingfeng Wu</a>, 
<a href="/search/stat?searchtype=author&query=Zou%2C+D">Difan Zou</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+Z">Zixiang Chen</a>, 
<a href="/search/stat?searchtype=author&query=Braverman%2C+V">Vladimir Braverman</a>, 
<a href="/search/stat?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>, 
<a href="/search/stat?searchtype=author&query=Bartlett%2C+P+L">Peter L. Bartlett</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Transformers pretrained on diverse tasks exhibit remarkable in-context
learning (ICL) capabilities, enabling them to solve unseen tasks solely based
on input contexts without adjusting model parameters. In this paper, we study
ICL in one of its simplest setups: pretraining a linearly parameterized
single-layer linear attention model for linear regression with a Gaussian
prior. We establish a statistical task complexity bound for the attention model
pretraining, showing that effective pretraining only requires a small number of
independent tasks. Furthermore, we prove that the pretrained model closely
matches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, by
achieving nearly Bayes optimal risk on unseen tasks under a fixed context
length. These theoretical findings complement prior experimental research and
shed light on the statistical foundations of ICL.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08416" title="Abstract">arXiv:2310.08416</a> (cross-list from math.NT) [<a href="/pdf/2310.08416" title="Download PDF">pdf</a>, <a href="/format/2310.08416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying reducible $k$-tuples of vectors with subspace-proximity  sensitive hashing/filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Holden%2C+G">Gabriella Holden</a>, 
<a href="/search/math?searchtype=author&query=Shiu%2C+D">Daniel Shiu</a>, 
<a href="/search/math?searchtype=author&query=Strutt%2C+L">Lauren Strutt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Number Theory (math.NT)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">We introduce and analyse a family of hash and predicate functions that are
more likely to produce collisions for small reducible configurations of
vectors. These may offer practical improvements to lattice sieving for short
vectors. In particular, in one asymptotic regime the family exhibits
significantly different convergent behaviour than existing hash functions and
predicates.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08439" title="Abstract">arXiv:2310.08439</a> (cross-list from physics.comp-ph) [<a href="/pdf/2310.08439" title="Download PDF">pdf</a>, <a href="/format/2310.08439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TensorMD: Scalable Tensor-Diagram based Machine Learning Interatomic  Potential on Heterogeneous Many-Core Processors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Chen%2C+X">Xin Chen</a>, 
<a href="/search/physics?searchtype=author&query=Ouyang%2C+Y">Yucheng Ouyang</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+X">Xin Chen</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+Z">Zhenchuan Chen</a>, 
<a href="/search/physics?searchtype=author&query=Lin%2C+R">Rongfen Lin</a>, 
<a href="/search/physics?searchtype=author&query=Gao%2C+X">Xingyu Gao</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+L">Lifang Wang</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+F">Fang Li</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+Y">Yin Liu</a>, 
<a href="/search/physics?searchtype=author&query=Shang%2C+H">Honghui Shang</a>, 
<a href="/search/physics?searchtype=author&query=Song%2C+H">Haifeng Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Molecular dynamics simulations have emerged as a potent tool for
investigating the physical properties and kinetic behaviors of materials at the
atomic scale, particularly in extreme conditions. Ab initio accuracy is now
achievable with machine learning based interatomic potentials. With recent
advancements in high-performance computing, highly accurate and large-scale
simulations become feasible. This study introduces TensorMD, a new machine
learning interatomic potential (MLIP) model that integrates physical principles
and tensor diagrams. The tensor formalism provides a more efficient computation
and greater flexibility for use with other scientific codes. Additionally, we
proposed several portable optimization strategies and developed a highly
optimized version for the new Sunway supercomputer. Our optimized TensorMD can
achieve unprecedented performance on the new Sunway, enabling simulations of up
to 52 billion atoms with a time-to-solution of 31 ps/step/atom, setting new
records for HPC + AI + MD.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08464" title="Abstract">arXiv:2310.08464</a> (cross-list from eess.AS) [<a href="/pdf/2310.08464" title="Download PDF">pdf</a>, <a href="/format/2310.08464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crowdsourced and Automatic Speech Prominence Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Morrison%2C+M">Max Morrison</a>, 
<a href="/search/eess?searchtype=author&query=Pawar%2C+P">Pranav Pawar</a>, 
<a href="/search/eess?searchtype=author&query=Pruyne%2C+N">Nathan Pruyne</a>, 
<a href="/search/eess?searchtype=author&query=Cole%2C+J">Jennifer Cole</a>, 
<a href="/search/eess?searchtype=author&query=Pardo%2C+B">Bryan Pardo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">The prominence of a spoken word is the degree to which an average native
listener perceives the word as salient or emphasized relative to its context.
Speech prominence estimation is the process of assigning a numeric value to the
prominence of each word in an utterance. These prominence labels are useful for
linguistic analysis, as well as training automated systems to perform
emphasis-controlled text-to-speech or emotion recognition. Manually annotating
prominence is time-consuming and expensive, which motivates the development of
automated methods for speech prominence estimation. However, developing such an
automated system using machine-learning methods requires human-annotated
training data. Using our system for acquiring such human annotations, we
collect and open-source crowdsourced annotations of a portion of the LibriTTS
dataset. We use these annotations as ground truth to train a neural speech
prominence estimator that generalizes to unseen speakers, datasets, and
speaking styles. We investigate design decisions for neural prominence
estimation as well as how neural prominence estimation improves as a function
of two key factors of annotation cost: dataset size and the number of
annotations per utterance.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08466" title="Abstract">arXiv:2310.08466</a> (cross-list from econ.GN) [<a href="/pdf/2310.08466" title="Download PDF">pdf</a>, <a href="/format/2310.08466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Belief formation and the persistence of biased beliefs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Compte%2C+O">Olivier Compte</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We propose a belief-formation model where agents attempt to discriminate
between two theories, and where the asymmetry in strength between confirming
and disconfirming evidence tilts beliefs in favor of theories that generate
strong (and possibly rare) confirming evidence and weak (and frequent)
disconfirming evidence. In our model, limitations on information processing
provide incentives to censor weak evidence, with the consequence that for some
discrimination problems, evidence may become mostly one-sided, independently of
the true underlying theory. Sophisticated agents who know the characteristics
of the censored data-generating process are not lured by this accumulation of
``evidence'', but less sophisticated ones end up with biased beliefs.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08495" title="Abstract">arXiv:2310.08495</a> (cross-list from stat.ML) [<a href="/pdf/2310.08495" title="Download PDF">pdf</a>, <a href="/format/2310.08495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing climate pathways using feature importance on echo state  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Goode%2C+K">Katherine Goode</a>, 
<a href="/search/stat?searchtype=author&query=Ries%2C+D">Daniel Ries</a>, 
<a href="/search/stat?searchtype=author&query=McClernon%2C+K">Kellie McClernon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
<p class="mathjax">The 2022 National Defense Strategy of the United States listed climate change
as a serious threat to national security. Climate intervention methods, such as
stratospheric aerosol injection, have been proposed as mitigation strategies,
but the downstream effects of such actions on a complex climate system are not
well understood. The development of algorithmic techniques for quantifying
relationships between source and impact variables related to a climate event
(i.e., a climate pathway) would help inform policy decisions. Data-driven deep
learning models have become powerful tools for modeling highly nonlinear
relationships and may provide a route to characterize climate variable
relationships. In this paper, we explore the use of an echo state network (ESN)
for characterizing climate pathways. ESNs are a computationally efficient
neural network variation designed for temporal data, and recent work proposes
ESNs as a useful tool for forecasting spatio-temporal climate data. Like other
neural networks, ESNs are non-interpretable black-box models, which poses a
hurdle for understanding variable relationships. We address this issue by
developing feature importance methods for ESNs in the context of
spatio-temporal data to quantify variable relationships captured by the model.
We conduct a simulation study to assess and compare the feature importance
techniques, and we demonstrate the approach on reanalysis climate data. In the
climate application, we select a time period that includes the 1991 volcanic
eruption of Mount Pinatubo. This event was a significant stratospheric aerosol
injection, which we use as a proxy for an artificial stratospheric aerosol
injection. Using the proposed approach, we are able to characterize
relationships between pathway variables associated with this event.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Fri, 13 Oct 23</h3>
<dl>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1908.04628" title="Abstract">arXiv:1908.04628</a> (replaced) [<a href="/pdf/1908.04628" title="Download PDF">pdf</a>, <a href="/format/1908.04628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> L2P: Learning to Place for Estimating Heavy-Tailed Distributed Outcomes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xindi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Varol%2C+O">Onur Varol</a>, 
<a href="/search/cs?searchtype=author&query=Eliassi-Rad%2C+T">Tina Eliassi-Rad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures, 2 tables Nature of changes from previous version: 1. Added complexity analysis in Section 2.2 2. Datasets change 3. Added LambdaMART in the baseline methods, also a brief discussion on why LambdaMart failed in our problem. 4. Figure updates
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1910.09143" title="Abstract">arXiv:1910.09143</a> (replaced) [<a href="/pdf/1910.09143" title="Download PDF">pdf</a>, <a href="/format/1910.09143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Subgoal-based Exploration via Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+Y">Yijia Wang</a>, 
<a href="/search/math?searchtype=author&query=Poloczek%2C+M">Matthias Poloczek</a>, 
<a href="/search/math?searchtype=author&query=Jiang%2C+D+R">Daniel R. Jiang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions on Machine Learning Research (09/2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.05421" title="Abstract">arXiv:2006.05421</a> (replaced) [<a href="/pdf/2006.05421" title="Download PDF">pdf</a>, <a href="/format/2006.05421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Sig-Wasserstein GANs for Time Series Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+S">Shujian Liao</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+H">Hao Ni</a>, 
<a href="/search/cs?searchtype=author&query=Szpruch%2C+L">Lukasz Szpruch</a>, 
<a href="/search/cs?searchtype=author&query=Wiese%2C+M">Magnus Wiese</a>, 
<a href="/search/cs?searchtype=author&query=Sabate-Vidales%2C+M">Marc Sabate-Vidales</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+B">Baoren Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for Mathematical Finance Special Issue on Machine Learning in Finance
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.04165" title="Abstract">arXiv:2107.04165</a> (replaced) [<a href="/pdf/2107.04165" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Being Together in Place as a Catalyst for Scientific Advance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duede%2C+E">Eamon Duede</a>, 
<a href="/search/cs?searchtype=author&query=Teplitskiy%2C+M">Misha Teplitskiy</a>, 
<a href="/search/cs?searchtype=author&query=Lakhani%2C+K">Karim Lakhani</a>, 
<a href="/search/cs?searchtype=author&query=Evans%2C+J">James Evans</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.07649" title="Abstract">arXiv:2107.07649</a> (replaced) [<a href="/pdf/2107.07649" title="Download PDF">pdf</a>, <a href="/format/2107.07649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reed-Muller Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Spandri%2C+M">Mattia Spandri</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+R">Roberto Ferrara</a>, 
<a href="/search/cs?searchtype=author&query=Deppe%2C+C">Christian Deppe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> V4: fixed typo in proof V3: capacity statement fixed; V2: published version in proceedings at International Zurich Seminar on Information and Communication (IZS) 2022 with wrong capacity statement; V1: wrong capacity statement (wrong proof that the codes do not achieve capacity while they do), submitted to 2021 IEEE Globecom: Workshop on Channel Coding beyond 5G
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Zurich Seminar on Information and Communication (IZS
  2022). Proceedings, pp. 74 - 78
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.14432" title="Abstract">arXiv:2107.14432</a> (replaced) [<a href="/pdf/2107.14432" title="Download PDF">pdf</a>, <a href="/format/2107.14432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Optimizers with Sparse Group Lasso for Neural Networks in CTR  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+Y">Yun Yue</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yongchao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+S">Suo Tong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Minghao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+C">Chunyang Wen</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+H">Huanjun Bao</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+L">Lihong Gu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinjie Gu</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+Y">Yixiang Mu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages. Published as a conference paper at ECML PKDD 2021. This version includes Appendix which was not included in the published version because of page limit
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Machine Learning and Knowledge Discovery in Databases. Research
  Track - European Conference, ECML PKDD 2021, Bilbao, Spain, September 13-17,
  2021, Proceedings, Part III
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.03539" title="Abstract">arXiv:2109.03539</a> (replaced) [<a href="/pdf/2109.03539" title="Download PDF">pdf</a>, <a href="/format/2109.03539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cooperative Operation of the Fleet Operator and Incentive-aware  Customers in an On-demand Delivery System: A Bi-level Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yao%2C+C">Canqi Yao</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+S">Shibo Chen</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+Z">Zaiyue Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE Internet of Things Journal
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Internet of Things Journal, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.03469" title="Abstract">arXiv:2110.03469</a> (replaced) [<a href="/pdf/2110.03469" title="Download PDF">pdf</a>, <a href="/format/2110.03469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning from Small Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamp%2C+M">Michael Kamp</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+J">Jonas Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Vreeken%2C+J">Jilles Vreeken</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.09726" title="Abstract">arXiv:2112.09726</a> (replaced) [<a href="/pdf/2112.09726" title="Download PDF">pdf</a>, <a href="/format/2112.09726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soundify: Matching Sound Effects to Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+D+C">David Chuan-En Lin</a>, 
<a href="/search/cs?searchtype=author&query=Germanidis%2C+A">Anastasis Germanidis</a>, 
<a href="/search/cs?searchtype=author&query=Valenzuela%2C+C">Crist&#xf3;bal Valenzuela</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yining Shi</a>, 
<a href="/search/cs?searchtype=author&query=Martelaro%2C+N">Nikolas Martelaro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Full paper in UIST 2023; Short paper in NeurIPS 2021 ML4CD Workshop; Online demo: <a href="https://soundify.cc">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.15202" title="Abstract">arXiv:2112.15202</a> (replaced) [<a href="/pdf/2112.15202" title="Download PDF">pdf</a>, <a href="/format/2112.15202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual and Object Geo-localization: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wilson%2C+D">Daniel Wilson</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaohan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sultani%2C+W">Waqas Sultani</a>, 
<a href="/search/cs?searchtype=author&query=Wshah%2C+S">Safwan Wshah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.15510" title="Abstract">arXiv:2112.15510</a> (replaced) [<a href="/pdf/2112.15510" title="Download PDF">pdf</a>, <a href="/format/2112.15510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Optimal Control of Bilinear Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yuan%2C+Z">Zhenyi Yuan</a>, 
<a href="/search/math?searchtype=author&query=Cortes%2C+J">Jorge Cortes</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Control Systems Letters, vol. 6, pp. 2479-2484, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.10662" title="Abstract">arXiv:2201.10662</a> (replaced) [<a href="/pdf/2201.10662" title="Download PDF">pdf</a>, <a href="/ps/2201.10662" title="Download PostScript">ps</a>, <a href="/format/2201.10662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bicategorical type theory: semantics and syntax
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahrens%2C+B">Benedikt Ahrens</a>, 
<a href="/search/cs?searchtype=author&query=North%2C+P+R">Paige Randall North</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Weide%2C+N">Niels van der Weide</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2: final version for LICS 2022. v3: long version - for detailed log, see Section 1.5 Version History. v4: Final version to be published in MSCS
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> v2: Logic in Computer Science (LICS) 2022. v4: Mathematical
  Structures in Computer Science, CUP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Category Theory (math.CT)

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.09348" title="Abstract">arXiv:2202.09348</a> (replaced) [<a href="/pdf/2202.09348" title="Download PDF">pdf</a>, <a href="/format/2202.09348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Machine Learning Paradigm for Studying Pictorial Realism: Are  Constable&#x27;s Clouds More Real than His Contemporaries?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuomin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mansfield%2C+E+C">Elizabeth C. Mansfield</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>, 
<a href="/search/cs?searchtype=author&query=Russell%2C+J">John Russell</a>, 
<a href="/search/cs?searchtype=author&query=Young%2C+G+S">George S. Young</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+C">Catherine Adams</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J+Z">James Z. Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Supplementary materials are available from the authors or <a href="http://wang.ist.psu.edu">this http URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.09233" title="Abstract">arXiv:2203.09233</a> (replaced) [<a href="/pdf/2203.09233" title="Download PDF">pdf</a>, <a href="/ps/2203.09233" title="Download PostScript">ps</a>, <a href="/format/2203.09233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Complexity of Techniques That Make Transition Systems  Implementable by Boolean Nets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Devillers%2C+R">Raymond Devillers</a>, 
<a href="/search/cs?searchtype=author&query=Tredup%2C+R">Ronny Tredup</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.01418" title="Abstract">arXiv:2204.01418</a> (replaced) [<a href="/pdf/2204.01418" title="Download PDF">pdf</a>, <a href="/ps/2204.01418" title="Download PostScript">ps</a>, <a href="/format/2204.01418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Ordinal Problems: Optimality of Comparison-based Algorithms and  their Cardinal Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gravin%2C+N">Nick Gravin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+E">Enze Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z+G">Zhihao Gavin Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at FOCS 2023. Abstract shortened to meet arXiv requirements
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.07580" title="Abstract">arXiv:2204.07580</a> (replaced) [<a href="/pdf/2204.07580" title="Download PDF">pdf</a>, <a href="/format/2204.07580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> mGPT: Few-Shot Learners Go Multilingual
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shliazhko%2C+O">Oleh Shliazhko</a>, 
<a href="/search/cs?searchtype=author&query=Fenogenova%2C+A">Alena Fenogenova</a>, 
<a href="/search/cs?searchtype=author&query=Tikhonova%2C+M">Maria Tikhonova</a>, 
<a href="/search/cs?searchtype=author&query=Mikhailov%2C+V">Vladislav Mikhailov</a>, 
<a href="/search/cs?searchtype=author&query=Kozlova%2C+A">Anastasia Kozlova</a>, 
<a href="/search/cs?searchtype=author&query=Shavrina%2C+T">Tatiana Shavrina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at Transactions of the Association for Computational Linguistics (TACL) To be presented at the Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.03659" title="Abstract">arXiv:2205.03659</a> (replaced) [<a href="/pdf/2205.03659" title="Download PDF">pdf</a>, <a href="/format/2205.03659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mechanising G&#xf6;del-L&#xf6;b provability logic in HOL Light
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maggesi%2C+M">Marco Maggesi</a>, 
<a href="/search/cs?searchtype=author&query=Brogi%2C+C+P">Cosimo Perini Brogi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint of the paper published in Journal of Automated Reasoning 67 (3) 2023. Please refer to the published paper for the most up-to-date version. arXiv admin note: text overlap with <a href="/abs/2102.05945">arXiv:2102.05945</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Automated Reasoning 67.3 (2023): 29
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.05188" title="Abstract">arXiv:2205.05188</a> (replaced) [<a href="/pdf/2205.05188" title="Download PDF">pdf</a>, <a href="/ps/2205.05188" title="Download PostScript">ps</a>, <a href="/format/2205.05188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Scale Space Radon Transform, Properties and Application in CT Image  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nacereddine%2C+N">Nafaa Nacereddine</a>, 
<a href="/search/cs?searchtype=author&query=Ziou%2C+D">Djemel Ziou</a>, 
<a href="/search/cs?searchtype=author&query=Goumeidane%2C+A+B">Aicha Baya Goumeidane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.13643" title="Abstract">arXiv:2205.13643</a> (replaced) [<a href="/pdf/2205.13643" title="Download PDF">pdf</a>, <a href="/format/2205.13643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable solver for time-dependent deformation problems with  contact
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zizhou Huang</a>, 
<a href="/search/cs?searchtype=author&query=Tozoni%2C+D+C">Davi Colli Tozoni</a>, 
<a href="/search/cs?searchtype=author&query=Gjoka%2C+A">Arvi Gjoka</a>, 
<a href="/search/cs?searchtype=author&query=Ferguson%2C+Z">Zachary Ferguson</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+T">Teseo Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Panozzo%2C+D">Daniele Panozzo</a>, 
<a href="/search/cs?searchtype=author&query=Zorin%2C+D">Denis Zorin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.15519" title="Abstract">arXiv:2205.15519</a> (replaced) [<a href="/pdf/2205.15519" title="Download PDF">pdf</a>, <a href="/format/2205.15519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vector-wise Joint Diagonalization of Almost Commuting Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+B">Bowen Li</a>, 
<a href="/search/math?searchtype=author&query=Lu%2C+J">Jianfeng Lu</a>, 
<a href="/search/math?searchtype=author&query=Yu%2C+Z">Ziang Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> revised
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.02136" title="Abstract">arXiv:2206.02136</a> (replaced) [<a href="/pdf/2206.02136" title="Download PDF">pdf</a>, <a href="/format/2206.02136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LDRNet: Enabling Real-time Document Localization on Mobile Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Han Wu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+H">Holland Qian</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Huaming Wu</a>, 
<a href="/search/cs?searchtype=author&query=van+Moorsel%2C+A">Aad van Moorsel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ECML-PKDD 2022 <a href="https://doi.org/10.1007/978-3-031-23618-1_42">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.08769" title="Abstract">arXiv:2207.08769</a> (replaced) [<a href="/pdf/2207.08769" title="Download PDF">pdf</a>, <a href="/format/2207.08769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical stability and tensor nuclear norm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dai%2C+Z">Zhen Dai</a>, 
<a href="/search/math?searchtype=author&query=Lim%2C+L">Lek-Heng Lim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.09339" title="Abstract">arXiv:2207.09339</a> (replaced) [<a href="/pdf/2207.09339" title="Download PDF">pdf</a>, <a href="/format/2207.09339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision Transformers: From Semantic Segmentation to Dense Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiachen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Sixiao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xinxuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiatian Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yanwei Fu</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+T">Tao Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jianfeng Feng</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P+H+S">Philip H.S. Torr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of CVPR 2021 paper <a href="/abs/2012.15840">arXiv:2012.15840</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.12389" title="Abstract">arXiv:2207.12389</a> (replaced) [<a href="/pdf/2207.12389" title="Download PDF">pdf</a>, <a href="/format/2207.12389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MemSAC: Memory Augmented Sample Consistency for Large Scale Unsupervised  Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalluri%2C+T">Tarun Kalluri</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Astuti Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Chandraker%2C+M">Manmohan Chandraker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ECCV 2022. Project Webpage: <a href="https://tarun005.github.io/MemSAC/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.02809" title="Abstract">arXiv:2208.02809</a> (replaced) [<a href="/pdf/2208.02809" title="Download PDF">pdf</a>, <a href="/format/2208.02809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Role of Morphological Variation in Evolutionary Robotics: Maximizing  Performance and Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carvalho%2C+J+T">Jonata Tyska Carvalho</a>, 
<a href="/search/cs?searchtype=author&query=Nolfi%2C+S">Stefano Nolfi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to MIT Evolutionary Computation Journal
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Carvalho, J. T., &amp; Nolfi, S. (2023). The role of morphological
  variation in evolutionary robotics: Maximizing performance and robustness.
  Evolutionary Computation, 1-18
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.10629" title="Abstract">arXiv:2208.10629</a> (replaced) [<a href="/pdf/2208.10629" title="Download PDF">pdf</a>, <a href="/format/2208.10629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Getting Bored of Cyberwar: Exploring the Role of Low-level Cybercrime  Actors in the Russia-Ukraine Conflict
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vu%2C+A+V">Anh V. Vu</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+D+R">Daniel R. Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+B">Ben Collier</a>, 
<a href="/search/cs?searchtype=author&query=Hutchings%2C+A">Alice Hutchings</a>, 
<a href="/search/cs?searchtype=author&query=Clayton%2C+R">Richard Clayton</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+R">Ross Anderson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.01094" title="Abstract">arXiv:2209.01094</a> (replaced) [<a href="/pdf/2209.01094" title="Download PDF">pdf</a>, <a href="/format/2209.01094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using aromas to search for preserved measures and integrals in Kahan&#x27;s  method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bogfjellmo%2C+G">Geir Bogfjellmo</a>, 
<a href="/search/math?searchtype=author&query=Celledoni%2C+E">Elena Celledoni</a>, 
<a href="/search/math?searchtype=author&query=McLachlan%2C+R">Robert McLachlan</a>, 
<a href="/search/math?searchtype=author&query=Owren%2C+B">Brynjulf Owren</a>, 
<a href="/search/math?searchtype=author&query=Quispel%2C+R">Reinout Quispel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.05167" title="Abstract">arXiv:2209.05167</a> (replaced) [<a href="/pdf/2209.05167" title="Download PDF">pdf</a>, <a href="/format/2209.05167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LF-VISLAM: A SLAM Framework for Large Field-of-View Cameras with  Negative Imaging Plane on Mobile Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ze Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kailun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Hao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+F">Fei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jian Bai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaiwei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE Transactions on Automation Science and Engineering (T-ASE). Extended version of IROS2022 paper <a href="/abs/2202.12613">arXiv:2202.12613</a>. Code and dataset will be open-sourced at <a href="https://github.com/flysoaryun/LF-SLAM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.07481" title="Abstract">arXiv:2209.07481</a> (replaced) [<a href="/pdf/2209.07481" title="Download PDF">pdf</a>, <a href="/format/2209.07481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quasi-Arithmetic Mixtures, Divergence Minimization, and Bregman  Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brekelmans%2C+R">Rob Brekelmans</a>, 
<a href="/search/cs?searchtype=author&query=Nielsen%2C+F">Frank Nielsen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages + appendix (rewritten + changed title in revision)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.10404" title="Abstract">arXiv:2209.10404</a> (replaced) [<a href="/pdf/2209.10404" title="Download PDF">pdf</a>, <a href="/format/2209.10404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GP-net: Flexible Viewpoint Grasp Proposal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Konrad%2C+A">Anna Konrad</a>, 
<a href="/search/cs?searchtype=author&query=McDonald%2C+J">John McDonald</a>, 
<a href="/search/cs?searchtype=author&query=Villing%2C+R">Rudi Villing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICAR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01244" title="Abstract">arXiv:2210.01244</a> (replaced) [<a href="/pdf/2210.01244" title="Download PDF">pdf</a>, <a href="/format/2210.01244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event-based Temporally Dense Optical Flow Estimation with Sequential  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ponghiran%2C+W">Wachirawit Ponghiran</a>, 
<a href="/search/cs?searchtype=author&query=Liyanagedera%2C+C+M">Chamika Mihiranga Liyanagedera</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+K">Kaushik Roy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.02081" title="Abstract">arXiv:2210.02081</a> (replaced) [<a href="/pdf/2210.02081" title="Download PDF">pdf</a>, <a href="/format/2210.02081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locate before Answering: Answer Guided Question Localization for Video  Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+T">Tianwen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+R">Ran Cui</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingjing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+P">Pai Peng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xiaowei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yu-Gang Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.02286" title="Abstract">arXiv:2210.02286</a> (replaced) [<a href="/pdf/2210.02286" title="Download PDF">pdf</a>, <a href="/format/2210.02286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient probabilistic reconciliation of forecasts for real-valued and  count time series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zambon%2C+L">Lorenzo Zambon</a>, 
<a href="/search/stat?searchtype=author&query=Azzimonti%2C+D">Dario Azzimonti</a>, 
<a href="/search/stat?searchtype=author&query=Corani%2C+G">Giorgio Corani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.05105" title="Abstract">arXiv:2210.05105</a> (replaced) [<a href="/pdf/2210.05105" title="Download PDF">pdf</a>, <a href="/format/2210.05105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed-Memory Randomized Algorithms for Sparse Tensor CP  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bharadwaj%2C+V">Vivek Bharadwaj</a>, 
<a href="/search/math?searchtype=author&query=Malik%2C+O+A">Osman Asif Malik</a>, 
<a href="/search/math?searchtype=author&query=Murray%2C+R">Riley Murray</a>, 
<a href="/search/math?searchtype=author&query=Bulu%C3%A7%2C+A">Aydin Bulu&#xe7;</a>, 
<a href="/search/math?searchtype=author&query=Demmel%2C+J">James Demmel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 10 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.09222" title="Abstract">arXiv:2210.09222</a> (replaced) [<a href="/pdf/2210.09222" title="Download PDF">pdf</a>, <a href="/format/2210.09222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MMTSA: Multimodal Temporal Segment Attention Network for Efficient Human  Activity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Ziqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuntao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianguo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+J">Junliang Xing</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+S">Shwetak Patel</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuanchun Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.11318" title="Abstract">arXiv:2210.11318</a> (replaced) [<a href="/pdf/2210.11318" title="Download PDF">pdf</a>, <a href="/format/2210.11318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Computer Vision Technologies In Urban and  Controlled-environment Agriculture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jiayun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Leung%2C+C">Cyril Leung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1 overview figures, 37 pages, 8 tables, accepted by ACM Computing Surveys
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.11355" title="Abstract">arXiv:2210.11355</a> (replaced) [<a href="/pdf/2210.11355" title="Download PDF">pdf</a>, <a href="/format/2210.11355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Synthetic Interventions: A Causal Framework for Panel Data Under  Network Interference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Agarwal%2C+A">Anish Agarwal</a>, 
<a href="/search/econ?searchtype=author&query=Cen%2C+S+H">Sarah H. Cen</a>, 
<a href="/search/econ?searchtype=author&query=Shah%2C+D">Devavrat Shah</a>, 
<a href="/search/econ?searchtype=author&query=Yu%2C+C+L">Christina Lee Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 49 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Econometrics (econ.EM)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.13660" title="Abstract">arXiv:2210.13660</a> (replaced) [<a href="/pdf/2210.13660" title="Download PDF">pdf</a>, <a href="/format/2210.13660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-SpacePhish: Extending the Evasion-space of Adversarial Attacks  against Phishing Website Detectors using Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Ying Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Apruzzese%2C+G">Giovanni Apruzzese</a>, 
<a href="/search/cs?searchtype=author&query=Conti%2C+M">Mauro Conti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.14558" title="Abstract">arXiv:2210.14558</a> (replaced) [<a href="/pdf/2210.14558" title="Download PDF">pdf</a>, <a href="/format/2210.14558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compressing And Debiasing Vision-Language Pre-Trained Models for Visual  Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Si%2C+Q">Qingyi Si</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+P">Peng Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiping Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.15889" title="Abstract">arXiv:2210.15889</a> (replaced) [<a href="/pdf/2210.15889" title="Download PDF">pdf</a>, <a href="/format/2210.15889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on  Neuro-Symbolic Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenguan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ongoing project
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.04653" title="Abstract">arXiv:2211.04653</a> (replaced) [<a href="/pdf/2211.04653" title="Download PDF">pdf</a>, <a href="/format/2211.04653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Follow the flow: Proximal flow inspired multi-step methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huang%2C+Y">Yushen Huang</a>, 
<a href="/search/math?searchtype=author&query=Sun%2C+Y">Yifan Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11184" title="Abstract">arXiv:2211.11184</a> (replaced) [<a href="/pdf/2211.11184" title="Download PDF">pdf</a>, <a href="/ps/2211.11184" title="Download PostScript">ps</a>, <a href="/format/2211.11184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Limit distribution theory for $f$-Divergences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sreekumar%2C+S">Sreejith Sreekumar</a>, 
<a href="/search/math?searchtype=author&query=Goldfeld%2C+Z">Ziv Goldfeld</a>, 
<a href="/search/math?searchtype=author&query=Kato%2C+K">Kengo Kato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.12345" title="Abstract">arXiv:2211.12345</a> (replaced) [<a href="/pdf/2211.12345" title="Download PDF">pdf</a>, <a href="/format/2211.12345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Sparse Feature Updates in Deep Networks using Iterative  Linearisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goldwaser%2C+A">Adrian Goldwaser</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+H">Hong Ge</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.14108" title="Abstract">arXiv:2211.14108</a> (replaced) [<a href="/pdf/2211.14108" title="Download PDF">pdf</a>, <a href="/format/2211.14108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3DDesigner: Towards Photorealistic 3D Object Generation and Editing with  Text-guided Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Heliang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaoyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Changwen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IJCV
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.15363" title="Abstract">arXiv:2211.15363</a> (replaced) [<a href="/pdf/2211.15363" title="Download PDF">pdf</a>, <a href="/format/2211.15363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Security Vulnerabilities of Text-to-SQL Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xutan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingfeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Stevenson%2C+M">Mark Stevenson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ISSRE 2023: Best Paper Candidate
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Databases (cs.DB); Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.00564" title="Abstract">arXiv:2212.00564</a> (replaced) [<a href="/pdf/2212.00564" title="Download PDF">pdf</a>, <a href="/format/2212.00564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Single-View Images for Unsupervised 3D Point Cloud Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lintai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qijian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Junhui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yong Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.00613" title="Abstract">arXiv:2212.00613</a> (replaced) [<a href="/pdf/2212.00613" title="Download PDF">pdf</a>, <a href="/format/2212.00613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and  Animation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+G">Giljoo Nam</a>, 
<a href="/search/cs?searchtype=author&query=Stuyck%2C+T">Tuur Stuyck</a>, 
<a href="/search/cs?searchtype=author&query=Lombardi%2C+S">Stephen Lombardi</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+C">Chen Cao</a>, 
<a href="/search/cs?searchtype=author&query=Saragih%2C+J">Jason Saragih</a>, 
<a href="/search/cs?searchtype=author&query=Zollhoefer%2C+M">Michael Zollhoefer</a>, 
<a href="/search/cs?searchtype=author&query=Hodgins%2C+J">Jessica Hodgins</a>, 
<a href="/search/cs?searchtype=author&query=Lassner%2C+C">Christoph Lassner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.06068" title="Abstract">arXiv:2212.06068</a> (replaced) [<a href="/pdf/2212.06068" title="Download PDF">pdf</a>, <a href="/format/2212.06068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving the Wide-band Inverse Scattering Problem via Equivariant Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+B">Borong Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zepeda-N%C3%BA%C3%B1ez%2C+L">Leonardo Zepeda-N&#xfa;&#xf1;ez</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+Q">Qin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 9 figures, and 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09408" title="Abstract">arXiv:2212.09408</a> (replaced) [<a href="/pdf/2212.09408" title="Download PDF">pdf</a>, <a href="/format/2212.09408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Object Detection with Large Vision Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+F">Feng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenze Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yonghong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+G">Guangming Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Fanglin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by International Journal of Computer Vision (IJCV). The 2nd place in the object detection track of the Robust Vision Challenge (RVC 2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10010" title="Abstract">arXiv:2212.10010</a> (replaced) [<a href="/pdf/2212.10010" title="Download PDF">pdf</a>, <a href="/format/2212.10010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying latent distances with Finslerian geometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pouplin%2C+A">Alison Pouplin</a>, 
<a href="/search/cs?searchtype=author&query=Eklund%2C+D">David Eklund</a>, 
<a href="/search/cs?searchtype=author&query=Ek%2C+C+H">Carl Henrik Ek</a>, 
<a href="/search/cs?searchtype=author&query=Hauberg%2C+S">S&#xf8;ren Hauberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 12 figures, accepted at TMLR (October 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.14116" title="Abstract">arXiv:2212.14116</a> (replaced) [<a href="/pdf/2212.14116" title="Download PDF">pdf</a>, <a href="/ps/2212.14116" title="Download PostScript">ps</a>, <a href="/format/2212.14116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coordination of Drones at Scale: Decentralized Energy-aware Swarm  Intelligence for Spatio-temporal Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+C">Chuhao Qin</a>, 
<a href="/search/cs?searchtype=author&query=Pournaras%2C+E">Evangelos Pournaras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 8 figures, 6 tables. Accepted in Transportation Research Part C: Emerging Technologies
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00743" title="Abstract">arXiv:2301.00743</a> (replaced) [<a href="/pdf/2301.00743" title="Download PDF">pdf</a>, <a href="/ps/2301.00743" title="Download PostScript">ps</a>, <a href="/format/2301.00743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing square roots in quaternion algebras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koprowski%2C+P">Przemys&#x142;aw Koprowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Final version of the paper formatted by the editorial board to appear in Fundamenta Informaticae
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>; Rings and Algebras (math.RA)

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.03342" title="Abstract">arXiv:2301.03342</a> (replaced) [<a href="/pdf/2301.03342" title="Download PDF">pdf</a>, <a href="/format/2301.03342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time Feedback Based Online Aggregate EV Power Flexibility  Characterization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yan%2C+D">Dongxiang Yan</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+S">Shihan Huang</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+Y">Yue Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.03900" title="Abstract">arXiv:2301.03900</a> (replaced) [<a href="/pdf/2301.03900" title="Download PDF">pdf</a>, <a href="/format/2301.03900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strong SDP based bounds on the cutwidth of a graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gaar%2C+E">Elisabeth Gaar</a>, 
<a href="/search/math?searchtype=author&query=Puges%2C+D">Diane Puges</a>, 
<a href="/search/math?searchtype=author&query=Wiegele%2C+A">Angelika Wiegele</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04491" title="Abstract">arXiv:2301.04491</a> (replaced) [<a href="/pdf/2301.04491" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PMMP -- PQC Migration Management Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=von+Nethen%2C+N">Nils von Nethen</a>, 
<a href="/search/cs?searchtype=author&query=Wiesmaier%2C+A">Alex Wiesmaier</a>, 
<a href="/search/cs?searchtype=author&query=Alnahawi%2C+N">Nouri Alnahawi</a>, 
<a href="/search/cs?searchtype=author&query=Henrich%2C+J">Johanna Henrich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04753" title="Abstract">arXiv:2301.04753</a> (replaced) [<a href="/pdf/2301.04753" title="Download PDF">pdf</a>, <a href="/format/2301.04753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cache-Aided $K$-User Broadcast Channels with State Information at  Receivers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reisizadeh%2C+H">Hadi Reisizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Maddah-Ali%2C+M+A">Mohammad Ali Maddah-Ali</a>, 
<a href="/search/cs?searchtype=author&query=Mohajer%2C+S">Soheil Mohajer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.07995" title="Abstract">arXiv:2301.07995</a> (replaced) [<a href="/pdf/2301.07995" title="Download PDF">pdf</a>, <a href="/format/2301.07995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequential learning and control: Targeted exploration for robust  performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Venkatasubramanian%2C+J">Janani Venkatasubramanian</a>, 
<a href="/search/eess?searchtype=author&query=K%C3%B6hler%2C+J">Johannes K&#xf6;hler</a>, 
<a href="/search/eess?searchtype=author&query=Berberich%2C+J">Julian Berberich</a>, 
<a href="/search/eess?searchtype=author&query=Allg%C3%B6wer%2C+F">Frank Allg&#xf6;wer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to IEEE Transactions on Automatic Control (TAC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.10886" title="Abstract">arXiv:2301.10886</a> (replaced) [<a href="/pdf/2301.10886" title="Download PDF">pdf</a>, <a href="/format/2301.10886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+M">Mingqi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xin Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+W">Wenjun Zeng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 16 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The Fortieth International Conference on Machine Learning
  (ICML2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.10902" title="Abstract">arXiv:2301.10902</a> (replaced) [<a href="/pdf/2301.10902" title="Download PDF">pdf</a>, <a href="/format/2301.10902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Hyperdimensional Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhanglu Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shida Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+K">Kaiwen Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+W">Weng-Fai Wong</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ECML PKDD 2023: Machine Learning and Knowledge Discovery in
  Databases: Research Track pp 141-155
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11514" title="Abstract">arXiv:2301.11514</a> (replaced) [<a href="/pdf/2301.11514" title="Download PDF">pdf</a>, <a href="/format/2301.11514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Industrial Image Anomaly Detection: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+G">Guoyang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinbao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shangnian Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+F">Feng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yaochu Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12082" title="Abstract">arXiv:2301.12082</a> (replaced) [<a href="/pdf/2301.12082" title="Download PDF">pdf</a>, <a href="/format/2301.12082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pushing the Limits of Fewshot Anomaly Detection in Industry Vision:  Graphcore
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+G">Guoyang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinbao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+F">Feng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yaochu Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12536" title="Abstract">arXiv:2301.12536</a> (replaced) [<a href="/pdf/2301.12536" title="Download PDF">pdf</a>, <a href="/ps/2301.12536" title="Download PostScript">ps</a>, <a href="/format/2301.12536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random points are good for universal discretization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dai%2C+F">F. Dai</a>, 
<a href="/search/math?searchtype=author&query=Temlyakov%2C+V">V. Temlyakov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Minor errors in earlier versions were corrected
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J. Math. Anal. Appl., Vol 529,2024, No. 1, Paper No. 127570, 28
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Functional Analysis (math.FA)</span>; Classical Analysis and ODEs (math.CA); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13326" title="Abstract">arXiv:2301.13326</a> (replaced) [<a href="/pdf/2301.13326" title="Download PDF">pdf</a>, <a href="/format/2301.13326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for Adapting Offline Algorithms to Solve Combinatorial  Multi-Armed Bandit Problems with Bandit Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nie%2C+G">Guanyu Nie</a>, 
<a href="/search/cs?searchtype=author&query=Nadew%2C+Y+Y">Yididiya Y Nadew</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yanhui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Quinn%2C+C+J">Christopher John Quinn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This extends the framework in previous version to adapt randomized offline approximation algorithms
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 40th International Conference on Machine
  Learning, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13359" title="Abstract">arXiv:2301.13359</a> (replaced) [<a href="/pdf/2301.13359" title="Download PDF">pdf</a>, <a href="/format/2301.13359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+G">Guoyang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinbao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+J">Jiayi Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+F">Feng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yaochu Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13613" title="Abstract">arXiv:2301.13613</a> (replaced) [<a href="/pdf/2301.13613" title="Download PDF">pdf</a>, <a href="/format/2301.13613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometry-based approximation of waves in complex domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Pradovera%2C+D">Davide Pradovera</a>, 
<a href="/search/math?searchtype=author&query=Nonino%2C+M">Monica Nonino</a>, 
<a href="/search/math?searchtype=author&query=Perugia%2C+I">Ilaria Perugia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00767" title="Abstract">arXiv:2302.00767</a> (replaced) [<a href="/pdf/2302.00767" title="Download PDF">pdf</a>, <a href="/format/2302.00767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ImageNomer: description of a functional connectivity and omics analysis  tool and case study identifying a race confound
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Orlichenko%2C+A">Anton Orlichenko</a>, 
<a href="/search/q-bio?searchtype=author&query=Daly%2C+G">Grant Daly</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhou%2C+Z">Ziyu Zhou</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+A">Anqi Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Shen%2C+H">Hui Shen</a>, 
<a href="/search/q-bio?searchtype=author&query=Deng%2C+H">Hong-Wen Deng</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+Y">Yu-Ping Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Populations and Evolution (q-bio.PE)</span>; Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02931" title="Abstract">arXiv:2302.02931</a> (replaced) [<a href="/pdf/2302.02931" title="Download PDF">pdf</a>, <a href="/format/2302.02931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bitrate-Constrained DRO: Beyond Worst Case Robustness To Unknown Group  Shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Setlur%2C+A">Amrith Setlur</a>, 
<a href="/search/cs?searchtype=author&query=Dennis%2C+D">Don Dennis</a>, 
<a href="/search/cs?searchtype=author&query=Eysenbach%2C+B">Benjamin Eysenbach</a>, 
<a href="/search/cs?searchtype=author&query=Raghunathan%2C+A">Aditi Raghunathan</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+V">Virginia Smith</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03770" title="Abstract">arXiv:2302.03770</a> (replaced) [<a href="/pdf/2302.03770" title="Download PDF">pdf</a>, <a href="/ps/2302.03770" title="Download PostScript">ps</a>, <a href="/format/2302.03770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provably Efficient Offline Goal-Conditioned Reinforcement Learning with  General Function Approximation and Single-Policy Concentrability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanlin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Amy Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023, 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03874" title="Abstract">arXiv:2302.03874</a> (replaced) [<a href="/pdf/2302.03874" title="Download PDF">pdf</a>, <a href="/format/2302.03874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Participatory Personalization in Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joren%2C+H">Hailey Joren</a>, 
<a href="/search/cs?searchtype=author&query=Nagpal%2C+C">Chirag Nagpal</a>, 
<a href="/search/cs?searchtype=author&query=Heller%2C+K">Katherine Heller</a>, 
<a href="/search/cs?searchtype=author&query=Ustun%2C+B">Berk Ustun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07863" title="Abstract">arXiv:2302.07863</a> (replaced) [<a href="/pdf/2302.07863" title="Download PDF">pdf</a>, <a href="/format/2302.07863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speculative Decoding with Big Little Decoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sehoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Mangalam%2C+K">Karttikeya Mangalam</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+S">Suhong Moon</a>, 
<a href="/search/cs?searchtype=author&query=Malik%2C+J">Jitendra Malik</a>, 
<a href="/search/cs?searchtype=author&query=Mahoney%2C+M+W">Michael W. Mahoney</a>, 
<a href="/search/cs?searchtype=author&query=Gholami%2C+A">Amir Gholami</a>, 
<a href="/search/cs?searchtype=author&query=Keutzer%2C+K">Kurt Keutzer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08594" title="Abstract">arXiv:2302.08594</a> (replaced) [<a href="/pdf/2302.08594" title="Download PDF">pdf</a>, <a href="/format/2302.08594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransUPR: A Transformer-based Uncertain Point Refiner for LiDAR Point  Cloud Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zifan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Meida Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhikang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+S">Suya You</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+R">Raghuveer Rao</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+S">Sanjeev Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+F">Fengbo Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages; Accepted by 2023 IROS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11713" title="Abstract">arXiv:2302.11713</a> (replaced) [<a href="/pdf/2302.11713" title="Download PDF">pdf</a>, <a href="/format/2302.11713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Pre-trained Vision and Language Models Answer Visual  Information-Seeking Questions?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hexiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Luan%2C+Y">Yi Luan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haitian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Changpinyo%2C+S">Soravit Changpinyo</a>, 
<a href="/search/cs?searchtype=author&query=Ritter%2C+A">Alan Ritter</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+M">Ming-Wei Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (main conference); Our dataset and evaluation is available at <a href="https://open-vision-language.github.io/infoseek/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.12461" title="Abstract">arXiv:2302.12461</a> (replaced) [<a href="/pdf/2302.12461" title="Download PDF">pdf</a>, <a href="/format/2302.12461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing And Editing Inner Mechanisms Of Backdoored Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lamparth%2C+M">Max Lamparth</a>, 
<a href="/search/cs?searchtype=author&query=Reuel%2C+A">Anka Reuel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> included new experimental results and addressed reviewer feedback
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14860" title="Abstract">arXiv:2302.14860</a> (replaced) [<a href="/pdf/2302.14860" title="Download PDF">pdf</a>, <a href="/ps/2302.14860" title="Download PostScript">ps</a>, <a href="/format/2302.14860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revocable Cryptography from Learning with Errors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Ananth%2C+P">Prabhanjan Ananth</a>, 
<a href="/search/quant-ph?searchtype=author&query=Poremba%2C+A">Alexander Poremba</a>, 
<a href="/search/quant-ph?searchtype=author&query=Vaikuntanathan%2C+V">Vinod Vaikuntanathan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 92 pages. Revised version. Proceedings of TCC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01476" title="Abstract">arXiv:2303.01476</a> (replaced) [<a href="/pdf/2303.01476" title="Download PDF">pdf</a>, <a href="/format/2303.01476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Oblivious Transfer from Zero-Knowledge Proofs, or How to Achieve  Round-Optimal Quantum Oblivious Transfer and Zero-Knowledge Proofs on Quantum  States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Colisson%2C+L">L&#xe9;o Colisson</a>, 
<a href="/search/quant-ph?searchtype=author&query=Muguruza%2C+G">Garazi Muguruza</a>, 
<a href="/search/quant-ph?searchtype=author&query=Speelman%2C+F">Florian Speelman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 67 pages (full version of the version published in ASIACRYPT 2023, added more details on ZKoQS and fixed a minor mistake)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01748" title="Abstract">arXiv:2303.01748</a> (replaced) [<a href="/pdf/2303.01748" title="Download PDF">pdf</a>, <a href="/format/2303.01748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Complete Recipe for Diffusion Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pandey%2C+K">Kushagra Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Mandt%2C+S">Stephan Mandt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ICCV'23 (Oral Presentation)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04466" title="Abstract">arXiv:2303.04466</a> (replaced) [<a href="/pdf/2303.04466" title="Download PDF">pdf</a>, <a href="/format/2303.04466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRADE: Generating Realistic Animated Dynamic Environments for Robotics  Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonetto%2C+E">Elia Bonetto</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenghao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+A">Aamir Ahmad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 10 tables, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07189" title="Abstract">arXiv:2303.07189</a> (replaced) [<a href="/pdf/2303.07189" title="Download PDF">pdf</a>, <a href="/format/2303.07189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Convolutional Neural Networks for Chronic Obstructive  Pulmonary Disease Detection in Clinical Computed Tomography Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dorosti%2C+T">Tina Dorosti</a>, 
<a href="/search/eess?searchtype=author&query=Schultheiss%2C+M">Manuel Schultheiss</a>, 
<a href="/search/eess?searchtype=author&query=Hofmann%2C+F">Felix Hofmann</a>, 
<a href="/search/eess?searchtype=author&query=Thalhammer%2C+J">Johannes Thalhammer</a>, 
<a href="/search/eess?searchtype=author&query=Kirchner%2C+L">Luisa Kirchner</a>, 
<a href="/search/eess?searchtype=author&query=Urban%2C+T">Theresa Urban</a>, 
<a href="/search/eess?searchtype=author&query=Pfeiffer%2C+F">Franz Pfeiffer</a>, 
<a href="/search/eess?searchtype=author&query=Schaff%2C+F">Florian Schaff</a>, 
<a href="/search/eess?searchtype=author&query=Lasser%2C+T">Tobias Lasser</a>, 
<a href="/search/eess?searchtype=author&query=Pfeiffer%2C+D">Daniela Pfeiffer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07312" title="Abstract">arXiv:2303.07312</a> (replaced) [<a href="/pdf/2303.07312" title="Download PDF">pdf</a>, <a href="/format/2303.07312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing LiDAR performance: Robust De-skewing Exclusively Relying on  Range Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salem%2C+O">Omar Salem</a>, 
<a href="/search/cs?searchtype=author&query=Giacomini%2C+E">Emanuele Giacomini</a>, 
<a href="/search/cs?searchtype=author&query=Brizi%2C+L">Leonardo Brizi</a>, 
<a href="/search/cs?searchtype=author&query=Di+Giammarino%2C+L">Luca Di Giammarino</a>, 
<a href="/search/cs?searchtype=author&query=Grisetti%2C+G">Giorgio Grisetti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages , 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08694" title="Abstract">arXiv:2303.08694</a> (replaced) [<a href="/pdf/2303.08694" title="Download PDF">pdf</a>, <a href="/format/2303.08694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quasi continuous level Monte Carlo for random elliptic PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Beschle%2C+C+A">Cedric Aaron Beschle</a>, 
<a href="/search/math?searchtype=author&query=Barth%2C+A">Andrea Barth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Probability (math.PR); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09033" title="Abstract">arXiv:2303.09033</a> (replaced) [<a href="/pdf/2303.09033" title="Download PDF">pdf</a>, <a href="/format/2303.09033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+A">Aadirupa Saha</a>, 
<a href="/search/cs?searchtype=author&query=Kveton%2C+B">Branislav Kveton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10108" title="Abstract">arXiv:2303.10108</a> (replaced) [<a href="/pdf/2303.10108" title="Download PDF">pdf</a>, <a href="/format/2303.10108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Centric Learning from Unlabeled Graphs with Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Gang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Inae%2C+E">Eric Inae</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiaxin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+T">Tengfei Luo</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Meng Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12535" title="Abstract">arXiv:2303.12535</a> (replaced) [<a href="/pdf/2303.12535" title="Download PDF">pdf</a>, <a href="/format/2303.12535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Effective Motion-Centric Paradigm for 3D Single Object Tracking in  Point Clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chaoda Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xu Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baoyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Shenghui Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+S">Shuguang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhen Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted version of the journal extension of M^2-Track. Accepted by TPAMI. arXiv admin note: substantial text overlap with <a href="/abs/2203.01730">arXiv:2203.01730</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17823" title="Abstract">arXiv:2303.17823</a> (replaced) [<a href="/pdf/2303.17823" title="Download PDF">pdf</a>, <a href="/format/2303.17823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An interpretable neural network-based non-proportional odds model for  ordinal regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Okuno%2C+A">Akifumi Okuno</a>, 
<a href="/search/stat?searchtype=author&query=Harada%2C+K">Kazuharu Harada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00457" title="Abstract">arXiv:2304.00457</a> (replaced) [<a href="/pdf/2304.00457" title="Download PDF">pdf</a>, <a href="/format/2304.00457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Puchert%2C+P">Patrik Puchert</a>, 
<a href="/search/cs?searchtype=author&query=Poonam%2C+P">Poonam Poonam</a>, 
<a href="/search/cs?searchtype=author&query=van+Onzenoodt%2C+C">Christian van Onzenoodt</a>, 
<a href="/search/cs?searchtype=author&query=Ropinski%2C+T">Timo Ropinski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01601" title="Abstract">arXiv:2304.01601</a> (replaced) [<a href="/pdf/2304.01601" title="Download PDF">pdf</a>, <a href="/format/2304.01601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Primitive Simultaneous Optimization of Similarity Metrics for Image  Registration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Waldmannstetter%2C+D">Diana Waldmannstetter</a>, 
<a href="/search/eess?searchtype=author&query=Wiestler%2C+B">Benedikt Wiestler</a>, 
<a href="/search/eess?searchtype=author&query=Schwarting%2C+J">Julian Schwarting</a>, 
<a href="/search/eess?searchtype=author&query=Ezhov%2C+I">Ivan Ezhov</a>, 
<a href="/search/eess?searchtype=author&query=Metz%2C+M">Marie Metz</a>, 
<a href="/search/eess?searchtype=author&query=Bakas%2C+S">Spyridon Bakas</a>, 
<a href="/search/eess?searchtype=author&query=Baheti%2C+B">Bhakti Baheti</a>, 
<a href="/search/eess?searchtype=author&query=Chakrabarty%2C+S">Satrajit Chakrabarty</a>, 
<a href="/search/eess?searchtype=author&query=Rueckert%2C+D">Daniel Rueckert</a>, 
<a href="/search/eess?searchtype=author&query=Kirschke%2C+J+S">Jan S. Kirschke</a>, 
<a href="/search/eess?searchtype=author&query=Heckemann%2C+R+A">Rolf A. Heckemann</a>, 
<a href="/search/eess?searchtype=author&query=Piraud%2C+M">Marie Piraud</a>, 
<a href="/search/eess?searchtype=author&query=Menze%2C+B+H">Bjoern H. Menze</a>, 
<a href="/search/eess?searchtype=author&query=Kofler%2C+F">Florian Kofler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02621" title="Abstract">arXiv:2304.02621</a> (replaced) [<a href="/pdf/2304.02621" title="Download PDF">pdf</a>, <a href="/format/2304.02621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jonnarth%2C+A">Arvi Jonnarth</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yushan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Felsberg%2C+M">Michael Felsberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03198" title="Abstract">arXiv:2304.03198</a> (replaced) [<a href="/pdf/2304.03198" title="Download PDF">pdf</a>, <a href="/ps/2304.03198" title="Download PostScript">ps</a>, <a href="/format/2304.03198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RFAConv: Innovating Spatial Attention and Standard Convolutional  Operation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Degang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+T">Tingting Song</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yichen Ye</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Ke Li</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yingze Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 11figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04234" title="Abstract">arXiv:2304.04234</a> (replaced) [<a href="/pdf/2304.04234" title="Download PDF">pdf</a>, <a href="/format/2304.04234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational operator learning: A unified paradigm marrying training  neural operators and solving partial differential equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tengfei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dachuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+P">Peng Hao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bo Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 6 figures with 5 extended figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05306" title="Abstract">arXiv:2304.05306</a> (replaced) [<a href="/pdf/2304.05306" title="Download PDF">pdf</a>, <a href="/format/2304.05306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Linear Correctors: A Tight Output Min-Entropy Bound and  Selection Technique
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gruji%C4%87%2C+M">Milo&#x161; Gruji&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Verbauwhede%2C+I">Ingrid Verbauwhede</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Final version after the review process. Accepted for publication in IEEE Transactions on Information Forensics and Security
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09663" title="Abstract">arXiv:2304.09663</a> (replaced) [<a href="/pdf/2304.09663" title="Download PDF">pdf</a>, <a href="/format/2304.09663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative modeling of time-dependent densities via optimal transport  and projection pursuit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Botvinick-Greenhouse%2C+J">Jonah Botvinick-Greenhouse</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+Y">Yunan Yang</a>, 
<a href="/search/stat?searchtype=author&query=Maulik%2C+R">Romit Maulik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article may be downloaded for personal use only. Any other use requires prior permission of the author and AIP Publishing. This article appeared in Chaos: An Interdisciplinary Journal of Nonlinear Science, Volume 33, Issue 10, October 2023 and may be found at <a href="https://doi.org/10.1063/5.0155783">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Chaos: An Interdisciplinary Journal of Nonlinear Science 33,
  103108 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10584" title="Abstract">arXiv:2304.10584</a> (replaced) [<a href="/pdf/2304.10584" title="Download PDF">pdf</a>, <a href="/ps/2304.10584" title="Download PostScript">ps</a>, <a href="/format/2304.10584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Algebra for Stabilizer Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Comfort%2C+C">Cole Comfort</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fixed errors. Made presentation much cleaner. Made results more precise
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Logic in Computer Science (cs.LO); Category Theory (math.CT)

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11657" title="Abstract">arXiv:2304.11657</a> (replaced) [<a href="/pdf/2304.11657" title="Download PDF">pdf</a>, <a href="/format/2304.11657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiashuo Sun</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yi Luo</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yeyun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yelong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jian Guo</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+N">Nan Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 10 figures, 21 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12233" title="Abstract">arXiv:2304.12233</a> (replaced) [<a href="/pdf/2304.12233" title="Download PDF">pdf</a>, <a href="/format/2304.12233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion-based Generative AI for Exploring Transition States from 2D  Molecular Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Kim%2C+S">Seonghwan Kim</a>, 
<a href="/search/physics?searchtype=author&query=Woo%2C+J">Jeheon Woo</a>, 
<a href="/search/physics?searchtype=author&query=Kim%2C+W+Y">Woo Youn Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12840" title="Abstract">arXiv:2304.12840</a> (replaced) [<a href="/pdf/2304.12840" title="Download PDF">pdf</a>, <a href="/format/2304.12840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatiotemporal gender differences in urban vibrancy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Collins%2C+T">Thomas Collins</a>, 
<a href="/search/physics?searchtype=author&query=Di+Clemente%2C+R">Riccardo Di Clemente</a>, 
<a href="/search/physics?searchtype=author&query=Guti%C3%A9rrez-Roig%2C+M">Mario Guti&#xe9;rrez-Roig</a>, 
<a href="/search/physics?searchtype=author&query=Botta%2C+F">Federico Botta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 10 figures: 1 figure and 1 table in main, 7 figures in supplementary material, 3 tables in supplementary material. Submitted to Environment and Planning B: Urban Analytics and City Science special issue on Spatial Inequalities and Cities
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13819" title="Abstract">arXiv:2304.13819</a> (replaced) [<a href="/pdf/2304.13819" title="Download PDF">pdf</a>, <a href="/format/2304.13819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAPConNet: Self-supervised 3D Pose Transfer with Mesh and Point  Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiaze Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhixiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Tae-Kyun Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00152" title="Abstract">arXiv:2305.00152</a> (replaced) [<a href="/pdf/2305.00152" title="Download PDF">pdf</a>, <a href="/format/2305.00152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Limits of Model Selection under Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hanneke%2C+S">Steve Hanneke</a>, 
<a href="/search/stat?searchtype=author&query=Kpotufe%2C+S">Samory Kpotufe</a>, 
<a href="/search/stat?searchtype=author&query=Mahdaviyeh%2C+Y">Yasaman Mahdaviyeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at the Conference on Learning Theory (COLT) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05570" title="Abstract">arXiv:2305.05570</a> (replaced) [<a href="/pdf/2305.05570" title="Download PDF">pdf</a>, <a href="/format/2305.05570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Engineering a Formally Verified Automated Bug Finder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Correnson%2C+A">Arthur Correnson</a>, 
<a href="/search/cs?searchtype=author&query=Steinhoefel%2C+D">Dominic Steinhoefel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06118" title="Abstract">arXiv:2305.06118</a> (replaced) [<a href="/pdf/2305.06118" title="Download PDF">pdf</a>, <a href="/format/2305.06118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeRF2: Neural Radio-Frequency Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiaopeng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+Z">Zhenlin An</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Q">Qingrui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06292" title="Abstract">arXiv:2305.06292</a> (replaced) [<a href="/pdf/2305.06292" title="Download PDF">pdf</a>, <a href="/format/2305.06292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Metrics Matter: A Better Standard for Trajectory Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weng%2C+E">Erica Weng</a>, 
<a href="/search/cs?searchtype=author&query=Hoshino%2C+H">Hana Hoshino</a>, 
<a href="/search/cs?searchtype=author&query=Ramanan%2C+D">Deva Ramanan</a>, 
<a href="/search/cs?searchtype=author&query=Kitani%2C+K">Kris Kitani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06648" title="Abstract">arXiv:2305.06648</a> (replaced) [<a href="/pdf/2305.06648" title="Download PDF">pdf</a>, <a href="/format/2305.06648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization bounds for neural ordinary differential equations and  deep residual networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Marion%2C+P">Pierre Marion</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023, 21 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07375" title="Abstract">arXiv:2305.07375</a> (replaced) [<a href="/pdf/2305.07375" title="Download PDF">pdf</a>, <a href="/format/2305.07375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jinglong Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xiao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bing Qin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Ting Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09548" title="Abstract">arXiv:2305.09548</a> (replaced) [<a href="/pdf/2305.09548" title="Download PDF">pdf</a>, <a href="/format/2305.09548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Social Dimensions of Self-Presentation in Social Media  Biographies with an Identity-based Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madani%2C+N">Navid Madani</a>, 
<a href="/search/cs?searchtype=author&query=Bandyopadhyay%2C+R">Rabiraj Bandyopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Swire-Thompson%2C+B">Briony Swire-Thompson</a>, 
<a href="/search/cs?searchtype=author&query=Yoder%2C+M+M">Michael Miller Yoder</a>, 
<a href="/search/cs?searchtype=author&query=Joseph%2C+K">Kenneth Joseph</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09656" title="Abstract">arXiv:2305.09656</a> (replaced) [<a href="/pdf/2305.09656" title="Download PDF">pdf</a>, <a href="/format/2305.09656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SatLM: Satisfiability-Aided Language Models Using Declarative Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xi Ye</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qiaochu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dillig%2C+I">Isil Dillig</a>, 
<a href="/search/cs?searchtype=author&query=Durrett%2C+G">Greg Durrett</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12952" title="Abstract">arXiv:2305.12952</a> (replaced) [<a href="/pdf/2305.12952" title="Download PDF">pdf</a>, <a href="/format/2305.12952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Capacity of Opportunistic Time-Sharing Downlink with a  Reconfigurable Intelligent Surface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Darsena%2C+D">Donatella Darsena</a>, 
<a href="/search/cs?searchtype=author&query=Verde%2C+F">Francesco Verde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE Communications Letters. Cite as: D. Darsena and F. Verde, "On the Capacity of Opportunistic Time-Sharing Downlink with a Reconfigurable Intelligent Surface," in IEEE Communications Letters, 2023, doi: 10.1109/LCOMM.2023.3323094
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Communications Letters 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14069" title="Abstract">arXiv:2305.14069</a> (replaced) [<a href="/pdf/2305.14069" title="Download PDF">pdf</a>, <a href="/format/2305.14069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Factual Consistency of Summaries with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shiqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Siyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junxian He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14133" title="Abstract">arXiv:2305.14133</a> (replaced) [<a href="/pdf/2305.14133" title="Download PDF">pdf</a>, <a href="/format/2305.14133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Mutual Information for Disentangled Representations in  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dunion%2C+M">Mhairi Dunion</a>, 
<a href="/search/cs?searchtype=author&query=McInroe%2C+T">Trevor McInroe</a>, 
<a href="/search/cs?searchtype=author&query=Luck%2C+K+S">Kevin Sebastian Luck</a>, 
<a href="/search/cs?searchtype=author&query=Hanna%2C+J+P">Josiah P. Hanna</a>, 
<a href="/search/cs?searchtype=author&query=Albrecht%2C+S+V">Stefano V. Albrecht</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference on Neural Information Processing Systems (NeurIPS), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14259" title="Abstract">arXiv:2305.14259</a> (replaced) [<a href="/pdf/2305.14259" title="Download PDF">pdf</a>, <a href="/format/2305.14259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Generate Novel Scientific Directions with Contextualized  Literature-based Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qingyun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Downey%2C+D">Doug Downey</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Hope%2C+T">Tom Hope</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages. Code and resource is available at <a href="https://github.com/EagleW/CLBD">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15020" title="Abstract">arXiv:2305.15020</a> (replaced) [<a href="/pdf/2305.15020" title="Download PDF">pdf</a>, <a href="/format/2305.15020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Multilingual Language Model Compression through Vocabulary  Trimming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ushio%2C+A">Asahi Ushio</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Camacho-Collados%2C+J">Jose Camacho-Collados</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15261" title="Abstract">arXiv:2305.15261</a> (replaced) [<a href="/pdf/2305.15261" title="Download PDF">pdf</a>, <a href="/ps/2305.15261" title="Download PostScript">ps</a>, <a href="/format/2305.15261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random periodic sampling patterns for shift-invariant spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Antezana%2C+J">Jorge Antezana</a>, 
<a href="/search/math?searchtype=author&query=Carbajal%2C+D">Diana Carbajal</a>, 
<a href="/search/math?searchtype=author&query=Romero%2C+J+L">Jos&#xe9; Luis Romero</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Functional Analysis (math.FA)</span>; Information Theory (cs.IT); Classical Analysis and ODEs (math.CA)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15394" title="Abstract">arXiv:2305.15394</a> (replaced) [<a href="/pdf/2305.15394" title="Download PDF">pdf</a>, <a href="/format/2305.15394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially-Private Decision Trees and Provable Robustness to Data  Poisoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vos%2C+D">Dani&#xeb;l Vos</a>, 
<a href="/search/cs?searchtype=author&query=Vos%2C+J">Jelle Vos</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Erkin%2C+Z">Zekeriya Erkin</a>, 
<a href="/search/cs?searchtype=author&query=Verwer%2C+S">Sicco Verwer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16319" title="Abstract">arXiv:2305.16319</a> (replaced) [<a href="/pdf/2305.16319" title="Download PDF">pdf</a>, <a href="/format/2305.16319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image as First-Order Norm+Linear Autoregression: Unveiling Mathematical  Invariance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yinpeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xiyang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dongdong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mengchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Youzuo Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17536" title="Abstract">arXiv:2305.17536</a> (replaced) [<a href="/pdf/2305.17536" title="Download PDF">pdf</a>, <a href="/format/2305.17536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Locally Identifying Coloring of Cartesian Product and Tensor Product  of Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bhyravarapu%2C+S">Sriram Bhyravarapu</a>, 
<a href="/search/math?searchtype=author&query=Kumari%2C+S">Swati Kumari</a>, 
<a href="/search/math?searchtype=author&query=Reddy%2C+I+V">I. Vinod Reddy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17924" title="Abstract">arXiv:2305.17924</a> (replaced) [<a href="/pdf/2305.17924" title="Download PDF">pdf</a>, <a href="/format/2305.17924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Second Order Asymptotics of Covert Communication over AWGN Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xinchun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+S">Shuangqing Wei</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shao-Lun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao-Ping Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19443" title="Abstract">arXiv:2305.19443</a> (replaced) [<a href="/pdf/2305.19443" title="Download PDF">pdf</a>, <a href="/format/2305.19443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OWAdapt: An adaptive loss function for deep learning using OWA operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maldonado%2C+S">Sebasti&#xe1;n Maldonado</a>, 
<a href="/search/cs?searchtype=author&query=Vairetti%2C+C">Carla Vairetti</a>, 
<a href="/search/cs?searchtype=author&query=Jara%2C+K">Katherine Jara</a>, 
<a href="/search/cs?searchtype=author&query=Carrasco%2C+M">Miguel Carrasco</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%B3pez%2C+J">Julio L&#xf3;pez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 1 figure, published
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Knowledge-based Systems 280, 111022 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19838" title="Abstract">arXiv:2305.19838</a> (replaced) [<a href="/pdf/2305.19838" title="Download PDF">pdf</a>, <a href="/format/2305.19838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relaxing the Additivity Constraints in Decentralized No-Regret  High-Dimensional Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bardou%2C+A">Anthony Bardou</a>, 
<a href="/search/cs?searchtype=author&query=Thiran%2C+P">Patrick Thiran</a>, 
<a href="/search/cs?searchtype=author&query=Begin%2C+T">Thomas Begin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00721" title="Abstract">arXiv:2306.00721</a> (replaced) [<a href="/pdf/2306.00721" title="Download PDF">pdf</a>, <a href="/format/2306.00721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iashchenko%2C+A">Anastasiia Iashchenko</a>, 
<a href="/search/cs?searchtype=author&query=Andreev%2C+P">Pavel Andreev</a>, 
<a href="/search/cs?searchtype=author&query=Shchekotov%2C+I">Ivan Shchekotov</a>, 
<a href="/search/cs?searchtype=author&query=Babaev%2C+N">Nicholas Babaev</a>, 
<a href="/search/cs?searchtype=author&query=Vetrov%2C+D">Dmitry Vetrov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Interspeech 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02010" title="Abstract">arXiv:2306.02010</a> (replaced) [<a href="/pdf/2306.02010" title="Download PDF">pdf</a>, <a href="/format/2306.02010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memorization Capacity of Multi-Head Attention in Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahdavi%2C+S">Sadegh Mahdavi</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+R">Renjie Liao</a>, 
<a href="/search/cs?searchtype=author&query=Thrampoulidis%2C+C">Christos Thrampoulidis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03410" title="Abstract">arXiv:2306.03410</a> (replaced) [<a href="/pdf/2306.03410" title="Download PDF">pdf</a>, <a href="/format/2306.03410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Simulate Tree-Branch Dynamics for Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jacob%2C+J">Jayadeep Jacob</a>, 
<a href="/search/cs?searchtype=author&query=Bandyopadhyay%2C+T">Tirthankar Bandyopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+J">Jason Williams</a>, 
<a href="/search/cs?searchtype=author&query=Borges%2C+P">Paulo Borges</a>, 
<a href="/search/cs?searchtype=author&query=Ramos%2C+F">Fabio Ramos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06323" title="Abstract">arXiv:2306.06323</a> (replaced) [<a href="/pdf/2306.06323" title="Download PDF">pdf</a>, <a href="/format/2306.06323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Joint Latent Space EBM Prior Model for Multi-layer Generator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jiali Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y+N">Ying Nian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tian Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06599" title="Abstract">arXiv:2306.06599</a> (replaced) [<a href="/pdf/2306.06599" title="Download PDF">pdf</a>, <a href="/format/2306.06599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Imbalanced Regression: Fair Uncertainty Quantification via  Probabilistic Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07951" title="Abstract">arXiv:2306.07951</a> (replaced) [<a href="/pdf/2306.07951" title="Download PDF">pdf</a>, <a href="/format/2306.07951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Questioning the Survey Responses of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dominguez-Olmedo%2C+R">Ricardo Dominguez-Olmedo</a>, 
<a href="/search/cs?searchtype=author&query=Hardt%2C+M">Moritz Hardt</a>, 
<a href="/search/cs?searchtype=author&query=Mendler-D%C3%BCnner%2C+C">Celestine Mendler-D&#xfc;nner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08762" title="Abstract">arXiv:2306.08762</a> (replaced) [<a href="/pdf/2306.08762" title="Download PDF">pdf</a>, <a href="/format/2306.08762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theoretical Hardness and Tractability of POMDPs in RL with Partial  Online State Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+M">Ming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yingbin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Shroff%2C+N">Ness Shroff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted for publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11014" title="Abstract">arXiv:2306.11014</a> (replaced) [<a href="/pdf/2306.11014" title="Download PDF">pdf</a>, <a href="/format/2306.11014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics Constrained Unsupervised Deep Learning for Rapid, High  Resolution Scanning Coherent Diffraction Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Hoidn%2C+O">Oliver Hoidn</a>, 
<a href="/search/physics?searchtype=author&query=Mishra%2C+A+A">Aashwin Ananda Mishra</a>, 
<a href="/search/physics?searchtype=author&query=Mehta%2C+A">Apurva Mehta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV); Optics (physics.optics)

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11670" title="Abstract">arXiv:2306.11670</a> (replaced) [<a href="/pdf/2306.11670" title="Download PDF">pdf</a>, <a href="/format/2306.11670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GIO: Gradient Information Optimization for Training Dataset Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Everaert%2C+D">Dante Everaert</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+C">Christopher Potts</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14041" title="Abstract">arXiv:2306.14041</a> (replaced) [<a href="/pdf/2306.14041" title="Download PDF">pdf</a>, <a href="/format/2306.14041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smoothed $f$-Divergence Distributionally Robust Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+Z">Zhenyuan Liu</a>, 
<a href="/search/math?searchtype=author&query=Van+Parys%2C+B+P+G">Bart P. G. Van Parys</a>, 
<a href="/search/math?searchtype=author&query=Lam%2C+H">Henry Lam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16335" title="Abstract">arXiv:2306.16335</a> (replaced) [<a href="/pdf/2306.16335" title="Download PDF">pdf</a>, <a href="/format/2306.16335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emulating the dynamics of complex systems using autoregressive models on  manifolds (mNARX)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sch%C3%A4r%2C+S">Styfen Sch&#xe4;r</a>, 
<a href="/search/stat?searchtype=author&query=Marelli%2C+S">Stefano Marelli</a>, 
<a href="/search/stat?searchtype=author&query=Sudret%2C+B">Bruno Sudret</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Machine Learning (cs.LG); Applications (stat.AP); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16605" title="Abstract">arXiv:2306.16605</a> (replaced) [<a href="/pdf/2306.16605" title="Download PDF">pdf</a>, <a href="/format/2306.16605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KITE: Keypoint-Conditioned Policies for Semantic Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sundaresan%2C+P">Priya Sundaresan</a>, 
<a href="/search/cs?searchtype=author&query=Belkhale%2C+S">Suneel Belkhale</a>, 
<a href="/search/cs?searchtype=author&query=Sadigh%2C+D">Dorsa Sadigh</a>, 
<a href="/search/cs?searchtype=author&query=Bohg%2C+J">Jeannette Bohg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17046" title="Abstract">arXiv:2306.17046</a> (replaced) [<a href="/pdf/2306.17046" title="Download PDF">pdf</a>, <a href="/format/2306.17046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spiking Denoising Diffusion Probabilistic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jiahang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hanzhong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Renjing Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02484" title="Abstract">arXiv:2307.02484</a> (replaced) [<a href="/pdf/2307.02484" title="Download PDF">pdf</a>, <a href="/format/2307.02484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Elastic Decision Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yueh-Hua Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hamaya%2C+M">Masashi Hamaya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03135" title="Abstract">arXiv:2307.03135</a> (replaced) [<a href="/pdf/2307.03135" title="Download PDF">pdf</a>, <a href="/format/2307.03135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distilling Large Vision-Language Model with Out-of-Distribution  Generalizability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuanlin Li</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yunhao Fang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Minghua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zhan Ling</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhuowen Tu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hao Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at International Conference on Computer Vision (ICCV) 2023. Poster at <a href="https://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03293" title="Abstract">arXiv:2307.03293</a> (replaced) [<a href="/pdf/2307.03293" title="Download PDF">pdf</a>, <a href="/format/2307.03293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CheXmask: a large-scale dataset of anatomical segmentation masks for  multi-center chest x-ray images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gaggion%2C+N">Nicol&#xe1;s Gaggion</a>, 
<a href="/search/eess?searchtype=author&query=Mosquera%2C+C">Candelaria Mosquera</a>, 
<a href="/search/eess?searchtype=author&query=Mansilla%2C+L">Lucas Mansilla</a>, 
<a href="/search/eess?searchtype=author&query=Aineseder%2C+M">Martina Aineseder</a>, 
<a href="/search/eess?searchtype=author&query=Milone%2C+D+H">Diego H. Milone</a>, 
<a href="/search/eess?searchtype=author&query=Ferrante%2C+E">Enzo Ferrante</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The CheXmask dataset is publicly available at <a href="https://physionet.org/content/chexmask-cxr-segmentation-data/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03486" title="Abstract">arXiv:2307.03486</a> (replaced) [<a href="/pdf/2307.03486" title="Download PDF">pdf</a>, <a href="/format/2307.03486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Hierarchical Achievements in Reinforcement Learning via  Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moon%2C+S">Seungyong Moon</a>, 
<a href="/search/cs?searchtype=author&query=Yeom%2C+J">Junyoung Yeom</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+B">Bumsoo Park</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H+O">Hyun Oh Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04112" title="Abstract">arXiv:2307.04112</a> (replaced) [<a href="/pdf/2307.04112" title="Download PDF">pdf</a>, <a href="/ps/2307.04112" title="Download PostScript">ps</a>, <a href="/format/2307.04112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Small Quasi-kernel conjecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Erd%C5%91s%2C+P+L">P&#xe9;ter L. Erd&#x151;s</a>, 
<a href="/search/math?searchtype=author&query=Gy%C5%91ri%2C+E">Ervin Gy&#x151;ri</a>, 
<a href="/search/math?searchtype=author&query=Mezei%2C+T+R">Tam&#xe1;s R&#xf3;bert Mezei</a>, 
<a href="/search/math?searchtype=author&query=Salia%2C+N">Nika Salia</a>, 
<a href="/search/math?searchtype=author&query=Tyomkyn%2C+M">Mykhaylo Tyomkyn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07763" title="Abstract">arXiv:2307.07763</a> (replaced) [<a href="/pdf/2307.07763" title="Download PDF">pdf</a>, <a href="/format/2307.07763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile  Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+K">Ke Cao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruiping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ze Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+K">Kunyu Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Junwei Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+Z">Zhifeng Teng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kailun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Stiefelhagen%2C+R">Rainer Stiefelhagen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ROBIO 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07864" title="Abstract">arXiv:2307.07864</a> (replaced) [<a href="/pdf/2307.07864" title="Download PDF">pdf</a>, <a href="/format/2307.07864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CIDER: Context sensitive sentiment analysis for short-form text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Young%2C+J+C">James C. Young</a>, 
<a href="/search/cs?searchtype=author&query=Arthur%2C+R">Rudy Arthur</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+H+T+P">Hywel T.P. Williams</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 2 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09855" title="Abstract">arXiv:2307.09855</a> (replaced) [<a href="/pdf/2307.09855" title="Download PDF">pdf</a>, <a href="/ps/2307.09855" title="Download PostScript">ps</a>, <a href="/format/2307.09855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-thread critical sections and efficient dynamic race prediction  methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sulzmann%2C+M">Martin Sulzmann</a>, 
<a href="/search/cs?searchtype=author&query=Thiemann%2C+P">Peter Thiemann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revised POPL'24 submission. 1. WCP is sound and show that WCP soundness proof can be adapted. 2. Cross-thread critical sections arise in practice, though the impact is not drastic. This in line with other works (like WCP) that advance the state of the art
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12664" title="Abstract">arXiv:2307.12664</a> (replaced) [<a href="/pdf/2307.12664" title="Download PDF">pdf</a>, <a href="/format/2307.12664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SafeSteps: Learning Safer Footstep Planning Policies for Legged Robots  via Model-Based Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Omar%2C+S">Shafeef Omar</a>, 
<a href="/search/cs?searchtype=author&query=Amatucci%2C+L">Lorenzo Amatucci</a>, 
<a href="/search/cs?searchtype=author&query=Barasuol%2C+V">Victor Barasuol</a>, 
<a href="/search/cs?searchtype=author&query=Turrisi%2C+G">Giulio Turrisi</a>, 
<a href="/search/cs?searchtype=author&query=Semini%2C+C">Claudio Semini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the 2023 IEEE-RAS International Conference on Humanoid Robots (Humanoids)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13826" title="Abstract">arXiv:2307.13826</a> (replaced) [<a href="/pdf/2307.13826" title="Download PDF">pdf</a>, <a href="/ps/2307.13826" title="Download PostScript">ps</a>, <a href="/format/2307.13826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lecture Notes on Spectral Independence and Bases of a Matroid:  Local-to-Global and Trickle-Down from a Markov Chain Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stefankovic%2C+D">Daniel Stefankovic</a>, 
<a href="/search/cs?searchtype=author&query=Vigoda%2C+E">Eric Vigoda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revised to include random bases of a matroid, including the proof of the associated Trickle-Down Theorem
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03807" title="Abstract">arXiv:2308.03807</a> (replaced) [<a href="/pdf/2308.03807" title="Download PDF">pdf</a>, <a href="/format/2308.03807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS  Image Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fan%2C+X">Xiaohong Fan</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+Y">Yin Yang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+K">Ke Chen</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+Y">Yujie Feng</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+J">Jianping Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages,our source codes are available at <a href="https://github.com/fanxiaohong/Nest-DGIL">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> This work is published in IEEE Transactions on Computational
  Imaging, vol. 9, pp. 819-833, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03853" title="Abstract">arXiv:2308.03853</a> (replaced) [<a href="/pdf/2308.03853" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring zero-shot capability of large language models in inferences  from medical oncology notes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sushil%2C+M">Madhumita Sushil</a>, 
<a href="/search/cs?searchtype=author&query=Kennedy%2C+V+E">Vanessa E. Kennedy</a>, 
<a href="/search/cs?searchtype=author&query=Mandair%2C+D">Divneet Mandair</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+B+Y">Brenda Y. Miao</a>, 
<a href="/search/cs?searchtype=author&query=Zack%2C+T">Travis Zack</a>, 
<a href="/search/cs?searchtype=author&query=Butte%2C+A+J">Atul J. Butte</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Source code available at: <a href="https://github.com/MadhumitaSushil/OncLLMExtraction">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03998" title="Abstract">arXiv:2308.03998</a> (replaced) [<a href="/pdf/2308.03998" title="Download PDF">pdf</a>, <a href="/format/2308.03998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time Strawberry Detection Based on Improved YOLOv5s Architecture  for Robotic Harvesting in open-field environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zixuan He</a> (1) (2), 
<a href="/search/cs?searchtype=author&query=Khanal%2C+S+R">Salik Ram Khanal</a> (1) (2), 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a> (3), 
<a href="/search/cs?searchtype=author&query=Karkee%2C+M">Manoj Karkee</a> (1) (2), 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qin Zhang</a> (1) (2) ((1) Center for Precision and Automated Agricultural Systems, Washington State University, (2) Department of Biological Systems Engineering, Washington State University, (3) Department of Agricultural and Biological Engineering, Mississippi State University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages; 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04102" title="Abstract">arXiv:2308.04102</a> (replaced) [<a href="/pdf/2308.04102" title="Download PDF">pdf</a>, <a href="/format/2308.04102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asynchronous Evolution of Deep Neural Network Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jason Liang</a>, 
<a href="/search/cs?searchtype=author&query=Shahrzad%2C+H">Hormoz Shahrzad</a>, 
<a href="/search/cs?searchtype=author&query=Miikkulainen%2C+R">Risto Miikkulainen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04263" title="Abstract">arXiv:2308.04263</a> (replaced) [<a href="/pdf/2308.04263" title="Download PDF">pdf</a>, <a href="/format/2308.04263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cagatan%2C+O+V">Omer Veysel Cagatan</a>, 
<a href="/search/cs?searchtype=author&query=Akgun%2C+B">Baris Akgun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACML 2023, Camera-Ready Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05035" title="Abstract">arXiv:2308.05035</a> (replaced) [<a href="/pdf/2308.05035" title="Download PDF">pdf</a>, <a href="/format/2308.05035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expert load matters: operating networks at high accuracy and low manual  effort
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sangalli%2C+S">Sara Sangalli</a>, 
<a href="/search/cs?searchtype=author&query=Erdil%2C+E">Ertunc Erdil</a>, 
<a href="/search/cs?searchtype=author&query=Konukoglu%2C+E">Ender Konukoglu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05636" title="Abstract">arXiv:2308.05636</a> (replaced) [<a href="/pdf/2308.05636" title="Download PDF">pdf</a>, <a href="/format/2308.05636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Homomorphic Encryption Framework for Privacy-Preserving Spiking Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nikfam%2C+F">Farzad Nikfam</a>, 
<a href="/search/cs?searchtype=author&query=Casaburi%2C+R">Raffaele Casaburi</a>, 
<a href="/search/cs?searchtype=author&query=Marchisio%2C+A">Alberto Marchisio</a>, 
<a href="/search/cs?searchtype=author&query=Martina%2C+M">Maurizio Martina</a>, 
<a href="/search/cs?searchtype=author&query=Shafique%2C+M">Muhammad Shafique</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Information 2023, 14, 537
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08469" title="Abstract">arXiv:2308.08469</a> (replaced) [<a href="/pdf/2308.08469" title="Download PDF">pdf</a>, <a href="/format/2308.08469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with  Pre-Trained LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+C">Ching Chang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Wen-Chih Peng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tien-Fu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is currently under review. The code will be made available upon acceptance
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09106" title="Abstract">arXiv:2308.09106</a> (replaced) [<a href="/pdf/2308.09106" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Closed Loop Control of G2V/V2G Action Using Model Predictive  Controller
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Singh%2C+S+V+P">Satya Vikram Pratap Singh</a>, 
<a href="/search/eess?searchtype=author&query=Kamila%2C+S">Siddharth Kamila</a>, 
<a href="/search/eess?searchtype=author&query=Agnihotri%2C+P">Prashanth Agnihotri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> \c{opyright}2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11534" title="Abstract">arXiv:2308.11534</a> (replaced) [<a href="/pdf/2308.11534" title="Download PDF">pdf</a>, <a href="/format/2308.11534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PlatoLM: Teaching LLMs via a Socratic Questioning User Simulator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+C">Chuyi Kong</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yaxin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+F">Feng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benyou Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11606" title="Abstract">arXiv:2308.11606</a> (replaced) [<a href="/pdf/2308.11606" title="Download PDF">pdf</a>, <a href="/format/2308.11606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StoryBench: A Multifaceted Benchmark for Continuous Story Visualization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bugliarello%2C+E">Emanuele Bugliarello</a>, 
<a href="/search/cs?searchtype=author&query=Moraldo%2C+H">Hernan Moraldo</a>, 
<a href="/search/cs?searchtype=author&query=Villegas%2C+R">Ruben Villegas</a>, 
<a href="/search/cs?searchtype=author&query=Babaeizadeh%2C+M">Mohammad Babaeizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Saffar%2C+M+T">Mohammad Taghi Saffar</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Erhan%2C+D">Dumitru Erhan</a>, 
<a href="/search/cs?searchtype=author&query=Ferrari%2C+V">Vittorio Ferrari</a>, 
<a href="/search/cs?searchtype=author&query=Kindermans%2C+P">Pieter-Jan Kindermans</a>, 
<a href="/search/cs?searchtype=author&query=Voigtlaender%2C+P">Paul Voigtlaender</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS D&amp;B 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12243" title="Abstract">arXiv:2308.12243</a> (replaced) [<a href="/pdf/2308.12243" title="Download PDF">pdf</a>, <a href="/format/2308.12243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Objective Optimization for Sparse Deep Neural Network Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hotegni%2C+S+S">S. S. Hotegni</a>, 
<a href="/search/cs?searchtype=author&query=Peitz%2C+S">S. Peitz</a>, 
<a href="/search/cs?searchtype=author&query=Berkemeier%2C+M">M. Berkemeier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12364" title="Abstract">arXiv:2308.12364</a> (replaced) [<a href="/pdf/2308.12364" title="Download PDF">pdf</a>, <a href="/format/2308.12364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Saliency-based Video Summarization for Face Anti-spoofing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muhammad%2C+U">Usman Muhammad</a>, 
<a href="/search/cs?searchtype=author&query=Oussalah%2C+M">Mourad Oussalah</a>, 
<a href="/search/cs?searchtype=author&query=Laaksonen%2C+J">Jorma Laaksonen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12435" title="Abstract">arXiv:2308.12435</a> (replaced) [<a href="/pdf/2308.12435" title="Download PDF">pdf</a>, <a href="/format/2308.12435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterising representation dynamics in recurrent neural networks for  object recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thorat%2C+S">Sushrut Thorat</a>, 
<a href="/search/cs?searchtype=author&query=Doerig%2C+A">Adrien Doerig</a>, 
<a href="/search/cs?searchtype=author&query=Kietzmann%2C+T+C">Tim C. Kietzmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures; revision of our Conference on Cognitive Computational Neuroscience (CCN) 2023 paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13422" title="Abstract">arXiv:2308.13422</a> (replaced) [<a href="/pdf/2308.13422" title="Download PDF">pdf</a>, <a href="/format/2308.13422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QKSAN: A Quantum Kernel Self-Attention Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Zhao%2C+R">Ren-Xin Zhao</a>, 
<a href="/search/quant-ph?searchtype=author&query=Shi%2C+J">Jinjing Shi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+X">Xuelong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14311" title="Abstract">arXiv:2308.14311</a> (replaced) [<a href="/pdf/2308.14311" title="Download PDF">pdf</a>, <a href="/format/2308.14311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spread Control Method on Unknown Networks Based on Hierarchical  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+W">Wenxiang Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhanjiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H+V">H.Vicky Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16198" title="Abstract">arXiv:2308.16198</a> (replaced) [<a href="/pdf/2308.16198" title="Download PDF">pdf</a>, <a href="/format/2308.16198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Collaborative Information Dissemination with Graph-based  Multi-Agent Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galliera%2C+R">Raffaele Galliera</a>, 
<a href="/search/cs?searchtype=author&query=Venable%2C+K+B">Kristen Brent Venable</a>, 
<a href="/search/cs?searchtype=author&query=Bassani%2C+M">Matteo Bassani</a>, 
<a href="/search/cs?searchtype=author&query=Suri%2C+N">Niranjan Suri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages (2 of Supplementary Materials), 4 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00480" title="Abstract">arXiv:2309.00480</a> (replaced) [<a href="/pdf/2309.00480" title="Download PDF">pdf</a>, <a href="/format/2309.00480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning-based NLOS Detection and Uncertainty Prediction of GNSS  Observations with Transformer-Enhanced LSTM Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhanxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Vallery%2C+H">Heike Vallery</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for the IEEE ITSC2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00848" title="Abstract">arXiv:2309.00848</a> (replaced) [<a href="/pdf/2309.00848" title="Download PDF">pdf</a>, <a href="/format/2309.00848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+N+S">Nazmus Sakib Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Noor%2C+S+S">Saad Sakib Noor</a>, 
<a href="/search/cs?searchtype=author&query=Sikder%2C+A+I+S">Ashraful Islam Shanto Sikder</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+A">Abhijit Paul</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01813" title="Abstract">arXiv:2309.01813</a> (replaced) [<a href="/pdf/2309.01813" title="Download PDF">pdf</a>, <a href="/format/2309.01813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse Dynamics Trajectory Optimization for Contact-Implicit Model  Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurtz%2C+V">Vince Kurtz</a>, 
<a href="/search/cs?searchtype=author&query=Castro%2C+A">Alejandro Castro</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96nol%2C+A+%C3%96">Aykut &#xd6;zg&#xfc;n &#xd6;nol</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hai Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02092" title="Abstract">arXiv:2309.02092</a> (replaced) [<a href="/pdf/2309.02092" title="Download PDF">pdf</a>, <a href="/format/2309.02092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Where are We in Event-centric Emotion Analysis? Bridging Emotion Role  Labeling and Appraisal-based Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klinger%2C+R">Roman Klinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to the Big Picture Workshop (<a href="https://bigpictureworkshop.com/">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02285" title="Abstract">arXiv:2309.02285</a> (replaced) [<a href="/pdf/2309.02285" title="Download PDF">pdf</a>, <a href="/format/2309.02285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PromptTTS 2: Describing and Generating Voices with Text Prompt
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Leng%2C+Y">Yichong Leng</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+Z">Zhifang Guo</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+K">Kai Shen</a>, 
<a href="/search/eess?searchtype=author&query=Tan%2C+X">Xu Tan</a>, 
<a href="/search/eess?searchtype=author&query=Ju%2C+Z">Zeqian Ju</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yanqing Liu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yufei Liu</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+D">Dongchao Yang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+L">Leying Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+K">Kaitao Song</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+L">Lei He</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xiang-Yang Li</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+S">Sheng Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Qin%2C+T">Tao Qin</a>, 
<a href="/search/eess?searchtype=author&query=Bian%2C+J">Jiang Bian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Demo page: <a href="https://speechresearch.github.io/prompttts2">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03004" title="Abstract">arXiv:2309.03004</a> (replaced) [<a href="/pdf/2309.03004" title="Download PDF">pdf</a>, <a href="/format/2309.03004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Theoretical Explanation of Activation Sparsity through Flat Minima and  Adversarial Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+Z">Ze Peng</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+L">Lei Qi</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yinghuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03084" title="Abstract">arXiv:2309.03084</a> (replaced) [<a href="/pdf/2309.03084" title="Download PDF">pdf</a>, <a href="/format/2309.03084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pure Monte Carlo Counterfactual Regret Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Ju Qi</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Ting Feng</a>, 
<a href="/search/cs?searchtype=author&query=Hei%2C+F">Falun Hei</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zhemei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yunfeng Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04370" title="Abstract">arXiv:2309.04370</a> (replaced) [<a href="/pdf/2309.04370" title="Download PDF">pdf</a>, <a href="/format/2309.04370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=DeFazio%2C+D">David DeFazio</a>, 
<a href="/search/cs?searchtype=author&query=Hirota%2C+E">Eisuke Hirota</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiqi Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CoRL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04946" title="Abstract">arXiv:2309.04946</a> (replaced) [<a href="/pdf/2309.04946" title="Download PDF">pdf</a>, <a href="/format/2309.04946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gan%2C+Y">Yuan Gan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zongxin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xihang Yue</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lingyun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV 2023. Project page: <a href="https://yuangan.github.io/eat/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05173" title="Abstract">arXiv:2309.05173</a> (replaced) [<a href="/pdf/2309.05173" title="Download PDF">pdf</a>, <a href="/format/2309.05173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhengxiang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Lipani%2C+A">Aldo Lipani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/ZhengxiangShi/DePT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05925" title="Abstract">arXiv:2309.05925</a> (replaced) [<a href="/pdf/2309.05925" title="Download PDF">pdf</a>, <a href="/format/2309.05925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Regularized Sparse Logistic Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kai Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICDM2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06188" title="Abstract">arXiv:2309.06188</a> (replaced) [<a href="/pdf/2309.06188" title="Download PDF">pdf</a>, <a href="/format/2309.06188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computer Vision Pipeline for Automated Antarctic Krill Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gudelis%2C+M">Mazvydas Gudelis</a>, 
<a href="/search/cs?searchtype=author&query=Mackiewicz%2C+M">Michal Mackiewicz</a>, 
<a href="/search/cs?searchtype=author&query=Bremner%2C+J">Julie Bremner</a>, 
<a href="/search/cs?searchtype=author&query=Fielding%2C+S">Sophie Fielding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to MVEO @ BMVC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09129" title="Abstract">arXiv:2309.09129</a> (replaced) [<a href="/pdf/2309.09129" title="Download PDF">pdf</a>, <a href="/ps/2309.09129" title="Download PostScript">ps</a>, <a href="/format/2309.09129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $L^1$ Estimation: On the Optimality of Linear Estimators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Barnes%2C+L+P">Leighton P. Barnes</a>, 
<a href="/search/math?searchtype=author&query=Dytso%2C+A">Alex Dytso</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+J">Jingbo Liu</a>, 
<a href="/search/math?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> updated some notation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10691" title="Abstract">arXiv:2309.10691</a> (replaced) [<a href="/pdf/2309.10691" title="Download PDF">pdf</a>, <a href="/format/2309.10691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language  Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xingyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiateng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yangyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lifan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available on our project website: <a href="https://xingyaoww.github.io/mint-bench">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10966" title="Abstract">arXiv:2309.10966</a> (replaced) [<a href="/pdf/2309.10966" title="Download PDF">pdf</a>, <a href="/format/2309.10966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MBR and QE Finetuning: Training-time Distillation of the Best and Most  Expensive Decoding Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Finkelstein%2C+M">Mara Finkelstein</a>, 
<a href="/search/cs?searchtype=author&query=Naskar%2C+S">Subhajit Naskar</a>, 
<a href="/search/cs?searchtype=author&query=Mirzazadeh%2C+M">Mehdi Mirzazadeh</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+A">Apurva Shah</a>, 
<a href="/search/cs?searchtype=author&query=Freitag%2C+M">Markus Freitag</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11523" title="Abstract">arXiv:2309.11523</a> (replaced) [<a href="/pdf/2309.11523" title="Download PDF">pdf</a>, <a href="/format/2309.11523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RMT: Retentive Networks Meet Vision Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Q">Qihang Fan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Huaibo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingrui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongmin Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ran He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The work is still in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12053" title="Abstract">arXiv:2309.12053</a> (replaced) [<a href="/pdf/2309.12053" title="Download PDF">pdf</a>, <a href="/format/2309.12053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AceGPT, Localizing Large Language Models in Arabic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Huang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jianqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xuening Sun</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dingjie Song</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhihong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Alharthi%2C+A">Abdulmohsen Alharthi</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bang An</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziche Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianquan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benyou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruoyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haizhou Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jinchao Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://github.com/FreedomIntelligence/AceGPT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12374" title="Abstract">arXiv:2309.12374</a> (replaced) [<a href="/pdf/2309.12374" title="Download PDF">pdf</a>, <a href="/ps/2309.12374" title="Download PostScript">ps</a>, <a href="/format/2309.12374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rational Aversion to Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Neth%2C+S">Sven Neth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Forthcoming in The British Journal for the Philosophy of Science
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Statistics (stat.OT)</span>; Computer Science and Game Theory (cs.GT); Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13336" title="Abstract">arXiv:2309.13336</a> (replaced) [<a href="/pdf/2309.13336" title="Download PDF">pdf</a>, <a href="/format/2309.13336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedDrive v2: an Analysis of the Impact of Label Skewness in Federated  Semantic Segmentation for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%C3%AC%2C+E">Eros Fan&#xec;</a>, 
<a href="/search/cs?searchtype=author&query=Ciccone%2C+M">Marco Ciccone</a>, 
<a href="/search/cs?searchtype=author&query=Caputo%2C+B">Barbara Caputo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5th Italian Conference on Robotics and Intelligent Machines (I-RIM) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14613" title="Abstract">arXiv:2309.14613</a> (replaced) [<a href="/pdf/2309.14613" title="Download PDF">pdf</a>, <a href="/format/2309.14613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design of a Superconducting Multiflux Non-Destructive Readout Memory  Unit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ucpinar%2C+B+Z">Beyza Zeynep Ucpinar</a>, 
<a href="/search/cs?searchtype=author&query=Kopur%2C+Y">Yasemin Kopur</a>, 
<a href="/search/cs?searchtype=author&query=Karamuftuoglu%2C+M+A">Mustafa Altay Karamuftuoglu</a>, 
<a href="/search/cs?searchtype=author&query=Razmkhah%2C+S">Sasan Razmkhah</a>, 
<a href="/search/cs?searchtype=author&query=Pedram%2C+M">Massoud Pedram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Applied Physics (physics.app-ph)

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14616" title="Abstract">arXiv:2309.14616</a> (replaced) [<a href="/pdf/2309.14616" title="Download PDF">pdf</a>, <a href="/format/2309.14616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized  Device Coordinates Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiawei Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chuming Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Keqiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yingjie Cai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongsheng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICCV 2023. Project page: <a href="https://jiawei-yao0812.github.io/NDC-Scene/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15395" title="Abstract">arXiv:2309.15395</a> (replaced) [<a href="/pdf/2309.15395" title="Download PDF">pdf</a>, <a href="/format/2309.15395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zihan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Honghao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+L">Lei Ying</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15505" title="Abstract">arXiv:2309.15505</a> (replaced) [<a href="/pdf/2309.15505" title="Download PDF">pdf</a>, <a href="/format/2309.15505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite Scalar Quantization: VQ-VAE Made Simple
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mentzer%2C+F">Fabian Mentzer</a>, 
<a href="/search/cs?searchtype=author&query=Minnen%2C+D">David Minnen</a>, 
<a href="/search/cs?searchtype=author&query=Agustsson%2C+E">Eirikur Agustsson</a>, 
<a href="/search/cs?searchtype=author&query=Tschannen%2C+M">Michael Tschannen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/google-research/google-research/tree/master/fsq">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16292" title="Abstract">arXiv:2309.16292</a> (replaced) [<a href="/pdf/2309.16292" title="Download PDF">pdf</a>, <a href="/format/2309.16292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+L">Licheng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+D">Daocheng Fu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xinyu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Tao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+P">Pinlong Cai</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+M">Min Dou</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Botian Shi</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Liang He</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16396" title="Abstract">arXiv:2309.16396</a> (replaced) [<a href="/pdf/2309.16396" title="Download PDF">pdf</a>, <a href="/format/2309.16396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey of Document-level Relation Extraction (2016-2023)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delaunay%2C+J">Julien Delaunay</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+H+T+H">Hanh Thi Hong Tran</a>, 
<a href="/search/cs?searchtype=author&query=Gonz%C3%A1lez-Gallardo%2C+C">Carlos-Emiliano Gonz&#xe1;lez-Gallardo</a>, 
<a href="/search/cs?searchtype=author&query=Bordea%2C+G">Georgeta Bordea</a>, 
<a href="/search/cs?searchtype=author&query=Sidere%2C+N">Nicolas Sidere</a>, 
<a href="/search/cs?searchtype=author&query=Doucet%2C+A">Antoine Doucet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16584" title="Abstract">arXiv:2309.16584</a> (replaced) [<a href="/pdf/2309.16584" title="Download PDF">pdf</a>, <a href="/format/2309.16584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Design Toolbox for the Development of Collaborative Distributed  Machine Learning Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">David Jin</a>, 
<a href="/search/cs?searchtype=author&query=Kannengie%C3%9Fer%2C+N">Niclas Kannengie&#xdf;er</a>, 
<a href="/search/cs?searchtype=author&query=Rank%2C+S">Sascha Rank</a>, 
<a href="/search/cs?searchtype=author&query=Sunyaev%2C+A">Ali Sunyaev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Emerging Technologies (cs.ET); Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17319" title="Abstract">arXiv:2309.17319</a> (replaced) [<a href="/pdf/2309.17319" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building Privacy-Preserving and Secure Geospatial Artificial  Intelligence Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rao%2C+J">Jinmeng Rao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Song Gao</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+G">Gengchen Mai</a>, 
<a href="/search/cs?searchtype=author&query=Janowicz%2C+K">Krzysztof Janowicz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1 figure
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ACM SIGSPATIAL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00111" title="Abstract">arXiv:2310.00111</a> (replaced) [<a href="/pdf/2310.00111" title="Download PDF">pdf</a>, <a href="/format/2310.00111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory-efficient compression of $\mathcal{DH}^2$-matrices for  high-frequency problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=B%C3%B6rm%2C+S">Steffen B&#xf6;rm</a>, 
<a href="/search/math?searchtype=author&query=Henningsen%2C+J">Janne Henningsen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00177" title="Abstract">arXiv:2310.00177</a> (replaced) [<a href="/pdf/2310.00177" title="Download PDF">pdf</a>, <a href="/format/2310.00177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann  Boundary Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lan%2C+K+W">Kai Weixian Lan</a>, 
<a href="/search/math?searchtype=author&query=Gueidon%2C+E">Elias Gueidon</a>, 
<a href="/search/math?searchtype=author&query=Kaneda%2C+A">Ayano Kaneda</a>, 
<a href="/search/math?searchtype=author&query=Panetta%2C+J">Julian Panetta</a>, 
<a href="/search/math?searchtype=author&query=Teran%2C+J">Joseph Teran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00327" title="Abstract">arXiv:2310.00327</a> (replaced) [<a href="/pdf/2310.00327" title="Download PDF">pdf</a>, <a href="/format/2310.00327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memorization with neural nets: going beyond the worst case
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dirksen%2C+S">Sjoerd Dirksen</a>, 
<a href="/search/stat?searchtype=author&query=Finke%2C+P">Patrick Finke</a>, 
<a href="/search/stat?searchtype=author&query=Genzel%2C+M">Martin Genzel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00656" title="Abstract">arXiv:2310.00656</a> (replaced) [<a href="/pdf/2310.00656" title="Download PDF">pdf</a>, <a href="/format/2310.00656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEGO-Prover: Neural Theorem Proving with Growing Libraries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xin%2C+H">Huajian Xin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chuanyang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qingxing Cao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yinya Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jing Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Han Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jian Yin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaodan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+H">Heng Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00737" title="Abstract">arXiv:2310.00737</a> (replaced) [<a href="/pdf/2310.00737" title="Download PDF">pdf</a>, <a href="/format/2310.00737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenAI Against Humanity: Nefarious Applications of Generative Artificial  Intelligence and Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to CACM (Viewpoint)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00847" title="Abstract">arXiv:2310.00847</a> (replaced) [<a href="/pdf/2310.00847" title="Download PDF">pdf</a>, <a href="/format/2310.00847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Pre-trained Networks Detect Familiar Out-of-Distribution Data?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miyai%2C+A">Atsuyuki Miyai</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Irie%2C+G">Go Irie</a>, 
<a href="/search/cs?searchtype=author&query=Aizawa%2C+K">Kiyoharu Aizawa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01221" title="Abstract">arXiv:2310.01221</a> (replaced) [<a href="/pdf/2310.01221" title="Download PDF">pdf</a>, <a href="/ps/2310.01221" title="Download PostScript">ps</a>, <a href="/format/2310.01221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlocal diffusion model with maximum principle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shi%2C+Z">Zuoqiang Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01649" title="Abstract">arXiv:2310.01649</a> (replaced) [<a href="/pdf/2310.01649" title="Download PDF">pdf</a>, <a href="/format/2310.01649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Training Derivative-Constrained Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lo%2C+K">KaiChieh Lo</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Daniel Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01662" title="Abstract">arXiv:2310.01662</a> (replaced) [<a href="/pdf/2310.01662" title="Download PDF">pdf</a>, <a href="/format/2310.01662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SYRAC: Synthesize, Rank, and Count
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%27Alessandro%2C+A">Adriano D&#x27;Alessandro</a>, 
<a href="/search/cs?searchtype=author&query=Mahdavi-Amiri%2C+A">Ali Mahdavi-Amiri</a>, 
<a href="/search/cs?searchtype=author&query=Hamarneh%2C+G">Ghassan Hamarneh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01889" title="Abstract">arXiv:2310.01889</a> (replaced) [<a href="/pdf/2310.01889" title="Download PDF">pdf</a>, <a href="/format/2310.01889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ring Attention with Blockwise Transformers for Near-Infinite Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zaharia%2C+M">Matei Zaharia</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01917" title="Abstract">arXiv:2310.01917</a> (replaced) [<a href="/pdf/2310.01917" title="Download PDF">pdf</a>, <a href="/format/2310.01917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Evaluation Framework: Best Practices for Human Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bojic%2C+I">Iva Bojic</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jessica Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+S+Y">Si Yuan Chang</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+Q+C">Qi Chwen Ong</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>, 
<a href="/search/cs?searchtype=author&query=Car%2C+J">Josip Car</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02244" title="Abstract">arXiv:2310.02244</a> (replaced) [<a href="/pdf/2310.02244" title="Download PDF">pdf</a>, <a href="/format/2310.02244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Greg Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dingli Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hayou%2C+S">Soufiane Hayou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02622" title="Abstract">arXiv:2310.02622</a> (replaced) [<a href="/pdf/2310.02622" title="Download PDF">pdf</a>, <a href="/ps/2310.02622" title="Download PostScript">ps</a>, <a href="/format/2310.02622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral vs Energy Efficiency in 6G: Impact of the Receiver Front-End
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lozano%2C+A">Angel Lozano</a>, 
<a href="/search/cs?searchtype=author&query=Rangan%2C+S">Sundeep Rangan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02772" title="Abstract">arXiv:2310.02772</a> (replaced) [<a href="/pdf/2310.02772" title="Download PDF">pdf</a>, <a href="/format/2310.02772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spike Accumulation Forwarding for Effective Training of Spiking Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saiin%2C+R">Ryuji Saiin</a>, 
<a href="/search/cs?searchtype=author&query=Shirakawa%2C+T">Tomoya Shirakawa</a>, 
<a href="/search/cs?searchtype=author&query=Yoshihara%2C+S">Sota Yoshihara</a>, 
<a href="/search/cs?searchtype=author&query=Sawada%2C+Y">Yoshihide Sawada</a>, 
<a href="/search/cs?searchtype=author&query=Kusumoto%2C+H">Hiroyuki Kusumoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures, Appendix:8 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03128" title="Abstract">arXiv:2310.03128</a> (replaced) [<a href="/pdf/2310.03128" title="Download PDF">pdf</a>, <a href="/format/2310.03128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MetaTool Benchmark for Large Language Models: Deciding Whether to Use  Tools and Which to Use
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yue Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiawen Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Chenrui Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Siyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qihui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+N+Z">Neil Zhenqiang Gong</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03131" title="Abstract">arXiv:2310.03131</a> (replaced) [<a href="/pdf/2310.03131" title="Download PDF">pdf</a>, <a href="/ps/2310.03131" title="Download PostScript">ps</a>, <a href="/format/2310.03131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Axiomatic Aggregations of Abductive Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Biradar%2C+G">Gagan Biradar</a>, 
<a href="/search/cs?searchtype=author&query=Izza%2C+Y">Yacine Izza</a>, 
<a href="/search/cs?searchtype=author&query=Lobo%2C+E">Elita Lobo</a>, 
<a href="/search/cs?searchtype=author&query=Viswanathan%2C+V">Vignesh Viswanathan</a>, 
<a href="/search/cs?searchtype=author&query=Zick%2C+Y">Yair Zick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03270" title="Abstract">arXiv:2310.03270</a> (replaced) [<a href="/pdf/2310.03270" title="Download PDF">pdf</a>, <a href="/format/2310.03270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yefei He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Weijia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+B">Bohan Zhuang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04099" title="Abstract">arXiv:2310.04099</a> (replaced) [<a href="/pdf/2310.04099" title="Download PDF">pdf</a>, <a href="/format/2310.04099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ClusVPR: Efficient Visual Place Recognition with Clustering-based  Weighted Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yifan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shamsolmoali%2C+P">Pourya Shamsolmoali</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jie Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04413" title="Abstract">arXiv:2310.04413</a> (replaced) [<a href="/pdf/2310.04413" title="Download PDF">pdf</a>, <a href="/format/2310.04413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced  Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+Z">Zhang-Wei Hong</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Aviral Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Karnik%2C+S">Sathwik Karnik</a>, 
<a href="/search/cs?searchtype=author&query=Bhandwaldar%2C+A">Abhishek Bhandwaldar</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+A">Akash Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Pajarinen%2C+J">Joni Pajarinen</a>, 
<a href="/search/cs?searchtype=author&query=Laroche%2C+R">Romain Laroche</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Abhishek Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Pulkit Agrawal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted NeurIPS 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04472" title="Abstract">arXiv:2310.04472</a> (replaced) [<a href="/pdf/2310.04472" title="Download PDF">pdf</a>, <a href="/format/2310.04472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective Slogan Generation with Noise Perturbation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jongeun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">MinChung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taehwan Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in CIKM 2023 short paper <a href="https://github.com/joannekim0420/SloganGeneration">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04610" title="Abstract">arXiv:2310.04610</a> (replaced) [<a href="/pdf/2310.04610" title="Download PDF">pdf</a>, <a href="/format/2310.04610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery  through Sophisticated AI System Technologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+S+L">Shuaiwen Leon Song</a>, 
<a href="/search/cs?searchtype=author&query=Kruft%2C+B">Bonnie Kruft</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Minjia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Conglong Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shiyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chengming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+M">Masahiro Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaoxia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Rasley%2C+J">Jeff Rasley</a>, 
<a href="/search/cs?searchtype=author&query=Awan%2C+A+A">Ammar Ahmad Awan</a>, 
<a href="/search/cs?searchtype=author&query=Holmes%2C+C">Connor Holmes</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+M">Martin Cai</a>, 
<a href="/search/cs?searchtype=author&query=Ghanem%2C+A">Adam Ghanem</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhongzhu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuxiong He</a>, 
<a href="/search/cs?searchtype=author&query=Luferenko%2C+P">Pete Luferenko</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+D">Divya Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Weyn%2C+J">Jonathan Weyn</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruixiong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Klocek%2C+S">Sylwester Klocek</a>, 
<a href="/search/cs?searchtype=author&query=Vragov%2C+V">Volodymyr Vragov</a>, 
<a href="/search/cs?searchtype=author&query=AlQuraishi%2C+M">Mohammed AlQuraishi</a>, 
<a href="/search/cs?searchtype=author&query=Ahdritz%2C+G">Gustaf Ahdritz</a>, 
<a href="/search/cs?searchtype=author&query=Floristean%2C+C">Christina Floristean</a>, 
<a href="/search/cs?searchtype=author&query=Negri%2C+C">Cristina Negri</a>, 
<a href="/search/cs?searchtype=author&query=Kotamarthi%2C+R">Rao Kotamarthi</a>, 
<a href="/search/cs?searchtype=author&query=Vishwanath%2C+V">Venkatram Vishwanath</a>, 
<a href="/search/cs?searchtype=author&query=Ramanathan%2C+A">Arvind Ramanathan</a>, 
<a href="/search/cs?searchtype=author&query=Foreman%2C+S">Sam Foreman</a>, 
<a href="/search/cs?searchtype=author&query=Hippe%2C+K">Kyle Hippe</a>, 
<a href="/search/cs?searchtype=author&query=Arcomano%2C+T">Troy Arcomano</a>, 
<a href="/search/cs?searchtype=author&query=Maulik%2C+R">Romit Maulik</a>, 
<a href="/search/cs?searchtype=author&query=Zvyagin%2C+M">Maxim Zvyagin</a>, 
<a href="/search/cs?searchtype=author&query=Brace%2C+A">Alexander Brace</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bohorquez%2C+C+O">Cindy Orozco Bohorquez</a>, 
<a href="/search/cs?searchtype=author&query=Clyde%2C+A">Austin Clyde</a>, 
<a href="/search/cs?searchtype=author&query=Kale%2C+B">Bharat Kale</a>, 
<a href="/search/cs?searchtype=author&query=Perez-Rivera%2C+D">Danilo Perez-Rivera</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Heng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Mann%2C+C+M">Carla M. Mann</a>, 
<a href="/search/cs?searchtype=author&query=Irvin%2C+M">Michael Irvin</a>, 
<a href="/search/cs?searchtype=author&query=Pauloski%2C+J+G">J. Gregory Pauloski</a>, 
<a href="/search/cs?searchtype=author&query=Ward%2C+L">Logan Ward</a>, 
<a href="/search/cs?searchtype=author&query=Hayot%2C+V">Valerie Hayot</a>, 
<a href="/search/cs?searchtype=author&query=Emani%2C+M">Murali Emani</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhen Xie</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Diangen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Shukla%2C+M">Maulik Shukla</a>, 
<a href="/search/cs?searchtype=author&query=Foster%2C+I">Ian Foster</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+J+J">James J. Davis</a>, 
<a href="/search/cs?searchtype=author&query=Papka%2C+M+E">Michael E. Papka</a>,  et al. (40 additional authors not shown)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04755" title="Abstract">arXiv:2310.04755</a> (replaced) [<a href="/pdf/2310.04755" title="Download PDF">pdf</a>, <a href="/format/2310.04755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pairwise GUI Dataset Construction Between Android Phones and Tablets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Han Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+H">Haolan Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yujin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Di Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures. arXiv admin note: substantial text overlap with <a href="/abs/2307.13225">arXiv:2307.13225</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04825" title="Abstract">arXiv:2310.04825</a> (replaced) [<a href="/pdf/2310.04825" title="Download PDF">pdf</a>, <a href="/format/2310.04825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative study of multi-person tracking methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akola%2C+D+M">Denis Mbey Akola</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04829" title="Abstract">arXiv:2310.04829</a> (replaced) [<a href="/pdf/2310.04829" title="Download PDF">pdf</a>, <a href="/format/2310.04829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to effectively train an ensemble of Faster R-CNN object detectors to  quantify uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akola%2C+D+M">Denis Mbey Akola</a>, 
<a href="/search/cs?searchtype=author&query=Franchi%2C+G">Gianni Franchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04874" title="Abstract">arXiv:2310.04874</a> (replaced) [<a href="/pdf/2310.04874" title="Download PDF">pdf</a>, <a href="/format/2310.04874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AirIMU: Learning Uncertainty Propagation for Inertial Odometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yuheng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xunfei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Youjie Xia</a>, 
<a href="/search/cs?searchtype=author&query=Scherer%2C+S">Sebastian Scherer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04948" title="Abstract">arXiv:2310.04948</a> (replaced) [<a href="/pdf/2310.04948" title="Download PDF">pdf</a>, <a href="/format/2310.04948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+D">Defu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+F">Furong Jia</a>, 
<a href="/search/cs?searchtype=author&query=Arik%2C+S+O">Sercan O Arik</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+T">Tomas Pfister</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yixiang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wen Ye</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 20 figures, 17 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04986" title="Abstract">arXiv:2310.04986</a> (replaced) [<a href="/pdf/2310.04986" title="Download PDF">pdf</a>, <a href="/format/2310.04986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A new economic and financial theory of money
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Glinsky%2C+M+E">Michael E. Glinsky</a>, 
<a href="/search/econ?searchtype=author&query=Sievert%2C+S">Sharon Sievert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 45 pages, 35 figures, 158 equations, to be submitted to Journal of Economic Affairs
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Artificial Intelligence (cs.AI); Classical Physics (physics.class-ph)

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05199" title="Abstract">arXiv:2310.05199</a> (replaced) [<a href="/pdf/2310.05199" title="Download PDF">pdf</a>, <a href="/format/2310.05199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning  from Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Wei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Rui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wenyu Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+S">Shihan Dou</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP findings 2023 (camera-ready)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05288" title="Abstract">arXiv:2310.05288</a> (replaced) [<a href="/pdf/2310.05288" title="Download PDF">pdf</a>, <a href="/format/2310.05288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustering Three-Way Data with Outliers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Clark%2C+K+M">Katharine M. Clark</a>, 
<a href="/search/stat?searchtype=author&query=McNicholas%2C+P+D">Paul D. McNicholas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05354" title="Abstract">arXiv:2310.05354</a> (replaced) [<a href="/pdf/2310.05354" title="Download PDF">pdf</a>, <a href="/format/2310.05354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Initial Investigation of Neural Replay Simulator for Over-the-Air  Adversarial Perturbations to Automatic Speaker Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Li Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+L">Liumeng Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhizheng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05624" title="Abstract">arXiv:2310.05624</a> (replaced) [<a href="/pdf/2310.05624" title="Download PDF">pdf</a>, <a href="/format/2310.05624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locality-Aware Generalizable Implicit Neural Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Doyup Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C">Chiheon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+M">Minsu Cho</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Wook-Shin Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05680" title="Abstract">arXiv:2310.05680</a> (replaced) [<a href="/pdf/2310.05680" title="Download PDF">pdf</a>, <a href="/ps/2310.05680" title="Download PostScript">ps</a>, <a href="/format/2310.05680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Argument Generation from Legal Facts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tuvey%2C+O">Oscar Tuvey</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+P">Procheta Sen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05898" title="Abstract">arXiv:2310.05898</a> (replaced) [<a href="/pdf/2310.05898" title="Download PDF">pdf</a>, <a href="/format/2310.05898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lizhang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+K">Kaizhao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Applications (stat.AP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05969" title="Abstract">arXiv:2310.05969</a> (replaced) [<a href="/pdf/2310.05969" title="Download PDF">pdf</a>, <a href="/format/2310.05969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Muharram%2C+A+P">Arief Purnama Muharram</a>, 
<a href="/search/eess?searchtype=author&query=Haryono%2C+H+P">Hollyana Puteri Haryono</a>, 
<a href="/search/eess?searchtype=author&query=Juma%2C+A+H">Abassi Haji Juma</a>, 
<a href="/search/eess?searchtype=author&query=Puspasari%2C+I">Ira Puspasari</a>, 
<a href="/search/eess?searchtype=author&query=Utama%2C+N+P">Nugraha Priya Utama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented in the 2023 IEEE International Conference on Data and Software Engineering (ICoDSE 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06193" title="Abstract">arXiv:2310.06193</a> (replaced) [<a href="/pdf/2310.06193" title="Download PDF">pdf</a>, <a href="/format/2310.06193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Natural Indirect Adaptive Controller for a Satellite-Mounted  Manipulator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Giordano%2C+J">Jacopo Giordano</a>, 
<a href="/search/eess?searchtype=author&query=Cenedese%2C+A">Angelo Cenedese</a>, 
<a href="/search/eess?searchtype=author&query=Serrani%2C+A">Andrea Serrani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06225" title="Abstract">arXiv:2310.06225</a> (replaced) [<a href="/pdf/2310.06225" title="Download PDF">pdf</a>, <a href="/format/2310.06225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silva%2C+B">Bruno Silva</a>, 
<a href="/search/cs?searchtype=author&query=Nunes%2C+L">Leonardo Nunes</a>, 
<a href="/search/cs?searchtype=author&query=Estev%C3%A3o%2C+R">Roberto Estev&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Aski%2C+V">Vijay Aski</a>, 
<a href="/search/cs?searchtype=author&query=Chandra%2C+R">Ranveer Chandra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06488" title="Abstract">arXiv:2310.06488</a> (replaced) [<a href="/pdf/2310.06488" title="Download PDF">pdf</a>, <a href="/format/2310.06488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenhao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Changze Lv</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jianhan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cenyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Muling Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiaoqing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06763" title="Abstract">arXiv:2310.06763</a> (replaced) [<a href="/pdf/2310.06763" title="Download PDF">pdf</a>, <a href="/format/2310.06763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FABind: Fast and Accurate Protein-Ligand Binding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pei%2C+Q">Qizhi Pei</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+K">Kaiyuan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lijun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jinhua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yingce Xia</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Shufang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+T">Tao Qin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Kun He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tie-Yan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+R">Rui Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06823" title="Abstract">arXiv:2310.06823</a> (replaced) [<a href="/pdf/2310.06823" title="Download PDF">pdf</a>, <a href="/format/2310.06823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NECO: NEural Collapse Based Out-of-distribution detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ammar%2C+M+B">Mou&#xef;n Ben Ammar</a>, 
<a href="/search/stat?searchtype=author&query=Belkhir%2C+N">Nacim Belkhir</a>, 
<a href="/search/stat?searchtype=author&query=Popescu%2C+S">Sebastian Popescu</a>, 
<a href="/search/stat?searchtype=author&query=Manzanera%2C+A">Antoine Manzanera</a>, 
<a href="/search/stat?searchtype=author&query=Franchi%2C+G">Gianni Franchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06933" title="Abstract">arXiv:2310.06933</a> (replaced) [<a href="/pdf/2310.06933" title="Download PDF">pdf</a>, <a href="/format/2310.06933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eclares: Energy-Aware Clarity-Driven Ergodic Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naveed%2C+K+B">Kaleb Ben Naveed</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+D">Devansh Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Vermillion%2C+C">Christopher Vermillion</a>, 
<a href="/search/cs?searchtype=author&query=Panagou%2C+D">Dimitra Panagou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to International Conference of Robotics and Automation (ICRA) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06970" title="Abstract">arXiv:2310.06970</a> (replaced) [<a href="/pdf/2310.06970" title="Download PDF">pdf</a>, <a href="/format/2310.06970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mathys%2C+J">Jo&#xeb;l Mathys</a>, 
<a href="/search/cs?searchtype=author&query=Gr%C3%B6tschla%2C+F">Florian Gr&#xf6;tschla</a>, 
<a href="/search/cs?searchtype=author&query=Nadimpalli%2C+K+V">Kalyan Varma Nadimpalli</a>, 
<a href="/search/cs?searchtype=author&query=Wattenhofer%2C+R">Roger Wattenhofer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07171" title="Abstract">arXiv:2310.07171</a> (replaced) [<a href="/pdf/2310.07171" title="Download PDF">pdf</a>, <a href="/format/2310.07171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Generalization via Information-Theoretic Distribution  Diversification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zheshun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zenglin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+D">Dun Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07244" title="Abstract">arXiv:2310.07244</a> (replaced) [<a href="/pdf/2310.07244" title="Download PDF">pdf</a>, <a href="/format/2310.07244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Sampling via Approximate Symmetries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ying%2C+L">Lexing Ying</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Statistics Theory (math.ST); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07246" title="Abstract">arXiv:2310.07246</a> (replaced) [<a href="/pdf/2310.07246" title="Download PDF">pdf</a>, <a href="/format/2310.07246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vec-Tok Speech: speech vectorization and tokenization for neural speech  generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinfa Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Y">Yuanjun Lv</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yi Lei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tao Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wendi He</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hongbin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Heng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lei Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07282" title="Abstract">arXiv:2310.07282</a> (replaced) [<a href="/pdf/2310.07282" title="Download PDF">pdf</a>, <a href="/format/2310.07282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Analysis on Large Language Models in Healthcare: A Case Study of  BioBERT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharaf%2C+S">Shyni Sharaf</a>, 
<a href="/search/cs?searchtype=author&query=Anoop%2C+V+S">V. S. Anoop</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07284" title="Abstract">arXiv:2310.07284</a> (replaced) [<a href="/pdf/2310.07284" title="Download PDF">pdf</a>, <a href="/format/2310.07284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Typing to Listen at the Cocktail Party: Text-Guided Target Speaker  Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hao%2C+X">Xiang Hao</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+J">Jibin Wu</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+J">Jianwei Yu</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+C">Chenglin Xu</a>, 
<a href="/search/eess?searchtype=author&query=Tan%2C+K+C">Kay Chen Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review, <a href="https://github.com/haoxiangsnr/llm-tse">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07297" title="Abstract">arXiv:2310.07297</a> (replaced) [<a href="/pdf/2310.07297" title="Download PDF">pdf</a>, <a href="/format/2310.07297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Score Regularized Policy Optimization through Diffusion Behavior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huayu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Cheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhengyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07312" title="Abstract">arXiv:2310.07312</a> (replaced) [<a href="/pdf/2310.07312" title="Download PDF">pdf</a>, <a href="/ps/2310.07312" title="Download PostScript">ps</a>, <a href="/format/2310.07312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Letafati%2C+M">Mehdi Letafati</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+S">Samad Ali</a>, 
<a href="/search/cs?searchtype=author&query=Latva-aho%2C+M">Matti Latva-aho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07352" title="Abstract">arXiv:2310.07352</a> (replaced) [<a href="/pdf/2310.07352" title="Download PDF">pdf</a>, <a href="/format/2310.07352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Distributionally Robust Planning for Renewable-Powered Fast  Charging Stations Under Decision-Dependent EV Diffusion Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yujia Li</a>, 
<a href="/search/eess?searchtype=author&query=Qiu%2C+F">Feng Qiu</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yixuan Chen</a>, 
<a href="/search/eess?searchtype=author&query=Hou%2C+Y">Yunhe Hou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07365" title="Abstract">arXiv:2310.07365</a> (replaced) [<a href="/pdf/2310.07365" title="Download PDF">pdf</a>, <a href="/ps/2310.07365" title="Download PostScript">ps</a>, <a href="/format/2310.07365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphControl: Adding Conditional Control to Universal Graph Pre-trained  Models for Graph Domain Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaoke Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Haizhou Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenshuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Siliang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07378" title="Abstract">arXiv:2310.07378</a> (replaced) [<a href="/pdf/2310.07378" title="Download PDF">pdf</a>, <a href="/format/2310.07378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RLaGA: A Reinforcement Learning Augmented Genetic Algorithm For  Searching Real and Diverse Marker-Based Landing Violations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+L">Linfeng Liang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Morton%2C+K">Kye Morton</a>, 
<a href="/search/cs?searchtype=author&query=Kallinen%2C+V">Valtteri Kallinen</a>, 
<a href="/search/cs?searchtype=author&query=James%2C+A">Alice James</a>, 
<a href="/search/cs?searchtype=author&query=Seth%2C+A">Avishkar Seth</a>, 
<a href="/search/cs?searchtype=author&query=Kuantama%2C+E">Endrowednes Kuantama</a>, 
<a href="/search/cs?searchtype=author&query=Mukhopadhyay%2C+S">Subhas Mukhopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+R">Richard Han</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xi Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07381" title="Abstract">arXiv:2310.07381</a> (replaced) [<a href="/pdf/2310.07381" title="Download PDF">pdf</a>, <a href="/format/2310.07381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extremal Mechanisms for Pointwise Maximal Leakage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grosse%2C+L">Leonhard Grosse</a>, 
<a href="/search/cs?searchtype=author&query=Saeidian%2C+S">Sara Saeidian</a>, 
<a href="/search/cs?searchtype=author&query=Oechtering%2C+T">Tobias Oechtering</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> corrected typo in Example 1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07402" title="Abstract">arXiv:2310.07402</a> (replaced) [<a href="/pdf/2310.07402" title="Download PDF">pdf</a>, <a href="/format/2310.07402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series  Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenguo Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xumeng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+W">Wei Cao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Congrui Huang</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Stephen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhirong Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07424" title="Abstract">arXiv:2310.07424</a> (replaced) [<a href="/pdf/2310.07424" title="Download PDF">pdf</a>, <a href="/format/2310.07424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analytical Die-to-Die 3D Placement with Bistratal Wirelength Model and  GPU Acceleration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+P">Peiyu Liao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuxuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Dawei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yibo Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bei Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07427" title="Abstract">arXiv:2310.07427</a> (replaced) [<a href="/pdf/2310.07427" title="Download PDF">pdf</a>, <a href="/format/2310.07427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field  and CNNs for Stock Return Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhengmeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hai Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Finance (q-fin.CP)

</div>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07433" title="Abstract">arXiv:2310.07433</a> (replaced) [<a href="/pdf/2310.07433" title="Download PDF">pdf</a>, <a href="/format/2310.07433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Imitation Learning from Observation with Automatic Discount Scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+W">Weijun Dong</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yingdong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+C">Chuan Wen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhao-Heng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chongjie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07449" title="Abstract">arXiv:2310.07449</a> (replaced) [<a href="/pdf/2310.07449" title="Download PDF">pdf</a>, <a href="/format/2310.07449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jia-Wang Bian</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+W">Wenjing Bian</a>, 
<a href="/search/cs?searchtype=author&query=Prisacariu%2C+V+A">Victor Adrian Prisacariu</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P">Philip Torr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07478" title="Abstract">arXiv:2310.07478</a> (replaced) [<a href="/pdf/2310.07478" title="Download PDF">pdf</a>, <a href="/format/2310.07478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Graph Learning for Generative Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+M">Minji Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Koh%2C+J+Y">Jing Yu Koh</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>, 
<a href="/search/cs?searchtype=author&query=Salakhutdinov%2C+R">Ruslan Salakhutdinov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07522" title="Abstract">arXiv:2310.07522</a> (replaced) [<a href="/pdf/2310.07522" title="Download PDF">pdf</a>, <a href="/format/2310.07522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S4C: Self-Supervised Semantic Scene Completion with Neural Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hayler%2C+A">Adrian Hayler</a>, 
<a href="/search/cs?searchtype=author&query=Wimbauer%2C+F">Felix Wimbauer</a>, 
<a href="/search/cs?searchtype=author&query=Muhle%2C+D">Dominik Muhle</a>, 
<a href="/search/cs?searchtype=author&query=Rupprecht%2C+C">Christian Rupprecht</a>, 
<a href="/search/cs?searchtype=author&query=Cremers%2C+D">Daniel Cremers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07579" title="Abstract">arXiv:2310.07579</a> (replaced) [<a href="/pdf/2310.07579" title="Download PDF">pdf</a>, <a href="/format/2310.07579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Unlearning: Language Models as Few Shot Unlearners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pawelczyk%2C+M">Martin Pawelczyk</a>, 
<a href="/search/cs?searchtype=author&query=Neel%2C+S">Seth Neel</a>, 
<a href="/search/cs?searchtype=author&query=Lakkaraju%2C+H">Himabindu Lakkaraju</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07637" title="Abstract">arXiv:2310.07637</a> (replaced) [<a href="/pdf/2310.07637" title="Download PDF">pdf</a>, <a href="/format/2310.07637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+C">Changhua Pei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Longlong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bohan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingze Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhirui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yongqian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shenglin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+G">Gaogang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xidao Wen</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+X">Xiaohui Nie</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+D">Dan Pei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07644" title="Abstract">arXiv:2310.07644</a> (replaced) [<a href="/pdf/2310.07644" title="Download PDF">pdf</a>, <a href="/format/2310.07644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking the BERT-like Pretraining for DNA Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+C">Chaoqi Liang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+W">Weiqiang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+L">Lifeng Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yuchen Ren</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jianle Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+P">Peng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hongliang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xinzhu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+W">Wangmeng Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07659" title="Abstract">arXiv:2310.07659</a> (replaced) [<a href="/pdf/2310.07659" title="Download PDF">pdf</a>, <a href="/format/2310.07659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for  Knowledge-Grounded Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+L">Lang Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+H">Hongru Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhenglu Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item401">Cross-lists</a></li>
<li><a href="#item443">Replacements</a></li>
</ul>
<small>[ total of 688 entries:  <b>1-688</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
