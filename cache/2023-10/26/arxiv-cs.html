<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Tue 24 Oct 23  to  Wed 25 Oct 23, announced Thu, 26 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item374">Cross-lists</a></li>
<li><a href="#item410">Replacements</a></li>
</ul>
<small>[ total of 670 entries:  <b>1-670</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Thu, 26 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16058" title="Abstract">arXiv:2310.16058</a> [<a href="/pdf/2310.16058" title="Download PDF">pdf</a>, <a href="/format/2310.16058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Sparse Bayesian Learning for Diagnosis of Nonstationary and Spatially  Correlated Faults with Application to Multistation Assembly Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+J">Jihoon Chung</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Z">Zhenyu Kong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP)

</div>
<p class="mathjax">Sensor technology developments provide a basis for effective fault diagnosis
in manufacturing systems. However, the limited number of sensors due to
physical constraints or undue costs hinders the accurate diagnosis in the
actual process. In addition, time-varying operational conditions that generate
nonstationary process faults and the correlation information in the process
require to consider for accurate fault diagnosis in the manufacturing systems.
This article proposes a novel fault diagnosis method: clustering spatially
correlated sparse Bayesian learning (CSSBL), and explicitly demonstrates its
applicability in a multistation assembly system that is vulnerable to the above
challenges. Specifically, the method is based on a practical assumption that it
will likely have a few process faults (sparse). In addition, the hierarchical
structure of CSSBL has several parameterized prior distributions to address the
above challenges. As posterior distributions of process faults do not have
closed form, this paper derives approximate posterior distributions through
Variational Bayes inference. The proposed method's efficacy is provided through
numerical and real-world case studies utilizing an actual autobody assembly
system. The generalizability of the proposed method allows the technique to be
applied in fault diagnosis in other domains, including communication and
healthcare systems.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16060" title="Abstract">arXiv:2310.16060</a> [<a href="/pdf/2310.16060" title="Download PDF">pdf</a>, <a href="/ps/2310.16060" title="Download PostScript">ps</a>, <a href="/format/2310.16060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Fuzzy Tracking Control for Nonlinear State Constrained  Pure-Feedback Systems With Input Delay via Dynamic Surface Technique
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+J">Ju Wu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+T">Tong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2310.15407">arXiv:2310.15407</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This brief constructs the adaptive backstepping control scheme for a class of
pure-feedback systems with input delay and full state constraints. With the
help of Mean Value Theorem, the pure-feedback system is transformed into
strict-feedback one. Barrier Lyapunov functions are employed to guarantee all
of the states remain constrained within predefined sets. By introducing the
Pade approximation method and corresponding intermediate, the impact generated
by input delay on the output tracking performance of the system can be
eliminated. Furthermore, a low-pass filter driven by a newly-defined control
input, is employed to generate the actual control input, which facilitates the
design of backstepping control. To approximate the unknown functions with a
desired level of accuracy, the fuzzy logic systems (FLSs) are utilized by
choosing appropriate fuzzy rules, logics and so on. The minimal learning
parameter (MLP) technique is employed to decrease the number of nodes and
parameters in FLSs, and dynamic surface control (DSC) technique is leveraged to
avoid so-called "explosion of complexity". Moreover, smooth robust compensators
are introduced to circumvent the influences of external disturbance and
approximation errors. By stability analysis, it is proved that all of signals
in the closed-loop system are semi-globally ultimately uniform bounded, and the
tracking error can be within a arbitrary small neighbor of origin via selecting
appropriate parameters of controllers. Finally, the results of numerical
illustration are provided to demonstrate the effectiveness of the designed
method.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16061" title="Abstract">arXiv:2310.16061</a> [<a href="/pdf/2310.16061" title="Download PDF">pdf</a>, <a href="/format/2310.16061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segue: Side-information Guided Generative Unlearnable Examples for  Facial Privacy Protection in Real World
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiling Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wenbo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Nenghai Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The widespread use of face recognition technology has given rise to privacy
concerns, as many individuals are worried about the collection and utilization
of their facial data. To address these concerns, researchers are actively
exploring the concept of ``unlearnable examples", by adding imperceptible
perturbation to data in the model training stage, which aims to prevent the
model from learning discriminate features of the target face. However, current
methods are inefficient and cannot guarantee transferability and robustness at
the same time, causing impracticality in the real world. To remedy it, we
propose a novel method called Segue: Side-information guided generative
unlearnable examples. Specifically, we leverage a once-trained multiple-used
model to generate the desired perturbation rather than the time-consuming
gradient-based method. To improve transferability, we introduce side
information such as true labels and pseudo labels, which are inherently
consistent across different scenarios. For robustness enhancement, a distortion
layer is integrated into the training pipeline. Extensive experiments
demonstrate that the proposed Segue is much faster than previous methods
(1000$\times$) and achieves transferable effectiveness across different
datasets and model architectures. Furthermore, it can resist JPEG compression,
adversarial training, and some standard data augmentations.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16062" title="Abstract">arXiv:2310.16062</a> [<a href="/pdf/2310.16062" title="Download PDF">pdf</a>, <a href="/format/2310.16062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained  Large Models Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shuoran Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qingcai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yang Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Youcheng Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiangping Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The excellent generalization, contextual learning, and emergence abilities in
the pre-trained large models (PLMs) handle specific tasks without direct
training data, making them the better foundation models in the adversarial
domain adaptation (ADA) methods to transfer knowledge learned from the source
domain to target domains. However, existing ADA methods fail to account for the
confounder properly, which is the root cause of the source data distribution
that differs from the target domains. This study proposes an adversarial domain
adaptation with confounder balancing for PLMs fine-tuning (ADA-CBF). The
ADA-CBF includes a PLM as the foundation model for a feature extractor, a
domain classifier and a confounder classifier, and they are jointly trained
with an adversarial loss. This loss is designed to improve the domain-invariant
representation learning by diluting the discrimination in the domain
classifier. At the same time, the adversarial loss also balances the confounder
distribution among source and unmeasured domains in training. Compared to
existing ADA methods, ADA-CBF can correctly identify confounders in
domain-invariant features, thereby eliminating the confounder biases in the
extracted features from PLMs. The confounder classifier in ADA-CBF is designed
as a plug-and-play and can be applied in the confounder measurable,
unmeasurable, or partially measurable environments. Empirical results on
natural language processing and computer vision downstream tasks show that
ADA-CBF outperforms the newest GPT-4, LLaMA2, ViT and ADA methods.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16063" title="Abstract">arXiv:2310.16063</a> [<a href="/pdf/2310.16063" title="Download PDF">pdf</a>, <a href="/format/2310.16063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Traffic Prediction with Learnable Filter Module
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuanshao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yongchao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J+J+Q">James J.Q. Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Modeling future traffic conditions often relies heavily on complex
spatial-temporal neural networks to capture spatial and temporal correlations,
which can overlook the inherent noise in the data. This noise, often
manifesting as unexpected short-term peaks or drops in traffic observation, is
typically caused by traffic accidents or inherent sensor vibration. In
practice, such noise can be challenging to model due to its stochastic nature
and can lead to overfitting risks if a neural network is designed to learn this
behavior. To address this issue, we propose a learnable filter module to filter
out noise in traffic data adaptively. This module leverages the Fourier
transform to convert the data to the frequency domain, where noise is filtered
based on its pattern. The denoised data is then recovered to the time domain
using the inverse Fourier transform. Our approach focuses on enhancing the
quality of the input data for traffic prediction models, which is a critical
yet often overlooked aspect in the field. We demonstrate that the proposed
module is lightweight, easy to integrate with existing models, and can
significantly improve traffic prediction performance. Furthermore, we validate
our approach with extensive experimental results on real-world datasets,
showing that it effectively mitigates noise and enhances prediction accuracy.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16065" title="Abstract">arXiv:2310.16065</a> [<a href="/pdf/2310.16065" title="Download PDF">pdf</a>, <a href="/format/2310.16065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Hyperdimensional Transform: a Holographic Representation of  Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dewulf%2C+P">Pieter Dewulf</a>, 
<a href="/search/cs?searchtype=author&query=Stock%2C+M">Michiel Stock</a>, 
<a href="/search/cs?searchtype=author&query=De+Baets%2C+B">Bernard De Baets</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Integral transforms are invaluable mathematical tools to map functions into
spaces where they are easier to characterize. We introduce the hyperdimensional
transform as a new kind of integral transform. It converts square-integrable
functions into noise-robust, holographic, high-dimensional representations
called hyperdimensional vectors. The central idea is to approximate a function
by a linear combination of random functions. We formally introduce a set of
stochastic, orthogonal basis functions and define the hyperdimensional
transform and its inverse. We discuss general transform-related properties such
as its uniqueness, approximation properties of the inverse transform, and the
representation of integrals and derivatives. The hyperdimensional transform
offers a powerful, flexible framework that connects closely with other integral
transforms, such as the Fourier, Laplace, and fuzzy transforms. Moreover, it
provides theoretical foundations and new insights for the field of
hyperdimensional computing, a computing paradigm that is rapidly gaining
attention for efficient and explainable machine learning algorithms, with
potential applications in statistical modelling and machine learning. In
addition, we provide straightforward and easily understandable code, which can
function as a tutorial and allows for the reproduction of the demonstrated
examples, from computing the transform to solving differential equations.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16069" title="Abstract">arXiv:2310.16069</a> [<a href="/pdf/2310.16069" title="Download PDF">pdf</a>, <a href="/format/2310.16069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CPSeg: Finer-grained Image Semantic Segmentation via Chain-of-Thought  Language Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Natural scene analysis and remote sensing imagery offer immense potential for
advancements in large-scale language-guided context-aware data utilization.
This potential is particularly significant for enhancing performance in
downstream tasks such as object detection and segmentation with designed
language prompting. In light of this, we introduce the CPSeg, Chain-of-Thought
Language Prompting for Finer-grained Semantic Segmentation), an innovative
framework designed to augment image segmentation performance by integrating a
novel "Chain-of-Thought" process that harnesses textual information associated
with images. This groundbreaking approach has been applied to a flood disaster
scenario. CPSeg encodes prompt texts derived from various sentences to
formulate a coherent chain-of-thought. We propose a new vision-language
dataset, FloodPrompt, which includes images, semantic masks, and corresponding
text information. This not only strengthens the semantic understanding of the
scenario but also aids in the key task of semantic segmentation through an
interplay of pixel and text matching maps. Our qualitative and quantitative
analyses validate the effectiveness of CPSeg.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16070" title="Abstract">arXiv:2310.16070</a> [<a href="/pdf/2310.16070" title="Download PDF">pdf</a>, <a href="/format/2310.16070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatial-Temporal Hypergraph Neural Network for Traffic Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+C">Chengzhi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junbo Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Traffic forecasting, which benefits from mobile Internet development and
position technologies, plays a critical role in Intelligent Transportation
Systems. It helps to implement rich and varied transportation applications and
bring convenient transportation services to people based on collected traffic
data. Most existing methods usually leverage graph-based deep learning networks
to model the complex road network for traffic forecasting shallowly. Despite
their effectiveness, these methods are generally limited in fully capturing
high-order spatial dependencies caused by road network topology and high-order
temporal dependencies caused by traffic dynamics. To tackle the above issues,
we focus on the essence of traffic system and propose STHODE: Spatio-Temporal
Hypergraph Neural Ordinary Differential Equation Network, which combines road
network topology and traffic dynamics to capture high-order spatio-temporal
dependencies in traffic data. Technically, STHODE consists of a spatial module
and a temporal module. On the one hand, we construct a spatial hypergraph and
leverage an adaptive MixHop hypergraph ODE network to capture high-order
spatial dependencies. On the other hand, we utilize a temporal hypergraph and
employ a hyperedge evolving ODE network to capture high-order temporal
dependencies. Finally, we aggregate the outputs of stacked STHODE layers to
mutually enhance the prediction performance. Extensive experiments conducted on
four real-world traffic datasets demonstrate the superior performance of our
proposed model compared to various baselines.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16071" title="Abstract">arXiv:2310.16071</a> [<a href="/pdf/2310.16071" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grid Frequency Forecasting in University Campuses using Convolutional  LSTM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sathe%2C+A">Aneesh Sathe</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W+R">Wen Ren Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The modern power grid is facing increasing complexities, primarily stemming
from the integration of renewable energy sources and evolving consumption
patterns. This paper introduces an innovative methodology that harnesses
Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks
to establish robust time series forecasting models for grid frequency. These
models effectively capture the spatiotemporal intricacies inherent in grid
frequency data, significantly enhancing prediction accuracy and bolstering
power grid reliability. The research explores the potential and development of
individualized Convolutional LSTM (ConvLSTM) models for buildings within a
university campus, enabling them to be independently trained and evaluated for
each building. Individual ConvLSTM models are trained on power consumption data
for each campus building and forecast the grid frequency based on historical
trends. The results convincingly demonstrate the superiority of the proposed
models over traditional forecasting techniques, as evidenced by performance
metrics such as Mean Square Error (MSE), Mean Absolute Error (MAE), and Mean
Absolute Percentage Error (MAPE). Additionally, an Ensemble Model is formulated
to aggregate insights from the building-specific models, delivering
comprehensive forecasts for the entire campus. This approach ensures the
privacy and security of power consumption data specific to each building.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16073" title="Abstract">arXiv:2310.16073</a> [<a href="/pdf/2310.16073" title="Download PDF">pdf</a>, <a href="/format/2310.16073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correlation Debiasing for Unbiased Scene Graph Generation in Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khandelwal%2C+A">Anant Khandelwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 tables, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Dynamic scene graph generation (SGG) from videos requires not only
comprehensive understanding of objects across the scenes that are prone to
temporal fluctuations but also a model the temporal motions and interactions
with different objects. Moreover, the long-tailed distribution of visual
relationships is the crucial bottleneck of most dynamic SGG methods, since most
of them focus on capturing spatio-temporal context using complex architectures,
which leads to the generation of biased scene graphs. To address these
challenges, we propose FloCoDe: Flow-aware temporal consistency and Correlation
Debiasing with uncertainty attenuation for unbiased dynamic scene graphs.
FloCoDe employs feature warping using flow to detect temporally consistent
objects across the frames. In addition, it uses correlation debiasing to learn
the unbiased relation representation for long-tailed classes. Moreover, to
attenuate the predictive uncertainties, it uses a mixture of sigmoidal
cross-entropy loss and contrastive loss to incorporate label correlations to
identify the commonly co-occurring relations and help debias the long-tailed
ones. Extensive experimental evaluation shows a performance gain as high as
4.1% showing the superiority of generating more unbiased scene graphs.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16074" title="Abstract">arXiv:2310.16074</a> [<a href="/pdf/2310.16074" title="Download PDF">pdf</a>, <a href="/format/2310.16074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RePoseDM: Recurrent Pose Alignment and Gradient Guidance for Pose Guided  Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khandelwal%2C+A">Anant Khandelwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 tables, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Pose-guided person image synthesis task requires re-rendering a reference
image, which should have a photorealistic appearance and flawless pose
transfer. Since person images are highly structured, existing approaches
require dense connections for complex deformations and occlusions because these
are generally handled through multi-level warping and masking in latent space.
But the feature maps generated by convolutional neural networks do not have
equivariance, and hence even the multi-level warping does not have a perfect
pose alignment. Inspired by the ability of the diffusion model to generate
photorealistic images from the given conditional guidance, we propose recurrent
pose alignment to provide pose-aligned texture features as conditional
guidance. Moreover, we propose gradient guidance from pose interaction fields,
which output the distance from the valid pose manifold given a target pose as
input. This helps in learning plausible pose transfer trajectories that result
in photorealism and undistorted texture details. Extensive results on two
large-scale benchmarks and a user study demonstrate the ability of our proposed
approach to generate photorealistic pose transfer under challenging scenarios.
Additionally, we prove the efficiency of gradient guidance in pose-guided image
generation on the HumanArt dataset with fine-tuned stable diffusion.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16076" title="Abstract">arXiv:2310.16076</a> [<a href="/pdf/2310.16076" title="Download PDF">pdf</a>, <a href="/format/2310.16076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical Computational Power of Linear Transformers and Their Recurrent  and Self-Referential Extensions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Irie%2C+K">Kazuki Irie</a>, 
<a href="/search/cs?searchtype=author&query=Csord%C3%A1s%2C+R">R&#xf3;bert Csord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Schmidhuber%2C+J">J&#xfc;rgen Schmidhuber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (short paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent studies of the computational power of recurrent neural networks (RNNs)
reveal a hierarchy of RNN architectures, given real-time and finite-precision
assumptions. Here we study auto-regressive Transformers with linearised
attention, a.k.a. linear Transformers (LTs) or Fast Weight Programmers (FWPs).
LTs are special in the sense that they are equivalent to RNN-like sequence
processors with a fixed-size state, while they can also be expressed as the
now-popular self-attention networks. We show that many well-known results for
the standard Transformer directly transfer to LTs/FWPs. Our formal language
recognition experiments demonstrate how recently proposed FWP extensions such
as recurrent FWPs and self-referential weight matrices successfully overcome
certain limitations of the LT, e.g., allowing for generalisation on the parity
problem. Our code is public.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16077" title="Abstract">arXiv:2310.16077</a> [<a href="/pdf/2310.16077" title="Download PDF">pdf</a>, <a href="/format/2310.16077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Femtosecond laser fabricated nitinol living hinges for millimeter-sized  robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hedrick%2C+A">Alexander Hedrick</a>, 
<a href="/search/cs?searchtype=author&query=Kabutz%2C+H">Heiko Kabutz</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+L">Lawrence Smith</a>, 
<a href="/search/cs?searchtype=author&query=MacCurdy%2C+R">Robert MacCurdy</a>, 
<a href="/search/cs?searchtype=author&query=Jayaram%2C+K">Kaushik Jayaram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, submitted to IEEE RA-L
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Nitinol is a smart material that can be used as an actuator, a sensor, or a
structural element, and has the potential to significantly enhance the
capabilities of microrobots. Femtosecond laser technology can be used to
process nitinol while avoiding heat-affected zones (HAZ), thus retaining
superelastic properties. In this work, we manufacture living hinges of
arbitrary cross-sections from nitinol using a femtosecond laser micromachining
process. We first determined the laser cutting parameters, 4.1 Jcm^-2 fluence
with 5 passes for 5 um ablation, by varying laser power level and number of
passes. Next, we modeled the hinges using an analytical model as well as
creating an Abaqus finite element method, and showed the accuracy of the models
by comparing them to the torque produced by eight different hinges, four with a
rectangular cross-section and four with an arc cross-section. Finally, we
manufactured three prototype miniature devices to illustrate the usefulness of
these nitinol hinges: a sample spherical 5-bar mechanism, a sarrus linkage, and
a piezoelectric actuated robotic wing mechanism.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16095" title="Abstract">arXiv:2310.16095</a> [<a href="/pdf/2310.16095" title="Download PDF">pdf</a>, <a href="/format/2310.16095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CR-COPEC: Causal Rationale of Corporate Performance Changes to Learn  from Financial Reports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chun%2C+Y+E">Ye Eun Chun</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+S">Sunjae Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Sohn%2C+K">Kyunghwan Sohn</a>, 
<a href="/search/cs?searchtype=author&query=Sung%2C+N">Nakwon Sung</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Junyoup Lee</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+B">Byungki Seo</a>, 
<a href="/search/cs?searchtype=author&query=Compher%2C+K">Kevin Compher</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S">Seung-won Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaesik Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">In this paper, we introduce CR-COPEC called Causal Rationale of Corporate
Performance Changes from financial reports. This is a comprehensive large-scale
domain-adaptation causal sentence dataset to detect financial performance
changes of corporate. CR-COPEC contributes to two major achievements. First, it
detects causal rationale from 10-K annual reports of the U.S. companies, which
contain experts' causal analysis following accounting standards in a formal
manner. This dataset can be widely used by both individual investors and
analysts as material information resources for investing and decision making
without tremendous effort to read through all the documents. Second, it
carefully considers different characteristics which affect the financial
performance of companies in twelve industries. As a result, CR-COPEC can
distinguish causal sentences in various industries by taking unique narratives
in each industry into consideration. We also provide an extensive analysis of
how well CR-COPEC dataset is constructed and suited for classifying target
sentences as causal ones with respect to industry characteristics. Our dataset
and experimental codes are publicly available.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16099" title="Abstract">arXiv:2310.16099</a> [<a href="/pdf/2310.16099" title="Download PDF">pdf</a>, <a href="/format/2310.16099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anatomically-aware Uncertainty for Semi-supervised Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=V%2C+S+A">Sukesh Adiga V</a>, 
<a href="/search/cs?searchtype=author&query=Dolz%2C+J">Jose Dolz</a>, 
<a href="/search/cs?searchtype=author&query=Lombaert%2C+H">Herve Lombaert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Medical Image Analysis. Code is available at: $\href{<a href="https://github.com/adigasu/Anatomically-aware_Uncertainty_for_Semi-supervised_Segmentation">this https URL</a>}{Github}$
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Semi-supervised learning relaxes the need of large pixel-wise labeled
datasets for image segmentation by leveraging unlabeled data. A prominent way
to exploit unlabeled data is to regularize model predictions. Since the
predictions of unlabeled data can be unreliable, uncertainty-aware schemes are
typically employed to gradually learn from meaningful and reliable predictions.
Uncertainty estimation methods, however, rely on multiple inferences from the
model predictions that must be computed for each training step, which is
computationally expensive. Moreover, these uncertainty maps capture pixel-wise
disparities and do not consider global information. This work proposes a novel
method to estimate segmentation uncertainty by leveraging global information
from the segmentation masks. More precisely, an anatomically-aware
representation is first learnt to model the available segmentation masks. The
learnt representation thereupon maps the prediction of a new segmentation into
an anatomically-plausible segmentation. The deviation from the plausible
segmentation aids in estimating the underlying pixel-level uncertainty in order
to further guide the segmentation network. The proposed method consequently
estimates the uncertainty using a single inference from our representation,
thereby reducing the total computation. We evaluate our method on two publicly
available segmentation datasets of left atria in cardiac MRIs and of multiple
organs in abdominal CTs. Our anatomically-aware method improves the
segmentation accuracy over the state-of-the-art semi-supervised methods in
terms of two commonly used evaluation metrics.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16100" title="Abstract">arXiv:2310.16100</a> [<a href="/pdf/2310.16100" title="Download PDF">pdf</a>, <a href="/format/2310.16100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Feature Registration for Unsupervised Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Youshan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Davison%2C+B+D">Brian D. Davison</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">While unsupervised domain adaptation has been explored to leverage the
knowledge from a labeled source domain to an unlabeled target domain, existing
methods focus on the distribution alignment between two domains. However, how
to better align source and target features is not well addressed. In this
paper, we propose a deep feature registration (DFR) model to generate
registered features that maintain domain invariant features and simultaneously
minimize the domain-dissimilarity of registered features and target features
via histogram matching. We further employ a pseudo label refinement process,
which considers both probabilistic soft selection and center-based hard
selection to improve the quality of pseudo labels in the target domain.
Extensive experiments on multiple UDA benchmarks demonstrate the effectiveness
of our DFR model, resulting in new state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16101" title="Abstract">arXiv:2310.16101</a> [<a href="/pdf/2310.16101" title="Download PDF">pdf</a>, <a href="/format/2310.16101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accuracy analysis for explicit-implicit finite volume schemes on cut  cell meshes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=May%2C+S">Sandra May</a>, 
<a href="/search/math?searchtype=author&query=Laakmann%2C+F">Fabian Laakmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The solution of time-dependent hyperbolic conservation laws on cut cell
meshes causes the small cell problem: standard schemes are not stable on the
arbitrarily small cut cells if an explicit time stepping scheme is used and the
time step size is chosen based on the size of the background cells. In [J. Sci.
Comput. 71, 919-943 (2017)], the mixed explicit implicit approach in general
and MUSCL-Trap in particular have been introduced to solve this problem by
using implicit time stepping on the cut cells. Theoretical and numerical
results have indicated that this might lead to a loss in accuracy when
switching between the explicit and implicit time stepping. In this contribution
we examine this in more detail and will prove in one dimension that the
specific combination MUSCL-Trap of an explicit second-order and an implicit
second-order scheme results in a fully second-order mixed scheme. As this
result is unlikely to hold in two dimensions, we also introduce two new
versions of mixed explicit implicit schemes based on exchanging the explicit
scheme. We present numerical tests in two dimensions where we compare the new
versions with the original MUSCL-Trap scheme.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16103" title="Abstract">arXiv:2310.16103</a> [<a href="/pdf/2310.16103" title="Download PDF">pdf</a>, <a href="/format/2310.16103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LaksNet: an end-to-end deep learning model for self-driving cars in  Udacity simulator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Polamreddy%2C+L+R">Lakshmikar R. Polamreddy</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Youshan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The majority of road accidents occur because of human errors, including
distraction, recklessness, and drunken driving. One of the effective ways to
overcome this dangerous situation is by implementing self-driving technologies
in vehicles. In this paper, we focus on building an efficient deep-learning
model for self-driving cars. We propose a new and effective convolutional
neural network model called `LaksNet' consisting of four convolutional layers
and two fully connected layers. We conduct extensive experiments using our
LaksNet model with the training data generated from the Udacity simulator. Our
model outperforms many existing pre-trained ImageNet and NVIDIA models in terms
of the duration of the car for which it drives without going off the track on
the simulator.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16105" title="Abstract">arXiv:2310.16105</a> [<a href="/pdf/2310.16105" title="Download PDF">pdf</a>, <a href="/format/2310.16105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locally Differentially Private Gradient Tracking for Distributed Online  Learning over Directed Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziqin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongqiang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Distributed online learning has been proven extremely effective in solving
large-scale machine learning problems involving streaming data. However,
information sharing between learners in distributed learning also raises
concerns about the potential leakage of individual learners' sensitive data. To
mitigate this risk, differential privacy, which is widely regarded as the "gold
standard" for privacy protection, has been widely employed in many existing
results on distributed online learning. However, these results often face a
fundamental tradeoff between learning accuracy and privacy. In this paper, we
propose a locally differentially private gradient tracking based distributed
online learning algorithm that successfully circumvents this tradeoff. Our
analysis shows that the proposed algorithm converges in mean square to the
exact optimal solution while ensuring rigorous local differential privacy, with
the cumulative privacy budget guaranteed to be finite even when the number of
iterations tends to infinity. The algorithm is applicable even when the
communication graph among learners is directed. To the best of our knowledge,
this is the first result that simultaneously ensures learning accuracy and
rigorous local differential privacy in distributed online learning over
directed graphs. We evaluate our algorithm's performance by using multiple
benchmark machine-learning applications, including logistic regression on the
"Mushrooms" dataset and CNN-based image classification on the "MNIST" and
"CIFAR-10" datasets, respectively. The experimental results confirm that the
proposed algorithm outperforms existing counterparts in both training and
testing accuracies.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16106" title="Abstract">arXiv:2310.16106</a> [<a href="/pdf/2310.16106" title="Download PDF">pdf</a>, <a href="/format/2310.16106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Learning over Wireless Networks with Broadcast-Based  Subgraph Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Herrera%2C+D+P">Daniel P&#xe9;rez Herrera</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Larsson%2C+E+G">Erik G. Larsson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, submitted for possible conference publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Information Theory (cs.IT); Systems and Control (eess.SY)

</div>
<p class="mathjax">This work centers on the communication aspects of decentralized learning over
wireless networks, using consensus-based decentralized stochastic gradient
descent (D-SGD). Considering the actual communication cost or delay caused by
in-network information exchange in an iterative process, our goal is to achieve
fast convergence of the algorithm measured by improvement per transmission
slot. We propose BASS, an efficient communication framework for D-SGD over
wireless networks with broadcast transmission and probabilistic subgraph
sampling. In each iteration, we activate multiple subsets of non-interfering
nodes to broadcast model updates to their neighbors. These subsets are randomly
activated over time, with probabilities reflecting their importance in network
connectivity and subject to a communication cost constraint (e.g., the average
number of transmission slots per iteration). During the consensus update step,
only bi-directional links are effectively preserved to maintain communication
symmetry. In comparison to existing link-based scheduling methods, the inherent
broadcasting nature of wireless channels offers intrinsic advantages in
speeding up convergence of decentralized learning by creating more communicated
links with the same number of transmission slots.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16108" title="Abstract">arXiv:2310.16108</a> [<a href="/pdf/2310.16108" title="Download PDF">pdf</a>, <a href="/format/2310.16108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Precise Distributed Satellite Navigation: Differential GPS with  Sensor-Coupling for Integer Ambiguity Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Low%2C+S+Y+W">Samuel Y W Low</a>, 
<a href="/search/eess?searchtype=author&query=D%27Amico%2C+S">Simone D&#x27;Amico</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 20 figures, IEEE AERO 2024 (pre-print)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Multiagent Systems (cs.MA); Applications (stat.AP)

</div>
<p class="mathjax">Precise relative navigation is a critical enabler for distributed satellites
to achieve new mission objectives impossible for a monolithic spacecraft.
Carrier phase differential GPS (CDGPS) with integer ambiguity resolution (IAR)
is a promising means of achieving cm-level accuracy for high-precision
Rendezvous, Proximity-Operations and Docking (RPOD), In-Space Servicing,
Assembly and Manufacturing (ISAM) as well as satellite formation flying and
swarming. However, IAR is sensitive to received GPS signal noise, especially
under severe multi-path or high thermal noise. This paper proposes a
sensor-fusion approach to achieve IAR under such conditions in two coupling
stages. A loose coupling stage fuses through an Extended Kalman Filter the
CDGPS measurements with on-board sensor measurements such as range from
cross-links, and vision-based bearing angles. A second tight-coupling stage
augments the cost function of the integer weighted least-squares minimization
with a soft constraint function using noise-weighted observed-minus-computed
residuals from these external sensor measurements. Integer acceptance tests are
empirically modified to reflect added constraints. Partial IAR is applied to
graduate integer fixing. These proposed techniques are packaged into
flight-capable software, with ground truths simulated by the Stanford Space
Rendezvous Laboratory's S3 library using state-of-the-art force modelling with
relevant sources of errors, and validated in two scenarios: (1) a high
multi-path scenario involving rendezvous and docking in low Earth orbit, and
(2) a high thermal noise scenario relying only on GPS side-lobe signals during
proximity operations in geostationary orbit. This study demonstrates successful
IAR in both cases, using the proposed sensor-fusion approach, thus
demonstrating potential for high-precision state estimation under adverse
signal-to-noise conditions.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16109" title="Abstract">arXiv:2310.16109</a> [<a href="/pdf/2310.16109" title="Download PDF">pdf</a>, <a href="/format/2310.16109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complex Image Generation SwinTransformer Network for Audio Denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Youshan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jialu Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Achieving high-performance audio denoising is still a challenging task in
real-world applications. Existing time-frequency methods often ignore the
quality of generated frequency domain images. This paper converts the audio
denoising problem into an image generation task. We first develop a complex
image generation SwinTransformer network to capture more information from the
complex Fourier domain. We then impose structure similarity and detailed loss
functions to generate high-quality images and develop an SDR loss to minimize
the difference between denoised and clean audios. Extensive experiments on two
benchmark datasets demonstrate that our proposed model is better than
state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16111" title="Abstract">arXiv:2310.16111</a> [<a href="/pdf/2310.16111" title="Download PDF">pdf</a>, <a href="/format/2310.16111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locally Differentially Private Document Generation Using Zero Shot  Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Utpala%2C+S">Saiteja Utpala</a>, 
<a href="/search/cs?searchtype=author&query=Hooker%2C+S">Sara Hooker</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P+Y">Pin Yu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Numerous studies have highlighted the privacy risks associated with
pretrained large language models. In contrast, our research offers a unique
perspective by demonstrating that pretrained large language models can
effectively contribute to privacy preservation. We propose a locally
differentially private mechanism called DP-Prompt, which leverages the power of
pretrained large language models and zero-shot prompting to counter author
de-anonymization attacks while minimizing the impact on downstream utility.
When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5),
we observe a notable reduction in the success rate of de-anonymization attacks,
showing that it surpasses existing approaches by a considerable margin despite
its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt
(with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving
a 46\% reduction in author identification F1 score against static attackers and
a 26\% reduction against adaptive attackers. We conduct extensive experiments
across six open-source large language models, ranging up to 7 billion
parameters, to analyze various effects of the privacy-utility tradeoff.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16112" title="Abstract">arXiv:2310.16112</a> [<a href="/pdf/2310.16112" title="Download PDF">pdf</a>, <a href="/format/2310.16112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards long-tailed, multi-label disease classification from chest  X-ray: Overview of the CXR-LT challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Holste%2C+G">Gregory Holste</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yiliang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Song Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jaiswal%2C+A">Ajay Jaiswal</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Mingquan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhuge%2C+S">Sherry Zhuge</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuzhe Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dongkyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen-Mau%2C+T">Trong-Hieu Nguyen-Mau</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+M">Minh-Triet Tran</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+J">Jaehyup Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+W">Wongi Park</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+J">Jongbin Ryu</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+F">Feng Hong</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+A">Arsh Verma</a>, 
<a href="/search/cs?searchtype=author&query=Yamagishi%2C+Y">Yosuke Yamagishi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C">Changhyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+H">Hyeryeong Seo</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+M">Myungjoo Kang</a>, 
<a href="/search/cs?searchtype=author&query=Celi%2C+L+A">Leo Anthony Celi</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Summers%2C+R+M">Ronald M. Summers</a>, 
<a href="/search/cs?searchtype=author&query=Shih%2C+G">George Shih</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yifan Peng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Many real-world image recognition problems, such as diagnostic medical
imaging exams, are "long-tailed" $\unicode{x2013}$ there are a few common
findings followed by many more relatively rare conditions. In chest
radiography, diagnosis is both a long-tailed and multi-label problem, as
patients often present with multiple findings simultaneously. While researchers
have begun to study the problem of long-tailed learning in medical image
recognition, few have studied the interaction of label imbalance and label
co-occurrence posed by long-tailed, multi-label disease classification. To
engage with the research community on this emerging topic, we conducted an open
challenge, CXR-LT, on long-tailed, multi-label thorax disease classification
from chest X-rays (CXRs). We publicly release a large-scale benchmark dataset
of over 350,000 CXRs, each labeled with at least one of 26 clinical findings
following a long-tailed distribution. We synthesize common themes of
top-performing solutions, providing practical recommendations for long-tailed,
multi-label medical image classification. Finally, we use these insights to
propose a path forward involving vision-language foundation models for few- and
zero-shot disease classification.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16113" title="Abstract">arXiv:2310.16113</a> [<a href="/pdf/2310.16113" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compressed representation of brain genetic transcription
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruffle%2C+J+K">James K Ruffle</a>, 
<a href="/search/cs?searchtype=author&query=Watkins%2C+H">Henry Watkins</a>, 
<a href="/search/cs?searchtype=author&query=Gray%2C+R+J">Robert J Gray</a>, 
<a href="/search/cs?searchtype=author&query=Hyare%2C+H">Harpreet Hyare</a>, 
<a href="/search/cs?searchtype=author&query=de+Schotten%2C+M+T">Michel Thiebaut de Schotten</a>, 
<a href="/search/cs?searchtype=author&query=Nachev%2C+P">Parashkev Nachev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 5 main figures, 1 supplementary figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Genomics (q-bio.GN); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">The architecture of the brain is too complex to be intuitively surveyable
without the use of compressed representations that project its variation into a
compact, navigable space. The task is especially challenging with
high-dimensional data, such as gene expression, where the joint complexity of
anatomical and transcriptional patterns demands maximum compression.
Established practice is to use standard principal component analysis (PCA),
whose computational felicity is offset by limited expressivity, especially at
great compression ratios. Employing whole-brain, voxel-wise Allen Brain Atlas
transcription data, here we systematically compare compressed representations
based on the most widely supported linear and non-linear methods-PCA, kernel
PCA, non-negative matrix factorization (NMF), t-stochastic neighbour embedding
(t-SNE), uniform manifold approximation and projection (UMAP), and deep
auto-encoding-quantifying reconstruction fidelity, anatomical coherence, and
predictive utility with respect to signalling, microstructural, and metabolic
targets. We show that deep auto-encoders yield superior representations across
all metrics of performance and target domains, supporting their use as the
reference standard for representing transcription patterns in the human brain.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16115" title="Abstract">arXiv:2310.16115</a> [<a href="/pdf/2310.16115" title="Download PDF">pdf</a>, <a href="/format/2310.16115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wakening Past Concepts without Past Data: Class-Incremental Learning  from Online Placebos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yaoyao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingying Li</a>, 
<a href="/search/cs?searchtype=author&query=Schiele%2C+B">Bernt Schiele</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qianru Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WACV 2024. Code: <a href="https://github.com/yaoyao-liu/online-placebos">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Not forgetting old class knowledge is a key challenge for class-incremental
learning (CIL) when the model continuously adapts to new classes. A common
technique to address this is knowledge distillation (KD), which penalizes
prediction inconsistencies between old and new models. Such prediction is made
with almost new class data, as old class data is extremely scarce due to the
strict memory limitation in CIL. In this paper, we take a deep dive into KD
losses and find that "using new class data for KD" not only hinders the model
adaption (for learning new classes) but also results in low efficiency for
preserving old class knowledge. We address this by "using the placebos of old
classes for KD", where the placebos are chosen from a free image stream, such
as Google Images, in an automatical and economical fashion. To this end, we
train an online placebo selection policy to quickly evaluate the quality of
streaming images (good or bad placebos) and use only good ones for one-time
feed-forward computation of KD. We formulate the policy training process as an
online Markov Decision Process (MDP), and introduce an online learning
algorithm to solve this MDP problem without causing much computation costs. In
experiments, we show that our method 1) is surprisingly effective even when
there is no class overlap between placebos and original old class data, 2) does
not require any additional supervision or memory budget, and 3) significantly
outperforms a number of top-performing CIL methods, in particular when using
lower memory budgets for old class exemplars, e.g., five exemplars per class.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16117" title="Abstract">arXiv:2310.16117</a> [<a href="/pdf/2310.16117" title="Download PDF">pdf</a>, <a href="/format/2310.16117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NADI 2023: The Fourth Nuanced Arabic Dialect Identification Shared Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>, 
<a href="/search/cs?searchtype=author&query=Elmadany%2C+A">AbdelRahim Elmadany</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chiyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Nagoudi%2C+E+M+B">El Moatez Billah Nagoudi</a>, 
<a href="/search/cs?searchtype=author&query=Bouamor%2C+H">Houda Bouamor</a>, 
<a href="/search/cs?searchtype=author&query=Habash%2C+N">Nizar Habash</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2210.09582">arXiv:2210.09582</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We describe the findings of the fourth Nuanced Arabic Dialect Identification
Shared Task (NADI 2023). The objective of NADI is to help advance
state-of-the-art Arabic NLP by creating opportunities for teams of researchers
to collaboratively compete under standardized conditions. It does so with a
focus on Arabic dialects, offering novel datasets and defining subtasks that
allow for meaningful comparisons between different approaches. NADI 2023
targeted both dialect identification (Subtask 1) and dialect-to-MSA machine
translation (Subtask 2 and Subtask 3). A total of 58 unique teams registered
for the shared task, of whom 18 teams have participated (with 76 valid
submissions during test phase). Among these, 16 teams participated in Subtask
1, 5 participated in Subtask 2, and 3 participated in Subtask 3. The winning
teams achieved 87.27
<br />F1 on Subtask 1, 14.76 Bleu in Subtask 2, and 21.10 Bleu in Subtask 3,
respectively. Results show that all three subtasks remain challenging, thereby
motivating future work in this area. We describe the methods employed by the
participating teams and briefly offer an outlook for NADI.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16119" title="Abstract">arXiv:2310.16119</a> [<a href="/pdf/2310.16119" title="Download PDF">pdf</a>, <a href="/format/2310.16119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for  Enhancing SocialBot Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kobza%2C+O">Ond&#x159;ej Kobza</a>, 
<a href="/search/cs?searchtype=author&query=%C4%8Cuhel%2C+J">Jan &#x10c;uhel</a>, 
<a href="/search/cs?searchtype=author&query=Gargiani%2C+T">Tommaso Gargiani</a>, 
<a href="/search/cs?searchtype=author&query=Herel%2C+D">David Herel</a>, 
<a href="/search/cs?searchtype=author&query=Marek%2C+P">Petr Marek</a> (Faculty of Electrical Engineering, CTU in Prague)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present our SocialBot -- Alquist~5.0 -- developed for the Alexa Prize
SocialBot Grand Challenge~5. Building upon previous versions of our system, we
introduce the NRG Barista and outline several innovative approaches for
integrating Barista into our SocialBot, improving the overall conversational
experience. Additionally, we extend our SocialBot to support multimodal
devices. This paper offers insights into the development of Alquist~5.0, which
meets evolving user expectations while maintaining empathetic and knowledgeable
conversational abilities across diverse topics.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16120" title="Abstract">arXiv:2310.16120</a> [<a href="/pdf/2310.16120" title="Download PDF">pdf</a>, <a href="/format/2310.16120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stereoscopic Depth Perception Through Foliage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kerschner%2C+R">Robert Kerschner</a>, 
<a href="/search/cs?searchtype=author&query=Nathan%2C+R+J+A+A">Rakesh John Amala Arokia Nathan</a>, 
<a href="/search/cs?searchtype=author&query=Mantiuk%2C+R">Rafal Mantiuk</a>, 
<a href="/search/cs?searchtype=author&query=Bimber%2C+O">Oliver Bimber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Both humans and computational methods struggle to discriminate the depths of
objects hidden beneath foliage. However, such discrimination becomes feasible
when we combine computational optical synthetic aperture sensing with the human
ability to fuse stereoscopic images. For object identification tasks, as
required in search and rescue, wildlife observation, surveillance, and early
wildfire detection, depth assists in differentiating true from false findings,
such as people, animals, or vehicles vs. sun-heated patches at the ground level
or in the tree crowns, or ground fires vs. tree trunks. We used video captured
by a drone above dense woodland to test users' ability to discriminate depth.
We found that this is impossible when viewing monoscopic video and relying on
motion parallax. The same was true with stereoscopic video because of the
occlusions caused by foliage. However, when synthetic aperture sensing was used
to reduce occlusions and disparity-scaled stereoscopic video was presented,
whereas computational (stereoscopic matching) methods were unsuccessful, human
observers successfully discriminated depth. This shows the potential of systems
which exploit the synergy between computational methods and human vision to
perform tasks that neither can perform alone.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16122" title="Abstract">arXiv:2310.16122</a> [<a href="/pdf/2310.16122" title="Download PDF">pdf</a>, <a href="/format/2310.16122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Performance-Portable SYCL Implementation of CRK-HACC for Exascale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rangel%2C+E+M">Esteban M. Rangel</a>, 
<a href="/search/cs?searchtype=author&query=Pennycook%2C+S+J">S. John Pennycook</a>, 
<a href="/search/cs?searchtype=author&query=Pope%2C+A">Adrian Pope</a>, 
<a href="/search/cs?searchtype=author&query=Frontiere%2C+N">Nicholas Frontiere</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhiqiang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Madananth%2C+V">Varsha Madananth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 13 figures, 2023 International Workshop on Performance, Portability &amp; Productivity in HPC
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>; Cosmology and Nongalactic Astrophysics (astro-ph.CO); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">The first generation of exascale systems will include a variety of machine
architectures, featuring GPUs from multiple vendors. As a result, many
developers are interested in adopting portable programming models to avoid
maintaining multiple versions of their code. It is necessary to document
experiences with such programming models to assist developers in understanding
the advantages and disadvantages of different approaches.
<br />To this end, this paper evaluates the performance portability of a SYCL
implementation of a large-scale cosmology application (CRK-HACC) running on
GPUs from three different vendors: AMD, Intel, and NVIDIA. We detail the
process of migrating the original code from CUDA to SYCL and show that
specializing kernels for specific targets can greatly improve performance
portability without significantly impacting programmer productivity. The SYCL
version of CRK-HACC achieves a performance portability of 0.96 with a code
divergence of almost 0, demonstrating that SYCL is a viable programming model
for performance-portable applications.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16123" title="Abstract">arXiv:2310.16123</a> [<a href="/pdf/2310.16123" title="Download PDF">pdf</a>, <a href="/format/2310.16123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anchor Space Optimal Transport: Accelerating Batch Processing of  Multiple OT Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jianming Huang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+X">Xun Su</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zhongxi Fang</a>, 
<a href="/search/cs?searchtype=author&query=Kasai%2C+H">Hiroyuki Kasai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 4 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The optimal transport (OT) theory provides an effective way to compare
probability distributions on a defined metric space, but it suffers from cubic
computational complexity. Although the Sinkhorn's algorithm greatly reduces the
computational complexity of OT solutions, the solutions of multiple OT problems
are still time-consuming and memory-comsuming in practice. However, many works
on the computational acceleration of OT are usually based on the premise of a
single OT problem, ignoring the potential common characteristics of the
distributions in a mini-batch. Therefore, we propose a translated OT problem
designated as the anchor space optimal transport (ASOT) problem, which is
specially designed for batch processing of multiple OT problem solutions. For
the proposed ASOT problem, the distributions will be mapped into a shared
anchor point space, which learns the potential common characteristics and thus
help accelerate OT batch processing. Based on the proposed ASOT, the
Wasserstein distance error to the original OT problem is proven to be bounded
by ground cost errors. Building upon this, we propose three methods to learn an
anchor space minimizing the distance error, each of which has its application
background. Numerical experiments on real-world datasets show that our proposed
methods can greatly reduce computational time while maintaining reasonable
approximation performance.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16125" title="Abstract">arXiv:2310.16125</a> [<a href="/pdf/2310.16125" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Thermal Field Prediction for Metal Additive Manufacturing of Thin  Walls
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yifan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Dehaghani%2C+M+R">M. Rahmani Dehaghani</a>, 
<a href="/search/cs?searchtype=author&query=Sajadi%2C+P">Pouyan Sajadi</a>, 
<a href="/search/cs?searchtype=author&query=Balani%2C+S+B">Shahriar Bakrani Balani</a>, 
<a href="/search/cs?searchtype=author&query=Dhalpe%2C+A">Akshay Dhalpe</a>, 
<a href="/search/cs?searchtype=author&query=Panicker%2C+S">Suraj Panicker</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Coatanea%2C+E">Eric Coatanea</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G+G">G. Gary Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 26 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper aims to study a practical issue in metal AM, i.e., how to predict
the thermal field of yet-to-print parts online when only a few sensors are
available. This work proposes an online thermal field prediction method using
mapping and reconstruction, which could be integrated into a metal AM process
for online performance control. Based on the similarity of temperature curves
(curve segments of a temperature profile of one point), the thermal field
mapping applies an artificial neural network to estimate the temperature curves
of points on the yet-to-print layer from measured temperatures of certain
points on the previously printed layer. With measured/predicted temperature
profiles of several points on the same layer, the thermal field reconstruction
proposes a reduced order model (ROM) to construct the temperature profiles of
all points on the same layer, which could be used to build the temperature
field of the entire layer. The training of ROM is performed with an extreme
learning machine (ELM) for computational efficiency. Fifteen wire arc AM
experiments and nine simulations are designed for thin walls with a fixed
length and unidirectional printing of each layer. The test results indicate
that the proposed prediction method could construct the thermal field of a
yet-to-print layer within 0.1 seconds on a low-cost desktop. Meanwhile, the
method has acceptable generalization capability in most cases from lower layers
to higher layers in the same simulation and from one simulation to a new
simulation on different AM process parameters. More importantly, after
fine-tuning the proposed method with limited experimental data, the relative
errors of all predicted temperature profiles on a new experiment are
sufficiently small, demonstrating the applicability and generalization of the
proposed thermal field prediction method in online applications for metal AM.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16127" title="Abstract">arXiv:2310.16127</a> [<a href="/pdf/2310.16127" title="Download PDF">pdf</a>, <a href="/format/2310.16127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Octopus: A Multitask Model and Toolkit for Arabic Natural Language  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elmadany%2C+A">AbdelRahim Elmadany</a>, 
<a href="/search/cs?searchtype=author&query=Nagoudi%2C+E+M+B">El Moatez Billah Nagoudi</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Understanding Arabic text and generating human-like responses is a
challenging endeavor. While many researchers have proposed models and solutions
for individual problems, there is an acute shortage of a comprehensive Arabic
natural language generation toolkit that is capable of handling a wide range of
tasks. In this work, we present a novel Arabic text-to-text Transformer model,
namely AraT5v2. Our new model is methodically trained on extensive and diverse
data, utilizing an extended sequence length of 2,048 tokens. We explore various
pretraining strategies including unsupervised, supervised, and joint
pertaining, under both single and multitask settings. Our models outperform
competitive baselines with large margins. We take our work one step further by
developing and publicly releasing Octopus, a Python-based package and
command-line toolkit tailored for eight Arabic generation tasks all exploiting
a single model. We release the models and the toolkit on our public repository.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16131" title="Abstract">arXiv:2310.16131</a> [<a href="/pdf/2310.16131" title="Download PDF">pdf</a>, <a href="/format/2310.16131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenKIE: Robust Generative Multimodal Document Key Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+P">Panfeng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ye Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zaiqiao Meng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023, Findings paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Key information extraction (KIE) from scanned documents has gained increasing
attention because of its applications in various domains. Although promising
results have been achieved by some recent KIE approaches, they are usually
built based on discriminative models, which lack the ability to handle optical
character recognition (OCR) errors and require laborious token-level labelling.
In this paper, we propose a novel generative end-to-end model, named GenKIE, to
address the KIE task. GenKIE is a sequence-to-sequence multimodal generative
model that utilizes multimodal encoders to embed visual, layout and textual
features and a decoder to generate the desired output. Well-designed prompts
are leveraged to incorporate the label semantics as the weakly supervised
signals and entice the generation of the key information. One notable advantage
of the generative model is that it enables automatic correction of OCR errors.
Besides, token-level granular annotation is not required. Extensive experiments
on multiple public real-world datasets show that GenKIE effectively generalizes
over different types of documents and achieves state-of-the-art results. Our
experiments also validate the model's robustness against OCR errors, making
GenKIE highly applicable in real-world scenarios.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16132" title="Abstract">arXiv:2310.16132</a> [<a href="/pdf/2310.16132" title="Download PDF">pdf</a>, <a href="/format/2310.16132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversity in Software Engineering Conferences and Journals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+A+S">Aditya Shankar Narayanan</a>, 
<a href="/search/cs?searchtype=author&query=Vagavolu%2C+D">Dheeraj Vagavolu</a>, 
<a href="/search/cs?searchtype=author&query=Day%2C+N+A">Nancy A Day</a>, 
<a href="/search/cs?searchtype=author&query=Nagappan%2C+M">Meiyappan Nagappan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Diversity with respect to ethnicity and gender has been studied in
open-source and industrial settings for software development. Publication
avenues such as academic conferences and journals contribute to the growing
technology industry. However, there have been very few diversity-related
studies conducted in the context of academia. In this paper, we study the
ethnic, gender, and geographical diversity of the authors published in Software
Engineering conferences and journals. We provide a systematic quantitative
analysis of the diversity of publications and organizing and program committees
of three top conferences and two top journals in Software Engineering, which
indicates the existence of bias and entry barriers towards authors and
committee members belonging to certain ethnicities, gender, and/or geographical
locations in Software Engineering conferences and journal publications. For our
study, we analyse publication (accepted authors) and committee data (Program
and Organizing committee/ Journal Editorial Board) from the conferences ICSE,
FSE, and ASE and the journals IEEE TSE and ACM TOSEM from 2010 to 2022. The
analysis of the data shows that across participants and committee members,
there are some communities that are consistently significantly lower in
representation, for example, publications from countries in Africa, South
America, and Oceania. However, a correlation study between the diversity of the
committees and the participants did not yield any conclusive evidence.
Furthermore, there is no conclusive evidence that papers with White authors or
male authors were more likely to be cited. Finally, we see an improvement in
the ethnic diversity of the authors over the years 2010-2022 but not in gender
or geographical diversity.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16134" title="Abstract">arXiv:2310.16134</a> [<a href="/pdf/2310.16134" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Evolution from Design to Verification of the Antenna System and  Mechanisms in the AcubeSAT mission
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bountzioukas%2C+P">Panagiotis Bountzioukas</a>, 
<a href="/search/eess?searchtype=author&query=Kikas%2C+G">Georgios Kikas</a>, 
<a href="/search/eess?searchtype=author&query=Tsiolakis%2C+C">Christoforos Tsiolakis</a>, 
<a href="/search/eess?searchtype=author&query=Stoupis%2C+D">Dimitrios Stoupis</a>, 
<a href="/search/eess?searchtype=author&query=Chatziargyriou%2C+E">Eleftheria Chatziargyriou</a>, 
<a href="/search/eess?searchtype=author&query=Hatzopoulos%2C+A">Alkis Hatzopoulos</a>, 
<a href="/search/eess?searchtype=author&query=Kourampa-Gottfroh%2C+V">Vasiliki Kourampa-Gottfroh</a>, 
<a href="/search/eess?searchtype=author&query=Karakosta-Amarantidou%2C+I">Ilektra Karakosta-Amarantidou</a>, 
<a href="/search/eess?searchtype=author&query=Mavropoulos%2C+A">Aggelos Mavropoulos</a>, 
<a href="/search/eess?searchtype=author&query=Komis%2C+I">Ioannis-Nikolaos Komis</a>, 
<a href="/search/eess?searchtype=author&query=Kita%2C+A">Afroditi Kita</a>, 
<a href="/search/eess?searchtype=author&query=Palma%2C+D">David Palma</a>, 
<a href="/search/eess?searchtype=author&query=Franchi%2C+L">Loris Franchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 74th International Astronautical Congress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM)

</div>
<p class="mathjax">AcubeSAT is an open-source CubeSat mission aiming to explore the effects of
microgravity and radiation on eukaryotic cells using a compact microfluidic LoC
platform. It is developed by SpaceDot, a volunteer, interdisciplinary student
team at the Aristotle University of Thessaloniki and supported by the "Fly Your
Satellite! 3" program of the ESA Education Office. The scientific data of the
mission is comprised of microscope images captured through the on-board
integrated camera setup. As the total size of the payload data is expected to
be close to 2GB over 12 months, a fast and efficient downlink fulfilling the
restrictive power, cost and complexity budgets is required. Currently, there is
no open-source communications system design which fully supports these specific
constraints, so we opted to develop our own solutions. The antenna system
underwent multiple iterations as the design matured, a process highly aided by
the feedback received from the ESA experts. The final communications system
configuration consists of an S-band microstrip antenna operating at 2.4GHz and
a UHF deployable antenna, for the payload data and TM&amp;TC respectively, both
in-house designed. In this paper, we will present AcubeSAT's antenna system
iterations that span over 3 years, as well as the rationale and analysis
results behind each. The development decisions will be highlighted throughout
the paper in an effort to aid in the future development of such a low-cost
CubeSat mission communications system.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16135" title="Abstract">arXiv:2310.16135</a> [<a href="/pdf/2310.16135" title="Download PDF">pdf</a>, <a href="/format/2310.16135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can You Follow Me? Testing Situational Understanding in ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chenghao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ettinger%2C+A">Allyson Ettinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Paper (Camera Ready)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Understanding sentence meanings and updating information states appropriately
across time -- what we call "situational understanding" (SU) -- is a critical
ability for human-like AI agents. SU is essential in particular for chat
models, such as ChatGPT, to enable consistent, coherent, and effective dialogue
between humans and AI. Previous works have identified certain SU limitations in
non-chatbot Large Language models (LLMs), but the extent and causes of these
limitations are not well understood, and capabilities of current chat-based
models in this domain have not been explored. In this work we tackle these
questions, proposing a novel synthetic environment for SU testing which allows
us to do controlled and systematic testing of SU in chat-oriented models,
through assessment of models' ability to track and enumerate environment
states. Our environment also allows for close analysis of dynamics of model
performance, to better understand underlying causes for performance patterns.
We apply our test to ChatGPT, the state-of-the-art chatbot, and find that
despite the fundamental simplicity of the task, the model's performance
reflects an inability to retain correct environment states across time. Our
follow-up analyses suggest that performance degradation is largely because
ChatGPT has non-persistent in-context memory (although it can access the full
dialogue history) and it is susceptible to hallucinated updates -- including
updates that artificially inflate accuracies. Our findings suggest overall that
ChatGPT is not currently equipped for robust tracking of situation states, and
that trust in the impressive dialogue performance of ChatGPT comes with risks.
We release the codebase for reproducing our test environment, as well as all
prompts and API responses from ChatGPT, at
https://github.com/yangalan123/SituationalTesting.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16137" title="Abstract">arXiv:2310.16137</a> [<a href="/pdf/2310.16137" title="Download PDF">pdf</a>, <a href="/format/2310.16137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Codebook-based Uplink Transmission Enhancement in 5G Advanced: Sub-band  Precoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Liu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Shabara%2C+Y">Yahia Shabara</a>, 
<a href="/search/cs?searchtype=author&query=Cheraghi%2C+P">Parisa Cheraghi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted by IEEE VCC 2023. 5 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">The transformative enhancements of fifth-generation (5G) mobile devices bring
about new challenges to achieve better uplink (UL) performance. Particularly,
in codebook-based transmission, the wide-band (WB) precoding and the legacy UL
codebook may become main bottlenecks for higher efficient data transmission. In
this paper, we investigate the codebook-based UL single-layer transmission
performance using fully coherent antenna ports in the context of sub-band (SB)
precoding. We analyze the SB precoder selection criteria and design an UL
codebook used for SB precoding by increasing the number of relative phase
shifts of each port. Via link-level simulations, we verify that the UL SB
precoding can improve up to 2 dB performance gain in terms of the block error
rate (BLER) compared with the UL WB precoding which is the current UL precoding
scheme. We also show that UL performance gain is sensitive to the SB size
selection as well as the relative phase shift diversity.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16138" title="Abstract">arXiv:2310.16138</a> [<a href="/pdf/2310.16138" title="Download PDF">pdf</a>, <a href="/format/2310.16138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subtle Signals: Video-based Detection of Infant Non-nutritive Sucking as  a Neurodevelopmental Cue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Shaotong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+M">Michael Wan</a>, 
<a href="/search/cs?searchtype=author&query=Manne%2C+S+K+R">Sai Kumar Reddy Manne</a>, 
<a href="/search/cs?searchtype=author&query=Zimmerman%2C+E">Emily Zimmerman</a>, 
<a href="/search/cs?searchtype=author&query=Ostadabbas%2C+S">Sarah Ostadabbas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Non-nutritive sucking (NNS), which refers to the act of sucking on a
pacifier, finger, or similar object without nutrient intake, plays a crucial
role in assessing healthy early development. In the case of preterm infants,
NNS behavior is a key component in determining their readiness for feeding. In
older infants, the characteristics of NNS behavior offer valuable insights into
neural and motor development. Additionally, NNS activity has been proposed as a
potential safeguard against sudden infant death syndrome (SIDS). However, the
clinical application of NNS assessment is currently hindered by labor-intensive
and subjective finger-in-mouth evaluations. Consequently, researchers often
resort to expensive pressure transducers for objective NNS signal measurement.
To enhance the accessibility and reliability of NNS signal monitoring for both
clinicians and researchers, we introduce a vision-based algorithm designed for
non-contact detection of NNS activity using baby monitor footage in natural
settings. Our approach involves a comprehensive exploration of optical flow and
temporal convolutional networks, enabling the detection and amplification of
subtle infant-sucking signals. We successfully classify short video clips of
uniform length into NNS and non-NNS periods. Furthermore, we investigate manual
and learning-based techniques to piece together local classification results,
facilitating the segmentation of longer mixed-activity videos into NNS and
non-NNS segments of varying duration. Our research introduces two novel
datasets of annotated infant videos, including one sourced from our clinical
study featuring 19 infant subjects and 183 hours of overnight baby monitor
footage.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16141" title="Abstract">arXiv:2310.16141</a> [<a href="/pdf/2310.16141" title="Download PDF">pdf</a>, <a href="/format/2310.16141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-aware explainable recommendations over knowledge graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+J">Jinfeng Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Negre%2C+E">Elsa Negre</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Knowledge graphs contain rich semantic relationships related to items and
incorporating such semantic relationships into recommender systems helps to
explore the latent connections of items, thus improving the accuracy of
prediction and enhancing the explainability of recommendations. However, such
explainability is not adapted to users' contexts, which can significantly
influence their preferences. In this work, we propose CA-KGCN (Context-Aware
Knowledge Graph Convolutional Network), an end-to-end framework that can model
users' preferences adapted to their contexts and can incorporate rich semantic
relationships in the knowledge graph related to items. This framework captures
users' attention to different factors: contexts and features of items. More
specifically, the framework can model users' preferences adapted to their
contexts and provide explanations adapted to the given context. Experiments on
three real-world datasets show the effectiveness of our framework: modeling
users' preferences adapted to their contexts and explaining the recommendations
generated.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16142" title="Abstract">arXiv:2310.16142</a> [<a href="/pdf/2310.16142" title="Download PDF">pdf</a>, <a href="/format/2310.16142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Language Model with Limited Memory Capacity Captures Interference in  Human Sentence Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Timkey%2C+W">William Timkey</a>, 
<a href="/search/cs?searchtype=author&query=Linzen%2C+T">Tal Linzen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Findings of the Association for Computational Linguistics: EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Two of the central factors believed to underpin human sentence processing
difficulty are expectations and retrieval from working memory. A recent attempt
to create a unified cognitive model integrating these two factors relied on the
parallels between the self-attention mechanism of transformer language models
and cue-based retrieval theories of working memory in human sentence processing
(Ryu and Lewis 2021). While Ryu and Lewis show that attention patterns in
specialized attention heads of GPT-2 are consistent with similarity-based
interference, a key prediction of cue-based retrieval models, their method
requires identifying syntactically specialized attention heads, and makes the
cognitively implausible assumption that hundreds of memory retrieval operations
take place in parallel. In the present work, we develop a recurrent neural
language model with a single self-attention head, which more closely parallels
the memory system assumed by cognitive theories. We show that our model's
single attention head captures semantic and syntactic interference effects
observed in human experiments.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16143" title="Abstract">arXiv:2310.16143</a> [<a href="/pdf/2310.16143" title="Download PDF">pdf</a>, <a href="/format/2310.16143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A particle method for the multispecies Landau equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Corrillo%2C+J+A">Jos&#xe9; A. Corrillo</a>, 
<a href="/search/math?searchtype=author&query=Hu%2C+J">Jingwei Hu</a>, 
<a href="/search/math?searchtype=author&query=Van+Fleet%2C+S+Q">Samuel Q. Van Fleet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 31 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The multispecies Landau collision operator describes the two-particle, small
scattering angle or grazing collisions in a plasma made up of different species
of particles such as electrons and ions. Recently, a structure preserving
deterministic particle method <a href="/abs/1910.03080">arXiv:1910.03080</a> has been developed for the
single species spatially homogeneous Landau equation. This method relies on a
regularization of the Landau collision operator so that an approximate
solution, which is a linear combination of Dirac delta distributions, is
well-defined. Based on a weak form of the regularized Landau equation, the time
dependent locations of the Dirac delta functions satisfy a system of ordinary
differential equations. In this work, we extend this particle method to the
multispecies case, and examine its conservation of mass, momentum, and energy,
and decay of entropy properties. We show that the equilibrium distribution of
the regularized multispecies Landau equation is a Maxwellian distribution, and
state a critical condition on the regularization parameters that guarantees a
species independent equilibrium temperature. A convergence study comparing an
exact multispecies BKW solution to the particle solution shows approximately
2nd order accuracy. Important physical properties such as conservation, decay
of entropy, and equilibrium distribution of the particle method are
demonstrated with several numerical examples.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16144" title="Abstract">arXiv:2310.16144</a> [<a href="/pdf/2310.16144" title="Download PDF">pdf</a>, <a href="/ps/2310.16144" title="Download PostScript">ps</a>, <a href="/format/2310.16144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ROM-Based Stochastic Optimization for a Continuous Manufacturing Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cruz-Oliver%2C+R">Raul Cruz-Oliver</a> (1), 
<a href="/search/eess?searchtype=author&query=Monzon%2C+L">Luis Monzon</a> (2), 
<a href="/search/eess?searchtype=author&query=Ramirez-Laboreo%2C+E">Edgar Ramirez-Laboreo</a> (3), 
<a href="/search/eess?searchtype=author&query=Rodriguez-Fortun%2C+J">Jose-Manuel Rodriguez-Fortun</a> (2) ((1) ETH Zurich, (2) Instituto Tecnologico de Aragon, (3) Universidad de Zaragoza)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper proposes a model-based optimization method for the production of
automotive seals in an extrusion process. The high production throughput,
coupled with quality constraints and the inherent uncertainty of the process,
encourages the search for operating conditions that minimize nonconformities.
The main uncertainties arise from the process variability and from the raw
material itself. The proposed method, based on Bayesian optimization, takes
these factors into account and obtains a robust set of process parameters. Due
to the high computational cost and complexity of performing detailed
simulations, a reduced order model is used to address the optimization. The
proposal has been evaluated in a virtual environment where it is shown that the
performance of the solution found minimizes the effects of process
uncertainties.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16145" title="Abstract">arXiv:2310.16145</a> [<a href="/pdf/2310.16145" title="Download PDF">pdf</a>, <a href="/ps/2310.16145" title="Download PostScript">ps</a>, <a href="/format/2310.16145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Positive Almost-Sure Termination -- Complexity and Proof Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Majumdar%2C+R">Rupak Majumdar</a>, 
<a href="/search/cs?searchtype=author&query=Sathiyanarayana%2C+V+R">V.R. Sathiyanarayana</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">We study the recursion-theoretic complexity of Positive Almost-Sure
Termination ($\mathsf{PAST}$) in an imperative programming language with
rational variables, bounded nondeterministic choice, and discrete probabilistic
choice. A program terminates positive almost-surely if, for every scheduler,
the program terminates almost-surely and the expected runtime to termination is
finite. We show that $\mathsf{PAST}$ for our language is complete for the
(lightface) co-analytic sets ($\Pi^1_1$-complete) - this is in contrast to the
related notions of Almost-Sure Termination ($\mathsf{AST}$) and Bounded
Termination ($\mathsf{BAST}$), both of which are arithmetical ($\Pi^0_2$ and
$\Sigma^0_2$ complete respectively).
<br />Our upper bound implies an effective procedure to reduce reasoning about
probabilistic termination to non-probabilistic fair termination in a model with
bounded nondeterminism, and to simple program termination in models with
unbounded nondeterminism. Our lower bound shows the opposite: for every program
with unbounded nondeterministic choice, there is an effectively computable
probabilistic program with bounded choice such that the original program is
terminating $iff$ the transformed program is $\mathsf{PAST}$.
<br />We show that every program has an effectively computable normal form, in
which each probabilistic choice either continues or terminates execution. For
normal form programs, we provide the first sound and complete proof rule for
$\mathsf{PAST}$. Our proof rule uses transfinite ordinals. We show that
reasoning about $\mathsf{PAST}$ requires transfinite ordinals up to
$\omega^{CK}_1$; thus, existing techniques for probabilistic termination based
on ranking supermartingales that map program states to reals do not suffice to
reason about $\mathsf{PAST}$.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16146" title="Abstract">arXiv:2310.16146</a> [<a href="/pdf/2310.16146" title="Download PDF">pdf</a>, <a href="/format/2310.16146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model  System for Answering Medical Questions using Scientific Literature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lozano%2C+A">Alejandro Lozano</a>, 
<a href="/search/cs?searchtype=author&query=Fleming%2C+S+L">Scott L Fleming</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+C">Chia-Chun Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Nigam Shah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint of an article published in Pacific Symposium on Biocomputing copyright 2024 World Scientific Publishing Co., Singapore, <a href="http://psb.stanford.edu/">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">The quickly-expanding nature of published medical literature makes it
challenging for clinicians and researchers to keep up with and summarize
recent, relevant findings in a timely manner. While several closed-source
summarization tools based on large language models (LLMs) now exist, rigorous
and systematic evaluations of their outputs are lacking. Furthermore, there is
a paucity of high-quality datasets and appropriate benchmark tasks with which
to evaluate these tools. We address these issues with four contributions: we
release Clinfo.ai, an open-source WebApp that answers clinical questions based
on dynamically retrieved scientific literature; we specify an information
retrieval and abstractive summarization task to evaluate the performance of
such retrieval-augmented LLM systems; we release a dataset of 200 questions and
corresponding answers derived from published systematic reviews, which we name
PubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for
Clinfo.ai and other publicly available OpenQA systems on PubMedRS-200.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16147" title="Abstract">arXiv:2310.16147</a> [<a href="/pdf/2310.16147" title="Download PDF">pdf</a>, <a href="/format/2310.16147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PreWoMe: Exploiting Presuppositions as Working Memory for Long Form  Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Wookje Han</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jinsol Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kyungjae Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages 3 figures, Accepted to EMNLP 2023 (short)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Information-seeking questions in long-form question answering (LFQA) often
prove misleading due to ambiguity or false presupposition in the question.
While many existing approaches handle misleading questions, they are tailored
to limited questions, which are insufficient in a real-world setting with
unpredictable input characteristics. In this work, we propose PreWoMe, a
unified approach capable of handling any type of information-seeking question.
The key idea of PreWoMe involves extracting presuppositions in the question and
exploiting them as working memory to generate feedback and action about the
question. Our experiment shows that PreWoMe is effective not only in tackling
misleading questions but also in handling normal ones, thereby demonstrating
the effectiveness of leveraging presuppositions, feedback, and action for
real-world QA settings.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16148" title="Abstract">arXiv:2310.16148</a> [<a href="/pdf/2310.16148" title="Download PDF">pdf</a>, <a href="/format/2310.16148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Yin Yang Convolutional Nets: Image Manifold Extraction by the Analysis  of Opposites
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=da+Rosa%2C+A+S">Augusto Seben da Rosa</a>, 
<a href="/search/cs?searchtype=author&query=de+Oliveira%2C+F+S">Frederico Santos de Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=da+Silva+Soares%2C+A">Anderson da Silva Soares</a>, 
<a href="/search/cs?searchtype=author&query=Junior%2C+A+C">Arnaldo Candido Junior</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 tables and 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Computer vision in general presented several advances such as training
optimizations, new architectures (pure attention, efficient block, vision
language models, generative models, among others). This have improved
performance in several tasks such as classification, and others. However, the
majority of these models focus on modifications that are taking distance from
realistic neuroscientific approaches related to the brain. In this work, we
adopt a more bio-inspired approach and present the Yin Yang Convolutional
Network, an architecture that extracts visual manifold, its blocks are intended
to separate analysis of colors and forms at its initial layers, simulating
occipital lobe's operations. Our results shows that our architecture provides
State-of-the-Art efficiency among low parameter architectures in the dataset
CIFAR-10. Our first model reached 93.32\% test accuracy, 0.8\% more than the
older SOTA in this category, while having 150k less parameters (726k in total).
Our second model uses 52k parameters, losing only 3.86\% test accuracy. We also
performed an analysis on ImageNet, where we reached 66.49\% validation accuracy
with 1.6M parameters. We make the code publicly available at:
https://github.com/NoSavedDATA/YinYang_CNN.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16152" title="Abstract">arXiv:2310.16152</a> [<a href="/pdf/2310.16152" title="Download PDF">pdf</a>, <a href="/format/2310.16152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLTrojan: Privacy Leakage Attacks against Federated Language Models  Through Selective Weight Tampering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rashid%2C+M+R+U">Md Rafi Ur Rashid</a>, 
<a href="/search/cs?searchtype=author&query=Dasu%2C+V+A">Vishnu Asutosh Dasu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+K">Kang Gu</a>, 
<a href="/search/cs?searchtype=author&query=Sultana%2C+N">Najrin Sultana</a>, 
<a href="/search/cs?searchtype=author&query=Mehnaz%2C+S">Shagufta Mehnaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages (including bibliography and Appendix), Submitted to USENIX Security '24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Federated learning (FL) is becoming a key component in many technology-based
applications including language modeling -- where individual FL participants
often have privacy-sensitive text data in their local datasets. However,
realizing the extent of privacy leakage in federated language models is not
straightforward and the existing attacks only intend to extract data regardless
of how sensitive or naive it is. To fill this gap, in this paper, we introduce
two novel findings with regard to leaking privacy-sensitive user data from
federated language models. Firstly, we make a key observation that model
snapshots from the intermediate rounds in FL can cause greater privacy leakage
than the final trained model. Secondly, we identify that privacy leakage can be
aggravated by tampering with a model's selective weights that are specifically
responsible for memorizing the sensitive training data. We show how a malicious
client can leak the privacy-sensitive data of some other user in FL even
without any cooperation from the server. Our best-performing method improves
the membership inference recall by 29% and achieves up to 70% private data
reconstruction, evidently outperforming existing attacks with stronger
assumptions of adversary capabilities.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16153" title="Abstract">arXiv:2310.16153</a> [<a href="/pdf/2310.16153" title="Download PDF">pdf</a>, <a href="/format/2310.16153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WojoodNER 2023: The First Arabic Named Entity Recognition Shared Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jarrar%2C+M">Mustafa Jarrar</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>, 
<a href="/search/cs?searchtype=author&query=Khalilia%2C+M">Mohammed Khalilia</a>, 
<a href="/search/cs?searchtype=author&query=Talafha%2C+B">Bashar Talafha</a>, 
<a href="/search/cs?searchtype=author&query=Elmadany%2C+A">AbdelRahim Elmadany</a>, 
<a href="/search/cs?searchtype=author&query=Hamad%2C+N">Nagham Hamad</a>, 
<a href="/search/cs?searchtype=author&query=Omar%2C+A">Alaa&#x27; Omar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present WojoodNER-2023, the first Arabic Named Entity Recognition (NER)
Shared Task. The primary focus of WojoodNER-2023 is on Arabic NER, offering
novel NER datasets (i.e., Wojood) and the definition of subtasks designed to
facilitate meaningful comparisons between different NER approaches.
WojoodNER-2023 encompassed two Subtasks: FlatNER and NestedNER. A total of 45
unique teams registered for this shared task, with 11 of them actively
participating in the test phase. Specifically, 11 teams participated in
FlatNER, while $8$ teams tackled NestedNER. The winning teams achieved F1
scores of 91.96 and 93.73 in FlatNER and NestedNER, respectively.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16154" title="Abstract">arXiv:2310.16154</a> [<a href="/pdf/2310.16154" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking the Curse of Dimensionality in Deep Neural Networks by Learning  Invariant Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petrini%2C+L">Leonardo Petrini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhD Thesis @ EPFL
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Artificial intelligence, particularly the subfield of machine learning, has
seen a paradigm shift towards data-driven models that learn from and adapt to
data. This has resulted in unprecedented advancements in various domains such
as natural language processing and computer vision, largely attributed to deep
learning, a special class of machine learning models. Deep learning arguably
surpasses traditional approaches by learning the relevant features from raw
data through a series of computational layers.
<br />This thesis explores the theoretical foundations of deep learning by studying
the relationship between the architecture of these models and the inherent
structures found within the data they process. In particular, we ask What
drives the efficacy of deep learning algorithms and allows them to beat the
so-called curse of dimensionality-i.e. the difficulty of generally learning
functions in high dimensions due to the exponentially increasing need for data
points with increased dimensionality? Is it their ability to learn relevant
representations of the data by exploiting their structure? How do different
architectures exploit different data structures? In order to address these
questions, we push forward the idea that the structure of the data can be
effectively characterized by its invariances-i.e. aspects that are irrelevant
for the task at hand.
<br />Our methodology takes an empirical approach to deep learning, combining
experimental studies with physics-inspired toy models. These simplified models
allow us to investigate and interpret the complex behaviors we observe in deep
learning systems, offering insights into their inner workings, with the
far-reaching goal of bridging the gap between theory and practice.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16157" title="Abstract">arXiv:2310.16157</a> [<a href="/pdf/2310.16157" title="Download PDF">pdf</a>, <a href="/format/2310.16157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-aware feature attribution through argumentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+J">Jinfeng Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Negre%2C+E">Elsa Negre</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Applications (stat.AP)

</div>
<p class="mathjax">Feature attribution is a fundamental task in both machine learning and data
analysis, which involves determining the contribution of individual features or
variables to a model's output. This process helps identify the most important
features for predicting an outcome. The history of feature attribution methods
can be traced back to General Additive Models (GAMs), which extend linear
regression models by incorporating non-linear relationships between dependent
and independent variables. In recent years, gradient-based methods and
surrogate models have been applied to unravel complex Artificial Intelligence
(AI) systems, but these methods have limitations. GAMs tend to achieve lower
accuracy, gradient-based methods can be difficult to interpret, and surrogate
models often suffer from stability and fidelity issues. Furthermore, most
existing methods do not consider users' contexts, which can significantly
influence their preferences. To address these limitations and advance the
current state-of-the-art, we define a novel feature attribution framework
called Context-Aware Feature Attribution Through Argumentation (CA-FATA). Our
framework harnesses the power of argumentation by treating each feature as an
argument that can either support, attack or neutralize a prediction.
Additionally, CA-FATA formulates feature attribution as an argumentation
procedure, and each computation has explicit semantics, which makes it
inherently interpretable. CA-FATA also easily integrates side information, such
as users' contexts, resulting in more accurate predictions.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16161" title="Abstract">arXiv:2310.16161</a> [<a href="/pdf/2310.16161" title="Download PDF">pdf</a>, <a href="/format/2310.16161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MyriadAL: Active Few Shot Learning for Histopathology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schiavone%2C+N">Nico Schiavone</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuangzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zemp%2C+R">Roger Zemp</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingyu Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Active Learning (AL) and Few Shot Learning (FSL) are two label-efficient
methods which have achieved excellent results recently. However, most prior
arts in both learning paradigms fail to explore the wealth of the vast
unlabelled data. In this study, we address this issue in the scenario where the
annotation budget is very limited, yet a large amount of unlabelled data for
the target task is available. We frame this work in the context of
histopathology where labelling is prohibitively expensive. To this end, we
introduce an active few shot learning framework, Myriad Active Learning (MAL),
including a contrastive-learning encoder, pseudo-label generation, and novel
query sample selection in the loop. Specifically, we propose to massage
unlabelled data in a self-supervised manner, where the obtained data
representations and clustering knowledge form the basis to activate the AL
loop. With feedback from the oracle in each AL cycle, the pseudo-labels of the
unlabelled data are refined by optimizing a shallow task-specific net on top of
the encoder. These updated pseudo-labels serve to inform and improve the active
learning query selection process. Furthermore, we introduce a novel recipe to
combine existing uncertainty measures and utilize the entire uncertainty list
to reduce sample redundancy in AL. Extensive experiments on two public
histopathology datasets show that MAL has superior test accuracy, macro
F1-score, and label efficiency compared to prior works, and can achieve a
comparable test accuracy to a fully supervised algorithm while labelling only
5% of the dataset.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16162" title="Abstract">arXiv:2310.16162</a> [<a href="/pdf/2310.16162" title="Download PDF">pdf</a>, <a href="/format/2310.16162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Brainchop: Next Generation Web-Based Neuroimaging Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masoud%2C+M">Mohamed Masoud</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+P">Pratyush Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+F">Farfalla Hu</a>, 
<a href="/search/cs?searchtype=author&query=Plis%2C+S">Sergey Plis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Performing volumetric image processing directly within the browser,
particularly with medical data, presents unprecedented challenges compared to
conventional backend tools. These challenges arise from limitations inherent in
browser environments, such as constrained computational resources and the
availability of frontend machine learning libraries. Consequently, there is a
shortage of neuroimaging frontend tools capable of providing comprehensive
end-to-end solutions for whole brain preprocessing and segmentation while
preserving end-user data privacy and residency. In light of this context, we
introduce Brainchop (<a href="http://www.brainchop.org">this http URL</a>) as a groundbreaking in-browser
neuroimaging tool that enables volumetric analysis of structural MRI using
pre-trained full-brain deep learning models, all without requiring technical
expertise or intricate setup procedures. Beyond its commitment to data privacy,
this frontend tool offers multiple features, including scalability, low
latency, user-friendly operation, cross-platform compatibility, and enhanced
accessibility. This paper outlines the processing pipeline of Brainchop and
evaluates the performance of models across various software and hardware
configurations. The results demonstrate the practicality of client-side
processing for volumetric data, owing to the robust MeshNet architecture, even
within the resource-constrained environment of web browsers.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16164" title="Abstract">arXiv:2310.16164</a> [<a href="/pdf/2310.16164" title="Download PDF">pdf</a>, <a href="/format/2310.16164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conversational Challenges in AI-Powered Data Science: Obstacles, Needs,  and Design Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chopra%2C+B">Bhavya Chopra</a>, 
<a href="/search/cs?searchtype=author&query=Singha%2C+A">Ananya Singha</a>, 
<a href="/search/cs?searchtype=author&query=Fariha%2C+A">Anna Fariha</a>, 
<a href="/search/cs?searchtype=author&query=Gulwani%2C+S">Sumit Gulwani</a>, 
<a href="/search/cs?searchtype=author&query=Parnin%2C+C">Chris Parnin</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+A">Ashish Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Henley%2C+A+Z">Austin Z. Henley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) are being increasingly employed in data science
for tasks like data preprocessing and analytics. However, data scientists
encounter substantial obstacles when conversing with LLM-powered chatbots and
acting on their suggestions and answers. We conducted a mixed-methods study,
including contextual observations, semi-structured interviews (n=14), and a
survey (n=114), to identify these challenges. Our findings highlight key issues
faced by data scientists, including contextual data retrieval, formulating
prompts for complex tasks, adapting generated code to local environments, and
refining prompts iteratively. Based on these insights, we propose actionable
design recommendations, such as data brushing to support context selection, and
inquisitive feedback loops to improve communications with AI-based assistants
in data-science tools.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16165" title="Abstract">arXiv:2310.16165</a> [<a href="/pdf/2310.16165" title="Download PDF">pdf</a>, <a href="/format/2310.16165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Staircase Codes with Arbitrary Bit Degree
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shehadeh%2C+M">Mohannad Shehadeh</a>, 
<a href="/search/cs?searchtype=author&query=Kschischang%2C+F+R">Frank R. Kschischang</a>, 
<a href="/search/cs?searchtype=author&query=Sukmadji%2C+a+A+Y">and Alvin Y. Sukmadji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to 2024 Optical Fiber Communication Conference (OFC 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We introduce a natural generalization of staircase codes in which each bit is
protected by arbitrarily many component codewords rather than two. This enables
powerful energy-efficient FEC based on iterative decoding of Hamming
components.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16167" title="Abstract">arXiv:2310.16167</a> [<a href="/pdf/2310.16167" title="Download PDF">pdf</a>, <a href="/format/2310.16167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> iNVS: Repurposing Diffusion Inpainters for Novel View Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kant%2C+Y">Yash Kant</a>, 
<a href="/search/cs?searchtype=author&query=Siarohin%2C+A">Aliaksandr Siarohin</a>, 
<a href="/search/cs?searchtype=author&query=Vasilkovsky%2C+M">Michael Vasilkovsky</a>, 
<a href="/search/cs?searchtype=author&query=Guler%2C+R+A">Riza Alp Guler</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jian Ren</a>, 
<a href="/search/cs?searchtype=author&query=Tulyakov%2C+S">Sergey Tulyakov</a>, 
<a href="/search/cs?searchtype=author&query=Gilitschenski%2C+I">Igor Gilitschenski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to SIGGRAPH Asia, 2023 (Conference Papers)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present a method for generating consistent novel views from a single
source image. Our approach focuses on maximizing the reuse of visible pixels
from the source image. To achieve this, we use a monocular depth estimator that
transfers visible pixels from the source view to the target view. Starting from
a pre-trained 2D inpainting diffusion model, we train our method on the
large-scale Objaverse dataset to learn 3D object priors. While training we use
a novel masking mechanism based on epipolar lines to further improve the
quality of our approach. This allows our framework to perform zero-shot novel
view synthesis on a variety of objects. We evaluate the zero-shot abilities of
our framework on three challenging datasets: Google Scanned Objects, Ray Traced
Multiview, and Common Objects in 3D. See our webpage for more details:
https://yashkant.github.io/invs/
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16169" title="Abstract">arXiv:2310.16169</a> [<a href="/pdf/2310.16169" title="Download PDF">pdf</a>, <a href="/format/2310.16169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bayesian model calibration framework for stochastic compartmental  models with both time-varying and time-invariant parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Robinson%2C+B">Brandon Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Bisaillon%2C+P">Philippe Bisaillon</a>, 
<a href="/search/cs?searchtype=author&query=Edwards%2C+J+D">Jodi D. Edwards</a>, 
<a href="/search/cs?searchtype=author&query=Kendzerska%2C+T">Tetyana Kendzerska</a>, 
<a href="/search/cs?searchtype=author&query=Khalil%2C+M">Mohammad Khalil</a>, 
<a href="/search/cs?searchtype=author&query=Poirel%2C+D">Dominique Poirel</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Abhijit Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">We consider state and parameter estimation for compartmental models having
both time-varying and time-invariant parameters. Though the described Bayesian
computational framework is general, we look at a specific application to the
susceptible-infectious-removed (SIR) model which describes a basic mechanism
for the spread of infectious diseases through a system of coupled nonlinear
differential equations. The SIR model consists of three states, namely, the
three compartments, and two parameters which control the coupling among the
states. The deterministic SIR model with time-invariant parameters has shown to
be overly simplistic for modelling the complex long-term dynamics of diseases
transmission. Recognizing that certain model parameters will naturally vary in
time due to seasonal trends, non-pharmaceutical interventions, and other random
effects, the estimation procedure must systematically permit these time-varying
effects to be captured, without unduly introducing artificial dynamics into the
system. To this end, we leverage the robustness of the Markov Chain Monte Carlo
(MCMC) algorithm for the estimation of time-invariant parameters alongside
nonlinear filters for the joint estimation of the system state and time-varying
parameters. We demonstrate performance of the framework by first considering a
series of examples using synthetic data, followed by an exposition on public
health data collected in the province of Ontario.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16170" title="Abstract">arXiv:2310.16170</a> [<a href="/pdf/2310.16170" title="Download PDF">pdf</a>, <a href="/format/2310.16170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A high order accurate bound-preserving compact finite difference scheme  for scalar convection diffusion equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/math?searchtype=author&query=Xie%2C+S">Shusen Xie</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+X">Xiangxiong Zhang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SIAM Journal on Numerical Analysis 56.6 (2018): 3308-3345
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We show that the classical fourth order accurate compact finite difference
scheme with high order strong stability preserving time discretizations for
convection diffusion problems satisfies a weak monotonicity property, which
implies that a simple limiter can enforce the bound-preserving property without
losing conservation and high order accuracy. Higher order accurate compact
finite difference schemes satisfying the weak monotonicity will also be
discussed.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16171" title="Abstract">arXiv:2310.16171</a> [<a href="/pdf/2310.16171" title="Download PDF">pdf</a>, <a href="/format/2310.16171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A high order accurate bound-preserving compact finite difference scheme  for two-dimensional incompressible flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+X">Xiangxiong Zhang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Communications on Applied Mathematics and Computation (2023): 1-29
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">For solving two-dimensional incompressible flow in the vorticity form by the
fourth-order compact finite difference scheme and explicit strong stability
preserving (SSP) temporal discretizations, we show that the simple
bound-preserving limiter in [Li H., Xie S., Zhang X., SIAM J. Numer. Anal., 56
(2018)]. can enforce the strict bounds of the vorticity, if the velocity field
satisfies a discrete divergence free constraint. For reducing oscillations, a
modified TVB limiter adapted from [Cockburn B., Shu CW., SIAM J. Numer. Anal.,
31 (1994)] is constructed without affecting the bound-preserving property. This
bound-preserving finite difference method can be used for any passive
convection equation with a divergence free velocity field.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16173" title="Abstract">arXiv:2310.16173</a> [<a href="/pdf/2310.16173" title="Download PDF">pdf</a>, <a href="/format/2310.16173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Convergence and Sample Complexity Analysis of Deep Q-Networks  with $&#x3b5;$-Greedy Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongkang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Meng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Miao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Songtao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Murugesan%2C+K">Keerthiram Murugesan</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhury%2C+S">Subhajit Chaudhury</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Neurips 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper provides a theoretical understanding of Deep Q-Network (DQN) with
the $\varepsilon$-greedy exploration in deep reinforcement learning. Despite
the tremendous empirical achievement of the DQN, its theoretical
characterization remains underexplored. First, the exploration strategy is
either impractical or ignored in the existing analysis. Second, in contrast to
conventional Q-learning algorithms, the DQN employs the target network and
experience replay to acquire an unbiased estimation of the mean-square Bellman
error (MSBE) utilized in training the Q-network. However, the existing
theoretical analysis of DQNs lacks convergence analysis or bypasses the
technical challenges by deploying a significantly overparameterized neural
network, which is not computationally efficient. This paper provides the first
theoretical convergence and sample complexity analysis of the practical setting
of DQNs with $\epsilon$-greedy policy. We prove an iterative procedure with
decaying $\epsilon$ converges to the optimal Q-value function geometrically.
Moreover, a higher level of $\epsilon$ values enlarges the region of
convergence but slows down the convergence, while the opposite holds for a
lower level of $\epsilon$ values. Experiments justify our established
theoretical insights on DQNs.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16176" title="Abstract">arXiv:2310.16176</a> [<a href="/pdf/2310.16176" title="Download PDF">pdf</a>, <a href="/format/2310.16176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correction with Backtracking Reduces Hallucination in Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenzhen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+C">Chao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Kishore%2C+V">Varsha Kishore</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J+P">Jin Peng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Minmin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Weinberger%2C+K+Q">Kilian Q. Weinberger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Abstractive summarization aims at generating natural language summaries of a
source document that are succinct while preserving the important elements.
Despite recent advances, neural text summarization models are known to be
susceptible to hallucinating (or more correctly confabulating), that is to
produce summaries with details that are not grounded in the source document. In
this paper, we introduce a simple yet efficient technique, CoBa, to reduce
hallucination in abstractive summarization. The approach is based on two steps:
hallucination detection and mitigation. We show that the former can be achieved
through measuring simple statistics about conditional word probabilities and
distance to context words. Further, we demonstrate that straight-forward
backtracking is surprisingly effective at mitigation. We thoroughly evaluate
the proposed method with prior art on three benchmark datasets for text
summarization. The results show that CoBa is effective and efficient in
reducing hallucination, and offers great adaptability and flexibility.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16181" title="Abstract">arXiv:2310.16181</a> [<a href="/pdf/2310.16181" title="Download PDF">pdf</a>, <a href="/format/2310.16181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hidden Citations Obscure True Impact in Science
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+X">Xiangyi Meng</a>, 
<a href="/search/cs?searchtype=author&query=Varol%2C+O">Onur Varol</a>, 
<a href="/search/cs?searchtype=author&query=Barab%C3%A1si%2C+A">Albert-L&#xe1;szl&#xf3; Barab&#xe1;si</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Digital Libraries (cs.DL); Social and Information Networks (cs.SI); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">References, the mechanism scientists rely on to signal previous knowledge,
lately have turned into widely used and misused measures of scientific impact.
Yet, when a discovery becomes common knowledge, citations suffer from
obliteration by incorporation. This leads to the concept of hidden citation,
representing a clear textual credit to a discovery without a reference to the
publication embodying it. Here, we rely on unsupervised interpretable machine
learning applied to the full text of each paper to systematically identify
hidden citations. We find that for influential discoveries hidden citations
outnumber citation counts, emerging regardless of publishing venue and
discipline. We show that the prevalence of hidden citations is not driven by
citation counts, but rather by the degree of the discourse on the topic within
the text of the manuscripts, indicating that the more discussed is a discovery,
the less visible it is to standard bibliometric analysis. Hidden citations
indicate that bibliometric measures offer a limited perspective on quantifying
the true impact of a discovery, raising the need to extract knowledge from the
full text of the scientific corpus.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16183" title="Abstract">arXiv:2310.16183</a> [<a href="/pdf/2310.16183" title="Download PDF">pdf</a>, <a href="/format/2310.16183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BLP 2023 Task 2: Sentiment Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M+A">Md. Arid Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+F">Firoj Alam</a>, 
<a href="/search/cs?searchtype=author&query=Anjum%2C+A">Anika Anjum</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Shudipta Das</a>, 
<a href="/search/cs?searchtype=author&query=Anjum%2C+A">Afiyat Anjum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in BLP Workshop at EMNLP-23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present an overview of the BLP Sentiment Shared Task, organized as part of
the inaugural BLP 2023 workshop, co-located with EMNLP 2023. The task is
defined as the detection of sentiment in a given piece of social media text.
This task attracted interest from 71 participants, among whom 29 and 30 teams
submitted systems during the development and evaluation phases, respectively.
In total, participants submitted 597 runs. However, a total of 15 teams
submitted system description papers. The range of approaches in the submitted
systems spans from classical machine learning models, fine-tuning pre-trained
models, to leveraging Large Language Model (LLMs) in zero- and few-shot
settings. In this paper, we provide a detailed account of the task setup,
including dataset development and evaluation setup. Additionally, we provide a
brief overview of the systems submitted by the participants. All datasets and
evaluation scripts from the shared task have been made publicly available for
the research community, to foster further research in this domain
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16185" title="Abstract">arXiv:2310.16185</a> [<a href="/pdf/2310.16185" title="Download PDF">pdf</a>, <a href="/format/2310.16185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visualizing Information on Smartwatch Faces: A Review and Design Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Islam%2C+A">Alaul Islam</a> (1), 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tingying He</a> (1), 
<a href="/search/cs?searchtype=author&query=Bezerianos%2C+A">Anastasia Bezerianos</a> (1), 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Bongshin Lee</a> (2), 
<a href="/search/cs?searchtype=author&query=Blascheck%2C+T">Tanja Blascheck</a> (3), 
<a href="/search/cs?searchtype=author&query=Isenberg%2C+P">Petra Isenberg</a> (1) ((1) Universit&#xe9; Paris-Saclay, CNRS, Inria, France, (2) Microsoft Research, Redmond, WA, US, (3) University of Stuttgart, Germany)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">We present a systematic review and design space for visualizations on
smartwatches and the context in which these visualizations are
displayed--smartwatch faces. A smartwatch face is the main smartwatch screen
that wearers see when checking the time. Smartwatch faces are small data
dashboards that present a variety of data to wearers in a compact form. Yet,
the usage context and form factor of smartwatch faces pose unique design
challenges for visualization. In this paper, we present an in-depth review and
analysis of visualization designs for popular premium smartwatch faces based on
their design styles, amount and types of data, as well as visualization styles
and encodings they included. From our analysis we derive a design space to
provide an overview of the important considerations for new data displays for
smartwatch faces and other small displays. Our design space can also serve as
inspiration for design choices and grounding of empirical work on smartwatch
visualization design. We end with a research agenda that points to open
opportunities in this nascent research direction. Supplementary material is
available at: https://osf.io/nwy2r/.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16186" title="Abstract">arXiv:2310.16186</a> [<a href="/pdf/2310.16186" title="Download PDF">pdf</a>, <a href="/format/2310.16186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Segmentation using U-Net Architecture for Powder X-ray Diffraction  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yanxon%2C+H">Howard Yanxon</a>, 
<a href="/search/cs?searchtype=author&query=Roberts%2C+E">Eric Roberts</a>, 
<a href="/search/cs?searchtype=author&query=Parraga%2C+H">Hannah Parraga</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+J">James Weng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenqian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ruett%2C+U">Uta Ruett</a>, 
<a href="/search/cs?searchtype=author&query=Hexemer%2C+A">Alexander Hexemer</a>, 
<a href="/search/cs?searchtype=author&query=Zwart%2C+P">Petrus Zwart</a>, 
<a href="/search/cs?searchtype=author&query=Schwarz%2C+N">Nickolas Schwarz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; High Energy Physics - Experiment (hep-ex)

</div>
<p class="mathjax">Scientific researchers frequently use the in situ synchrotron high-energy
powder X-ray diffraction (XRD) technique to examine the crystallographic
structures of materials in functional devices such as rechargeable battery
materials. We propose a method for identifying artifacts in experimental XRD
images. The proposed method uses deep learning convolutional neural network
architectures, such as tunable U-Nets to identify the artifacts. In particular,
the predicted artifacts are evaluated against the corresponding ground truth
(manually implemented) using the overall true positive rate or recall. The
result demonstrates that the U-Nets can consistently produce great recall
performance at 92.4% on the test dataset, which is not included in the
training, with a 34% reduction in average false positives in comparison to the
conventional method. The U-Nets also reduce the time required to identify and
separate artifacts by more than 50%. Furthermore, the exclusion of the
artifacts shows major changes in the integrated 1D XRD pattern, enhancing
further analysis of the post-processing XRD data.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16187" title="Abstract">arXiv:2310.16187</a> [<a href="/pdf/2310.16187" title="Download PDF">pdf</a>, <a href="/format/2310.16187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient deep data assimilation with sparse observations and  time-varying sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Sibo Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Che Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yike Guo</a>, 
<a href="/search/cs?searchtype=author&query=Arcucci%2C+R">Rossella Arcucci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Mathematical Physics (math-ph)

</div>
<p class="mathjax">Variational Data Assimilation (DA) has been broadly used in engineering
problems for field reconstruction and prediction by performing a weighted
combination of multiple sources of noisy data. In recent years, the integration
of deep learning (DL) techniques in DA has shown promise in improving the
efficiency and accuracy in high-dimensional dynamical systems. Nevertheless,
existing deep DA approaches face difficulties in dealing with unstructured
observation data, especially when the placement and number of sensors are
dynamic over time. We introduce a novel variational DA scheme, named
Voronoi-tessellation Inverse operator for VariatIonal Data assimilation
(VIVID), that incorporates a DL inverse operator into the assimilation
objective function. By leveraging the capabilities of the Voronoi-tessellation
and convolutional neural networks, VIVID is adept at handling sparse,
unstructured, and time-varying sensor data. Furthermore, the incorporation of
the DL inverse operator establishes a direct link between observation and state
space, leading to a reduction in the number of minimization steps required for
DA. Additionally, VIVID can be seamlessly integrated with Proper Orthogonal
Decomposition (POD) to develop an end-to-end reduced-order DA scheme, which can
further expedite field reconstruction. Numerical experiments in a fluid
dynamics system demonstrate that VIVID can significantly outperform existing DA
and DL algorithms. The robustness of VIVID is also accessed through the
application of various levels of prior error, the utilization of varying
numbers of sensors, and the misspecification of error covariance in DA.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16189" title="Abstract">arXiv:2310.16189</a> [<a href="/pdf/2310.16189" title="Download PDF">pdf</a>, <a href="/format/2310.16189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Jacobian-based tasks: Extended set-based tasks for multi-task  execution and prioritization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Notomista%2C+G">Gennaro Notomista</a>, 
<a href="/search/eess?searchtype=author&query=Selvaggio%2C+M">Mario Selvaggio</a>, 
<a href="/search/eess?searchtype=author&query=Santos%2C+M">Mar&#xed;a Santos</a>, 
<a href="/search/eess?searchtype=author&query=Mayya%2C+S">Siddharth Mayya</a>, 
<a href="/search/eess?searchtype=author&query=Pagano%2C+F">Francesca Pagano</a>, 
<a href="/search/eess?searchtype=author&query=Lippiello%2C+V">Vincenzo Lippiello</a>, 
<a href="/search/eess?searchtype=author&query=Secchi%2C+C">Cristian Secchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO); Optimization and Control (math.OC)

</div>
<p class="mathjax">The ability of executing multiple tasks simultaneously is an important
feature of redundant robotic systems. As a matter of fact, complex behaviors
can often be obtained as a result of the execution of several tasks. Moreover,
in safety-critical applications, tasks designed to ensure the safety of the
robot and its surroundings have to be executed along with other nominal tasks.
In such cases, it is also important to prioritize the former over the latter.
In this paper, we formalize the definition of extended set-based tasks, i.e.,
tasks which can be executed by rendering subsets of the task space
asymptotically stable or forward invariant. We propose a mathematical
representation of such tasks that allows for the execution of more complex and
time-varying prioritized stacks of tasks using kinematic and dynamic robot
models alike. We present and analyze an optimization-based framework which is
computationally efficient, accounts for input bounds, and allows for the stable
execution of time-varying prioritized stacks of extended set-based tasks. The
proposed framework is validated using extensive simulations and experiments
with robotic manipulators.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16190" title="Abstract">arXiv:2310.16190</a> [<a href="/pdf/2310.16190" title="Download PDF">pdf</a>, <a href="/format/2310.16190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilayer Environment and Toolchain for Holistic NetwOrk Design and  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rezabek%2C+F">Filip Rezabek</a>, 
<a href="/search/cs?searchtype=author&query=Glas%2C+K">Kilian Glas</a>, 
<a href="/search/cs?searchtype=author&query=von+Seck%2C+R">Richard von Seck</a>, 
<a href="/search/cs?searchtype=author&query=Aroua%2C+A">Achraf Aroua</a>, 
<a href="/search/cs?searchtype=author&query=Leonhardt%2C+T">Tizian Leonhardt</a>, 
<a href="/search/cs?searchtype=author&query=Carle%2C+G">Georg Carle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Cryptography and Security (cs.CR); Performance (cs.PF)

</div>
<p class="mathjax">The recent developments and research in distributed ledger technologies and
blockchain have contributed to the increasing adoption of distributed systems.
To collect relevant insights into systems' behavior, we observe many evaluation
frameworks focusing mainly on the system under test throughput. However, these
frameworks often need more comprehensiveness and generality, particularly in
adopting a distributed applications' cross-layer approach. This work analyses
in detail the requirements for distributed systems assessment. We summarize
these findings into a structured methodology and experimentation framework
called TURBO. Our approach emphasizes setting up and assessing a broader
spectrum of distributed systems and addresses a notable research gap. We
showcase the effectiveness of the framework by evaluating four distinct systems
and their interaction, leveraging a diverse set of eight carefully selected
metrics and 12 essential parameters. Through experimentation and analysis we
demonstrate the framework's capabilities to provide valuable insights across
various use cases. For instance, we identify that a combination of Trusted
Execution Environments with threshold signature scheme FROST introduces minimal
overhead on the performance with average latency around \SI{40}{\ms}. We
showcase an emulation of realistic systems behavior, e.g., Maximal Extractable
Value is possible and could be used to further model such dynamics. The TURBO
framework enables a deeper understanding of distributed systems and is a
powerful tool for researchers and practitioners navigating the complex
landscape of modern computing infrastructures.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16191" title="Abstract">arXiv:2310.16191</a> [<a href="/pdf/2310.16191" title="Download PDF">pdf</a>, <a href="/format/2310.16191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Virtual Reality Protect Users from Keystroke Inference Attacks?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhuolin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sarwar%2C+Z">Zain Sarwar</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+I">Iris Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Bhaskar%2C+R">Ronik Bhaskar</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B+Y">Ben Y. Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Haitao Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by USENIX 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Virtual Reality (VR) has gained popularity by providing immersive and
interactive experiences without geographical limitations. It also provides a
sense of personal privacy through physical separation. In this paper, we show
that despite assumptions of enhanced privacy, VR is unable to shield its users
from side-channel attacks that steal private information. Ironically, this
vulnerability arises from VR's greatest strength, its immersive and interactive
nature. We demonstrate this by designing and implementing a new set of
keystroke inference attacks in shared virtual environments, where an attacker
(VR user) can recover the content typed by another VR user by observing their
avatar. While the avatar displays noisy telemetry of the user's hand motion, an
intelligent attacker can use that data to recognize typed keys and reconstruct
typed content, without knowing the keyboard layout or gathering labeled data.
We evaluate the proposed attacks using IRB-approved user studies across
multiple VR scenarios. For 13 out of 15 tested users, our attacks accurately
recognize 86%-98% of typed keys, and the recovered content retains up to 98% of
the meaning of the original typed content. We also discuss potential defenses.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16193" title="Abstract">arXiv:2310.16193</a> [<a href="/pdf/2310.16193" title="Download PDF">pdf</a>, <a href="/format/2310.16193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Length is a Curse and a Blessing for Document-level Semantics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chenghao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yizhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Hudson%2C+G+T">G Thomas Hudson</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Moubayed%2C+N+A">Noura Al Moubayed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023. Our code is publicly available at <a href="https://github.com/gowitheflow-1998/LA-SER-cubed">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent years, contrastive learning (CL) has been extensively utilized to
recover sentence and document-level encoding capability from pre-trained
language models. In this work, we question the length generalizability of
CL-based models, i.e., their vulnerability towards length-induced semantic
shift. We verify not only that length vulnerability is a significant yet
overlooked research gap, but we can devise unsupervised CL methods solely
depending on the semantic signal provided by document length. We first derive
the theoretical foundations underlying length attacks, showing that elongating
a document would intensify the high intra-document similarity that is already
brought by CL. Moreover, we found that isotropy promised by CL is highly
dependent on the length range of text exposed in training. Inspired by these
findings, we introduce a simple yet universal document representation learning
framework, LA(SER)$^{3}$: length-agnostic self-reference for semantically
robust sentence representation learning, achieving state-of-the-art
unsupervised performance on the standard information retrieval benchmark.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16194" title="Abstract">arXiv:2310.16194</a> [<a href="/pdf/2310.16194" title="Download PDF">pdf</a>, <a href="/format/2310.16194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Low-Rank Latent Spaces with Simple Deterministic Autoencoder:  Theoretical and Empirical Insights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mazumder%2C+A">Alokendu Mazumder</a>, 
<a href="/search/cs?searchtype=author&query=Baruah%2C+T">Tirthajit Baruah</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+B">Bhartendu Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Rishab Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Pattanaik%2C+V">Vishwajeet Pattanaik</a>, 
<a href="/search/cs?searchtype=author&query=Rathore%2C+P">Punit Rathore</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted @ IEEE/CVF WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The autoencoder is an unsupervised learning paradigm that aims to create a
compact latent representation of data by minimizing the reconstruction loss.
However, it tends to overlook the fact that most data (images) are embedded in
a lower-dimensional space, which is crucial for effective data representation.
To address this limitation, we propose a novel approach called Low-Rank
Autoencoder (LoRAE). In LoRAE, we incorporated a low-rank regularizer to
adaptively reconstruct a low-dimensional latent space while preserving the
basic objective of an autoencoder. This helps embed the data in a
lower-dimensional space while preserving important information. It is a simple
autoencoder extension that learns low-rank latent space. Theoretically, we
establish a tighter error bound for our model. Empirically, our model's
superiority shines through various tasks such as image generation and
downstream classification. Both theoretical and practical outcomes highlight
the importance of acquiring low-dimensional embeddings.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16197" title="Abstract">arXiv:2310.16197</a> [<a href="/pdf/2310.16197" title="Download PDF">pdf</a>, <a href="/format/2310.16197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Background Summarization of Event Timelines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pratapa%2C+A">Adithya Pratapa</a>, 
<a href="/search/cs?searchtype=author&query=Small%2C+K">Kevin Small</a>, 
<a href="/search/cs?searchtype=author&query=Dreyer%2C+M">Markus Dreyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Generating concise summaries of news events is a challenging natural language
processing task. While journalists often curate timelines to highlight key
sub-events, newcomers to a news event face challenges in catching up on its
historical context. In this paper, we address this need by introducing the task
of background news summarization, which complements each timeline update with a
background summary of relevant preceding events. We construct a dataset by
merging existing timeline datasets and asking human annotators to write a
background summary for each timestep of each news event. We establish strong
baseline performance using state-of-the-art summarization systems and propose a
query-focused variant to generate background summaries. To evaluate background
summary quality, we present a question-answering-based evaluation metric,
Background Utility Score (BUS), which measures the percentage of questions
about a current event timestep that a background summary answers. Our
experiments show the effectiveness of instruction fine-tuned systems such as
Flan-T5, in addition to strong zero-shot performance using GPT-3.5.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16201" title="Abstract">arXiv:2310.16201</a> [<a href="/pdf/2310.16201" title="Download PDF">pdf</a>, <a href="/format/2310.16201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Convex Parameterization of Controllers Constrained to use only  Relative Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Marshall%2C+W">Walden Marshall</a>, 
<a href="/search/eess?searchtype=author&query=Bamieh%2C+B">Bassam Bamieh</a>, 
<a href="/search/eess?searchtype=author&query=Jensen%2C+E">Emily Jensen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">We consider the optimal controller design problem for distributed systems in
which subsystems are equipped with sensors that measure only differences of
quantities such as relative (rather than absolute) positions and velocities.
While such problems can be set up as a standard problem of robust output
feedback control, we illustrate with a counterexample that this may be
undesirable and then propose an alternate equivalent formulation. In
particular, we provide an example of sparsity constraints that are not
quadratically-invariant with respect to a standard formulation of a given
plant, but that can be written as quadratically-invariant constraints with
respect to a transformed version of this problem. In effect, our transformation
provides a path to convert the controller design problem to an equivalent
convex program. This problem transformation relies on a novel parameterization
of controllers with general relative measurement structures that we derive
here. We further illustrate the usefulness of this novel parameterization
through an example of optimal consensus design with prescribed communication
delays within the controller.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16209" title="Abstract">arXiv:2310.16209</a> [<a href="/pdf/2310.16209" title="Download PDF">pdf</a>, <a href="/format/2310.16209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ELM Ridge Regression Boosting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andrecut%2C+M">M. Andrecut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We discuss a boosting approach for the Ridge Regression (RR) method, with
applications to the Extreme Learning Machine (ELM), and we show that the
proposed method significantly improves the classification performance and
robustness of ELMs.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16210" title="Abstract">arXiv:2310.16210</a> [<a href="/pdf/2310.16210" title="Download PDF">pdf</a>, <a href="/format/2310.16210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sea-Land-Cloud Segmentation in Satellite Hyperspectral Imagery by Deep  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Justo%2C+J+A">Jon Alvarez Justo</a>, 
<a href="/search/cs?searchtype=author&query=Garrett%2C+J+L">Joseph Landon Garrett</a>, 
<a href="/search/cs?searchtype=author&query=Georgescu%2C+M">Mariana-Iuliana Georgescu</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez-Llorente%2C+J">Jesus Gonzalez-Llorente</a>, 
<a href="/search/cs?searchtype=author&query=Ionescu%2C+R+T">Radu Tudor Ionescu</a>, 
<a href="/search/cs?searchtype=author&query=Johansen%2C+T+A">Tor Arne Johansen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Remote Sensing, Satellite Imagery, Hyperspectral Imaging, Deep Learning, Segmentation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Satellites are increasingly adopting on-board Artificial Intelligence (AI)
techniques to enhance platforms' autonomy through edge inference. In this
context, the utilization of deep learning (DL) techniques for segmentation in
HS satellite imagery offers advantages for remote sensing applications, and
therefore, we train 16 different models, whose codes are made available through
our study, which we consider to be relevant for on-board multi-class
segmentation of HS imagery, focusing on classifying oceanic (sea), terrestrial
(land), and cloud formations. We employ the HYPSO-1 mission as an illustrative
case for sea-land-cloud segmentation, and to demonstrate the utility of the
segments, we introduce a novel sea-land-cloud ranking application scenario. Our
system prioritizes HS image downlink based on sea, land, and cloud coverage
levels from the segmented images. We comparatively evaluate the models for
in-orbit deployment, considering performance, parameter count, and inference
time. The models include both shallow and deep models, and after we propose
four new DL models, we demonstrate that segmenting single spectral signatures
(1D) outperforms 3D data processing comprising both spectral (1D) and spatial
(2D) contexts. We conclude that our lightweight DL model, called
1D-Justo-LiuNet, consistently surpasses state-of-the-art models for
sea-land-cloud segmentation, such as U-Net and its variations, in terms of
performance (0.93 accuracy) and parameter count (4,563). However, the 1D models
present longer inference time (15s) in the tested processing architecture,
which is clearly suboptimal. Finally, after demonstrating that in-orbit image
segmentation should occur post L1b radiance calibration rather than on raw
data, we additionally show that reducing spectral channels down to 3 lowers
models' parameters and inference time, at the cost of weaker segmentation
performance.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16212" title="Abstract">arXiv:2310.16212</a> [<a href="/pdf/2310.16212" title="Download PDF">pdf</a>, <a href="/format/2310.16212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ShadowSense: Unsupervised Domain Adaptation and Feature Fusion for  Shadow-Agnostic Tree Crown Detection from RGB-Thermal Drone Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kapil%2C+R">Rudraksh Kapil</a>, 
<a href="/search/cs?searchtype=author&query=Marvasti-Zadeh%2C+S+M">Seyed Mojtaba Marvasti-Zadeh</a>, 
<a href="/search/cs?searchtype=author&query=Erbilgin%2C+N">Nadir Erbilgin</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+N">Nilanjan Ray</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE/CVF Winter Applications of Computer Vision (WACV) 2024 main conference! 8 pages (11 with bibliography), 5 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Accurate detection of individual tree crowns from remote sensing data poses a
significant challenge due to the dense nature of forest canopy and the presence
of diverse environmental variations, e.g., overlapping canopies, occlusions,
and varying lighting conditions. Additionally, the lack of data for training
robust models adds another limitation in effectively studying complex forest
conditions. This paper presents a novel method for detecting shadowed tree
crowns and provides a challenging dataset comprising roughly 50k paired
RGB-thermal images to facilitate future research for illumination-invariant
detection. The proposed method (ShadowSense) is entirely self-supervised,
leveraging domain adversarial training without source domain annotations for
feature extraction and foreground feature alignment for feature pyramid
networks to adapt domain-invariant representations by focusing on visible
foreground regions, respectively. It then fuses complementary information of
both modalities to effectively improve upon the predictions of an RGB-trained
detector and boost the overall accuracy. Extensive experiments demonstrate the
superiority of the proposed method over both the baseline RGB-trained detector
and state-of-the-art techniques that rely on unsupervised domain adaptation or
early image fusion. Our code and data are available:
https://github.com/rudrakshkapil/ShadowSense
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16214" title="Abstract">arXiv:2310.16214</a> [<a href="/pdf/2310.16214" title="Download PDF">pdf</a>, <a href="/format/2310.16214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Tuning for GPU-Embedded Systems: Machine-Learning-based and  Analytical Model-driven Tuning Methodologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dieguez%2C+A+P">Adrian Perez Dieguez</a>, 
<a href="/search/cs?searchtype=author&query=Lopez%2C+M+A">Margarita Amor Lopez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG); Performance (cs.PF)

</div>
<p class="mathjax">GPU-embedded systems have gained popularity across various domains due to
their efficient power consumption. However, in order to meet the demands of
real-time or time-consuming applications running on these systems, it is
crucial for them to be tuned to exhibit high performance. This paper addresses
the issue by developing and comparing two tuning methodologies on GPU-embedded
systems, and also provides performance insights for developers and researchers
seeking to optimize applications running on these architectures. We focus on
parallel prefix operations, such as FFT, scan primitives, and tridiagonal
system solvers, which are performance-critical components in many applications.
The study introduces an analytical model-driven tuning methodology and a
Machine Learning (ML)-based tuning methodology. We evaluate the performance of
the two tuning methodologies for different parallel prefix implementations of
the BPLG library in an NVIDIA Jetson system, and compare their performance to
the ones achieved through an exhaustive search. The findings shed light on the
best strategies for handling the open challenge of performance portability for
major computational patterns among server and embedded devices, providing
practical guidance for offline and online tuning. We also address the existing
gap in performance studies for parallel computational patterns in GPU-embedded
systems by comparing the BPLG performance against other state-of-the-art
libraries, including CUSPARSE, CUB, and CUFFT.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16216" title="Abstract">arXiv:2310.16216</a> [<a href="/pdf/2310.16216" title="Download PDF">pdf</a>, <a href="/ps/2310.16216" title="Download PostScript">ps</a>, <a href="/format/2310.16216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An augmented Lagrangian-based preconditioning technique for a class of  block three-by-three linear systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Beik%2C+F+P+A">Fatemeh P. A. Beik</a>, 
<a href="/search/math?searchtype=author&query=Benzi%2C+M">Michele Benzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose an augmented Lagrangian-based preconditioner to accelerate the
convergence of Krylov subspace methods applied to linear systems of equations
with a block three-by-three structure such as those arising from mixed finite
element discretizations of the coupled Stokes-Darcy flow problem. We analyze
the spectrum of the preconditioned matrix and we show how the new
preconditioner can be efficiently applied. Numerical experiments are reported
to illustrate the effectiveness of the preconditioner in conjunction with
flexible GMRES for solving linear systems of equations arising from a 3D test
problem.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16217" title="Abstract">arXiv:2310.16217</a> [<a href="/pdf/2310.16217" title="Download PDF">pdf</a>, <a href="/format/2310.16217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information-Theoretically Secret Reed-Muller Identification with Affine  Designs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Spandri%2C+M">Mattia Spandri</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+R">Roberto Ferrara</a>, 
<a href="/search/cs?searchtype=author&query=Deppe%2C+C">Christian Deppe</a>, 
<a href="/search/cs?searchtype=author&query=Wiese%2C+M">Moritz Wiese</a>, 
<a href="/search/cs?searchtype=author&query=Boche%2C+H">Holger Boche</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, accepted at European Wireless 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We consider the problem of information-theoretic secrecy in identification
schemes rather than transmission schemes. In identification, large identities
are encoded into small challenges sent with the sole goal of allowing at the
receiver reliable verification of whether the challenge could have been
generated by a (possibly different) identity of his choice. One of the reasons
to consider identification is that it trades decoding for an exponentially
larger rate, however this may come with such encoding complexity and latency
that it can render this advantage unusable. Identification still bears one
unique advantage over transmission in that practical implementation of
information-theoretic secrecy becomes possible, even considering that the
information-theoretic secrecy definition needed in identification is that of
semantic secrecy. Here, we implement a family of encryption schemes, recently
shown to achieve semantic-secrecy capacity, and apply it to a recently-studied
family of identification codes, confirming that, indeed, adding secrecy to
identification comes at essentially no cost. While this is still within the
one-way communication scenario, it is a necessary step into implementing
semantic secrecy with two-way communication, where the information-theoretic
assumptions are more realistic.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16218" title="Abstract">arXiv:2310.16218</a> [<a href="/pdf/2310.16218" title="Download PDF">pdf</a>, <a href="/format/2310.16218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Editing for Large Language Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Song Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yaochen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haochen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zaiyi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=L%2C+J">Jundong L</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have recently transformed both the academic and
industrial landscapes due to their remarkable capacity to understand, analyze,
and generate texts based on their vast knowledge and reasoning ability.
Nevertheless, one major drawback of LLMs is their substantial computational
cost for pre-training due to their unprecedented amounts of parameters. The
disadvantage is exacerbated when new knowledge frequently needs to be
introduced into the pre-trained model. Therefore, it is imperative to develop
effective and efficient techniques to update pre-trained LLMs. Traditional
methods encode new knowledge in pre-trained LLMs through direct fine-tuning.
However, naively re-training LLMs can be computationally intensive and risks
degenerating valuable pre-trained knowledge irrelevant to the update in the
model. Recently, Knowledge-based Model Editing (KME) has attracted increasing
attention, which aims to precisely modify the LLMs to incorporate specific
knowledge, without negatively influencing other irrelevant knowledge. In this
survey, we aim to provide a comprehensive and in-depth overview of recent
advances in the field of KME. We first introduce a general formulation of KME
to encompass different KME strategies. Afterward, we provide an innovative
taxonomy of KME techniques based on how the new knowledge is introduced into
pre-trained LLMs, and investigate existing KME strategies while analyzing key
insights, advantages, and limitations of methods from each category. Moreover,
representative metrics, datasets, and applications of KME are introduced
accordingly. Finally, we provide an in-depth analysis regarding the
practicality and remaining challenges of KME and suggest promising research
directions for further advancement in this field.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16221" title="Abstract">arXiv:2310.16221</a> [<a href="/pdf/2310.16221" title="Download PDF">pdf</a>, <a href="/format/2310.16221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Randomized Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scholten%2C+Y">Yan Scholten</a>, 
<a href="/search/cs?searchtype=author&query=Schuchardt%2C+J">Jan Schuchardt</a>, 
<a href="/search/cs?searchtype=author&query=Bojchevski%2C+A">Aleksandar Bojchevski</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnnemann%2C+S">Stephan G&#xfc;nnemann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">Real-world data is complex and often consists of objects that can be
decomposed into multiple entities (e.g. images into pixels, graphs into
interconnected nodes). Randomized smoothing is a powerful framework for making
models provably robust against small changes to their inputs - by guaranteeing
robustness of the majority vote when randomly adding noise before
classification. Yet, certifying robustness on such complex data via randomized
smoothing is challenging when adversaries do not arbitrarily perturb entire
objects (e.g. images) but only a subset of their entities (e.g. pixels). As a
solution, we introduce hierarchical randomized smoothing: We partially smooth
objects by adding random noise only on a randomly selected subset of their
entities. By adding noise in a more targeted manner than existing methods we
obtain stronger robustness guarantees while maintaining high accuracy. We
initialize hierarchical smoothing using different noising distributions,
yielding novel robustness certificates for discrete and continuous domains. We
experimentally demonstrate the importance of hierarchical smoothing in image
and node classification, where it yields superior robustness-accuracy
trade-offs. Overall, hierarchical smoothing is an important contribution
towards models that are both - certifiably robust to perturbations and
accurate.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16224" title="Abstract">arXiv:2310.16224</a> [<a href="/pdf/2310.16224" title="Download PDF">pdf</a>, <a href="/format/2310.16224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Poison is Not Traceless: Fully-Agnostic Detection of Poisoning Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xinglong Chang</a>, 
<a href="/search/cs?searchtype=author&query=Dost%2C+K">Katharina Dost</a>, 
<a href="/search/cs?searchtype=author&query=Dobbie%2C+G">Gillian Dobbie</a>, 
<a href="/search/cs?searchtype=author&query=Wicker%2C+J">J&#xf6;rg Wicker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The performance of machine learning models depends on the quality of the
underlying data. Malicious actors can attack the model by poisoning the
training data. Current detectors are tied to either specific data types,
models, or attacks, and therefore have limited applicability in real-world
scenarios. This paper presents a novel fully-agnostic framework, DIVA
(Detecting InVisible Attacks), that detects attacks solely relying on analyzing
the potentially poisoned data set. DIVA is based on the idea that poisoning
attacks can be detected by comparing the classifier's accuracy on poisoned and
clean data and pre-trains a meta-learner using Complexity Measures to estimate
the otherwise unknown accuracy on a hypothetical clean dataset. The framework
applies to generic poisoning attacks. For evaluation purposes, in this paper,
we test DIVA on label-flipping attacks.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16225" title="Abstract">arXiv:2310.16225</a> [<a href="/pdf/2310.16225" title="Download PDF">pdf</a>, <a href="/format/2310.16225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=R%C3%BCcker%2C+S">Susanna R&#xfc;cker</a>, 
<a href="/search/cs?searchtype=author&query=Akbik%2C+A">Alan Akbik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The CoNLL-03 corpus is arguably the most well-known and utilized benchmark
dataset for named entity recognition (NER). However, prior works found
significant numbers of annotation errors, incompleteness, and inconsistencies
in the data. This poses challenges to objectively comparing NER approaches and
analyzing their errors, as current state-of-the-art models achieve F1-scores
that are comparable to or even exceed the estimated noise level in CoNLL-03. To
address this issue, we present a comprehensive relabeling effort assisted by
automatic consistency checking that corrects 7.0% of all labels in the English
CoNLL-03. Our effort adds a layer of entity linking annotation both for better
explainability of NER labels and as additional safeguard of annotation quality.
Our experimental evaluation finds not only that state-of-the-art approaches
reach significantly higher F1-scores (97.1%) on our data, but crucially that
the share of correct predictions falsely counted as errors due to annotation
noise drops from 47% to 6%. This indicates that our resource is well suited to
analyze the remaining errors made by state-of-the-art models, and that the
theoretical upper bound even on high resource, coarse-grained NER is not yet
reached. To facilitate such analysis, we make CleanCoNLL publicly available to
the research community.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16226" title="Abstract">arXiv:2310.16226</a> [<a href="/pdf/2310.16226" title="Download PDF">pdf</a>, <a href="/format/2310.16226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TiC-CLIP: Continual Training of CLIP Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Saurabh Garg</a>, 
<a href="/search/cs?searchtype=author&query=Farajtabar%2C+M">Mehrdad Farajtabar</a>, 
<a href="/search/cs?searchtype=author&query=Pouransari%2C+H">Hadi Pouransari</a>, 
<a href="/search/cs?searchtype=author&query=Vemulapalli%2C+R">Raviteja Vemulapalli</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+S">Sachin Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Tuzel%2C+O">Oncel Tuzel</a>, 
<a href="/search/cs?searchtype=author&query=Shankar%2C+V">Vaishaal Shankar</a>, 
<a href="/search/cs?searchtype=author&query=Faghri%2C+F">Fartash Faghri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Keeping large foundation models up to date on latest data is inherently
expensive. To avoid the prohibitive costs of constantly retraining, it is
imperative to continually train these models. This problem is exacerbated by
the lack of any large scale continual learning benchmarks or baselines. We
introduce the first set of web-scale Time-Continual (TiC) benchmarks for
training vision-language models: TiC-DataCompt, TiC-YFCC, and TiC-RedCaps with
over 12.7B timestamped image-text pairs spanning 9 years (2014--2022). We first
use our benchmarks to curate various dynamic evaluations to measure temporal
robustness of existing models. We show OpenAI's CLIP (trained on data up to
2020) loses $\approx 8\%$ zero-shot accuracy on our curated retrieval task from
2021--2022 compared with more recently trained models in OpenCLIP repository.
We then study how to efficiently train models on time-continuous data. We
demonstrate that a simple rehearsal-based approach that continues training from
the last checkpoint and replays old data reduces compute by $2.5\times$ when
compared to the standard practice of retraining from scratch.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16228" title="Abstract">arXiv:2310.16228</a> [<a href="/pdf/2310.16228" title="Download PDF">pdf</a>, <a href="/format/2310.16228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Foundations of Shortcut Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hermann%2C+K+L">Katherine L. Hermann</a>, 
<a href="/search/cs?searchtype=author&query=Mobahi%2C+H">Hossein Mobahi</a>, 
<a href="/search/cs?searchtype=author&query=Fel%2C+T">Thomas Fel</a>, 
<a href="/search/cs?searchtype=author&query=Mozer%2C+M+C">Michael C. Mozer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Deep-learning models can extract a rich assortment of features from data.
Which features a model uses depends not only on predictivity-how reliably a
feature indicates train-set labels-but also on availability-how easily the
feature can be extracted, or leveraged, from inputs. The literature on shortcut
learning has noted examples in which models privilege one feature over another,
for example texture over shape and image backgrounds over foreground objects.
Here, we test hypotheses about which input properties are more available to a
model, and systematically study how predictivity and availability interact to
shape models' feature use. We construct a minimal, explicit generative
framework for synthesizing classification datasets with two latent features
that vary in predictivity and in factors we hypothesize to relate to
availability, and quantify a model's shortcut bias-its over-reliance on the
shortcut (more available, less predictive) feature at the expense of the core
(less available, more predictive) feature. We find that linear models are
relatively unbiased, but introducing a single hidden layer with ReLU or Tanh
units yields a bias. Our empirical findings are consistent with a theoretical
account based on Neural Tangent Kernels. Finally, we study how models used in
practice trade off predictivity and availability in naturalistic datasets,
discovering availability manipulations which increase models' degree of
shortcut bias. Taken together, these findings suggest that the propensity to
learn shortcut features is a fundamental characteristic of deep nonlinear
architectures warranting systematic study given its role in shaping how models
solve tasks.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16231" title="Abstract">arXiv:2310.16231</a> [<a href="/pdf/2310.16231" title="Download PDF">pdf</a>, <a href="/format/2310.16231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention-Based Ensemble Pooling for Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patel%2C+D">Dhruvit Patel</a>, 
<a href="/search/cs?searchtype=author&query=Wikner%2C+A">Alexander Wikner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Chaotic Dynamics (nlin.CD)

</div>
<p class="mathjax">A common technique to reduce model bias in time-series forecasting is to use
an ensemble of predictive models and pool their output into an ensemble
forecast. In cases where each predictive model has different biases, however,
it is not always clear exactly how each model forecast should be weighed during
this pooling. We propose a method for pooling that performs a weighted average
over candidate model forecasts, where the weights are learned by an
attention-based ensemble pooling model. We test this method on two time-series
forecasting problems: multi-step forecasting of the dynamics of the
non-stationary Lorenz `63 equation, and one-step forecasting of the weekly
incident deaths due to COVID-19. We find that while our model achieves
excellent valid times when forecasting the non-stationary Lorenz `63 equation,
it does not consistently perform better than the existing ensemble pooling when
forecasting COVID-19 weekly incident deaths.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16234" title="Abstract">arXiv:2310.16234</a> [<a href="/pdf/2310.16234" title="Download PDF">pdf</a>, <a href="/format/2310.16234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pixel-Level Clustering Network for Unsupervised Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoang%2C+C+M">Cuong Manh Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+B">Byeongkeun Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Engineering Applications of Artificial Intelligence, Volume 127,
  Part B, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While image segmentation is crucial in various computer vision applications,
such as autonomous driving, grasping, and robot navigation, annotating all
objects at the pixel-level for training is nearly impossible. Therefore, the
study of unsupervised image segmentation methods is essential. In this paper,
we present a pixel-level clustering framework for segmenting images into
regions without using ground truth annotations. The proposed framework includes
feature embedding modules with an attention mechanism, a feature statistics
computing module, image reconstruction, and superpixel segmentation to achieve
accurate unsupervised segmentation. Additionally, we propose a training
strategy that utilizes intra-consistency within each superpixel,
inter-similarity/dissimilarity between neighboring superpixels, and structural
similarity between images. To avoid potential over-segmentation caused by
superpixel-based losses, we also propose a post-processing method. Furthermore,
we present an extension of the proposed method for unsupervised semantic
segmentation. We conducted experiments on three publicly available datasets
(Berkeley segmentation dataset, PASCAL VOC 2012 dataset, and COCO-Stuff
dataset) to demonstrate the effectiveness of the proposed framework. The
experimental results show that the proposed framework outperforms previous
state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16236" title="Abstract">arXiv:2310.16236</a> [<a href="/pdf/2310.16236" title="Download PDF">pdf</a>, <a href="/format/2310.16236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query-Efficient Algorithms to Find the Unique Nash Equilibrium in a  Two-Player Zero-Sum Matrix Game
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maiti%2C+A">Arnab Maiti</a>, 
<a href="/search/cs?searchtype=author&query=Boczar%2C+R">Ross Boczar</a>, 
<a href="/search/cs?searchtype=author&query=Jamieson%2C+K">Kevin Jamieson</a>, 
<a href="/search/cs?searchtype=author&query=Ratliff%2C+L+J">Lillian J. Ratliff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study the query complexity of identifying Nash equilibria in two-player
zero-sum matrix games. Grigoriadis and Khachiyan (1995) showed that any
deterministic algorithm needs to query $\Omega(n^2)$ entries in worst case from
an $n\times n$ input matrix in order to compute an $\varepsilon$-approximate
Nash equilibrium, where $\varepsilon&lt;\frac{1}{2}$. Moreover, they designed a
randomized algorithm that queries $\mathcal O(\frac{n\log n}{\varepsilon^2})$
entries from the input matrix in expectation and returns an
$\varepsilon$-approximate Nash equilibrium when the entries of the matrix are
bounded between $-1$ and $1$. However, these two results do not completely
characterize the query complexity of finding an exact Nash equilibrium in
two-player zero-sum matrix games. In this work, we characterize the query
complexity of finding an exact Nash equilibrium for two-player zero-sum matrix
games that have a unique Nash equilibrium $(x_\star,y_\star)$. We first show
that any randomized algorithm needs to query $\Omega(nk)$ entries of the input
matrix $A\in\mathbb{R}^{n\times n}$ in expectation in order to find the unique
Nash equilibrium where $k=|\text{supp}(x_\star)|$. We complement this lower
bound by presenting a simple randomized algorithm that, with probability
$1-\delta$, returns the unique Nash equilibrium by querying at most $\mathcal
O(nk^4\cdot \text{polylog}(\frac{n}{\delta}))$ entries of the input matrix
$A\in\mathbb{R}^{n\times n}$. In the special case when the unique Nash
Equilibrium is a pure-strategy Nash equilibrium (PSNE), we design a simple
deterministic algorithm that finds the PSNE by querying at most $\mathcal O(n)$
entries of the input matrix.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16237" title="Abstract">arXiv:2310.16237</a> [<a href="/pdf/2310.16237" title="Download PDF">pdf</a>, <a href="/format/2310.16237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An entropy stable discontinuous Galerkin method for the spherical  thermal shallow water equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ricardo%2C+K">Kieran Ricardo</a>, 
<a href="/search/math?searchtype=author&query=Duru%2C+K">Kenneth Duru</a>, 
<a href="/search/math?searchtype=author&query=Lee%2C+D">David Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">We develop a novel discontinuous Galerkin method for solving the rotating
thermal shallow water equations (TRSW) on a curvilinear mesh. Our method is
provably entropy stable, conserves mass, buoyancy and vorticity, while also
semi-discretely conserving energy. This is achieved by using novel numerical
fluxes and splitting the pressure and convection operators. We implement our
method on a cubed sphere mesh and numerically verify our theoretical results.
Our experiments demonstrate the robustness of the method for a regime of well
developed turbulence, where it can be run stably without any dissipation. The
entropy stable fluxes are sufficient to control the grid scale noise generated
by geostrophic turbulence, eliminating the need for artificial stabilization.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16240" title="Abstract">arXiv:2310.16240</a> [<a href="/pdf/2310.16240" title="Download PDF">pdf</a>, <a href="/format/2310.16240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting  Pre-trained Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Raymond Li</a>, 
<a href="/search/cs?searchtype=author&query=Murray%2C+G">Gabriel Murray</a>, 
<a href="/search/cs?searchtype=author&query=Carenini%2C+G">Giuseppe Carenini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures, Camera-Ready for EMNLP 2023 Findings (Long Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this work, we propose a method that combines two popular research areas by
injecting linguistic structures into pre-trained language models in the
parameter-efficient fine-tuning (PEFT) setting. In our approach, parallel
adapter modules encoding different linguistic structures are combined using a
novel Mixture-of-Linguistic-Experts architecture, where Gumbel-Softmax gates
are used to determine the importance of these modules at each layer of the
model. To reduce the number of parameters, we first train the model for a fixed
small number of steps before pruning the experts based on their importance
scores. Our experiment results with three different pre-trained models show
that our approach can outperform state-of-the-art PEFT methods with a
comparable number of parameters. In addition, we provide additional analysis to
examine the experts selected by each model at each layer to provide insights
for future studies.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16241" title="Abstract">arXiv:2310.16241</a> [<a href="/pdf/2310.16241" title="Download PDF">pdf</a>, <a href="/format/2310.16241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task Grouping for Automated Multi-Task Machine Learning via Task  Affinity Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ayman%2C+A">Afiya Ayman</a>, 
<a href="/search/cs?searchtype=author&query=Mukhopadhyay%2C+A">Ayan Mukhopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Laszka%2C+A">Aron Laszka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">When a number of similar tasks have to be learned simultaneously, multi-task
learning (MTL) models can attain significantly higher accuracy than single-task
learning (STL) models. However, the advantage of MTL depends on various
factors, such as the similarity of the tasks, the sizes of the datasets, and so
on; in fact, some tasks might not benefit from MTL and may even incur a loss of
accuracy compared to STL. Hence, the question arises: which tasks should be
learned together? Domain experts can attempt to group tasks together following
intuition, experience, and best practices, but manual grouping can be
labor-intensive and far from optimal. In this paper, we propose a novel
automated approach for task grouping. First, we study the affinity of tasks for
MTL using four benchmark datasets that have been used extensively in the MTL
literature, focusing on neural network-based MTL models. We identify inherent
task features and STL characteristics that can help us to predict whether a
group of tasks should be learned together using MTL or if they should be
learned independently using STL. Building on this predictor, we introduce a
randomized search algorithm, which employs the predictor to minimize the number
of MTL trainings performed during the search for task groups. We demonstrate on
the four benchmark datasets that our predictor-driven search approach can find
better task groupings than existing baseline approaches.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16242" title="Abstract">arXiv:2310.16242</a> [<a href="/pdf/2310.16242" title="Download PDF">pdf</a>, <a href="/format/2310.16242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ZzzGPT: An Interactive GPT Approach to Enhance Sleep Quality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khaokaew%2C+Y">Yonchanok Khaokaew</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+H">Thuc Hanh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+K">Kaixin Ji</a>, 
<a href="/search/cs?searchtype=author&query=Kegalle%2C+H">Hiruni Kegalle</a>, 
<a href="/search/cs?searchtype=author&query=Alaofi%2C+M">Marwah Alaofi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In today's world, sleep quality is pivotal for overall well-being. While
wearable sensors offer real-time monitoring, they often lack actionable
insights, leading to user abandonment. This paper delves into the role of
technology in understanding sleep patterns. We introduce a two-stage framework,
utilizing Large Language Models (LLMs), aiming to provide accurate sleep
predictions with actionable feedback. Leveraging the GLOBEM dataset and
synthetic data from LLMs, we highlight enhanced results with models like
XGBoost. Our approach merges advanced machine learning with user-centric
design, blending scientific accuracy with practicality.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16248" title="Abstract">arXiv:2310.16248</a> [<a href="/pdf/2310.16248" title="Download PDF">pdf</a>, <a href="/format/2310.16248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GlotLID: Language Identification for Low-Resource Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kargaran%2C+A+H">Amir Hossein Kargaran</a>, 
<a href="/search/cs?searchtype=author&query=Imani%2C+A">Ayyoob Imani</a>, 
<a href="/search/cs?searchtype=author&query=Yvon%2C+F">Fran&#xe7;ois Yvon</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+H">Hinrich Sch&#xfc;tze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Several recent papers have published good solutions for language
identification (LID) for about 300 high-resource and medium-resource languages.
However, there is no LID available that (i) covers a wide range of low-resource
languages, (ii) is rigorously evaluated and reliable and (iii) efficient and
easy to use. Here, we publish GlotLID-M, an LID model that satisfies the
desiderata of wide coverage, reliability and efficiency. It identifies 1665
languages, a large increase in coverage compared to prior work. In our
experiments, GlotLID-M outperforms four baselines (CLD3, FT176, OpenLID and
NLLB) when balancing F1 and false positive rate (FPR). We analyze the unique
challenges that low-resource LID poses: incorrect corpus metadata, leakage from
high-resource languages, difficulty separating closely related languages,
handling of macrolanguage vs varieties and in general noisy data. We hope that
integrating GlotLID-M into dataset creation pipelines will improve quality and
enhance accessibility of NLP technology for low-resource languages and
cultures. GlotLID-M model, code, and list of data sources are available:
https://github.com/cisnlp/GlotLID.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16249" title="Abstract">arXiv:2310.16249</a> [<a href="/pdf/2310.16249" title="Download PDF">pdf</a>, <a href="/format/2310.16249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A clustering tool for interrogating finite element models based on  eigenvectors of graph adjacency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kannan%2C+R">Ramaseshan Kannan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Artificial Intelligence (cs.AI); Numerical Analysis (math.NA)

</div>
<p class="mathjax">This note introduces an unsupervised learning algorithm to debug errors in
finite element (FE) simulation models and details how it was productionised.
The algorithm clusters degrees of freedom in the FE model using numerical
properties of the adjacency of its stiffness matrix. The algorithm has been
deployed as a tool called `Model Stability Analysis' tool within the commercial
structural FE suite Oasys GSA (www.oasys-software.com/gsa). It has been used
successfully by end-users for debugging real world FE models and we present
examples of the tool in action.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16251" title="Abstract">arXiv:2310.16251</a> [<a href="/pdf/2310.16251" title="Download PDF">pdf</a>, <a href="/format/2310.16251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speakerly: A Voice-based Writing Assistant for Text Composition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+D">Dhruv Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Raheja%2C+V">Vipul Raheja</a>, 
<a href="/search/cs?searchtype=author&query=Kaiser-Schatzlein%2C+A">Alice Kaiser-Schatzlein</a>, 
<a href="/search/cs?searchtype=author&query=Perry%2C+R">Robyn Perry</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+A">Apurva Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Hugues-Nuger%2C+J">Justin Hugues-Nuger</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+S">Samuel Lou</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+N">Navid Chowdhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 Industry Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present Speakerly, a new real-time voice-based writing assistance system
that helps users with text composition across various use cases such as emails,
instant messages, and notes. The user can interact with the system through
instructions or dictation, and the system generates a well-formatted and
coherent document. We describe the system architecture and detail how we
address the various challenges while building and deploying such a system at
scale. More specifically, our system uses a combination of small, task-specific
models as well as pre-trained language models for fast and effective text
composition while supporting a variety of input modes for better usability.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16252" title="Abstract">arXiv:2310.16252</a> [<a href="/pdf/2310.16252" title="Download PDF">pdf</a>, <a href="/format/2310.16252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Optimal Pure Exploration in Matrix Games: A Generalization of  Stochastic Bandits &amp; Dueling Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maiti%2C+A">Arnab Maiti</a>, 
<a href="/search/cs?searchtype=author&query=Boczar%2C+R">Ross Boczar</a>, 
<a href="/search/cs?searchtype=author&query=Jamieson%2C+K">Kevin Jamieson</a>, 
<a href="/search/cs?searchtype=author&query=Ratliff%2C+L+J">Lillian J. Ratliff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We study the sample complexity of identifying the pure strategy Nash
equilibrium (PSNE) in a two-player zero-sum matrix game with noise. Formally,
we are given a stochastic model where any learner can sample an entry $(i,j)$
of the input matrix $A\in[-1,1]^{n\times m}$ and observe $A_{i,j}+\eta$ where
$\eta$ is a zero-mean 1-sub-Gaussian noise. The aim of the learner is to
identify the PSNE of $A$, whenever it exists, with high probability while
taking as few samples as possible. Zhou et al. (2017) presents an
instance-dependent sample complexity lower bound that depends only on the
entries in the row and column in which the PSNE lies. We design a near-optimal
algorithm whose sample complexity matches the lower bound, up to log factors.
The problem of identifying the PSNE also generalizes the problem of pure
exploration in stochastic multi-armed bandits and dueling bandits, and our
result matches the optimal bounds, up to log factors, in both the settings.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16253" title="Abstract">arXiv:2310.16253</a> [<a href="/pdf/2310.16253" title="Download PDF">pdf</a>, <a href="/format/2310.16253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConDefects: A New Dataset to Address the Data Leakage Concern for  LLM-based Fault Localization and Program Repair
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yonghao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+M">Jie M. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the growing interest on Large Language Models (LLMs) for fault
localization and program repair, ensuring the integrity and generalizability of
the LLM-based methods becomes paramount. The code in existing widely-adopted
benchmarks for these tasks was written before the the bloom of LLMs and may be
included in the training data of existing popular LLMs, thereby suffering from
the threat of data leakage, leading to misleadingly optimistic performance
metrics. To address this issue, we introduce "ConDefects", a novel dataset of
real faults meticulously curated to eliminate such overlap. ConDefects contains
1,254 Java faulty programs and 1,625 Python faulty programs. All these programs
are sourced from the online competition platform AtCoder and were produced
between October 2021 and September 2023. We pair each fault with fault
locations and the corresponding repaired code versions, making it tailored for
in fault localization and program repair related research. We also provide
interfaces for selecting subsets based on different time windows and coding
task difficulties. While inspired by LLM-based tasks, ConDefects can be adopted
for benchmarking ALL types of fault localization and program repair methods.
The dataset is publicly available, and a demo video can be found at
https://www.youtube.com/watch?v=22j15Hj5ONk.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16255" title="Abstract">arXiv:2310.16255</a> [<a href="/pdf/2310.16255" title="Download PDF">pdf</a>, <a href="/format/2310.16255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maxey%2C+C">Christopher Maxey</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaehoon Choi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyungtae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+H">Heesung Kwon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Video Link: <a href="https://www.youtube.com/watch?v=ucPzbPLqqpI">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Tremendous variations coupled with large degrees of freedom in UAV-based
imaging conditions lead to a significant lack of data in adequately learning
UAV-based perception models. Using various synthetic renderers in conjunction
with perception models is prevalent to create synthetic data to augment the
learning in the ground-based imaging domain. However, severe challenges in the
austere UAV-based domain require distinctive solutions to image synthesis for
data augmentation. In this work, we leverage recent advancements in neural
rendering to improve static and dynamic novelview UAV-based image synthesis,
especially from high altitudes, capturing salient scene attributes. Finally, we
demonstrate a considerable performance boost is achieved when a state-ofthe-art
detection model is optimized primarily on hybrid sets of real and synthetic
data instead of the real or synthetic data separately.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16256" title="Abstract">arXiv:2310.16256</a> [<a href="/pdf/2310.16256" title="Download PDF">pdf</a>, <a href="/format/2310.16256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Causal Disentangled Multi-Granularity Graph Classification Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Li Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Penggang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Youmin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoyin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME)

</div>
<p class="mathjax">Graph data widely exists in real life, with large amounts of data and complex
structures. It is necessary to map graph data to low-dimensional embedding.
Graph classification, a critical graph task, mainly relies on identifying the
important substructures within the graph. At present, some graph classification
methods do not combine the multi-granularity characteristics of graph data.
This lack of granularity distinction in modeling leads to a conflation of key
information and false correlations within the model. So, achieving the desired
goal of a credible and interpretable model becomes challenging. This paper
proposes a causal disentangled multi-granularity graph representation learning
method (CDM-GNN) to solve this challenge. The CDM-GNN model disentangles the
important substructures and bias parts within the graph from a
multi-granularity perspective. The disentanglement of the CDM-GNN model reveals
important and bias parts, forming the foundation for its classification task,
specifically, model interpretations. The CDM-GNN model exhibits strong
classification performance and generates explanatory outcomes aligning with
human cognitive patterns. In order to verify the effectiveness of the model,
this paper compares the three real-world datasets MUTAG, PTC, and IMDM-M. Six
state-of-the-art models, namely GCN, GAT, Top-k, ASAPool, SUGAR, and SAT are
employed for comparison purposes. Additionally, a qualitative analysis of the
interpretation results is conducted.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16261" title="Abstract">arXiv:2310.16261</a> [<a href="/pdf/2310.16261" title="Download PDF">pdf</a>, <a href="/format/2310.16261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Distributional Hypothesis Does Not Fully Explain the Benefits of  Masked Language Model Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiang%2C+T">Ting-Rui Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Yogatama%2C+D">Dani Yogatama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We analyze the masked language modeling pretraining objective function from
the perspective of the distributional hypothesis. We investigate whether better
sample efficiency and the better generalization capability of models pretrained
with masked language modeling can be attributed to the semantic similarity
encoded in the pretraining data's distributional property. Via a synthetic
dataset, our analysis suggests that distributional property indeed leads to the
better sample efficiency of pretrained masked language models, but does not
fully explain the generalization capability. We also conduct analyses over two
real-world datasets and demonstrate that the distributional property does not
explain the generalization ability of pretrained natural language models
either. Our results illustrate our limited understanding of model pretraining
and provide future research directions.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16262" title="Abstract">arXiv:2310.16262</a> [<a href="/pdf/2310.16262" title="Download PDF">pdf</a>, <a href="/format/2310.16262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> rTisane: Externalizing conceptual models for data analysis increases  engagement with domain knowledge and improves statistical model quality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jun%2C+E">Eunice Jun</a>, 
<a href="/search/cs?searchtype=author&query=Misback%2C+E">Edward Misback</a>, 
<a href="/search/cs?searchtype=author&query=Heer%2C+J">Jeffrey Heer</a>, 
<a href="/search/cs?searchtype=author&query=Just%2C+R">Ren&#xe9; Just</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL); Computation (stat.CO)

</div>
<p class="mathjax">Statistical models should accurately reflect analysts' domain knowledge about
variables and their relationships. While recent tools let analysts express
these assumptions and use them to produce a resulting statistical model, it
remains unclear what analysts want to express and how externalization impacts
statistical model quality. This paper addresses these gaps. We first conduct an
exploratory study of analysts using a domain-specific language (DSL) to express
conceptual models. We observe a preference for detailing how variables relate
and a desire to allow, and then later resolve, ambiguity in their conceptual
models. We leverage these findings to develop rTisane, a DSL for expressing
conceptual models augmented with an interactive disambiguation process. In a
controlled evaluation, we find that rTisane's DSL helps analysts engage more
deeply with and accurately externalize their assumptions. rTisane also leads to
statistical models that match analysts' assumptions, maintain analysis intent,
and better fit the data.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16263" title="Abstract">arXiv:2310.16263</a> [<a href="/pdf/2310.16263" title="Download PDF">pdf</a>, <a href="/format/2310.16263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Large Language Models for Secure Code Generation: A  Dataset-driven Study on Vulnerability Mitigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiexin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Liuwen Cao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xitong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhiping Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jiayuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Jatowt%2C+A">Adam Jatowt</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yi Cai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Large language models (LLMs) have brought significant advancements to code
generation, benefiting both novice and experienced developers. However, their
training using unsanitized data from open-source repositories, like GitHub,
introduces the risk of inadvertently propagating security vulnerabilities. To
effectively mitigate this concern, this paper presents a comprehensive study
focused on evaluating and enhancing code LLMs from a software security
perspective. We introduce SecuCoGen\footnote{SecuCoGen has been uploaded as
supplemental material and will be made publicly available after publication.},
a meticulously curated dataset targeting 21 critical vulnerability types.
SecuCoGen comprises 180 samples and serves as the foundation for conducting
experiments on three crucial code-related tasks: code generation, code repair
and vulnerability classification, with a strong emphasis on security. Our
experimental results reveal that existing models often overlook security
concerns during code generation, leading to the generation of vulnerable code.
To address this, we propose effective approaches to mitigate the security
vulnerabilities and enhance the overall robustness of code generated by LLMs.
Moreover, our study identifies weaknesses in existing models' ability to repair
vulnerable code, even when provided with vulnerability information.
Additionally, certain vulnerability types pose challenges for the models,
hindering their performance in vulnerability classification. Based on these
findings, we believe our study will have a positive impact on the software
engineering community, inspiring the development of improved methods for
training and utilizing LLMs, thereby leading to safer and more trustworthy
model deployment.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16267" title="Abstract">arXiv:2310.16267</a> [<a href="/pdf/2310.16267" title="Download PDF">pdf</a>, <a href="/format/2310.16267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SCB-ST-Dataset4: Extending the Spatio-Temporal Behavior Dataset in  Student Classroom Scenarios Through Image Dataset Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaofei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2310.02522">arXiv:2310.02522</a>; text overlap with <a href="/abs/2306.03318">arXiv:2306.03318</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Using deep learning methods to detect students' classroom behavior
automatically is a promising approach for analyzing their class performance and
improving teaching effectiveness. However, the lack of publicly available
spatio-temporal datasets on student behavior, as well as the high cost of
manually labeling such datasets, pose significant challenges for researchers in
this field. To address this issue, we proposed a method for extending the
spatio-temporal behavior dataset in Student Classroom Scenarios
(SCB-ST-Dataset4) through image dataset. Our SCB-ST-Dataset4 comprises 754094
images with 25670 labels, focusing on 3 behaviors: hand-raising, reading,
writing. Our proposed method can rapidly generate spatio-temporal behavioral
datasets without requiring annotation. Furthermore, we proposed a Behavior
Similarity Index (BSI) to explore the similarity of behaviors. We evaluated the
dataset using the YOLOv5, YOLOv7, YOLOv8, and SlowFast algorithms, achieving a
mean average precision (map) of up to 82.3%. The experiment further
demonstrates the effectiveness of our method. This dataset provides a robust
foundation for future research in student behavior detection, potentially
contributing to advancements in this field. The SCB-ST-Dataset4 is available
for download at: https://github.com/Whiffe/SCB-dataset.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16269" title="Abstract">arXiv:2310.16269</a> [<a href="/pdf/2310.16269" title="Download PDF">pdf</a>, <a href="/format/2310.16269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilingual Coarse Political Stance Classification of Media. The  Editorial Line of a ChatGPT and Bard Newspaper
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Espa%C3%B1a-Bonet%2C+C">Cristina Espa&#xf1;a-Bonet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published at EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Neutrality is difficult to achieve and, in politics, subjective. Traditional
media typically adopt an editorial line that can be used by their potential
readers as an indicator of the media bias. Several platforms currently rate
news outlets according to their political bias. The editorial line and the
ratings help readers in gathering a balanced view of news. But in the advent of
instruction-following language models, tasks such as writing a newspaper
article can be delegated to computers. Without imposing a biased persona, where
would an AI-based news outlet lie within the bias ratings? In this work, we use
the ratings of authentic news outlets to create a multilingual corpus of news
with coarse stance annotations (Left and Right) along with automatically
extracted topic annotations. We show that classifiers trained on this data are
able to identify the editorial line of most unseen newspapers in English,
German, Spanish and Catalan. We then apply the classifiers to 101
newspaper-like articles written by ChatGPT and Bard in the 4 languages at
different time periods. We observe that, similarly to traditional newspapers,
ChatGPT editorial line evolves with time and, being a data-driven system, the
stance of the generated articles differs among languages.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16270" title="Abstract">arXiv:2310.16270</a> [<a href="/pdf/2310.16270" title="Download PDF">pdf</a>, <a href="/format/2310.16270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention Lens: A Tool for Mechanistically Interpreting the Attention  Head Information Retrieval Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sakarvadia%2C+M">Mansi Sakarvadia</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Arham Khan</a>, 
<a href="/search/cs?searchtype=author&query=Ajith%2C+A">Aswathy Ajith</a>, 
<a href="/search/cs?searchtype=author&query=Grzenda%2C+D">Daniel Grzenda</a>, 
<a href="/search/cs?searchtype=author&query=Hudson%2C+N">Nathaniel Hudson</a>, 
<a href="/search/cs?searchtype=author&query=Bauer%2C+A">Andr&#xe9; Bauer</a>, 
<a href="/search/cs?searchtype=author&query=Chard%2C+K">Kyle Chard</a>, 
<a href="/search/cs?searchtype=author&query=Foster%2C+I">Ian Foster</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Transformer-based Large Language Models (LLMs) are the state-of-the-art for
natural language tasks. Recent work has attempted to decode, by reverse
engineering the role of linear layers, the internal mechanisms by which LLMs
arrive at their final predictions for text completion tasks. Yet little is
known about the specific role of attention heads in producing the final token
prediction. We propose Attention Lens, a tool that enables researchers to
translate the outputs of attention heads into vocabulary tokens via learned
attention-head-specific transformations called lenses. Preliminary findings
from our trained lenses indicate that attention heads play highly specialized
roles in language models. The code for Attention Lens is available at
github.com/msakarvadia/AttentionLens.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16271" title="Abstract">arXiv:2310.16271</a> [<a href="/pdf/2310.16271" title="Download PDF">pdf</a>, <a href="/format/2310.16271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CycleAlign: Iterative Distillation from Black-box LLM to White-box  Models for Better Human Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Jixiang Hong</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Q">Quan Tu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Changyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xing Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+R">Rui Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Language models trained on large-scale corpus often generate content that is
harmful, toxic, or contrary to human preferences, making their alignment with
human values a critical concern. Reinforcement learning from human feedback
(RLHF) with algorithms like PPO is a prevalent approach for alignment but is
often complex, unstable, and resource-intensive. Recently, ranking-based
alignment methods have emerged, offering stability and effectiveness by
replacing the RL framework with supervised fine-tuning, but they are costly due
to the need for annotated data. Considering that existing large language models
(LLMs) like ChatGPT are already relatively well-aligned and cost-friendly,
researchers have begun to align the language model with human preference from
AI feedback. The common practices, which unidirectionally distill the
instruction-following responses from LLMs, are constrained by their bottleneck.
Thus we introduce CycleAlign to distill alignment capabilities from
parameter-invisible LLMs (black-box) to a parameter-visible model (white-box)
in an iterative manner. With in-context learning (ICL) as the core of the
cycle, the black-box models are able to rank the model-generated responses
guided by human-craft instruction and demonstrations about their preferences.
During iterative interaction, the white-box models also have a judgment about
responses generated by them. Consequently, the agreement ranking could be
viewed as a pseudo label to dynamically update the in-context demonstrations
and improve the preference ranking ability of black-box models. Through
multiple interactions, the CycleAlign framework could align the white-box model
with the black-box model effectively in a low-resource way. Empirical results
illustrate that the model fine-tuned by CycleAlign remarkably exceeds existing
methods, and achieves the state-of-the-art performance in alignment with human
value.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16273" title="Abstract">arXiv:2310.16273</a> [<a href="/pdf/2310.16273" title="Download PDF">pdf</a>, <a href="/format/2310.16273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning for Plant Identification and Disease Classification from  Leaf Images: Multi-prediction Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jianping Yao</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+S+N">Son N. Tran</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Saurabh Garg</a>, 
<a href="/search/cs?searchtype=author&query=Sawyer%2C+S">Samantha Sawyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Jianping and Son are joint first authors (equal contribution)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep learning plays an important role in modern agriculture, especially in
plant pathology using leaf images where convolutional neural networks (CNN) are
attracting a lot of attention. While numerous reviews have explored the
applications of deep learning within this research domain, there remains a
notable absence of an empirical study to offer insightful comparisons due to
the employment of varied datasets in the evaluation. Furthermore, a majority of
these approaches tend to address the problem as a singular prediction task,
overlooking the multifaceted nature of predicting various aspects of plant
species and disease types. Lastly, there is an evident need for a more profound
consideration of the semantic relationships that underlie plant species and
disease types. In this paper, we start our study by surveying current deep
learning approaches for plant identification and disease classification. We
categorise the approaches into multi-model, multi-label, multi-output, and
multi-task, in which different backbone CNNs can be employed. Furthermore,
based on the survey of existing approaches in plant pathology and the study of
available approaches in machine learning, we propose a new model named
Generalised Stacking Multi-output CNN (GSMo-CNN). To investigate the
effectiveness of different backbone CNNs and learning approaches, we conduct an
intensive experiment on three benchmark datasets Plant Village, Plant Leaves,
and PlantDoc. The experimental results demonstrate that InceptionV3 can be a
good choice for a backbone CNN as its performance is better than AlexNet,
VGG16, ResNet101, EfficientNet, MobileNet, and a custom CNN developed by us.
Interestingly, empirical results support the hypothesis that using a single
model can be comparable or better than using two models. Finally, we show that
the proposed GSMo-CNN achieves state-of-the-art performance on three benchmark
datasets.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16274" title="Abstract">arXiv:2310.16274</a> [<a href="/pdf/2310.16274" title="Download PDF">pdf</a>, <a href="/ps/2310.16274" title="Download PostScript">ps</a>, <a href="/format/2310.16274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A monotone $Q^1$ finite element method for anisotropic elliptic  equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+X">Xiangxiong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We construct a monotone continuous $Q^1$ finite element method on the uniform
mesh for the anisotropic diffusion problem with a diagonally dominant diffusion
coefficient matrix. The monotonicity implies the discrete maximum principle.
Convergence of the new scheme is rigorously proven. On quadrilateral meshes,
the matrix coefficient conditions translate into specific a mesh constraint.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16277" title="Abstract">arXiv:2310.16277</a> [<a href="/pdf/2310.16277" title="Download PDF">pdf</a>, <a href="/format/2310.16277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Domain Invariant Learning via Posterior Generalization of  Parameter Distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shiyu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+B">Bin Pan</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+T">Tianyang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tao Li</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhenwei Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Domain invariant learning aims to learn models that extract invariant
features over various training domains, resulting in better generalization to
unseen target domains. Recently, Bayesian Neural Networks have achieved
promising results in domain invariant learning, but most works concentrate on
aligning features distributions rather than parameter distributions. Inspired
by the principle of Bayesian Neural Network, we attempt to directly learn the
domain invariant posterior distribution of network parameters. We first propose
a theorem to show that the invariant posterior of parameters can be implicitly
inferred by aggregating posteriors on different training domains. Our
assumption is more relaxed and allows us to extract more domain invariant
information. We also propose a simple yet effective method, named PosTerior
Generalization (PTG), that can be used to estimate the invariant parameter
distribution. PTG fully exploits variational inference to approximate parameter
distributions, including the invariant posterior and the posteriors on training
domains. Furthermore, we develop a lite version of PTG for widespread
applications. PTG shows competitive performance on various domain
generalization benchmarks on DomainBed. Additionally, PTG can use any existing
domain generalization methods as its prior, and combined with previous
state-of-the-art method the performance can be further improved. Code will be
made public.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16278" title="Abstract">arXiv:2310.16278</a> [<a href="/pdf/2310.16278" title="Download PDF">pdf</a>, <a href="/format/2310.16278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XFEVER: Exploring Fact Verification across Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yi-Chen Chang</a>, 
<a href="/search/cs?searchtype=author&query=Kruengkrai%2C+C">Canasai Kruengkrai</a>, 
<a href="/search/cs?searchtype=author&query=Yamagishi%2C+J">Junichi Yamagishi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for an oral presentation at the 35th Conference on Computational Linguistics and Speech Processing (ROCLING 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper introduces the Cross-lingual Fact Extraction and VERification
(XFEVER) dataset designed for benchmarking the fact verification models across
different languages. We constructed it by translating the claim and evidence
texts of the Fact Extraction and VERification (FEVER) dataset into six
languages. The training and development sets were translated using machine
translation, whereas the test set includes texts translated by professional
translators and machine-translated texts. Using the XFEVER dataset, two
cross-lingual fact verification scenarios, zero-shot learning and
translate-train learning, are defined, and baseline models for each scenario
are also proposed in this paper. Experimental results show that the
multilingual language model can be used to build fact verification models in
different languages efficiently. However, the performance varies by language
and is somewhat inferior to the English case. We also found that we can
effectively mitigate model miscalibration by considering the prediction
similarity between the English and target languages. The XFEVER dataset, code,
and model checkpoints are available at
https://github.com/nii-yamagishilab/xfever.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16279" title="Abstract">arXiv:2310.16279</a> [<a href="/pdf/2310.16279" title="Download PDF">pdf</a>, <a href="/format/2310.16279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransPose: 6D Object Pose Estimation with Geometry-Aware Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xiao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Deming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guangliang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chengju Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qijun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, IEEE Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Estimating the 6D object pose is an essential task in many applications. Due
to the lack of depth information, existing RGB-based methods are sensitive to
occlusion and illumination changes. How to extract and utilize the geometry
features in depth information is crucial to achieve accurate predictions. To
this end, we propose TransPose, a novel 6D pose framework that exploits
Transformer Encoder with geometry-aware module to develop better learning of
point cloud feature representations. Specifically, we first uniformly sample
point cloud and extract local geometry features with the designed local feature
extractor base on graph convolution network. To improve robustness to
occlusion, we adopt Transformer to perform the exchange of global information,
making each local feature contains global information. Finally, we introduce
geometry-aware module in Transformer Encoder, which to form an effective
constrain for point cloud feature learning and makes the global information
exchange more tightly coupled with point cloud tasks. Extensive experiments
indicate the effectiveness of TransPose, our pose estimation pipeline achieves
competitive results on three benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16280" title="Abstract">arXiv:2310.16280</a> [<a href="/pdf/2310.16280" title="Download PDF">pdf</a>, <a href="/format/2310.16280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Directly 3D Printed, Pneumatically Actuated Multi-Material Robotic Hand
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Matusik%2C+H">Hanna Matusik</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Rus%2C+D">Daniela Rus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Soft robotic manipulators with many degrees of freedom can carry out complex
tasks safely around humans. However, manufacturing of soft robotic hands with
several degrees of freedom requires a complex multi-step manual process, which
significantly increases their cost. We present a design of a multi-material 15
DoF robotic hand with five fingers including an opposable thumb. Our design has
15 pneumatic actuators based on a series of hollow chambers that are driven by
an external pressure system. The thumb utilizes rigid joints and the palm
features internal rigid structure and soft skin. The design can be directly 3D
printed using a multi-material additive manufacturing process without any
assembly process and therefore our hand can be manufactured for less than 300
dollars. We test the hand in conjunction with a low-cost vision-based
teleoperation system on different tasks.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16283" title="Abstract">arXiv:2310.16283</a> [<a href="/pdf/2310.16283" title="Download PDF">pdf</a>, <a href="/format/2310.16283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling and Analysis of the Lead-Lag Network of Economic Indicators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goodrick%2C+A">Amanda Goodrick</a>, 
<a href="/search/cs?searchtype=author&query=Sayama%2C+H">Hiroki Sayama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 7 figures, 1 table; peer reviewed and accepted for presentation at International Conference on Data Science, Computation and Security (IDSCS-2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">We propose a method of analyzing multivariate time series data that
investigates lead-lag relationships among economic indicators during the
COVID-19 era with a weighted directed network of lagged variables. The analysis
includes a stock index, average unemployment, and several variables that are
used to calculate inflation. Three complex networks are created, with these
variables and several lags of each as the network nodes. Network edges are
weighted based on three relationship metrics: correlation, mutual information,
and transfer entropy. In each network, nodes are merged, and edges are
aggregated to simplify the weighted directed graph. Pagerank is used to
determine the most influential and the most influenced node over the time
period. Results were reasonably robust within each network, but they were
heavily dependent on the choice of metric.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16287" title="Abstract">arXiv:2310.16287</a> [<a href="/pdf/2310.16287" title="Download PDF">pdf</a>, <a href="/format/2310.16287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Streaming Speech-to-Avatar Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prabhune%2C+T+S">Tejas S. Prabhune</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Peter Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bohan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Anumanchipalli%2C+G+K">Gopala K. Anumanchipalli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Graphics (cs.GR); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Streaming speech-to-avatar synthesis creates real-time animations for a
virtual character from audio data. Accurate avatar representations of speech
are important for the visualization of sound in linguistics, phonetics, and
phonology, visual feedback to assist second language acquisition, and virtual
embodiment for paralyzed patients. Previous works have highlighted the
capability of deep articulatory inversion to perform high-quality avatar
animation using electromagnetic articulography (EMA) features. However, these
models focus on offline avatar synthesis with recordings rather than real-time
audio, which is necessary for live avatar visualization or embodiment. To
address this issue, we propose a method using articulatory inversion for
streaming high quality facial and inner-mouth avatar animation from real-time
audio. Our approach achieves 130ms average streaming latency for every 0.1
seconds of audio with a 0.792 correlation with ground truth articulations.
Finally, we show generated mouth and tongue animations to demonstrate the
efficacy of our methodology.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16288" title="Abstract">arXiv:2310.16288</a> [<a href="/pdf/2310.16288" title="Download PDF">pdf</a>, <a href="/format/2310.16288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MotionAGFormer: Enhancing 3D Human Pose Estimation with a  Transformer-GCNFormer Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehraban%2C+S">Soroush Mehraban</a>, 
<a href="/search/cs?searchtype=author&query=Adeli%2C+V">Vida Adeli</a>, 
<a href="/search/cs?searchtype=author&query=Taati%2C+B">Babak Taati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent transformer-based approaches have demonstrated excellent performance
in 3D human pose estimation. However, they have a holistic view and by encoding
global relationships between all the joints, they do not capture the local
dependencies precisely. In this paper, we present a novel Attention-GCNFormer
(AGFormer) block that divides the number of channels by using two parallel
transformer and GCNFormer streams. Our proposed GCNFormer module exploits the
local relationship between adjacent joints, outputting a new representation
that is complementary to the transformer output. By fusing these two
representation in an adaptive way, AGFormer exhibits the ability to better
learn the underlying 3D structure. By stacking multiple AGFormer blocks, we
propose MotionAGFormer in four different variants, which can be chosen based on
the speed-accuracy trade-off. We evaluate our model on two popular benchmark
datasets: Human3.6M and MPI-INF-3DHP. MotionAGFormer-B achieves
state-of-the-art results, with P1 errors of 38.4mm and 16.2mm, respectively.
Remarkably, it uses a quarter of the parameters and is three times more
computationally efficient than the previous leading model on Human3.6M dataset.
Code and models are available at https://github.com/TaatiTeam/MotionAGFormer.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16293" title="Abstract">arXiv:2310.16293</a> [<a href="/pdf/2310.16293" title="Download PDF">pdf</a>, <a href="/format/2310.16293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crowd-Certain: Label Aggregation in Crowdsourced and Ensemble Learning  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Majdi%2C+M+S">Mohammad S. Majdi</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+J+J">Jeffrey J. Rodriguez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 49 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Crowdsourcing systems have been used to accumulate massive amounts of labeled
data for applications such as computer vision and natural language processing.
However, because crowdsourced labeling is inherently dynamic and uncertain,
developing a technique that can work in most situations is extremely
challenging. In this paper, we introduce Crowd-Certain, a novel approach for
label aggregation in crowdsourced and ensemble learning classification tasks
that offers improved performance and computational efficiency for different
numbers of annotators and a variety of datasets. The proposed method uses the
consistency of the annotators versus a trained classifier to determine a
reliability score for each annotator. Furthermore, Crowd-Certain leverages
predicted probabilities, enabling the reuse of trained classifiers on future
sample data, thereby eliminating the need for recurrent simulation processes
inherent in existing methods. We extensively evaluated our approach against ten
existing techniques across ten different datasets, each labeled by varying
numbers of annotators. The findings demonstrate that Crowd-Certain outperforms
the existing methods (Tao, Sheng, KOS, MACE, MajorityVote, MMSR, Wawa,
Zero-Based Skill, GLAD, and Dawid Skene), in nearly all scenarios, delivering
higher average accuracy, F1 scores, and AUC rates. Additionally, we introduce a
variation of two existing confidence score measurement techniques. Finally we
evaluate these two confidence score techniques using two evaluation metrics:
Expected Calibration Error (ECE) and Brier Score Loss. Our results show that
Crowd-Certain achieves higher Brier Score, and lower ECE across the majority of
the examined datasets, suggesting better calibrated results.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16295" title="Abstract">arXiv:2310.16295</a> [<a href="/pdf/2310.16295" title="Download PDF">pdf</a>, <a href="/format/2310.16295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instance-wise Linearization of Neural Network for Model Interpretation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhimin Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shusen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bhavya%2C+K">Kailkhura Bhavya</a>, 
<a href="/search/cs?searchtype=author&query=Bremer%2C+T">Timo Bremer</a>, 
<a href="/search/cs?searchtype=author&query=Pascucci%2C+V">Valerio Pascucci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Neural network have achieved remarkable successes in many scientific fields.
However, the interpretability of the neural network model is still a major
bottlenecks to deploy such technique into our daily life. The challenge can
dive into the non-linear behavior of the neural network, which rises a critical
question that how a model use input feature to make a decision. The classical
approach to address this challenge is feature attribution, which assigns an
important score to each input feature and reveal its importance of current
prediction. However, current feature attribution approaches often indicate the
importance of each input feature without detail of how they are actually
processed by a model internally. These attribution approaches often raise a
concern that whether they highlight correct features for a model prediction.
<br />For a neural network model, the non-linear behavior is often caused by
non-linear activation units of a model. However, the computation behavior of a
prediction from a neural network model is locally linear, because one
prediction has only one activation pattern. Base on the observation, we propose
an instance-wise linearization approach to reformulates the forward computation
process of a neural network prediction. This approach reformulates different
layers of convolution neural networks into linear matrix multiplication.
Aggregating all layers' computation, a prediction complex convolution neural
network operations can be described as a linear matrix multiplication $F(x) = W
\cdot x + b$. This equation can not only provides a feature attribution map
that highlights the important of the input features but also tells how each
input feature contributes to a prediction exactly. Furthermore, we discuss the
application of this technique in both supervise classification and unsupervised
neural network learning parametric t-SNE dimension reduction.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16298" title="Abstract">arXiv:2310.16298</a> [<a href="/pdf/2310.16298" title="Download PDF">pdf</a>, <a href="/format/2310.16298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stencil Computation with Vector Outer Product
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenxuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Liang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Baicheng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+P">Penghao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunquan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Long Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhe Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Matrix computation units have been equipped in current architectures to
accelerate AI and high performance computing applications. The matrix
multiplication and vector outer product are two basic instruction types. The
latter one is lighter since the inputs are vectors. Thus it provides more
opportunities to develop flexible algorithms for problems other than dense
linear algebra computing and more possibilities to optimize the implementation.
Stencil computations represent a common class of nested loops in scientific and
engineering applications. This paper proposes a novel stencil algorithm using
vector outer products. Unlike previous work, the new algorithm arises from the
stencil definition in the scatter mode and is initially expressed with formulas
of vector outer products. The implementation incorporates a set of
optimizations to improve the memory reference pattern, execution pipeline and
data reuse by considering various algorithmic options and the data sharing
between input vectors. Evaluation on a simulator shows that our design achieves
a substantial speedup compared with vectorized stencil algorithm.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16299" title="Abstract">arXiv:2310.16299</a> [<a href="/pdf/2310.16299" title="Download PDF">pdf</a>, <a href="/format/2310.16299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FoundLoc: Vision-based Onboard Aerial Localization in the Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yao He</a>, 
<a href="/search/cs?searchtype=author&query=Cisneros%2C+I">Ivan Cisneros</a>, 
<a href="/search/cs?searchtype=author&query=Keetha%2C+N">Nikhil Keetha</a>, 
<a href="/search/cs?searchtype=author&query=Patrikar%2C+J">Jay Patrikar</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Zelin Ye</a>, 
<a href="/search/cs?searchtype=author&query=Higgins%2C+I">Ian Higgins</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yaoyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Kapoor%2C+P">Parv Kapoor</a>, 
<a href="/search/cs?searchtype=author&query=Scherer%2C+S">Sebastian Scherer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Robust and accurate localization for Unmanned Aerial Vehicles (UAVs) is an
essential capability to achieve autonomous, long-range flights. Current methods
either rely heavily on GNSS, face limitations in visual-based localization due
to appearance variances and stylistic dissimilarities between camera and
reference imagery, or operate under the assumption of a known initial pose. In
this paper, we developed a GNSS-denied localization approach for UAVs that
harnesses both Visual-Inertial Odometry (VIO) and Visual Place Recognition
(VPR) using a foundation model. This paper presents a novel vision-based
pipeline that works exclusively with a nadir-facing camera, an Inertial
Measurement Unit (IMU), and pre-existing satellite imagery for robust, accurate
localization in varied environments and conditions. Our system demonstrated
average localization accuracy within a $20$-meter range, with a minimum error
below $1$ meter, under real-world conditions marked by drastic changes in
environmental appearance and with no assumption of the vehicle's initial pose.
The method is proven to be effective and robust, addressing the crucial need
for reliable UAV localization in GNSS-denied environments, while also being
computationally efficient enough to be deployed on resource-constrained
platforms.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16300" title="Abstract">arXiv:2310.16300</a> [<a href="/pdf/2310.16300" title="Download PDF">pdf</a>, <a href="/format/2310.16300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Snapshot: Fast, Userspace Crash Consistency for CXL and PM Using msync
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahar%2C+S">Suyash Mahar</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+M">Mingyao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Kelly%2C+T">Terence Kelly</a>, 
<a href="/search/cs?searchtype=author&query=Swanson%2C+S">Steven Swanson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A shorter version of this paper appeared in the Proceedings of ICCD 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Operating Systems (cs.OS)

</div>
<p class="mathjax">Crash consistency using persistent memory programming libraries requires
programmers to use complex transactions and manual annotations. In contrast,
the failure-atomic msync() (FAMS) interface is much simpler as it transparently
tracks updates and guarantees that modified data is atomically durable on a
call to the failure-atomic variant of msync(). However, FAMS suffers from
several drawbacks, like the overhead of msync() and the write amplification
from page-level dirty data tracking.
<br />To address these drawbacks while preserving the advantages of FAMS, we
propose Snapshot, an efficient userspace implementation of FAMS.
<br />Snapshot uses compiler-based annotation to transparently track updates in
userspace and syncs them with the backing byte-addressable storage copy on a
call to msync(). By keeping a copy of application data in DRAM, Snapshot
improves access latency. Moreover, with automatic tracking and syncing changes
only on a call to msync(), Snapshot provides crash-consistency guarantees,
unlike the POSIX msync() system call.
<br />For a KV-Store backed by Intel Optane running the YCSB benchmark, Snapshot
achieves at least 1.2$\times$ speedup over PMDK while significantly
outperforming conventional (non-crash-consistent) msync(). On an emulated CXL
memory semantic SSD, Snapshot outperforms PMDK by up to 10.9$\times$ on all but
one YCSB workload, where PMDK is 1.2$\times$ faster than Snapshot. Further,
Kyoto Cabinet commits perform up to 8.0$\times$ faster with Snapshot than its
built-in, msync()-based crash-consistency mechanism.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16301" title="Abstract">arXiv:2310.16301</a> [<a href="/pdf/2310.16301" title="Download PDF">pdf</a>, <a href="/format/2310.16301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is ChatGPT a Good Multi-Party Conversation Solver?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Chao-Hong Tan</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jia-Chen Gu</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zhen-Hua Ling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have emerged as influential instruments within
the realm of natural language processing; nevertheless, their capacity to
handle multi-party conversations (MPCs) -- a scenario marked by the presence of
multiple interlocutors involved in intricate information exchanges -- remains
uncharted. In this paper, we delve into the potential of generative LLMs such
as ChatGPT and GPT-4 within the context of MPCs. An empirical analysis is
conducted to assess the zero-shot learning capabilities of ChatGPT and GPT-4 by
subjecting them to evaluation across three MPC datasets that encompass five
representative tasks. The findings reveal that ChatGPT's performance on a
number of evaluated MPC tasks leaves much to be desired, whilst GPT-4's results
portend a promising future. Additionally, we endeavor to bolster performance
through the incorporation of MPC structures, encompassing both speaker and
addressee architecture. This study provides an exhaustive evaluation and
analysis of applying generative LLMs to MPCs, casting a light upon the
conception and creation of increasingly effective and robust MPC agents.
Concurrently, this work underscores the challenges implicit in the utilization
of LLMs for MPCs, such as deciphering graphical information flows and
generating stylistically consistent responses.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16302" title="Abstract">arXiv:2310.16302</a> [<a href="/pdf/2310.16302" title="Download PDF">pdf</a>, <a href="/format/2310.16302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Imperfect Digital Twin Assisted Low Cost Reinforcement Training for  Multi-UAV Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiucheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+N">Nan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Longfei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhisheng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Luan%2C+T">Tom. Luan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+N">Ning Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Deep Reinforcement Learning (DRL) is widely used to optimize the performance
of multi-UAV networks. However, the training of DRL relies on the frequent
interactions between the UAVs and the environment, which consumes lots of
energy due to the flying and communication of UAVs in practical experiments.
Inspired by the growing digital twin (DT) technology, which can simulate the
performance of algorithms in the digital space constructed by coping features
of the physical space, the DT is introduced to reduce the costs of practical
training, e.g., energy and hardware purchases. Different from previous
DT-assisted works with an assumption of perfect reflecting real physics by
virtual digital, we consider an imperfect DT model with deviations for
assisting the training of multi-UAV networks. Remarkably, to trade off the
training cost, DT construction cost, and the impact of deviations of DT on
training, the natural and virtually generated UAV mixing deployment method is
proposed. Two cascade neural networks (NN) are used to optimize the joint
number of virtually generated UAVs, the DT construction cost, and the
performance of multi-UAV networks. These two NNs are trained by unsupervised
and reinforcement learning, both low-cost label-free training methods.
Simulation results show the training cost can significantly decrease while
guaranteeing the training performance. This implies that an efficient decision
can be made with imperfect DTs in multi-UAV networks.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16303" title="Abstract">arXiv:2310.16303</a> [<a href="/pdf/2310.16303" title="Download PDF">pdf</a>, <a href="/format/2310.16303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> URL-BERT: Training Webpage Representations via Social Media Engagements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qamar%2C+A">Ayesha Qamar</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+C">Chetan Verma</a>, 
<a href="/search/cs?searchtype=author&query=El-Kishky%2C+A">Ahmed El-Kishky</a>, 
<a href="/search/cs?searchtype=author&query=Binnani%2C+S">Sumit Binnani</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+S">Sneha Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T">Taylor Berg-Kirkpatrick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Understanding and representing webpages is crucial to online social networks
where users may share and engage with URLs. Common language model (LM) encoders
such as BERT can be used to understand and represent the textual content of
webpages. However, these representations may not model thematic information of
web domains and URLs or accurately capture their appeal to social media users.
In this work, we introduce a new pre-training objective that can be used to
adapt LMs to understand URLs and webpages. Our proposed framework consists of
two steps: (1) scalable graph embeddings to learn shallow representations of
URLs based on user engagement on social media and (2) a contrastive objective
that aligns LM representations with the aforementioned graph-based
representation. We apply our framework to the multilingual version of BERT to
obtain the model URL-BERT. We experimentally demonstrate that our continued
pre-training approach improves webpage understanding on a variety of tasks and
Twitter internal and external benchmarks.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16305" title="Abstract">arXiv:2310.16305</a> [<a href="/pdf/2310.16305" title="Download PDF">pdf</a>, <a href="/format/2310.16305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dolfin: Diffusion Layout Transformers without Autoencoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yilin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zeyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+L">Liangjun Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zheng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+Z">Zhizhou Sha</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhuowen Tu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we introduce a novel generative model, Diffusion Layout
Transformers without Autoencoder (Dolfin), which significantly improves the
modeling capability with reduced complexity compared to existing methods.
Dolfin employs a Transformer-based diffusion process to model layout
generation. In addition to an efficient bi-directional (non-causal joint)
sequence representation, we further propose an autoregressive diffusion model
(Dolfin-AR) that is especially adept at capturing rich semantic correlations
for the neighboring objects, such as alignment, size, and overlap. When
evaluated against standard generative layout benchmarks, Dolfin notably
improves performance across various metrics (fid, alignment, overlap, MaxIoU
and DocSim scores), enhancing transparency and interoperability in the process.
Moreover, Dolfin's applications extend beyond layout generation, making it
suitable for modeling geometric structures, such as line segments. Our
experiments present both qualitative and quantitative results to demonstrate
the advantages of Dolfin.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16310" title="Abstract">arXiv:2310.16310</a> [<a href="/pdf/2310.16310" title="Download PDF">pdf</a>, <a href="/format/2310.16310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Score Matching-based Pseudolikelihood Estimation of Neural Marked  Spatio-Temporal Point Process with Uncertainty Quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zichong Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qunzhi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhenghao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Y">Yajun Mei</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tuo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+H">Hongyuan Zha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Spatio-temporal point processes (STPPs) are potent mathematical tools for
modeling and predicting events with both temporal and spatial features. Despite
their versatility, most existing methods for learning STPPs either assume a
restricted form of the spatio-temporal distribution, or suffer from inaccurate
approximations of the intractable integral in the likelihood training
objective. These issues typically arise from the normalization term of the
probability density function. Moreover, current techniques fail to provide
uncertainty quantification for model predictions, such as confidence intervals
for the predicted event's arrival time and confidence regions for the event's
location, which is crucial given the considerable randomness of the data. To
tackle these challenges, we introduce SMASH: a Score MAtching-based
pSeudolikeliHood estimator for learning marked STPPs with uncertainty
quantification. Specifically, our framework adopts a normalization-free
objective by estimating the pseudolikelihood of marked STPPs through
score-matching and offers uncertainty quantification for the predicted event
time, location and mark by computing confidence regions over the generated
samples. The superior performance of our proposed framework is demonstrated
through extensive experiments in both event prediction and uncertainty
quantification.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16314" title="Abstract">arXiv:2310.16314</a> [<a href="/pdf/2310.16314" title="Download PDF">pdf</a>, <a href="/format/2310.16314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Code Semantics: An Evaluation of Transformer Models in  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondal%2C+D">Debanjan Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Lodha%2C+A">Abhilasha Lodha</a>, 
<a href="/search/cs?searchtype=author&query=Sahoo%2C+A">Ankita Sahoo</a>, 
<a href="/search/cs?searchtype=author&query=Kumari%2C+B">Beena Kumari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> All authors are co-first authors and have equal contributions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper delves into the intricacies of code summarization using advanced
transformer-based language models. Through empirical studies, we evaluate the
efficacy of code summarization by altering function and variable names to
explore whether models truly understand code semantics or merely rely on
textual cues. We have also introduced adversaries like dead code and commented
code across three programming languages (Python, Javascript, and Java) to
further scrutinize the model's understanding. Ultimately, our research aims to
offer valuable insights into the inner workings of transformer-based LMs,
enhancing their ability to understand code and contributing to more efficient
software development practices and maintenance workflows.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16316" title="Abstract">arXiv:2310.16316</a> [<a href="/pdf/2310.16316" title="Download PDF">pdf</a>, <a href="/format/2310.16316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sum-of-Parts Models: Faithful Attributions for Groups of Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+W">Weiqiu You</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Helen Qu</a>, 
<a href="/search/cs?searchtype=author&query=Gatti%2C+M">Marco Gatti</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+B">Bhuvnesh Jain</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+E">Eric Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">An explanation of a machine learning model is considered "faithful" if it
accurately reflects the model's decision-making process. However, explanations
such as feature attributions for deep learning are not guaranteed to be
faithful, and can produce potentially misleading interpretations. In this work,
we develop Sum-of-Parts (SOP), a class of models whose predictions come with
grouped feature attributions that are faithful-by-construction. This model
decomposes a prediction into an interpretable sum of scores, each of which is
directly attributable to a sparse group of features. We evaluate SOP on
benchmarks with standard interpretability metrics, and in a case study, we use
the faithful explanations from SOP to help astrophysicists discover new
knowledge about galaxy formation.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16318" title="Abstract">arXiv:2310.16318</a> [<a href="/pdf/2310.16318" title="Download PDF">pdf</a>, <a href="/format/2310.16318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked  Auto-Encoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jang%2C+H">Huiwon Jang</a>, 
<a href="/search/cs?searchtype=author&query=Tack%2C+J">Jihoon Tack</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+D">Daewon Choi</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+J">Jongheon Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jinwoo Shin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023. The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite its practical importance across a wide range of modalities, recent
advances in self-supervised learning (SSL) have been primarily focused on a few
well-curated domains, e.g., vision and language, often relying on their
domain-specific knowledge. For example, Masked Auto-Encoder (MAE) has become
one of the popular architectures in these domains, but less has explored its
potential in other modalities. In this paper, we develop MAE as a unified,
modality-agnostic SSL framework. In turn, we argue meta-learning as a key to
interpreting MAE as a modality-agnostic learner, and propose enhancements to
MAE from the motivation to jointly improve its SSL across diverse modalities,
coined MetaMAE as a result. Our key idea is to view the mask reconstruction of
MAE as a meta-learning task: masked tokens are predicted by adapting the
Transformer meta-learner through the amortization of unmasked tokens. Based on
this novel interpretation, we propose to integrate two advanced meta-learning
techniques. First, we adapt the amortized latent of the Transformer encoder
using gradient-based meta-learning to enhance the reconstruction. Then, we
maximize the alignment between amortized and adapted latents through task
contrastive learning which guides the Transformer encoder to better encode the
task-specific knowledge. Our experiment demonstrates the superiority of MetaMAE
in the modality-agnostic SSL benchmark (called DABS), significantly
outperforming prior baselines. Code is available at
https://github.com/alinlab/MetaMAE.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16319" title="Abstract">arXiv:2310.16319</a> [<a href="/pdf/2310.16319" title="Download PDF">pdf</a>, <a href="/format/2310.16319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiQAD: A Benchmark Dataset for End-to-End Open-domain Dialogue  Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yukun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+L">Lingyong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Weiwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+C">Chong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuaiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhicong Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhaochun Ren</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Dialogue assessment plays a critical role in the development of open-domain
dialogue systems. Existing work are uncapable of providing an end-to-end and
human-epistemic assessment dataset, while they only provide sub-metrics like
coherence or the dialogues are conversed between annotators far from real user
settings. In this paper, we release a large-scale dialogue quality assessment
dataset (DiQAD), for automatically assessing open-domain dialogue quality.
Specifically, we (1) establish the assessment criteria based on the dimensions
conforming to human judgements on dialogue qualities, and (2) annotate
large-scale dialogues that conversed between real users based on these
annotation criteria, which contains around 100,000 dialogues. We conduct
several experiments and report the performances of the baselines as the
benchmark on DiQAD. The dataset is openly accessible at
https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16322" title="Abstract">arXiv:2310.16322</a> [<a href="/pdf/2310.16322" title="Download PDF">pdf</a>, <a href="/format/2310.16322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Samsung R&amp;D Institute Philippines at WMT 2023
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cruz%2C+J+C+B">Jan Christian Blaise Cruz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Proceedings of the Eighth Conference on Machine Translation 2023 (WMT)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we describe the constrained MT systems submitted by Samsung
R&amp;D Institute Philippines to the WMT 2023 General Translation Task for two
directions: en$\rightarrow$he and he$\rightarrow$en. Our systems comprise of
Transformer-based sequence-to-sequence models that are trained with a mix of
best practices: comprehensive data preprocessing pipelines, synthetic
backtranslated data, and the use of noisy channel reranking during online
decoding. Our models perform comparably to, and sometimes outperform, strong
baseline unconstrained systems such as mBART50 M2M and NLLB 200 MoE despite
having significantly fewer parameters on two public benchmarks: FLORES-200 and
NTREX-128.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16324" title="Abstract">arXiv:2310.16324</a> [<a href="/pdf/2310.16324" title="Download PDF">pdf</a>, <a href="/format/2310.16324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extracting Design Knowledge from Optimization Data: Enhancing  Engineering Design in Fluid Based Thermal Management Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bayat%2C+S">Saeid Bayat</a>, 
<a href="/search/eess?searchtype=author&query=Shahmansouri%2C+N">Nastaran Shahmansouri</a>, 
<a href="/search/eess?searchtype=author&query=Peddada%2C+S+R">Satya RT Peddada</a>, 
<a href="/search/eess?searchtype=author&query=Tessier%2C+A">Alex Tessier</a>, 
<a href="/search/eess?searchtype=author&query=Butscher%2C+A">Adrian Butscher</a>, 
<a href="/search/eess?searchtype=author&query=Allison%2C+J+T">James T Allison</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">As mechanical systems become more complex and technological advances
accelerate, the traditional reliance on heritage designs for engineering
endeavors is being diminished in its effectiveness. Considering the dynamic
nature of the design industry where new challenges are continually emerging,
alternative sources of knowledge need to be sought to guide future design
efforts. One promising avenue lies in the analysis of design optimization data,
which has the potential to offer valuable insights and overcome the limitations
of heritage designs. This paper presents a step toward extracting knowledge
from optimization data in multi-split fluid-based thermal management systems
using different classification machine learning methods, so that designers can
use it to guide decisions in future design efforts. This approach offers
several advantages over traditional design heritage methods, including
applicability in cases where there is no design heritage and the ability to
derive optimal designs. We showcase our framework through four case studies
with varying levels of complexity. These studies demonstrate its effectiveness
in enhancing the design of complex thermal management systems. Our results show
that the knowledge extracted from the configuration design optimization data
provides a good basis for more general design of complex thermal management
systems. It is shown that the objective value of the estimated optimal
configuration closely approximates the true optimal configuration with less
than 1 percent error, achieved using basic features based on the system heat
loads without involving the corresponding optimal open loop control (OLOC)
features. This eliminates the need to solve the OLOC problem, leading to
reduced computation costs.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16326" title="Abstract">arXiv:2310.16326</a> [<a href="/pdf/2310.16326" title="Download PDF">pdf</a>, <a href="/format/2310.16326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning for SBM Graphon Games with Re-Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huo%2C+P">Peihan Huo</a>, 
<a href="/search/cs?searchtype=author&query=Peralta%2C+O">Oscar Peralta</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Junyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qiaomin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Minca%2C+A">Andreea Minca</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The Mean-Field approximation is a tractable approach for studying large
population dynamics. However, its assumption on homogeneity and universal
connections among all agents limits its applicability in many real-world
scenarios. Multi-Population Mean-Field Game (MP-MFG) models have been
introduced in the literature to address these limitations. When the underlying
Stochastic Block Model is known, we show that a Policy Mirror Ascent algorithm
finds the MP-MFG Nash Equilibrium. In more realistic scenarios where the block
model is unknown, we propose a re-sampling scheme from a graphon integrated
with the finite N-player MP-MFG model. We develop a novel learning framework
based on a Graphon Game with Re-Sampling (GGR-S) model, which captures the
complex network structures of agents' connections. We analyze GGR-S dynamics
and establish the convergence to dynamics of MP-MFG. Leveraging this result, we
propose an efficient sample-based N-player Reinforcement Learning algorithm for
GGR-S without population manipulation, and provide a rigorous convergence
analysis with finite sample guarantee.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16329" title="Abstract">arXiv:2310.16329</a> [<a href="/pdf/2310.16329" title="Download PDF">pdf</a>, <a href="/format/2310.16329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment  of Coherence in Generated Texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maimon%2C+A">Aviya Maimon</a>, 
<a href="/search/cs?searchtype=author&query=Tsarfaty%2C+R">Reut Tsarfaty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB)

</div>
<p class="mathjax">Coherence is a linguistic term that refers to the relations between small
textual units (sentences, propositions), which make the text logically
consistent and meaningful to the reader. With the advances of generative
foundational models in NLP, there is a pressing need to automatically assess
the human-perceived coherence of automatically generated texts. Up until now,
little work has been done on explicitly assessing the coherence of generated
texts and analyzing the factors contributing to (in)coherence. Previous work on
the topic used other tasks, e.g., sentence reordering, as proxies of coherence,
rather than approaching coherence detection heads on. In this paper, we
introduce {\sc CoheSentia}, a novel benchmark of human-perceived coherence of
automatically generated texts. Our annotation protocol reflects two
perspectives; one is global, assigning a single coherence score, and the other
is incremental, scoring sentence by sentence. The incremental method produces
an (in)coherence score for each text fragment and also pinpoints reasons for
incoherence at that point. Our benchmark contains 500 automatically-generated
and human-annotated paragraphs, each annotated in both methods, by multiple
raters. Our analysis shows that the inter-annotator agreement in the
incremental mode is higher than in the holistic alternative, and our
experiments show that standard LMs fine-tuned for coherence detection show
varied performance on the different factors contributing to (in)coherence. All
in all, these models yield unsatisfactory performance, emphasizing the need for
developing more reliable methods for coherence assessment.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16331" title="Abstract">arXiv:2310.16331</a> [<a href="/pdf/2310.16331" title="Download PDF">pdf</a>, <a href="/format/2310.16331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Brain-Inspired Reservoir Computing Using Memristors with Tunable  Dynamics and Short-Term Plasticity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Armendarez%2C+N+X">Nicholas X. Armendarez</a>, 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+A+S">Ahmed S. Mohamed</a>, 
<a href="/search/cs?searchtype=author&query=Dhungel%2C+A">Anurag Dhungel</a>, 
<a href="/search/cs?searchtype=author&query=Hossain%2C+M+R">Md Razuan Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M+S">Md Sakib Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Najem%2C+J+S">Joseph S. Najem</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent advancements in reservoir computing research have created a demand for
analog devices with dynamics that can facilitate the physical implementation of
reservoirs, promising faster information processing while consuming less energy
and occupying a smaller area footprint. Studies have demonstrated that dynamic
memristors, with nonlinear and short-term memory dynamics, are excellent
candidates as information-processing devices or reservoirs for temporal
classification and prediction tasks. Previous implementations relied on
nominally identical memristors that applied the same nonlinear transformation
to the input data, which is not enough to achieve a rich state space. To
address this limitation, researchers either diversified the data encoding
across multiple memristors or harnessed the stochastic device-to-device
variability among the memristors. However, this approach requires additional
pre-processing steps and leads to synchronization issues. Instead, it is
preferable to encode the data once and pass it through a reservoir layer
consisting of memristors with distinct dynamics. Here, we demonstrate that
ion-channel-based memristors with voltage-dependent dynamics can be
controllably and predictively tuned through voltage or adjustment of the ion
channel concentration to exhibit diverse dynamic properties. We show, through
experiments and simulations, that reservoir layers constructed with a small
number of distinct memristors exhibit significantly higher predictive and
classification accuracies with a single data encoding. We found that for a
second-order nonlinear dynamical system prediction task, the varied memristor
reservoir experimentally achieved a normalized mean square error of 0.0015
using only five distinct memristors. Moreover, in a neural activity
classification task, a reservoir of just three distinct memristors
experimentally attained an accuracy of 96.5%.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16332" title="Abstract">arXiv:2310.16332</a> [<a href="/pdf/2310.16332" title="Download PDF">pdf</a>, <a href="/format/2310.16332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corrupting Neuron Explanations of Deep Visual Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+D">Divyansh Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Oikarinen%2C+T">Tuomas Oikarinen</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+T">Tsui-Wei Weng</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2023, pp. 1877-1886
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The inability of DNNs to explain their black-box behavior has led to a recent
surge of explainability methods. However, there are growing concerns that these
explainability methods are not robust and trustworthy. In this work, we perform
the first robustness analysis of Neuron Explanation Methods under a unified
pipeline and show that these explanations can be significantly corrupted by
random noises and well-designed perturbations added to their probing data. We
find that even adding small random noise with a standard deviation of 0.02 can
already change the assigned concepts of up to 28% neurons in the deeper layers.
Furthermore, we devise a novel corruption algorithm and show that our algorithm
can manipulate the explanation of more than 80% neurons by poisoning less than
10% of probing data. This raises the concern of trusting Neuron Explanation
Methods in real-life safety and fairness critical applications.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16333" title="Abstract">arXiv:2310.16333</a> [<a href="/pdf/2310.16333" title="Download PDF">pdf</a>, <a href="/format/2310.16333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Optimal Power Management for Large-Scale Battery Energy Storage  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Farakhor%2C+A">Amir Farakhor</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yebin Wang</a>, 
<a href="/search/eess?searchtype=author&query=Fang%2C+H">Huazhen Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Transactions on Transportation Electrification
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Large-scale battery energy storage systems (BESS) are helping transition the
world towards sustainability with their broad use, among others, in electrified
transportation, power grid, and renewables. However, optimal power management
for them is often computationally formidable. To overcome this challenge, we
develop a scalable approach in the paper. The proposed approach partitions the
constituting cells of a large-scale BESS into clusters based on their
state-of-charge (SoC), temperature, and internal resistance. Each cluster is
characterized by a representative model that approximately captures its
collective SoC and temperature dynamics, as well as its overall power losses in
charging/discharging. Based on the clusters, we then formulate a problem of
receding-horizon optimal power control to minimize the power losses while
promoting SoC and temperature balancing. The cluster-based power optimization
will decide the power quota for each cluster, and then every cluster will split
the quota among the constituent cells. Since the number of clusters is much
fewer than the number of cells, the proposed approach significantly reduces the
computational costs, allowing optimal power management to scale up to
large-scale BESS. Extensive simulations are performed to evaluate the proposed
approach. The obtained results highlight a significant computational overhead
reduction by more than 60% for a small-scale and 98% for a large-scale BESS
compared to the conventional cell-level optimization. Experimental validation
based on a 20-cell prototype further demonstrates its effectiveness and
utility.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16334" title="Abstract">arXiv:2310.16334</a> [<a href="/pdf/2310.16334" title="Download PDF">pdf</a>, <a href="/format/2310.16334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AccoMontage-3: Full-Band Accompaniment Arrangement via Sequential Style  Transfer and Multi-Track Function Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jingwei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+G">Gus Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ye Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We propose AccoMontage-3, a symbolic music automation system capable of
generating multi-track, full-band accompaniment based on the input of a lead
melody with chords (i.e., a lead sheet). The system contains three modular
components, each modelling a vital aspect of full-band composition. The first
component is a piano arranger that generates piano accompaniment for the lead
sheet by transferring texture styles to the chords using latent chord-texture
disentanglement and heuristic retrieval of texture donors. The second component
orchestrates the piano accompaniment score into full-band arrangement according
to the orchestration style encoded by individual track functions. The third
component, which connects the previous two, is a prior model characterizing the
global structure of orchestration style over the whole piece of music. From end
to end, the system learns to generate full-band accompaniment in a
self-supervised fashion, applying style transfer at two levels of polyphonic
composition: texture and orchestration. Experiments show that our system
outperforms the baselines significantly, and the modular design offers
effective controls in a musically meaningful way.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16335" title="Abstract">arXiv:2310.16335</a> [<a href="/pdf/2310.16335" title="Download PDF">pdf</a>, <a href="/format/2310.16335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defense Against Model Extraction Attacks on Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sixiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hongzhi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongxu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+C">Cheng Long</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The robustness of recommender systems has become a prominent topic within the
research community. Numerous adversarial attacks have been proposed, but most
of them rely on extensive prior knowledge, such as all the white-box attacks or
most of the black-box attacks which assume that certain external knowledge is
available. Among these attacks, the model extraction attack stands out as a
promising and practical method, involving training a surrogate model by
repeatedly querying the target model. However, there is a significant gap in
the existing literature when it comes to defending against model extraction
attacks on recommender systems. In this paper, we introduce Gradient-based
Ranking Optimization (GRO), which is the first defense strategy designed to
counter such attacks. We formalize the defense as an optimization problem,
aiming to minimize the loss of the protected target model while maximizing the
loss of the attacker's surrogate model. Since top-k ranking lists are
non-differentiable, we transform them into swap matrices which are instead
differentiable. These swap matrices serve as input to a student model that
emulates the surrogate model's behavior. By back-propagating the loss of the
student model, we obtain gradients for the swap matrices. These gradients are
used to compute a swap loss, which maximizes the loss of the student model. We
conducted experiments on three benchmark datasets to evaluate the performance
of GRO, and the results demonstrate its superior effectiveness in defending
against model extraction attacks.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16336" title="Abstract">arXiv:2310.16336</a> [<a href="/pdf/2310.16336" title="Download PDF">pdf</a>, <a href="/format/2310.16336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMURF-THP: Score Matching-based UnceRtainty quantiFication for  Transformer Hawkes Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zichong Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yanbo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+S">Simiao Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haoming Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tuo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+H">Hongyuan Zha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Transformer Hawkes process models have shown to be successful in modeling
event sequence data. However, most of the existing training methods rely on
maximizing the likelihood of event sequences, which involves calculating some
intractable integral. Moreover, the existing methods fail to provide
uncertainty quantification for model predictions, e.g., confidence intervals
for the predicted event's arrival time. To address these issues, we propose
SMURF-THP, a score-based method for learning Transformer Hawkes process and
quantifying prediction uncertainty. Specifically, SMURF-THP learns the score
function of events' arrival time based on a score-matching objective that
avoids the intractable computation. With such a learned score function, we can
sample arrival time of events from the predictive distribution. This naturally
allows for the quantification of uncertainty by computing confidence intervals
over the generated samples. We conduct extensive experiments in both event type
prediction and uncertainty quantification of arrival time. In all the
experiments, SMURF-THP outperforms existing likelihood-based methods in
confidence calibration while exhibiting comparable prediction accuracy.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16340" title="Abstract">arXiv:2310.16340</a> [<a href="/pdf/2310.16340" title="Download PDF">pdf</a>, <a href="/format/2310.16340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RCAgent: Cloud Root Cause Analysis by Autonomous Agents with  Tool-Augmented Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zefan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zichuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yingying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+A">Aoxiao Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Lunting Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lingfei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language model (LLM) applications in cloud root cause analysis (RCA)
have been actively explored recently. However, current methods are still
reliant on manual workflow settings and do not unleash LLMs' decision-making
and environment interaction capabilities. We present RCAgent, a tool-augmented
LLM autonomous agent framework for practical and privacy-aware industrial RCA
usage. Running on an internally deployed model rather than GPT families,
RCAgent is capable of free-form data collection and comprehensive analysis with
tools. Our framework combines a variety of enhancements, including a unique
Self-Consistency for action trajectories, and a suite of methods for context
management, stabilization, and importing domain knowledge. Our experiments show
RCAgent's evident and consistent superiority over ReAct across all aspects of
RCA -- predicting root causes, solutions, evidence, and responsibilities -- and
tasks covered or uncovered by current rules, as validated by both automated
metrics and human evaluations. Furthermore, RCAgent has already been integrated
into the diagnosis and issue discovery workflow of the Real-time Compute
Platform for Apache Flink of Alibaba Cloud.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16343" title="Abstract">arXiv:2310.16343</a> [<a href="/pdf/2310.16343" title="Download PDF">pdf</a>, <a href="/format/2310.16343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Evaluation of Constrained Text Generation for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiaojun Wan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Advancements in natural language generation (NLG) and large language models
(LLMs) have led to proficient text generation in various tasks. However,
integrating intricate constraints into neural text generation, due to LLMs'
opacity, remains challenging. This study investigates constrained text
generation for LLMs, where predefined constraints are applied during LLM's
generation process. Our research examines multiple LLMs, including ChatGPT and
GPT-4, categorizing constraints into lexical, structural, and relation-based
types. We also present various benchmarks to facilitate fair evaluation. The
study addresses some key research questions, including the extent of LLMs'
compliance with constraints. Results illuminate LLMs' capacity and deficiency
to incorporate constraints and provide insights for future developments in
constrained text generation. Codes and datasets will be released upon
acceptance.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16344" title="Abstract">arXiv:2310.16344</a> [<a href="/pdf/2310.16344" title="Download PDF">pdf</a>, <a href="/format/2310.16344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Baby PIH: Parameterized Inapproximability of Min CSP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guruswami%2C+V">Venkatesan Guruswami</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xuandi Ren</a>, 
<a href="/search/cs?searchtype=author&query=Sandeep%2C+S">Sai Sandeep</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">The Parameterized Inapproximability Hypothesis (PIH) is the analog of the PCP
theorem in the world of parameterized complexity. It asserts that no FPT
algorithm can distinguish a satisfiable 2CSP instance from one which is only
$(1-\varepsilon)$-satisfiable (where the parameter is the number of variables)
for some constant $0&lt;\varepsilon&lt;1$.
<br />We consider a minimization version of CSPs (Min-CSP), where one may assign
$r$ values to each variable, and the goal is to ensure that every constraint is
satisfied by some choice among the $r \times r$ pairs of values assigned to its
variables (call such a CSP instance $r$-list-satisfiable). We prove the
following strong parameterized inapproximability for Min CSP: For every $r \ge
1$, it is W[1]-hard to tell if a 2CSP instance is satisfiable or is not even
$r$-list-satisfiable. We refer to this statement as "Baby PIH", following the
recently proved Baby PCP Theorem (Barto and Kozik, 2021). Our proof adapts the
combinatorial arguments underlying the Baby PCP theorem, overcoming some basic
obstacles that arise in the parameterized setting. Furthermore, our reduction
runs in time polynomially bounded in both the number of variables and the
alphabet size, and thus implies the Baby PCP theorem as well.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16349" title="Abstract">arXiv:2310.16349</a> [<a href="/pdf/2310.16349" title="Download PDF">pdf</a>, <a href="/format/2310.16349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffRef3D: A Diffusion-based Proposal Refinement Framework for 3D Object  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Se-Ho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Koo%2C+I">Inyong Koo</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+I">Inyoung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+B">Byeongjun Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C">Changick Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Denoising diffusion models show remarkable performances in generative tasks,
and their potential applications in perception tasks are gaining interest. In
this paper, we introduce a novel framework named DiffRef3D which adopts the
diffusion process on 3D object detection with point clouds for the first time.
Specifically, we formulate the proposal refinement stage of two-stage 3D object
detectors as a conditional diffusion process. During training, DiffRef3D
gradually adds noise to the residuals between proposals and target objects,
then applies the noisy residuals to proposals to generate hypotheses. The
refinement module utilizes these hypotheses to denoise the noisy residuals and
generate accurate box predictions. In the inference phase, DiffRef3D generates
initial hypotheses by sampling noise from a Gaussian distribution as residuals
and refines the hypotheses through iterative steps. DiffRef3D is a versatile
proposal refinement framework that consistently improves the performance of
existing 3D object detection models. We demonstrate the significance of
DiffRef3D through extensive experiments on the KITTI benchmark. Code will be
available.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16350" title="Abstract">arXiv:2310.16350</a> [<a href="/pdf/2310.16350" title="Download PDF">pdf</a>, <a href="/format/2310.16350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unraveling Feature Extraction Mechanisms in Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaobing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wei Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The underlying mechanism of neural networks in capturing precise knowledge
has been the subject of consistent research efforts. In this work, we propose a
theoretical approach based on Neural Tangent Kernels (NTKs) to investigate such
mechanisms. Specifically, considering the infinite network width, we
hypothesize the learning dynamics of target models may intuitively unravel the
features they acquire from training data, deepening our insights into their
internal mechanisms. We apply our approach to several fundamental models and
reveal how these models leverage statistical features during gradient descent
and how they are integrated into final decisions. We also discovered that the
choice of activation function can affect feature extraction. For instance, the
use of the \textit{ReLU} activation function could potentially introduce a bias
in features, providing a plausible explanation for its replacement with
alternative functions in recent pre-trained language models. Additionally, we
find that while self-attention and CNN models may exhibit limitations in
learning n-grams, multiplication-based models seem to excel in this area. We
verify these theoretical findings through experiments and find that they can be
applied to analyze language modeling tasks, which can be regarded as a special
variant of classification. Our contributions offer insights into the roles and
capacities of fundamental components within large language models, thereby
aiding the broader understanding of these complex systems.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16351" title="Abstract">arXiv:2310.16351</a> [<a href="/pdf/2310.16351" title="Download PDF">pdf</a>, <a href="/format/2310.16351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Algorithms for Separable Linear Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+S">Sally Dong</a>, 
<a href="/search/cs?searchtype=author&query=Goranci%2C+G">Gramoz Goranci</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lawrence Li</a>, 
<a href="/search/cs?searchtype=author&query=Sachdeva%2C+S">Sushant Sachdeva</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+G">Guanghao Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 55 pages. To appear at SODA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In numerical linear algebra, considerable effort has been devoted to
obtaining faster algorithms for linear systems whose underlying matrices
exhibit structural properties. A prominent success story is the method of
generalized nested dissection~[Lipton-Rose-Tarjan'79] for separable matrices.
On the other hand, the majority of recent developments in the design of
efficient linear program (LP) solves do not leverage the ideas underlying these
faster linear system solvers nor consider the separable structure of the
constraint matrix.
<br />We give a faster algorithm for separable linear programs. Specifically, we
consider LPs of the form $\min_{\mathbf{A}\mathbf{x}=\mathbf{b},
\mathbf{l}\leq\mathbf{x}\leq\mathbf{u}} \mathbf{c}^\top\mathbf{x}$, where the
graphical support of the constraint matrix $\mathbf{A} \in \mathbb{R}^{n\times
m}$ is $O(n^\alpha)$-separable. These include flow problems on planar graphs
and low treewidth matrices among others. We present an $\tilde{O}((m+m^{1/2 +
2\alpha}) \log(1/\epsilon))$ time algorithm for these LPs, where $\epsilon$ is
the relative accuracy of the solution.
<br />Our new solver has two important implications: for the $k$-multicommodity
flow problem on planar graphs, we obtain an algorithm running in
$\tilde{O}(k^{5/2} m^{3/2})$ time in the high accuracy regime; and when the
support of $\mathbf{A}$ is $O(n^\alpha)$-separable with $\alpha \leq 1/4$, our
algorithm runs in $\tilde{O}(m)$ time, which is nearly optimal. The latter
significantly improves upon the natural approach of combining interior point
methods and nested dissection, whose time complexity is lower bounded by
$\Omega(\sqrt{m}(m+m^{\alpha\omega}))=\Omega(m^{3/2})$, where $\omega$ is the
matrix multiplication constant. Lastly, in the setting of low-treewidth LPs, we
recover the results of [DLY,STOC21] and [GS,22] with significantly simpler data
structure machinery.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16354" title="Abstract">arXiv:2310.16354</a> [<a href="/pdf/2310.16354" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RAMPART: RowHammer Mitigation and Repair for Server Memory Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Woo%2C+S+C">Steven C. Woo</a>, 
<a href="/search/cs?searchtype=author&query=Elsasser%2C+W">Wendy Elsasser</a>, 
<a href="/search/cs?searchtype=author&query=Hamburg%2C+M">Mike Hamburg</a>, 
<a href="/search/cs?searchtype=author&query=Linstadt%2C+E">Eric Linstadt</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+M+R">Michael R. Miller</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+T">Taeksang Song</a>, 
<a href="/search/cs?searchtype=author&query=Tringali%2C+J">James Tringali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 13 figures. A version of this paper will appear in the Proceedings of MEMSYS23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">RowHammer attacks are a growing security and reliability concern for DRAMs
and computer systems as they can induce many bit errors that overwhelm error
detection and correction capabilities. System-level solutions are needed as
process technology and circuit improvements alone are unlikely to provide
complete protection against RowHammer attacks in the future. This paper
introduces RAMPART, a novel approach to mitigating RowHammer attacks and
improving server memory system reliability by remapping addresses in each DRAM
in a way that confines RowHammer bit flips to a single device for any victim
row address. When RAMPART is paired with Single Device Data Correction (SDDC)
and patrol scrub, error detection and correction methods in use today, the
system can detect and correct bit flips from a successful attack, allowing the
memory system to heal itself. RAMPART is compatible with DDR5 RowHammer
mitigation features, as well as a wide variety of algorithmic and probabilistic
tracking methods. We also introduce BRC-VL, a variation of DDR5 Bounded Refresh
Configuration (BRC) that improves system performance by reducing mitigation
overhead and show that it works well with probabilistic sampling methods to
combat traditional and victim-focused mitigation attacks like Half-Double. The
combination of RAMPART, SDDC, and scrubbing enables stronger RowHammer
resistance by correcting bit flips from one successful attack. Uncorrectable
errors are much less likely, requiring two successful attacks before the memory
system is scrubbed.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16355" title="Abstract">arXiv:2310.16355</a> [<a href="/pdf/2310.16355" title="Download PDF">pdf</a>, <a href="/format/2310.16355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Redco: A Lightweight Tool to Automate Distributed Training of LLMs on  Any GPU/TPUs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+B">Bowen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lijuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yonghao Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jindong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E">Eric Xing</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhiting Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Released under Apache License 2.0 at <a href="https://github.com/tanyuqian/redco">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The recent progress of AI can be largely attributed to large language models
(LLMs). However, their escalating memory requirements introduce challenges for
machine learning (ML) researchers and engineers. Addressing this requires
developers to partition a large model to distribute it across multiple GPUs or
TPUs. This necessitates considerable coding and intricate configuration efforts
with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa.
These tools require users' expertise in machine learning systems (MLSys),
creating a bottleneck in LLM development, particularly for developers without
MLSys background. In this work, we present Redco, a lightweight and
user-friendly tool crafted to automate distributed training and inference for
LLMs, as well as to simplify ML pipeline development. The design of Redco
emphasizes two key aspects. Firstly, to automate model parallism, our study
identifies two straightforward rules to generate tensor parallel strategies for
any given LLM. Integrating these rules into Redco facilitates effortless
distributed LLM training and inference, eliminating the need of additional
coding or complex configurations. We demonstrate the effectiveness by applying
Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to
the size of 66B. Secondly, we propose a mechanism that allows for the
customization of diverse ML pipelines through the definition of merely three
functions, eliminating redundant and formulaic code like multi-host related
processing. This mechanism proves adaptable across a spectrum of ML algorithms,
from foundational language modeling to complex algorithms like meta-learning
and reinforcement learning. Consequently, Redco implementations exhibit much
fewer code lines compared to their official counterparts.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16356" title="Abstract">arXiv:2310.16356</a> [<a href="/pdf/2310.16356" title="Download PDF">pdf</a>, <a href="/format/2310.16356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-Modal Multilingual Benchmark for Document Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fujinuma%2C+Y">Yoshinari Fujinuma</a>, 
<a href="/search/cs?searchtype=author&query=Varia%2C+S">Siddharth Varia</a>, 
<a href="/search/cs?searchtype=author&query=Sankaran%2C+N">Nishant Sankaran</a>, 
<a href="/search/cs?searchtype=author&query=Appalaraju%2C+S">Srikar Appalaraju</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+B">Bonan Min</a>, 
<a href="/search/cs?searchtype=author&query=Vyas%2C+Y">Yogarshi Vyas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Document image classification is different from plain-text document
classification and consists of classifying a document by understanding the
content and structure of documents such as forms, emails, and other such
documents. We show that the only existing dataset for this task (Lewis et al.,
2006) has several limitations and we introduce two newly curated multilingual
datasets WIKI-DOC and MULTIEURLEX-DOC that overcome these limitations. We
further undertake a comprehensive study of popular visually-rich document
understanding or Document AI models in previously untested setting in document
image classification such as 1) multi-label classification, and 2) zero-shot
cross-lingual transfer setup. Experimental results show limitations of
multilingual Document AI models on cross-lingual transfer across typologically
distant languages. Our datasets and findings open the door for future research
into improving Document AI models.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16358" title="Abstract">arXiv:2310.16358</a> [<a href="/pdf/2310.16358" title="Download PDF">pdf</a>, <a href="/format/2310.16358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Simple to Complex: A Progressive Framework for Document-level  Informative Argument Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Quzhe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanxi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dongyan Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Findings of EMNLP 2023 (Long Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Document-level Event Argument Extraction (EAE) requires the model to extract
arguments of multiple events from a single document. Considering the underlying
dependencies between these events, recent efforts leverage the idea of
"memory", where the results of already predicted events are cached and can be
retrieved to help the prediction of upcoming events. These methods extract
events according to their appearance order in the document, however, the event
that appears in the first sentence does not mean that it is the easiest to
extract. Existing methods might introduce noise to the extraction of upcoming
events if they rely on an incorrect prediction of previous events. In order to
provide more reliable memory, we propose a simple-to-complex progressive
framework for document-level EAE. Specifically, we first calculate the
difficulty of each event and then, we conduct the extraction following a
simple-to-complex order. In this way, the memory will store the most certain
results, and the model could use these reliable sources to help the prediction
of more difficult events. Experiments on WikiEvents show that our model
outperforms SOTA by 1.4% in F1, indicating the proposed simple-to-complex
framework is useful in the EAE task.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16360" title="Abstract">arXiv:2310.16360</a> [<a href="/pdf/2310.16360" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Review of AI-enabled Unmanned Aerial Vehicle: Trends,  Vision , and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pal%2C+O+K">Osim Kumar Pal</a>, 
<a href="/search/cs?searchtype=author&query=Shovon%2C+M+S+H">Md Sakib Hossain Shovon</a>, 
<a href="/search/cs?searchtype=author&query=Mridha%2C+M+F">M. F. Mridha</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jungpil Shin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">In recent years, the combination of artificial intelligence (AI) and unmanned
aerial vehicles (UAVs) has brought about advancements in various areas. This
comprehensive analysis explores the changing landscape of AI-powered UAVs and
friendly computing in their applications. It covers emerging trends, futuristic
visions, and the inherent challenges that come with this relationship. The
study examines how AI plays a role in enabling navigation, detecting and
tracking objects, monitoring wildlife, enhancing precision agriculture,
facilitating rescue operations, conducting surveillance activities, and
establishing communication among UAVs using environmentally conscious computing
techniques. By delving into the interaction between AI and UAVs, this analysis
highlights the potential for these technologies to revolutionise industries
such as agriculture, surveillance practices, disaster management strategies,
and more. While envisioning possibilities, it also takes a look at ethical
considerations, safety concerns, regulatory frameworks to be established, and
the responsible deployment of AI-enhanced UAV systems. By consolidating
insights from research endeavours in this field, this review provides an
understanding of the evolving landscape of AI-powered UAVs while setting the
stage for further exploration in this transformative domain.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16361" title="Abstract">arXiv:2310.16361</a> [<a href="/pdf/2310.16361" title="Download PDF">pdf</a>, <a href="/format/2310.16361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstructPTS: Instruction-Tuning LLMs for Product Title Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fetahu%2C+B">Besnik Fetahu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhiyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Rokhlenko%2C+O">Oleg Rokhlenko</a>, 
<a href="/search/cs?searchtype=author&query=Malmasi%2C+S">Shervin Malmasi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 (Industry Track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">E-commerce product catalogs contain billions of items. Most products have
lengthy titles, as sellers pack them with product attributes to improve
retrieval, and highlight key product aspects. This results in a gap between
such unnatural products titles, and how customers refer to them. It also limits
how e-commerce stores can use these seller-provided titles for recommendation,
QA, or review summarization.
<br />Inspired by recent work on instruction-tuned LLMs, we present InstructPTS, a
controllable approach for the task of Product Title Summarization (PTS).
Trained using a novel instruction fine-tuning strategy, our approach is able to
summarize product titles according to various criteria (e.g. number of words in
a summary, inclusion of specific phrases, etc.). Extensive evaluation on a
real-world e-commerce catalog shows that compared to simple fine-tuning of
LLMs, our proposed approach can generate more accurate product name summaries,
with an improvement of over 14 and 8 BLEU and ROUGE points, respectively.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16362" title="Abstract">arXiv:2310.16362</a> [<a href="/pdf/2310.16362" title="Download PDF">pdf</a>, <a href="/format/2310.16362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Potential Field for Obstacle-Aware Local Motion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alhaddad%2C+M">Muhammad Alhaddad</a>, 
<a href="/search/cs?searchtype=author&query=Mironov%2C+K">Konstantin Mironov</a>, 
<a href="/search/cs?searchtype=author&query=Staroverov%2C+A">Aleksey Staroverov</a>, 
<a href="/search/cs?searchtype=author&query=Panov%2C+A">Aleksandr Panov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Model predictive control (MPC) may provide local motion planning for mobile
robotic platforms. The challenging aspect is the analytic representation of
collision cost for the case when both the obstacle map and robot footprint are
arbitrary. We propose a Neural Potential Field: a neural network model that
returns a differentiable collision cost based on robot pose, obstacle map, and
robot footprint. The differentiability of our model allows its usage within the
MPC solver. It is computationally hard to solve problems with a very high
number of parameters. Therefore, our architecture includes neural image
encoders, which transform obstacle maps and robot footprints into embeddings,
which reduce problem dimensionality by two orders of magnitude. The reference
data for network training are generated based on algorithmic calculation of a
signed distance function. Comparative experiments showed that the proposed
approach is comparable with existing local planners: it provides trajectories
with outperforming smoothness, comparable path length, and safe distance from
obstacles. Experiment on Husky UGV mobile robot showed that our approach allows
real-time and safe local planning. The code for our approach is presented at
https://github.com/cog-isa/NPField together with demo video.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16363" title="Abstract">arXiv:2310.16363</a> [<a href="/pdf/2310.16363" title="Download PDF">pdf</a>, <a href="/format/2310.16363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite Time Analysis of Constrained Actor Critic and Constrained Natural  Actor Critic Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panda%2C+P">Prashansa Panda</a>, 
<a href="/search/cs?searchtype=author&query=Bhatnagar%2C+S">Shalabh Bhatnagar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Actor Critic methods have found immense applications on a wide range of
Reinforcement Learning tasks especially when the state-action space is large.
In this paper, we consider actor critic and natural actor critic algorithms
with function approximation for constrained Markov decision processes (C-MDP)
involving inequality constraints and carry out a non-asymptotic analysis for
both of these algorithms in a non-i.i.d (Markovian) setting. We consider the
long-run average cost criterion where both the objective and the constraint
functions are suitable policy-dependent long-run averages of certain prescribed
cost functions. We handle the inequality constraints using the Lagrange
multiplier method. We prove that these algorithms are guaranteed to find a
first-order stationary point (i.e., $\Vert \nabla L(\theta,\gamma)\Vert_2^2
\leq \epsilon$) of the performance (Lagrange) function $L(\theta,\gamma)$, with
a sample complexity of $\mathcal{\tilde{O}}(\epsilon^{-2.5})$ in the case of
both Constrained Actor Critic (C-AC) and Constrained Natural Actor Critic
(C-NAC) algorithms.We also show the results of experiments on a few different
grid world settings and observe good empirical performance using both of these
algorithms. In particular, for large grid sizes, Constrained Natural Actor
Critic shows slightly better results than Constrained Actor Critic while the
latter is slightly better for a small grid size.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16364" title="Abstract">arXiv:2310.16364</a> [<a href="/pdf/2310.16364" title="Download PDF">pdf</a>, <a href="/format/2310.16364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Large-scale Masked Face Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Manyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+B">Bingqi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+G">Guanglu Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongsheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> the top1 solution for ICCV2021-MFR challenge
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">During the COVID-19 coronavirus epidemic, almost everyone is wearing masks,
which poses a huge challenge for deep learning-based face recognition
algorithms. In this paper, we will present our \textbf{championship} solutions
in ICCV MFR WebFace260M and InsightFace unconstrained tracks. We will focus on
four challenges in large-scale masked face recognition, i.e., super-large scale
training, data noise handling, masked and non-masked face recognition accuracy
balancing, and how to design inference-friendly model architecture. We hope
that the discussion on these four aspects can guide future research towards
more robust masked face recognition systems.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16368" title="Abstract">arXiv:2310.16368</a> [<a href="/pdf/2310.16368" title="Download PDF">pdf</a>, <a href="/format/2310.16368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-based Live Update Generation for Soccer Matches from  Microblog Posts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oshika%2C+M">Masashi Oshika</a>, 
<a href="/search/cs?searchtype=author&query=Yamada%2C+K">Kosuke Yamada</a>, 
<a href="/search/cs?searchtype=author&query=Sasano%2C+R">Ryohei Sasano</a>, 
<a href="/search/cs?searchtype=author&query=Takeda%2C+K">Koichi Takeda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">It has been known to be difficult to generate adequate sports updates from a
sequence of vast amounts of diverse live tweets, although the live sports
viewing experience with tweets is gaining the popularity. In this paper, we
focus on soccer matches and work on building a system to generate live updates
for soccer matches from tweets so that users can instantly grasp a match's
progress and enjoy the excitement of the match from raw tweets. Our proposed
system is based on a large pre-trained language model and incorporates a
mechanism to control the number of updates and a mechanism to reduce the
redundancy of duplicate and similar updates.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16370" title="Abstract">arXiv:2310.16370</a> [<a href="/pdf/2310.16370" title="Download PDF">pdf</a>, <a href="/format/2310.16370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PartRePer-MPI: Combining Fault Tolerance and Performance for MPI  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joshi%2C+S">Sarthak Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Vadhiyar%2C+S">Sathish Vadhiyar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">As we have entered Exascale computing, the faults in high-performance systems
are expected to increase considerably. To compensate for a higher failure rate,
the standard checkpoint/restart technique would need to create checkpoints at a
much higher frequency resulting in an excessive amount of overhead which would
not be sustainable for many scientific applications. Replication allows for
fast recovery from failures by simply dropping the failed processes and using
their replicas to continue the regular operation of the application.
<br />In this paper, we have implemented PartRePer-MPI, a novel fault-tolerant MPI
library that adopts partial replication of some of the launched MPI processes
in order to provide resilience from failures. The novelty of our work is that
it combines both fault tolerance, due to the use of the User Level Failure
Mitigation (ULFM) framework in the Open MPI library, and high performance, due
to the use of communication protocols in the native MPI library that is
generally fine-tuned for specific HPC platforms. We have implemented efficient
and parallel communication strategies with computational and replica processes,
and our library can seamlessly provide fault tolerance support to an existing
MPI application. Our experiments using seven NAS Parallel Benchmarks and two
scientific applications show that the failure-free overheads in PartRePer-MPI
when compared to the baseline MVAPICH2, are only up to 6.4% for the NAS
parallel benchmarks and up to 9.7% for the scientific applications.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16371" title="Abstract">arXiv:2310.16371</a> [<a href="/pdf/2310.16371" title="Download PDF">pdf</a>, <a href="/format/2310.16371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synergizing Airborne Non-Terrestrial Networks and Reconfigurable  Intelligent Surfaces-Aided 6G IoT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamshed%2C+M+A">Muhammad Ali Jamshed</a>, 
<a href="/search/cs?searchtype=author&query=Kaushik%2C+A">Aryan Kaushik</a>, 
<a href="/search/cs?searchtype=author&query=Toka%2C+M">Mesut Toka</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+W">Wonjae Shin</a>, 
<a href="/search/cs?searchtype=author&query=Shakir%2C+M+Z">Muhammad Zeeshan Shakir</a>, 
<a href="/search/cs?searchtype=author&query=Dash%2C+S+P">Soumya P. Dash</a>, 
<a href="/search/cs?searchtype=author&query=Dardari%2C+D">Davide Dardari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">On the one hand, Reconfigurable Intelligent Surfaces (RISs) emerge as a
promising solution to meet the demand for higher data rates, improved coverage,
and efficient spectrum utilization. On the other hand, Non-Terrestrial Networks
(NTNs) offer unprecedented possibilities for global connectivity. Moreover, the
NTN can also support the upsurge in the number of Internet of Things (IoT)
devices by providing reliable and ubiquitous connectivity. Although NTNs have
shown promising results, there are several challenges associated with their
usage, such as signal propagation delays, interference, security, etc. In this
article, we have discussed the possibilities of integrating RIS with an NTN
platform to overcome the issues associated with NTN. Furthermore, through
experimental validation, we have demonstrated that the RIS-assisted NTN can
play a pivotal role in improving the performance of the entire communication
system.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16374" title="Abstract">arXiv:2310.16374</a> [<a href="/pdf/2310.16374" title="Download PDF">pdf</a>, <a href="/format/2310.16374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Distributional Learning via Cramer-Wold Distance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+S">Seunghwan An</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+J">Jong-June Jeon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The assumption of conditional independence among observed variables,
primarily used in the Variational Autoencoder (VAE) decoder modeling, has
limitations when dealing with high-dimensional datasets or complex correlation
structures among observed variables. To address this issue, we introduced the
Cramer-Wold distance regularization, which can be computed in a closed-form, to
facilitate joint distributional learning for high-dimensional datasets.
Additionally, we introduced a two-step learning method to enable flexible prior
modeling and improve the alignment between the aggregated posterior and the
prior distribution. Furthermore, we provide theoretical distinctions from
existing methods within this category. To evaluate the synthetic data
generation performance of our proposed approach, we conducted experiments on
high-dimensional datasets with multiple categorical variables. Given that many
readily available datasets and data science applications involve such datasets,
our experiments demonstrate the effectiveness of our proposed methodology.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16375" title="Abstract">arXiv:2310.16375</a> [<a href="/pdf/2310.16375" title="Download PDF">pdf</a>, <a href="/format/2310.16375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DyExplainer: Explainable Dynamic Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianchun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+D">Dongsheng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Neural Networks (GNNs) resurge as a trending research subject owing to
their impressive ability to capture representations from graph-structured data.
However, the black-box nature of GNNs presents a significant challenge in terms
of comprehending and trusting these models, thereby limiting their practical
applications in mission-critical scenarios. Although there has been substantial
progress in the field of explaining GNNs in recent years, the majority of these
studies are centered on static graphs, leaving the explanation of dynamic GNNs
largely unexplored. Dynamic GNNs, with their ever-evolving graph structures,
pose a unique challenge and require additional efforts to effectively capture
temporal dependencies and structural relationships. To address this challenge,
we present DyExplainer, a novel approach to explaining dynamic GNNs on the fly.
DyExplainer trains a dynamic GNN backbone to extract representations of the
graph at each snapshot, while simultaneously exploring structural relationships
and temporal dependencies through a sparse attention technique. To preserve the
desired properties of the explanation, such as structural consistency and
temporal continuity, we augment our approach with contrastive learning
techniques to provide priori-guided regularization. To model longer-term
temporal dependencies, we develop a buffer-based live-updating scheme for
training. The results of our extensive experiments on various datasets
demonstrate the superiority of DyExplainer, not only providing faithful
explainability of the model predictions but also significantly improving the
model prediction accuracy, as evidenced in the link prediction task.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16376" title="Abstract">arXiv:2310.16376</a> [<a href="/pdf/2310.16376" title="Download PDF">pdf</a>, <a href="/format/2310.16376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GADY: Unsupervised Anomaly Detection on Dynamic Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lou%2C+S">Shiqi Lou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingyue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shujie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuyang Tian</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhaoxuan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+M">Minnan Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Anomaly detection on dynamic graphs refers to detecting entities whose
behaviors obviously deviate from the norms observed within graphs and their
temporal information. This field has drawn increasing attention due to its
application in finance, network security, social networks, and more. However,
existing methods face two challenges: dynamic structure constructing challenge
- difficulties in capturing graph structure with complex time information and
negative sampling challenge - unable to construct excellent negative samples
for unsupervised learning. To address these challenges, we propose Unsupervised
Generative Anomaly Detection on Dynamic Graphs (GADY). To tackle the first
challenge, we propose a continuous dynamic graph model to capture the
fine-grained information, which breaks the limit of existing discrete methods.
Specifically, we employ a message-passing framework combined with positional
features to get edge embeddings, which are decoded to identify anomalies. For
the second challenge, we pioneer the use of Generative Adversarial Networks to
generate negative interactions. Moreover, we design a loss function to alter
the training goal of the generator while ensuring the diversity and quality of
generated samples. Extensive experiments demonstrate that our proposed GADY
significantly outperforms the previous state-of-the-art method on three
real-world datasets. Supplementary experiments further validate the
effectiveness of our model design and the necessity of each module.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16377" title="Abstract">arXiv:2310.16377</a> [<a href="/pdf/2310.16377" title="Download PDF">pdf</a>, <a href="/format/2310.16377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear steering control under input magnitude and rate constraints  with exponential convergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Suyama%2C+R">Rin Suyama</a>, 
<a href="/search/eess?searchtype=author&query=Satoh%2C+S">Satoshi Satoh</a>, 
<a href="/search/eess?searchtype=author&query=Maki%2C+A">Atsuo Maki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures, a preprint submitted to the Journal of Marine Science and Technology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">A ship steering control is designed for a nonlinear maneuvering model whose
rudder manipulation is constrained in both magnitude and rate. In our method,
the tracking problem of the target heading angle with input constraints is
converted into the tracking problem for a strict-feedback system without any
input constraints. To derive this system, hyperbolic tangent ($\tanh$) function
and auxiliary variables are introduced to deal with the input constraints.
Furthermore, using the feature of the derivative of $\tanh$ function, auxiliary
systems are successfully derived in the strict-feedback form. The backstepping
method is utilized to construct the feedback control law for the resulting
cascade system. The proposed steering control is verified in numerical
experiments, and the result shows that the tracking of the target heading angle
is successful using the proposed control law.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16379" title="Abstract">arXiv:2310.16379</a> [<a href="/pdf/2310.16379" title="Download PDF">pdf</a>, <a href="/format/2310.16379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating General-Purpose AI with Psychometrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Liming Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Hernandez-Orallo%2C+J">Jose Hernandez-Orallo</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Luning Sun</a>, 
<a href="/search/cs?searchtype=author&query=Stillwell%2C+D">David Stillwell</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Fang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Artificial intelligence (AI) has witnessed an evolution from task-specific to
general-purpose systems that trend toward human versatility. As AI systems
begin to play pivotal roles in society, it is important to ensure that they are
adequately evaluated. Current AI benchmarks typically assess performance on
collections of specific tasks. This has drawbacks when used for assessing
general-purpose AI systems. First, it is difficult to predict whether AI
systems could complete a new task it has never seen or that did not previously
exist. Second, these benchmarks often focus on overall performance metrics,
potentially overlooking the finer details crucial for making informed
decisions. Lastly, there are growing concerns about the reliability of existing
benchmarks and questions about what is being measured. To solve these
challenges, this paper suggests that psychometrics, the science of
psychological measurement, should be placed at the core of evaluating
general-purpose AI. Psychometrics provides a rigorous methodology for
identifying and measuring the latent constructs that underlie performance
across multiple tasks. We discuss its merits, warn against potential pitfalls,
and propose a framework for putting it into practice. Finally, we explore
future opportunities to integrate psychometrics with AI.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16380" title="Abstract">arXiv:2310.16380</a> [<a href="/pdf/2310.16380" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A model for multi-attack classification to improve intrusion detection  performance using deep learning approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silivery%2C+A+K">Arun Kumar Silivery</a>, 
<a href="/search/cs?searchtype=author&query=Kovvur%2C+R+M+R">Ram Mohan Rao Kovvur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This proposed model introduces novel deep learning methodologies. The
objective here is to create a reliable intrusion detection mechanism to help
identify malicious attacks. Deep learning based solution framework is developed
consisting of three approaches. The first approach is Long-Short Term Memory
Recurrent Neural Network (LSTM-RNN) with seven optimizer functions such as
adamax, SGD, adagrad, adam, RMSprop, nadam and adadelta. The model is evaluated
on NSL-KDD dataset and classified multi attack classification. The model has
outperformed with adamax optimizer in terms of accuracy, detection rate and low
false alarm rate. The results of LSTM-RNN with adamax optimizer is compared
with existing shallow machine and deep learning models in terms of accuracy,
detection rate and low false alarm rate. The multi model methodology consisting
of Recurrent Neural Network (RNN), Long-Short Term Memory Recurrent Neural
Network (LSTM-RNN), and Deep Neural Network (DNN). The multi models are
evaluated on bench mark datasets such as KDD99, NSL-KDD, and UNSWNB15 datasets.
The models self-learnt the features and classifies the attack classes as
multi-attack classification. The models RNN, and LSTM-RNN provide considerable
performance compared to other existing methods on KDD99 and NSL-KDD dataset
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16383" title="Abstract">arXiv:2310.16383</a> [<a href="/pdf/2310.16383" title="Download PDF">pdf</a>, <a href="/format/2310.16383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-NeRF: Towards Open Vocabulary NeRF Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fang Li</a>, 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+N">Narendra Ahuja</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we address the challenge of decomposing Neural Radiance Fields
(NeRF) into objects from an open vocabulary, a critical task for object
manipulation in 3D reconstruction and view synthesis. Current techniques for
NeRF decomposition involve a trade-off between the flexibility of processing
open-vocabulary queries and the accuracy of 3D segmentation. We present,
Open-vocabulary Embedded Neural Radiance Fields (Open-NeRF), that leverage
large-scale, off-the-shelf, segmentation models like the Segment Anything Model
(SAM) and introduce an integrate-and-distill paradigm with hierarchical
embeddings to achieve both the flexibility of open-vocabulary querying and 3D
segmentation accuracy. Open-NeRF first utilizes large-scale foundation models
to generate hierarchical 2D mask proposals from varying viewpoints. These
proposals are then aligned via tracking approaches and integrated within the 3D
space and subsequently distilled into the 3D field. This process ensures
consistent recognition and granularity of objects from different viewpoints,
even in challenging scenarios involving occlusion and indistinct features. Our
experimental results show that the proposed Open-NeRF outperforms
state-of-the-art methods such as LERF \cite{lerf} and FFD \cite{ffd} in
open-vocabulary scenarios. Open-NeRF offers a promising solution to NeRF
decomposition, guided by open-vocabulary queries, enabling novel applications
in robotics and vision-language interaction in open-world 3D scenes.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16384" title="Abstract">arXiv:2310.16384</a> [<a href="/pdf/2310.16384" title="Download PDF">pdf</a>, <a href="/format/2310.16384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Uncertainty Quantification of Kernel Interpolation on  Spheres
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lin%2C+S">Shao-Bo Lin</a>, 
<a href="/search/math?searchtype=author&query=Sun%2C+X">Xingping Sun</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+D">Di Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages,6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG); Spectral Theory (math.SP)

</div>
<p class="mathjax">For radial basis function (RBF) kernel interpolation of scattered data,
Schaback in 1995 proved that the attainable approximation error and the
condition number of the underlying interpolation matrix cannot be made small
simultaneously. He referred to this finding as an "uncertainty relation", an
undesirable consequence of which is that RBF kernel interpolation is
susceptible to noisy data. In this paper, we propose and study a distributed
interpolation method to manage and quantify the uncertainty brought on by
interpolating noisy spherical data of non-negligible magnitude. We also present
numerical simulation results showing that our method is practical and robust in
terms of handling noisy data from challenging computing environments.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16388" title="Abstract">arXiv:2310.16388</a> [<a href="/pdf/2310.16388" title="Download PDF">pdf</a>, <a href="/format/2310.16388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deepfake Detection: Leveraging the Power of 2D and 3D CNN Ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bakliwal%2C+A">Aagam Bakliwal</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+A+D">Amit D. Joshi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the dynamic realm of deepfake detection, this work presents an innovative
approach to validate video content. The methodology blends advanced
2-dimensional and 3-dimensional Convolutional Neural Networks. The 3D model is
uniquely tailored to capture spatiotemporal features via sliding filters,
extending through both spatial and temporal dimensions. This configuration
enables nuanced pattern recognition in pixel arrangement and temporal evolution
across frames. Simultaneously, the 2D model leverages EfficientNet
architecture, harnessing auto-scaling in Convolutional Neural Networks.
Notably, this ensemble integrates Voting Ensembles and Adaptive Weighted
Ensembling. Strategic prioritization of the 3-dimensional model's output
capitalizes on its exceptional spatio-temporal feature extraction. Experimental
validation underscores the effectiveness of this strategy, showcasing its
potential in countering deepfake generation's deceptive practices.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16389" title="Abstract">arXiv:2310.16389</a> [<a href="/pdf/2310.16389" title="Download PDF">pdf</a>, <a href="/format/2310.16389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MVFAN: Multi-View Feature Assisted Network for 4D Radar Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Q">Qiao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yihan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 Pages, 7 figures, Accepted by ICONIP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">4D radar is recognized for its resilience and cost-effectiveness under
adverse weather conditions, thus playing a pivotal role in autonomous driving.
While cameras and LiDAR are typically the primary sensors used in perception
modules for autonomous vehicles, radar serves as a valuable supplementary
sensor. Unlike LiDAR and cameras, radar remains unimpaired by harsh weather
conditions, thereby offering a dependable alternative in challenging
environments. Developing radar-based 3D object detection not only augments the
competency of autonomous vehicles but also provides economic benefits. In
response, we propose the Multi-View Feature Assisted Network (\textit{MVFAN}),
an end-to-end, anchor-free, and single-stage framework for 4D-radar-based 3D
object detection for autonomous vehicles. We tackle the issue of insufficient
feature utilization by introducing a novel Position Map Generation module to
enhance feature learning by reweighing foreground and background points, and
their features, considering the irregular distribution of radar point clouds.
Additionally, we propose a pioneering backbone, the Radar Feature Assisted
backbone, explicitly crafted to fully exploit the valuable Doppler velocity and
reflectivity data provided by the 4D radar sensor. Comprehensive experiments
and ablation studies carried out on Astyx and VoD datasets attest to the
efficacy of our framework. The incorporation of Doppler velocity and RCS
reflectivity dramatically improves the detection performance for small moving
objects such as pedestrians and cyclists. Consequently, our approach culminates
in a highly optimized 4D-radar-based 3D object detection capability for
autonomous driving systems, setting a new standard in the field.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16390" title="Abstract">arXiv:2310.16390</a> [<a href="/pdf/2310.16390" title="Download PDF">pdf</a>, <a href="/format/2310.16390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Pre-trained Language Models for Repairing API Misuses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Ting Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Irsan%2C+I+C">Ivana Clairine Irsan</a>, 
<a href="/search/cs?searchtype=author&query=Thung%2C+F">Ferdian Thung</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+D">David Lo</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Asankhaya Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Lingxiao Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review by TOSEM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">API misuses often lead to software bugs, crashes, and vulnerabilities. While
several API misuse detectors have been proposed, there are no automatic repair
tools specifically designed for this purpose. In a recent study,
test-suite-based automatic program repair (APR) tools were found to be
ineffective in repairing API misuses. Still, since the study focused on
non-learning-aided APR tools, it remains unknown whether learning-aided APR
tools are capable of fixing API misuses. In recent years, pre-trained language
models (PLMs) have succeeded greatly in many natural language processing tasks.
There is a rising interest in applying PLMs to APR. However, there has not been
any study that investigates the effectiveness of PLMs in repairing API misuse.
<br />To fill this gap, we conduct a comprehensive empirical study on 11
learning-aided APR tools, which include 9 of the state-of-the-art
general-purpose PLMs and two APR tools. We evaluate these models with an
API-misuse repair dataset, consisting of two variants. Our results show that
PLMs perform better than the studied APR tools in repairing API misuses. Among
the 9 pre-trained models tested, CodeT5 is the best performer in the exact
match. We also offer insights and potential exploration directions for future
research.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16391" title="Abstract">arXiv:2310.16391</a> [<a href="/pdf/2310.16391" title="Download PDF">pdf</a>, <a href="/format/2310.16391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Winning Prize Comes from Losing Tickets: Improve Invariant Learning by  Exploring Variant Parameters for Out-of-Distribution Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhuo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Muyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+C">Chen Gong</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Out-of-Distribution (OOD) Generalization aims to learn robust models that
generalize well to various environments without fitting to
distribution-specific features. Recent studies based on Lottery Ticket
Hypothesis (LTH) address this problem by minimizing the learning target to find
some of the parameters that are critical to the task. However, in OOD problems,
such solutions are suboptimal as the learning task contains severe distribution
noises, which can mislead the optimization process. Therefore, apart from
finding the task-related parameters (i.e., invariant parameters), we propose
Exploring Variant parameters for Invariant Learning (EVIL) which also leverages
the distribution knowledge to find the parameters that are sensitive to
distribution shift (i.e., variant parameters). Once the variant parameters are
left out of invariant learning, a robust subnetwork that is resistant to
distribution shift can be found. Additionally, the parameters that are
relatively stable across distributions can be considered invariant ones to
improve invariant learning. By fully exploring both variant and invariant
parameters, our EVIL can effectively identify a robust subnetwork to improve
OOD generalization. In extensive experiments on integrated testbed: DomainBed,
EVIL can effectively and efficiently enhance many popular methods, such as ERM,
IRM, SAM, etc.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16393" title="Abstract">arXiv:2310.16393</a> [<a href="/pdf/2310.16393" title="Download PDF">pdf</a>, <a href="/format/2310.16393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source  Ensembling of Language Adapters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rathore%2C+V">Vipul Rathore</a>, 
<a href="/search/cs?searchtype=author&query=Dhingra%2C+R">Rajdeep Dhingra</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+P">Parag Singla</a>, 
<a href="/search/cs?searchtype=author&query=Mausam">Mausam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We tackle the problem of zero-shot cross-lingual transfer in NLP tasks via
the use of language adapters (LAs). Most of the earlier works have explored
training with adapter of a single source (often English), and testing either
using the target LA or LA of another related language. Training target LA
requires unlabeled data, which may not be readily available for low resource
unseen languages: those that are neither seen by the underlying multilingual
language model (e.g., mBERT), nor do we have any (labeled or unlabeled) data
for them. We posit that for more effective cross-lingual transfer, instead of
just one source LA, we need to leverage LAs of multiple (linguistically or
geographically related) source languages, both at train and test-time - which
we investigate via our novel neural architecture, ZGUL. Extensive
experimentation across four language groups, covering 15 unseen target
languages, demonstrates improvements of up to 3.2 average F1 points over
standard fine-tuning and other strong baselines on POS tagging and NER tasks.
We also extend ZGUL to settings where either (1) some unlabeled data or (2)
few-shot training examples are available for the target language. We find that
ZGUL continues to outperform baselines in these settings too.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16397" title="Abstract">arXiv:2310.16397</a> [<a href="/pdf/2310.16397" title="Download PDF">pdf</a>, <a href="/format/2310.16397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Efficient Surrogate Dynamic Models with Graph Spline Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hua%2C+C">Chuanbo Hua</a>, 
<a href="/search/math?searchtype=author&query=Berto%2C+F">Federico Berto</a>, 
<a href="/search/math?searchtype=author&query=Poli%2C+M">Michael Poli</a>, 
<a href="/search/math?searchtype=author&query=Massaroli%2C+S">Stefano Massaroli</a>, 
<a href="/search/math?searchtype=author&query=Park%2C+J">Jinkyoo Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">While complex simulations of physical systems have been widely used in
engineering and scientific computing, lowering their often prohibitive
computational requirements has only recently been tackled by deep learning
approaches. In this paper, we present GraphSplineNets, a novel deep-learning
method to speed up the forecasting of physical systems by reducing the grid
size and number of iteration steps of deep surrogate models. Our method uses
two differentiable orthogonal spline collocation methods to efficiently predict
response at any location in time and space. Additionally, we introduce an
adaptive collocation strategy in space to prioritize sampling from the most
important regions. GraphSplineNets improve the accuracy-speedup tradeoff in
forecasting various dynamical systems with increasing complexity, including the
heat equation, damped wave propagation, Navier-Stokes equations, and real-world
ocean currents in both regular and irregular domains.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16400" title="Abstract">arXiv:2310.16400</a> [<a href="/pdf/2310.16400" title="Download PDF">pdf</a>, <a href="/format/2310.16400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fuse Your Latents: Video Editing with Multi-source Latent Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+T">Tianyi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiaxi Gu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+R">Renjing Pei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Songcen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zuxuan Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Latent Diffusion Models (LDMs) are renowned for their powerful capabilities
in image and video synthesis. Yet, video editing methods suffer from
insufficient pre-training data or video-by-video re-training cost. In
addressing this gap, we propose FLDM (Fused Latent Diffusion Model), a
training-free framework to achieve text-guided video editing by applying
off-the-shelf image editing methods in video LDMs. Specifically, FLDM fuses
latents from an image LDM and an video LDM during the denoising process. In
this way, temporal consistency can be kept with video LDM while high-fidelity
from the image LDM can also be exploited. Meanwhile, FLDM possesses high
flexibility since both image LDM and video LDM can be replaced so advanced
image editing methods such as InstructPix2Pix and ControlNet can be exploited.
To the best of our knowledge, FLDM is the first method to adapt off-the-shelf
image editing methods into video LDMs for video editing. Extensive quantitative
and qualitative experiments demonstrate that FLDM can improve the textual
alignment and temporal consistency of edited videos.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16401" title="Abstract">arXiv:2310.16401</a> [<a href="/pdf/2310.16401" title="Download PDF">pdf</a>, <a href="/format/2310.16401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks with a Distribution of Parametrized Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S+H">See Hian Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+F">Feng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+K">Kelin Xia</a>, 
<a href="/search/cs?searchtype=author&query=Tay%2C+W+P">Wee Peng Tay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Traditionally, graph neural networks have been trained using a single
observed graph. However, the observed graph represents only one possible
realization. In many applications, the graph may encounter uncertainties, such
as having erroneous or missing edges, as well as edge weights that provide
little informative value. To address these challenges and capture additional
information previously absent in the observed graph, we introduce latent
variables to parameterize and generate multiple graphs. We obtain the maximum
likelihood estimate of the network parameters in an Expectation-Maximization
(EM) framework based on the multiple graphs. Specifically, we iteratively
determine the distribution of the graphs using a Markov Chain Monte Carlo
(MCMC) method, incorporating the principles of PAC-Bayesian theory. Numerical
experiments demonstrate improvements in performance against baseline models on
node classification for heterogeneous graphs and graph regression on chemistry
datasets.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16402" title="Abstract">arXiv:2310.16402</a> [<a href="/pdf/2310.16402" title="Download PDF">pdf</a>, <a href="/format/2310.16402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video Referring Expression Comprehension via Transformer with  Content-conditioned Query
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Ji Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+M">Meng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+T">Tengtao Song</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Long Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Y">Yuexian Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACM International Conference on Multimedia Workshop (ACM MM), 2023. arXiv admin note: substantial text overlap with <a href="/abs/2210.02953">arXiv:2210.02953</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Video Referring Expression Comprehension (REC) aims to localize a target
object in videos based on the queried natural language. Recent improvements in
video REC have been made using Transformer-based methods with learnable
queries. However, we contend that this naive query design is not ideal given
the open-world nature of video REC brought by text supervision. With numerous
potential semantic categories, relying on only a few slow-updated queries is
insufficient to characterize them. Our solution to this problem is to create
dynamic queries that are conditioned on both the input video and language to
model the diverse objects referred to. Specifically, we place a fixed number of
learnable bounding boxes throughout the frame and use corresponding region
features to provide prior information. Also, we noticed that current query
features overlook the importance of cross-modal alignment. To address this, we
align specific phrases in the sentence with semantically relevant visual areas,
annotating them in existing video datasets (VID-Sentence and VidSTG). By
incorporating these two designs, our proposed model (called ConFormer)
outperforms other models on widely benchmarked datasets. For example, in the
testing split of VID-Sentence dataset, ConFormer achieves 8.75% absolute
improvement on Accu.@0.6 compared to the previous state-of-the-art model.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16405" title="Abstract">arXiv:2310.16405</a> [<a href="/pdf/2310.16405" title="Download PDF">pdf</a>, <a href="/format/2310.16405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Binary State Recognition by Robots using Visual Question Answering of  Pre-Trained Vision-Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kawaharazuka%2C+K">Kento Kawaharazuka</a>, 
<a href="/search/cs?searchtype=author&query=Obinata%2C+Y">Yoshiki Obinata</a>, 
<a href="/search/cs?searchtype=author&query=Kanazawa%2C+N">Naoaki Kanazawa</a>, 
<a href="/search/cs?searchtype=author&query=Okada%2C+K">Kei Okada</a>, 
<a href="/search/cs?searchtype=author&query=Inaba%2C+M">Masayuki Inaba</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Recognition of the current state is indispensable for the operation of a
robot. There are various states to be recognized, such as whether an elevator
door is open or closed, whether an object has been grasped correctly, and
whether the TV is turned on or off. Until now, these states have been
recognized by programmatically describing the state of a point cloud or raw
image, by annotating and learning images, by using special sensors, etc. In
contrast to these methods, we apply Visual Question Answering (VQA) from a
Pre-Trained Vision-Language Model (PTVLM) trained on a large-scale dataset, to
such binary state recognition. This idea allows us to intuitively describe
state recognition in language without any re-training, thereby improving the
recognition ability of robots in a simple and general way. We summarize various
techniques in questioning methods and image processing, and clarify their
properties through experiments.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16406" title="Abstract">arXiv:2310.16406</a> [<a href="/pdf/2310.16406" title="Download PDF">pdf</a>, <a href="/format/2310.16406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Challenges of Radio Frequency Fingerprinting: From Data Collection to  Deployment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alhazbi%2C+S">Saeif Alhazbi</a>, 
<a href="/search/cs?searchtype=author&query=Hussain%2C+A">Ahmed Hussain</a>, 
<a href="/search/cs?searchtype=author&query=Sciancalepore%2C+S">Savio Sciancalepore</a>, 
<a href="/search/cs?searchtype=author&query=Oligeri%2C+G">Gabriele Oligeri</a>, 
<a href="/search/cs?searchtype=author&query=Papadimitratos%2C+P">Panos Papadimitratos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 1 table, and 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Radio Frequency Fingerprinting (RFF) techniques promise to authenticate
wireless devices at the physical layer based on inherent hardware imperfections
introduced during manufacturing. Such RF transmitter imperfections are
reflected into over-the-air signals, allowing receivers to accurately identify
the RF transmitting source. Recent advances in Machine Learning, particularly
in Deep Learning (DL), have improved the ability of RFF systems to extract and
learn complex features that make up the device-specific fingerprint. However,
integrating DL techniques with RFF and operating the system in real-world
scenarios presents numerous challenges. This article identifies and analyzes
these challenges while considering the three reference phases of any DL-based
RFF system: (i) data collection and preprocessing, (ii) training, and finally,
(iii) deployment. Our investigation points out the current open problems that
prevent real deployment of RFF while discussing promising future directions,
thus paving the way for further research in the area.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16407" title="Abstract">arXiv:2310.16407</a> [<a href="/pdf/2310.16407" title="Download PDF">pdf</a>, <a href="/format/2310.16407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information-Theoretic Generalization Analysis for Topology-aware  Heterogeneous Federated Edge Learning over Noisy Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zheshun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zenglin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hongfang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jie Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With the rapid growth of edge intelligence, the deployment of federated
learning (FL) over wireless networks has garnered increasing attention, which
is called Federated Edge Learning (FEEL). In FEEL, both mobile devices
transmitting model parameters over noisy channels and collecting data in
diverse environments pose challenges to the generalization of trained models.
Moreover, devices can engage in decentralized FL via Device-to-Device
communication while the communication topology of connected devices also
impacts the generalization of models. Most recent theoretical studies overlook
the incorporation of all these effects into FEEL when developing generalization
analyses. In contrast, our work presents an information-theoretic
generalization analysis for topology-aware FEEL in the presence of data
heterogeneity and noisy channels. Additionally, we propose a novel
regularization method called Federated Global Mutual Information Reduction
(FedGMIR) to enhance the performance of models based on our analysis. Numerical
results validate our theoretical findings and provide evidence for the
effectiveness of the proposed method.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16409" title="Abstract">arXiv:2310.16409</a> [<a href="/pdf/2310.16409" title="Download PDF">pdf</a>, <a href="/format/2310.16409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiple Key-value Strategy in Recommendation Systems Incorporating  Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+X">Xiangyu Hou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaohui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Renbing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+D">Daiyue Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CIKM2023 workshop at GenRec'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recommendation system (RS) plays significant roles in matching users
information needs for Internet applications, and it usually utilizes the
vanilla neural network as the backbone to handle embedding details. Recently,
the large language model (LLM) has exhibited emergent abilities and achieved
great breakthroughs both in the CV and NLP communities. Thus, it is logical to
incorporate RS with LLM better, which has become an emerging research
direction. Although some existing works have made their contributions to this
issue, they mainly consider the single key situation (e.g. historical
interactions), especially in sequential recommendation. The situation of
multiple key-value data is simply neglected. This significant scenario is
mainstream in real practical applications, where the information of users (e.g.
age, occupation, etc) and items (e.g. title, category, etc) has more than one
key. Therefore, we aim to implement sequential recommendations based on
multiple key-value data by incorporating RS with LLM. In particular, we
instruct tuning a prevalent open-source LLM (Llama 7B) in order to inject
domain knowledge of RS into the pre-trained LLM. Since we adopt multiple
key-value strategies, LLM is hard to learn well among these keys. Thus the
general and innovative shuffle and mask strategies, as an innovative manner of
data argument, are designed. To demonstrate the effectiveness of our approach,
extensive experiments are conducted on the popular and suitable dataset
MovieLens which contains multiple keys-value. The experimental results
demonstrate that our approach can nicely and effectively complete this
challenging issue.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16410" title="Abstract">arXiv:2310.16410</a> [<a href="/pdf/2310.16410" title="Download PDF">pdf</a>, <a href="/format/2310.16410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in  AlphaZero
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schut%2C+L">Lisa Schut</a>, 
<a href="/search/cs?searchtype=author&query=Tomasev%2C+N">Nenad Tomasev</a>, 
<a href="/search/cs?searchtype=author&query=McGrath%2C+T">Tom McGrath</a>, 
<a href="/search/cs?searchtype=author&query=Hassabis%2C+D">Demis Hassabis</a>, 
<a href="/search/cs?searchtype=author&query=Paquet%2C+U">Ulrich Paquet</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Been Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 61 pages, 29 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Artificial Intelligence (AI) systems have made remarkable progress, attaining
super-human performance across various domains. This presents us with an
opportunity to further human knowledge and improve human expert performance by
leveraging the hidden knowledge encoded within these highly performant AI
systems. Yet, this knowledge is often hard to extract, and may be hard to
understand or learn from. Here, we show that this is possible by proposing a
new method that allows us to extract new chess concepts in AlphaZero, an AI
system that mastered the game of chess via self-play without human supervision.
Our analysis indicates that AlphaZero may encode knowledge that extends beyond
the existing human knowledge, but knowledge that is ultimately not beyond human
grasp, and can be successfully learned from. In a human study, we show that
these concepts are learnable by top human experts, as four top chess
grandmasters show improvements in solving the presented concept prototype
positions. This marks an important first milestone in advancing the frontier of
human knowledge by leveraging AI; a development that could bear profound
implications and help us shape how we interact with AI systems across many AI
applications.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16411" title="Abstract">arXiv:2310.16411</a> [<a href="/pdf/2310.16411" title="Download PDF">pdf</a>, <a href="/format/2310.16411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding Stumpers: Large Language Models vs. Human Problem-Solvers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goldstein%2C+A">Alon Goldstein</a>, 
<a href="/search/cs?searchtype=author&query=Havin%2C+M">Miriam Havin</a>, 
<a href="/search/cs?searchtype=author&query=Reichart%2C+R">Roi Reichart</a>, 
<a href="/search/cs?searchtype=author&query=Goldstein%2C+A">Ariel Goldstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">This paper investigates the problem-solving capabilities of Large Language
Models (LLMs) by evaluating their performance on stumpers, unique single-step
intuition problems that pose challenges for human solvers but are easily
verifiable. We compare the performance of four state-of-the-art LLMs
(Davinci-2, Davinci-3, GPT-3.5-Turbo, GPT-4) to human participants. Our
findings reveal that the new-generation LLMs excel in solving stumpers and
surpass human performance. However, humans exhibit superior skills in verifying
solutions to the same problems. This research enhances our understanding of
LLMs' cognitive abilities and provides insights for enhancing their
problem-solving potential across various domains.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16412" title="Abstract">arXiv:2310.16412</a> [<a href="/pdf/2310.16412" title="Download PDF">pdf</a>, <a href="/format/2310.16412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FlatMatch: Bridging Labeled Data and Unlabeled Data with Cross-Sharpness  for Semi-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhuo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Semi-Supervised Learning (SSL) has been an effective way to leverage abundant
unlabeled data with extremely scarce labeled data. However, most SSL methods
are commonly based on instance-wise consistency between different data
transformations. Therefore, the label guidance on labeled data is hard to be
propagated to unlabeled data. Consequently, the learning process on labeled
data is much faster than on unlabeled data which is likely to fall into a local
minima that does not favor unlabeled data, leading to sub-optimal
generalization performance. In this paper, we propose FlatMatch which minimizes
a cross-sharpness measure to ensure consistent learning performance between the
two datasets. Specifically, we increase the empirical risk on labeled data to
obtain a worst-case model which is a failure case that needs to be enhanced.
Then, by leveraging the richness of unlabeled data, we penalize the prediction
difference (i.e., cross-sharpness) between the worst-case model and the
original model so that the learning direction is beneficial to generalization
on unlabeled data. Therefore, we can calibrate the learning process without
being limited to insufficient label information. As a result, the mismatched
learning performance can be mitigated, further enabling the effective
exploitation of unlabeled data and improving SSL performance. Through
comprehensive validation, we show FlatMatch achieves state-of-the-art results
in many SSL settings.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16417" title="Abstract">arXiv:2310.16417</a> [<a href="/pdf/2310.16417" title="Download PDF">pdf</a>, <a href="/format/2310.16417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Simultaneous Machine Translation with Word-level Policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kang Kim</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+H">Hankyu Cho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent years have seen remarkable advances in the field of Simultaneous
Machine Translation (SiMT) due to the introduction of innovative policies that
dictate whether to READ or WRITE at each step of the translation process.
However, a common assumption in many existing studies is that operations are
carried out at the subword level, even though the standard unit for input and
output in most practical scenarios is typically at the word level. This paper
demonstrates that policies devised and validated at the subword level are
surpassed by those operating at the word level, which process multiple subwords
to form a complete word in a single step. Additionally, we suggest a method to
boost SiMT models using language models (LMs), wherein the proposed word-level
policy plays a vital role in addressing the subword disparity between LMs and
SiMT models. Code is available at https://github.com/xl8-ai/WordSiMT.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16419" title="Abstract">arXiv:2310.16419</a> [<a href="/pdf/2310.16419" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Knowledge Base Canonicalization with Multi-task Unlearning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bingchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+S">Shihao Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+W">Weixin Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shijun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Li Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The construction of large open knowledge bases (OKBs) is integral to many
applications in the field of mobile computing. Noun phrases and relational
phrases in OKBs often suffer from redundancy and ambiguity, which calls for the
investigation on OKB canonicalization. However, in order to meet the
requirements of some privacy protection regulations and to ensure the
timeliness of the data, the canonicalized OKB often needs to remove some
sensitive information or outdated data. The machine unlearning in OKB
canonicalization is an excellent solution to the above problem. Current
solutions address OKB canonicalization by devising advanced clustering
algorithms and using knowledge graph embedding (KGE) to further facilitate the
canonicalization process. Effective schemes are urgently needed to fully
synergise machine unlearning with clustering and KGE learning. To this end, we
put forward a multi-task unlearning framework, namely MulCanon, to tackle
machine unlearning problem in OKB canonicalization. Specifically, the noise
characteristics in the diffusion model are utilized to achieve the effect of
machine unlearning for data in OKB. MulCanon unifies the learning objectives of
diffusion model, KGE and clustering algorithms, and adopts a two-step
multi-task learning paradigm for training. A thorough experimental study on
popular OKB canonicalization datasets validates that MulCanon achieves advanced
machine unlearning effects.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16421" title="Abstract">arXiv:2310.16421</a> [<a href="/pdf/2310.16421" title="Download PDF">pdf</a>, <a href="/format/2310.16421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Agent: Explicit Reasoning Agent for Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qinyong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhenxiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Rong Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Graph embedding methods such as Graph Neural Networks (GNNs) and Graph
Transformers have contributed to the development of graph reasoning algorithms
for various tasks on knowledge graphs. However, the lack of interpretability
and explainability of graph embedding methods has limited their applicability
in scenarios requiring explicit reasoning. In this paper, we introduce the
Graph Agent (GA), an intelligent agent methodology of leveraging large language
models (LLMs), inductive-deductive reasoning modules, and long-term memory for
knowledge graph reasoning tasks. GA integrates aspects of symbolic reasoning
and existing graph embedding methods to provide an innovative approach for
complex graph reasoning tasks. By converting graph structures into textual
data, GA enables LLMs to process, reason, and provide predictions alongside
human-interpretable explanations. The effectiveness of the GA was evaluated on
node classification and link prediction tasks. Results showed that GA reached
state-of-the-art performance, demonstrating accuracy of 90.65%, 95.48%, and
89.32% on Cora, PubMed, and PrimeKG datasets, respectively. Compared to
existing GNN and transformer models, GA offered advantages of explicit
reasoning ability, free-of-training, easy adaption to various graph reasoning
tasks
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16427" title="Abstract">arXiv:2310.16427</a> [<a href="/pdf/2310.16427" title="Download PDF">pdf</a>, <a href="/format/2310.16427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PromptAgent: Strategic Planning with Language Models Enables  Expert-level Prompt Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+F">Fan Bai</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haotian Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiayou Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jojic%2C+N">Nebojsa Jojic</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E+P">Eric P. Xing</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhiting Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Highly effective, task-specific prompts are often heavily engineered by
experts to integrate detailed instructions and domain insights based on a deep
understanding of both instincts of large language models (LLMs) and the
intricacies of the target task. However, automating the generation of such
expert-level prompts remains elusive. Existing prompt optimization methods tend
to overlook the depth of domain knowledge and struggle to efficiently explore
the vast space of expert-level prompts. Addressing this, we present
PromptAgent, an optimization method that autonomously crafts prompts equivalent
in quality to those handcrafted by experts. At its core, PromptAgent views
prompt optimization as a strategic planning problem and employs a principled
planning algorithm, rooted in Monte Carlo tree search, to strategically
navigate the expert-level prompt space. Inspired by human-like trial-and-error
exploration, PromptAgent induces precise expert-level insights and in-depth
instructions by reflecting on model errors and generating constructive error
feedback. Such a novel framework allows the agent to iteratively examine
intermediate prompts (states), refine them based on error feedbacks (actions),
simulate future rewards, and search for high-reward paths leading to expert
prompts. We apply PromptAgent to 12 tasks spanning three practical domains:
BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing
it significantly outperforms strong Chain-of-Thought and recent prompt
optimization baselines. Extensive analyses emphasize its capability to craft
expert-level, detailed, and domain-insightful prompts with great efficiency and
generalizability.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16430" title="Abstract">arXiv:2310.16430</a> [<a href="/pdf/2310.16430" title="Download PDF">pdf</a>, <a href="/format/2310.16430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Integrative Paradigm for Enhanced Stroke Prediction: Synergizing  XGBoost and xDeepFM Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+W">Weinan Dai</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yifeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+C">Chengjie Mou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chongyu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Stroke prediction plays a crucial role in preventing and managing this
debilitating condition. In this study, we address the challenge of stroke
prediction using a comprehensive dataset, and propose an ensemble model that
combines the power of XGBoost and xDeepFM algorithms. Our work aims to improve
upon existing stroke prediction models by achieving higher accuracy and
robustness. Through rigorous experimentation, we validate the effectiveness of
our ensemble model using the AUC metric. Through comparing our findings with
those of other models in the field, we gain valuable insights into the merits
and drawbacks of various approaches. This, in turn, contributes significantly
to the progress of machine learning and deep learning techniques specifically
in the domain of stroke prediction.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16431" title="Abstract">arXiv:2310.16431</a> [<a href="/pdf/2310.16431" title="Download PDF">pdf</a>, <a href="/format/2310.16431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An experimental protocol to access immersiveness in video games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malaspina%2C+M">Marika Malaspina</a>, 
<a href="/search/cs?searchtype=author&query=Barbato%2C+J+A">Jessica Amianto Barbato</a>, 
<a href="/search/cs?searchtype=author&query=Cremaschi%2C+M">Marco Cremaschi</a>, 
<a href="/search/cs?searchtype=author&query=Gasparini%2C+F">Francesca Gasparini</a>, 
<a href="/search/cs?searchtype=author&query=Grossi%2C+A">Alessandra Grossi</a>, 
<a href="/search/cs?searchtype=author&query=Saibene%2C+A">Aurora Saibene</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the Italian Workshop on Artificial Intelligence for Human Machine Interaction (AIxHMI 2023), November 06, 2023, Rome, Italy
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">In the video game industry, great importance is given to the experience that
the user has while playing a game. In particular, this experience benefits from
the players' perceived sense of being in the game or immersion. The level of
user immersion depends not only on the game's content but also on how the game
is displayed, thus on its User Interface (UI) and the Head's-Up Display (HUD).
Another factor influencing immersiveness that has been found in the literature
is the player's expertise: the more experience the user has with a specific
game, the less they need information on the screen to be immersed in the game.
Player's level of immersion can be accessed by using both questionnaires of
their perceived experience and exploiting their behavioural and physiological
responses while playing the target game. Therefore, in this paper, we propose
an experimental protocol to access immersiveness of gamers while playing a
third-person shooter (Fortnite) with UIs with a standard, a dietetic, and a
proposed HUD. A subjective evaluation of the immersion will be provided by
completing the Immersive Experience Questionnaire (IEQ), while objective
indicators will be provided by face tracking, behaviour and physiological
responses analyses. The ultimate goal of this study is to define guidelines for
video game UI development that can enhance the players' immersion.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16433" title="Abstract">arXiv:2310.16433</a> [<a href="/pdf/2310.16433" title="Download PDF">pdf</a>, <a href="/format/2310.16433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RIPencapsulation: Defeating IP Encapsulation on TI MSP Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sah%2C+P">Prakhar Sah</a>, 
<a href="/search/cs?searchtype=author&query=Hicks%2C+M">Matthew Hicks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 3 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Internet of Things (IoT) devices sit at the intersection of unwieldy software
complexity and unprecedented attacker access. This unique position comes with a
daunting security challenge: how can I protect both proprietary code and
confidential data on a device that the attacker has unfettered access to?
Trusted Execution Environments (TEEs) promise to solve this challenge through
hardware-based separation of trusted and untrusted computation and data. While
TEEs do an adequate job of protecting secrets on desktop-class devices, we
reveal that trade-offs made in one of the most widely-used commercial IoT
devices undermine their TEE's security.
<br />This paper uncovers two fundamental weaknesses in IP Encapsulation (IPE), the
TEE deployed by Texas Instruments for MSP430 and MSP432 devices. We observe
that lack of call site enforcement and residual state after unexpected TEE
exits enable an attacker to reveal all proprietary code and secret data within
the IPE. We design and implement an attack called RIPencapsulation, which
systematically executes portions of code within the IPE and uses the partial
state revealed through the register file to exfiltrate secret data and to
identify gadget instructions. The attack then uses gadget instructions to
reveal all proprietary code within the IPE. Our evaluation with commodity
devices and a production compiler and settings shows that -- even after
following all manufacturer-recommended secure coding practices --
RIPencapsultaion reveals, within minutes, both the code and keys from
third-party cryptographic implementations protected by the IPE.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16435" title="Abstract">arXiv:2310.16435</a> [<a href="/pdf/2310.16435" title="Download PDF">pdf</a>, <a href="/format/2310.16435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Pixel-level Performance Assessment in Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rafiei%2C+M">Mehdi Rafiei</a>, 
<a href="/search/cs?searchtype=author&query=Breckon%2C+T+P">Toby P. Breckon</a>, 
<a href="/search/cs?searchtype=author&query=Iosifidis%2C+A">Alexandros Iosifidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Anomaly detection methods have demonstrated remarkable success across various
applications. However, assessing their performance, particularly at the
pixel-level, presents a complex challenge due to the severe imbalance that is
most commonly present between normal and abnormal samples. Commonly adopted
evaluation metrics designed for pixel-level detection may not effectively
capture the nuanced performance variations arising from this class imbalance.
In this paper, we dissect the intricacies of this challenge, underscored by
visual evidence and statistical analysis, leading to delve into the need for
evaluation metrics that account for the imbalance. We offer insights into more
accurate metrics, using eleven leading contemporary anomaly detection methods
on twenty-one anomaly detection problems. Overall, from this extensive
experimental evaluation, we can conclude that Precision-Recall-based metrics
can better capture relative method performance, making them more suitable for
the task.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16436" title="Abstract">arXiv:2310.16436</a> [<a href="/pdf/2310.16436" title="Download PDF">pdf</a>, <a href="/format/2310.16436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning  in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Ge Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiajin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hong-Yu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sibei Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 13 figures, to be published in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">A long-standing goal of AI systems is to perform complex multimodal reasoning
like humans. Recently, large language models (LLMs) have made remarkable
strides in such multi-step reasoning on the language modality solely by
leveraging the chain of thought (CoT) to mimic human thinking. However, the
transfer of these advancements to multimodal contexts introduces heightened
challenges, including but not limited to the impractical need for
labor-intensive annotation and the limitations in terms of flexibility,
generalizability, and explainability. To evoke CoT reasoning in multimodality,
this work first conducts an in-depth analysis of these challenges posed by
multimodality and presents two key insights: "keeping critical thinking" and
"letting everyone do their jobs" in multimodal CoT reasoning. Furthermore, this
study proposes a novel DDCoT prompting that maintains a critical attitude
through negative-space prompting and incorporates multimodality into reasoning
by first dividing the reasoning responsibility of LLMs into reasoning and
recognition and then integrating the visual recognition capability of visual
models into the joint reasoning process. The rationales generated by DDCoT not
only improve the reasoning abilities of both large and small language models in
zero-shot prompting and fine-tuning learning, significantly outperforming
state-of-the-art methods but also exhibit impressive generalizability and
explainability.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16442" title="Abstract">arXiv:2310.16442</a> [<a href="/pdf/2310.16442" title="Download PDF">pdf</a>, <a href="/ps/2310.16442" title="Download PostScript">ps</a>, <a href="/format/2310.16442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Right preconditioned GMRES for arbitrary singular systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sugihara%2C+K">Kota Sugihara</a>, 
<a href="/search/math?searchtype=author&query=Hayami%2C+K">Ken Hayami</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Brown and Walker (1997) showed that GMRES determines a least squares solution
of $ A x = b $ where $ A \in {\bf R}^{n \times n} $ without breakdown for
arbitrary $ b, x_0 \in {\bf R}^n $ if and only if $A$ is range-symmetric, i.e.
$ {\cal R} (A^{\rm T}) = {\cal R} (A) $, where $ A $ may be singular and $ b $
may not be in the range space ${\cal R} A)$ of $A$. In this paper, we propose
applying GMRES to $ A C A^{\rm T} z = b $, where $ C \in {\bf R}^{n \times n} $
is symmetric positive definite. This determines a least squares solution $ x =
CA^{\rm T} z $ of $ A x = b $ without breakdown for arbitrary (singular) matrix
$A \in {\bf R}^{n \times n}$ and $ b \in {\bf R}^n $. To make the method
numerically stable, we propose using the pseudoinverse with an appropriate
threshold parameter to suppress the influence of tiny singular values when
solving the severely ill-conditioned Hessenberg systems which arise in the
Arnoldi process of GMRES when solving inconsistent range-symmetric systems.
Numerical experiments show that the method taking $C$ to be the identity matrix
gives the least squares solution even when $A$ is not range-symmetric,
including the case when $ {\rm index}(A) &gt;1$.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16444" title="Abstract">arXiv:2310.16444</a> [<a href="/pdf/2310.16444" title="Download PDF">pdf</a>, <a href="/ps/2310.16444" title="Download PostScript">ps</a>, <a href="/format/2310.16444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How can neuromorphic hardware attain brain-like functional capabilities?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maass%2C+W">Wolfgang Maass</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Research on neuromorphic computing is driven by the vision that we can
emulate brain-like computing capability, learning capability, and
energy-efficiency in novel hardware. Unfortunately, this vision has so far been
pursued in a half-hearted manner. Most current neuromorphic hardware (NMHW)
employs brain-like spiking neurons instead of standard artificial neurons. This
is a good first step, which does improve the energy-efficiency of some
computations, see \citep{rao2022long} for one of many examples. But current
architectures and training methods for networks of spiking neurons in NMHW are
largely copied from artificial neural networks. Hence it is not surprising that
they inherit many deficiencies of artificial neural networks, rather than
attaining brain-like functional capabilities.
<br />Of course, the brain is very complex, and we cannot implement all its details
in NMHW. Instead, we need to focus on principles that are both easy to
implement in NMHW and are likely to support brain-like functionality. The goal
of this article is to highlight some of them.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16446" title="Abstract">arXiv:2310.16446</a> [<a href="/pdf/2310.16446" title="Download PDF">pdf</a>, <a href="/format/2310.16446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversity Enhanced Narrative Question Generation for Storybooks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+H">Hokeun Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Bak%2C+J">JinYeong Bak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Question generation (QG) from a given context can enhance comprehension,
engagement, assessment, and overall efficacy in learning or conversational
environments. Despite recent advancements in QG, the challenge of enhancing or
measuring the diversity of generated questions often remains unaddressed. In
this paper, we introduce a multi-question generation model (mQG), which is
capable of generating multiple, diverse, and answerable questions by focusing
on context and questions. To validate the answerability of the generated
questions, we employ a SQuAD2.0 fine-tuned question answering model,
classifying the questions as answerable or not. We train and evaluate mQG on
the FairytaleQA dataset, a well-structured QA dataset based on storybooks, with
narrative questions. We further apply a zero-shot adaptation on the TellMeWhy
and SQuAD1.1 datasets. mQG shows promising results across various evaluation
metrics, among strong baselines.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16447" title="Abstract">arXiv:2310.16447</a> [<a href="/pdf/2310.16447" title="Download PDF">pdf</a>, <a href="/format/2310.16447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChimpACT: A Longitudinal Dataset for Understanding Chimpanzee Behaviors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaoxuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Kaufhold%2C+S+P">Stephan P. Kaufhold</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jiajun Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wentao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Terwilliger%2C+J">Jack Terwilliger</a>, 
<a href="/search/cs?searchtype=author&query=Meza%2C+A">Andres Meza</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yixin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Rossano%2C+F">Federico Rossano</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhou Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Understanding the behavior of non-human primates is crucial for improving
animal welfare, modeling social behavior, and gaining insights into
distinctively human and phylogenetically shared behaviors. However, the lack of
datasets on non-human primate behavior hinders in-depth exploration of primate
social interactions, posing challenges to research on our closest living
relatives. To address these limitations, we present ChimpACT, a comprehensive
dataset for quantifying the longitudinal behavior and social relations of
chimpanzees within a social group. Spanning from 2015 to 2018, ChimpACT
features videos of a group of over 20 chimpanzees residing at the Leipzig Zoo,
Germany, with a particular focus on documenting the developmental trajectory of
one young male, Azibo. ChimpACT is both comprehensive and challenging,
consisting of 163 videos with a cumulative 160,500 frames, each richly
annotated with detection, identification, pose estimation, and fine-grained
spatiotemporal behavior labels. We benchmark representative methods of three
tracks on ChimpACT: (i) tracking and identification, (ii) pose estimation, and
(iii) spatiotemporal action detection of the chimpanzees. Our experiments
reveal that ChimpACT offers ample opportunities for both devising new methods
and adapting existing ones to solve fundamental computer vision tasks applied
to chimpanzee groups, such as detection, pose estimation, and behavior
analysis, ultimately deepening our comprehension of communication and sociality
in non-human primates.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16450" title="Abstract">arXiv:2310.16450</a> [<a href="/pdf/2310.16450" title="Download PDF">pdf</a>, <a href="/format/2310.16450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLEX: Continuous Length Extrapolation for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guanzheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zaiqiao Meng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Shangsong Liang</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Transformer-based Large Language Models (LLMs) are pioneering advances in
many natural language processing tasks, however, their exceptional capabilities
are restricted within the preset context window of Transformer. Position
Embedding (PE) scaling methods, while effective in extending the context window
to a specific length, demonstrate either notable limitations in their
extrapolation abilities or sacrificing partial performance within the context
window. Length extrapolation methods, although theoretically capable of
extending the context window beyond the training sequence length, often
underperform in practical long-context applications. To address these
challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We
generalise the PE scaling approaches to model the continuous dynamics by
ordinary differential equations over the length scaling factor, thereby
overcoming the constraints of current PE scaling methods designed for specific
lengths. Moreover, by extending the dynamics to desired context lengths beyond
the training sequence length, CLEX facilitates the length extrapolation with
impressive performance in practical tasks. We demonstrate that CLEX can be
seamlessly incorporated into LLMs equipped with Rotary Position Embedding, such
as LLaMA and GPT-NeoX, with negligible impact on training and inference
latency. Experimental results reveal that CLEX can effectively extend the
context window to over 4x or almost 8x training length, with no deterioration
in performance. Furthermore, when evaluated on the practical LongBench
benchmark, our model trained on a 4k length exhibits competitive performance
against state-of-the-art open-source models trained on context lengths up to
32k.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16452" title="Abstract">arXiv:2310.16452</a> [<a href="/pdf/2310.16452" title="Download PDF">pdf</a>, <a href="/format/2310.16452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faithful Path Language Modelling for Explainable Recommendation over  Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balloccu%2C+G">Giacomo Balloccu</a>, 
<a href="/search/cs?searchtype=author&query=Boratto%2C+L">Ludovico Boratto</a>, 
<a href="/search/cs?searchtype=author&query=Cancedda%2C+C">Christian Cancedda</a>, 
<a href="/search/cs?searchtype=author&query=Fenu%2C+G">Gianni Fenu</a>, 
<a href="/search/cs?searchtype=author&query=Marras%2C+M">Mirko Marras</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Path reasoning methods over knowledge graphs have gained popularity for their
potential to improve transparency in recommender systems. However, the
resulting models still rely on pre-trained knowledge graph embeddings, fail to
fully exploit the interdependence between entities and relations in the KG for
recommendation, and may generate inaccurate explanations. In this paper, we
introduce PEARLM, a novel approach that efficiently captures user behaviour and
product-side knowledge through language modelling. With our approach, knowledge
graph embeddings are directly learned from paths over the KG by the language
model, which also unifies entities and relations in the same optimisation
space. Constraints on the sequence decoding additionally guarantee path
faithfulness with respect to the KG. Experiments on two datasets show the
effectiveness of our approach compared to state-of-the-art baselines. Source
code and datasets: AVAILABLE AFTER GETTING ACCEPTED.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16453" title="Abstract">arXiv:2310.16453</a> [<a href="/pdf/2310.16453" title="Download PDF">pdf</a>, <a href="/format/2310.16453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ClearMark: Intuitive and Robust Model Watermarking via Transposed Model  Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krau%C3%9F%2C+T">Torsten Krau&#xdf;</a>, 
<a href="/search/cs?searchtype=author&query=Stang%2C+J">Jasper Stang</a>, 
<a href="/search/cs?searchtype=author&query=Dmitrienko%2C+A">Alexandra Dmitrienko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 18 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Due to costly efforts during data acquisition and model training, Deep Neural
Networks (DNNs) belong to the intellectual property of the model creator.
Hence, unauthorized use, theft, or modification may lead to legal
repercussions. Existing DNN watermarking methods for ownership proof are often
non-intuitive, embed human-invisible marks, require trust in algorithmic
assessment that lacks human-understandable attributes, and rely on rigid
thresholds, making it susceptible to failure in cases of partial watermark
erasure.
<br />This paper introduces ClearMark, the first DNN watermarking method designed
for intuitive human assessment. ClearMark embeds visible watermarks, enabling
human decision-making without rigid value thresholds while allowing
technology-assisted evaluations. ClearMark defines a transposed model
architecture allowing to use of the model in a backward fashion to interwove
the watermark with the main task within all model parameters. Compared to
existing watermarking methods, ClearMark produces visual watermarks that are
easy for humans to understand without requiring complex verification algorithms
or strict thresholds. The watermark is embedded within all model parameters and
entangled with the main task, exhibiting superior robustness. It shows an
8,544-bit watermark capacity comparable to the strongest existing work.
Crucially, ClearMark's effectiveness is model and dataset-agnostic, and
resilient against adversarial model manipulations, as demonstrated in a
comprehensive study performed with four datasets and seven architectures.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16457" title="Abstract">arXiv:2310.16457</a> [<a href="/pdf/2310.16457" title="Download PDF">pdf</a>, <a href="/format/2310.16457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Explainability in Monocular Depth Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arampatzakis%2C+V">Vasileios Arampatzakis</a>, 
<a href="/search/cs?searchtype=author&query=Pavlidis%2C+G">George Pavlidis</a>, 
<a href="/search/cs?searchtype=author&query=Pantoglou%2C+K">Kyriakos Pantoglou</a>, 
<a href="/search/cs?searchtype=author&query=Mitianoudis%2C+N">Nikolaos Mitianoudis</a>, 
<a href="/search/cs?searchtype=author&query=Papamarkos%2C+N">Nikos Papamarkos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The estimation of depth in two-dimensional images has long been a challenging
and extensively studied subject in computer vision. Recently, significant
progress has been made with the emergence of Deep Learning-based approaches,
which have proven highly successful. This paper focuses on the explainability
in monocular depth estimation methods, in terms of how humans perceive depth.
This preliminary study emphasizes on one of the most significant visual cues,
the relative size, which is prominent in almost all viewed images. We designed
a specific experiment to mimic the experiments in humans and have tested
state-of-the-art methods to indirectly assess the explainability in the context
defined. In addition, we observed that measuring the accuracy required further
attention and a particular approach is proposed to this end. The results show
that a mean accuracy of around 77% across methods is achieved, with some of the
methods performing markedly better, thus, indirectly revealing their
corresponding potential to uncover monocular depth cues, like relative size.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16459" title="Abstract">arXiv:2310.16459</a> [<a href="/pdf/2310.16459" title="Download PDF">pdf</a>, <a href="/format/2310.16459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DualMatch: Robust Semi-Supervised Learning with Dual-Level Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaofeng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Guo2%2C+L">Lanzhe Guo2</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zenglin Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 8 figures, Accepted by ECMLPKDD 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Semi-supervised learning provides an expressive framework for exploiting
unlabeled data when labels are insufficient. Previous semi-supervised learning
methods typically match model predictions of different data-augmented views in
a single-level interaction manner, which highly relies on the quality of
pseudo-labels and results in semi-supervised learning not robust. In this
paper, we propose a novel SSL method called DualMatch, in which the class
prediction jointly invokes feature embedding in a dual-level interaction
manner. DualMatch requires consistent regularizations for data augmentation,
specifically, 1) ensuring that different augmented views are regulated with
consistent class predictions, and 2) ensuring that different data of one class
are regulated with similar feature embeddings. Extensive experiments
demonstrate the effectiveness of DualMatch. In the standard SSL setting, the
proposal achieves 9% error reduction compared with SOTA methods, even in a more
challenging class-imbalanced setting, the proposal can still achieve 6% error
reduction. Code is available at https://github.com/CWangAI/DualMatch
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16466" title="Abstract">arXiv:2310.16466</a> [<a href="/pdf/2310.16466" title="Download PDF">pdf</a>, <a href="/format/2310.16466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Continuous Network Emerging Dynamics from Scarce Observations  via Data-Adaptive Stochastic Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jiaxu Cui</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Bingyi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bo Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">Learning network dynamics from the empirical structure and spatio-temporal
observation data is crucial to revealing the interaction mechanisms of complex
networks in a wide range of domains. However, most existing methods only aim at
learning network dynamic behaviors generated by a specific ordinary
differential equation instance, resulting in ineffectiveness for new ones, and
generally require dense observations. The observed data, especially from
network emerging dynamics, are usually difficult to obtain, which brings
trouble to model learning. Therefore, how to learn accurate network dynamics
with sparse, irregularly-sampled, partial, and noisy observations remains a
fundamental challenge. We introduce Neural ODE Processes for Network Dynamics
(NDP4ND), a new class of stochastic processes governed by stochastic
data-adaptive network dynamics, to overcome the challenge and learn continuous
network dynamics from scarce observations. Intensive experiments conducted on
various network dynamics in ecological population evolution, phototaxis
movement, brain activity, epidemic spreading, and real-world empirical systems,
demonstrate that the proposed method has excellent data adaptability and
computational efficiency, and can adapt to unseen network emerging dynamics,
producing accurate interpolation and extrapolation with reducing the ratio of
required observation data to only about 6\% and improving the learning speed
for new dynamics by three orders of magnitude.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16468" title="Abstract">arXiv:2310.16468</a> [<a href="/pdf/2310.16468" title="Download PDF">pdf</a>, <a href="/format/2310.16468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formal Runtime Error Detection During Development in the Automotive  Industry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hecking-Harbusch%2C+J">Jesko Hecking-Harbusch</a>, 
<a href="/search/cs?searchtype=author&query=Quante%2C+J">Jochen Quante</a>, 
<a href="/search/cs?searchtype=author&query=Schlund%2C+M">Maximilian Schlund</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be published in VMCAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Modern automotive software is highly complex and consists of millions lines
of code. For safety-relevant automotive software, it is recommended to use
sound static program analysis to prove the absence of runtime errors. However,
the analysis is often perceived as burdensome by developers because it runs for
a long time and produces many false alarms. If the analysis is performed on the
integrated software system, there is a scalability problem, and the analysis is
only possible at a late stage of development. If the analysis is performed on
individual modules instead, this is possible at an early stage of development,
but the usage context of modules is missing, which leads to too many false
alarms. In this case study, we present how automatically inferred contracts add
context to module-level analysis. Leveraging these contracts with an
off-the-shelf tool for abstract interpretation makes module-level analysis more
precise and more scalable. We evaluate this framework quantitatively on
industrial case studies from different automotive domains. Additionally, we
report on our qualitative experience for the verification of large-scale
embedded software projects.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16472" title="Abstract">arXiv:2310.16472</a> [<a href="/pdf/2310.16472" title="Download PDF">pdf</a>, <a href="/ps/2310.16472" title="Download PostScript">ps</a>, <a href="/format/2310.16472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semiring Provenance for Lightweight Description Logics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bourgaux%2C+C">Camille Bourgaux</a>, 
<a href="/search/cs?searchtype=author&query=Ozaki%2C+A">Ana Ozaki</a>, 
<a href="/search/cs?searchtype=author&query=Pe%C3%B1aloza%2C+R">Rafael Pe&#xf1;aloza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper currently under review. 102 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB)

</div>
<p class="mathjax">We investigate semiring provenance--a successful framework originally defined
in the relational database setting--for description logics. In this context,
the ontology axioms are annotated with elements of a commutative semiring and
these annotations are propagated to the ontology consequences in a way that
reflects how they are derived. We define a provenance semantics for a language
that encompasses several lightweight description logics and show its
relationships with semantics that have been defined for ontologies annotated
with a specific kind of annotation (such as fuzzy degrees). We show that under
some restrictions on the semiring, the semantics satisfies desirable properties
(such as extending the semiring provenance defined for databases). We then
focus on the well-known why-provenance, which allows to compute the semiring
provenance for every additively and multiplicatively idempotent commutative
semiring, and for which we study the complexity of problems related to the
provenance of an axiom or a conjunctive query answer. Finally, we consider two
more restricted cases which correspond to the so-called positive Boolean
provenance and lineage in the database setting. For these cases, we exhibit
relationships with well-known notions related to explanations in description
logics and complete our complexity analysis. As a side contribution, we provide
conditions on an ELHI_bot ontology that guarantee tractable reasoning.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16473" title="Abstract">arXiv:2310.16473</a> [<a href="/pdf/2310.16473" title="Download PDF">pdf</a>, <a href="/format/2310.16473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symphony of experts: orchestration with adversarial insights in  reinforcement learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jonckheere%2C+M">Matthieu Jonckheere</a> (LAAS), 
<a href="/search/cs?searchtype=author&query=Mignacco%2C+C">Chiara Mignacco</a> (LMO, CELESTE), 
<a href="/search/cs?searchtype=author&query=Stoltz%2C+G">Gilles Stoltz</a> (LMO, CELESTE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Structured reinforcement learning leverages policies with advantageous
properties to reach better performance, particularly in scenarios where
exploration poses challenges. We explore this field through the concept of
orchestration, where a (small) set of expert policies guides decision-making;
the modeling thereof constitutes our first contribution. We then establish
value-functions regret bounds for orchestration in the tabular setting by
transferring regret-bound results from adversarial settings. We generalize and
extend the analysis of natural policy gradient in Agarwal et al. [2021, Section
5.3] to arbitrary adversarial aggregation strategies. We also extend it to the
case of estimated advantage functions, providing insights into sample
complexity both in expectation and high probability. A key point of our
approach lies in its arguably more transparent proofs compared to existing
methods. Finally, we present simulations for a stochastic matching toy model.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16475" title="Abstract">arXiv:2310.16475</a> [<a href="/pdf/2310.16475" title="Download PDF">pdf</a>, <a href="/format/2310.16475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Serverless Function Scheduling at the Network Edge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiong%2C+L">Lou Jiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhiqing%2C+T">Tang Zhiqing</a>, 
<a href="/search/cs?searchtype=author&query=Shijing%2C+Y">Yuan Shijing</a>, 
<a href="/search/cs?searchtype=author&query=Jie%2C+L">Li Jie</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chengtao Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Serverless computing is a promising approach for edge computing since its
inherent features, e.g., lightweight virtualization, rapid scalability, and
economic efficiency. However, previous studies have not studied well the issues
of significant cold start latency and highly dynamic workloads in serverless
function scheduling, which are exacerbated at the resource-limited network
edge. In this paper, we formulate the Serverless Function Scheduling (SFS)
problem for resource-limited edge computing, aiming to minimize the average
response time. To efficiently solve this intractable scheduling problem, we
first consider a simplified offline form of the problem and design a
polynomial-time optimal scheduling algorithm based on each function's weight.
Furthermore, we propose an Enhanced Shortest Function First (ESFF) algorithm,
in which the function weight represents the scheduling urgency. To avoid
frequent cold starts, ESFF selectively decides the initialization of new
function instances when receiving requests. To deal with dynamic workloads,
ESFF judiciously replaces serverless functions based on the function weight at
the completion time of requests. Extensive simulations based on real-world
serverless request traces are conducted, and the results show that ESFF
consistently and substantially outperforms existing baselines under different
settings.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16477" title="Abstract">arXiv:2310.16477</a> [<a href="/pdf/2310.16477" title="Download PDF">pdf</a>, <a href="/format/2310.16477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Show from Tell: Audio-Visual Modelling in Clinical Settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J">Jianbo Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Alsharid%2C+M">Mohammad Alsharid</a>, 
<a href="/search/cs?searchtype=author&query=Drukker%2C+L">Lior Drukker</a>, 
<a href="/search/cs?searchtype=author&query=Papageorghiou%2C+A+T">Aris T. Papageorghiou</a>, 
<a href="/search/cs?searchtype=author&query=Zisserman%2C+A">Andrew Zisserman</a>, 
<a href="/search/cs?searchtype=author&query=Noble%2C+J+A">J. Alison Noble</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Auditory and visual signals usually present together and correlate with each
other, not only in natural environments but also in clinical settings. However,
the audio-visual modelling in the latter case can be more challenging, due to
the different sources of audio/video signals and the noise (both signal-level
and semantic-level) in auditory signals -- usually speech. In this paper, we
consider audio-visual modelling in a clinical setting, providing a solution to
learn medical representations that benefit various clinical tasks, without
human expert annotation. A simple yet effective multi-modal self-supervised
learning framework is proposed for this purpose. The proposed approach is able
to localise anatomical regions of interest during ultrasound imaging, with only
speech audio as a reference. Experimental evaluations on a large-scale clinical
multi-modal ultrasound video dataset show that the proposed self-supervised
method learns good transferable anatomical representations that boost the
performance of automated downstream clinical tasks, even outperforming
fully-supervised solutions.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16483" title="Abstract">arXiv:2310.16483</a> [<a href="/pdf/2310.16483" title="Download PDF">pdf</a>, <a href="/format/2310.16483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gramian Attention Heads are Strong yet Efficient Vision Learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ryu%2C+J">Jongbin Ryu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dongyoon Han</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+J">Jongwoo Lim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We introduce a novel architecture design that enhances expressiveness by
incorporating multiple head classifiers (\ie, classification heads) instead of
relying on channel expansion or additional building blocks. Our approach
employs attention-based aggregation, utilizing pairwise feature similarity to
enhance multiple lightweight heads with minimal resource overhead. We compute
the Gramian matrices to reinforce class tokens in an attention layer for each
head. This enables the heads to learn more discriminative representations,
enhancing their aggregation capabilities. Furthermore, we propose a learning
algorithm that encourages heads to complement each other by reducing
correlation for aggregation. Our models eventually surpass state-of-the-art
CNNs and ViTs regarding the accuracy-throughput trade-off on ImageNet-1K and
deliver remarkable performance across various downstream tasks, such as COCO
object instance segmentation, ADE20k semantic segmentation, and fine-grained
visual classification datasets. The effectiveness of our framework is
substantiated by practical experimental results and further underpinned by
generalization error bound. We release the code publicly at:
https://github.com/Lab-LVM/imagenet-models.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16484" title="Abstract">arXiv:2310.16484</a> [<a href="/pdf/2310.16484" title="Download PDF">pdf</a>, <a href="/format/2310.16484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subspace Chronicles: How Linguistic Information Emerges, Shifts and  Interacts during Language Model Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller-Eberstein%2C+M">Max M&#xfc;ller-Eberstein</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Goot%2C+R">Rob van der Goot</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>, 
<a href="/search/cs?searchtype=author&query=Titov%2C+I">Ivan Titov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Representational spaces learned via language modeling are fundamental to
Natural Language Processing (NLP), however there has been limited understanding
regarding how and when during training various types of linguistic information
emerge and interact. Leveraging a novel information theoretic probing suite,
which enables direct comparisons of not just task performance, but their
representational subspaces, we analyze nine tasks covering syntax, semantics
and reasoning, across 2M pre-training steps and five seeds. We identify
critical learning phases across tasks and time, during which subspaces emerge,
share information, and later disentangle to specialize. Across these phases,
syntactic knowledge is acquired rapidly after 0.5% of full training. Continued
performance improvements primarily stem from the acquisition of open-domain
knowledge, while semantics and reasoning tasks benefit from later boosts to
long-range contextualization and higher specialization. Measuring cross-task
similarity further reveals that linguistically related tasks share information
throughout training, and do so more during the critical phase of learning than
before or after. Our findings have implications for model interpretability,
multi-task learning, and learning from limited data.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16485" title="Abstract">arXiv:2310.16485</a> [<a href="/pdf/2310.16485" title="Download PDF">pdf</a>, <a href="/format/2310.16485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Python Library for Deep Learning-Based Event Detection  in Multivariate Time Series Data and Information Retrieval in NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azib%2C+M">Menouar Azib</a>, 
<a href="/search/cs?searchtype=author&query=Renard%2C+B">Benjamin Renard</a>, 
<a href="/search/cs?searchtype=author&query=Garnier%2C+P">Philippe Garnier</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%A9not%2C+V">Vincent G&#xe9;not</a>, 
<a href="/search/cs?searchtype=author&query=Andr%C3%A9%2C+N">Nicolas Andr&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for the 22nd International Conference on Machine Learning and Applications (ICMLA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Event detection in time series data is crucial in various domains, including
finance, healthcare, cybersecurity, and science. Accurately identifying events
in time series data is vital for making informed decisions, detecting
anomalies, and predicting future trends. Despite extensive research exploring
diverse methods for event detection in time series, with deep learning
approaches being among the most advanced, there is still room for improvement
and innovation in this field. In this paper, we present a new deep learning
supervised method for detecting events in multivariate time series data. Our
method combines four distinct novelties compared to existing deep-learning
supervised methods. Firstly, it is based on regression instead of binary
classification. Secondly, it does not require labeled datasets where each point
is labeled; instead, it only requires reference events defined as time points
or intervals of time. Thirdly, it is designed to be robust by using a stacked
ensemble learning meta-model that combines deep learning models, ranging from
classic feed-forward neural networks (FFNs) to state-of-the-art architectures
like transformers. This ensemble approach can mitigate individual model
weaknesses and biases, resulting in more robust predictions. Finally, to
facilitate practical implementation, we have developed a Python package to
accompany our proposed method. The package, called eventdetector-ts, can be
installed through the Python Package Index (PyPI). In this paper, we present
our method and provide a comprehensive guide on the usage of the package. We
showcase its versatility and effectiveness through different real-world use
cases from natural language processing (NLP) to financial security domains.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16487" title="Abstract">arXiv:2310.16487</a> [<a href="/pdf/2310.16487" title="Download PDF">pdf</a>, <a href="/format/2310.16487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyperparameter Optimization for Multi-Objective Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Felten%2C+F">Florian Felten</a>, 
<a href="/search/cs?searchtype=author&query=Gareev%2C+D">Daniel Gareev</a>, 
<a href="/search/cs?searchtype=author&query=Talbi%2C+E">El-Ghazali Talbi</a>, 
<a href="/search/cs?searchtype=author&query=Danoy%2C+G">Gr&#xe9;goire Danoy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at the MODeM workshop <a href="https://modem2023.vub.ac.be/#">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Reinforcement learning (RL) has emerged as a powerful approach for tackling
complex problems. The recent introduction of multi-objective reinforcement
learning (MORL) has further expanded the scope of RL by enabling agents to make
trade-offs among multiple objectives. This advancement not only has broadened
the range of problems that can be tackled but also created numerous
opportunities for exploration and advancement. Yet, the effectiveness of RL
agents heavily relies on appropriately setting their hyperparameters. In
practice, this task often proves to be challenging, leading to unsuccessful
deployments of these techniques in various instances. Hence, prior research has
explored hyperparameter optimization in RL to address this concern.
<br />This paper presents an initial investigation into the challenge of
hyperparameter optimization specifically for MORL. We formalize the problem,
highlight its distinctive challenges, and propose a systematic methodology to
address it. The proposed methodology is applied to a well-known environment
using a state-of-the-art MORL algorithm, and preliminary results are reported.
Our findings indicate that the proposed methodology can effectively provide
hyperparameter configurations that significantly enhance the performance of
MORL agents. Furthermore, this study identifies various future research
opportunities to further advance the field of hyperparameter optimization for
MORL.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16491" title="Abstract">arXiv:2310.16491</a> [<a href="/pdf/2310.16491" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TSONN: Time-stepping-oriented neural network for solving partial  differential equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+W">Wenbo Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weiwei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Deep neural networks (DNNs), especially physics-informed neural networks
(PINNs), have recently become a new popular method for solving forward and
inverse problems governed by partial differential equations (PDEs). However,
these methods still face challenges in achieving stable training and obtaining
correct results in many problems, since minimizing PDE residuals with PDE-based
soft constraint make the problem ill-conditioned. Different from all existing
methods that directly minimize PDE residuals, this work integrates
time-stepping method with deep learning, and transforms the original
ill-conditioned optimization problem into a series of well-conditioned
sub-problems over given pseudo time intervals. The convergence of model
training is significantly improved by following the trajectory of the pseudo
time-stepping process, yielding a robust optimization-based PDE solver. Our
results show that the proposed method achieves stable training and correct
results in many problems that standard PINNs fail to solve, requiring only a
simple modification on the loss function. In addition, we demonstrate several
novel properties and advantages of time-stepping methods within the framework
of neural network-based optimization approach, in comparison to traditional
grid-based numerical method. Specifically, explicit scheme allows significantly
larger time step, while implicit scheme can be implemented as straightforwardly
as explicit scheme.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16492" title="Abstract">arXiv:2310.16492</a> [<a href="/pdf/2310.16492" title="Download PDF">pdf</a>, <a href="/format/2310.16492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Powerfulness of Textual Outlier Exposure for Visual OoD Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sangha Park</a>, 
<a href="/search/cs?searchtype=author&query=Mok%2C+J">Jisoo Mok</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+D">Dahuin Jung</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Saehyung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Sungroh Yoon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Successful detection of Out-of-Distribution (OoD) data is becoming
increasingly important to ensure safe deployment of neural networks. One of the
main challenges in OoD detection is that neural networks output overconfident
predictions on OoD data, make it difficult to determine OoD-ness of data solely
based on their predictions. Outlier exposure addresses this issue by
introducing an additional loss that encourages low-confidence predictions on
OoD data during training. While outlier exposure has shown promising potential
in improving OoD detection performance, all previous studies on outlier
exposure have been limited to utilizing visual outliers. Drawing inspiration
from the recent advancements in vision-language pre-training, this paper
venture out to the uncharted territory of textual outlier exposure. First, we
uncover the benefits of using textual outliers by replacing real or virtual
outliers in the image-domain with textual equivalents. Then, we propose various
ways of generating preferable textual outliers. Our extensive experiments
demonstrate that generated textual outliers achieve competitive performance on
large-scale OoD and hard OoD benchmarks. Furthermore, we conduct empirical
analyses of textual outliers to provide primary criteria for designing
advantageous textual outliers: near-distribution, descriptiveness, and
inclusion of visual semantics.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16494" title="Abstract">arXiv:2310.16494</a> [<a href="/pdf/2310.16494" title="Download PDF">pdf</a>, <a href="/format/2310.16494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lang3DSG: Language-based contrastive pre-training for 3D Scene Graph  prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koch%2C+S">Sebastian Koch</a>, 
<a href="/search/cs?searchtype=author&query=Hermosilla%2C+P">Pedro Hermosilla</a>, 
<a href="/search/cs?searchtype=author&query=Vaskevicius%2C+N">Narunas Vaskevicius</a>, 
<a href="/search/cs?searchtype=author&query=Colosi%2C+M">Mirco Colosi</a>, 
<a href="/search/cs?searchtype=author&query=Ropinski%2C+T">Timo Ropinski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3DV 2024. Project page: <a href="https://kochsebastian.com/lang3dsg">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">D scene graphs are an emerging 3D scene representation, that models both the
objects present in the scene as well as their relationships. However, learning
3D scene graphs is a challenging task because it requires not only object
labels but also relationship annotations, which are very scarce in datasets.
While it is widely accepted that pre-training is an effective approach to
improve model performance in low data regimes, in this paper, we find that
existing pre-training methods are ill-suited for 3D scene graphs. To solve this
issue, we present the first language-based pre-training approach for 3D scene
graphs, whereby we exploit the strong relationship between scene graphs and
language. To this end, we leverage the language encoder of CLIP, a popular
vision-language model, to distill its knowledge into our graph-based network.
We formulate a contrastive pre-training, which aligns text embeddings of
relationships (subject-predicate-object triplets) and predicted 3D graph
features. Our method achieves state-of-the-art results on the main semantic 3D
scene graph benchmark by showing improved effectiveness over pre-training
baselines and outperforming all the existing fully supervised scene graph
prediction methods by a significant margin. Furthermore, since our scene graph
features are language-aligned, it allows us to query the language space of the
features in a zero-shot manner. In this paper, we show an example of utilizing
this property of the features to predict the room type of a scene without
further training.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16496" title="Abstract">arXiv:2310.16496</a> [<a href="/pdf/2310.16496" title="Download PDF">pdf</a>, <a href="/format/2310.16496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Citizen participation: crowd-sensed sustainable indoor location services
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nasios%2C+I">Ioannis Nasios</a>, 
<a href="/search/cs?searchtype=author&query=Vogklis%2C+K">Konstantinos Vogklis</a>, 
<a href="/search/cs?searchtype=author&query=Malhi%2C+A">Avleen Malhi</a>, 
<a href="/search/cs?searchtype=author&query=Vayona%2C+A">Anastasia Vayona</a>, 
<a href="/search/cs?searchtype=author&query=Chatziadam%2C+P">Panos Chatziadam</a>, 
<a href="/search/cs?searchtype=author&query=Katos%2C+V">Vasilis Katos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to Elsevier
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">In the present era of sustainable innovation, the circular economy paradigm
dictates the optimal use and exploitation of existing finite resources. At the
same time, the transition to smart infrastructures requires considerable
investment in capital, resources and people. In this work, we present a general
machine learning approach for offering indoor location awareness without the
need to invest in additional and specialised hardware. We explore use cases
where visitors equipped with their smart phone would interact with the
available WiFi infrastructure to estimate their location, since the indoor
requirement poses a limitation to standard GPS solutions. Results have shown
that the proposed approach achieves a less than 2m accuracy and the model is
resilient even in the case where a substantial number of BSSIDs are dropped.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16499" title="Abstract">arXiv:2310.16499</a> [<a href="/pdf/2310.16499" title="Download PDF">pdf</a>, <a href="/format/2310.16499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Optimization in Deep Learning: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+O">Ou Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+R">Rujing Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large-scale, high-quality data are considered an essential factor for the
successful application of many deep learning techniques. Meanwhile, numerous
real-world deep learning tasks still have to contend with the lack of
sufficient amounts of high-quality data. Additionally, issues such as model
robustness, fairness, and trustworthiness are also closely related to training
data. Consequently, a huge number of studies in the existing literature have
focused on the data aspect in deep learning tasks. Some typical data
optimization techniques include data augmentation, logit perturbation, sample
weighting, and data condensation. These techniques usually come from different
deep learning divisions and their theoretical inspirations or heuristic
motivations may seem unrelated to each other. This study aims to organize a
wide range of existing data optimization methodologies for deep learning from
the previous literature, and makes the effort to construct a comprehensive
taxonomy for them. The constructed taxonomy considers the diversity of split
dimensions, and deep sub-taxonomies are constructed for each dimension. On the
basis of the taxonomy, connections among the extensive data optimization
methods for deep learning are built in terms of four aspects. We probe into
rendering several promising and interesting future directions. The constructed
taxonomy and the revealed connections will enlighten the better understanding
of existing methods and the design of novel data optimization techniques.
Furthermore, our aspiration for this survey is to promote data optimization as
an independent subdivision of deep learning. A curated, up-to-date list of
resources related to data optimization in deep learning is available at
\url{https://github.com/YaoRujing/Data-Optimization}.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16504" title="Abstract">arXiv:2310.16504</a> [<a href="/pdf/2310.16504" title="Download PDF">pdf</a>, <a href="/ps/2310.16504" title="Download PostScript">ps</a>, <a href="/format/2310.16504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure of CSS and CSS-T Quantum Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berardini%2C+E">Elena Berardini</a>, 
<a href="/search/cs?searchtype=author&query=Caminata%2C+A">Alessio Caminata</a>, 
<a href="/search/cs?searchtype=author&query=Ravagnani%2C+A">Alberto Ravagnani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We investigate CSS and CSS-T quantum error-correcting codes from the point of
view of their existence, rarity, and performance. We give a lower bound on the
number of pairs of linear codes that give rise to a CSS code with good
correction capability, showing that such pairs are easy to produce with a
randomized construction. We then prove that CSS-T codes exhibit the opposite
behaviour, showing also that, under very natural assumptions, their rate and
relative distance cannot be simultaneously large. This partially answers an
open question on the feasible parameters of CSS-T codes. We conclude with a
simple construction of CSS-T codes from Hermitian curves. The paper also offers
a concise introduction to CSS and CSS-T codes from the point of view of
classical coding theory.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16506" title="Abstract">arXiv:2310.16506</a> [<a href="/pdf/2310.16506" title="Download PDF">pdf</a>, <a href="/ps/2310.16506" title="Download PostScript">ps</a>, <a href="/format/2310.16506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Reasons for Bias: An Argumentation-Based Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waller%2C+M">Madeleine Waller</a>, 
<a href="/search/cs?searchtype=author&query=Rodrigues%2C+O">Odinaldo Rodrigues</a>, 
<a href="/search/cs?searchtype=author&query=Cocarascu%2C+O">Oana Cocarascu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">As algorithmic decision-making systems become more prevalent in society,
ensuring the fairness of these systems is becoming increasingly important.
Whilst there has been substantial research in building fair algorithmic
decision-making systems, the majority of these methods require access to the
training data, including personal characteristics, and are not transparent
regarding which individuals are classified unfairly. In this paper, we propose
a novel model-agnostic argumentation-based method to determine why an
individual is classified differently in comparison to similar individuals. Our
method uses a quantitative argumentation framework to represent attribute-value
pairs of an individual and of those similar to them, and uses a well-known
semantics to identify the attribute-value pairs in the individual contributing
most to their different classification. We evaluate our method on two datasets
commonly used in the fairness literature and illustrate its effectiveness in
the identification of bias.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16507" title="Abstract">arXiv:2310.16507</a> [<a href="/pdf/2310.16507" title="Download PDF">pdf</a>, <a href="/ps/2310.16507" title="Download PostScript">ps</a>, <a href="/format/2310.16507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information-theoretical analysis of event-triggered molecular  communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Labidi%2C+W">Wafa Labidi</a>, 
<a href="/search/cs?searchtype=author&query=Deppe%2C+C">Christian Deppe</a>, 
<a href="/search/cs?searchtype=author&query=Boche%2C+H">Holger Boche</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Numerous applications in the field of molecular communications (MC) such as
healthcare systems are often event-driven. The conventional Shannon capacity
may not be the appropriate metric for assessing performance in such cases. We
propose the identification (ID) capacity as an alternative metric.
Particularly, we consider randomized identification (RI) over the discrete-time
Poisson channel (DTPC), which is typically used as a model for MC systems that
utilize molecule-counting receivers. In the ID paradigm, the receiver's focus
is not on decoding the message sent. However, he wants to determine whether a
message of particular significance to him has been sent or not. In contrast to
Shannon transmission codes, the size of ID codes for a Discrete Memoryless
Channel (DMC) grows doubly exponentially fast with the blocklength, if
randomized encoding is used. In this paper, we derive the capacity formula for
RI over the DTPC subject to some peak and average power constraints.
Furthermore, we analyze the case of state-dependent DTPC.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16510" title="Abstract">arXiv:2310.16510</a> [<a href="/pdf/2310.16510" title="Download PDF">pdf</a>, <a href="/ps/2310.16510" title="Download PostScript">ps</a>, <a href="/format/2310.16510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance best practices using Java and AWS Lambda
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Men%C3%A9ndez%2C+J+M">Juan Mera Men&#xe9;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Bartlett%2C+M">Martin Bartlett</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Despite its already widespread popularity, it continues to gain adoption.
More and more developers and architects continue to adopt and apply the FaaS
(Function as a Service) model in cloud solutions. The most extensively used
FaaS service is AWS Lambda, provided by Amazon Web Services. Moreover, despite
the new trends in programming languages, Java still maintains a significant
share of usage. The main problem that arises when using these two technologies
together is widely known: significant latencies and the dreaded cold start.
However, it is possible to greatly mitigate this problem without dedicating too
much effort. In this article, various techniques, strategies and approaches
will be studied with the aim of reducing the cold start and significantly
improving the performance of Lambda functions with Java. Starting from a system
that involves AWS lambda, java, DynamoDB and Api Gateway. Each approach will be
tested independently, analyzing its impact through load tests. Subsequently,
they will be tested in combination in an effort to achieve the greatest
possible performance improvement.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16517" title="Abstract">arXiv:2310.16517</a> [<a href="/pdf/2310.16517" title="Download PDF">pdf</a>, <a href="/format/2310.16517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OccuQuest: Mitigating Occupational Bias for Inclusive Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+M">Mingfeng Xue</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dayiheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kexin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+W">Wenqiang Lei</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The emergence of large language models (LLMs) has revolutionized natural
language processing tasks. However, existing instruction-tuning datasets suffer
from occupational bias: the majority of data relates to only a few occupations,
which hampers the instruction-tuned LLMs to generate helpful responses to
professional queries from practitioners in specific fields. To mitigate this
issue and promote occupation-inclusive LLMs, we create an instruction-tuning
dataset named \emph{OccuQuest}, which contains 110,000+ prompt-completion pairs
and 30,000+ dialogues covering over 1,000 occupations in 26 occupational
categories. We systematically request ChatGPT, organizing queries
hierarchically based on Occupation, Responsibility, Topic, and Question, to
ensure a comprehensive coverage of occupational specialty inquiries. By
comparing with three commonly used datasets (Dolly, ShareGPT, and WizardLM), we
observe that OccuQuest exhibits a more balanced distribution across
occupations. Furthermore, we assemble three test sets for comprehensive
evaluation, an occu-test set covering 25 occupational categories, an estate set
focusing on real estate, and an occu-quora set containing real-world questions
from Quora. We then fine-tune LLaMA on OccuQuest to obtain OccuLLaMA, which
significantly outperforms state-of-the-art LLaMA variants (Vicuna, Tulu, and
WizardLM) on professional questions in GPT-4 and human evaluations. Notably, on
the occu-quora set, OccuLLaMA reaches a high win rate of 86.4\% against
WizardLM.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16519" title="Abstract">arXiv:2310.16519</a> [<a href="/pdf/2310.16519" title="Download PDF">pdf</a>, <a href="/format/2310.16519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MsATL: a Tool for SAT-Based ATL Satisfiability Checking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niewiadomski%2C+A">Artur Niewiadomski</a>, 
<a href="/search/cs?searchtype=author&query=Kacprzak%2C+M">Magdalena Kacprzak</a>, 
<a href="/search/cs?searchtype=author&query=Kurpiewski%2C+D">Damian Kurpiewski</a>, 
<a href="/search/cs?searchtype=author&query=Knapik%2C+M">Micha&#x142; Knapik</a>, 
<a href="/search/cs?searchtype=author&query=Penczek%2C+W">Wojciech Penczek</a>, 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 19th International Conference on Autonomous
  Agents and Multiagent Systems, {AAMAS} '20, Auckland, New Zealand, May 9-13,
  2020, 2111--2113
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">We present MsATL: the first tool for deciding the satisfiability of
Alternating-time Temporal Logic (ATL) with imperfect information. MsATL
combines SAT Modulo Monotonic Theories solvers with existing ATL model
checkers: MCMAS and STV. The tool can deal with various semantics of ATL,
including perfect and imperfect information, and can handle additional
practical requirements. MsATL can be applied for synthesis of games that
conform to a given specification, with the synthesised game often being
minimal.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16520" title="Abstract">arXiv:2310.16520</a> [<a href="/pdf/2310.16520" title="Download PDF">pdf</a>, <a href="/format/2310.16520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Self-Interpretable Graph-Level Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+K">Kaize Ding</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Q">Qinghua Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fuyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L+Y">Leo Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages; accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph-level anomaly detection (GLAD) aims to identify graphs that exhibit
notable dissimilarity compared to the majority in a collection. However,
current works primarily focus on evaluating graph-level abnormality while
failing to provide meaningful explanations for the predictions, which largely
limits their reliability and application scope. In this paper, we investigate a
new challenging problem, explainable GLAD, where the learning objective is to
predict the abnormality of each graph sample with corresponding explanations,
i.e., the vital subgraph that leads to the predictions. To address this
challenging problem, we propose a Self-Interpretable Graph aNomaly dETection
model (SIGNET for short) that detects anomalous graphs as well as generates
informative explanations simultaneously. Specifically, we first introduce the
multi-view subgraph information bottleneck (MSIB) framework, serving as the
design basis of our self-interpretable GLAD approach. This way SIGNET is able
to not only measure the abnormality of each graph based on cross-view mutual
information but also provide informative graph rationales by extracting
bottleneck subgraphs from the input graph and its dual hypergraph in a
self-supervised way. Extensive experiments on 16 datasets demonstrate the
anomaly detection capability and self-interpretability of SIGNET.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16523" title="Abstract">arXiv:2310.16523</a> [<a href="/pdf/2310.16523" title="Download PDF">pdf</a>, <a href="/format/2310.16523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Diversity of Demographic Representation in Large Language  Models via Collective-Critiques and Self-Voting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lahoti%2C+P">Preethi Lahoti</a>, 
<a href="/search/cs?searchtype=author&query=Blumm%2C+N">Nicholas Blumm</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Kotikalapudi%2C+R">Raghavendra Kotikalapudi</a>, 
<a href="/search/cs?searchtype=author&query=Potluri%2C+S">Sahitya Potluri</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Q">Qijun Tan</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+H">Hansa Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Packer%2C+B">Ben Packer</a>, 
<a href="/search/cs?searchtype=author&query=Beirami%2C+A">Ahmad Beirami</a>, 
<a href="/search/cs?searchtype=author&query=Beutel%2C+A">Alex Beutel</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jilin Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A crucial challenge for generative large language models (LLMs) is diversity:
when a user's prompt is under-specified, models may follow implicit assumptions
while generating a response, which may result in homogenization of the
responses, as well as certain demographic groups being under-represented or
even erased from the generated responses. In this paper, we formalize diversity
of representation in generative LLMs. We present evaluation datasets and
propose metrics to measure diversity in generated responses along people and
culture axes. We find that LLMs understand the notion of diversity, and that
they can reason and critique their own responses for that goal. This finding
motivated a new prompting technique called collective-critique and self-voting
(CCSV) to self-improve people diversity of LLMs by tapping into its diversity
reasoning capabilities, without relying on handcrafted examples or prompt
tuning. Extensive empirical experiments with both human and automated
evaluations show that our proposed approach is effective at improving people
and culture diversity, and outperforms all baseline methods by a large margin.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16524" title="Abstract">arXiv:2310.16524</a> [<a href="/pdf/2310.16524" title="Download PDF">pdf</a>, <a href="/format/2310.16524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can You Rely on Your Model Evaluation? Improving Model Evaluation with  Synthetic Test Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Breugel%2C+B">Boris van Breugel</a>, 
<a href="/search/cs?searchtype=author&query=Seedat%2C+N">Nabeel Seedat</a>, 
<a href="/search/cs?searchtype=author&query=Imrie%2C+F">Fergus Imrie</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Advances in Neural Information Processing Systems 36 (NeurIPS 2023). Van Breugel &amp; Seedat contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Evaluating the performance of machine learning models on diverse and
underrepresented subgroups is essential for ensuring fairness and reliability
in real-world applications. However, accurately assessing model performance
becomes challenging due to two main issues: (1) a scarcity of test data,
especially for small subgroups, and (2) possible distributional shifts in the
model's deployment setting, which may not align with the available test data.
In this work, we introduce 3S Testing, a deep generative modeling framework to
facilitate model evaluation by generating synthetic test sets for small
subgroups and simulating distributional shifts. Our experiments demonstrate
that 3S Testing outperforms traditional baselines -- including real test data
alone -- in estimating model performance on minority subgroups and under
plausible distributional shifts. In addition, 3S offers intervals around its
performance estimates, exhibiting superior coverage of the ground truth
compared to existing approaches. Overall, these results raise the question of
whether we need a paradigm shift away from limited real test data towards
synthetic test data.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16525" title="Abstract">arXiv:2310.16525</a> [<a href="/pdf/2310.16525" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cyclic Directed Probabilistic Graphical Model: A Proposal Based on  Structured Outcomes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sirotkin%2C+O">Oleksii Sirotkin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 11 figures, <a href="/abs/2206.06089">arXiv:2206.06089v1</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In the process of building (structural learning) a probabilistic graphical
model from a set of observed data, the directional, cyclic dependencies between
the random variables of the model are often found. Existing graphical models
such as Bayesian and Markov networks can reflect such dependencies. However,
this requires complicating those models, such as adding additional variables or
dividing the model graph into separate subgraphs. Herein, we describe a
probabilistic graphical model - probabilistic relation network - that allows
the direct capture of directional cyclic dependencies during structural
learning. This model is based on the simple idea that each sample of the
observed data can be represented by an arbitrary graph (structured outcome),
which reflects the structure of the dependencies of the variables included in
the sample. Each of the outcomes contains only a part of the graphical model
structure; however, a complete graph of the probabilistic model is obtained by
combining different outcomes. Such a graph, unlike Bayesian and Markov
networks, can be directed and can have cycles. We explored the full joint
distribution and conditional distribution and conditional independence
properties of variables in the proposed model. We defined the algorithms for
constructing of the model from the dataset and for calculating the conditional
and full joint distributions. We also performed a numerical comparison with
Bayesian and Markov networks. This model does not violate the probability
axioms, and it supports learning from observed data. Notably, it supports
probabilistic inference, making it a prospective tool in data analysis and in
expert and design-making applications.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16527" title="Abstract">arXiv:2310.16527</a> [<a href="/pdf/2310.16527" title="Download PDF">pdf</a>, <a href="/format/2310.16527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Document Information Analysis with Multi-Task Pre-training: A  Robust Approach for Information Extraction in Visually-Rich Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ali%2C+T">Tofik Ali</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+P+P">Partha Pratim Roy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper introduces a deep learning model tailored for document information
analysis, emphasizing document classification, entity relation extraction, and
document visual question answering. The proposed model leverages
transformer-based models to encode all the information present in a document
image, including textual, visual, and layout information. The model is
pre-trained and subsequently fine-tuned for various document image analysis
tasks. The proposed model incorporates three additional tasks during the
pre-training phase, including reading order identification of different layout
segments in a document image, layout segments categorization as per PubLayNet,
and generation of the text sequence within a given layout segment (text block).
The model also incorporates a collective pre-training scheme where losses of
all the tasks under consideration, including pre-training and fine-tuning tasks
with all datasets, are considered. Additional encoder and decoder blocks are
added to the RoBERTa network to generate results for all tasks. The proposed
model achieved impressive results across all tasks, with an accuracy of 95.87%
on the RVL-CDIP dataset for document classification, F1 scores of 0.9306,
0.9804, 0.9794, and 0.8742 on the FUNSD, CORD, SROIE, and Kleister-NDA datasets
respectively for entity relation extraction, and an ANLS score of 0.8468 on the
DocVQA dataset for visual question answering. The results highlight the
effectiveness of the proposed model in understanding and interpreting complex
document layouts and content, making it a promising tool for document analysis
tasks.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16528" title="Abstract">arXiv:2310.16528</a> [<a href="/pdf/2310.16528" title="Download PDF">pdf</a>, <a href="/format/2310.16528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CUNI Submission to MRL 2023 Shared Task on Multi-lingual Multi-task  Information Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Helcl%2C+J">Jind&#x159;ich Helcl</a>, 
<a href="/search/cs?searchtype=author&query=Libovick%C3%BD%2C+J">Jind&#x159;ich Libovick&#xfd;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures; System description paper at the MRL 2023 workshop at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present the Charles University system for the MRL~2023 Shared Task on
Multi-lingual Multi-task Information Retrieval. The goal of the shared task was
to develop systems for named entity recognition and question answering in
several under-represented languages. Our solutions to both subtasks rely on the
translate-test approach. We first translate the unlabeled examples into English
using a multilingual machine translation model. Then, we run inference on the
translated data using a strong task-specific model. Finally, we project the
labeled data back into the original language. To keep the inferred tags on the
correct positions in the original language, we propose a method based on
scoring the candidate positions using a label-sensitive translation model. In
both settings, we experiment with finetuning the classification models on the
translated data. However, due to a domain mismatch between the development data
and the shared task validation and test sets, the finetuned models could not
outperform our baselines.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16530" title="Abstract">arXiv:2310.16530</a> [<a href="/pdf/2310.16530" title="Download PDF">pdf</a>, <a href="/format/2310.16530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Practical Privacy-Preserving Convolutional Neural Networks  Exploiting Fully Homomorphic Encryption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jaiyoung Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Donghwan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jongmin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sangpyo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+W">Wonkyung Jung</a>, 
<a href="/search/cs?searchtype=author&query=Cheon%2C+J+H">Jung Hee Cheon</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+J+H">Jung Ho Ahn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, 1 figure, appears at DISCC 2023 (2nd Workshop on Data Integrity and Secure Cloud Computing, in conjunction with the 56th International Symposium on Microarchitecture (MICRO 2023))
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Incorporating fully homomorphic encryption (FHE) into the inference process
of a convolutional neural network (CNN) draws enormous attention as a viable
approach for achieving private inference (PI). FHE allows delegating the entire
computation process to the server while ensuring the confidentiality of
sensitive client-side data. However, practical FHE implementation of a CNN
faces significant hurdles, primarily due to FHE's substantial computational and
memory overhead. To address these challenges, we propose a set of
optimizations, which includes GPU/ASIC acceleration, an efficient activation
function, and an optimized packing scheme. We evaluate our method using the
ResNet models on the CIFAR-10 and ImageNet datasets, achieving several orders
of magnitude improvement compared to prior work and reducing the latency of the
encrypted CNN inference to 1.4 seconds on an NVIDIA A100 GPU. We also show that
the latency drops to a mere 0.03 seconds with a custom hardware design.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16531" title="Abstract">arXiv:2310.16531</a> [<a href="/pdf/2310.16531" title="Download PDF">pdf</a>, <a href="/ps/2310.16531" title="Download PostScript">ps</a>, <a href="/format/2310.16531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pretty Good Strategies and Where to Find Them
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Kurpiewski%2C+D">Damian Kurpiewski</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Lecture Notes in Computer Science 14282 (2023), 363--380
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Synthesis of bulletproof strategies in imperfect information scenarios is a
notoriously hard problem. In this paper, we suggest that it is sometimes a
viable alternative to aim at "reasonably good" strategies instead. This makes
sense not only when an ideal strategy cannot be found due to the complexity of
the problem, but also when no winning strategy exists at all. We propose an
algorithm for synthesis of such "pretty good" strategies. The idea is to first
generate a surely winning strategy with perfect information, and then
iteratively improve it with respect to two criteria of dominance: one based on
the amount of conflicting decisions in the strategy, and the other related to
the tightness of its outcome set. We focus on reachability goals and evaluate
the algorithm experimentally with very promising results.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16532" title="Abstract">arXiv:2310.16532</a> [<a href="/pdf/2310.16532" title="Download PDF">pdf</a>, <a href="/format/2310.16532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Robust Deep Visual Representations from EEG Brain Recordings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+P">Prajwal Singh</a>, 
<a href="/search/cs?searchtype=author&query=Dalal%2C+D">Dwip Dalal</a>, 
<a href="/search/cs?searchtype=author&query=Vashishtha%2C+G">Gautam Vashishtha</a>, 
<a href="/search/cs?searchtype=author&query=Miyapuram%2C+K">Krishna Miyapuram</a>, 
<a href="/search/cs?searchtype=author&query=Raman%2C+S">Shanmuganathan Raman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Decoding the human brain has been a hallmark of neuroscientists and
Artificial Intelligence researchers alike. Reconstruction of visual images from
brain Electroencephalography (EEG) signals has garnered a lot of interest due
to its applications in brain-computer interfacing. This study proposes a
two-stage method where the first step is to obtain EEG-derived features for
robust learning of deep representations and subsequently utilize the learned
representation for image generation and classification. We demonstrate the
generalizability of our feature extraction pipeline across three different
datasets using deep-learning architectures with supervised and contrastive
learning methods. We have performed the zero-shot EEG classification task to
support the generalizability claim further. We observed that a subject
invariant linearly separable visual representation was learned using EEG data
alone in an unimodal setting that gives better k-means accuracy as compared to
a joint representation learning between EEG and images. Finally, we propose a
novel framework to transform unseen images into the EEG space and reconstruct
them with approximation, showcasing the potential for image reconstruction from
EEG signals. Our proposed image synthesis method from EEG shows 62.9% and
36.13% inception score improvement on the EEGCVPR40 and the Thoughtviz
datasets, which is better than state-of-the-art performance in GAN.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16534" title="Abstract">arXiv:2310.16534</a> [<a href="/pdf/2310.16534" title="Download PDF">pdf</a>, <a href="/format/2310.16534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Early Evaluation of GPT-4V(ision)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shilong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tian Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongbo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yanyan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bing Qin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report. Data are available at <a href="https://github.com/albertwy/GPT-4V-Evaluation">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this paper, we evaluate different abilities of GPT-4V including visual
understanding, language understanding, visual puzzle solving, and understanding
of other modalities such as depth, thermal, video, and audio. To estimate
GPT-4V's performance, we manually construct 656 test instances and carefully
evaluate the results of GPT-4V. The highlights of our findings are as follows:
(1) GPT-4V exhibits impressive performance on English visual-centric benchmarks
but fails to recognize simple Chinese texts in the images; (2) GPT-4V shows
inconsistent refusal behavior when answering questions related to sensitive
traits such as gender, race, and age; (3) GPT-4V obtains worse results than
GPT-4 (API) on language understanding tasks including general language
understanding benchmarks and visual commonsense knowledge evaluation
benchmarks; (4) Few-shot prompting can improve GPT-4V's performance on both
visual understanding and language understanding; (5) GPT-4V struggles to find
the nuances between two similar images and solve the easy math picture puzzles;
(6) GPT-4V shows non-trivial performance on the tasks of similar modalities to
image, such as video and thermal. Our experimental results reveal the ability
and limitations of GPT-4V and we hope our paper can provide some insights into
the application and research of GPT-4V.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16535" title="Abstract">arXiv:2310.16535</a> [<a href="/pdf/2310.16535" title="Download PDF">pdf</a>, <a href="/format/2310.16535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought  Reasoning in Large Language Models under Noisy Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Qingyuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanlun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yunshi Lan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the help of Chain-of-Thought (CoT) prompting, Large Language Models
(LLMs) have achieved remarkable performance on various reasoning tasks.
However, most of them have been evaluated under noise-free context and the
dilemma for LLMs to produce inaccurate results under the noisy context has not
been fully investigated. Existing studies utilize trigger sentences to
encourage LLMs to concentrate on the relevant information but the trigger has
limited effect on final answer prediction. Inspired by interactive CoT method,
where intermediate reasoning steps are promoted by multiple rounds of
interaction between users and LLMs, we propose a novel prompting method, namely
R$^3$ prompting, for CoT reasoning under noisy context. Specifically, R$^3$
prompting interacts with LLMs to perform key sentence extraction, variable
declaration and answer prediction, which corresponds to a thought process of
reviewing, rephrasing and resolving. The responses generated at the last
interaction will perform as hints to guide toward the responses of the next
interaction. Our experiments show that R$^3$ prompting significantly
outperforms existing CoT prompting methods on five reasoning tasks under noisy
context. With GPT-3.5-turbo, we observe 3.7% accuracy improvement on average on
the reasoning tasks under noisy context compared to the most competitive
prompting baseline. More analyses and ablation studies show the robustness and
generalization of R$^3$ prompting method in solving reasoning tasks in LLMs
under noisy context.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16537" title="Abstract">arXiv:2310.16537</a> [<a href="/pdf/2310.16537" title="Download PDF">pdf</a>, <a href="/ps/2310.16537" title="Download PostScript">ps</a>, <a href="/format/2310.16537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A characterization of linear independence of THB-splines in  $\mathbb{R}^n$ and application to B&#xe9;zier projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dijkstra%2C+K">Kevin Dijkstra</a>, 
<a href="/search/math?searchtype=author&query=Toshniwal%2C+D">Deepesh Toshniwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper we propose a local projector for truncated hierarchical
B-splines (THB-splines). The local THB-spline projector is an adaptation of the
B\'ezier projector proposed by Thomas et al. (Comput Methods Appl Mech Eng 284,
2015) for B-splines and analysis-suitable T-splines (AS T-splines). For
THB-splines, there are elements on which the restrictions of THB-splines are
linearly dependent, contrary to B-splines and AS T-splines. Therefore, we
cluster certain local mesh elements together such that the THB-splines with
support over these clusters are linearly independent, and the B\'ezier
projector is adapted to use these clusters. We introduce general extensions for
which optimal convergence is shown theoretically and numerically. In addition,
a simple adaptive refinement scheme is introduced and compared to Giust et al.
(Comput. Aided Geom. Des. 80, 2020), where we find that our simple approach
shows promise.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16538" title="Abstract">arXiv:2310.16538</a> [<a href="/pdf/2310.16538" title="Download PDF">pdf</a>, <a href="/format/2310.16538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedTherapist: Mental Health Monitoring with User-Generated Linguistic  Expressions on Smartphones via Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jaemin Shin</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+H">Hyungjun Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seungjoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sungjoon Park</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yunxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J+D">Jinho D. Choi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sung-Ju Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Psychiatrists diagnose mental disorders via the linguistic use of patients.
Still, due to data privacy, existing passive mental health monitoring systems
use alternative features such as activity, app usage, and location via mobile
devices. We propose FedTherapist, a mobile mental health monitoring system that
utilizes continuous speech and keyboard input in a privacy-preserving way via
federated learning. We explore multiple model designs by comparing their
performance and overhead for FedTherapist to overcome the complex nature of
on-device language model training on smartphones. We further propose a
Context-Aware Language Learning (CALL) methodology to effectively utilize
smartphones' large and noisy text for mental health signal sensing. Our
IRB-approved evaluation of the prediction of self-reported depression, stress,
anxiety, and mood from 46 participants shows higher accuracy of FedTherapist
compared with the performance with non-language features, achieving 0.15 AUROC
improvement and 8.21% MAE reduction.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16540" title="Abstract">arXiv:2310.16540</a> [<a href="/pdf/2310.16540" title="Download PDF">pdf</a>, <a href="/format/2310.16540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual Defense: Adversarial, Traceable, and Invisible Robust Watermarking  against Face Swapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+D">Dengpan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Caiyun Xie</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+L">Long Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chuanxi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jiacheng Deng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The malicious applications of deep forgery, represented by face swapping,
have introduced security threats such as misinformation dissemination and
identity fraud. While some research has proposed the use of robust watermarking
methods to trace the copyright of facial images for post-event traceability,
these methods cannot effectively prevent the generation of forgeries at the
source and curb their dissemination. To address this problem, we propose a
novel comprehensive active defense mechanism that combines traceability and
adversariality, called Dual Defense. Dual Defense invisibly embeds a single
robust watermark within the target face to actively respond to sudden cases of
malicious face swapping. It disrupts the output of the face swapping model
while maintaining the integrity of watermark information throughout the entire
dissemination process. This allows for watermark extraction at any stage of
image tracking for traceability. Specifically, we introduce a watermark
embedding network based on original-domain feature impersonation attack. This
network learns robust adversarial features of target facial images and embeds
watermarks, seeking a well-balanced trade-off between watermark invisibility,
adversariality, and traceability through perceptual adversarial encoding
strategies. Extensive experiments demonstrate that Dual Defense achieves
optimal overall defense success rates and exhibits promising universality in
anti-face swapping tasks and dataset generalization ability. It maintains
impressive adversariality and traceability in both original and robust
settings, surpassing current forgery defense methods that possess only one of
these capabilities, including CMUA-Watermark, Anti-Forgery, FakeTagger, or PGD
methods.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16542" title="Abstract">arXiv:2310.16542</a> [<a href="/pdf/2310.16542" title="Download PDF">pdf</a>, <a href="/format/2310.16542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ParisLuco3D: A high-quality target dataset for domain generalization of  LiDAR perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sanchez%2C+J">Jules Sanchez</a>, 
<a href="/search/cs?searchtype=author&query=Soum-Fontez%2C+L">Louis Soum-Fontez</a>, 
<a href="/search/cs?searchtype=author&query=Deschaud%2C+J">Jean-Emmanuel Deschaud</a>, 
<a href="/search/cs?searchtype=author&query=Goulette%2C+F">Francois Goulette</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">LiDAR is a sensor system that supports autonomous driving by gathering
precise geometric information about the scene. Exploiting this information for
perception is interesting as the amount of available data increases.
<br />As the quantitative performance of various perception tasks has improved, the
focus has shifted from source-to-source perception to domain adaptation and
domain generalization for perception. These new goals require access to a large
variety of domains for evaluation. Unfortunately, the various annotation
strategies of data providers complicate the computation of cross-domain
performance based on the available data
<br />This paper provides a novel dataset, specifically designed for cross-domain
evaluation to make it easier to evaluate the performance of various source
datasets. Alongside the dataset, a flexible online benchmark is provided to
ensure a fair comparison across methods.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16546" title="Abstract">arXiv:2310.16546</a> [<a href="/pdf/2310.16546" title="Download PDF">pdf</a>, <a href="/format/2310.16546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pitfall of Optimism: Distributional Reinforcement Learning by  Randomizing Risk Criterion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+T">Taehyun Cho</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Seungyub Han</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Heesoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kyungjae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jungwoo Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Distributional reinforcement learning algorithms have attempted to utilize
estimated uncertainty for exploration, such as optimism in the face of
uncertainty. However, using the estimated variance for optimistic exploration
may cause biased data collection and hinder convergence or performance. In this
paper, we present a novel distributional reinforcement learning algorithm that
selects actions by randomizing risk criterion to avoid one-sided tendency on
risk. We provide a perturbed distributional Bellman optimality operator by
distorting the risk measure and prove the convergence and optimality of the
proposed method with the weaker contraction property. Our theoretical results
support that the proposed method does not fall into biased exploration and is
guaranteed to converge to an optimal return. Finally, we empirically show that
our method outperforms other existing distribution-based algorithms in various
environments including Atari 55 games.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16547" title="Abstract">arXiv:2310.16547</a> [<a href="/pdf/2310.16547" title="Download PDF">pdf</a>, <a href="/format/2310.16547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaMEC: Towards a Context-Adaptive and Dynamically-Combinable DNN  Deployment Framework for Mobile Edge Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+B">Bowen Pang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sicong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongli Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+B">Bin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuzhan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Z">Zhenli Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhiwen Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">With the rapid development of deep learning, recent research on intelligent
and interactive mobile applications (e.g., health monitoring, speech
recognition) has attracted extensive attention. And these applications
necessitate the mobile edge computing scheme, i.e., offloading partial
computation from mobile devices to edge devices for inference acceleration and
transmission load reduction. The current practices have relied on collaborative
DNN partition and offloading to satisfy the predefined latency requirements,
which is intractable to adapt to the dynamic deployment context at runtime.
AdaMEC, a context-adaptive and dynamically-combinable DNN deployment framework
is proposed to meet these requirements for mobile edge computing, which
consists of three novel techniques. First, once-for-all DNN pre-partition
divides DNN at the primitive operator level and stores partitioned modules into
executable files, defined as pre-partitioned DNN atoms. Second,
context-adaptive DNN atom combination and offloading introduces a graph-based
decision algorithm to quickly search the suitable combination of atoms and
adaptively make the offloading plan under dynamic deployment contexts. Third,
runtime latency predictor provides timely latency feedback for DNN deployment
considering both DNN configurations and dynamic contexts. Extensive experiments
demonstrate that AdaMEC outperforms state-of-the-art baselines in terms of
latency reduction by up to 62.14% and average memory saving by 55.21%.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16548" title="Abstract">arXiv:2310.16548</a> [<a href="/pdf/2310.16548" title="Download PDF">pdf</a>, <a href="/format/2310.16548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Terahertz-Enpowered Communications and Sensing in 6G Systems:  Opportunities and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Schotten%2C+H+D">Hans D. Schotten</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 the 9th International Conference on Computer and Communications (ICCC). arXiv admin note: text overlap with <a href="/abs/2307.10321">arXiv:2307.10321</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">The current focus of academia and the telecommunications industry has been
shifted to the development of the six-generation (6G) cellular technology, also
formally referred to as IMT-2030. Unprecedented applications that 6G aims to
accommodate demand extreme communications performance and, in addition,
disruptive capabilities such as network sensing. Recently, there has been a
surge of interest in terahertz (THz) frequencies as it offers not only massive
spectral resources for communication but also distinct advantages in sensing,
positioning, and imaging. The aim of this paper is to provide a brief outlook
on opportunities opened by this under-exploited band and challenges that must
be addressed to materialize the potential of THz-based communications and
sensing in 6G systems.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16550" title="Abstract">arXiv:2310.16550</a> [<a href="/pdf/2310.16550" title="Download PDF">pdf</a>, <a href="/format/2310.16550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Processing Neural Network Architecture For Hearing Loss  Compensation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Drgas%2C+S">Szymon Drgas</a>, 
<a href="/search/cs?searchtype=author&query=Bramsl%C3%B8w%2C+L">Lars Bramsl&#xf8;w</a>, 
<a href="/search/cs?searchtype=author&query=Politis%2C+A">Archontis Politis</a>, 
<a href="/search/cs?searchtype=author&query=Naithani%2C+G">Gaurav Naithani</a>, 
<a href="/search/cs?searchtype=author&query=Virtanen%2C+T">Tuomas Virtanen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper proposes neural networks for compensating sensorineural hearing
loss. The aim of the hearing loss compensation task is to transform a speech
signal to increase speech intelligibility after further processing by a person
with a hearing impairment, which is modeled by a hearing loss model. We propose
an interpretable model called dynamic processing network, which has a structure
similar to band-wise dynamic compressor. The network is differentiable, and
therefore allows to learn its parameters to maximize speech intelligibility.
More generic models based on convolutional layers were tested as well. The
performance of the tested architectures was assessed using spectro-temporal
objective index (STOI) with hearing-threshold noise and hearing aid speech
intelligibility (HASPI) metrics. The dynamic processing network gave a
significant improvement of STOI and HASPI in comparison to popular compressive
gain prescription rule Camfit. A large enough convolutional network could
outperform the interpretable model with the cost of larger computational load.
Finally, a combination of the dynamic processing network with convolutional
neural network gave the best results in terms of STOI and HASPI.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16552" title="Abstract">arXiv:2310.16552</a> [<a href="/pdf/2310.16552" title="Download PDF">pdf</a>, <a href="/ps/2310.16552" title="Download PostScript">ps</a>, <a href="/format/2310.16552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DECWA : Density-Based Clustering using Wasserstein Distance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malki%2C+N+E">Nabil El Malki</a>, 
<a href="/search/cs?searchtype=author&query=Cugny%2C+R">Robin Cugny</a>, 
<a href="/search/cs?searchtype=author&query=Teste%2C+O">Olivier Teste</a>, 
<a href="/search/cs?searchtype=author&query=Ravat%2C+F">Franck Ravat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, CIKM 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Clustering is a data analysis method for extracting knowledge by discovering
groups of data called clusters. Among these methods, state-of-the-art
density-based clustering methods have proven to be effective for
arbitrary-shaped clusters. Despite their encouraging results, they suffer to
find low-density clusters, near clusters with similar densities, and
high-dimensional data. Our proposals are a new characterization of clusters and
a new clustering algorithm based on spatial density and probabilistic approach.
First of all, sub-clusters are built using spatial density represented as
probability density function ($p.d.f$) of pairwise distances between points. A
method is then proposed to agglomerate similar sub-clusters by using both their
density ($p.d.f$) and their spatial distance. The key idea we propose is to use
the Wasserstein metric, a powerful tool to measure the distance between $p.d.f$
of sub-clusters. We show that our approach outperforms other state-of-the-art
density-based clustering methods on a wide variety of datasets.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16555" title="Abstract">arXiv:2310.16555</a> [<a href="/pdf/2310.16555" title="Download PDF">pdf</a>, <a href="/ps/2310.16555" title="Download PostScript">ps</a>, <a href="/format/2310.16555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Information Theory-Based Discovery of Equivariances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Charvin%2C+H">Hippolyte Charvin</a>, 
<a href="/search/cs?searchtype=author&query=Volpi%2C+N+C">Nicola Catenacci Volpi</a>, 
<a href="/search/cs?searchtype=author&query=Polani%2C+D">Daniel Polani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 0 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Neural and Evolutionary Computing (cs.NE); Group Theory (math.GR)

</div>
<p class="mathjax">The presence of symmetries imposes a stringent set of constraints on a
system. This constrained structure allows intelligent agents interacting with
such a system to drastically improve the efficiency of learning and
generalization, through the internalisation of the system's symmetries into
their information-processing. In parallel, principled models of
complexity-constrained learning and behaviour make increasing use of
information-theoretic methods. Here, we wish to marry these two perspectives
and understand whether and in which form the information-theoretic lens can
"see" the effect of symmetries of a system. For this purpose, we propose a
novel variant of the Information Bottleneck principle, which has served as a
productive basis for many principled studies of learning and
information-constrained adaptive behaviour. We show (in the discrete case) that
our approach formalises a certain duality between symmetry and information
parsimony: namely, channel equivariances can be characterised by the optimal
mutual information-preserving joint compression of the channel's input and
output. This information-theoretic treatment furthermore suggests a principled
notion of "soft" equivariance, whose "coarseness" is measured by the amount of
input-output mutual information preserved by the corresponding optimal
compression. This new notion offers a bridge between the field of bounded
rationality and the study of symmetries in neural representations. The
framework may also allow (exact and soft) equivariances to be automatically
discovered.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16559" title="Abstract">arXiv:2310.16559</a> [<a href="/pdf/2310.16559" title="Download PDF">pdf</a>, <a href="/ps/2310.16559" title="Download PostScript">ps</a>, <a href="/format/2310.16559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-structure Objects Points-to Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+X">Xun An</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">An important dimension of pointer analysis is field-Sensitive, which has been
proven to effectively enhance the accuracy of pointer analysis results. A
crucial area of research within field-Sensitive is Structure-Sensitive.
Structure-Sensitive has been shown to further enhance the precision of pointer
analysis. However, existing structure-sensitive methods cannot handle cases
where an object possesses multiple structures, even though it's common for an
object to have multiple structures throughout its lifecycle. This paper
introduces MTO-SS, a flow-sensitive pointer analysis method for objects with
multiple structures. Our observation is that it's common for an object to
possess multiple structures throughout its lifecycle. The novelty of MTO-SS
lies in: MTO-SS introduces Structure-Flow-Sensitive. An object has different
structure information at different locations in the program. To ensure the
completeness of an object's structure information, MTO-SS always performs weak
updates on the object's type. This means that once an object possesses a
structure, this structure will accompany the object throughout its lifecycle.
We evaluated our method of multi-structured object pointer analysis using the
12 largest programs in GNU Coreutils and compared the experimental results with
sparse flow-sensitive method and another method, TYPECLONE, which only allows
an object to have one structure information. Our experimental results confirm
that MTO-SS is more precise than both sparse flow-sensitive pointer analysis
and TYPECLONE, being able to answer, on average, over 22\% more alias queries
with a no-alias result compared to the former, and over 3\% more compared to
the latter. Additionally, the time overhead introduced by our method is very
low.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16560" title="Abstract">arXiv:2310.16560</a> [<a href="/pdf/2310.16560" title="Download PDF">pdf</a>, <a href="/format/2310.16560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label Propagation for Graph Label Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+C">Caihua Shan</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yifei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Siqiang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongsheng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Label noise is a common challenge in large datasets, as it can significantly
degrade the generalization ability of deep neural networks. Most existing
studies focus on noisy labels in computer vision; however, graph models
encompass both node features and graph topology as input, and become more
susceptible to label noise through message-passing mechanisms. Recently, only a
few works have been proposed to tackle the label noise on graphs. One major
limitation is that they assume the graph is homophilous and the labels are
smoothly distributed. Nevertheless, real-world graphs may contain varying
degrees of heterophily or even be heterophily-dominated, leading to the
inadequacy of current methods. In this paper, we study graph label noise in the
context of arbitrary heterophily, with the aim of rectifying noisy labels and
assigning labels to previously unlabeled nodes. We begin by conducting two
empirical analyses to explore the impact of graph homophily on graph label
noise. Following observations, we propose a simple yet efficient algorithm,
denoted as LP4GLN. Specifically, LP4GLN is an iterative algorithm with three
steps: (1) reconstruct the graph to recover the homophily property, (2) utilize
label propagation to rectify the noisy labels, (3) select high-confidence
labels to retain for the next iteration. By iterating these steps, we obtain a
set of correct labels, ultimately achieving high accuracy in the node
classification task. The theoretical analysis is also provided to demonstrate
its remarkable denoising "effect". Finally, we conduct experiments on 10
benchmark datasets under varying graph heterophily levels and noise types,
comparing the performance of LP4GLN with 7 typical baselines. Our results
illustrate the superior performance of the proposed LP4GLN.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16562" title="Abstract">arXiv:2310.16562</a> [<a href="/pdf/2310.16562" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trustworthy Cross-Border Interoperable Identity System for Developing  Countries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ibor%2C+A+E">Ayei E. Ibor</a>, 
<a href="/search/cs?searchtype=author&query=Hooper%2C+M">Mark Hooper</a>, 
<a href="/search/cs?searchtype=author&query=Maple%2C+C">Carsten Maple</a>, 
<a href="/search/cs?searchtype=author&query=Epiphaniou%2C+G">Gregory Epiphaniou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 4 figures, In 2023 Trustworthy Digital Identity International Conference, Bengaluru, India
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Foundational identity systems (FIDS) have been used to optimise service
delivery and inclusive economic growth in developing countries. As developing
nations increasingly seek to use FIDS for the identification and authentication
of identity (ID) holders, trustworthy interoperability will help to develop a
cross-border dimension of e-Government. Despite this potential, there has not
been any significant research on the interoperability of FIDS in the African
identity ecosystem. There are several challenges to this; on one hand, complex
internal political dynamics have resulted in weak institutions, implying that
FIDS could be exploited for political gains. On the other hand, the trust in
the government by the citizens or ID holders is habitually low, in which case,
data security and privacy protection concerns become paramount. In the same
sense, some FIDS are technology-locked, thus interoperability is primarily
ambiguous. There are also issues of cross-system compatibility, legislation,
vendor-locked system design principles and unclear regulatory provisions for
data sharing. Fundamentally, interoperability is an essential prerequisite for
e-Government services and underpins optimal service delivery in education,
social security, and financial services including gender and equality as
already demonstrated by the European Union. Furthermore, cohesive data exchange
through an interoperable identity system will create an ecosystem of efficient
data governance and the integration of cross-border FIDS. Consequently, this
research identifies the challenges, opportunities, and requirements for
cross-border interoperability in an African context. Our findings show that
interoperability in the African identity ecosystem is vital to strengthen the
seamless authentication and verification of ID holders for inclusive economic
growth and widen the dimensions of e-Government across the continent.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16566" title="Abstract">arXiv:2310.16566</a> [<a href="/pdf/2310.16566" title="Download PDF">pdf</a>, <a href="/format/2310.16566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-enhanced Contrastive Reinforcement Learning for Sequential  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengpeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhengyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jizhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiancan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dingxian Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiangnan He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Reinforcement learning (RL) has been widely applied in recommendation systems
due to its potential in optimizing the long-term engagement of users. From the
perspective of RL, recommendation can be formulated as a Markov decision
process (MDP), where recommendation system (agent) can interact with users
(environment) and acquire feedback (reward signals).However, it is impractical
to conduct online interactions with the concern on user experience and
implementation complexity, and we can only train RL recommenders with offline
datasets containing limited reward signals and state transitions. Therefore,
the data sparsity issue of reward signals and state transitions is very severe,
while it has long been overlooked by existing RL recommenders.Worse still, RL
methods learn through the trial-and-error mode, but negative feedback cannot be
obtained in implicit feedback recommendation tasks, which aggravates the
overestimation problem of offline RL recommender. To address these challenges,
we propose a novel RL recommender named model-enhanced contrastive
reinforcement learning (MCRL). On the one hand, we learn a value function to
estimate the long-term engagement of users, together with a conservative value
learning mechanism to alleviate the overestimation problem.On the other hand,
we construct some positive and negative state-action pairs to model the reward
function and state transition function with contrastive learning to exploit the
internal structure information of MDP. Experiments demonstrate that the
proposed method significantly outperforms existing offline RL and
self-supervised RL methods with different representative backbone networks on
two real-world datasets.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16568" title="Abstract">arXiv:2310.16568</a> [<a href="/pdf/2310.16568" title="Download PDF">pdf</a>, <a href="/format/2310.16568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 1-PAGER: One Pass Answer Generation and Evidence Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+P">Palak Jain</a>, 
<a href="/search/cs?searchtype=author&query=Soares%2C+L+B">Livio Baldini Soares</a>, 
<a href="/search/cs?searchtype=author&query=Kwiatkowski%2C+T">Tom Kwiatkowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present 1-Pager the first system that answers a question and retrieves
evidence using a single Transformer-based model and decoding process. 1-Pager
incrementally partitions the retrieval corpus using constrained decoding to
select a document and answer string, and we show that this is competitive with
comparable retrieve-and-read alternatives according to both retrieval and
answer accuracy metrics. 1-Pager also outperforms the equivalent closed-book
question answering model, by grounding predictions in an evidence corpus. While
1-Pager is not yet on-par with more expensive systems that read many more
documents before generating an answer, we argue that it provides an important
step toward attributed generation by folding retrieval into the
sequence-to-sequence paradigm that is currently dominant in NLP. We also show
that the search paths used to partition the corpus are easy to read and
understand, paving a way forward for interpretable neural retrieval.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16569" title="Abstract">arXiv:2310.16569</a> [<a href="/pdf/2310.16569" title="Download PDF">pdf</a>, <a href="/format/2310.16569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flow-Attention-based Spatio-Temporal Aggregation Network for 3D Mask  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yuxin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yumeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Derui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+M">Minhui Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures. Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Anti-spoofing detection has become a necessity for face recognition systems
due to the security threat posed by spoofing attacks. Despite great success in
traditional attacks, most deep-learning-based methods perform poorly in 3D
masks, which can highly simulate real faces in appearance and structure,
suffering generalizability insufficiency while focusing only on the spatial
domain with single frame input. This has been mitigated by the recent
introduction of a biomedical technology called rPPG (remote
photoplethysmography). However, rPPG-based methods are sensitive to noisy
interference and require at least one second (&gt; 25 frames) of observation time,
which induces high computational overhead. To address these challenges, we
propose a novel 3D mask detection framework, called FASTEN
(Flow-Attention-based Spatio-Temporal aggrEgation Network). We tailor the
network for focusing more on fine-grained details in large movements, which can
eliminate redundant spatio-temporal feature interference and quickly capture
splicing traces of 3D masks in fewer frames. Our proposed network contains
three key modules: 1) a facial optical flow network to obtain non-RGB
inter-frame flow information; 2) flow attention to assign different
significance to each frame; 3) spatio-temporal aggregation to aggregate
high-level spatial features and temporal transition features. Through extensive
experiments, FASTEN only requires five frames of input and outperforms eight
competitors for both intra-dataset and cross-dataset evaluations in terms of
multiple detection metrics. Moreover, FASTEN has been deployed in real-world
mobile devices for practical 3D mask detection.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16570" title="Abstract">arXiv:2310.16570</a> [<a href="/pdf/2310.16570" title="Download PDF">pdf</a>, <a href="/format/2310.16570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Youssef%2C+P">Paul Youssef</a>, 
<a href="/search/cs?searchtype=author&query=Kora%C5%9F%2C+O+A">Osman Alperen Kora&#x15f;</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Meijie Li</a>, 
<a href="/search/cs?searchtype=author&query=Schl%C3%B6tterer%2C+J">J&#xf6;rg Schl&#xf6;tterer</a>, 
<a href="/search/cs?searchtype=author&query=Seifert%2C+C">Christin Seifert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pre-trained Language Models (PLMs) are trained on vast unlabeled data, rich
in world knowledge. This fact has sparked the interest of the community in
quantifying the amount of factual knowledge present in PLMs, as this explains
their performance on downstream tasks, and potentially justifies their use as
knowledge bases. In this work, we survey methods and datasets that are used to
probe PLMs for factual knowledge. Our contributions are: (1) We propose a
categorization scheme for factual probing methods that is based on how their
inputs, outputs and the probed PLMs are adapted; (2) We provide an overview of
the datasets used for factual probing; (3) We synthesize insights about
knowledge retention and prompt optimization in PLMs, analyze obstacles to
adopting PLMs as knowledge bases and outline directions for future work.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16572" title="Abstract">arXiv:2310.16572</a> [<a href="/pdf/2310.16572" title="Download PDF">pdf</a>, <a href="/ps/2310.16572" title="Download PostScript">ps</a>, <a href="/format/2310.16572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correctness Witness Validation by Abstract Interpretation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saan%2C+S">Simmo Saan</a>, 
<a href="/search/cs?searchtype=author&query=Schwarz%2C+M">Michael Schwarz</a>, 
<a href="/search/cs?searchtype=author&query=Erhard%2C+J">Julian Erhard</a>, 
<a href="/search/cs?searchtype=author&query=Seidl%2C+H">Helmut Seidl</a>, 
<a href="/search/cs?searchtype=author&query=Tilscher%2C+S">Sarah Tilscher</a>, 
<a href="/search/cs?searchtype=author&query=Vojdani%2C+V">Vesal Vojdani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 4 figures, 2 tables, extended version of the paper which is to appear at VMCAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Witnesses record automated program analysis results and make them
exchangeable. To validate correctness witnesses through abstract
interpretation, we introduce a novel abstract operation unassume. This operator
incorporates witness invariants into the abstract program state. Given suitable
invariants, the unassume operation can accelerate fixpoint convergence and
yield more precise results. We demonstrate the feasibility of this approach by
augmenting an abstract interpreter with unassume operators and evaluating the
impact of incorporating witnesses on performance and precision. Using manually
crafted witnesses, we can confirm verification results for multi-threaded
programs with a reduction in effort ranging from 7% to 47% in CPU time. More
intriguingly, we discover that using witnesses from model checkers can guide
our analyzer to verify program properties that it could not verify on its own.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16573" title="Abstract">arXiv:2310.16573</a> [<a href="/pdf/2310.16573" title="Download PDF">pdf</a>, <a href="/format/2310.16573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapt Anything: Tailor Any Image Classifiers across Domains And  Categories Using Text-to-Image Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weijie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shicai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanning Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Luojun Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+D">Di Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yueting Zhuang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
<p class="mathjax">We do not pursue a novel method in this paper, but aim to study if a modern
text-to-image diffusion model can tailor any task-adaptive image classifier
across domains and categories. Existing domain adaptive image classification
works exploit both source and target data for domain alignment so as to
transfer the knowledge learned from the labeled source data to the unlabeled
target data. However, as the development of the text-to-image diffusion model,
we wonder if the high-fidelity synthetic data from the text-to-image generator
can serve as a surrogate of the source data in real world. In this way, we do
not need to collect and annotate the source data for each domain adaptation
task in a one-for-one manner. Instead, we utilize only one off-the-shelf
text-to-image model to synthesize images with category labels derived from the
corresponding text prompts, and then leverage the surrogate data as a bridge to
transfer the knowledge embedded in the task-agnostic text-to-image generator to
the task-oriented image classifier via domain adaptation. Such a one-for-all
adaptation paradigm allows us to adapt anything in the world using only one
text-to-image generator as well as the corresponding unlabeled target data.
Extensive experiments validate the feasibility of the proposed idea, which even
surpasses the state-of-the-art domain adaptation works using the source data
collected and annotated in real world.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16579" title="Abstract">arXiv:2310.16579</a> [<a href="/pdf/2310.16579" title="Download PDF">pdf</a>, <a href="/format/2310.16579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WSDMS: Debunk Fake News via Weakly Supervised Detection of Misinforming  Sentences with Contextualized Social Wisdom
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Ruichao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jing Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hongzhan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhiwei Yang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 2023 Conference on Empirical Methods in Natural Language
  Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In recent years, we witness the explosion of false and unconfirmed
information (i.e., rumors) that went viral on social media and shocked the
public. Rumors can trigger versatile, mostly controversial stance expressions
among social media users. Rumor verification and stance detection are different
yet relevant tasks. Fake news debunking primarily focuses on determining the
truthfulness of news articles, which oversimplifies the issue as fake news
often combines elements of both truth and falsehood. Thus, it becomes crucial
to identify specific instances of misinformation within the articles. In this
research, we investigate a novel task in the field of fake news debunking,
which involves detecting sentence-level misinformation. One of the major
challenges in this task is the absence of a training dataset with
sentence-level annotations regarding veracity. Inspired by the Multiple
Instance Learning (MIL) approach, we propose a model called Weakly Supervised
Detection of Misinforming Sentences (WSDMS). This model only requires bag-level
labels for training but is capable of inferring both sentence-level
misinformation and article-level veracity, aided by relevant social media
conversations that are attentively contextualized with news sentences. We
evaluate WSDMS on three real-world benchmarks and demonstrate that it
outperforms existing state-of-the-art baselines in debunking fake news at both
the sentence and article levels.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16581" title="Abstract">arXiv:2310.16581</a> [<a href="/pdf/2310.16581" title="Download PDF">pdf</a>, <a href="/format/2310.16581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Minimax-MCTS and Difficulty Adjustment for General Game Playing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Aguiar+Vieira%2C+M+A+A">Marco Ant&#xf4;nio Athayde de Aguiar Vieira</a>, 
<a href="/search/cs?searchtype=author&query=Tavares%2C+A+R">Anderson Rocha Tavares</a>, 
<a href="/search/cs?searchtype=author&query=Ribas%2C+R+P">Renato Perez Ribas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Board games are a great source of entertainment for all ages, as they create
a competitive and engaging environment, as well as stimulating learning and
strategic thinking. It is common for digital versions of board games, as any
other type of digital games, to offer the option to select the difficulty of
the game. This is usually done by customizing the search parameters of the AI
algorithm. However, this approach cannot be extended to General Game Playing
agents, as different games might require different parametrization for each
difficulty level. In this paper, we present a general approach to implement an
artificial intelligence opponent with difficulty levels for zero-sum games,
together with a propose of a Minimax-MCTS hybrid algorithm, which combines the
minimax search process with GGP aspects of MCTS. This approach was tested in
our mobile application LoBoGames, an extensible board games platform, that is
intended to have an broad catalog of games, with an emphasis on accessibility:
the platform is friendly to visually-impaired users, and is compatible with
more than 92\% of Android devices. The tests in this work indicate that both
the hybrid Minimax-MCTS and the new difficulty adjustment system are promising
GGP approaches that could be expanded in future work.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16582" title="Abstract">arXiv:2310.16582</a> [<a href="/pdf/2310.16582" title="Download PDF">pdf</a>, <a href="/format/2310.16582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tailoring Personality Traits in Large Language Models via  Unsupervisedly-Built Personalized Lexicons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiaoqing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Personality plays a pivotal role in shaping human expression patterns, and
empowering and manipulating large language models (LLMs) with personality
traits holds significant promise in enhancing the user experience of LLMs.
However, prior approaches either rely on fine-tuning LLMs on a corpus enriched
with personalized expressions or necessitate the manual crafting of prompts to
induce LLMs to produce personalized responses. The former approaches demand
substantial time and resources for collecting sufficient training examples
while the latter might fail in enabling the precise manipulation of the
personality traits at a fine-grained level (e.g., achieving high agreeableness
while reducing openness). In this study, we introduce a novel approach for
tailoring personality traits within LLMs, allowing for the incorporation of any
combination of the Big Five factors (i.e., openness, conscientiousness,
extraversion, agreeableness, and neuroticism) in a pluggable manner. This is
achieved by employing a set of Unsupervisedly-Built Personalized Lexicons
(UBPL) that are utilized to adjust the probability of the next token predicted
by the original LLMs during the decoding phase. This adjustment encourages the
models to generate words present in the personalized lexicons while preserving
the naturalness of the generated texts. Extensive experimentation demonstrates
the effectiveness of our approach in finely manipulating LLMs' personality
traits. Furthermore, our method can be seamlessly integrated into other LLMs
without necessitating updates to their parameters.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16584" title="Abstract">arXiv:2310.16584</a> [<a href="/pdf/2310.16584" title="Download PDF">pdf</a>, <a href="/format/2310.16584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Explain: A Model-Agnostic Framework for Explaining Black Box  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barkan%2C+O">Oren Barkan</a>, 
<a href="/search/cs?searchtype=author&query=Asher%2C+Y">Yuval Asher</a>, 
<a href="/search/cs?searchtype=author&query=Eshel%2C+A">Amit Eshel</a>, 
<a href="/search/cs?searchtype=author&query=Elisha%2C+Y">Yehonatan Elisha</a>, 
<a href="/search/cs?searchtype=author&query=Koenigstein%2C+N">Noam Koenigstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present Learning to Explain (LTX), a model-agnostic framework designed for
providing post-hoc explanations for vision models. The LTX framework introduces
an "explainer" model that generates explanation maps, highlighting the crucial
regions that justify the predictions made by the model being explained. To
train the explainer, we employ a two-stage process consisting of initial
pretraining followed by per-instance finetuning. During both stages of
training, we utilize a unique configuration where we compare the explained
model's prediction for a masked input with its original prediction for the
unmasked input. This approach enables the use of a novel counterfactual
objective, which aims to anticipate the model's output using masked versions of
the input image. Importantly, the LTX framework is not restricted to a specific
model architecture and can provide explanations for both Transformer-based and
convolutional models. Through our evaluations, we demonstrate that LTX
significantly outperforms the current state-of-the-art in explainability across
various metrics.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16587" title="Abstract">arXiv:2310.16587</a> [<a href="/pdf/2310.16587" title="Download PDF">pdf</a>, <a href="/format/2310.16587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Uncertainty Estimation via High-Dimensional Testing on Latent  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chan%2C+T+H">Tsai Hor Chan</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+K+W">Kin Wai Lau</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jiajun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+G">Guosheng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lequan Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Uncertainty estimation aims to evaluate the confidence of a trained deep
neural network. However, existing uncertainty estimation approaches rely on
low-dimensional distributional assumptions and thus suffer from the high
dimensionality of latent features. Existing approaches tend to focus on
uncertainty on discrete classification probabilities, which leads to poor
generalizability to uncertainty estimation for other tasks. Moreover, most of
the literature requires seeing the out-of-distribution (OOD) data in the
training for better estimation of uncertainty, which limits the uncertainty
estimation performance in practice because the OOD data are typically unseen.
To overcome these limitations, we propose a new framework using data-adaptive
high-dimensional hypothesis testing for uncertainty estimation, which leverages
the statistical properties of the feature representations. Our method directly
operates on latent representations and thus does not require retraining the
feature encoder under a modified objective. The test statistic relaxes the
feature distribution assumptions to high dimensionality, and it is more
discriminative to uncertainties in the latent representations. We demonstrate
that encoding features with Bayesian neural networks can enhance testing
performance and lead to more accurate uncertainty estimation. We further
introduce a family-wise testing procedure to determine the optimal threshold of
OOD detection, which minimizes the false discovery rate (FDR). Extensive
experiments validate the satisfactory performance of our framework on
uncertainty estimation and task-specific prediction over a variety of
competitors. The experiments on the OOD detection task also show satisfactory
performance of our method when the OOD data are unseen in the training. Codes
are available at https://github.com/HKU-MedAI/bnn_uncertainty.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16588" title="Abstract">arXiv:2310.16588</a> [<a href="/pdf/2310.16588" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-parallel-task Time-delay Reservoir Computing combining a Silicon  Microring with WDM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castro%2C+B+J+G">Bernard J. Giron Castro</a>, 
<a href="/search/cs?searchtype=author&query=Peucheret%2C+C">Christophe Peucheret</a>, 
<a href="/search/cs?searchtype=author&query=Zibar%2C+D">Darko Zibar</a>, 
<a href="/search/cs?searchtype=author&query=Da+Ros%2C+F">Francesco Da Ros</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, 2 figures, Submitted to Optical Fiber Communication Conference (OFC) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Emerging Technologies (cs.ET); Machine Learning (cs.LG); Optics (physics.optics)

</div>
<p class="mathjax">We numerically demonstrate a microring-based time-delay reservoir computing
scheme that simultaneously solves three tasks involving time-series prediction,
classification, and wireless channel equalization. Each task performed on a
wavelength-multiplexed channel achieves state-of-the-art performance with
optimized power and frequency detuning.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16590" title="Abstract">arXiv:2310.16590</a> [<a href="/pdf/2310.16590" title="Download PDF">pdf</a>, <a href="/format/2310.16590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $\mathbb{VD}$-$\mathbb{GR}$: Boosting $\mathbb{V}$isual  $\mathbb{D}$ialog with Cascaded Spatial-Temporal Multi-Modal  $\mathbb{GR}$aphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdessaied%2C+A">Adnen Abdessaied</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Lei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Bulling%2C+A">Andreas Bulling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose $\mathbb{VD}$-$\mathbb{GR}$ - a novel visual dialog model that
combines pre-trained language models (LMs) with graph neural networks (GNNs).
Prior works mainly focused on one class of models at the expense of the other,
thus missing out on the opportunity of combining their respective benefits. At
the core of $\mathbb{VD}$-$\mathbb{GR}$ is a novel integration mechanism that
alternates between spatial-temporal multi-modal GNNs and BERT layers, and that
covers three distinct contributions: First, we use multi-modal GNNs to process
the features of each modality (image, question, and dialog history) and exploit
their local structures before performing BERT global attention. Second, we
propose hub-nodes that link to all other nodes within one modality graph,
allowing the model to propagate information from one GNN (modality) to the
other in a cascaded manner. Third, we augment the BERT hidden states with
fine-grained multi-modal GNN features before passing them to the next
$\mathbb{VD}$-$\mathbb{GR}$ layer. Evaluations on VisDial v1.0, VisDial v0.9,
VisDialConv, and VisPro show that $\mathbb{VD}$-$\mathbb{GR}$ achieves new
state-of-the-art results across all four datasets.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16592" title="Abstract">arXiv:2310.16592</a> [<a href="/pdf/2310.16592" title="Download PDF">pdf</a>, <a href="/format/2310.16592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Over-the-air Federated Policy Gradient
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huiwen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lingying Huang</a>, 
<a href="/search/cs?searchtype=author&query=Dey%2C+S">Subhrakanti Dey</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Ling Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Signal Processing (eess.SP)

</div>
<p class="mathjax">In recent years, over-the-air aggregation has been widely considered in
large-scale distributed learning, optimization, and sensing. In this paper, we
propose the over-the-air federated policy gradient algorithm, where all agents
simultaneously broadcast an analog signal carrying local information to a
common wireless channel, and a central controller uses the received aggregated
waveform to update the policy parameters. We investigate the effect of noise
and channel distortion on the convergence of the proposed algorithm, and
establish the complexities of communication and sampling for finding an
$\epsilon$-approximate stationary point. Finally, we present some simulation
results to show the effectiveness of the algorithm.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16595" title="Abstract">arXiv:2310.16595</a> [<a href="/pdf/2310.16595" title="Download PDF">pdf</a>, <a href="/format/2310.16595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Encoding impredicative hierarchy of type universes with variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%A9ran%2C+Y">Yoan G&#xe9;ran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Logical frameworks can be used to translate proofs from a proof system to
another one. For this purpose, we should be able to encode the theory of the
proof system in the logical framework. The Lambda Pi calculus modulo theory is
one of these logical frameworks. Powerful theories such as pure type systems
with an infinite hierarchy of universes have been encoded, leading to partial
encodings of proof systems such as Coq, Matita or Agda. In order to fully
represent systems such as Coq and Lean, we introduce a representation of an
infinite universe hierarchy with an impredicative universe and universe
variables where universe equivalence is equality, and implement it as a
terminating and confluent rewrite system.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16602" title="Abstract">arXiv:2310.16602</a> [<a href="/pdf/2310.16602" title="Download PDF">pdf</a>, <a href="/format/2310.16602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parcel loss prediction in last-mile delivery: deep and non-deep  approaches with insights from Explainable AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Leeuw%2C+J">Jan de Leeuw</a>, 
<a href="/search/cs?searchtype=author&query=Bukhsh%2C+Z">Zaharah Bukhsh</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yingqian Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Within the domain of e-commerce retail, an important objective is the
reduction of parcel loss during the last-mile delivery phase. The
ever-increasing availability of data, including product, customer, and order
information, has made it possible for the application of machine learning in
parcel loss prediction. However, a significant challenge arises from the
inherent imbalance in the data, i.e., only a very low percentage of parcels are
lost. In this paper, we propose two machine learning approaches, namely, Data
Balance with Supervised Learning (DBSL) and Deep Hybrid Ensemble Learning
(DHEL), to accurately predict parcel loss. The practical implication of such
predictions is their value in aiding e-commerce retailers in optimizing
insurance-related decision-making policies. We conduct a comprehensive
evaluation of the proposed machine learning models using one year data from
Belgian shipments. The findings show that the DHEL model, which combines a
feed-forward autoencoder with a random forest, achieves the highest
classification performance. Furthermore, we use the techniques from Explainable
AI (XAI) to illustrate how prediction models can be used in enhancing business
processes and augmenting the overall value proposition for e-commerce retailers
in the last mile delivery.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16603" title="Abstract">arXiv:2310.16603</a> [<a href="/pdf/2310.16603" title="Download PDF">pdf</a>, <a href="/format/2310.16603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Certifying Bimanual RRT Motion Plans in a Second
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amice%2C+A">Alexandre Amice</a>, 
<a href="/search/cs?searchtype=author&query=Werner%2C+P">Peter Werner</a>, 
<a href="/search/cs?searchtype=author&query=Tedrake%2C+R">Russ Tedrake</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">We present an efficient method for certifying non-collision for
piecewise-polynomial motion plans in algebraic reparametrizations of
configuration space. Such motion plans include those generated by popular
randomized methods including RRTs and PRMs, as well as those generated by many
methods in trajectory optimization. Based on Sums-of-Squares optimization, our
method provides exact, rigorous certificates of non-collision; it can never
falsely claim that a motion plan containing collisions is collision-free. We
demonstrate that our formulation is practical for real world deployment,
certifying the safety of a twelve degree of freedom motion plan in just over a
second. Moreover, the method is capable of discriminating the safety or lack
thereof of two motion plans which differ by only millimeters.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16605" title="Abstract">arXiv:2310.16605</a> [<a href="/pdf/2310.16605" title="Download PDF">pdf</a>, <a href="/format/2310.16605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributionally Robust Unsupervised Dense Retrieval Training on Web  Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+P">Peixuan Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Chenyan Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">This paper introduces Web-DRO, an unsupervised dense retrieval model, which
clusters documents based on web structures and reweights the groups during
contrastive training. Specifically, we first leverage web graph links and
contrastively train an embedding model for clustering anchor-document pairs.
Then we use Group Distributional Robust Optimization to reweight different
clusters of anchor-document pairs, which guides the model to assign more
weights to the group with higher contrastive loss and pay more attention to the
worst case during training. Our experiments on MS MARCO and BEIR show that our
model, Web-DRO, significantly improves the retrieval effectiveness in
unsupervised scenarios. A comparison of clustering techniques shows that
training on the web graph combining URL information reaches optimal performance
on clustering. Further analysis confirms that group weights are stable and
valid, indicating consistent model preferences as well as effective
up-weighting of valuable groups and down-weighting of uninformative ones. The
code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16606" title="Abstract">arXiv:2310.16606</a> [<a href="/pdf/2310.16606" title="Download PDF">pdf</a>, <a href="/ps/2310.16606" title="Download PostScript">ps</a>, <a href="/format/2310.16606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AirFL-Mem: Improving Communication-Learning Trade-Off by Long-Term  Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">Haifeng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+H">Hong Xing</a>, 
<a href="/search/cs?searchtype=author&query=Simeone%2C+O">Osvaldo Simeone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures, this is the full version of the conference version that is submitted to IEEE WCNC2024 for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Addressing the communication bottleneck inherent in federated learning (FL),
over-the-air FL (AirFL) has emerged as a promising solution, which is, however,
hampered by deep fading conditions. In this paper, we propose AirFL-Mem, a
novel scheme designed to mitigate the impact of deep fading by implementing a
\emph{long-term} memory mechanism. Convergence bounds are provided that account
for long-term memory, as well as for existing AirFL variants with short-term
memory, for general non-convex objectives. The theory demonstrates that
AirFL-Mem exhibits the same convergence rate of federated averaging (FedAvg)
with ideal communication, while the performance of existing schemes is
generally limited by error floors. The theoretical results are also leveraged
to propose a novel convex optimization strategy for the truncation threshold
used for power control in the presence of Rayleigh fading channels.
Experimental results validate the analysis, confirming the advantages of a
long-term memory mechanism for the mitigation of deep fading.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16607" title="Abstract">arXiv:2310.16607</a> [<a href="/pdf/2310.16607" title="Download PDF">pdf</a>, <a href="/format/2310.16607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Interplay between Fairness and Explainability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brandl%2C+S">Stephanie Brandl</a>, 
<a href="/search/cs?searchtype=author&query=Bugliarello%2C+E">Emanuele Bugliarello</a>, 
<a href="/search/cs?searchtype=author&query=Chalkidis%2C+I">Ilias Chalkidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages (incl Appendix), 4 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In order to build reliable and trustworthy NLP applications, models need to
be both fair across different demographics and explainable. Usually these two
objectives, fairness and explainability, are optimized and/or examined
independently of each other. Instead, we argue that forthcoming, trustworthy
NLP systems should consider both. In this work, we perform a first study to
understand how they influence each other: do fair(er) models rely on more
plausible rationales? and vice versa. To this end, we conduct experiments on
two English multi-class text classification datasets, BIOS and ECtHR, that
provide information on gender and nationality, respectively, as well as
human-annotated rationales. We fine-tune pre-trained language models with
several methods for (i) bias mitigation, which aims to improve fairness; (ii)
rationale extraction, which aims to produce plausible explanations. We find
that bias mitigation algorithms do not always lead to fairer models. Moreover,
we discover that empirical fairness and explainability are orthogonal.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16608" title="Abstract">arXiv:2310.16608</a> [<a href="/pdf/2310.16608" title="Download PDF">pdf</a>, <a href="/format/2310.16608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performative Prediction: Past and Future
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hardt%2C+M">Moritz Hardt</a>, 
<a href="/search/cs?searchtype=author&query=Mendler-D%C3%BCnner%2C+C">Celestine Mendler-D&#xfc;nner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Predictions in the social world generally influence the target of prediction,
a phenomenon known as performativity. Self-fulfilling and self-negating
predictions are examples of performativity. Of fundamental importance to
economics, finance, and the social sciences, the notion has been absent from
the development of machine learning. In machine learning applications,
performativity often surfaces as distribution shift. A predictive model
deployed on a digital platform, for example, influences consumption and thereby
changes the data-generating distribution. We survey the recently founded area
of performative prediction that provides a definition and conceptual framework
to study performativity in machine learning. A consequence of performative
prediction is a natural equilibrium notion that gives rise to new optimization
challenges. Another consequence is a distinction between learning and steering,
two mechanisms at play in performative prediction. The notion of steering is in
turn intimately related to questions of power in digital markets. We review the
notion of performative power that gives an answer to the question how much a
platform can steer participants through its predictions. We end on a discussion
of future directions, such as the role that performativity plays in contesting
algorithmic systems.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16609" title="Abstract">arXiv:2310.16609</a> [<a href="/pdf/2310.16609" title="Download PDF">pdf</a>, <a href="/format/2310.16609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Back Transcription as a Method for Evaluating Robustness of Natural  Language Understanding Models to Speech Recognition Errors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kubis%2C+M">Marek Kubis</a>, 
<a href="/search/cs?searchtype=author&query=Sk%C3%B3rzewski%2C+P">Pawe&#x142; Sk&#xf3;rzewski</a>, 
<a href="/search/cs?searchtype=author&query=Sowa%C5%84ski%2C+M">Marcin Sowa&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Zi%C4%99tkiewicz%2C+T">Tomasz Zi&#x119;tkiewicz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In a spoken dialogue system, an NLU model is preceded by a speech recognition
system that can deteriorate the performance of natural language understanding.
This paper proposes a method for investigating the impact of speech recognition
errors on the performance of natural language understanding models. The
proposed method combines the back transcription procedure with a fine-grained
technique for categorizing the errors that affect the performance of NLU
models. The method relies on the usage of synthesized speech for NLU
evaluation. We show that the use of synthesized speech in place of audio
recording does not change the outcomes of the presented technique in a
significant way.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16613" title="Abstract">arXiv:2310.16613</a> [<a href="/pdf/2310.16613" title="Download PDF">pdf</a>, <a href="/format/2310.16613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Proactive Generation of Unsafe Images From Text-To-Image Models  Using Benign Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yixin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Ning Yu</a>, 
<a href="/search/cs?searchtype=author&query=Backes%2C+M">Michael Backes</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 31 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Text-to-image models like Stable Diffusion have had a profound impact on
daily life by enabling the generation of photorealistic images from textual
prompts, fostering creativity, and enhancing visual experiences across various
applications. However, these models also pose risks. Previous studies have
successfully demonstrated that manipulated prompts can elicit text-to-image
models to generate unsafe images, e.g., hateful meme variants. Yet, these
studies only unleash the harmful power of text-to-image models in a passive
manner. In this work, we focus on the proactive generation of unsafe images
using targeted benign prompts via poisoning attacks. We propose two poisoning
attacks: a basic attack and a utility-preserving attack. We qualitatively and
quantitatively evaluate the proposed attacks using four representative hateful
memes and multiple query prompts. Experimental results indicate that
text-to-image models are vulnerable to the basic attack even with five
poisoning samples. However, the poisoning effect can inadvertently spread to
non-targeted prompts, leading to undesirable side effects. Root cause analysis
identifies conceptual similarity as an important contributing factor to the
side effects. To address this, we introduce the utility-preserving attack as a
viable mitigation strategy to maintain the attack stealthiness, while ensuring
decent attack performance. Our findings underscore the potential risks of
adopting text-to-image models in real-world scenarios, calling for future
research and safety measures in this space.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16616" title="Abstract">arXiv:2310.16616</a> [<a href="/pdf/2310.16616" title="Download PDF">pdf</a>, <a href="/format/2310.16616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context Does Matter: End-to-end Panoptic Narrative Grounding with  Deformable Attention Refined Matching Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yiming Lin</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiao-Bo Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiufeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaizhu Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICDM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Panoramic Narrative Grounding (PNG) is an emerging visual grounding task that
aims to segment visual objects in images based on dense narrative captions. The
current state-of-the-art methods first refine the representation of phrase by
aggregating the most similar $k$ image pixels, and then match the refined text
representations with the pixels of the image feature map to generate
segmentation results. However, simply aggregating sampled image features
ignores the contextual information, which can lead to phrase-to-pixel
mis-match. In this paper, we propose a novel learning framework called
Deformable Attention Refined Matching Network (DRMN), whose main idea is to
bring deformable attention in the iterative process of feature learning to
incorporate essential context information of different scales of pixels. DRMN
iteratively re-encodes pixels with the deformable attention network after
updating the feature representation of the top-$k$ most similar pixels. As
such, DRMN can lead to accurate yet discriminative pixel representations,
purify the top-$k$ most similar pixels, and consequently alleviate the
phrase-to-pixel mis-match substantially.Experimental results show that our
novel design significantly improves the matching results between text phrases
and image pixels. Concretely, DRMN achieves new state-of-the-art performance on
the PNG benchmark with an average recall improvement 3.5%. The codes are
available in: https://github.com/JaMesLiMers/DRMN.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16618" title="Abstract">arXiv:2310.16618</a> [<a href="/pdf/2310.16618" title="Download PDF">pdf</a>, <a href="/format/2310.16618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time 6-DoF Pose Estimation by an Event-based Camera using Active  LED Markers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ebmer%2C+G">Gerald Ebmer</a>, 
<a href="/search/cs?searchtype=author&query=Loch%2C+A">Adam Loch</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+M+N">Minh Nhat Vu</a>, 
<a href="/search/cs?searchtype=author&query=Haessig%2C+G">Germain Haessig</a>, 
<a href="/search/cs?searchtype=author&query=Mecca%2C+R">Roberto Mecca</a>, 
<a href="/search/cs?searchtype=author&query=Vincze%2C+M">Markus Vincze</a>, 
<a href="/search/cs?searchtype=author&query=Hartl-Nesic%2C+C">Christian Hartl-Nesic</a>, 
<a href="/search/cs?searchtype=author&query=Kugi%2C+A">Andreas Kugi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 12 figures, this paper has been accepted to WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Real-time applications for autonomous operations depend largely on fast and
robust vision-based localization systems. Since image processing tasks require
processing large amounts of data, the computational resources often limit the
performance of other processes. To overcome this limitation, traditional
marker-based localization systems are widely used since they are easy to
integrate and achieve reliable accuracy. However, classical marker-based
localization systems significantly depend on standard cameras with low frame
rates, which often lack accuracy due to motion blur. In contrast, event-based
cameras provide high temporal resolution and a high dynamic range, which can be
utilized for fast localization tasks, even under challenging visual conditions.
This paper proposes a simple but effective event-based pose estimation system
using active LED markers (ALM) for fast and accurate pose estimation. The
proposed algorithm is able to operate in real time with a latency below
\SI{0.5}{\milli\second} while maintaining output rates of \SI{3}{\kilo \hertz}.
Experimental results in static and dynamic scenarios are presented to
demonstrate the performance of the proposed approach in terms of computational
speed and absolute accuracy, using the OptiTrack system as the basis for
measurement.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16620" title="Abstract">arXiv:2310.16620</a> [<a href="/pdf/2310.16620" title="Download PDF">pdf</a>, <a href="/format/2310.16620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpikingJelly: An open-source machine learning infrastructure platform  for spike-based intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+W">Wei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yanqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jianhao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhaofei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Masquelier%2C+T">Timoth&#xe9;e Masquelier</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Ding Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Liwei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huihui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guoqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yonghong Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Science Advances (<a href="https://www.science.org/doi/10.1126/sciadv.adi1480">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
<p class="mathjax">Spiking neural networks (SNNs) aim to realize brain-inspired intelligence on
neuromorphic chips with high energy efficiency by introducing neural dynamics
and spike properties. As the emerging spiking deep learning paradigm attracts
increasing interest, traditional programming frameworks cannot meet the demands
of the automatic differentiation, parallel computation acceleration, and high
integration of processing neuromorphic datasets and deployment. In this work,
we present the SpikingJelly framework to address the aforementioned dilemma. We
contribute a full-stack toolkit for pre-processing neuromorphic datasets,
building deep SNNs, optimizing their parameters, and deploying SNNs on
neuromorphic chips. Compared to existing methods, the training of deep SNNs can
be accelerated $11\times$, and the superior extensibility and flexibility of
SpikingJelly enable users to accelerate custom models at low costs through
multilevel inheritance and semiautomatic code generation. SpikingJelly paves
the way for synthesizing truly energy-efficient SNN-based machine intelligence
systems, which will enrich the ecology of neuromorphic computing.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16621" title="Abstract">arXiv:2310.16621</a> [<a href="/pdf/2310.16621" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ArTST: Arabic Text and Speech Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toyin%2C+H+O">Hawau Olamide Toyin</a>, 
<a href="/search/cs?searchtype=author&query=Djanibekov%2C+A">Amirbek Djanibekov</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+A">Ajinkya Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Aldarmaki%2C+H">Hanan Aldarmaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 figure, SIGARAB ArabicNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We present ArTST, a pre-trained Arabic text and speech transformer for
supporting open-source speech technologies for the Arabic language. The model
architecture follows the unified-modal framework, SpeechT5, that was recently
released for English, and is focused on Modern Standard Arabic (MSA), with
plans to extend the model for dialectal and code-switched Arabic in future
editions. We pre-trained the model from scratch on MSA speech and text data,
and fine-tuned it for the following tasks: Automatic Speech Recognition (ASR),
Text-To-Speech synthesis (TTS), and spoken dialect identification. In our
experiments comparing ArTST with SpeechT5, as well as with previously reported
results in these tasks, ArTST performs on a par with or exceeding the current
state-of-the-art in all three tasks. Moreover, we find that our pre-training is
conducive for generalization, which is particularly evident in the low-resource
TTS task. The pre-trained model as well as the fine-tuned ASR and TTS models
are released for research use.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16624" title="Abstract">arXiv:2310.16624</a> [<a href="/pdf/2310.16624" title="Download PDF">pdf</a>, <a href="/format/2310.16624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Free-form Flows: Make Any Architecture a Normalizing Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Draxler%2C+F">Felix Draxler</a>, 
<a href="/search/cs?searchtype=author&query=Sorrenson%2C+P">Peter Sorrenson</a>, 
<a href="/search/cs?searchtype=author&query=Zimmermann%2C+L">Lea Zimmermann</a>, 
<a href="/search/cs?searchtype=author&query=Rousselot%2C+A">Armand Rousselot</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6the%2C+U">Ullrich K&#xf6;the</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Normalizing Flows are generative models that directly maximize the
likelihood. Previously, the design of normalizing flows was largely constrained
by the need for analytical invertibility. We overcome this constraint by a
training procedure that uses an efficient estimator for the gradient of the
change of variables formula. This enables any dimension-preserving neural
network to serve as a generative model through maximum likelihood training. Our
approach allows placing the emphasis on tailoring inductive biases precisely to
the task at hand. Specifically, we achieve excellent results in molecule
generation benchmarks utilizing $E(n)$-equivariant networks. Moreover, our
method is competitive in an inverse problem benchmark, while employing
off-the-shelf ResNet architectures.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16629" title="Abstract">arXiv:2310.16629</a> [<a href="/pdf/2310.16629" title="Download PDF">pdf</a>, <a href="/format/2310.16629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EdgeCalib: Multi-Frame Weighted Edge Features for Automatic Targetless  LiDAR-Camera Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingchen Li</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Y">Yifan Duan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Beibei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Haojie Ren</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+G">Guoliang You</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Y">Yu Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jianmin Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanyong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">In multimodal perception systems, achieving precise extrinsic calibration
between LiDAR and camera is of critical importance. Previous calibration
methods often required specific targets or manual adjustments, making them both
labor-intensive and costly. Online calibration methods based on features have
been proposed, but these methods encounter challenges such as imprecise feature
extraction, unreliable cross-modality associations, and high scene-specific
requirements. To address this, we introduce an edge-based approach for
automatic online calibration of LiDAR and cameras in real-world scenarios. The
edge features, which are prevalent in various environments, are aligned in both
images and point clouds to determine the extrinsic parameters. Specifically,
stable and robust image edge features are extracted using a SAM-based method
and the edge features extracted from the point cloud are weighted through a
multi-frame weighting strategy for feature filtering. Finally, accurate
extrinsic parameters are optimized based on edge correspondence constraints. We
conducted evaluations on both the KITTI dataset and our dataset. The results
show a state-of-the-art rotation accuracy of 0.086{\deg} and a translation
accuracy of 0.977 cm, outperforming existing edge-based calibration methods in
both precision and robustness.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16630" title="Abstract">arXiv:2310.16630</a> [<a href="/pdf/2310.16630" title="Download PDF">pdf</a>, <a href="/format/2310.16630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Experimental Performance Evaluation of Data Distribution  Service (DDS) Implementations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peeroo%2C+K">Kaleem Peeroo</a>, 
<a href="/search/cs?searchtype=author&query=Popov%2C+P">Peter Popov</a>, 
<a href="/search/cs?searchtype=author&query=Stankovic%2C+V">Vladimir Stankovic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages and 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Performance (cs.PF)

</div>
<p class="mathjax">The Data Distribution Service (DDS) is a widely used communication
specification for real-time mission-critical systems that follow the principles
of publish-subscribe middleware. DDS has an extensive set of quality of service
(QoS) parameters allowing a thorough customisation of the intended
communication. An extensive survey of the performance of the implementations of
this communication middleware is lacking. This paper closes the gap by
surveying the state of the art in performance of various DDS implementations
and identifying any research gaps that exist within this domain.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16632" title="Abstract">arXiv:2310.16632</a> [<a href="/pdf/2310.16632" title="Download PDF">pdf</a>, <a href="/format/2310.16632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing the relationship between subjective trust, confidence  measurements, and mouse trajectory characteristics in an online task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dechant%2C+M">Martin Dechant</a>, 
<a href="/search/cs?searchtype=author&query=Poeller%2C+S">Susanne Poeller</a>, 
<a href="/search/cs?searchtype=author&query=Hosp%2C+B">Benedikt Hosp</a>, 
<a href="/search/cs?searchtype=author&query=Lukashova-Sanz%2C+O">Olga Lukashova-Sanz</a>, 
<a href="/search/cs?searchtype=author&query=Sipatchin%2C+A">Alexandra Sipatchin</a>, 
<a href="/search/cs?searchtype=author&query=Wahl%2C+S">Siegfried Wahl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to CHI 2023 and rejected
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Trust is essential for our interactions with others but also with artificial
intelligence (AI) based systems. To understand whether a user trusts an AI,
researchers need reliable measurement tools. However, currently discussed
markers mostly rely on expensive and invasive sensors, like
electroencephalograms, which may cause discomfort. The analysis of mouse
trajectory has been suggested as a convenient tool for trust assessment.
However, the relationship between trust, confidence and mouse trajectory is not
yet fully understood. To provide more insights into this relationship, we asked
participants (n = 146) to rate whether several tweets were offensive while an
AI suggested its assessment. Our results reveal which aspects of the mouse
trajectory are affected by the users subjective trust and confidence ratings;
yet they indicate that these measures might not explain sufficiently the
variance to be used on their own. This work examines a potential low-cost trust
assessment in AI systems.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16633" title="Abstract">arXiv:2310.16633</a> [<a href="/pdf/2310.16633" title="Download PDF">pdf</a>, <a href="/format/2310.16633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Photometric Redshifts with Copula Entropy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jian Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 7 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cosmology and Nongalactic Astrophysics (astro-ph.CO); Instrumentation and Methods for Astrophysics (astro-ph.IM); Applications (stat.AP)

</div>
<p class="mathjax">In this paper we propose to apply copula entropy (CE) to photometric
redshifts. CE is used to measure the correlations between photometric
measurements and redshifts and then the measurements associated with high CEs
are selected for predicting redshifts. We verified the proposed method on the
SDSS quasar data. Experimental results show that the accuracy of photometric
redshifts is improved with the selected measurements compared to the results
with all the measurements used in the experiments, especially for the samples
with high redshifts. The measurements selected with CE include luminosity
magnitude, the brightness in ultraviolet band with standard deviation, and the
brightness of the other four bands. Since CE is a rigorously defined
mathematical concept, the models such derived is interpretable.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16639" title="Abstract">arXiv:2310.16639</a> [<a href="/pdf/2310.16639" title="Download PDF">pdf</a>, <a href="/format/2310.16639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Driving through the Concept Gridlock: Unraveling Explainability  Bottlenecks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Echterhoff%2C+J">Jessica Echterhoff</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+A">An Yan</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kyungtae Han</a>, 
<a href="/search/cs?searchtype=author&query=Abdelraouf%2C+A">Amr Abdelraouf</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R">Rohit Gupta</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Concept bottleneck models have been successfully used for explainable machine
learning by encoding information within the model with a set of human-defined
concepts. In the context of human-assisted or autonomous driving,
explainability models can help user acceptance and understanding of decisions
made by the autonomous vehicle, which can be used to rationalize and explain
driver or vehicle behavior. We propose a new approach using concept bottlenecks
as visual features for control command predictions and explanations of user and
vehicle behavior. We learn a human-understandable concept layer that we use to
explain sequential driving scenes while learning vehicle control commands. This
approach can then be used to determine whether a change in a preferred gap or
steering commands from a human (or autonomous vehicle) is led by an external
stimulus or change in preferences. We achieve competitive performance to latent
visual features while gaining interpretability within our model setup.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16640" title="Abstract">arXiv:2310.16640</a> [<a href="/pdf/2310.16640" title="Download PDF">pdf</a>, <a href="/ps/2310.16640" title="Download PostScript">ps</a>, <a href="/format/2310.16640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Foteinopoulou%2C+N+M">Niki Maria Foteinopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Patras%2C+I">Ioannis Patras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Facial Expression Recognition (FER) is a crucial task in affective computing,
but its conventional focus on the seven basic emotions limits its applicability
to the complex and expanding emotional spectrum. To address the issue of new
and unseen emotions present in dynamic in-the-wild FER, we propose a novel
vision-language model that utilises sample-level text descriptions (i.e.
captions of the context, expressions or emotional cues) as natural language
supervision, aiming to enhance the learning of rich latent representations, for
zero-shot classification. To test this, we evaluate using zero-shot
classification of the model trained on sample-level descriptions on four
popular dynamic FER datasets. Our findings show that this approach yields
significant improvements when compared to baseline methods. Specifically, for
zero-shot video FER, we outperform CLIP by over 10\% in terms of Weighted
Average Recall and 5\% in terms of Unweighted Average Recall on several
datasets. Furthermore, we evaluate the representations obtained from the
network trained using sample-level descriptions on the downstream task of
mental health symptom estimation, achieving performance comparable or superior
to state-of-the-art methods and strong agreement with human experts. Namely, we
achieve a Pearson's Correlation Coefficient of up to 0.85 on schizophrenia
symptom severity estimation, which is comparable to human experts' agreement.
The code is publicly available at: https://github.com/NickyFot/EmoCLIP.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16641" title="Abstract">arXiv:2310.16641</a> [<a href="/pdf/2310.16641" title="Download PDF">pdf</a>, <a href="/format/2310.16641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Next Evolution of Artificial Sense of Touch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gro%C3%9F%2C+S">Sonja Gro&#xdf;</a>, 
<a href="/search/eess?searchtype=author&query=Ganguly%2C+A">Amartya Ganguly</a>, 
<a href="/search/eess?searchtype=author&query=Dietz%2C+H">Hendrik Dietz</a>, 
<a href="/search/eess?searchtype=author&query=Haddadin%2C+S">Sami Haddadin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19, 2 figures, journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">We propose the next evolution of the artificial sense of touch, including an
in-depth examination of the latest advancements in tactile sensing technology
and the challenges that remain. We delve into the forefront of DNA and
nanomaterials that enable the design of functionalized nanostructures in
combination with the advantages of auto-assembly mechanisms. We evaluate the
impact those technologies have on the challenges still faced in tactile sensing
technology, including self-healing mechanisms, self-adaption, multi-modal,
stretchable sensor structures, neuromorphic signal transmission, and scalable
manufacturing. To conclude, this evolving technology has the potential to
redefine the artificial sense of touch, offering mechanisms that enable
advanced artificial somatosensory systems that equal or surpass human
capabilities.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16646" title="Abstract">arXiv:2310.16646</a> [<a href="/pdf/2310.16646" title="Download PDF">pdf</a>, <a href="/format/2310.16646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model predictive control-based value estimation for efficient  reinforcement learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qizhen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kexin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Reinforcement learning suffers from limitations in real practices primarily
due to the numbers of required interactions with virtual environments. It
results in a challenging problem that we are implausible to obtain an optimal
strategy only with a few attempts for many learning method. Hereby, we design
an improved reinforcement learning method based on model predictive control
that models the environment through a data-driven approach. Based on learned
environmental model, it performs multi-step prediction to estimate the value
function and optimize the policy. The method demonstrates higher learning
efficiency, faster convergent speed of strategies tending to the optimal value,
and fewer sample capacity space required by experience replay buffers.
Experimental results, both in classic databases and in a dynamic obstacle
avoidance scenario for unmanned aerial vehicle, validate the proposed
approaches.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16647" title="Abstract">arXiv:2310.16647</a> [<a href="/pdf/2310.16647" title="Download PDF">pdf</a>, <a href="/ps/2310.16647" title="Download PostScript">ps</a>, <a href="/format/2310.16647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Achieving Constraints in Neural Networks: A Stochastic Augmented  Lagrangian Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lavado%2C+D">Diogo Lavado</a>, 
<a href="/search/cs?searchtype=author&query=Soares%2C+C">Cl&#xe1;udia Soares</a>, 
<a href="/search/cs?searchtype=author&query=Micheletti%2C+A">Alessandra Micheletti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Regularizing Deep Neural Networks (DNNs) is essential for improving
generalizability and preventing overfitting. Fixed penalty methods, though
common, lack adaptability and suffer from hyperparameter sensitivity. In this
paper, we propose a novel approach to DNN regularization by framing the
training process as a constrained optimization problem. Where the data fidelity
term is the minimization objective and the regularization terms serve as
constraints. Then, we employ the Stochastic Augmented Lagrangian (SAL) method
to achieve a more flexible and efficient regularization mechanism. Our approach
extends beyond black-box regularization, demonstrating significant improvements
in white-box models, where weights are often subject to hard constraints to
ensure interpretability. Experimental results on image-based classification on
MNIST, CIFAR10, and CIFAR100 datasets validate the effectiveness of our
approach. SAL consistently achieves higher Accuracy while also achieving better
constraint satisfaction, thus showcasing its potential for optimizing DNNs
under constrained settings.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16648" title="Abstract">arXiv:2310.16648</a> [<a href="/pdf/2310.16648" title="Download PDF">pdf</a>, <a href="/format/2310.16648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Posterior Consistency for Missing Data in Variational Autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sudak%2C+T">Timur Sudak</a>, 
<a href="/search/cs?searchtype=author&query=Tschiatschek%2C+S">Sebastian Tschiatschek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First published in ECML PKDD 2023, Proceedings, Part II, by Springer Nature (<a href="https://doi.org/10.1007/978-3-031-43415-0_30">this https URL</a>). This version of the work has been extended with the addition of an Appendix, which includes proofs, the derivation of the posterior regularization, additional background information on technical topics, an extended related work section, and additional experimental results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider the problem of learning Variational Autoencoders (VAEs), i.e., a
type of deep generative model, from data with missing values. Such data is
omnipresent in real-world applications of machine learning because complete
data is often impossible or too costly to obtain. We particularly focus on
improving a VAE's amortized posterior inference, i.e., the encoder, which in
the case of missing data can be susceptible to learning inconsistent posterior
distributions regarding the missingness. To this end, we provide a formal
definition of posterior consistency and propose an approach for regularizing an
encoder's posterior distribution which promotes this consistency. We observe
that the proposed regularization suggests a different training objective than
that typically considered in the literature when facing missing values.
Furthermore, we empirically demonstrate that our regularization leads to
improved performance in missing value settings in terms of reconstruction
quality and downstream tasks utilizing uncertainty in the latent space. This
improved performance can be observed for many classes of VAEs including VAEs
equipped with normalizing flows.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16652" title="Abstract">arXiv:2310.16652</a> [<a href="/pdf/2310.16652" title="Download PDF">pdf</a>, <a href="/format/2310.16652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Robust is Federated Learning to Communication Error? A Comparison  Study Between Uplink and Downlink Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qu%2C+L">Linping Qu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shenghui Song</a>, 
<a href="/search/cs?searchtype=author&query=Tsui%2C+C">Chi-Ying Tsui</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yuyi Mao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Because of its privacy-preserving capability, federated learning (FL) has
attracted significant attention from both academia and industry. However, when
being implemented over wireless networks, it is not clear how much
communication error can be tolerated by FL. This paper investigates the
robustness of FL to the uplink and downlink communication error. Our
theoretical analysis reveals that the robustness depends on two critical
parameters, namely the number of clients and the numerical range of model
parameters. It is also shown that the uplink communication in FL can tolerate a
higher bit error rate (BER) than downlink communication, and this difference is
quantified by a proposed formula. The findings and theoretical analyses are
further validated by extensive experiments.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16654" title="Abstract">arXiv:2310.16654</a> [<a href="/pdf/2310.16654" title="Download PDF">pdf</a>, <a href="/format/2310.16654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT is a Potential Zero-Shot Dependency Parser
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Boda Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xinyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+B">Binghao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+X">Xiaocheng Gong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Si Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pre-trained language models have been widely used in dependency parsing task
and have achieved significant improvements in parser performance. However, it
remains an understudied question whether pre-trained language models can
spontaneously exhibit the ability of dependency parsing without introducing
additional parser structure in the zero-shot scenario. In this paper, we
propose to explore the dependency parsing ability of large language models such
as ChatGPT and conduct linguistic analysis. The experimental results
demonstrate that ChatGPT is a potential zero-shot dependency parser, and the
linguistic analysis also shows some unique preferences in parsing outputs.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16655" title="Abstract">arXiv:2310.16655</a> [<a href="/pdf/2310.16655" title="Download PDF">pdf</a>, <a href="/format/2310.16655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Control-Centric Representations in Reinforcement Learning from  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zang%2C+H">Hongyu Zang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Heng%2C+Y">Yong Heng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zhen Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yisen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingzhong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Image-based Reinforcement Learning is a practical yet challenging task. A
major hurdle lies in extracting control-centric representations while
disregarding irrelevant information. While approaches that follow the
bisimulation principle exhibit the potential in learning state representations
to address this issue, they still grapple with the limited expressive capacity
of latent dynamics and the inadaptability to sparse reward environments. To
address these limitations, we introduce ReBis, which aims to capture
control-centric information by integrating reward-free control information
alongside reward-specific knowledge. ReBis utilizes a transformer architecture
to implicitly model the dynamics and incorporates block-wise masking to
eliminate spatiotemporal redundancy. Moreover, ReBis combines
bisimulation-based loss with asymmetric reconstruction loss to prevent feature
collapse in environments with sparse rewards. Empirical studies on two large
benchmarks, including Atari games and DeepMind Control Suit, demonstrate that
ReBis has superior performance compared to existing methods, proving its
effectiveness.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16656" title="Abstract">arXiv:2310.16656</a> [<a href="/pdf/2310.16656" title="Download PDF">pdf</a>, <a href="/format/2310.16656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Picture is Worth a Thousand Words: Principled Recaptioning Improves  Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Segalis%2C+E">Eyal Segalis</a>, 
<a href="/search/cs?searchtype=author&query=Valevski%2C+D">Dani Valevski</a>, 
<a href="/search/cs?searchtype=author&query=Lumen%2C+D">Danny Lumen</a>, 
<a href="/search/cs?searchtype=author&query=Matias%2C+Y">Yossi Matias</a>, 
<a href="/search/cs?searchtype=author&query=Leviathan%2C+Y">Yaniv Leviathan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Text-to-image diffusion models achieved a remarkable leap in capabilities
over the last few years, enabling high-quality and diverse synthesis of images
from a textual prompt. However, even the most advanced models often struggle to
precisely follow all of the directions in their prompts. The vast majority of
these models are trained on datasets consisting of (image, caption) pairs where
the images often come from the web, and the captions are their HTML alternate
text. A notable example is the LAION dataset, used by Stable Diffusion and
other models. In this work we observe that these captions are often of low
quality, and argue that this significantly affects the model's capability to
understand nuanced semantics in the textual prompts. We show that by relabeling
the corpus with a specialized automatic captioning model and training a
text-to-image model on the recaptioned dataset, the model benefits
substantially across the board. First, in overall image quality: e.g. FID 14.84
vs. the baseline of 17.87, and 64.3% improvement in faithful image generation
according to human evaluation. Second, in semantic alignment, e.g. semantic
object accuracy 84.34 vs. 78.90, counting alignment errors 1.32 vs. 1.44 and
positional alignment 62.42 vs. 57.60. We analyze various ways to relabel the
corpus and provide evidence that this technique, which we call RECAP, both
reduces the train-inference discrepancy and provides the model with more
information per example, increasing sample efficiency and allowing the model to
better understand the relations between captions and images.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16658" title="Abstract">arXiv:2310.16658</a> [<a href="/pdf/2310.16658" title="Download PDF">pdf</a>, <a href="/format/2310.16658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Online Self-calibrating Refractive Camera Model with Application to  Underwater Odometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mohit Singh</a>, 
<a href="/search/cs?searchtype=author&query=Dharmadhikari%2C+M">Mihir Dharmadhikari</a>, 
<a href="/search/cs?searchtype=author&query=Alexis%2C+K">Kostas Alexis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, Submitted to the IEEE International Conference on Robotics and Automation, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This work presents a camera model for refractive media such as water and its
application in underwater visual-inertial odometry. The model is
self-calibrating in real-time and is free of known correspondences or
calibration targets. It is separable as a distortion model (dependent on
refractive index $n$ and radial pixel coordinate) and a virtual pinhole model
(as a function of $n$). We derive the self-calibration formulation leveraging
epipolar constraints to estimate the refractive index and subsequently correct
for distortion. Through experimental studies using an underwater robot
integrating cameras and inertial sensing, the model is validated regarding the
accurate estimation of the refractive index and its benefits for robust
odometry estimation in an extended envelope of conditions. Lastly, we show the
transition between media and the estimation of the varying refractive index
online, thus allowing computer vision tasks across refractive media.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16659" title="Abstract">arXiv:2310.16659</a> [<a href="/pdf/2310.16659" title="Download PDF">pdf</a>, <a href="/format/2310.16659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UAV Pathfinding in Dynamic Obstacle Avoidance with Multi-agent  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qizhen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kexin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+J">Jinhu Lv</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Multi-agent reinforcement learning based methods are significant for online
planning of feasible and safe paths for agents in dynamic and uncertain
scenarios. Although some methods like fully centralized and fully decentralized
methods achieve a certain measure of success, they also encounter problems such
as dimension explosion and poor convergence, respectively. In this paper, we
propose a novel centralized training with decentralized execution method based
on multi-agent reinforcement learning to solve the dynamic obstacle avoidance
problem online. In this approach, each agent communicates only with the central
planner or only with its neighbors, respectively, to plan feasible and safe
paths online. We improve our methods based on the idea of model predictive
control to increase the training efficiency and sample utilization of agents.
The experimental results in both simulation, indoor, and outdoor environments
validate the effectiveness of our method. The video is available at
https://www.bilibili.com/video/BV1gw41197hV/?vd_source=9de61aecdd9fb684e546d032ef7fe7bf
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16665" title="Abstract">arXiv:2310.16665</a> [<a href="/pdf/2310.16665" title="Download PDF">pdf</a>, <a href="/format/2310.16665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Source-Free Domain Adaptation for Fundus Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lingrui Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yanfeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Ge Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, WACV2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Unsupervised Domain Adaptation (UDA) is a learning technique that transfers
knowledge learned in the source domain from labelled training data to the
target domain with only unlabelled data. It is of significant importance to
medical image segmentation because of the usual lack of labelled training data.
Although extensive efforts have been made to optimize UDA techniques to improve
the ac?curacy of segmentation models in the target domain, few studies have
addressed the robustness of these models under UDA. In this study, we propose a
two-stage training strat?egy for robust domain adaptation. In the source
training stage, we utilize adversarial sample augmentation to en?hance the
robustness and generalization capability of the source model. And in the target
training stage, we propose a novel robust pseudo-label and pseudo-boundary
(PLPB) method, which effectively utilizes unlabeled target data to generate
pseudo labels and pseudo boundaries that enable model self-adaptation without
requiring source data. Ex?tensive experimental results on cross-domain fundus
image segmentation confirm the effectiveness and versatility of our method.
Source code of this study is openly accessible at
https://github.com/LinGrayy/PLPB.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16667" title="Abstract">arXiv:2310.16667</a> [<a href="/pdf/2310.16667" title="Download PDF">pdf</a>, <a href="/format/2310.16667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary  Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chuofan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zehuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiaojuan Qi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deriving reliable region-word alignment from image-text pairs is critical to
learn object-level vision-language representations for open-vocabulary object
detection. Existing methods typically rely on pre-trained or self-trained
vision-language models for alignment, which are prone to limitations in
localization accuracy or generalization capabilities. In this paper, we propose
CoDet, a novel approach that overcomes the reliance on pre-aligned
vision-language space by reformulating region-word alignment as a co-occurring
object discovery problem. Intuitively, by grouping images that mention a shared
concept in their captions, objects corresponding to the shared concept shall
exhibit high co-occurrence among the group. CoDet then leverages visual
similarities to discover the co-occurring objects and align them with the
shared concept. Extensive experiments demonstrate that CoDet has superior
performances and compelling scalability in open-vocabulary detection, e.g., by
scaling up the visual backbone, CoDet achieves 37.0 $\text{AP}^m_{novel}$ and
44.7 $\text{AP}^m_{all}$ on OV-LVIS, surpassing the previous SoTA by 4.2
$\text{AP}^m_{novel}$ and 9.8 $\text{AP}^m_{all}$. Code is available at
https://github.com/CVMI-Lab/CoDet.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16668" title="Abstract">arXiv:2310.16668</a> [<a href="/pdf/2310.16668" title="Download PDF">pdf</a>, <a href="/format/2310.16668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SkelFMM: A Simplified Fast Multipole Method Based on Recursive  Skeletonization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yesypenko%2C+A">Anna Yesypenko</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+C">Chao Chen</a>, 
<a href="/search/math?searchtype=author&query=Martinsson%2C+P">Per-Gunnar Martinsson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This work introduces the kernel-independent multi-level algorithm "skelFMM"
for evaluating all pairwise interactions between $N$ points connected through a
kernel such as the fundamental solution of the Laplace or the Helmholtz
equations. The method is based on linear algebraic tools such as randomized low
rank approximation and "skeleton representations" of far-field interactions.
The work is related to previously proposed linear algebraic reformulations of
the fast multipole method (FMM), but is distinguished by relying on simpler
data structures. In particular, skelFMM does not require an "interaction list",
as it relies instead on algebraically-modified kernel interactions between
near-neighbors at every level. Like other kernel independent algorithms, it
only requires evaluation of the kernel function, allowing the methodology to
easily be extended to a range of different kernels in 2D and 3D. The simplicity
of the algorithm makes it particularly amenable to parallel implementation on
heterogeneous hardware architectures.
<br />The performance of the algorithm is demonstrated through numerical
experiments conducted on uniform and non-uniform point distributions in 2D and
3D, involving Laplace and (low frequency) Helmholtz kernels. The algorithm
relies on a precomputation stage that constructs a tailored representation for
a given geometry of points. Once the precomputation has completed, the
matrix-vector multiplication attains high speed through GPU acceleration that
leverages batched linear algebra.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16672" title="Abstract">arXiv:2310.16672</a> [<a href="/pdf/2310.16672" title="Download PDF">pdf</a>, <a href="/format/2310.16672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In the user&#x27;s eyes we find trust: Using gaze data as a predictor or  trust in an artifical intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dechant%2C+M+J">Martin Johannes Dechant</a>, 
<a href="/search/cs?searchtype=author&query=Lukashova-Sanz%2C+O">Olga Lukashova-Sanz</a>, 
<a href="/search/cs?searchtype=author&query=Wahl%2C+S">Siegfried Wahl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Workshop submission of a proposed research project at TRAIT 2023 (held at CHI2023 in Hamburg)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Trust is essential for our interactions with others but also with artificial
intelligence (AI) based systems. To understand whether a user trusts an AI,
researchers need reliable measurement tools. However, currently discussed
markers mostly rely on expensive and invasive sensors, like
electroencephalograms, which may cause discomfort. The analysis of gaze data
has been suggested as a convenient tool for trust assessment. However, the
relationship between trust and several aspects of the gaze behaviour is not yet
fully understood. To provide more insights into this relationship, we propose a
exploration study in virtual reality where participants have to perform a
sorting task together with a simulated AI in a simulated robotic arm embedded
in a gaming. We discuss the potential benefits of this approach and outline our
study design in this submission.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16673" title="Abstract">arXiv:2310.16673</a> [<a href="/pdf/2310.16673" title="Download PDF">pdf</a>, <a href="/format/2310.16673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Large Language Models for Code Explanation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+P">Paheli Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+M">Manojit Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Palepu%2C+K+N+S+N">Kartheek N S N Palepu</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+V">Vikas Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Dindorkar%2C+I">Ishan Dindorkar</a>, 
<a href="/search/cs?searchtype=author&query=Rajpurohit%2C+R">Rakesh Rajpurohit</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R">Rishabh Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the Forum for Information Retrieval Evaluation 2023 (IRSE Track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Automating code documentation through explanatory text can prove highly
beneficial in code understanding. Large Language Models (LLMs) have made
remarkable strides in Natural Language Processing, especially within software
engineering tasks such as code generation and code summarization. This study
specifically delves into the task of generating natural-language summaries for
code snippets, using various LLMs. The findings indicate that Code LLMs
outperform their generic counterparts, and zero-shot methods yield superior
results when dealing with datasets with dissimilar distributions between
training and testing sets.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16675" title="Abstract">arXiv:2310.16675</a> [<a href="/pdf/2310.16675" title="Download PDF">pdf</a>, <a href="/format/2310.16675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Agreeing to Stop: Reliable Latency-Adaptive Decision Making via  Ensembles of Spiking Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiechen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sangwoo Park</a>, 
<a href="/search/cs?searchtype=author&query=Simeone%2C+O">Osvaldo Simeone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Spiking neural networks (SNNs) are recurrent models that can leverage
sparsity in input time series to efficiently carry out tasks such as
classification. Additional efficiency gains can be obtained if decisions are
taken as early as possible as a function of the complexity of the input time
series. The decision on when to stop inference and produce a decision must rely
on an estimate of the current accuracy of the decision. Prior work demonstrated
the use of conformal prediction (CP) as a principled way to quantify
uncertainty and support adaptive-latency decisions in SNNs. In this paper, we
propose to enhance the uncertainty quantification capabilities of SNNs by
implementing ensemble models for the purpose of improving the reliability of
stopping decisions. Intuitively, an ensemble of multiple models can decide when
to stop more reliably by selecting times at which most models agree that the
current accuracy level is sufficient. The proposed method relies on different
forms of information pooling from ensemble models, and offers theoretical
reliability guarantees. We specifically show that variational inference-based
ensembles with p-variable pooling significantly reduce the average latency of
state-of-the-art methods, while maintaining reliability guarantees.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16676" title="Abstract">arXiv:2310.16676</a> [<a href="/pdf/2310.16676" title="Download PDF">pdf</a>, <a href="/format/2310.16676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SSLCL: An Efficient Model-Agnostic Supervised Contrastive Learning  Framework for Emotion Recognition in Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+T">Tao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yaoyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+X">Xinyi Tong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shao-Lun Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Emotion recognition in conversations (ERC) is a rapidly evolving task within
the natural language processing community, which aims to detect the emotions
expressed by speakers during a conversation. Recently, a growing number of ERC
methods have focused on leveraging supervised contrastive learning (SCL) to
enhance the robustness and generalizability of learned features. However,
current SCL-based approaches in ERC are impeded by the constraint of large
batch sizes and the lack of compatibility with most existing ERC models. To
address these challenges, we propose an efficient and model-agnostic SCL
framework named Supervised Sample-Label Contrastive Learning with Soft-HGR
Maximal Correlation (SSLCL), which eliminates the need for a large batch size
and can be seamlessly integrated with existing ERC models without introducing
any model-specific assumptions. Specifically, we introduce a novel perspective
on utilizing label representations by projecting discrete labels into dense
embeddings through a shallow multilayer perceptron, and formulate the training
objective to maximize the similarity between sample features and their
corresponding ground-truth label embeddings, while minimizing the similarity
between sample features and label embeddings of disparate classes. Moreover, we
innovatively adopt the Soft-HGR maximal correlation as a measure of similarity
between sample features and label embeddings, leading to significant
performance improvements over conventional similarity measures. Additionally,
multimodal cues of utterances are effectively leveraged by SSLCL as data
augmentations to boost model performances. Extensive experiments on two ERC
benchmark datasets, IEMOCAP and MELD, demonstrate the compatibility and
superiority of our proposed SSLCL framework compared to existing
state-of-the-art SCL methods. Our code is available at
\url{https://github.com/TaoShi1998/SSLCL}.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16677" title="Abstract">arXiv:2310.16677</a> [<a href="/pdf/2310.16677" title="Download PDF">pdf</a>, <a href="/format/2310.16677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning Approaches for Fine-Grained Symptom Estimation in  Schizophrenia: A Comprehensive Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Foteinopoulou%2C+N+M">Niki Maria Foteinopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Patras%2C+I">Ioannis Patras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Schizophrenia is a severe yet treatable mental disorder, it is diagnosed
using a multitude of primary and secondary symptoms. Diagnosis and treatment
for each individual depends on the severity of the symptoms, therefore there is
a need for accurate, personalised assessments. However, the process can be both
time-consuming and subjective; hence, there is a motivation to explore
automated methods that can offer consistent diagnosis and precise symptom
assessments, thereby complementing the work of healthcare practitioners.
Machine Learning has demonstrated impressive capabilities across numerous
domains, including medicine; the use of Machine Learning in patient assessment
holds great promise for healthcare professionals and patients alike, as it can
lead to more consistent and accurate symptom estimation.This survey aims to
review methodologies that utilise Machine Learning for diagnosis and assessment
of schizophrenia. Contrary to previous reviews that primarily focused on binary
classification, this work recognises the complexity of the condition and
instead, offers an overview of Machine Learning methods designed for
fine-grained symptom estimation. We cover multiple modalities, namely Medical
Imaging, Electroencephalograms and Audio-Visual, as the illness symptoms can
manifest themselves both in a patient's pathology and behaviour. Finally, we
analyse the datasets and methodologies used in the studies and identify trends,
gaps as well as opportunities for future research.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16678" title="Abstract">arXiv:2310.16678</a> [<a href="/pdf/2310.16678" title="Download PDF">pdf</a>, <a href="/format/2310.16678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust and Actively Secure Serverless Collaborative Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Franzese%2C+O">Olive Franzese</a>, 
<a href="/search/cs?searchtype=author&query=Dziedzic%2C+A">Adam Dziedzic</a>, 
<a href="/search/cs?searchtype=author&query=Choquette-Choo%2C+C+A">Christopher A. Choquette-Choo</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+M+R">Mark R. Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Kaleem%2C+M+A">Muhammad Ahmad Kaleem</a>, 
<a href="/search/cs?searchtype=author&query=Rabanser%2C+S">Stephan Rabanser</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+C">Congyu Fang</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Somesh Jha</a>, 
<a href="/search/cs?searchtype=author&query=Papernot%2C+N">Nicolas Papernot</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Collaborative machine learning (ML) is widely used to enable institutions to
learn better models from distributed data. While collaborative approaches to
learning intuitively protect user data, they remain vulnerable to either the
server, the clients, or both, deviating from the protocol. Indeed, because the
protocol is asymmetric, a malicious server can abuse its power to reconstruct
client data points. Conversely, malicious clients can corrupt learning with
malicious updates. Thus, both clients and servers require a guarantee when the
other cannot be trusted to fully cooperate. In this work, we propose a
peer-to-peer (P2P) learning scheme that is secure against malicious servers and
robust to malicious clients. Our core contribution is a generic framework that
transforms any (compatible) algorithm for robust aggregation of model updates
to the setting where servers and clients can act maliciously. Finally, we
demonstrate the computational efficiency of our approach even with 1-million
parameter models trained by 100s of peers on standard datasets.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16681" title="Abstract">arXiv:2310.16681</a> [<a href="/pdf/2310.16681" title="Download PDF">pdf</a>, <a href="/format/2310.16681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BabyStories: Can Reinforcement Learning Teach Baby Language Models to  Write Better Stories?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xingmeng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tongnian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Osborn%2C+S">Sheri Osborn</a>, 
<a href="/search/cs?searchtype=author&query=Rios%2C+A">Anthony Rios</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to BabyLM workshop at CoNLL
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Language models have seen significant growth in the size of their corpus,
leading to notable performance improvements. Yet, there has been limited
progress in developing models that handle smaller, more human-like datasets. As
part of the BabyLM shared task, this study explores the impact of reinforcement
learning from human feedback (RLHF) on language models pretrained from scratch
with a limited training corpus. Comparing two GPT-2 variants, the larger model
performs better in storytelling tasks after RLHF fine-tuning. These findings
suggest that RLHF techniques may be more advantageous for larger models due to
their higher learning and adaptation capacity, though more experiments are
needed to confirm this finding. These insights highlight the potential benefits
of RLHF fine-tuning for language models within limited data, enhancing their
ability to maintain narrative focus and coherence while adhering better to
initial instructions in storytelling tasks. The code for this work is publicly
at https://github.com/Zephyr1022/BabyStories-UTSA.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16684" title="Abstract">arXiv:2310.16684</a> [<a href="/pdf/2310.16684" title="Download PDF">pdf</a>, <a href="/format/2310.16684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Statistics for Generative Image Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wong%2C+Y+J">Yung Jer Wong</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+T+K">Teck Khim Ng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Diffusion models (DMs) are generative models that learn to synthesize images
from Gaussian noise. DMs can be trained to do a variety of tasks such as image
generation and image super-resolution. Researchers have made significant
improvement in the capability of synthesizing photorealistic images in the past
few years. These successes also hasten the need to address the potential misuse
of synthesized images. In this paper, we highlight the effectiveness of
computing local statistics, as opposed to global statistics, in distinguishing
digital camera images from DM-generated images. We hypothesized that local
statistics should be used to address the spatial non-stationarity problem in
images. We show that our approach produced promising results and it is also
robust to various perturbations such as image resizing and JPEG compression.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16685" title="Abstract">arXiv:2310.16685</a> [<a href="/pdf/2310.16685" title="Download PDF">pdf</a>, <a href="/format/2310.16685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection of news written by the ChatGPT through authorship attribution  performed by a Bidirectional LSTM model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iaquinta%2C+A+F">Amanda Ferrari Iaquinta</a>, 
<a href="/search/cs?searchtype=author&query=von+Atzingen%2C+G+V">Gustavo Voltani von Atzingen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The large language based-model chatbot ChatGPT gained a lot of popularity
since its launch and has been used in a wide range of situations. This research
centers around a particular situation, when the ChatGPT is used to produce news
that will be consumed by the population, causing the facilitation in the
production of fake news, spread of misinformation and lack of trust in news
sources. Aware of these problems, this research aims to build an artificial
intelligence model capable of performing authorship attribution on news
articles, identifying the ones written by the ChatGPT. To achieve this goal, a
dataset containing equal amounts of human and ChatGPT written news was
assembled and different natural processing language techniques were used to
extract features from it that were used to train, validate and test three
models built with different techniques. The best performance was produced by
the Bidirectional Long Short Term Memory (LSTM) Neural Network model, achiving
91.57\% accuracy when tested against the data from the testing set.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16686" title="Abstract">arXiv:2310.16686</a> [<a href="/pdf/2310.16686" title="Download PDF">pdf</a>, <a href="/format/2310.16686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamics Generalisation in Reinforcement Learning via Adaptive  Context-Aware Policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beukman%2C+M">Michael Beukman</a>, 
<a href="/search/cs?searchtype=author&query=Jarvis%2C+D">Devon Jarvis</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+R">Richard Klein</a>, 
<a href="/search/cs?searchtype=author&query=James%2C+S">Steven James</a>, 
<a href="/search/cs?searchtype=author&query=Rosman%2C+B">Benjamin Rosman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">While reinforcement learning has achieved remarkable successes in several
domains, its real-world application is limited due to many methods failing to
generalise to unfamiliar conditions. In this work, we consider the problem of
generalising to new transition dynamics, corresponding to cases in which the
environment's response to the agent's actions differs. For example, the
gravitational force exerted on a robot depends on its mass and changes the
robot's mobility. Consequently, in such cases, it is necessary to condition an
agent's actions on extrinsic state information and pertinent contextual
information reflecting how the environment responds. While the need for
context-sensitive policies has been established, the manner in which context is
incorporated architecturally has received less attention. Thus, in this work,
we present an investigation into how context information should be incorporated
into behaviour learning to improve generalisation. To this end, we introduce a
neural network architecture, the Decision Adapter, which generates the weights
of an adapter module and conditions the behaviour of an agent on the context
information. We show that the Decision Adapter is a useful generalisation of a
previously proposed architecture and empirically demonstrate that it results in
superior generalisation performance compared to previous approaches in several
environments. Beyond this, the Decision Adapter is more robust to irrelevant
distractor variables than several alternative methods.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16688" title="Abstract">arXiv:2310.16688</a> [<a href="/pdf/2310.16688" title="Download PDF">pdf</a>, <a href="/format/2310.16688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning-based adaption of robotic friction models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scholl%2C+P">Philipp Scholl</a>, 
<a href="/search/cs?searchtype=author&query=Iskandar%2C+M">Maged Iskandar</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+S">Sebastian Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jinoh Lee</a>, 
<a href="/search/cs?searchtype=author&query=Bacho%2C+A">Aras Bacho</a>, 
<a href="/search/cs?searchtype=author&query=Dietrich%2C+A">Alexander Dietrich</a>, 
<a href="/search/cs?searchtype=author&query=Albu-Sch%C3%A4ffer%2C+A">Alin Albu-Sch&#xe4;ffer</a>, 
<a href="/search/cs?searchtype=author&query=Kutyniok%2C+G">Gitta Kutyniok</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the Fourth Industrial Revolution, wherein artificial intelligence and the
automation of machines occupy a central role, the deployment of robots is
indispensable. However, the manufacturing process using robots, especially in
collaboration with humans, is highly intricate. In particular, modeling the
friction torque in robotic joints is a longstanding problem due to the lack of
a good mathematical description. This motivates the usage of data-driven
methods in recent works. However, model-based and data-driven models often
exhibit limitations in their ability to generalize beyond the specific dynamics
they were trained on, as we demonstrate in this paper. To address this
challenge, we introduce a novel approach based on residual learning, which aims
to adapt an existing friction model to new dynamics using as little data as
possible. We validate our approach by training a base neural network on a
symmetric friction data set to learn an accurate relation between the velocity
and the friction torque. Subsequently, to adapt to more complex asymmetric
settings, we train a second network on a small dataset, focusing on predicting
the residual of the initial network's output. By combining the output of both
networks in a suitable manner, our proposed estimator outperforms the
conventional model-based approach and the base neural network significantly.
Furthermore, we evaluate our method on trajectories involving external loads
and still observe a substantial improvement, approximately 60-70\%, over the
conventional approach. Our method does not rely on data with external load
during training, eliminating the need for external torque sensors. This
demonstrates the generalization capability of our approach, even with a small
amount of data-only 43 seconds of a robot movement-enabling adaptation to
diverse scenarios based on prior knowledge about friction in different
settings.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16694" title="Abstract">arXiv:2310.16694</a> [<a href="/pdf/2310.16694" title="Download PDF">pdf</a>, <a href="/format/2310.16694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DSAM-GN:Graph Network based on Dynamic Similarity Adjacency Matrices for  Vehicle Re-identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiao%2C+Y">Yuejun Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+S">Song Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingsong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dingding Han</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qingli Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yue Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by the 20th Pacific Rim International Conference on Artificial Intelligence in 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In recent years, vehicle re-identification (Re-ID) has gained increasing
importance in various applications such as assisted driving systems, traffic
flow management, and vehicle tracking, due to the growth of intelligent
transportation systems. However, the presence of extraneous background
information and occlusions can interfere with the learning of discriminative
features, leading to significant variations in the same vehicle image across
different scenarios. This paper proposes a method, named graph network based on
dynamic similarity adjacency matrices (DSAM-GN), which incorporates a novel
approach for constructing adjacency matrices to capture spatial relationships
of local features and reduce background noise. Specifically, the proposed
method divides the extracted vehicle features into different patches as nodes
within the graph network. A spatial attention-based similarity adjacency matrix
generation (SASAMG) module is employed to compute similarity matrices of nodes,
and a dynamic erasure operation is applied to disconnect nodes with low
similarity, resulting in similarity adjacency matrices. Finally, the nodes and
similarity adjacency matrices are fed into graph networks to extract more
discriminative features for vehicle Re-ID. Experimental results on public
datasets VeRi-776 and VehicleID demonstrate the effectiveness of the proposed
method compared with recent works.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16695" title="Abstract">arXiv:2310.16695</a> [<a href="/pdf/2310.16695" title="Download PDF">pdf</a>, <a href="/format/2310.16695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Pointwise to Powerhouse: Initialising Neural Networks with  Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harder%2C+C">Christian Harder</a>, 
<a href="/search/cs?searchtype=author&query=Fuchs%2C+M">Moritz Fuchs</a>, 
<a href="/search/cs?searchtype=author&query=Tolkach%2C+Y">Yuri Tolkach</a>, 
<a href="/search/cs?searchtype=author&query=Mukhopadhyay%2C+A">Anirban Mukhopadhyay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Traditional initialisation methods, e.g. He and Xavier, have been effective
in avoiding the problem of vanishing or exploding gradients in neural networks.
However, they only use simple pointwise distributions, which model
one-dimensional variables. Moreover, they ignore most information about the
architecture and disregard past training experiences. These limitations can be
overcome by employing generative models for initialisation. In this paper, we
introduce two groups of new initialisation methods. First, we locally
initialise weight groups by employing variational autoencoders. Secondly, we
globally initialise full weight sets by employing graph hypernetworks. We
thoroughly evaluate the impact of the employed generative models on
state-of-the-art neural networks in terms of accuracy, convergence speed and
ensembling. Our results show that global initialisations result in higher
accuracy and faster initial convergence speed. However, the implementation
through graph hypernetworks leads to diminished ensemble performance on out of
distribution data. To counteract, we propose a modification called noise graph
hypernetwork, which encourages diversity in the produced ensemble members.
Furthermore, our approach might be able to transfer learned knowledge to
different image distributions. Our work provides insights into the potential,
the trade-offs and possible modifications of these new initialisation methods.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16696" title="Abstract">arXiv:2310.16696</a> [<a href="/pdf/2310.16696" title="Download PDF">pdf</a>, <a href="/format/2310.16696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable time series neural representation for classification  purposes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naour%2C+E+L">Etienne Le Naour</a>, 
<a href="/search/cs?searchtype=author&query=Agoua%2C+G">Ghislain Agoua</a>, 
<a href="/search/cs?searchtype=author&query=Baskiotis%2C+N">Nicolas Baskiotis</a>, 
<a href="/search/cs?searchtype=author&query=Guigue%2C+V">Vincent Guigue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Data Science and Advanced Analytics (DSAA) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Deep learning has made significant advances in creating efficient
representations of time series data by automatically identifying complex
patterns. However, these approaches lack interpretability, as the time series
is transformed into a latent vector that is not easily interpretable. On the
other hand, Symbolic Aggregate approximation (SAX) methods allow the creation
of symbolic representations that can be interpreted but do not capture complex
patterns effectively. In this work, we propose a set of requirements for a
neural representation of univariate time series to be interpretable. We propose
a new unsupervised neural architecture that meets these requirements. The
proposed model produces consistent, discrete, interpretable, and visualizable
representations. The model is learned independently of any downstream tasks in
an unsupervised setting to ensure robustness. As a demonstration of the
effectiveness of the proposed model, we propose experiments on classification
tasks using UCR archive datasets. The obtained results are extensively compared
to other interpretable models and state-of-the-art neural representation
learning models. The experiments show that the proposed model yields, on
average better results than other interpretable approaches on multiple
datasets. We also present qualitative experiments to asses the interpretability
of the approach.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16697" title="Abstract">arXiv:2310.16697</a> [<a href="/pdf/2310.16697" title="Download PDF">pdf</a>, <a href="/format/2310.16697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $O(1/\varepsilon)$ is the answer in online weighted throughput  maximization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eberle%2C+F">Franziska Eberle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We study a fundamental online scheduling problem where jobs with processing
times, weights, and deadlines arrive online over time at their release dates.
The task is to preemptively schedule these jobs on a single or multiple
(possibly unrelated) machines with the objective to maximize the weighted
throughput, the total weight of jobs that complete before their deadline. To
overcome known lower bounds for the competitive analysis, we assume that each
job arrives with some slack $\varepsilon &gt; 0$; that is, the time window for
processing job $j$ on any machine $i$ on which it can be executed has length at
least $(1+\varepsilon)$ times $j$'s processing time on machine $i$. Our
contribution is a best possible online algorithm for weighted throughput
maximization on unrelated machines: Our algorithm is
$O\big(\frac1\varepsilon\big)$-competitive, which matches the lower bound for
unweighted throughput maximization on a single machine. Even for a single
machine, it was not known whether the problem with weighted jobs is "harder"
than the problem with unweighted jobs. Thus, we answer this question and close
weighted throughput maximization on a single machine with a best possible
competitive ratio $\Theta\big(\frac1\varepsilon\big)$. While we focus on
non-migratory schedules, our algorithm achieves the same (up to constants)
performance guarantee when compared to an optimal migratory schedule.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16700" title="Abstract">arXiv:2310.16700</a> [<a href="/pdf/2310.16700" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Streamlining Knowledge Graph Construction with a fa&#xe7;ade: The SPARQL  Anything project
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asprino%2C+L">Luigi Asprino</a>, 
<a href="/search/cs?searchtype=author&query=Daga%2C+E">Enrico Daga</a>, 
<a href="/search/cs?searchtype=author&query=Dowdy%2C+J">Justin Dowdy</a>, 
<a href="/search/cs?searchtype=author&query=Mulholland%2C+P">Paul Mulholland</a>, 
<a href="/search/cs?searchtype=author&query=Gangemi%2C+A">Aldo Gangemi</a>, 
<a href="/search/cs?searchtype=author&query=Ratta%2C+M">Marco Ratta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2nd EuropeaN Data conference On Reference data and SEmantics
  (ENDORSE) 2023. Virtual event 14-16 March 2023. Proceedings, Publications
  Office of the European Union, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">What should a data integration framework for knowledge engineers look like?
Recent research on Knowledge Graph construction proposes the design of a
fa\c{c}ade, a notion borrowed from object-oriented software engineering. This
idea is applied to SPARQL Anything, a system that allows querying heterogeneous
resources as-if they were in RDF, in plain SPARQL 1.1, by overloading the
SERVICE clause. SPARQL Anything supports a wide variety of file formats, from
popular ones (CSV, JSON, XML, Spreadsheets) to others that are not supported by
alternative solutions (Markdown, YAML, DOCx, Bibtex). Features include querying
Web APIs with high flexibility, parametrised queries, and chaining multiple
transformations into complex pipelines. In this paper, we describe the design
rationale and software architecture of the SPARQL Anything system. We provide
references to an extensive set of reusable, real-world scenarios from various
application domains. We report on the value-to-users of the founding
assumptions of its design, compared to alternative solutions through a
community survey and a field report from the industry.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16704" title="Abstract">arXiv:2310.16704</a> [<a href="/pdf/2310.16704" title="Download PDF">pdf</a>, <a href="/format/2310.16704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-centred explanation of rule-based decision-making systems in the  legal domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zuurmond%2C+S">Suzan Zuurmond</a>, 
<a href="/search/cs?searchtype=author&query=Borg%2C+A">AnneMarie Borg</a>, 
<a href="/search/cs?searchtype=author&query=van+Kempen%2C+M">Matthijs van Kempen</a>, 
<a href="/search/cs?searchtype=author&query=Wieten%2C+R">Remi Wieten</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the full version of a demo at the 36th International Conference on Legal Knowledge and Information Systems (JURIX'23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">We propose a human-centred explanation method for rule-based automated
decision-making systems in the legal domain. Firstly, we establish a conceptual
framework for developing explanation methods, representing its key internal
components (content, communication and adaptation) and external dependencies
(decision-making system, human recipient and domain). Secondly, we propose an
explanation method that uses a graph database to enable question-driven
explanations and multimedia display. This way, we can tailor the explanation to
the user. Finally, we show how our conceptual framework is applicable to a
real-world scenario at the Dutch Tax and Customs Administration and implement
our explanation method for this scenario.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16705" title="Abstract">arXiv:2310.16705</a> [<a href="/pdf/2310.16705" title="Download PDF">pdf</a>, <a href="/format/2310.16705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wasserstein Gradient Flow over Variational Parameter Space for  Variational Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D+H">Dai Hai Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Sakurai%2C+T">Tetsuya Sakurai</a>, 
<a href="/search/cs?searchtype=author&query=Mamitsuka%2C+H">Hiroshi Mamitsuka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Variational inference (VI) can be cast as an optimization problem in which
the variational parameters are tuned to closely align a variational
distribution with the true posterior. The optimization task can be approached
through vanilla gradient descent in black-box VI or natural-gradient descent in
natural-gradient VI. In this work, we reframe VI as the optimization of an
objective that concerns probability distributions defined over a
\textit{variational parameter space}. Subsequently, we propose Wasserstein
gradient descent for tackling this optimization problem. Notably, the
optimization techniques, namely black-box VI and natural-gradient VI, can be
reinterpreted as specific instances of the proposed Wasserstein gradient
descent. To enhance the efficiency of optimization, we develop practical
methods for numerically solving the discrete gradient flows. We validate the
effectiveness of the proposed methods through empirical experiments on a
synthetic dataset, supplemented by theoretical analyses.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16706" title="Abstract">arXiv:2310.16706</a> [<a href="/pdf/2310.16706" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nighttime Driver Behavior Prediction Using Taillight Signal Recognition  via CNN-SVM Classifier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barshooi%2C+A+H">Amir Hossein Barshooi</a>, 
<a href="/search/cs?searchtype=author&query=Bagheri%2C+E">Elmira Bagheri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper aims to enhance the ability to predict nighttime driving behavior
by identifying taillights of both human-driven and autonomous vehicles. The
proposed model incorporates a customized detector designed to accurately detect
front-vehicle taillights on the road. At the beginning of the detector, a
learnable pre-processing block is implemented, which extracts deep features
from input images and calculates the data rarity for each feature. In the next
step, drawing inspiration from soft attention, a weighted binary mask is
designed that guides the model to focus more on predetermined regions. This
research utilizes Convolutional Neural Networks (CNNs) to extract
distinguishing characteristics from these areas, then reduces dimensions using
Principal Component Analysis (PCA). Finally, the Support Vector Machine (SVM)
is used to predict the behavior of the vehicles. To train and evaluate the
model, a large-scale dataset is collected from two types of dash-cams and
Insta360 cameras from the rear view of Ford Motor Company vehicles. This
dataset includes over 12k frames captured during both daytime and nighttime
hours. To address the limited nighttime data, a unique pixel-wise image
processing technique is implemented to convert daytime images into realistic
night images. The findings from the experiments demonstrate that the proposed
methodology can accurately categorize vehicle behavior with 92.14% accuracy,
97.38% specificity, 92.09% sensitivity, 92.10% F1-measure, and 0.895 Cohen's
Kappa Statistic. Further details are available at
https://github.com/DeepCar/Taillight_Recognition.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16712" title="Abstract">arXiv:2310.16712</a> [<a href="/pdf/2310.16712" title="Download PDF">pdf</a>, <a href="/format/2310.16712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Performance Predictors are good initializers for Architecture Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jawahar%2C+G">Ganesh Jawahar</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>, 
<a href="/search/cs?searchtype=author&query=Lakshmanan%2C+L+V+S">Laks V. S. Lakshmanan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+D">Dujian Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have become an integral component in solving a
wide range of NLP tasks. In this work, we explore a novel use case of using
LLMs to build performance predictors (PP): models that, given a specific deep
neural network architecture, predict its performance on a downstream task. We
design PP prompts for LLMs consisting of: (i) role: description of the role
assigned to the LLM, (ii) instructions: set of instructions to be followed by
the LLM to carry out performance prediction, (iii) hyperparameters: a
definition of each architecture-specific hyperparameter and (iv)
demonstrations: sample architectures along with their efficiency metrics and
'training from scratch' performance. For machine translation (MT) tasks, we
discover that GPT-4 with our PP prompts (LLM-PP) can predict the performance of
architecture with a mean absolute error matching the SOTA and a marginal
degradation in rank correlation coefficient compared to SOTA performance
predictors. Further, we show that the predictions from LLM-PP can be distilled
to a small regression model (LLM-Distill-PP). LLM-Distill-PP models
surprisingly retain the performance of LLM-PP largely and can be a
cost-effective alternative for heavy use cases of performance estimation.
Specifically, for neural architecture search (NAS), we propose a Hybrid-Search
algorithm for NAS (HS-NAS), which uses LLM-Distill-PP for the initial part of
search, resorting to the baseline predictor for rest of the search. We show
that HS-NAS performs very similar to SOTA NAS across benchmarks, reduces search
hours by 50% roughly, and in some cases, improves latency, GFLOPs, and model
size.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16713" title="Abstract">arXiv:2310.16713</a> [<a href="/pdf/2310.16713" title="Download PDF">pdf</a>, <a href="/format/2310.16713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SkyMath: Technical Report
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haihua Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wenjun Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenxia Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yifu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lunan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jianfei Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+T">Tianwen Wei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Biye Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bo Zhu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jujie He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guoliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuejie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xilin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Rui Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have shown great potential to solve varieties of
natural language processing (NLP) tasks, including mathematical reasoning. In
this work, we present SkyMath, a large language model for mathematics with 13
billion parameters. By applying self-compare fine-tuning, we have enhanced
mathematical reasoning abilities of Skywork-13B-Base remarkably. On GSM8K,
SkyMath outperforms all known open-source models of similar size and has
established a new SOTA performance.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16717" title="Abstract">arXiv:2310.16717</a> [<a href="/pdf/2310.16717" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rebuild City Buildings from Off-Nadir Aerial Images with Offset-Building  Model (OBM)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kai Li</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yupeng Deng</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Y">Yunlong Kong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Diyou Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingbo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Y">Yu Meng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Junxian Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Accurate measurement of the offset from roof-to-footprint in
very-high-resolution remote sensing imagery is crucial for urban information
extraction tasks. With the help of deep learning, existing methods typically
rely on two-stage CNN models to extract regions of interest on building feature
maps. At the first stage, a Region Proposal Network (RPN) is applied to extract
thousands of ROIs (Region of Interests) which will post-imported into a
Region-based Convolutional Neural Networks (RCNN) to extract wanted
information. However, because of inflexible RPN, these methods often lack
effective user interaction, encounter difficulties in instance correspondence,
and struggle to keep up with the advancements in general artificial
intelligence. This paper introduces an interactive Transformer model combined
with a prompt encoder to precisely extract building segmentation as well as the
offset vectors from roofs to footprints. In our model, a powerful module,
namely ROAM, was tailored for common problems in predicting roof-to-footprint
offsets. We tested our model's feasibility on the publicly available BONAI
dataset, achieving a significant reduction in Prompt-Instance-Level offset
errors ranging from 14.6% to 16.3%. Additionally, we developed a Distance-NMS
algorithm tailored for large-scale building offsets, significantly enhancing
the accuracy of predicted building offset angles and lengths in a
straightforward and efficient manner. To further validate the model's
robustness, we created a new test set using 0.5m remote sensing imagery from
Huizhou, China, for inference testing. Our code, training methods, and the
updated dataset will be accessable at https://github.com/likaiucas.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16723" title="Abstract">arXiv:2310.16723</a> [<a href="/pdf/2310.16723" title="Download PDF">pdf</a>, <a href="/format/2310.16723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harmonic model predictive control for tracking periodic references
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Krupa%2C+P">Pablo Krupa</a>, 
<a href="/search/eess?searchtype=author&query=Limon%2C+D">Daniel Limon</a>, 
<a href="/search/eess?searchtype=author&query=Bemporad%2C+A">Alberto Bemporad</a>, 
<a href="/search/eess?searchtype=author&query=Alamo%2C+T">Teodoro Alamo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> (11 pages, 14 figures)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Harmonic model predictive control (HMPC) is a recent model predictive control
(MPC) formulation for tracking piece-wise constant references that includes a
parameterized artificial harmonic reference as a decision variable, resulting
in an increased performance and domain of attraction with respect to other MPC
formulations. This article presents an extension of the HMPC formulation to
track periodic harmonic references and discusses its use to track arbitrary
references. The proposed formulation inherits the benefits of its predecessor,
namely its good performance and large domain of attraction when using small
prediction horizons, and that the complexity of its optimization problem does
not depend on the period of the periodic reference. We show closed-loop results
discussing its performance and comparing it to other MPC formulations.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16727" title="Abstract">arXiv:2310.16727</a> [<a href="/pdf/2310.16727" title="Download PDF">pdf</a>, <a href="/format/2310.16727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Hazard Management: A framework for the systematic management of root  causes for AI risks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schnitzer%2C+R">Ronald Schnitzer</a>, 
<a href="/search/cs?searchtype=author&query=Hapfelmeier%2C+A">Andreas Hapfelmeier</a>, 
<a href="/search/cs?searchtype=author&query=Gaube%2C+S">Sven Gaube</a>, 
<a href="/search/cs?searchtype=author&query=Zillner%2C+S">Sonja Zillner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent advancements in the field of Artificial Intelligence (AI) establish
the basis to address challenging tasks. However, with the integration of AI,
new risks arise. Therefore, to benefit from its advantages, it is essential to
adequately handle the risks associated with AI. Existing risk management
processes in related fields, such as software systems, need to sufficiently
consider the specifics of AI. A key challenge is to systematically and
transparently identify and address AI risks' root causes - also called AI
hazards. This paper introduces the AI Hazard Management (AIHM) framework, which
provides a structured process to systematically identify, assess, and treat AI
hazards. The proposed process is conducted in parallel with the development to
ensure that any AI hazard is captured at the earliest possible stage of the AI
system's life cycle. In addition, to ensure the AI system's auditability, the
proposed framework systematically documents evidence that the potential impact
of identified AI hazards could be reduced to a tolerable level. The framework
builds upon an AI hazard list from a comprehensive state-of-the-art analysis.
Also, we provide a taxonomy that supports the optimal treatment of the
identified AI hazards. Additionally, we illustrate how the AIHM framework can
increase the overall quality of a power grid AI use case by systematically
reducing the impact of identified hazards to an acceptable level.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16730" title="Abstract">arXiv:2310.16730</a> [<a href="/pdf/2310.16730" title="Download PDF">pdf</a>, <a href="/format/2310.16730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiPrompter: Cooperative Prompt Optimization with Multi-Agent  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dong-Ki Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sohn%2C+S">Sungryull Sohn</a>, 
<a href="/search/cs?searchtype=author&query=Logeswaran%2C+L">Lajanugen Logeswaran</a>, 
<a href="/search/cs?searchtype=author&query=Shim%2C+D">Dongsub Shim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Honglak Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, there has been an increasing interest in automated prompt
optimization based on reinforcement learning (RL). This approach offers
important advantages, such as generating interpretable prompts and being
compatible with black-box foundation models. However, the substantial prompt
space size poses challenges for RL-based methods, often leading to suboptimal
policy convergence. This paper introduces MultiPrompter, a new framework that
views prompt optimization as a cooperative game between prompters which take
turns composing a prompt together. Our cooperative prompt optimization
effectively reduces the problem size and helps prompters learn optimal prompts.
We test our method on the text-to-image task and show its ability to generate
higher-quality images than baselines.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16731" title="Abstract">arXiv:2310.16731</a> [<a href="/pdf/2310.16731" title="Download PDF">pdf</a>, <a href="/format/2310.16731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mirzaee%2C+R">Roshanak Mirzaee</a>, 
<a href="/search/cs?searchtype=author&query=Kordjamshidi%2C+P">Parisa Kordjamshidi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in EMNLP-Finding 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Spatial reasoning over text is challenging as the models not only need to
extract the direct spatial information from the text but also reason over those
and infer implicit spatial relations. Recent studies highlight the struggles
even large language models encounter when it comes to performing spatial
reasoning over text. In this paper, we explore the potential benefits of
disentangling the processes of information extraction and reasoning in models
to address this challenge. To explore this, we design various models that
disentangle extraction and reasoning(either symbolic or neural) and compare
them with state-of-the-art(SOTA) baselines with no explicit design for these
parts. Our experimental results consistently demonstrate the efficacy of
disentangling, showcasing its ability to enhance models' generalizability
within realistic data domains.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16732" title="Abstract">arXiv:2310.16732</a> [<a href="/pdf/2310.16732" title="Download PDF">pdf</a>, <a href="/format/2310.16732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A No-Reference Quality Assessment Method for Digital Human Head
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yingjie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zicheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+X">Xiongkuo Min</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xianghe Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+G">Guangtao Zhai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">In recent years, digital humans have been widely applied in augmented/virtual
reality (A/VR), where viewers are allowed to freely observe and interact with
the volumetric content. However, the digital humans may be degraded with
various distortions during the procedure of generation and transmission.
Moreover, little effort has been put into the perceptual quality assessment of
digital humans. Therefore, it is urgent to carry out objective quality
assessment methods to tackle the challenge of digital human quality assessment
(DHQA). In this paper, we develop a novel no-reference (NR) method based on
Transformer to deal with DHQA in a multi-task manner. Specifically, the front
2D projections of the digital humans are rendered as inputs and the vision
transformer (ViT) is employed for the feature extraction. Then we design a
multi-task module to jointly classify the distortion types and predict the
perceptual quality levels of digital humans. The experimental results show that
the proposed method well correlates with the subjective ratings and outperforms
the state-of-the-art quality assessment methods.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16734" title="Abstract">arXiv:2310.16734</a> [<a href="/pdf/2310.16734" title="Download PDF">pdf</a>, <a href="/ps/2310.16734" title="Download PostScript">ps</a>, <a href="/format/2310.16734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Gaussian approximation for the magnetic Schr&#xf6;dinger  equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Burkhard%2C+S">Selina Burkhard</a>, 
<a href="/search/math?searchtype=author&query=D%C3%B6rich%2C+B">Benjamin D&#xf6;rich</a>, 
<a href="/search/math?searchtype=author&query=Hochbruck%2C+M">Marlis Hochbruck</a>, 
<a href="/search/math?searchtype=author&query=Lasser%2C+C">Caroline Lasser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph)

</div>
<p class="mathjax">In the present paper we consider the semiclassical magnetic Schr\"odinger
equation, which describes the dynamics of particles under the influence of a
magnetic field. The solution of the time-dependent Schr\"odinger equation is
approximated by a single Gaussian wave packet via the time-dependent
Dirac--Frenkel variational principle. For the approximation we derive ordinary
differential equations of motion for the parameters of the variational
solution. Moreover, we prove $L^2$-error bounds and observable error bounds for
the approximating Gaussian wave packet.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16735" title="Abstract">arXiv:2310.16735</a> [<a href="/pdf/2310.16735" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mapping the Empirical Evidence of the GDPR (In-)Effectiveness: A  Systematic Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zihao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenkai Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yueming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Aolan Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Information Theory (cs.IT)

</div>
<p class="mathjax">In the realm of data protection, a striking disconnect prevails between
traditional domains of doctrinal, legal, theoretical, and policy-based
inquiries and a burgeoning body of empirical evidence. Much of the scholarly
and regulatory discourse remains entrenched in abstract legal principles or
normative frameworks, leaving the empirical landscape uncharted or minimally
engaged. Since the birth of EU data protection law, a modest body of empirical
evidence has been generated but remains widely scattered and unexamined. Such
evidence offers vital insights into the perception, impact, clarity, and
effects of data protection measures but languishes on the periphery,
inadequately integrated into the broader conversation. To make a meaningful
connection, we conduct a comprehensive review and synthesis of empirical
research spanning nearly three decades (1995- March 2022), advocating for a
more robust integration of empirical evidence into the evaluation and review of
the GDPR, while laying a methodological foundation for future empirical
research.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16737" title="Abstract">arXiv:2310.16737</a> [<a href="/pdf/2310.16737" title="Download PDF">pdf</a>, <a href="/format/2310.16737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Translating Universal Scene Descriptions into Knowledge Graphs for  Robotic Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+G+H">Giang Hoang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Bessler%2C+D">Daniel Bessler</a>, 
<a href="/search/cs?searchtype=author&query=Stelter%2C+S">Simon Stelter</a>, 
<a href="/search/cs?searchtype=author&query=Pomarlan%2C+M">Mihai Pomarlan</a>, 
<a href="/search/cs?searchtype=author&query=Beetz%2C+M">Michael Beetz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR)

</div>
<p class="mathjax">Robots performing human-scale manipulation tasks require an extensive amount
of knowledge about their surroundings in order to perform their actions
competently and human-like. In this work, we investigate the use of virtual
reality technology as an implementation for robot environment modeling, and
present a technique for translating scene graphs into knowledge bases. To this
end, we take advantage of the Universal Scene Description (USD) format which is
an emerging standard for the authoring, visualization and simulation of complex
environments. We investigate the conversion of USD-based environment models
into Knowledge Graph (KG) representations that facilitate semantic querying and
integration with additional knowledge sources.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16738" title="Abstract">arXiv:2310.16738</a> [<a href="/pdf/2310.16738" title="Download PDF">pdf</a>, <a href="/format/2310.16738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Conversational Recommendation Systems via Bias Analysis and  Language-Model-Enhanced Data Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+H+A">Hossein A. Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiqun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yilmaz%2C+E">Emine Yilmaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Conversational Recommendation System (CRS) is a rapidly growing research area
that has gained significant attention alongside advancements in language
modelling techniques. However, the current state of conversational
recommendation faces numerous challenges due to its relative novelty and
limited existing contributions. In this study, we delve into benchmark datasets
for developing CRS models and address potential biases arising from the
feedback loop inherent in multi-turn interactions, including selection bias and
multiple popularity bias variants. Drawing inspiration from the success of
generative data via using language models and data augmentation techniques, we
present two novel strategies, 'Once-Aug' and 'PopNudge', to enhance model
performance while mitigating biases. Through extensive experiments on ReDial
and TG-ReDial benchmark datasets, we show a consistent improvement of CRS
techniques with our data augmentation approaches and offer additional insights
on addressing multiple newly formulated biases.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16740" title="Abstract">arXiv:2310.16740</a> [<a href="/pdf/2310.16740" title="Download PDF">pdf</a>, <a href="/ps/2310.16740" title="Download PostScript">ps</a>, <a href="/format/2310.16740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reachability in Fixed VASS: Expressiveness and Lower Bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Draghici%2C+A">Andrei Draghici</a>, 
<a href="/search/cs?searchtype=author&query=Haase%2C+C">Christoph Haase</a>, 
<a href="/search/cs?searchtype=author&query=Ryzhikov%2C+A">Andrew Ryzhikov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">The recent years have seen remarkable progress in establishing the complexity
of the reachability problem for vector addition systems with states (VASS),
equivalently known as Petri nets. Existing work primarily considers the case in
which both the VASS as well as the initial and target configurations are part
of the input. In this paper, we investigate the reachability problem in the
setting where the VASS is fixed and only the initial configuration is variable.
We show that fixed VASS fully express arithmetic on initial segments of the
natural numbers. It follows that there is a very weak reduction from any fixed
such number-theoretic predicate (e.g. primality or square-freeness) to
reachability in fixed VASS where configurations are presented in unary. If
configurations are given in binary, we show that there is a fixed VASS with
five counters whose reachability problem is PSPACE-hard.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16741" title="Abstract">arXiv:2310.16741</a> [<a href="/pdf/2310.16741" title="Download PDF">pdf</a>, <a href="/format/2310.16741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Latent Transformer: Efficient Modelling of Stochastically  Forced Zonal Jets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shokar%2C+I+J+S">Ira J. S. Shokar</a>, 
<a href="/search/cs?searchtype=author&query=Kerswell%2C+R+R">Rich R. Kerswell</a>, 
<a href="/search/cs?searchtype=author&query=Haynes%2C+P+H">Peter H. Haynes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">We introduce the 'Stochastic Latent Transformer', a probabilistic deep
learning approach for efficient reduced-order modelling of stochastic partial
differential equations (SPDEs). Despite recent advances in deep learning for
fluid mechanics, limited research has explored modelling stochastically driven
flows - which play a crucial role in understanding a broad spectrum of
phenomena, from jets on giant planets to ocean circulation and the variability
of midlatitude weather. The model architecture consists of a
stochastically-forced transformer, paired with a translation-equivariant
autoencoder, that we demonstrate is capable of reproducing system dynamics
across various integration periods. We demonstrate its effectiveness applied to
a well-researched zonal jet system, with the neural network achieving a
five-order-of-magnitude speedup compared to numerical integration. This
facilitates the cost-effective generation of large ensembles, enabling the
exploration of statistical questions concerning probabilities of spontaneous
transition events.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16745" title="Abstract">arXiv:2310.16745</a> [<a href="/pdf/2310.16745" title="Download PDF">pdf</a>, <a href="/format/2310.16745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design Space Exploration of Sparsity-Aware Application-Specific Spiking  Neural Network Accelerators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Svoboda%2C+I+A+K">Ilkin Aliyev. Kama Svoboda</a>, 
<a href="/search/cs?searchtype=author&query=Adegbija%2C+T">Tosiron Adegbija</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Spiking Neural Networks (SNNs) offer a promising alternative to Artificial
Neural Networks (ANNs) for deep learning applications, particularly in
resource-constrained systems. This is largely due to their inherent sparsity,
influenced by factors such as the input dataset, the length of the spike train,
and the network topology. While a few prior works have demonstrated the
advantages of incorporating sparsity into the hardware design, especially in
terms of reducing energy consumption, the impact on hardware resources has not
yet been explored. This is where design space exploration (DSE) becomes
crucial, as it allows for the optimization of hardware performance by tailoring
both the hardware and model parameters to suit specific application needs.
However, DSE can be extremely challenging given the potentially large design
space and the interplay of hardware architecture design choices and
application-specific model parameters.
<br />In this paper, we propose a flexible hardware design that leverages the
sparsity of SNNs to identify highly efficient, application-specific accelerator
designs. We develop a high-level, cycle-accurate simulation framework for this
hardware and demonstrate the framework's benefits in enabling detailed and
fine-grained exploration of SNN design choices, such as the layer-wise
logical-to-hardware ratio (LHR). Our experimental results show that our design
can (i) achieve up to $76\%$ reduction in hardware resources and (ii) deliver a
speed increase of up to $31.25\times$, while requiring $27\%$ fewer hardware
resources compared to sparsity-oblivious designs. We further showcase the
robustness of our framework by varying spike train lengths with different
neuron population sizes to find the optimal trade-off points between accuracy
and hardware latency.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16746" title="Abstract">arXiv:2310.16746</a> [<a href="/pdf/2310.16746" title="Download PDF">pdf</a>, <a href="/format/2310.16746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tripto%2C+N+I">Nafis Irtiza Tripto</a>, 
<a href="/search/cs?searchtype=author&query=Uchendu%2C+A">Adaku Uchendu</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Thai Le</a>, 
<a href="/search/cs?searchtype=author&query=Setzu%2C+M">Mattia Setzu</a>, 
<a href="/search/cs?searchtype=author&query=Giannotti%2C+F">Fosca Giannotti</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongwon Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, EMNLP-23 findings, 5 pages appendix, 6 figures, 17 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Authorship Analysis, also known as stylometry, has been an essential aspect
of Natural Language Processing (NLP) for a long time. Likewise, the recent
advancement of Large Language Models (LLMs) has made authorship analysis
increasingly crucial for distinguishing between human-written and AI-generated
texts. However, these authorship analysis tasks have primarily been focused on
written texts, not considering spoken texts. Thus, we introduce the largest
benchmark for spoken texts - HANSEN (Human ANd ai Spoken tExt beNchmark).
HANSEN encompasses meticulous curation of existing speech datasets accompanied
by transcripts, alongside the creation of novel AI-generated spoken text
datasets. Together, it comprises 17 human datasets, and AI-generated spoken
texts created using 3 prominent LLMs: ChatGPT, PaLM2, and Vicuna13B. To
evaluate and demonstrate the utility of HANSEN, we perform Authorship
Attribution (AA) &amp; Author Verification (AV) on human-spoken datasets and
conducted Human vs. AI spoken text detection using state-of-the-art (SOTA)
models. While SOTA methods, such as, character ngram or Transformer-based
model, exhibit similar AA &amp; AV performance in human-spoken datasets compared to
written ones, there is much room for improvement in AI-generated spoken text
detection. The HANSEN benchmark is available at:
https://huggingface.co/datasets/HANSEN-REPO/HANSEN.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16748" title="Abstract">arXiv:2310.16748</a> [<a href="/pdf/2310.16748" title="Download PDF">pdf</a>, <a href="/format/2310.16748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrated Freeway and Arterial Traffic Control to Improve Freeway  Mobility without Compromising Arterial Traffic Conditions Using Q-Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yuan%2C+T">Tianchen Yuan</a>, 
<a href="/search/eess?searchtype=author&query=Ioannou%2C+P+A">Petros A. Ioannou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Freeway and arterial transportation networks are operated individually in
most cities nowadays. The lack of coordination between the two increases the
severity of traffic congestion when they are heavily loaded. To address the
issue, we propose an integrated traffic control strategy that coordinates
freeway traffic control (variable speed limit control, lane change
recommendations, ramp metering) and arterial signal timing using Q-learning.
The agent is trained offline in a single-section road network first, and then
implemented online in a large simulation network with real-world traffic
demands. The online data are collected to further improve the agent's
performance via continuous learning. We observe significant reductions in
freeway travel time and number of stops and a slight increase in on-ramp queue
lengths by implementing the proposed approach in scenarios with traffic
congestion. Meanwhile, the queue lengths of adjacent arterial intersections are
maintained at the same level. The benefits of the coordination mechanism is
verified by comparing the proposed approach with an uncoordinated Q-learning
algorithm and a decentralized feedback control strategy.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16749" title="Abstract">arXiv:2310.16749</a> [<a href="/pdf/2310.16749" title="Download PDF">pdf</a>, <a href="/format/2310.16749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DISCO: A Large Scale Human Annotated Corpus for Disfluency Correction in  Indo-European Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhat%2C+V">Vineet Bhat</a>, 
<a href="/search/cs?searchtype=author&query=Jyothi%2C+P">Preethi Jyothi</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharyya%2C+P">Pushpak Bhattacharyya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Disfluency correction (DC) is the process of removing disfluent elements like
fillers, repetitions and corrections from spoken utterances to create readable
and interpretable text. DC is a vital post-processing step applied to Automatic
Speech Recognition (ASR) outputs, before subsequent processing by downstream
language understanding tasks. Existing DC research has primarily focused on
English due to the unavailability of large-scale open-source datasets. Towards
the goal of multilingual disfluency correction, we present a high-quality
human-annotated DC corpus covering four important Indo-European languages:
English, Hindi, German and French. We provide extensive analysis of results of
state-of-the-art DC models across all four languages obtaining F1 scores of
97.55 (English), 94.29 (Hindi), 95.89 (German) and 92.97 (French). To
demonstrate the benefits of DC on downstream tasks, we show that DC leads to
5.65 points increase in BLEU scores on average when used in conjunction with a
state-of-the-art Machine Translation (MT) system. We release code to run our
experiments along with our annotated dataset here.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16750" title="Abstract">arXiv:2310.16750</a> [<a href="/pdf/2310.16750" title="Download PDF">pdf</a>, <a href="/format/2310.16750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metrically Scaled Monocular Depth Estimation through Sparse Priors for  Underwater Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ebner%2C+L">Luca Ebner</a>, 
<a href="/search/cs?searchtype=author&query=Billings%2C+G">Gideon Billings</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+S">Stefan Williams</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">In this work, we address the problem of real-time dense depth estimation from
monocular images for mobile underwater vehicles. We formulate a deep learning
model that fuses sparse depth measurements from triangulated features to
improve the depth predictions and solve the problem of scale ambiguity. To
allow prior inputs of arbitrary sparsity, we apply a dense parameterization
method. Our model extends recent state-of-the-art approaches to monocular image
based depth estimation, using an efficient encoder-decoder backbone and modern
lightweight transformer optimization stage to encode global context. The
network is trained in a supervised fashion on the forward-looking underwater
dataset, FLSea. Evaluation results on this dataset demonstrate significant
improvement in depth prediction accuracy by the fusion of the sparse feature
priors. In addition, without any retraining, our method achieves similar depth
prediction accuracy on a downward looking dataset we collected with a diver
operated camera rig, conducting a survey of a coral reef. The method achieves
real-time performance, running at 160 FPS on a laptop GPU and 7 FPS on a single
CPU core and is suitable for direct deployment on embedded systems. The
implementation of this work is made publicly available at
https://github.com/ebnerluca/uw_depth.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16752" title="Abstract">arXiv:2310.16752</a> [<a href="/pdf/2310.16752" title="Download PDF">pdf</a>, <a href="/format/2310.16752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple, Scalable and Effective Clustering via One-Dimensional  Projections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Charikar%2C+M">Moses Charikar</a>, 
<a href="/search/cs?searchtype=author&query=Henzinger%2C+M">Monika Henzinger</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Lunjia Hu</a>, 
<a href="/search/cs?searchtype=author&query=V%C3%B6tsch%2C+M">Maxmilian V&#xf6;tsch</a>, 
<a href="/search/cs?searchtype=author&query=Waingarten%2C+E">Erik Waingarten</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 6 figures, to appear in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Clustering is a fundamental problem in unsupervised machine learning with
many applications in data analysis. Popular clustering algorithms such as
Lloyd's algorithm and $k$-means++ can take $\Omega(ndk)$ time when clustering
$n$ points in a $d$-dimensional space (represented by an $n\times d$ matrix
$X$) into $k$ clusters. In applications with moderate to large $k$, the
multiplicative $k$ factor can become very expensive. We introduce a simple
randomized clustering algorithm that provably runs in expected time
$O(\mathrm{nnz}(X) + n\log n)$ for arbitrary $k$. Here $\mathrm{nnz}(X)$ is the
total number of non-zero entries in the input dataset $X$, which is upper
bounded by $nd$ and can be significantly smaller for sparse datasets. We prove
that our algorithm achieves approximation ratio $\smash{\widetilde{O}(k^4)}$ on
any input dataset for the $k$-means objective. We also believe that our
theoretical analysis is of independent interest, as we show that the
approximation ratio of a $k$-means algorithm is approximately preserved under a
class of projections and that $k$-means++ seeding can be implemented in
expected $O(n \log n)$ time in one dimension. Finally, we show experimentally
that our clustering algorithm gives a new tradeoff between running time and
cluster quality compared to previous state-of-the-art methods for these tasks.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16753" title="Abstract">arXiv:2310.16753</a> [<a href="/pdf/2310.16753" title="Download PDF">pdf</a>, <a href="/format/2310.16753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PROMINET: Prototype-based Multi-View Network for Interpretable Email  Response Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Vijayaraghavan%2C+P">Prashanth Vijayaraghavan</a>, 
<a href="/search/cs?searchtype=author&query=Degan%2C+E">Ehsan Degan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (industry)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Email is a widely used tool for business communication, and email marketing
has emerged as a cost-effective strategy for enterprises. While previous
studies have examined factors affecting email marketing performance, limited
research has focused on understanding email response behavior by considering
email content and metadata. This study proposes a Prototype-based Multi-view
Network (PROMINET) that incorporates semantic and structural information from
email data. By utilizing prototype learning, the PROMINET model generates
latent exemplars, enabling interpretable email response prediction. The model
maps learned semantic and structural exemplars to observed samples in the
training data at different levels of granularity, such as document, sentence,
or phrase. The approach is evaluated on two real-world email datasets: the
Enron corpus and an in-house Email Marketing corpus. Experimental results
demonstrate that the PROMINET model outperforms baseline models, achieving a
~3% improvement in F1 score on both datasets. Additionally, the model provides
interpretability through prototypes at different granularity levels while
maintaining comparable performance to non-interpretable models. The learned
prototypes also show potential for generating suggestions to enhance email text
editing and improve the likelihood of effective email responses. This research
contributes to enhancing sender-receiver communication and customer engagement
in email interactions.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16754" title="Abstract">arXiv:2310.16754</a> [<a href="/pdf/2310.16754" title="Download PDF">pdf</a>, <a href="/format/2310.16754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAD -- Contextual Multi-modal Alignment for Dynamic AVQA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nadeem%2C+A">Asmar Nadeem</a>, 
<a href="/search/cs?searchtype=author&query=Hilton%2C+A">Adrian Hilton</a>, 
<a href="/search/cs?searchtype=author&query=Dawes%2C+R">Robert Dawes</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+G">Graham Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Mustafa%2C+A">Armin Mustafa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the context of Audio Visual Question Answering (AVQA) tasks, the audio
visual modalities could be learnt on three levels: 1) Spatial, 2) Temporal, and
3) Semantic. Existing AVQA methods suffer from two major shortcomings; the
audio-visual (AV) information passing through the network isn't aligned on
Spatial and Temporal levels; and, inter-modal (audio and visual) Semantic
information is often not balanced within a context; this results in poor
performance. In this paper, we propose a novel end-to-end Contextual
Multi-modal Alignment (CAD) network that addresses the challenges in AVQA
methods by i) introducing a parameter-free stochastic Contextual block that
ensures robust audio and visual alignment on the Spatial level; ii) proposing a
pre-training technique for dynamic audio and visual alignment on Temporal level
in a self-supervised setting, and iii) introducing a cross-attention mechanism
to balance audio and visual information on Semantic level. The proposed novel
CAD network improves the overall performance over the state-of-the-art methods
on average by 9.4% on the MUSIC-AVQA dataset. We also demonstrate that our
proposed contributions to AVQA can be added to the existing methods to improve
their performance without additional complexity requirements.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16755" title="Abstract">arXiv:2310.16755</a> [<a href="/pdf/2310.16755" title="Download PDF">pdf</a>, <a href="/format/2310.16755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning  in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yinghui He</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yufan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yilin Jia</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yulong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+N">Naihao Deng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Findings of EMNLP 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Theory of Mind (ToM) is the ability to reason about one's own and others'
mental states. ToM plays a critical role in the development of intelligence,
language understanding, and cognitive processes. While previous work has
primarily focused on first and second-order ToM, we explore higher-order ToM,
which involves recursive reasoning on others' beliefs. We introduce HI-TOM, a
Higher Order Theory of Mind benchmark. Our experimental evaluation using
various Large Language Models (LLMs) indicates a decline in performance on
higher-order ToM tasks, demonstrating the limitations of current LLMs. We
conduct a thorough analysis of different failure cases of LLMs, and share our
thoughts on the implications of our findings on the future of NLP.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16757" title="Abstract">arXiv:2310.16757</a> [<a href="/pdf/2310.16757" title="Download PDF">pdf</a>, <a href="/ps/2310.16757" title="Download PostScript">ps</a>, <a href="/format/2310.16757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> All-rounder: A flexible DNN accelerator with diverse data format support
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Noh%2C+S">Seock-Hwan Noh</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seungpyo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+B">Banseok Shin</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sehun Park</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+Y">Yongjoo Jang</a>, 
<a href="/search/cs?searchtype=author&query=Kung%2C+J">Jaeha Kung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Recognizing the explosive increase in the use of DNN-based applications,
several industrial companies developed a custom ASIC (e.g., Google TPU, IBM
RaPiD, Intel NNP-I/NNP-T) and constructed a hyperscale cloud infrastructure
with it. The ASIC performs operations of the inference or training process of
DNN models which are requested by users. Since the DNN models have different
data formats and types of operations, the ASIC needs to support diverse data
formats and generality for the operations. However, the conventional ASICs do
not fulfill these requirements. To overcome the limitations of it, we propose a
flexible DNN accelerator called All-rounder. The accelerator is designed with
an area-efficient multiplier supporting multiple precisions of integer and
floating point datatypes. In addition, it constitutes a flexibly fusible and
fissionable MAC array to support various types of DNN operations efficiently.
We implemented the register transfer level (RTL) design using Verilog and
synthesized it in 28nm CMOS technology. To examine practical effectiveness of
our proposed designs, we designed two multiply units and three state-of-the-art
DNN accelerators. We compare our multiplier with the multiply units and perform
architectural evaluation on performance and energy efficiency with eight
real-world DNN models. Furthermore, we compare benefits of the All-rounder
accelerator to a high-end GPU card, i.e., NVIDIA GeForce RTX30390. The proposed
All-rounder accelerator universally has speedup and high energy efficiency in
various DNN benchmarks than the baselines.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16760" title="Abstract">arXiv:2310.16760</a> [<a href="/pdf/2310.16760" title="Download PDF">pdf</a>, <a href="/format/2310.16760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Knowledge Awareness to improve Safety of Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Calvagna%2C+A">Andrea Calvagna</a>, 
<a href="/search/eess?searchtype=author&query=Ghosh%2C+A">Arabinda Ghosh</a>, 
<a href="/search/eess?searchtype=author&query=Soudjani%2C+S">Sadegh Soudjani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">We present a method, which incorporates knowledge awareness into the symbolic
computation of discrete controllers for reactive cyber physical systems, to
improve decision making about the unknown operating environment under
uncertain/incomplete inputs. Assuming an abstract model of the system and the
environment, we translate the knowledge awareness of the operating context into
linear temporal logic formulas and incorporate them into the system
specifications to synthesize a controller. The knowledge base is built upon an
ontology model of the environment objects and behavioural rules, which includes
also symbolic models of partial input features. The resulting symbolic
controller support smoother, early reactions, which improves the security of
the system over existing approaches based on incremental symbolic perception. A
motion planning case study for an autonomous vehicle has been implemented to
validate the approach, and presented results show significant improvements with
respect to safety of state-of-the-art symbolic controllers for reactive
systems.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16761" title="Abstract">arXiv:2310.16761</a> [<a href="/pdf/2310.16761" title="Download PDF">pdf</a>, <a href="/format/2310.16761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IntenDD: A Unified Contrastive Learning Approach for Intent Detection  and Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singhal%2C+B">Bhavuk Singhal</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Ashim Gupta</a>, 
<a href="/search/cs?searchtype=author&query=P%2C+S+V">Shivasankaran V P</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+A">Amrith Krishna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Identifying intents from dialogue utterances forms an integral component of
task-oriented dialogue systems. Intent-related tasks are typically formulated
either as a classification task, where the utterances are classified into
predefined categories or as a clustering task when new and previously unknown
intent categories need to be discovered from these utterances. Further, the
intent classification may be modeled in a multiclass (MC) or multilabel (ML)
setup. While typically these tasks are modeled as separate tasks, we propose
IntenDD, a unified approach leveraging a shared utterance encoding backbone.
IntenDD uses an entirely unsupervised contrastive learning strategy for
representation learning, where pseudo-labels for the unlabeled utterances are
generated based on their lexical features. Additionally, we introduce a
two-step post-processing setup for the classification tasks using modified
adsorption. Here, first, the residuals in the training data are propagated
followed by smoothing the labels both modeled in a transductive setting.
Through extensive evaluations on various benchmark datasets, we find that our
approach consistently outperforms competitive baselines across all three tasks.
On average, IntenDD reports percentage improvements of 2.32%, 1.26%, and 1.52%
in their respective metrics for few-shot MC, few-shot ML, and the intent
discovery tasks respectively.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16762" title="Abstract">arXiv:2310.16762</a> [<a href="/pdf/2310.16762" title="Download PDF">pdf</a>, <a href="/format/2310.16762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Infinite Needle in a Finite Haystack: Finding Infinite Counter-Models  in Deductive Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elad%2C+N">Neta Elad</a>, 
<a href="/search/cs?searchtype=author&query=Padon%2C+O">Oded Padon</a>, 
<a href="/search/cs?searchtype=author&query=Shoham%2C+S">Sharon Shoham</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">First-order logic, and quantifiers in particular, are widely used in
deductive verification. Quantifiers are essential for describing systems with
unbounded domains, but prove difficult for automated solvers. Significant
effort has been dedicated to finding quantifier instantiations that establish
unsatisfiability, thus ensuring validity of a system's verification conditions.
However, in many cases the formulas are satisfiable: this is often the case in
intermediate steps of the verification process. For such cases, existing tools
are limited to finding finite models as counterexamples. Yet, some quantified
formulas are satisfiable but only have infinite models. Such infinite
counter-models are especially typical when first-order logic is used to
approximate inductive definitions such as linked lists or the natural numbers.
The inability of solvers to find infinite models makes them diverge in these
cases. In this paper, we tackle the problem of finding such infinite models.
These models allow the user to identify and fix bugs in the modeling of the
system and its properties. Our approach consists of three parts. First, we
introduce symbolic structures as a way to represent certain infinite models.
Second, we describe an effective model finding procedure that symbolically
explores a given family of symbolic structures. Finally, we identify a new
decidable fragment of first-order logic that extends and subsumes the
many-sorted variant of EPR, where satisfiable formulas always have a model
representable by a symbolic structure within a known family. We evaluate our
approach on examples from the domains of distributed consensus protocols and of
heap-manipulating programs. Our implementation quickly finds infinite
counter-models that demonstrate the source of verification failures in a simple
way, while SMT solvers and theorem provers such as Z3, cvc5, and Vampire
diverge.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16763" title="Abstract">arXiv:2310.16763</a> [<a href="/pdf/2310.16763" title="Download PDF">pdf</a>, <a href="/format/2310.16763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SuperHF: Supervised Iterative Learning from Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukobi%2C+G">Gabriel Mukobi</a>, 
<a href="/search/cs?searchtype=author&query=Chatain%2C+P">Peter Chatain</a>, 
<a href="/search/cs?searchtype=author&query=Fong%2C+S">Su Fong</a>, 
<a href="/search/cs?searchtype=author&query=Windesheim%2C+R">Robert Windesheim</a>, 
<a href="/search/cs?searchtype=author&query=Kutyniok%2C+G">Gitta Kutyniok</a>, 
<a href="/search/cs?searchtype=author&query=Bhatia%2C+K">Kush Bhatia</a>, 
<a href="/search/cs?searchtype=author&query=Alberti%2C+S">Silas Alberti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Socially Responsible Language Modelling Research (SoLaR) workshop at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">While large language models demonstrate remarkable capabilities, they often
present challenges in terms of safety, alignment with human values, and
stability during training. Here, we focus on two prevalent methods used to
align these models, Supervised Fine-Tuning (SFT) and Reinforcement Learning
from Human Feedback (RLHF). SFT is simple and robust, powering a host of
open-source models, while RLHF is a more sophisticated method used in top-tier
models like ChatGPT but also suffers from instability and susceptibility to
reward hacking. We propose a novel approach, Supervised Iterative Learning from
Human Feedback (SuperHF), which seeks to leverage the strengths of both
methods. Our hypothesis is two-fold: that the reward model used in RLHF is
critical for efficient data use and model generalization and that the use of
Proximal Policy Optimization (PPO) in RLHF may not be necessary and could
contribute to instability issues. SuperHF replaces PPO with a simple supervised
loss and a Kullback-Leibler (KL) divergence prior. It creates its own training
data by repeatedly sampling a batch of model outputs and filtering them through
the reward model in an online learning regime. We then break down the reward
optimization problem into three components: robustly optimizing the training
rewards themselves, preventing reward hacking-exploitation of the reward model
that degrades model performance-as measured by a novel METEOR similarity
metric, and maintaining good performance on downstream evaluations. Our
experimental results show SuperHF exceeds PPO-based RLHF on the training
objective, easily and favorably trades off high reward with low reward hacking,
improves downstream calibration, and performs the same on our GPT-4 based
qualitative evaluation scheme all the while being significantly simpler to
implement, highlighting SuperHF's potential as a competitive language model
alignment technique.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16764" title="Abstract">arXiv:2310.16764</a> [<a href="/pdf/2310.16764" title="Download PDF">pdf</a>, <a href="/format/2310.16764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConvNets Match Vision Transformers at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smith%2C+S+L">Samuel L. Smith</a>, 
<a href="/search/cs?searchtype=author&query=Brock%2C+A">Andrew Brock</a>, 
<a href="/search/cs?searchtype=author&query=Berrada%2C+L">Leonard Berrada</a>, 
<a href="/search/cs?searchtype=author&query=De%2C+S">Soham De</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Many researchers believe that ConvNets perform well on small or moderately
sized datasets, but are not competitive with Vision Transformers when given
access to datasets on the web-scale. We challenge this belief by evaluating a
performant ConvNet architecture pre-trained on JFT-4B, a large labelled dataset
of images often used for training foundation models. We consider pre-training
compute budgets between 0.4k and 110k TPU-v4 core compute hours, and train a
series of networks of increasing depth and width from the NFNet model family.
We observe a log-log scaling law between held out loss and compute budget.
After fine-tuning on ImageNet, NFNets match the reported performance of Vision
Transformers with comparable compute budgets. Our strongest fine-tuned model
achieves a Top-1 accuracy of 90.4%.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16772" title="Abstract">arXiv:2310.16772</a> [<a href="/pdf/2310.16772" title="Download PDF">pdf</a>, <a href="/format/2310.16772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Agent as Urban Planner: Steering Stakeholder Dynamics in Urban  Planning via Consensus-based Multi-Agent Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+K">Kejiang Qian</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+L">Lingjun Mao</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yimin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xinran Wei</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Ziyi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiajie Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">In urban planning, land use readjustment plays a pivotal role in aligning
land use configurations with the current demands for sustainable urban
development. However, present-day urban planning practices face two main
issues. Firstly, land use decisions are predominantly dependent on human
experts. Besides, while resident engagement in urban planning can promote urban
sustainability and livability, it is challenging to reconcile the diverse
interests of stakeholders. To address these challenges, we introduce a
Consensus-based Multi-Agent Reinforcement Learning framework for real-world
land use readjustment. This framework serves participatory urban planning,
allowing diverse intelligent agents as stakeholder representatives to vote for
preferred land use types. Within this framework, we propose a novel consensus
mechanism in reward design to optimize land utilization through collective
decision making. To abstract the structure of the complex urban system, the
geographic information of cities is transformed into a spatial graph structure
and then processed by graph neural networks. Comprehensive experiments on both
traditional top-down planning and participatory planning methods from
real-world communities indicate that our computational framework enhances
global benefits and accommodates diverse interests, leading to improved
satisfaction across different demographic groups. By integrating Multi-Agent
Reinforcement Learning, our framework ensures that participatory urban planning
decisions are more dynamic and adaptive to evolving community needs and
provides a robust platform for automating complex real-world urban planning
processes.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16776" title="Abstract">arXiv:2310.16776</a> [<a href="/pdf/2310.16776" title="Download PDF">pdf</a>, <a href="/format/2310.16776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DEFT: Data Efficient Fine-Tuning for Large Language Models via  Unsupervised Core-Set Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+D">Devleena Das</a>, 
<a href="/search/cs?searchtype=author&query=Khetan%2C+V">Vivek Khetan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advances have led to the availability of many pre-trained language
models (PLMs); however, a question that remains is how much data is truly
needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT,
a data-efficient fine-tuning framework that leverages unsupervised core-set
selection to minimize the amount of data needed to fine-tune PLMs for
downstream tasks. We demonstrate the efficacy of our DEFT framework in the
context of text-editing LMs, and compare to the state-of-the art text-editing
model, CoEDIT (Raheja et al., 2023). Our quantitative and qualitative results
demonstrate that DEFT models are just as accurate as CoEDIT while being
finetuned on ~70% less data.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16779" title="Abstract">arXiv:2310.16779</a> [<a href="/pdf/2310.16779" title="Download PDF">pdf</a>, <a href="/format/2310.16779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-scale Diffusion Denoised Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+J">Jongheon Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jinwoo Shin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages; NeurIPS 2023; Code is available at <a href="https://github.com/jh-jeong/smoothing-multiscale">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Along with recent diffusion models, randomized smoothing has become one of a
few tangible approaches that offers adversarial robustness to models at scale,
e.g., those of large pre-trained models. Specifically, one can perform
randomized smoothing on any classifier via a simple "denoise-and-classify"
pipeline, so-called denoised smoothing, given that an accurate denoiser is
available - such as diffusion model. In this paper, we investigate the
trade-off between accuracy and certified robustness of denoised smoothing: for
example, we question on which representation of diffusion model would maximize
the certified robustness of denoised smoothing. We consider a new objective
that aims collective robustness of smoothed classifiers across multiple noise
levels at a shared diffusion model, which also suggests a new way to compensate
the cost of accuracy in randomized smoothing for its certified robustness. This
objective motivates us to fine-tune diffusion model (a) to perform consistent
denoising whenever the original image is recoverable, but (b) to generate
rather diverse outputs otherwise. Our experiments show that this fine-tuning
scheme of diffusion models combined with the multi-scale smoothing enables a
strong certified robustness possible at highest noise level while maintaining
the accuracy closer to non-smoothed classifiers.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16781" title="Abstract">arXiv:2310.16781</a> [<a href="/pdf/2310.16781" title="Download PDF">pdf</a>, <a href="/format/2310.16781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kiki or Bouba? Sound Symbolism in Vision-and-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alper%2C+M">Morris Alper</a>, 
<a href="/search/cs?searchtype=author&query=Averbuch-Elor%2C+H">Hadar Averbuch-Elor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 (spotlight). Project webpage: <a href="https://kiki-bouba.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Although the mapping between sound and meaning in human language is assumed
to be largely arbitrary, research in cognitive science has shown that there are
non-trivial correlations between particular sounds and meanings across
languages and demographic groups, a phenomenon known as sound symbolism. Among
the many dimensions of meaning, sound symbolism is particularly salient and
well-demonstrated with regards to cross-modal associations between language and
the visual domain. In this work, we address the question of whether sound
symbolism is reflected in vision-and-language models such as CLIP and Stable
Diffusion. Using zero-shot knowledge probing to investigate the inherent
knowledge of these models, we find strong evidence that they do show this
pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our
work provides a novel method for demonstrating sound symbolism and
understanding its nature using computational tools. Our code will be made
publicly available.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16783" title="Abstract">arXiv:2310.16783</a> [<a href="/pdf/2310.16783" title="Download PDF">pdf</a>, <a href="/format/2310.16783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S$^3$-TTA: Scale-Style Selection for Test-Time Augmentation in  Biomedical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+K">Kangxian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Siyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ordone%2C+S+C">Sebastian Cajas Ordone</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+H">Hanspeter Pfister</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+D">Donglai Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep-learning models have been successful in biomedical image segmentation.
To generalize for real-world deployment, test-time augmentation (TTA) methods
are often used to transform the test image into different versions that are
hopefully closer to the training domain. Unfortunately, due to the vast
diversity of instance scale and image styles, many augmented test images
produce undesirable results, thus lowering the overall performance. This work
proposes a new TTA framework, S$^3$-TTA, which selects the suitable image scale
and style for each test image based on a transformation consistency metric. In
addition, S$^3$-TTA constructs an end-to-end augmentation-segmentation
joint-training pipeline to ensure a task-oriented augmentation. On public
benchmarks for cell and lung segmentation, S$^3$-TTA demonstrates improvements
over the prior art by 3.4% and 1.3%, respectively, by simply augmenting the
input data in testing phase.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16787" title="Abstract">arXiv:2310.16787</a> [<a href="/pdf/2310.16787" title="Download PDF">pdf</a>, <a href="/format/2310.16787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing  &amp; Attribution in AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Longpre%2C+S">Shayne Longpre</a>, 
<a href="/search/cs?searchtype=author&query=Mahari%2C+R">Robert Mahari</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Anthony Chen</a>, 
<a href="/search/cs?searchtype=author&query=Obeng-Marnu%2C+N">Naana Obeng-Marnu</a>, 
<a href="/search/cs?searchtype=author&query=Sileo%2C+D">Damien Sileo</a>, 
<a href="/search/cs?searchtype=author&query=Brannon%2C+W">William Brannon</a>, 
<a href="/search/cs?searchtype=author&query=Muennighoff%2C+N">Niklas Muennighoff</a>, 
<a href="/search/cs?searchtype=author&query=Khazam%2C+N">Nathan Khazam</a>, 
<a href="/search/cs?searchtype=author&query=Kabbara%2C+J">Jad Kabbara</a>, 
<a href="/search/cs?searchtype=author&query=Perisetla%2C+K">Kartik Perisetla</a>, 
<a href="/search/cs?searchtype=author&query=Xinyi">Xinyi</a> (Alexis)Wu, 
<a href="/search/cs?searchtype=author&query=Shippole%2C+E">Enrico Shippole</a>, 
<a href="/search/cs?searchtype=author&query=Bollacker%2C+K">Kurt Bollacker</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tongshuang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Villa%2C+L">Luis Villa</a>, 
<a href="/search/cs?searchtype=author&query=Pentland%2C+S">Sandy Pentland</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+D">Deb Roy</a>, 
<a href="/search/cs?searchtype=author&query=Hooker%2C+S">Sara Hooker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages (18 main), 6 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The race to train language models on vast, diverse, and inconsistently
documented datasets has raised pressing concerns about the legal and ethical
risks for practitioners. To remedy these practices threatening data
transparency and understanding, we convene a multi-disciplinary effort between
legal and machine learning experts to systematically audit and trace 1800+ text
datasets. We develop tools and standards to trace the lineage of these
datasets, from their source, creators, series of license conditions,
properties, and subsequent use. Our landscape analysis highlights the sharp
divides in composition and focus of commercially open vs closed datasets, with
closed datasets monopolizing important categories: lower resource languages,
more creative tasks, richer topic variety, newer and more synthetic training
data. This points to a deepening divide in the types of data that are made
available under different license conditions, and heightened implications for
jurisdictional legal interpretations of copyright and fair use. We also observe
frequent miscategorization of licenses on widely used dataset hosting sites,
with license omission of 72%+ and error rates of 50%+. This points to a crisis
in misattribution and informed use of the most popular datasets driving many
recent breakthroughs. As a contribution to ongoing improvements in dataset
transparency and responsible use, we release our entire audit, with an
interactive UI, the Data Provenance Explorer, which allows practitioners to
trace and filter on data provenance for the most popular open source finetuning
data collections: www.dataprovenance.org.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16788" title="Abstract">arXiv:2310.16788</a> [<a href="/pdf/2310.16788" title="Download PDF">pdf</a>, <a href="/format/2310.16788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The GOOSE Dataset for Perception in Unstructured Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mortimer%2C+P">Peter Mortimer</a>, 
<a href="/search/cs?searchtype=author&query=Hagmanns%2C+R">Raphael Hagmanns</a>, 
<a href="/search/cs?searchtype=author&query=Granero%2C+M">Miguel Granero</a>, 
<a href="/search/cs?searchtype=author&query=Luettel%2C+T">Thorsten Luettel</a>, 
<a href="/search/cs?searchtype=author&query=Petereit%2C+J">Janko Petereit</a>, 
<a href="/search/cs?searchtype=author&query=Wuensche%2C+H">Hans-Joachim Wuensche</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint; Submitted to IEEE for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">The potential for deploying autonomous systems can be significantly increased
by improving the perception and interpretation of the environment. However, the
development of deep learning-based techniques for autonomous systems in
unstructured outdoor environments poses challenges due to limited data
availability for training and testing. To address this gap, we present the
German Outdoor and Offroad Dataset (GOOSE), a comprehensive dataset
specifically designed for unstructured outdoor environments. The GOOSE dataset
incorporates 10 000 labeled pairs of images and point clouds, which are
utilized to train a range of state-of-the-art segmentation models on both image
and point cloud data. We open source the dataset, along with an ontology for
unstructured terrain, as well as dataset standards and guidelines. This
initiative aims to establish a common framework, enabling the seamless
inclusion of existing datasets and a fast way to enhance the perception
capabilities of various robots operating in unstructured environments. The
dataset, pre-trained models for offroad perception, and additional
documentation can be found at https://goose-dataset.de/.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16789" title="Abstract">arXiv:2310.16789</a> [<a href="/pdf/2310.16789" title="Download PDF">pdf</a>, <a href="/format/2310.16789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Pretraining Data from Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weijia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Ajith%2C+A">Anirudh Ajith</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Mengzhou Xia</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yangsibo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Daogao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Blevins%2C+T">Terra Blevins</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Danqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zettlemoyer%2C+L">Luke Zettlemoyer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Although large language models (LLMs) are widely deployed, the data used to
train them is rarely disclosed. Given the incredible scale of this data, up to
trillions of tokens, it is all but certain that it includes potentially
problematic text such as copyrighted materials, personally identifiable
information, and test data for widely reported reference benchmarks. However,
we currently have no way to know which data of these types is included or in
what proportions. In this paper, we study the pretraining data detection
problem: given a piece of text and black-box access to an LLM without knowing
the pretraining data, can we determine if the model was trained on the provided
text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that
uses data created before and after model training to support gold truth
detection. We also introduce a new detection method Min-K% Prob based on a
simple hypothesis: an unseen example is likely to contain a few outlier words
with low probabilities under the LLM, while a seen example is less likely to
have words with such low probabilities. Min-K% Prob can be applied without any
knowledge about the pretraining corpus or any additional training, departing
from previous detection methods that require training a reference model on data
that is similar to the pretraining data. Moreover, our experiments demonstrate
that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous
methods. We apply Min-K% Prob to two real-world scenarios, copyrighted book
detection, and contaminated downstream example detection, and find it a
consistently effective solution.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16790" title="Abstract">arXiv:2310.16790</a> [<a href="/pdf/2310.16790" title="Download PDF">pdf</a>, <a href="/format/2310.16790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving a Named Entity Recognizer Trained on Noisy Data with a Few  Clean Instances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+Z">Zhendong Chu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruiyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Rajiv Jain</a>, 
<a href="/search/cs?searchtype=author&query=Morariu%2C+V+I">Vlad I Morariu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiuxiang Gu</a>, 
<a href="/search/cs?searchtype=author&query=Nenkova%2C+A">Ani Nenkova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">To achieve state-of-the-art performance, one still needs to train NER models
on large-scale, high-quality annotated data, an asset that is both costly and
time-intensive to accumulate. In contrast, real-world applications often resort
to massive low-quality labeled data through non-expert annotators via
crowdsourcing and external knowledge bases via distant supervision as a
cost-effective alternative. However, these annotation methods result in noisy
labels, which in turn lead to a notable decline in performance. Hence, we
propose to denoise the noisy NER data with guidance from a small set of clean
instances. Along with the main NER model we train a discriminator model and use
its outputs to recalibrate the sample weights. The discriminator is capable of
detecting both span and category errors with different discriminative prompts.
Results on public crowdsourcing and distant supervision datasets show that the
proposed method can consistently improve performance with a small guidance set.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16791" title="Abstract">arXiv:2310.16791</a> [<a href="/pdf/2310.16791" title="Download PDF">pdf</a>, <a href="/format/2310.16791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Covert Planning against Imperfect Observers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Haoxiang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chongyang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shuo Han</a>, 
<a href="/search/cs?searchtype=author&query=Dorothy%2C+M+R">Michael R. Dorothy</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Covert planning refers to a class of constrained planning problems where an
agent aims to accomplish a task with minimal information leaked to a passive
observer to avoid detection. However, existing methods of covert planning often
consider deterministic environments or do not exploit the observer's imperfect
information. This paper studies how covert planning can leverage the coupling
of stochastic dynamics and the observer's imperfect observation to achieve
optimal task performance without being detected. Specifically, we employ a
Markov decision process to model the interaction between the agent and its
stochastic environment, and a partial observation function to capture the
leaked information to a passive observer. Assuming the observer employs
hypothesis testing to detect if the observation deviates from a nominal policy,
the covert planning agent aims to maximize the total discounted reward while
keeping the probability of being detected as an adversary below a given
threshold. We prove that finite-memory policies are more powerful than
Markovian policies in covert planning. Then, we develop a primal-dual proximal
policy gradient method with a two-time-scale update to compute a (locally)
optimal covert policy. We demonstrate the effectiveness of our methods using a
stochastic gridworld example. Our experimental results illustrate that the
proposed method computes a policy that maximizes the adversary's expected
reward without violating the detection constraint, and empirically demonstrates
how the environmental noises can influence the performance of the covert
policies.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16792" title="Abstract">arXiv:2310.16792</a> [<a href="/pdf/2310.16792" title="Download PDF">pdf</a>, <a href="/format/2310.16792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Independent Program and Architecture Representations for  Generalizable Performance Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lingda Li</a>, 
<a href="/search/cs?searchtype=author&query=Flynn%2C+T">Thomas Flynn</a>, 
<a href="/search/cs?searchtype=author&query=Hoisie%2C+A">Adolfy Hoisie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">This paper proposes PerfVec, a novel deep learning-based performance modeling
framework that learns high-dimensional, independent/orthogonal program and
microarchitecture representations. Once learned, a program representation can
be used to predict its performance on any microarchitecture, and likewise, a
microarchitecture representation can be applied in the performance prediction
of any program. Additionally, PerfVec yields a foundation model that captures
the performance essence of instructions, which can be directly used by
developers in numerous performance modeling related tasks without incurring its
training cost. The evaluation demonstrates that PerfVec is more general,
efficient, and accurate than previous approaches.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16795" title="Abstract">arXiv:2310.16795</a> [<a href="/pdf/2310.16795" title="Download PDF">pdf</a>, <a href="/format/2310.16795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frantar%2C+E">Elias Frantar</a>, 
<a href="/search/cs?searchtype=author&query=Alistarh%2C+D">Dan Alistarh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Mixture-of-Experts (MoE) architectures offer a general solution to the high
inference costs of large language models (LLMs) via sparse routing, bringing
faster and more accurate models, at the cost of massive parameter counts. For
example, the SwitchTransformer-c2048 model has 1.6 trillion parameters,
requiring 3.2TB of accelerator memory to run efficiently, which makes practical
deployment challenging and expensive. In this paper, we present a solution to
this memory problem, in form of a new compression and execution framework
called QMoE. Specifically, QMoE consists of a scalable algorithm which
accurately compresses trillion-parameter MoEs to less than 1 bit per parameter,
in a custom format co-designed with bespoke GPU decoding kernels to facilitate
efficient end-to-end compressed inference, with minor runtime overheads
relative to uncompressed execution. Concretely, QMoE can compress the 1.6
trillion parameter SwitchTransformer-c2048 model to less than 160GB (20x
compression, 0.8 bits per parameter) at only minor accuracy loss, in less than
a day on a single GPU. This enables, for the first time, the execution of a
trillion-parameter model on affordable commodity hardware, like a single server
with 4x NVIDIA A6000 or 8x NVIDIA 3090 GPUs, at less than 5% runtime overhead
relative to ideal uncompressed inference. The source code and compressed models
are available at github.com/IST-DASLab/qmoe.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16798" title="Abstract">arXiv:2310.16798</a> [<a href="/pdf/2310.16798" title="Download PDF">pdf</a>, <a href="/format/2310.16798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reachability in Continuous Pushdown VASS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+A+R">A. R. Balasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Majumdar%2C+R">Rupak Majumdar</a>, 
<a href="/search/cs?searchtype=author&query=Thinniyam%2C+R+S">Ramanathan S. Thinniyam</a>, 
<a href="/search/cs?searchtype=author&query=Zetzsche%2C+G">Georg Zetzsche</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">Pushdown Vector Addition Systems with States (PVASS) consist of finitely many
control states, a pushdown stack, and a set of counters that can be incremented
and decremented, but not tested for zero. Whether the reachability problem is
decidable for PVASS is a long-standing open problem.
<br />We consider continuous PVASS, which are PVASS with a continuous semantics.
This means, the counter values are rational numbers and whenever a vector is
added to the current counter values, this vector is first scaled with an
arbitrarily chosen rational factor between zero and one. We show that
reachability in continuous PVASS is NEXPTIME-complete. Our result is unusually
robust: Reachability can be decided in NEXPTIME even if all numbers are
specified in binary. On the other hand, NEXPTIME-hardness already holds for
coverability, in fixed dimension, for bounded stack, and even if all numbers
are specified in unary.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16801" title="Abstract">arXiv:2310.16801</a> [<a href="/pdf/2310.16801" title="Download PDF">pdf</a>, <a href="/format/2310.16801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding the saddlepoint faster than sorting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dallant%2C+J">Justin Dallant</a>, 
<a href="/search/cs?searchtype=author&query=Haagensen%2C+F">Frederik Haagensen</a>, 
<a href="/search/cs?searchtype=author&query=Jacob%2C+R">Riko Jacob</a>, 
<a href="/search/cs?searchtype=author&query=Kozma%2C+L">L&#xe1;szl&#xf3; Kozma</a>, 
<a href="/search/cs?searchtype=author&query=Wild%2C+S">Sebastian Wild</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be presented at SOSA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">A saddlepoint of an $n \times n$ matrix $A$ is an entry of $A$ that is a
maximum in its row and a minimum in its column. Knuth (1968) gave several
different algorithms for finding a saddlepoint. The worst-case running time of
these algorithms is $\Theta(n^2)$, and Llewellyn, Tovey, and Trick (1988)
showed that this cannot be improved, as in the worst case all entries of A may
need to be queried.
<br />A strict saddlepoint of $A$ is an entry that is the strict maximum in its row
and the strict minimum in its column. The strict saddlepoint (if it exists) is
unique, and Bienstock, Chung, Fredman, Sch\"affer, Shor, and Suri (1991) showed
that it can be found in time $O(n \log{n})$, where a dominant runtime
contribution is sorting the diagonal of the matrix. This upper bound has not
been improved since 1991. In this paper we show that the strict saddlepoint can
be found in $O(n \log^{*}{n})$ time, where $\log^{*}$ denotes the very slowly
growing iterated logarithm function, coming close to the lower bound of
$\Omega(n)$. In fact, we can also compute, within the same runtime, the value
of a non-strict saddlepoint, assuming one exists. Our algorithm is based on a
simple recursive approach, a feasibility test inspired by searching in sorted
matrices, and a relaxed notion of saddlepoint.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16802" title="Abstract">arXiv:2310.16802</a> [<a href="/pdf/2310.16802" title="Download PDF">pdf</a>, <a href="/format/2310.16802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Molecules to Materials: Pre-training Large Generalizable Models for  Atomic Property Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shoghi%2C+N">Nima Shoghi</a>, 
<a href="/search/cs?searchtype=author&query=Kolluru%2C+A">Adeesh Kolluru</a>, 
<a href="/search/cs?searchtype=author&query=Kitchin%2C+J+R">John R. Kitchin</a>, 
<a href="/search/cs?searchtype=author&query=Ulissi%2C+Z+W">Zachary W. Ulissi</a>, 
<a href="/search/cs?searchtype=author&query=Zitnick%2C+C+L">C. Lawrence Zitnick</a>, 
<a href="/search/cs?searchtype=author&query=Wood%2C+B+M">Brandon M. Wood</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Foundation models have been transformational in machine learning fields such
as natural language processing and computer vision. Similar success in atomic
property prediction has been limited due to the challenges of training
effective models across multiple chemical domains. To address this, we
introduce Joint Multi-domain Pre-training (JMP), a supervised pre-training
strategy that simultaneously trains on multiple datasets from different
chemical domains, treating each dataset as a unique pre-training task within a
multi-task framework. Our combined training dataset consists of $\sim$120M
systems from OC20, OC22, ANI-1x, and Transition-1x. We evaluate performance and
generalization by fine-tuning over a diverse set of downstream tasks and
datasets including: QM9, rMD17, MatBench, QMOF, SPICE, and MD22. JMP
demonstrates an average improvement of 59% over training from scratch, and
matches or sets state-of-the-art on 34 out of 40 tasks. Our work highlights the
potential of pre-training strategies that utilize diverse data to advance
property prediction across chemical domains, especially for low-data tasks.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16803" title="Abstract">arXiv:2310.16803</a> [<a href="/pdf/2310.16803" title="Download PDF">pdf</a>, <a href="/format/2310.16803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Agnostic Code Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Utpala%2C+S">Saiteja Utpala</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+A">Alex Gu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P+Y">Pin Yu Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, code language models have achieved notable advancements in
addressing a diverse array of essential code comprehension and generation
tasks. Yet, the field lacks a comprehensive deep dive and understanding of the
code embeddings of multilingual code models. In this paper, we present a
comprehensive study on multilingual code embeddings, focusing on the
cross-lingual capabilities of these embeddings across different programming
languages. Through probing experiments, we demonstrate that code embeddings
comprise two distinct components: one deeply tied to the nuances and syntax of
a specific language, and the other remaining agnostic to these details,
primarily focusing on semantics. Further, we show that when we isolate and
eliminate this language-specific component, we witness significant improvements
in downstream code retrieval tasks, leading to an absolute increase of up to
+17 in the Mean Reciprocal Rank (MRR).
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16804" title="Abstract">arXiv:2310.16804</a> [<a href="/pdf/2310.16804" title="Download PDF">pdf</a>, <a href="/format/2310.16804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning COVID-19 Regional Transmission Using Universal Differential  Equations in a SIR model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rojas-Campos%2C+A">Adrian Rojas-Campos</a>, 
<a href="/search/cs?searchtype=author&query=Stelz%2C+L">Lukas Stelz</a>, 
<a href="/search/cs?searchtype=author&query=Nieters%2C+P">Pascal Nieters</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Highly-interconnected societies difficult to model the spread of infectious
diseases such as COVID-19. Single-region SIR models fail to account for
incoming forces of infection and expanding them to a large number of
interacting regions involves many assumptions that do not hold in the real
world. We propose using Universal Differential Equations (UDEs) to capture the
influence of neighboring regions and improve the model's predictions in a
combined SIR+UDE model. UDEs are differential equations totally or partially
defined by a deep neural network (DNN). We include an additive term to the SIR
equations composed by a DNN that learns the incoming force of infection from
the other regions. The learning is performed using automatic differentiation
and gradient descent to approach the change in the target system caused by the
state of the neighboring regions. We compared the proposed model using a
simulated COVID-19 outbreak against a single-region SIR and a fully data-driven
model composed only of a DNN. The proposed UDE+SIR model generates predictions
that capture the outbreak dynamic more accurately, but a decay in performance
is observed at the last stages of the outbreak. The single-area SIR and the
fully data-driven approach do not capture the proper dynamics accurately. Once
the predictions were obtained, we employed the SINDy algorithm to substitute
the DNN with a regression, removing the black box element of the model with no
considerable increase in the error levels.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16807" title="Abstract">arXiv:2310.16807</a> [<a href="/pdf/2310.16807" title="Download PDF">pdf</a>, <a href="/ps/2310.16807" title="Download PostScript">ps</a>, <a href="/format/2310.16807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-Sided Matching Markets: Impossibility Results on Existence of  Efficient and Envy Free Solutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tr%C3%B6bst%2C+T">Thorben Tr&#xf6;bst</a>, 
<a href="/search/cs?searchtype=author&query=Vazirani%2C+V+V">Vijay V Vazirani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">The Hylland-Zeckhauser gave a classic pricing-based mechanism (HZ) for a
one-sided matching market; it yields allocations satisfying Pareto optimality
and envy-freeness (Hylland and Zeckhauser, 1979), and the mechanism is
incentive compatible in the large (He et al., 2018). They also studied the
exchange extension of HZ and gave an example showing that it may not even admit
an equilibrium. In this paper, we consider two models of two sided matching
markets: when utility functions are symmetric and when they are non-symmetric.
We ask if these models always admit allocations satisfying the two basic
properties of Pareto efficiency and envy freeness. Our results are negative. A
corollary of the former result is a negative result for non-bipartite matching
markets as well.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16808" title="Abstract">arXiv:2310.16808</a> [<a href="/pdf/2310.16808" title="Download PDF">pdf</a>, <a href="/format/2310.16808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fingervein Verification using Convolutional Multi-Head Attention Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramachandra%2C+R">Raghavendra Ramachandra</a>, 
<a href="/search/cs?searchtype=author&query=Venkatesh%2C+S">Sushma Venkatesh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Biometric verification systems are deployed in various security-based
access-control applications that require user-friendly and reliable person
verification. Among the different biometric characteristics, fingervein
biometrics have been extensively studied owing to their reliable verification
performance. Furthermore, fingervein patterns reside inside the skin and are
not visible outside; therefore, they possess inherent resistance to
presentation attacks and degradation due to external factors. In this paper, we
introduce a novel fingervein verification technique using a convolutional
multihead attention network called VeinAtnNet. The proposed VeinAtnNet is
designed to achieve light weight with a smaller number of learnable parameters
while extracting discriminant information from both normal and enhanced
fingervein images. The proposed VeinAtnNet was trained on the newly constructed
fingervein dataset with 300 unique fingervein patterns that were captured in
multiple sessions to obtain 92 samples per unique fingervein. Extensive
experiments were performed on the newly collected dataset FV-300 and the
publicly available FV-USM and FV-PolyU fingervein dataset. The performance of
the proposed method was compared with five state-of-the-art fingervein
verification systems, indicating the efficacy of the proposed VeinAtnNet.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16809" title="Abstract">arXiv:2310.16809</a> [<a href="/pdf/2310.16809" title="Download PDF">pdf</a>, <a href="/format/2310.16809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and  In-depth Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yongxin Shi</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+D">Dezhi Peng</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+W">Wenhui Liao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zening Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinhong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chongyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Lianwen Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents a comprehensive evaluation of the Optical Character
Recognition (OCR) capabilities of the recently released GPT-4V(ision), a Large
Multimodal Model (LMM). We assess the model's performance across a range of OCR
tasks, including scene text recognition, handwritten text recognition,
handwritten mathematical expression recognition, table structure recognition,
and information extraction from visually-rich document. The evaluation reveals
that GPT-4V performs well in recognizing and understanding Latin contents, but
struggles with multilingual scenarios and complex tasks. Based on these
observations, we delve deeper into the necessity of specialized OCR models and
deliberate on the strategies to fully harness the pretrained general LMMs like
GPT-4V for OCR downstream tasks. The study offers a critical reference for
future research in OCR with LMMs. Evaluation pipeline and results are available
at https://github.com/SCUT-DLVCLab/GPT-4V_OCR.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16810" title="Abstract">arXiv:2310.16810</a> [<a href="/pdf/2310.16810" title="Download PDF">pdf</a>, <a href="/format/2310.16810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can GPT models Follow Human Summarization Guidelines? Evaluating ChatGPT  and GPT-4 for Dialogue Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yongxin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ringeval%2C+F">Fabien Ringeval</a>, 
<a href="/search/cs?searchtype=author&query=Portet%2C+F">Fran&#xe7;ois Portet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study explores the capabilities of prompt-driven Large Language Models
(LLMs) like ChatGPT and GPT-4 in adhering to human guidelines for dialogue
summarization. Experiments employed DialogSum (English social conversations)
and DECODA (French call center interactions), testing various prompts:
including prompts from existing literature and those from human summarization
guidelines, as well as a two-step prompt approach. Our findings indicate that
GPT models often produce lengthy summaries and deviate from human summarization
guidelines. However, using human guidelines as an intermediate step shows
promise, outperforming direct word-length constraint prompts in some cases. The
results reveal that GPT models exhibit unique stylistic tendencies in their
summaries. While BERTScores did not dramatically decrease for GPT outputs
suggesting semantic similarity to human references and specialised pre-trained
models, ROUGE scores reveal grammatical and lexical disparities between
GPT-generated and human-written summaries. These findings shed light on the
capabilities and limitations of GPT models in following human instructions for
dialogue summarization.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16812" title="Abstract">arXiv:2310.16812</a> [<a href="/pdf/2310.16812" title="Download PDF">pdf</a>, <a href="/format/2310.16812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accurate Crop Spraying with RTK and Machine Learning on an Autonomous  Field Robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wijesundara%2C+W+M+T+D">W. M. T. D. Wijesundara</a>, 
<a href="/search/cs?searchtype=author&query=Wanigathunga%2C+T+D">T. D. Wanigathunga</a>, 
<a href="/search/cs?searchtype=author&query=Waas%2C+M+N+C">M. N. C. Waas</a>, 
<a href="/search/cs?searchtype=author&query=Hithanadura%2C+R+T">R. T. Hithanadura</a>, 
<a href="/search/cs?searchtype=author&query=Munasinghe%2C+S+R">S. R. Munasinghe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 12 figures, Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The agriculture sector requires a lot of labor and resources. Hence, farmers
are constantly being pressed for technology and automation to be
cost-effective. In this context, autonomous robots can play a very important
role in carrying out agricultural tasks such as spraying, sowing, inspection,
and even harvesting. This paper presents one such autonomous robot that is able
to identify plants and spray agro-chemicals precisely. The robot uses machine
vision technologies to find plants and RTK-GPS technology to navigate the robot
along a predetermined path. The experiments were conducted in a field of potted
plants in which successful results have been obtained.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16818" title="Abstract">arXiv:2310.16818</a> [<a href="/pdf/2310.16818" title="Download PDF">pdf</a>, <a href="/format/2310.16818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion  Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingxiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+R">Ruizhi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lizhen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhenda Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yebin Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://mrtornado24.github.io/DreamCraft3D/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">We present DreamCraft3D, a hierarchical 3D content generation method that
produces high-fidelity and coherent 3D objects. We tackle the problem by
leveraging a 2D reference image to guide the stages of geometry sculpting and
texture boosting. A central focus of this work is to address the consistency
issue that existing works encounter. To sculpt geometries that render
coherently, we perform score distillation sampling via a view-dependent
diffusion model. This 3D prior, alongside several training strategies,
prioritizes the geometry consistency but compromises the texture fidelity. We
further propose Bootstrapped Score Distillation to specifically boost the
texture. We train a personalized diffusion model, Dreambooth, on the augmented
renderings of the scene, imbuing it with 3D knowledge of the scene being
optimized. The score distillation from this 3D-aware diffusion prior provides
view-consistent guidance for the scene. Notably, through an alternating
optimization of the diffusion prior and 3D scene representation, we achieve
mutually reinforcing improvements: the optimized 3D scene aids in training the
scene-specific diffusion model, which offers increasingly view-consistent
guidance for 3D optimization. The optimization is thus bootstrapped and leads
to substantial texture boosting. With tailored 3D priors throughout the
hierarchical generation, DreamCraft3D generates coherent 3D objects with
photorealistic renderings, advancing the state-of-the-art in 3D content
generation. Code available at https://github.com/deepseek-ai/DreamCraft3D.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16822" title="Abstract">arXiv:2310.16822</a> [<a href="/pdf/2310.16822" title="Download PDF">pdf</a>, <a href="/format/2310.16822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt Me Up: Unleashing the Power of Alignments for Multimodal Entity  and Relation Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xuming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junzhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Aiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+S">Shiao Meng</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+L">Lijie Wen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACM Multimedia 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
<p class="mathjax">How can we better extract entities and relations from text? Using multimodal
extraction with images and text obtains more signals for entities and
relations, and aligns them through graphs or hierarchical fusion, aiding in
extraction. Despite attempts at various fusions, previous works have overlooked
many unlabeled image-caption pairs, such as NewsCLIPing. This paper proposes
innovative pre-training objectives for entity-object and relation-image
alignment, extracting objects from images and aligning them with entity and
relation prompts for soft pseudo-labels. These labels are used as
self-supervised signals for pre-training, enhancing the ability to extract
entities and relations. Experiments on three datasets show an average 3.41% F1
improvement over prior SOTA. Additionally, our method is orthogonal to previous
multimodal fusions, and using it on prior SOTA fusions further improves 5.47%
F1.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16825" title="Abstract">arXiv:2310.16825</a> [<a href="/pdf/2310.16825" title="Download PDF">pdf</a>, <a href="/format/2310.16825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CommonCanvas: An Open Diffusion Model Trained with Creative-Commons  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gokaslan%2C+A">Aaron Gokaslan</a>, 
<a href="/search/cs?searchtype=author&query=Cooper%2C+A+F">A. Feder Cooper</a>, 
<a href="/search/cs?searchtype=author&query=Collins%2C+J">Jasmine Collins</a>, 
<a href="/search/cs?searchtype=author&query=Seguin%2C+L">Landan Seguin</a>, 
<a href="/search/cs?searchtype=author&query=Jacobson%2C+A">Austin Jacobson</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+M">Mihir Patel</a>, 
<a href="/search/cs?searchtype=author&query=Frankle%2C+J">Jonathan Frankle</a>, 
<a href="/search/cs?searchtype=author&query=Stephenson%2C+C">Cory Stephenson</a>, 
<a href="/search/cs?searchtype=author&query=Kuleshov%2C+V">Volodymyr Kuleshov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">We assemble a dataset of Creative-Commons-licensed (CC) images, which we use
to train a set of open diffusion models that are qualitatively competitive with
Stable Diffusion 2 (SD2). This task presents two challenges: (1)
high-resolution CC images lack the captions necessary to train text-to-image
generative models; (2) CC images are relatively scarce. In turn, to address
these challenges, we use an intuitive transfer learning technique to produce a
set of high-quality synthetic captions paired with curated CC images. We then
develop a data- and compute-efficient training recipe that requires as little
as 3% of the LAION-2B data needed to train existing SD2 models, but obtains
comparable quality. These results indicate that we have a sufficient number of
CC images (~70 million) for training high-quality models. Our training recipe
also implements a variety of optimizations that achieve ~3X training speed-ups,
enabling rapid model iteration. We leverage this recipe to train several
high-quality text-to-image models, which we dub the CommonCanvas family. Our
largest model achieves comparable performance to SD2 on a human evaluation,
despite being trained on our CC dataset that is significantly smaller than
LAION and using synthetic captions for training. We release our models, data,
and code at
https://github.com/mosaicml/diffusion/blob/main/assets/common-canvas.md
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16827" title="Abstract">arXiv:2310.16827</a> [<a href="/pdf/2310.16827" title="Download PDF">pdf</a>, <a href="/format/2310.16827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Sparsification for Matroid Intersection with Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chien-Chung Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sellier%2C+F">Fran&#xe7;ois Sellier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Matroid intersection is a classical optimization problem where, given two
matroids over the same ground set, the goal is to find the largest common
independent set. In this paper, we show that there exists a certain
"sparsifer": a subset of elements, of size $O(|S^{opt}| \cdot 1/\varepsilon)$,
where $S^{opt}$ denotes the optimal solution, that is guaranteed to contain a
$3/2 + \varepsilon$ approximation, while guaranteeing certain robustness
properties. We call such a small subset a Density Constrained Subset (DCS),
which is inspired by the Edge-Degree Constrained Subgraph (EDCS) [Bernstein and
Stein, 2015], originally designed for the maximum cardinality matching problem
in a graph. Our proof is constructive and hinges on a greedy decomposition of
matroids, which we call the density-based decomposition. We show that this
sparsifier has certain robustness properties that can be used in one-way
communication and random-order streaming models.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16828" title="Abstract">arXiv:2310.16828</a> [<a href="/pdf/2310.16828" title="Download PDF">pdf</a>, <a href="/format/2310.16828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TD-MPC2: Scalable, Robust World Models for Continuous Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hansen%2C+N">Nicklas Hansen</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hao Su</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Explore videos, models, data, code, and more at <a href="https://nicklashansen.github.io/td-mpc2">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
<p class="mathjax">TD-MPC is a model-based reinforcement learning (RL) algorithm that performs
local trajectory optimization in the latent space of a learned implicit
(decoder-free) world model. In this work, we present TD-MPC2: a series of
improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves
significantly over baselines across 104 online RL tasks spanning 4 diverse task
domains, achieving consistently strong results with a single set of
hyperparameters. We further show that agent capabilities increase with model
and data size, and successfully train a single 317M parameter agent to perform
80 tasks across multiple task domains, embodiments, and action spaces. We
conclude with an account of lessons, opportunities, and risks associated with
large TD-MPC2 agents. Explore videos, models, data, code, and more at
https://nicklashansen.github.io/td-mpc2
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16829" title="Abstract">arXiv:2310.16829</a> [<a href="/pdf/2310.16829" title="Download PDF">pdf</a>, <a href="/format/2310.16829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lattice Multislice Algorithm for Fast Simulation of Scanning  Transmission Electron Microscopy Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Doberstein%2C+C">Christian Doberstein</a>, 
<a href="/search/math?searchtype=author&query=Binev%2C+P">Peter Binev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We introduce a new approach to the numerical simulation of Scanning
Transmission Electron Microscopy images. The Lattice Multislice Algorithm (LMA)
takes advantage of the fact that electron waves passing through the specimen
have limited bandwidth and therefore can be approximated very well by a
low-dimensional linear space spanned by translations of a well-localized
function $u$. Just like in the PRISM algorithm recently published by C. Ophus,
we utilize the linearity of the Schr\"odinger equation, but perform the
approximations with functions that are well localized in real space instead of
Fourier space. This way, we achieve a similar computational speedup as PRISM,
but at a much lower memory consumption and reduced numerical error due to
avoiding virtual copies of the probe waves interfering with the result. Our
approach also facilitates faster recomputations if local changes are made to
the specimen such as changing a single atomic column.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16831" title="Abstract">arXiv:2310.16831</a> [<a href="/pdf/2310.16831" title="Download PDF">pdf</a>, <a href="/format/2310.16831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PERF: Panoramic Neural Radiance Field from a Single Panorama
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guangcong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaoxi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Loy%2C+C+C">Chen Change Loy</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page and code: <a href="https://perf-project.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Neural Radiance Field (NeRF) has achieved substantial progress in novel view
synthesis given multi-view images. Recently, some works have attempted to train
a NeRF from a single image with 3D priors. They mainly focus on a limited field
of view and there are few invisible occlusions, which greatly limits their
scalability to real-world 360-degree panoramic scenarios with large-size
occlusions. In this paper, we present PERF, a 360-degree novel view synthesis
framework that trains a panoramic neural radiance field from a single panorama.
Notably, PERF allows 3D roaming in a complex scene without expensive and
tedious image collection. To achieve this goal, we propose a novel
collaborative RGBD inpainting method and a progressive inpainting-and-erasing
method to lift up a 360-degree 2D scene to a 3D scene. Specifically, we first
predict a panoramic depth map as initialization given a single panorama, and
reconstruct visible 3D regions with volume rendering. Then we introduce a
collaborative RGBD inpainting approach into a NeRF for completing RGB images
and depth maps from random views, which is derived from an RGB Stable Diffusion
model and a monocular depth estimator. Finally, we introduce an
inpainting-and-erasing strategy to avoid inconsistent geometry between a
newly-sampled view and reference views. The two components are integrated into
the learning of NeRFs in a unified optimization framework and achieve promising
results. Extensive experiments on Replica and a new dataset PERF-in-the-wild
demonstrate the superiority of our PERF over state-of-the-art methods. Our PERF
can be widely used for real-world applications, such as panorama-to-3D,
text-to-3D, and 3D scene stylization applications. Project page and code are
available at https://perf-project.github.io/.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16832" title="Abstract">arXiv:2310.16832</a> [<a href="/pdf/2310.16832" title="Download PDF">pdf</a>, <a href="/format/2310.16832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LightSpeed: Light and Fast Neural Light Fields on Mobile Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Aarush Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Junli Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaoyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Ju Hu</a>, 
<a href="/search/cs?searchtype=author&query=Tulyakov%2C+S">Sergey Tulyakov</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jian Ren</a>, 
<a href="/search/cs?searchtype=author&query=Jeni%2C+L+A">L&#xe1;szl&#xf3; A Jeni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="http://lightspeed-r2l.github.io/website/">this http URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Real-time novel-view image synthesis on mobile devices is prohibitive due to
the limited computational power and storage. Using volumetric rendering
methods, such as NeRF and its derivatives, on mobile devices is not suitable
due to the high computational cost of volumetric rendering. On the other hand,
recent advances in neural light field representations have shown promising
real-time view synthesis results on mobile devices. Neural light field methods
learn a direct mapping from a ray representation to the pixel color. The
current choice of ray representation is either stratified ray sampling or
Pl\"{u}cker coordinates, overlooking the classic light slab (two-plane)
representation, the preferred representation to interpolate between light field
views. In this work, we find that using the light slab representation is an
efficient representation for learning a neural light field. More importantly,
it is a lower-dimensional ray representation enabling us to learn the 4D ray
space using feature grids which are significantly faster to train and render.
Although mostly designed for frontal views, we show that the light-slab
representation can be further extended to non-frontal scenes using a
divide-and-conquer strategy. Our method offers superior rendering quality
compared to previous light field methods and achieves a significantly improved
trade-off between rendering quality and speed.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16835" title="Abstract">arXiv:2310.16835</a> [<a href="/pdf/2310.16835" title="Download PDF">pdf</a>, <a href="/format/2310.16835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proposal-Contrastive Pretraining for Object Detection from Fewer Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bouniot%2C+Q">Quentin Bouniot</a>, 
<a href="/search/cs?searchtype=author&query=Audigier%2C+R">Romaric Audigier</a>, 
<a href="/search/cs?searchtype=author&query=Loesch%2C+A">Ang&#xe9;lique Loesch</a>, 
<a href="/search/cs?searchtype=author&query=Habrard%2C+A">Amaury Habrard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The use of pretrained deep neural networks represents an attractive way to
achieve strong results with few data available. When specialized in dense
problems such as object detection, learning local rather than global
information in images has proven to be more efficient. However, for
unsupervised pretraining, the popular contrastive learning requires a large
batch size and, therefore, a lot of resources. To address this problem, we are
interested in transformer-based object detectors that have recently gained
traction in the community with good performance and with the particularity of
generating many diverse object proposals.
<br />In this work, we present Proposal Selection Contrast (ProSeCo), a novel
unsupervised overall pretraining approach that leverages this property. ProSeCo
uses the large number of object proposals generated by the detector for
contrastive learning, which allows the use of a smaller batch size, combined
with object-level features to learn local information in the images. To improve
the effectiveness of the contrastive loss, we introduce the object location
information in the selection of positive examples to take into account multiple
overlapping object proposals. When reusing pretrained backbone, we advocate for
consistency in learning local information between the backbone and the
detection head.
<br />We show that our method outperforms state of the art in unsupervised
pretraining for object detection on standard and novel benchmarks in learning
with fewer data.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16836" title="Abstract">arXiv:2310.16836</a> [<a href="/pdf/2310.16836" title="Download PDF">pdf</a>, <a href="/format/2310.16836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-FP4: 4-Bit Floating-Point Quantized Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shih-yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zechun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xijie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+P">Pingcheng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+K">Kwang-Ting Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We propose LLM-FP4 for quantizing both weights and activations in large
language models (LLMs) down to 4-bit floating-point values, in a post-training
manner. Existing post-training quantization (PTQ) solutions are primarily
integer-based and struggle with bit widths below 8 bits. Compared to integer
quantization, floating-point (FP) quantization is more flexible and can better
handle long-tail or bell-shaped distributions, and it has emerged as a default
choice in many hardware platforms. One characteristic of FP quantization is
that its performance largely depends on the choice of exponent bits and
clipping range. In this regard, we construct a strong FP-PTQ baseline by
searching for the optimal quantization parameters. Furthermore, we observe a
high inter-channel variance and low intra-channel variance pattern in
activation distributions, which adds activation quantization difficulty. We
recognize this pattern to be consistent across a spectrum of transformer models
designed for diverse tasks, such as LLMs, BERT, and Vision Transformer models.
To tackle this, we propose per-channel activation quantization and show that
these additional scaling factors can be reparameterized as exponential biases
of weights, incurring a negligible cost. Our method, for the first time, can
quantize both weights and activations in the LLaMA-13B to only 4-bit and
achieves an average score of 63.1 on the common sense zero-shot reasoning
tasks, which is only 5.8 lower than the full-precision model, significantly
outperforming the previous state-of-the-art by 12.7 points. Code is available
at: https://github.com/nbasyl/LLM-FP4.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16837" title="Abstract">arXiv:2310.16837</a> [<a href="/pdf/2310.16837" title="Download PDF">pdf</a>, <a href="/format/2310.16837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RDBench: ML Benchmark for Relational Databases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zizhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+L">Lutong Zou</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">He Wen</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Tao Feng</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+J">Jiaxuan You</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Benefiting from high-quality datasets and standardized evaluation metrics,
machine learning (ML) has achieved sustained progress and widespread
applications. However, while applying machine learning to relational databases
(RDBs), the absence of a well-established benchmark remains a significant
obstacle to the development of ML. To address this issue, we introduce ML
Benchmark For Relational Databases (RDBench), a standardized benchmark that
aims to promote reproducible ML research on RDBs that include multiple tables.
RDBench offers diverse RDB datasets of varying scales, domains, and relational
structures, organized into 4 levels. Notably, to simplify the adoption of
RDBench for diverse ML domains, for any given database, RDBench exposes three
types of interfaces including tabular data, homogeneous graphs, and
heterogeneous graphs, sharing the same underlying task definition. For the
first time, RDBench enables meaningful comparisons between ML methods from
diverse domains, ranging from XGBoost to Graph Neural Networks, under RDB
prediction tasks. We design multiple classification and regression tasks for
each RDB dataset and report averaged results over the same dataset, further
enhancing the robustness of the experimental findings. RDBench is implemented
with DBGym, a user-friendly platform for ML research and application on
databases, enabling benchmarking new ML methods with RDBench at ease.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16838" title="Abstract">arXiv:2310.16838</a> [<a href="/pdf/2310.16838" title="Download PDF">pdf</a>, <a href="/format/2310.16838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous  Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianxu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haotong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Congyue Deng</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yixin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Guibas%2C+L">Leonidas Guibas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Humans excel at transferring manipulation skills across diverse object
shapes, poses, and appearances due to their understanding of semantic
correspondences between different instances. To endow robots with a similar
high-level understanding, we develop a Distilled Feature Field (DFF) for 3D
scenes, leveraging large 2D vision models to distill semantic features from
multiview images. While current research demonstrates advanced performance in
reconstructing DFFs from dense views, the development of learning a DFF from
sparse views is relatively nascent, despite its prevalence in numerous
manipulation tasks with fixed cameras. In this work, we introduce SparseDFF, a
novel method for acquiring view-consistent 3D DFFs from sparse RGBD
observations, enabling one-shot learning of dexterous manipulations that are
transferable to novel scenes. Specifically, we map the image features to the 3D
point cloud, allowing for propagation across the 3D space to establish a dense
feature field. At the core of SparseDFF is a lightweight feature refinement
network, optimized with a contrastive loss between pairwise views after
back-projecting the image features onto the 3D point cloud. Additionally, we
implement a point-pruning mechanism to augment feature continuity within each
local neighborhood. By establishing coherent feature fields on both source and
target scenes, we devise an energy function that facilitates the minimization
of feature discrepancies w.r.t. the end-effector parameters between the
demonstration and the target manipulation. We evaluate our approach using a
dexterous hand, mastering real-world manipulations on both rigid and deformable
objects, and showcase robust generalization in the face of object and
scene-context variations.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Thu, 26 Oct 23</h3>
<dl>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16015" title="Abstract">arXiv:2310.16015</a> (cross-list from physics.ao-ph) [<a href="/pdf/2310.16015" title="Download PDF">pdf</a>, <a href="/format/2310.16015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physically Explainable Deep Learning for Convective Initiation  Nowcasting Using GOES-16 Satellite Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Fan%2C+D">Da Fan</a>, 
<a href="/search/physics?searchtype=author&query=Greybush%2C+S+J">Steven J. Greybush</a>, 
<a href="/search/physics?searchtype=author&query=Gagne%2C+D+J">David John Gagne II</a>, 
<a href="/search/physics?searchtype=author&query=Clothiaux%2C+E+E">Eugene E. Clothiaux</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Convection initiation (CI) nowcasting remains a challenging problem for both
numerical weather prediction models and existing nowcasting algorithms. In this
study, object-based probabilistic deep learning models are developed to predict
CI based on multichannel infrared GOES-R satellite observations. The data come
from patches surrounding potential CI events identified in Multi-Radar
Multi-Sensor Doppler weather radar products over the Great Plains region from
June and July 2020 and June 2021. An objective radar-based approach is used to
identify these events. The deep learning models significantly outperform the
classical logistic model at lead times up to 1 hour, especially on the false
alarm ratio. Through case studies, the deep learning model exhibits the
dependence on the characteristics of clouds and moisture at multiple levels.
Model explanation further reveals the model's decision-making process with
different baselines. The explanation results highlight the importance of
moisture and cloud features at different levels depending on the choice of
baseline. Our study demonstrates the advantage of using different baselines in
further understanding model behavior and gaining scientific insights.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16096" title="Abstract">arXiv:2310.16096</a> (cross-list from stat.ML) [<a href="/pdf/2310.16096" title="Download PDF">pdf</a>, <a href="/ps/2310.16096" title="Download PostScript">ps</a>, <a href="/format/2310.16096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextual Bandits for Evaluating and Improving Inventory Control  Policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Foster%2C+D">Dean Foster</a>, 
<a href="/search/stat?searchtype=author&query=Jia%2C+R">Randy Jia</a>, 
<a href="/search/stat?searchtype=author&query=Madeka%2C+D">Dhruv Madeka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Solutions to address the periodic review inventory control problem with
nonstationary random demand, lost sales, and stochastic vendor lead times
typically involve making strong assumptions on the dynamics for either
approximation or simulation, and applying methods such as optimization, dynamic
programming, or reinforcement learning. Therefore, it is important to analyze
and evaluate any inventory control policy, in particular to see if there is
room for improvement. We introduce the concept of an equilibrium policy, a
desirable property of a policy that intuitively means that, in hindsight,
changing only a small fraction of actions does not result in materially more
reward. We provide a light-weight contextual bandit-based algorithm to evaluate
and occasionally tweak policies, and show that this method achieves favorable
guarantees, both theoretically and in empirical studies.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16102" title="Abstract">arXiv:2310.16102</a> (cross-list from eess.IV) [<a href="/pdf/2310.16102" title="Download PDF">pdf</a>, <a href="/format/2310.16102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learned, Uncertainty-driven Adaptive Acquisition for Photon-Efficient  Multiphoton Microscopy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ye%2C+C+T">Cassandra Tong Ye</a>, 
<a href="/search/eess?searchtype=author&query=Han%2C+J">Jiashu Han</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+K">Kunzan Liu</a>, 
<a href="/search/eess?searchtype=author&query=Angelopoulos%2C+A">Anastasios Angelopoulos</a>, 
<a href="/search/eess?searchtype=author&query=Griffith%2C+L">Linda Griffith</a>, 
<a href="/search/eess?searchtype=author&query=Monakhova%2C+K">Kristina Monakhova</a>, 
<a href="/search/eess?searchtype=author&query=You%2C+S">Sixian You</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Optics (physics.optics)

</div>
<p class="mathjax">Multiphoton microscopy (MPM) is a powerful imaging tool that has been a
critical enabler for live tissue imaging. However, since most multiphoton
microscopy platforms rely on point scanning, there is an inherent trade-off
between acquisition time, field of view (FOV), phototoxicity, and image
quality, often resulting in noisy measurements when fast, large FOV, and/or
gentle imaging is needed. Deep learning could be used to denoise multiphoton
microscopy measurements, but these algorithms can be prone to hallucination,
which can be disastrous for medical and scientific applications. We propose a
method to simultaneously denoise and predict pixel-wise uncertainty for
multiphoton imaging measurements, improving algorithm trustworthiness and
providing statistical guarantees for the deep learning predictions.
Furthermore, we propose to leverage this learned, pixel-wise uncertainty to
drive an adaptive acquisition technique that rescans only the most uncertain
regions of a sample. We demonstrate our method on experimental noisy MPM
measurements of human endometrium tissues, showing that we can maintain fine
features and outperform other denoising methods while predicting uncertainty at
each pixel. Finally, with our adaptive acquisition technique, we demonstrate a
120X reduction in acquisition time and total light dose while successfully
recovering fine features in the sample. We are the first to demonstrate
distribution-free uncertainty quantification for a denoising task with real
experimental data and the first to propose adaptive acquisition based on
reconstruction uncertainty
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16121" title="Abstract">arXiv:2310.16121</a> (cross-list from hep-ph) [<a href="/pdf/2310.16121" title="Download PDF">pdf</a>, <a href="/format/2310.16121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 19 Parameters Is All You Need: Tiny Neural Networks for Particle Physics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ph?searchtype=author&query=Bogatskiy%2C+A">Alexander Bogatskiy</a>, 
<a href="/search/hep-ph?searchtype=author&query=Hoffman%2C+T">Timothy Hoffman</a>, 
<a href="/search/hep-ph?searchtype=author&query=Offermann%2C+J+T">Jan T. Offermann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, submitted to the "Machine Learning and the Physical Sciences" NeurIPS 2023 Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Phenomenology (hep-ph)</span>; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)

</div>
<p class="mathjax">As particle accelerators increase their collision rates, and deep learning
solutions prove their viability, there is a growing need for lightweight and
fast neural network architectures for low-latency tasks such as triggering. We
examine the potential of one recent Lorentz- and permutation-symmetric
architecture, PELICAN, and present its instances with as few as 19 trainable
parameters that outperform generic architectures with tens of thousands of
parameters when compared on the binary classification task of top quark jet
tagging.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16136" title="Abstract">arXiv:2310.16136</a> (cross-list from stat.AP) [<a href="/pdf/2310.16136" title="Download PDF">pdf</a>, <a href="/format/2310.16136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Disparity and Temporal Progression of Internet Quality through  Crowdsourced Measurements with Bias-Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lee%2C+H">Hyeongseong Lee</a>, 
<a href="/search/stat?searchtype=author&query=Paul%2C+U">Udit Paul</a>, 
<a href="/search/stat?searchtype=author&query=Gupta%2C+A">Arpit Gupta</a>, 
<a href="/search/stat?searchtype=author&query=Belding%2C+E">Elizabeth Belding</a>, 
<a href="/search/stat?searchtype=author&query=Gu%2C+M">Mengyang Gu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Crowdsourced speedtest measurements are an important tool for studying
internet performance from the end user perspective. Nevertheless, despite the
accuracy of individual measurements, simplistic aggregation of these data
points is problematic due to their intrinsic sampling bias. In this work, we
utilize a dataset of nearly 1 million individual Ookla Speedtest measurements,
correlate each datapoint with 2019 Census demographic data, and develop new
methods to present a novel analysis to quantify regional sampling bias and the
relationship of internet performance to demographic profile. We find that the
crowdsourced Ookla Speedtest data points contain significant sampling bias
across different census block groups based on a statistical test of
homogeneity. We introduce two methods to correct the regional bias by the
population of each census block group. Whereas the sampling bias leads to a
small discrepancy in the overall cumulative distribution function of internet
speed in a city between estimation from original samples and bias-corrected
estimation, the discrepancy is much smaller compared to the size of the
sampling heterogeneity across regions. Further, we show that the sampling bias
is strongly associated with a few demographic variables, such as income,
education level, age, and ethnic distribution. Through regression analysis, we
find that regions with higher income, younger populations, and lower
representation of Hispanic residents tend to measure faster internet speeds
along with substantial collinearity amongst socioeconomic attributes and ethnic
composition. Finally, we find that average internet speed increases over time
based on both linear and nonlinear analysis from state space models, though the
regional sampling bias may result in a small overestimation of the temporal
increase of internet speed.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16139" title="Abstract">arXiv:2310.16139</a> (cross-list from eess.IV) [<a href="/pdf/2310.16139" title="Download PDF">pdf</a>, <a href="/format/2310.16139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pix2HDR -- A pixel-wise acquisition and deep learning-based synthesis  approach for high-speed HDR videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+C">Caixin Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wilson%2C+M+A">Matthew A. Wilson</a>, 
<a href="/search/eess?searchtype=author&query=Etienne-Cummings%2C+R">Ralph Etienne-Cummings</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Accurately capturing dynamic scenes with wide-ranging motion and light
intensity is crucial for many vision applications. However, acquiring
high-speed high dynamic range (HDR) video is challenging because the camera's
frame rate restricts its dynamic range. Existing methods sacrifice speed to
acquire multi-exposure frames. Yet, misaligned motion in these frames can still
pose complications for HDR fusion algorithms, resulting in artifacts. Instead
of frame-based exposures, we sample the videos using individual pixels at
varying exposures and phase offsets. Implemented on a pixel-wise programmable
image sensor, our sampling pattern simultaneously captures fast motion at a
high dynamic range. We then transform pixel-wise outputs into an HDR video
using end-to-end learned weights from deep neural networks, achieving high
spatiotemporal resolution with minimized motion blurring. We demonstrate
aliasing-free HDR video acquisition at 1000 FPS, resolving fast motion under
low-light conditions and against bright backgrounds - both challenging
conditions for conventional cameras. By combining the versatility of pixel-wise
sampling patterns with the strength of deep neural networks at decoding complex
scenes, our method greatly enhances the vision system's adaptability and
performance in dynamic conditions.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16140" title="Abstract">arXiv:2310.16140</a> (cross-list from eess.AS) [<a href="/pdf/2310.16140" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IA Para el Mantenimiento Predictivo en Canteras: Modelado
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Marcos%2C+F">Fernando Marcos</a>, 
<a href="/search/eess?searchtype=author&query=Tamaki%2C+R">Rodrigo Tamaki</a>, 
<a href="/search/eess?searchtype=author&query=C%C3%A1mara%2C+M">Mateo C&#xe1;mara</a>, 
<a href="/search/eess?searchtype=author&query=Yag%C3%BCe%2C+V">Virginia Yag&#xfc;e</a>, 
<a href="/search/eess?searchtype=author&query=Blanco%2C+J+L">Jos&#xe9; Luis Blanco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, in Spanish language, 5 figures. Presented in Tecniacustica 2023 conference (Cuenca, Spain)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Dependence on raw materials, especially in the mining sector, is a key part
of today's economy. Aggregates are vital, being the second most used raw
material after water. Digitally transforming this sector is key to optimizing
operations. However, supervision and maintenance (predictive and corrective)
are challenges little explored in this sector, due to the particularities of
the sector, machinery and environmental conditions. All this, despite the
successes achieved in other scenarios in monitoring with acoustic and contact
sensors. We present an unsupervised learning scheme that trains a variational
autoencoder model on a set of sound records. This is the first such dataset
collected during processing plant operations, containing information from
different points of the processing line. Our results demonstrate the model's
ability to reconstruct and represent in latent space the recorded sounds, the
differences in operating conditions and between different equipment. In the
future, this should facilitate the classification of sounds, as well as the
detection of anomalies and degradation patterns in the operation of the
machinery.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16175" title="Abstract">arXiv:2310.16175</a> (cross-list from eess.IV) [<a href="/pdf/2310.16175" title="Download PDF">pdf</a>, <a href="/format/2310.16175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> G-CASCADE: Efficient Cascaded Graph Convolutional Decoding for 2D  Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rahman%2C+M+M">Md Mostafijur Rahman</a>, 
<a href="/search/eess?searchtype=author&query=Marculescu%2C+R">Radu Marculescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, medical image segmentation has become an important
application in the field of computer-aided diagnosis. In this paper, we are the
first to propose a new graph convolution-based decoder namely, Cascaded Graph
Convolutional Attention Decoder (G-CASCADE), for 2D medical image segmentation.
G-CASCADE progressively refines multi-stage feature maps generated by
hierarchical transformer encoders with an efficient graph convolution block.
The encoder utilizes the self-attention mechanism to capture long-range
dependencies, while the decoder refines the feature maps preserving long-range
information due to the global receptive fields of the graph convolution block.
Rigorous evaluations of our decoder with multiple transformer encoders on five
medical image segmentation tasks (i.e., Abdomen organs, Cardiac organs, Polyp
lesions, Skin lesions, and Retinal vessels) show that our model outperforms
other state-of-the-art (SOTA) methods. We also demonstrate that our decoder
achieves better DICE scores than the SOTA CASCADE decoder with 80.8% fewer
parameters and 82.3% fewer FLOPs. Our decoder can easily be used with other
hierarchical encoders for general-purpose semantic and medical image
segmentation tasks.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16195" title="Abstract">arXiv:2310.16195</a> (cross-list from physics.app-ph) [<a href="/pdf/2310.16195" title="Download PDF">pdf</a>, <a href="/format/2310.16195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Systematic Physics-Compliant Analysis of Over-the-Air Channel  Equalization in RIS-Parametrized Wireless Networks-on-Chip
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Tapie%2C+J">Jean Tapie</a>, 
<a href="/search/physics?searchtype=author&query=Prod%27homme%2C+H">Hugo Prod&#x27;homme</a>, 
<a href="/search/physics?searchtype=author&query=Imani%2C+M+F">Mohammadreza F. Imani</a>, 
<a href="/search/physics?searchtype=author&query=del+Hougne%2C+P">Philipp del Hougne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures, submitted to an IEEE Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applied Physics (physics.app-ph)</span>; Hardware Architecture (cs.AR); Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Wireless networks-on-chip (WNoCs) are an enticing complementary interconnect
technology for multi-core chips but face severe resource constraints. Being
limited to simple on-off-keying modulation, the reverberant nature of the chip
enclosure imposes limits on allowed modulation speeds in sight of inter-symbol
interference, casting doubts on the competitiveness of WNoCs as interconnect
technology. Fortunately, this vexing problem was recently overcome by
parametrizing the on-chip radio environment with a reconfigurable intelligent
surface (RIS). By suitably configuring the RIS, selected channel impulse
responses (CIRs) can be tuned to be (almost) pulse-like despite rich scattering
thanks to judiciously tailored multi-bounce path interferences. However, the
exploration of this "over-the-air" (OTA) equalization is thwarted by (i) the
overwhelming complexity of the propagation environment, and (ii) the non-linear
dependence of the CIR on the RIS configuration, requiring a costly and lengthy
full-wave simulation for every optimization step. Here, we show that a
reduced-basis physics-compliant model for RIS-parametrized WNoCs can be
calibrated with a single full-wave simulation. Thereby, we unlock the
possibility of predicting the CIR for any RIS configuration almost
instantaneously without any additional full-wave simulation. We leverage this
new tool to systematically explore OTA equalization in RIS-parametrized WNoCs
regarding the optimal choice of delay time for the RIS-shaped CIR's peak. We
also study the simultaneous optimization of multiple on-chip wireless links for
broadcasting. Looking forward, the introduced tools will enable the efficient
exploration of various types of OTA analog computing in RIS-parametrized WNoCs.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16202" title="Abstract">arXiv:2310.16202</a> (cross-list from math.AP) [<a href="/pdf/2310.16202" title="Download PDF">pdf</a>, <a href="/format/2310.16202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Existence of solution to a system of PDEs modeling the crystal growth  inside lithium batteries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lakkis%2C+O">Omar Lakkis</a>, 
<a href="/search/math?searchtype=author&query=Skouras%2C+A">Alexandros Skouras</a>, 
<a href="/search/math?searchtype=author&query=Styles%2C+V">Vanessa Styles</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 22 figures, free software and open source code available
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Materials Science (cond-mat.mtrl-sci); Numerical Analysis (math.NA)

</div>
<p class="mathjax">The life-cycle of electric batteries depends on a complex system of
interacting electrochemical and growth phenomena that produce dendritic
structures during the discharge cycle. We study herein a system of 3 partial
differential equations combining an Allen--Cahn phase-field model (simulating
the dendrite-electrolyte interface) with the Poisson--Nernst--Planck systems
simulating the electrodynamics and leading to the formation of such dendritic
structures. We prove novel existence, uniqueness and stability results for this
system and use it to produce simulations based on a finite element code.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16246" title="Abstract">arXiv:2310.16246</a> (cross-list from math.OC) [<a href="/pdf/2310.16246" title="Download PDF">pdf</a>, <a href="/format/2310.16246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design of General Purpose Minimal-Auxiliary Ising Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Martin%2C+I+K">Isaac K. Martin</a>, 
<a href="/search/math?searchtype=author&query=Moore%2C+A+G">Andrew G. Moore</a>, 
<a href="/search/math?searchtype=author&query=Daly%2C+J+T">John T. Daly</a>, 
<a href="/search/math?searchtype=author&query=Meyer%2C+J+J">Jess J. Meyer</a>, 
<a href="/search/math?searchtype=author&query=Ranadive%2C+T+M">Teresa M. Ranadive</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures, submitted to IEEE International Conference on Rebooting Computing 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Emerging Technologies (cs.ET); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Ising machines are a form of quantum-inspired processing-in-memory computer
which has shown great promise for overcoming the limitations of traditional
computing paradigms while operating at a fraction of the energy use. The
process of designing Ising machines is known as the reverse Ising problem.
Unfortunately, this problem is in general computationally intractable: it is a
nonconvex mixed-integer linear programming problem which cannot be naively
brute-forced except in the simplest cases due to exponential scaling of runtime
with number of spins. We prove new theoretical results which allow us to reduce
the search space to one with quadratic scaling. We utilize this theory to
develop general purpose algorithmic solutions to the reverse Ising problem. In
particular, we demonstrate Ising formulations of 3-bit and 4-bit integer
multiplication which use fewer total spins than previously known methods by a
factor of more than three. Our results increase the practicality of
implementing such circuits on modern Ising hardware, where spins are at a
premium.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16276" title="Abstract">arXiv:2310.16276</a> (cross-list from physics.soc-ph) [<a href="/pdf/2310.16276" title="Download PDF">pdf</a>, <a href="/format/2310.16276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity of Government response to Covid-19 pandemic: A perspective of  coupled dynamics on information heterogeneity and epidemic outbreak
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Zhang%2C+X">Xiaoqi Zhang</a>, 
<a href="/search/physics?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/physics?searchtype=author&query=Hua%2C+S">Sheng Hua</a>, 
<a href="/search/physics?searchtype=author&query=Liang%2C+H">Han Liang</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+Z">Zi-Ke Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version contains the full-resolution figures for the paper DOI: 10.1007/s11071-023-08427-5
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">This study aims at modeling the universal failure in preventing the outbreak
of COVID-19 via real-world data from the perspective of complexity and network
science. Through formalizing information heterogeneity and government
intervention in the coupled dynamics of epidemic and infodemic spreading;
first, we find that information heterogeneity and its induced variation in
human responses significantly increase the complexity of the government
intervention decision. The complexity results in a dilemma between the socially
optimal intervention that is risky for the government and the privately optimal
intervention that is safer for the government but harmful to the social
welfare. Second, via counterfactual analysis against the COVID-19 crisis in
Wuhan, 2020, we find that the intervention dilemma becomes even worse if the
initial decision time and the decision horizon vary. In the short horizon, both
socially and privately optimal interventions agree with each other and require
blocking the spread of all COVID-19-related information, leading to a
negligible infection ratio 30 days after the initial reporting time. However,
if the time horizon is prolonged to 180 days, only the privately optimal
intervention requires information blocking, which would induce a
catastrophically higher infection ratio than that in the counter-factual world
where the socially optimal intervention encourages early-stage information
spread. These findings contribute to the literature by revealing the complexity
incurred by the coupled infodemic-epidemic dynamics and information
heterogeneity to the governmental intervention decision, which also sheds
insight into the design of an effective early warning system against the
epidemic crisis in the future.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16285" title="Abstract">arXiv:2310.16285</a> (cross-list from astro-ph.CO) [<a href="/pdf/2310.16285" title="Download PDF">pdf</a>, <a href="/format/2310.16285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Removing Dust from CMB Observations with Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Heurtel-Depeiges%2C+D">David Heurtel-Depeiges</a>, 
<a href="/search/astro-ph?searchtype=author&query=Burkhart%2C+B">Blakesley Burkhart</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ohana%2C+R">Ruben Ohana</a>, 
<a href="/search/astro-ph?searchtype=author&query=Blancard%2C+B+R">Bruno R&#xe9;galdo-Saint Blancard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5+6 pages, 2+3 figures, submitted to "Machine Learning and the Physical Sciences" NeurIPS Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cosmology and Nongalactic Astrophysics (astro-ph.CO)</span>; Astrophysics of Galaxies (astro-ph.GA); Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG)

</div>
<p class="mathjax">In cosmology, the quest for primordial $B$-modes in cosmic microwave
background (CMB) observations has highlighted the critical need for a refined
model of the Galactic dust foreground. We investigate diffusion-based modeling
of the dust foreground and its interest for component separation. Under the
assumption of a Gaussian CMB with known cosmology (or covariance matrix), we
show that diffusion models can be trained on examples of dust emission maps
such that their sampling process directly coincides with posterior sampling in
the context of component separation. We illustrate this on simulated mixtures
of dust emission and CMB. We show that common summary statistics (power
spectrum, Minkowski functionals) of the components are well recovered by this
process. We also introduce a model conditioned by the CMB cosmology that
outperforms models trained using a single cosmology on component separation.
Such a model will be used in future work for diffusion-based cosmological
inference.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16320" title="Abstract">arXiv:2310.16320</a> (cross-list from stat.ML) [<a href="/pdf/2310.16320" title="Download PDF">pdf</a>, <a href="/format/2310.16320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Low-Precision Sampling via Stochastic Gradient Hamiltonian  Monte Carlo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wang%2C+Z">Ziyi Wang</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+Y">Yujie Chen</a>, 
<a href="/search/stat?searchtype=author&query=Song%2C+Q">Qifan Song</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+R">Ruqi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Low-precision training has emerged as a promising low-cost technique to
enhance the training efficiency of deep neural networks without sacrificing
much accuracy. Its Bayesian counterpart can further provide uncertainty
quantification and improved generalization accuracy. This paper investigates
low-precision sampling via Stochastic Gradient Hamiltonian Monte Carlo (SGHMC)
with low-precision and full-precision gradient accumulators for both strongly
log-concave and non-log-concave distributions. Theoretically, our results show
that, to achieve $\epsilon$-error in the 2-Wasserstein distance for
non-log-concave distributions, low-precision SGHMC achieves quadratic
improvement
($\widetilde{\mathbf{O}}\left({\epsilon^{-2}{\mu^*}^{-2}\log^2\left({\epsilon^{-1}}\right)}\right)$)
compared to the state-of-the-art low-precision sampler, Stochastic Gradient
Langevin Dynamics (SGLD)
($\widetilde{\mathbf{O}}\left({{\epsilon}^{-4}{\lambda^{*}}^{-1}\log^5\left({\epsilon^{-1}}\right)}\right)$).
Moreover, we prove that low-precision SGHMC is more robust to the quantization
error compared to low-precision SGLD due to the robustness of the
momentum-based update w.r.t. gradient noise. Empirically, we conduct
experiments on synthetic data, and {MNIST, CIFAR-10 \&amp; CIFAR-100} datasets,
which validate our theoretical findings. Our study highlights the potential of
low-precision SGHMC as an efficient and accurate sampling method for
large-scale and resource-limited machine learning.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16323" title="Abstract">arXiv:2310.16323</a> (cross-list from stat.ML) [<a href="/pdf/2310.16323" title="Download PDF">pdf</a>, <a href="/format/2310.16323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Federated X -armed Bandit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+W">Wenjie Li</a>, 
<a href="/search/stat?searchtype=author&query=Song%2C+Q">Qifan Song</a>, 
<a href="/search/stat?searchtype=author&query=Honorio%2C+J">Jean Honorio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we study the personalized federated $\mathcal{X}$-armed bandit
problem, where the heterogeneous local objectives of the clients are optimized
simultaneously in the federated learning paradigm. We propose the
\texttt{PF-PNE} algorithm with a unique double elimination strategy, which
safely eliminates the non-optimal regions while encouraging federated
collaboration through biased but effective evaluations of the local objectives.
The proposed \texttt{PF-PNE} algorithm is able to optimize local objectives
with arbitrary levels of heterogeneity, and its limited communications protects
the confidentiality of the client-wise reward data. Our theoretical analysis
shows the benefit of the proposed algorithm over single-client algorithms.
Experimentally, \texttt{PF-PNE} outperforms multiple baselines on both
synthetic and real life datasets.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16338" title="Abstract">arXiv:2310.16338</a> (cross-list from eess.AS) [<a href="/pdf/2310.16338" title="Download PDF">pdf</a>, <a href="/format/2310.16338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Pre-training for Speech with Flow Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+A+H">Alexander H. Liu</a>, 
<a href="/search/eess?searchtype=author&query=Le%2C+M">Matt Le</a>, 
<a href="/search/eess?searchtype=author&query=Vyas%2C+A">Apoorv Vyas</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+B">Bowen Shi</a>, 
<a href="/search/eess?searchtype=author&query=Tjandra%2C+A">Andros Tjandra</a>, 
<a href="/search/eess?searchtype=author&query=Hsu%2C+W">Wei-Ning Hsu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">Generative models have gained more and more attention in recent years for
their remarkable success in tasks that required estimating and sampling data
distribution to generate high-fidelity synthetic data. In speech,
text-to-speech synthesis and neural vocoder are good examples where generative
models have shined. While generative models have been applied to different
applications in speech, there exists no general-purpose generative model that
models speech directly. In this work, we take a step toward this direction by
showing a single pre-trained generative model can be adapted to different
downstream tasks with strong performance. Specifically, we pre-trained a
generative model, named SpeechFlow, on 60k hours of untranscribed speech with
Flow Matching and masked conditions. Experiment results show the pre-trained
generative model can be fine-tuned with task-specific data to match or surpass
existing expert models on speech enhancement, separation, and synthesis. Our
work suggested a foundational model for generation tasks in speech can be built
with generative pre-training.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16387" title="Abstract">arXiv:2310.16387</a> (cross-list from eess.IV) [<a href="/pdf/2310.16387" title="Download PDF">pdf</a>, <a href="/format/2310.16387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Frequency-Aware Transformer for Learned Image Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Han Li</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shaohui Li</a>, 
<a href="/search/eess?searchtype=author&query=Dai%2C+W">Wenrui Dai</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+C">Chenglin Li</a>, 
<a href="/search/eess?searchtype=author&query=Zou%2C+J">Junni Zou</a>, 
<a href="/search/eess?searchtype=author&query=Xiong%2C+H">Hongkai Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Learned image compression (LIC) has gained traction as an effective solution
for image storage and transmission in recent years. However, existing LIC
methods are redundant in latent representation due to limitations in capturing
anisotropic frequency components and preserving directional details. To
overcome these challenges, we propose a novel frequency-aware transformer (FAT)
block that for the first time achieves multiscale directional ananlysis for
LIC. The FAT block comprises frequency-decomposition window attention (FDWA)
modules to capture multiscale and directional frequency components of natural
images. Additionally, we introduce frequency-modulation feed-forward network
(FMFFN) to adaptively modulate different frequency components, improving
rate-distortion performance. Furthermore, we present a transformer-based
channel-wise autoregressive (T-CA) model that effectively exploits channel
dependencies. Experiments show that our method achieves state-of-the-art
rate-distortion performance compared to existing LIC methods, and evidently
outperforms latest standardized codec VTM-12.1 by 14.5%, 15.1%, 13.0% in
BD-rate on the Kodak, Tecnick, and CLIC datasets.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16437" title="Abstract">arXiv:2310.16437</a> (cross-list from math.AT) [<a href="/pdf/2310.16437" title="Download PDF">pdf</a>, <a href="/format/2310.16437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-isotropic Persistent Homology: Leveraging the Metric Dependency of  PH
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Grande%2C+V+P">Vincent P. Grande</a>, 
<a href="/search/math?searchtype=author&query=Schaub%2C+M+T">Michael T. Schaub</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 17 figures, comments welcome!
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Topology (math.AT)</span>; Computational Geometry (cs.CG); Machine Learning (cs.LG)

</div>
<p class="mathjax">Persistent Homology is a widely used topological data analysis tool that
creates a concise description of the topological properties of a point cloud
based on a specified filtration. Most filtrations used for persistent homology
depend (implicitly) on a chosen metric, which is typically agnostically chosen
as the standard Euclidean metric on $\mathbb{R}^n$. Recent work has tried to
uncover the 'true' metric on the point cloud using distance-to-measure
functions, in order to obtain more meaningful persistent homology results. Here
we propose an alternative look at this problem: we posit that information on
the point cloud is lost when restricting persistent homology to a single
(correct) distance function. Instead, we show how by varying the distance
function on the underlying space and analysing the corresponding shifts in the
persistence diagrams, we can extract additional topological and geometrical
information. Finally, we numerically show that non-isotropic persistent
homology can extract information on orientation, orientational variance, and
scaling of randomly generated point clouds with good accuracy and conduct some
experiments on real-world data.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16441" title="Abstract">arXiv:2310.16441</a> (cross-list from stat.ML) [<a href="/pdf/2310.16441" title="Download PDF">pdf</a>, <a href="/format/2310.16441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grokking in Linear Estimators -- A Solvable Model that Groks without  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Levi%2C+N">Noam Levi</a>, 
<a href="/search/stat?searchtype=author&query=Beck%2C+A">Alon Beck</a>, 
<a href="/search/stat?searchtype=author&query=Bar-Sinai%2C+Y">Yohai Bar-Sinai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG); Mathematical Physics (math-ph)

</div>
<p class="mathjax">Grokking is the intriguing phenomenon where a model learns to generalize long
after it has fit the training data. We show both analytically and numerically
that grokking can surprisingly occur in linear networks performing linear tasks
in a simple teacher-student setup with Gaussian inputs. In this setting, the
full training dynamics is derived in terms of the training and generalization
data covariance matrix. We present exact predictions on how the grokking time
depends on input and output dimensionality, train sample size, regularization,
and network initialization. We demonstrate that the sharp increase in
generalization accuracy may not imply a transition from "memorization" to
"understanding", but can simply be an artifact of the accuracy measure. We
provide empirical verification for our calculations, along with preliminary
results indicating that some predictions also hold for deeper networks, with
non-linear activations.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16463" title="Abstract">arXiv:2310.16463</a> (cross-list from math.CO) [<a href="/pdf/2310.16463" title="Download PDF">pdf</a>, <a href="/ps/2310.16463" title="Download PostScript">ps</a>, <a href="/format/2310.16463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constructing disjoint Steiner trees in Sierpi&#x144;ski graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yang%2C+C">Chenxu Yang</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+P">Ping Li</a>, 
<a href="/search/math?searchtype=author&query=Mao%2C+Y">Yaping Mao</a>, 
<a href="/search/math?searchtype=author&query=Cheng%2C+E">Eddie Cheng</a>, 
<a href="/search/math?searchtype=author&query=Klasing%2C+R">Ralf Klasing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Let $G$ be a graph and $S\subseteq V(G)$ with $|S|\geq 2$. Then the trees
$T_1, T_2, \cdots, T_\ell$ in $G$ are \emph{internally disjoint Steiner trees}
connecting $S$ (or $S$-Steiner trees) if $E(T_i) \cap E(T_j )=\emptyset$ and
$V(T_i)\cap V(T_j)=S$ for every pair of distinct integers $i,j$, $1 \leq i, j
\leq \ell$. Similarly, if we only have the condition $E(T_i) \cap E(T_j
)=\emptyset$ but without the condition $V(T_i)\cap V(T_j)=S$, then they are
\emph{edge-disjoint Steiner trees}. The \emph{generalized $k$-connectivity},
denoted by $\kappa_k(G)$, of a graph $G$, is defined as
$\kappa_k(G)=\min\{\kappa_G(S)|S \subseteq V(G) \ \textrm{and} \ |S|=k \}$,
where $\kappa_G(S)$ is the maximum number of internally disjoint $S$-Steiner
trees. The \emph{generalized local edge-connectivity} $\lambda_{G}(S)$ is the
maximum number of edge-disjoint Steiner trees connecting $S$ in $G$. The {\it
generalized $k$-edge-connectivity} $\lambda_k(G)$ of $G$ is defined as
$\lambda_k(G)=\min\{\lambda_{G}(S)\,|\,S\subseteq V(G) \ and \ |S|=k\}$. These
measures are generalizations of the concepts of connectivity and
edge-connectivity, and they and can be used as measures of vulnerability of
networks. It is, in general, difficult to compute these generalized
connectivities. However, there are precise results for some special classes of
graphs. In this paper, we obtain the exact value of $\lambda_{k}(S(n,\ell))$
for $3\leq k\leq \ell^n$, and the exact value of $\kappa_{k}(S(n,\ell))$ for
$3\leq k\leq \ell$, where $S(n, \ell)$ is the Sierpi\'{n}ski graphs with order
$\ell^n$. As a direct consequence, these graphs provide additional interesting
examples when $\lambda_{k}(S(n,\ell))=\kappa_{k}(S(n,\ell))$. We also study the
some network properties of Sierpi\'{n}ski graphs.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16481" title="Abstract">arXiv:2310.16481</a> (cross-list from eess.AS) [<a href="/pdf/2310.16481" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Approach for Object Based Audio Broadcasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hasanabadi%2C+M+R">Mohammad Reza Hasanabadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ABU Technical Review Journal 2020/9
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Object Based Audio (OBA) provides a new kind of audio experience, delivered
to the audience to personalize and customize their experience of listening and
to give them choice of what and how to hear their audio content. OBA can be
applied to different platforms such as broadcasting, streaming and cinema
sound. This paper presents a novel approach for creating object-based audio on
the production side. The approach here presents Sample-by-Sample Object Based
Audio (SSOBA) embedding. SSOBA places audio object samples in such a way that
allows audiences to easily individualize their chosen audio sources according
to their interests and needs. SSOBA is an extra service and not an alternative,
so it is also compliant with legacy audio players. The biggest advantage of
SSOBA is that it does not require any special additional hardware in the
broadcasting chain and it is therefore easy to implement and equip legacy
players and decoders with enhanced ability. Input audio objects, number of
output channels and sampling rates are three important factors affecting SSOBA
performance and specifying it to be lossless or lossy. SSOBA adopts
interpolation at the decoder side to compensate for eliminated samples. Both
subjective and objective experiments are carried out to evaluate the output
results at each step. MUSHRA subjective experiments conducted after the
encoding step shows good-quality performance of SSOBA with up to five objects.
SNR measurements and objective experiments, performed after decoding and
interpolation, show significant successful recovery and separation of audio
objects. Experimental results show that a minimum sampling rate of 96 kHz is
indicated to encode up to five objects in a Stereo-mode channel to acquire good
subjective and objective results simultaneously.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16516" title="Abstract">arXiv:2310.16516</a> (cross-list from stat.ML) [<a href="/pdf/2310.16516" title="Download PDF">pdf</a>, <a href="/format/2310.16516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Particle-based Variational Inference with Generalized Wasserstein  Gradient Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cheng%2C+Z">Ziheng Cheng</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+S">Shiyue Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Yu%2C+L">Longlin Yu</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+C">Cheng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Particle-based variational inference methods (ParVIs) such as Stein
variational gradient descent (SVGD) update the particles based on the
kernelized Wasserstein gradient flow for the Kullback-Leibler (KL) divergence.
However, the design of kernels is often non-trivial and can be restrictive for
the flexibility of the method. Recent works show that functional gradient flow
approximations with quadratic form regularization terms can improve
performance. In this paper, we propose a ParVI framework, called generalized
Wasserstein gradient descent (GWG), based on a generalized Wasserstein gradient
flow of the KL divergence, which can be viewed as a functional gradient method
with a broader class of regularizers induced by convex functions. We show that
GWG exhibits strong convergence guarantees. We also provide an adaptive version
that automatically chooses Wasserstein metric to accelerate convergence. In
experiments, we demonstrate the effectiveness and efficiency of the proposed
framework on both simulated and real data problems.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16574" title="Abstract">arXiv:2310.16574</a> (cross-list from stat.ML) [<a href="/pdf/2310.16574" title="Download PDF">pdf</a>, <a href="/format/2310.16574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-scale magnetic field maps using structured kernel interpolation  for Gaussian process regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Menzen%2C+C">Clara Menzen</a>, 
<a href="/search/stat?searchtype=author&query=Fetter%2C+M">Marnix Fetter</a>, 
<a href="/search/stat?searchtype=author&query=Kok%2C+M">Manon Kok</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a mapping algorithm to compute large-scale magnetic field maps in
indoor environments with approximate Gaussian process (GP) regression. Mapping
the spatial variations in the ambient magnetic field can be used for
localization algorithms in indoor areas. To compute such a map, GP regression
is a suitable tool because it provides predictions of the magnetic field at new
locations along with uncertainty quantification. Because full GP regression has
a complexity that grows cubically with the number of data points,
approximations for GPs have been extensively studied. In this paper, we build
on the structured kernel interpolation (SKI) framework, speeding up inference
by exploiting efficient Krylov subspace methods. More specifically, we
incorporate SKI with derivatives (D-SKI) into the scalar potential model for
magnetic field modeling and compute both predictive mean and covariance with a
complexity that is linear in the data points. In our simulations, we show that
our method achieves better accuracy than current state-of-the-art methods on
magnetic field maps with a growing mapping area. In our large-scale
experiments, we construct magnetic field maps from up to 40000
three-dimensional magnetic field measurements in less than two minutes on a
standard laptop.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16577" title="Abstract">arXiv:2310.16577</a> (cross-list from stat.ML) [<a href="/pdf/2310.16577" title="Download PDF">pdf</a>, <a href="/format/2310.16577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mapping the magnetic field using a magnetometer array with noisy input  Gaussian process regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Edridge%2C+T">Thomas Edridge</a>, 
<a href="/search/stat?searchtype=author&query=Kok%2C+M">Manon Kok</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Ferromagnetic materials in indoor environments give rise to disturbances in
the ambient magnetic field. Maps of these magnetic disturbances can be used for
indoor localisation. A Gaussian process can be used to learn the spatially
varying magnitude of the magnetic field using magnetometer measurements and
information about the position of the magnetometer. The position of the
magnetometer, however, is frequently only approximately known. This negatively
affects the quality of the magnetic field map. In this paper, we investigate
how an array of magnetometers can be used to improve the quality of the
magnetic field map. The position of the array is approximately known, but the
relative locations of the magnetometers on the array are known. We include this
information in a novel method to make a map of the ambient magnetic field. We
study the properties of our method in simulation and show that our method
improves the map quality. We also demonstrate the efficacy of our method with
experimental data for the mapping of the magnetic field using an array of 30
magnetometers.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16597" title="Abstract">arXiv:2310.16597</a> (cross-list from stat.ML) [<a href="/pdf/2310.16597" title="Download PDF">pdf</a>, <a href="/format/2310.16597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond IID weights: sparse and low-rank deep Neural Networks are also  Gaussian Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nait-Saada%2C+T">Thiziri Nait-Saada</a>, 
<a href="/search/stat?searchtype=author&query=Naderi%2C+A">Alireza Naderi</a>, 
<a href="/search/stat?searchtype=author&query=Tanner%2C+J">Jared Tanner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The infinitely wide neural network has been proven a useful and manageable
mathematical model that enables the understanding of many phenomena appearing
in deep learning. One example is the convergence of random deep networks to
Gaussian processes that allows a rigorous analysis of the way the choice of
activation function and network weights impacts the training dynamics. In this
paper, we extend the seminal proof of Matthews et al. (2018) to a larger class
of initial weight distributions (which we call PSEUDO-IID), including the
established cases of IID and orthogonal weights, as well as the emerging
low-rank and structured sparse settings celebrated for their computational
speed-up benefits. We show that fully-connected and convolutional networks
initialized with PSEUDO-IID distributions are all effectively equivalent up to
their variance. Using our results, one can identify the Edge-of-Chaos for a
broader class of neural networks and tune them at criticality in order to
enhance their training.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16600" title="Abstract">arXiv:2310.16600</a> (cross-list from stat.ME) [<a href="/pdf/2310.16600" title="Download PDF">pdf</a>, <a href="/format/2310.16600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balancing central and marginal rejection when combining independent  significance tests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Salahub%2C+C">Chris Salahub</a>, 
<a href="/search/stat?searchtype=author&query=Oldford%2C+R+W">R. Wayne Oldford</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 55 page, 18 figures, public technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">A common approach to evaluating the significance of a collection of
$p$-values combines them with a pooling function, in particular when the
original data are not available. These pooled $p$-values convert a sample of
$p$-values into a single number which behaves like a univariate $p$-value. To
clarify discussion of these functions, a telescoping series of alternative
hypotheses are introduced that communicate the strength and prevalence of
non-null evidence in the $p$-values before general pooling formulae are
discussed. A pattern noticed in the UMP pooled $p$-value for a particular
alternative motivates the definition and discussion of central and marginal
rejection levels at $\alpha$. It is proven that central rejection is always
greater than or equal to marginal rejection, motivating a quotient to measure
the balance between the two for pooled $p$-values. A combining function based
on the $\chi^2_{\kappa}$ quantile transformation is proposed to control this
quotient and shown to be robust to mis-specified parameters relative to the
UMP. Different powers for different parameter settings motivate a map of
plausible alternatives based on where this pooled $p$-value is minimized.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16638" title="Abstract">arXiv:2310.16638</a> (cross-list from stat.ME) [<a href="/pdf/2310.16638" title="Download PDF">pdf</a>, <a href="/format/2310.16638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Covariate Shift Adaptation Robust to Density-Ratio Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kato%2C+M">Masahiro Kato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Econometrics (econ.EM); Machine Learning (stat.ML)

</div>
<p class="mathjax">Consider a scenario where we have access to train data with both covariates
and outcomes while test data only contains covariates. In this scenario, our
primary aim is to predict the missing outcomes of the test data. With this
objective in mind, we train parametric regression models under a covariate
shift, where covariate distributions are different between the train and test
data. For this problem, existing studies have proposed covariate shift
adaptation via importance weighting using the density ratio. This approach
averages the train data losses, each weighted by an estimated ratio of the
covariate densities between the train and test data, to approximate the
test-data risk. Although it allows us to obtain a test-data risk minimizer, its
performance heavily relies on the accuracy of the density ratio estimation.
Moreover, even if the density ratio can be consistently estimated, the
estimation errors of the density ratio also yield bias in the estimators of the
regression model's parameters of interest. To mitigate these challenges, we
introduce a doubly robust estimator for covariate shift adaptation via
importance weighting, which incorporates an additional estimator for the
regression function. Leveraging double machine learning techniques, our
estimator reduces the bias arising from the density ratio estimation errors. We
demonstrate the asymptotic distribution of the regression parameter estimator.
Notably, our estimator remains consistent if either the density ratio estimator
or the regression function is consistent, showcasing its robustness against
potential errors in density ratio estimation. Finally, we confirm the soundness
of our proposed method via simulation studies.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16662" title="Abstract">arXiv:2310.16662</a> (cross-list from eess.IV) [<a href="/pdf/2310.16662" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Techniques for Cervical Cancer Diagnosis based on  Pathology and Colposcopy Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sarhangi%2C+H+A">Hana Ahmadzadeh Sarhangi</a>, 
<a href="/search/eess?searchtype=author&query=Beigifard%2C+D">Dorsa Beigifard</a>, 
<a href="/search/eess?searchtype=author&query=Farmani%2C+E">Elahe Farmani</a>, 
<a href="/search/eess?searchtype=author&query=Bolhasani%2C+H">Hamidreza Bolhasani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Cervical cancer is a prevalent disease affecting millions of women worldwide
every year. It requires significant attention, as early detection during the
precancerous stage provides an opportunity for a cure. The screening and
diagnosis of cervical cancer rely on cytology and colposcopy methods. Deep
learning, a promising technology in computer vision, has emerged as a potential
solution to improve the accuracy and efficiency of cervical cancer screening
compared to traditional clinical inspection methods that are prone to human
error. This review article discusses cervical cancer and its screening
processes, followed by the Deep Learning training process and the
classification, segmentation, and detection tasks for cervical cancer
diagnosis. Additionally, we explored the most common public datasets used in
both cytology and colposcopy and highlighted the popular and most utilized
architectures that researchers have applied to both cytology and colposcopy. We
reviewed 24 selected practical papers in this study and summarized them. This
article highlights the remarkable efficiency in enhancing the precision and
speed of cervical cancer analysis by Deep Learning, bringing us closer to early
diagnosis and saving lives.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16715" title="Abstract">arXiv:2310.16715</a> (cross-list from math.PR) [<a href="/pdf/2310.16715" title="Download PDF">pdf</a>, <a href="/ps/2310.16715" title="Download PostScript">ps</a>, <a href="/format/2310.16715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Algorithm to Recover Shredded Random Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Atamanchuk%2C+C">Caelan Atamanchuk</a>, 
<a href="/search/math?searchtype=author&query=Devroye%2C+L">Luc Devroye</a>, 
<a href="/search/math?searchtype=author&query=Vicenzo%2C+M">Massimo Vicenzo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS); Combinatorics (math.CO)

</div>
<p class="mathjax">Given some binary matrix $M$, suppose we are presented with the collection of
its rows and columns in independent arbitrary orderings. From this information,
are we able to recover the unique original orderings and matrix? We present an
algorithm that identifies whether there is a unique ordering associated with a
set of rows and columns, and outputs either the unique correct orderings for
the rows and columns or the full collection of all valid orderings and valid
matrices. We show that there is a constant $c &gt; 0$ such that the algorithm
terminates in $O(n^2)$ time with high probability and in expectation for random
$n \times n$ binary matrices with i.i.d.\ Bernoulli $(p)$ entries
$(m_{ij})_{ij=1}^n$ such that $\frac{c\log^2(n)}{n(\log\log(n))^2} \leq p \leq
\frac{1}{2}$.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16724" title="Abstract">arXiv:2310.16724</a> (cross-list from eess.SP) [<a href="/pdf/2310.16724" title="Download PDF">pdf</a>, <a href="/format/2310.16724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spherical Wavefront Near-Field DoA Estimation in THz Automotive Radar
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Elbir%2C+A+M">Ahmet M. Elbir</a>, 
<a href="/search/eess?searchtype=author&query=Mishra%2C+K+V">Kumar Vijay Mishra</a>, 
<a href="/search/eess?searchtype=author&query=Chatzinotas%2C+S">Symeon Chatzinotas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Automotive radar at terahertz (THz) band has the potential to provide compact
design. The availability of wide bandwidth at THz-band leads to high range
resolution. Further, very narrow beamwidth arising from large arrays yields
high angular resolution up to milli-degree level direction-of-arrival (DoA)
estimation. At THz frequencies and extremely large arrays, the signal wavefront
is spherical in the near-field that renders traditional far-field DoA
estimation techniques unusable. In this work, we examine near-field DoA
estimation for THz automotive radar. We propose an algorithm using multiple
signal classification (MUSIC) to estimate target DoAs and ranges while also
taking beam-squint in near-field into account. Using an array transformation
approach, we compensate for near-field beam-squint in noise subspace
computations to construct the beam-squint-free MUSIC spectra. Numerical
experiments show the effectiveness of the proposed method to accurately
estimate the target parameters.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16742" title="Abstract">arXiv:2310.16742</a> (cross-list from quant-ph) [<a href="/pdf/2310.16742" title="Download PDF">pdf</a>, <a href="/format/2310.16742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interferometric Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Sehrawat%2C+A">Arun Sehrawat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">On the one hand, artificial neural networks have many successful applications
in the field of machine learning and optimization. On the other hand,
interferometers are integral parts of any field that deals with waves such as
optics, astronomy, and quantum physics. Here, we introduce neural networks
composed of interferometers and then build generative adversarial networks from
them. Our networks do not have any classical layer and can be realized on
quantum computers or photonic chips. We demonstrate their applicability for
combinatorial optimization, image classification, and image generation. For
combinatorial optimization, our network consistently converges to the global
optimum or remains within a narrow range of it. In multi-class image
classification tasks, our networks achieve accuracies of 93% and 83%. Lastly,
we show their capability to generate images of digits from 0 to 9 as well as
human faces.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16777" title="Abstract">arXiv:2310.16777</a> (cross-list from stat.ML) [<a href="/pdf/2310.16777" title="Download PDF">pdf</a>, <a href="/format/2310.16777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MixerFlow for Image Modelling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=English%2C+E">Eshant English</a>, 
<a href="/search/stat?searchtype=author&query=Kirchler%2C+M">Matthias Kirchler</a>, 
<a href="/search/stat?searchtype=author&query=Lippert%2C+C">Christoph Lippert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Normalising flows are statistical models that transform a complex density
into a simpler density through the use of bijective transformations enabling
both density estimation and data generation from a single model. In the context
of image modelling, the predominant choice has been the Glow-based
architecture, whereas alternative architectures remain largely unexplored in
the research community. In this work, we propose a novel architecture called
MixerFlow, based on the MLP-Mixer architecture, further unifying the generative
and discriminative modelling architectures. MixerFlow offers an effective
mechanism for weight sharing for flow-based models. Our results demonstrate
better density estimation on image datasets under a fixed computational budget
and scales well as the image resolution increases, making MixeFlow a powerful
yet simple alternative to the Glow-based architectures. We also show that
MixerFlow provides more informative embeddings than Glow-based architectures.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16786" title="Abstract">arXiv:2310.16786</a> (cross-list from astro-ph.CO) [<a href="/pdf/2310.16786" title="Download PDF">pdf</a>, <a href="/format/2310.16786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Simplest Inflationary Potentials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Sousa%2C+T">Tom&#xe1;s Sousa</a>, 
<a href="/search/astro-ph?searchtype=author&query=Bartlett%2C+D+J">Deaglan J. Bartlett</a>, 
<a href="/search/astro-ph?searchtype=author&query=Desmond%2C+H">Harry Desmond</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ferreira%2C+P+G">Pedro G. Ferreira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13+4 pages, 4 figures; submitted to Physical Review D
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cosmology and Nongalactic Astrophysics (astro-ph.CO)</span>; Machine Learning (cs.LG); General Relativity and Quantum Cosmology (gr-qc); High Energy Physics - Phenomenology (hep-ph); High Energy Physics - Theory (hep-th)

</div>
<p class="mathjax">Inflation is a highly favoured theory for the early Universe. It is
compatible with current observations of the cosmic microwave background and
large scale structure and is a driver in the quest to detect primordial
gravitational waves. It is also, given the current quality of the data, highly
under-determined with a large number of candidate implementations. We use a new
method in symbolic regression to generate all possible simple scalar field
potentials for one of two possible basis sets of operators. Treating these as
single-field, slow-roll inflationary models we then score them with an
information-theoretic metric ("minimum description length") that quantifies
their efficiency in compressing the information in the Planck data. We explore
two possible priors on the parameter space of potentials, one related to the
functions' structural complexity and one that uses a Katz back-off language
model to prefer functions that may be theoretically motivated. This enables us
to identify the inflaton potentials that optimally balance simplicity with
accuracy at explaining the Planck data, which may subsequently find theoretical
motivation. Our exploratory study opens the door to extraction of fundamental
physics directly from data, and may be augmented with more refined theoretical
priors in the quest for a complete understanding of the early Universe.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16819" title="Abstract">arXiv:2310.16819</a> (cross-list from econ.EM) [<a href="/pdf/2310.16819" title="Download PDF">pdf</a>, <a href="/format/2310.16819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CATE Lasso: Conditional Average Treatment Effect Estimation with  High-Dimensional Linear Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Kato%2C+M">Masahiro Kato</a>, 
<a href="/search/econ?searchtype=author&query=Imaizumi%2C+M">Masaaki Imaizumi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Econometrics (econ.EM)</span>; Machine Learning (cs.LG); Applications (stat.AP); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">In causal inference about two treatments, Conditional Average Treatment
Effects (CATEs) play an important role as a quantity representing an
individualized causal effect, defined as a difference between the expected
outcomes of the two treatments conditioned on covariates. This study assumes
two linear regression models between a potential outcome and covariates of the
two treatments and defines CATEs as a difference between the linear regression
models. Then, we propose a method for consistently estimating CATEs even under
high-dimensional and non-sparse parameters. In our study, we demonstrate that
desirable theoretical properties, such as consistency, remain attainable even
without assuming sparsity explicitly if we assume a weaker assumption called
implicit sparsity originating from the definition of CATEs. In this assumption,
we suppose that parameters of linear models in potential outcomes can be
divided into treatment-specific and common parameters, where the
treatment-specific parameters take difference values between each linear
regression model, while the common parameters remain identical. Thus, in a
difference between two linear regression models, the common parameters
disappear, leaving only differences in the treatment-specific parameters.
Consequently, the non-zero parameters in CATEs correspond to the differences in
the treatment-specific parameters. Leveraging this assumption, we develop a
Lasso regression method specialized for CATE estimation and present that the
estimator is consistent. Finally, we confirm the soundness of the proposed
method by simulation studies.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16826" title="Abstract">arXiv:2310.16826</a> (cross-list from astro-ph.EP) [<a href="/pdf/2310.16826" title="Download PDF">pdf</a>, <a href="/format/2310.16826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep machine learning for meteor monitoring: advances with transfer  learning and gradient-weighted class activation mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Pe%C3%B1a-Asensio%2C+E">Eloy Pe&#xf1;a-Asensio</a>, 
<a href="/search/astro-ph?searchtype=author&query=Trigo-Rodr%C3%ADguez%2C+J+M">Josep M. Trigo-Rodr&#xed;guez</a>, 
<a href="/search/astro-ph?searchtype=author&query=Gr%C3%A8bol-Tom%C3%A0s%2C+P">Pau Gr&#xe8;bol-Tom&#xe0;s</a>, 
<a href="/search/astro-ph?searchtype=author&query=Regordosa-Avellana%2C+D">David Regordosa-Avellana</a>, 
<a href="/search/astro-ph?searchtype=author&query=Rimola%2C+A">Albert Rimola</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Planetary and Space Science
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Earth and Planetary Astrophysics (astro-ph.EP)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">In recent decades, the use of optical detection systems for meteor studies
has increased dramatically, resulting in huge amounts of data being analyzed.
Automated meteor detection tools are essential for studying the continuous
meteoroid incoming flux, recovering fresh meteorites, and achieving a better
understanding of our Solar System. Concerning meteor detection, distinguishing
false positives between meteor and non-meteor images has traditionally been
performed by hand, which is significantly time-consuming. To address this
issue, we developed a fully automated pipeline that uses Convolutional Neural
Networks (CNNs) to classify candidate meteor detections. Our new method is able
to detect meteors even in images that contain static elements such as clouds,
the Moon, and buildings. To accurately locate the meteor within each frame, we
employ the Gradient-weighted Class Activation Mapping (Grad-CAM) technique.
This method facilitates the identification of the region of interest by
multiplying the activations from the last convolutional layer with the average
of the gradients across the feature map of that layer. By combining these
findings with the activation map derived from the first convolutional layer, we
effectively pinpoint the most probable pixel location of the meteor. We trained
and evaluated our model on a large dataset collected by the Spanish Meteor
Network (SPMN) and achieved a precision of 98\%. Our new methodology presented
here has the potential to reduce the workload of meteor scientists and station
operators and improve the accuracy of meteor tracking and classification.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16834" title="Abstract">arXiv:2310.16834</a> (cross-list from stat.ML) [<a href="/pdf/2310.16834" title="Download PDF">pdf</a>, <a href="/format/2310.16834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discrete Diffusion Language Modeling by Estimating the Ratios of the  Data Distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lou%2C+A">Aaron Lou</a>, 
<a href="/search/stat?searchtype=author&query=Meng%2C+C">Chenlin Meng</a>, 
<a href="/search/stat?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite their groundbreaking performance for many generative modeling tasks,
diffusion models have fallen short on discrete data domains such as natural
language. Crucially, standard diffusion models rely on the well-established
theory of score matching, but efforts to generalize this to discrete structures
have not yielded the same empirical gains. In this work, we bridge this gap by
proposing score entropy, a novel discrete score matching loss that is more
stable than existing methods, forms an ELBO for maximum likelihood training,
and can be efficiently optimized with a denoising variant. We scale our Score
Entropy Discrete Diffusion models (SEDD) to the experimental setting of GPT-2,
achieving highly competitive likelihoods while also introducing distinct
algorithmic advantages. In particular, when comparing similarly sized SEDD and
GPT-2 models, SEDD attains comparable perplexities (normally within $+10\%$ of
and sometimes outperforming the baseline). Furthermore, SEDD models learn a
more faithful sequence distribution (around $4\times$ better compared to GPT-2
models with ancestral sampling as measured by large models), can trade off
compute for generation quality (needing only $16\times$ fewer network
evaluations to match GPT-2), and enables arbitrary infilling beyond the
standard left to right prompting.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Thu, 26 Oct 23</h3>
<dl>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1703.10146" title="Abstract">arXiv:1703.10146</a> (replaced) [<a href="/pdf/1703.10146" title="Download PDF">pdf</a>, <a href="/format/1703.10146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Community Detection and Stochastic Block Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Abbe%2C+E">Emmanuel Abbe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Computational Complexity (cs.CC); Information Theory (cs.IT); Social and Information Networks (cs.SI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1804.03967" title="Abstract">arXiv:1804.03967</a> (replaced) [<a href="/e-print/1804.03967" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incremental Predictive Process Monitoring: How to Deal with the  Variability of Real Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di+Francescomarino%2C+C">Chiara Di Francescomarino</a>, 
<a href="/search/cs?searchtype=author&query=Ghidini%2C+C">Chiara Ghidini</a>, 
<a href="/search/cs?searchtype=author&query=Maggi%2C+F+M">Fabrizio Maria Maggi</a>, 
<a href="/search/cs?searchtype=author&query=Rizzi%2C+W">Williams Rizzi</a>, 
<a href="/search/cs?searchtype=author&query=Persia%2C+C+D">Cosimo Damiano Persia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is replaced by paper <a href="/abs/2109.03501">arXiv:2109.03501</a> which containes a more recent version of this work which was not submitted as an update by mistake
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1903.02758" title="Abstract">arXiv:1903.02758</a> (replaced) [<a href="/pdf/1903.02758" title="Download PDF">pdf</a>, <a href="/format/1903.02758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A face cover perspective to $\ell_1$ embeddings of planar graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Filtser%2C+A">Arnold Filtser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1912.13490" title="Abstract">arXiv:1912.13490</a> (replaced) [<a href="/pdf/1912.13490" title="Download PDF">pdf</a>, <a href="/format/1912.13490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Neurocomputational Account of Consciousness: The Goal-Aligning  Representation Internal Manipulation Theory (GARIM)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baldassarre%2C+G">Gianluca Baldassarre</a>, 
<a href="/search/cs?searchtype=author&query=Granato%2C+G">Giovanni Granato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.16205" title="Abstract">arXiv:2006.16205</a> (replaced) [<a href="/pdf/2006.16205" title="Download PDF">pdf</a>, <a href="/format/2006.16205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for  Improved Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+S+M">Sang Michael Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Tengyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021 Long talk
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.16785" title="Abstract">arXiv:2006.16785</a> (replaced) [<a href="/pdf/2006.16785" title="Download PDF">pdf</a>, <a href="/format/2006.16785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial  Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blond%C3%A9%2C+L">Lionel Blond&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Strasser%2C+P">Pablo Strasser</a>, 
<a href="/search/cs?searchtype=author&query=Kalousis%2C+A">Alexandros Kalousis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Machine Learning 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.06134" title="Abstract">arXiv:2102.06134</a> (replaced) [<a href="/pdf/2102.06134" title="Download PDF">pdf</a>, <a href="/format/2102.06134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sweeps, polytopes, oriented matroids, and allowable graphs of  permutations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Padrol%2C+A">Arnau Padrol</a>, 
<a href="/search/math?searchtype=author&query=Philippe%2C+E">Eva Philippe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages, 14 figures, published version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Combinatorica (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Geometry (cs.CG); Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.10385" title="Abstract">arXiv:2103.10385</a> (replaced) [<a href="/pdf/2103.10385" title="Download PDF">pdf</a>, <a href="/format/2103.10385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT Understands, Too
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yanan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Z">Zhengxiao Du</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Ming Ding</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yujie Qian</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhilin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.03232" title="Abstract">arXiv:2106.03232</a> (replaced) [<a href="/pdf/2106.03232" title="Download PDF">pdf</a>, <a href="/format/2106.03232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Targeted Assessment of Incremental Processing in Neural LanguageModels  and Humans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wilcox%2C+E+G">Ethan Gotlieb Wilcox</a>, 
<a href="/search/cs?searchtype=author&query=Vani%2C+P">Pranali Vani</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+R+P">Roger P. Levy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in the proceedings of ACL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.11959" title="Abstract">arXiv:2106.11959</a> (replaced) [<a href="/pdf/2106.11959" title="Download PDF">pdf</a>, <a href="/format/2106.11959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Deep Learning Models for Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gorishniy%2C+Y">Yury Gorishniy</a>, 
<a href="/search/cs?searchtype=author&query=Rubachev%2C+I">Ivan Rubachev</a>, 
<a href="/search/cs?searchtype=author&query=Khrulkov%2C+V">Valentin Khrulkov</a>, 
<a href="/search/cs?searchtype=author&query=Babenko%2C+A">Artem Babenko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2021 camera-ready. Code: <a href="https://github.com/yandex-research/tabular-dl-revisiting-models">this https URL</a> (v4: minor update)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.08472" title="Abstract">arXiv:2107.08472</a> (replaced) [<a href="/pdf/2107.08472" title="Download PDF">pdf</a>, <a href="/format/2107.08472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A locally calculable $P^3$-pressure in a decoupled method for  incompressible Stokes equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Park%2C+C">Chunjae Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2104.05149">arXiv:2104.05149</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.03501" title="Abstract">arXiv:2109.03501</a> (replaced) [<a href="/pdf/2109.03501" title="Download PDF">pdf</a>, <a href="/format/2109.03501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How do I update my model? On the resilience of Predictive Process  Monitoring models to change
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rizzi%2C+W">Williams Rizzi</a>, 
<a href="/search/cs?searchtype=author&query=Di+Francescomarino%2C+C">Chiara Di Francescomarino</a>, 
<a href="/search/cs?searchtype=author&query=Ghidini%2C+C">Chiara Ghidini</a>, 
<a href="/search/cs?searchtype=author&query=Maggi%2C+F+M">Fabrizio Maria Maggi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Knowl. Inf. Syst. 64(5): 1385-1416 (2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.03427" title="Abstract">arXiv:2110.03427</a> (replaced) [<a href="/pdf/2110.03427" title="Download PDF">pdf</a>, <a href="/format/2110.03427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Attention always needed? A Case Study on Language Identification from  Speech
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mandal%2C+A">Atanu Mandal</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+S">Santanu Pal</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+I">Indranil Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+M">Mahidas Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Naskar%2C+S+K">Sudip Kumar Naskar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Natural Language Engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.03754" title="Abstract">arXiv:2110.03754</a> (replaced) [<a href="/pdf/2110.03754" title="Download PDF">pdf</a>, <a href="/format/2110.03754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Process Extraction from Text: Benchmarking the State of the Art and  Paving the Way for Future Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bellan%2C+P">Patrizio Bellan</a>, 
<a href="/search/cs?searchtype=author&query=Dragoni%2C+M">Mauro Dragoni</a>, 
<a href="/search/cs?searchtype=author&query=Ghidini%2C+C">Chiara Ghidini</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Aa%2C+H">Han van der Aa</a>, 
<a href="/search/cs?searchtype=author&query=Ponzetto%2C+S+P">Simone Paolo Ponzetto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.07058" title="Abstract">arXiv:2111.07058</a> (replaced) [<a href="/pdf/2111.07058" title="Download PDF">pdf</a>, <a href="/format/2111.07058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bolstering Stochastic Gradient Descent with Model Building
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Birbil%2C+S+I">S. Ilker Birbil</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+O">Ozgur Martin</a>, 
<a href="/search/cs?searchtype=author&query=Onay%2C+G">Gonenc Onay</a>, 
<a href="/search/cs?searchtype=author&query=Oztoprak%2C+F">Figen Oztoprak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.02814" title="Abstract">arXiv:2112.02814</a> (replaced) [<a href="/pdf/2112.02814" title="Download PDF">pdf</a>, <a href="/format/2112.02814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Deep Learning for Low-Shot Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qihan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haofei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+M">Mengqi Xue</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jie Song</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Mingli Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.12717" title="Abstract">arXiv:2112.12717</a> (replaced) [<a href="/pdf/2112.12717" title="Download PDF">pdf</a>, <a href="/format/2112.12717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forward Composition Propagation for Explainable Neural Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grau%2C+I">Isel Grau</a>, 
<a href="/search/cs?searchtype=author&query=N%C3%A1poles%2C+G">Gonzalo N&#xe1;poles</a>, 
<a href="/search/cs?searchtype=author&query=Bello%2C+M">Marilyn Bello</a>, 
<a href="/search/cs?searchtype=author&query=Salgueiro%2C+Y">Yamisleydi Salgueiro</a>, 
<a href="/search/cs?searchtype=author&query=Jastrzebska%2C+A">Agnieszka Jastrzebska</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.08022" title="Abstract">arXiv:2201.08022</a> (replaced) [<a href="/pdf/2201.08022" title="Download PDF">pdf</a>, <a href="/format/2201.08022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HEAM: High-Efficiency Approximate Multiplier Optimization for Deep  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Su Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jingbo Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jide Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lingli Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2022 IEEE International Symposium on Circuits and Systems (ISCAS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.12859" title="Abstract">arXiv:2201.12859</a> (replaced) [<a href="/pdf/2201.12859" title="Download PDF">pdf</a>, <a href="/ps/2201.12859" title="Download PostScript">ps</a>, <a href="/format/2201.12859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deciding Asynchronous Hyperproperties for Recursive Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gutsfeld%2C+J+O">Jens Oliver Gutsfeld</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller-Olm%2C+M">Markus M&#xfc;ller-Olm</a>, 
<a href="/search/cs?searchtype=author&query=Ohrem%2C+C">Christoph Ohrem</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.06877" title="Abstract">arXiv:2202.06877</a> (replaced) [<a href="/pdf/2202.06877" title="Download PDF">pdf</a>, <a href="/format/2202.06877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review of zk-SNARKs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Thomas Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Kunpittaya%2C+T">Teeramet Kunpittaya</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+A">Alan Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.13490" title="Abstract">arXiv:2202.13490</a> (replaced) [<a href="/pdf/2202.13490" title="Download PDF">pdf</a>, <a href="/ps/2202.13490" title="Download PostScript">ps</a>, <a href="/format/2202.13490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Limitations of Deep Learning for Inverse Problems on Digital Hardware
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boche%2C+H">Holger Boche</a>, 
<a href="/search/cs?searchtype=author&query=Fono%2C+A">Adalbert Fono</a>, 
<a href="/search/cs?searchtype=author&query=Kutyniok%2C+G">Gitta Kutyniok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in IEEE Transactions on Information Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.03782" title="Abstract">arXiv:2204.03782</a> (replaced) [<a href="/pdf/2204.03782" title="Download PDF">pdf</a>, <a href="/ps/2204.03782" title="Download PostScript">ps</a>, <a href="/format/2204.03782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Testing Positive Semidefiniteness Using Linear Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Needell%2C+D">Deanna Needell</a>, 
<a href="/search/cs?searchtype=author&query=Swartworth%2C+W">William Swartworth</a>, 
<a href="/search/cs?searchtype=author&query=Woodruff%2C+D+P">David P. Woodruff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.04109" title="Abstract">arXiv:2205.04109</a> (replaced) [<a href="/pdf/2205.04109" title="Download PDF">pdf</a>, <a href="/format/2205.04109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A coherent differential PCF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ehrhard%2C+T">Thomas Ehrhard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.05831" title="Abstract">arXiv:2205.05831</a> (replaced) [<a href="/pdf/2205.05831" title="Download PDF">pdf</a>, <a href="/format/2205.05831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature Extractor Stacking for Cross-domain Few-shot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Frank%2C+E">Eibe Frank</a>, 
<a href="/search/cs?searchtype=author&query=Pfahringer%2C+B">Bernhard Pfahringer</a>, 
<a href="/search/cs?searchtype=author&query=Mayo%2C+M">Michael Mayo</a>, 
<a href="/search/cs?searchtype=author&query=Holmes%2C+G">Geoffrey Holmes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.10908" title="Abstract">arXiv:2205.10908</a> (replaced) [<a href="/pdf/2205.10908" title="Download PDF">pdf</a>, <a href="/format/2205.10908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A note on the probabilistic stability of randomized Taylor schemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bochacik%2C+T">Tomasz Bochacik</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Electron. Trans. Numer. Anal. 58 (2023), 101-114
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.12787" title="Abstract">arXiv:2205.12787</a> (replaced) [<a href="/pdf/2205.12787" title="Download PDF">pdf</a>, <a href="/format/2205.12787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impartial Games: A Challenge for Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Riis%2C+S">S&#xf8;ren Riis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.15141" title="Abstract">arXiv:2205.15141</a> (replaced) [<a href="/pdf/2205.15141" title="Download PDF">pdf</a>, <a href="/format/2205.15141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theme Aspect Argumentation Model for Handling Fallacies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arisaka%2C+R">Ryuta Arisaka</a>, 
<a href="/search/cs?searchtype=author&query=Nakai%2C+R">Ryoma Nakai</a>, 
<a href="/search/cs?searchtype=author&query=Kawamoto%2C+Y">Yusuke Kawamoto</a>, 
<a href="/search/cs?searchtype=author&query=Ito%2C+T">Takayuki Ito</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.04883" title="Abstract">arXiv:2206.04883</a> (replaced) [<a href="/pdf/2206.04883" title="Download PDF">pdf</a>, <a href="/format/2206.04883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Complexity of Sampling Redistricting Plans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Charikar%2C+M">Moses Charikar</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Paul Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Vuong%2C+T">Thuy-Duong Vuong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Correcting the definition of Markov chain to sample from spanning tree distribution
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.07918" title="Abstract">arXiv:2206.07918</a> (replaced) [<a href="/pdf/2206.07918" title="Download PDF">pdf</a>, <a href="/format/2206.07918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Understanding Robustness Lottery&quot;: A Geometric Visual Comparative  Analysis of Neural Network Pruning Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhimin Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shusen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Bhavya%2C+K">Kailkhura Bhavya</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jie Cao</a>, 
<a href="/search/cs?searchtype=author&query=Daniel%2C+D+J">Diffenderfer James Daniel</a>, 
<a href="/search/cs?searchtype=author&query=Bremer%2C+P">Peer-Timo Bremer</a>, 
<a href="/search/cs?searchtype=author&query=Pascucci%2C+V">Valerio Pascucci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.12198" title="Abstract">arXiv:2206.12198</a> (replaced) [<a href="/pdf/2206.12198" title="Download PDF">pdf</a>, <a href="/format/2206.12198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Space-time reduced basis methods for parametrized unsteady Stokes  equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tenderini%2C+R">Riccardo Tenderini</a> (1), 
<a href="/search/math?searchtype=author&query=Mueller%2C+N">Nicholas Mueller</a> (2), 
<a href="/search/math?searchtype=author&query=Deparis%2C+S">Simone Deparis</a> (1) ((1) Institute of Mathematics, &#xc9;cole Polytechnique F&#xe9;d&#xe9;rale de Lausanne (EPFL), Lausanne, Switzerland, (2) Department of Mathematics, Monash University, Clayton, Victoria, Australia)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages (25 + 5 in appendix), 4 figures, 4 tables. To appear on SIAM Journal on Scientific Computing (SISC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.01933" title="Abstract">arXiv:2207.01933</a> (replaced) [<a href="/pdf/2207.01933" title="Download PDF">pdf</a>, <a href="/ps/2207.01933" title="Download PostScript">ps</a>, <a href="/format/2207.01933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence of a time discrete scheme for a chemotaxis-consumption model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Guill%C3%A9n-Gonz%C3%A1lez%2C+F">Francisco Guill&#xe9;n-Gonz&#xe1;lez</a>, 
<a href="/search/math?searchtype=author&query=Filho%2C+A+L+C+V">Andr&#xe9; Luiz Corr&#xea;a Vianna Filho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.03182" title="Abstract">arXiv:2207.03182</a> (replaced) [<a href="/pdf/2207.03182" title="Download PDF">pdf</a>, <a href="/format/2207.03182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chilled Sampling for Uncertainty Quantification: A Motivation From A  Meteorological Inverse Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=H%C3%A9as%2C+P">Patrick H&#xe9;as</a>, 
<a href="/search/stat?searchtype=author&query=C%C3%A9rou%2C+F">Fr&#xe9;d&#xe9;ric C&#xe9;rou</a>, 
<a href="/search/stat?searchtype=author&query=Rousset%2C+M">Mathias Rousset</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Computer Vision and Pattern Recognition (cs.CV); Probability (math.PR); Statistics Theory (math.ST); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.09785" title="Abstract">arXiv:2207.09785</a> (replaced) [<a href="/pdf/2207.09785" title="Download PDF">pdf</a>, <a href="/format/2207.09785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised energy disaggregation via convolutional sparse coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Aarset%2C+C">Christian Aarset</a> (1), 
<a href="/search/math?searchtype=author&query=Habring%2C+A">Andreas Habring</a> (1), 
<a href="/search/math?searchtype=author&query=Holler%2C+M">Martin Holler</a> (1), 
<a href="/search/math?searchtype=author&query=Mitter%2C+M">Mario Mitter</a> (2) ((1) University of Graz, (2) Solgenium OG)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.14776" title="Abstract">arXiv:2207.14776</a> (replaced) [<a href="/pdf/2207.14776" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-radiomics: A Collection of Standardized Datasets and a Technical  Protocol for Reproducible Radiomics Machine Learning Pipelines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Namdar%2C+K">Khashayar Namdar</a>, 
<a href="/search/q-bio?searchtype=author&query=Wagner%2C+M+W">Matthias W. Wagner</a>, 
<a href="/search/q-bio?searchtype=author&query=Ertl-Wagner%2C+B+B">Birgit B. Ertl-Wagner</a>, 
<a href="/search/q-bio?searchtype=author&query=Khalvati%2C+F">Farzad Khalvati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.01191" title="Abstract">arXiv:2208.01191</a> (replaced) [<a href="/pdf/2208.01191" title="Download PDF">pdf</a>, <a href="/format/2208.01191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Two-Tower Policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yunfan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Q">Qingkai Pan</a>, 
<a href="/search/cs?searchtype=author&query=Choromanski%2C+K">Krzysztof Choromanski</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+D">Deepali Jain</a>, 
<a href="/search/cs?searchtype=author&query=Sindhwani%2C+V">Vikas Sindhwani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.09140" title="Abstract">arXiv:2208.09140</a> (replaced) [<a href="/pdf/2208.09140" title="Download PDF">pdf</a>, <a href="/format/2208.09140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy Efficient Obfuscation of Side-Channel Leakage for Preventing  Side-Channel Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Shan Jin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minghua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yiwei Cai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.13092" title="Abstract">arXiv:2208.13092</a> (replaced) [<a href="/pdf/2208.13092" title="Download PDF">pdf</a>, <a href="/format/2208.13092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lottery Aware Sparsity Hunting: Enabling Federated Learning on  Resource-Limited Edge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Babakniya%2C+S">Sara Babakniya</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Souvik Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+S">Saurav Prakash</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Y">Yue Niu</a>, 
<a href="/search/cs?searchtype=author&query=Avestimehr%2C+S">Salman Avestimehr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in TMLR, <a href="https://openreview.net/forum?id=iHyhdpsnyi">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.14921" title="Abstract">arXiv:2208.14921</a> (replaced) [<a href="/pdf/2208.14921" title="Download PDF">pdf</a>, <a href="/format/2208.14921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A heuristic algorithm using tree decompositions for the maximum happy  vertices problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carpentier%2C+L">Louis Carpentier</a>, 
<a href="/search/cs?searchtype=author&query=Jooken%2C+J">Jorik Jooken</a>, 
<a href="/search/cs?searchtype=author&query=Goedgebeur%2C+J">Jan Goedgebeur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, to appear in Journal of Heuristics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.08673" title="Abstract">arXiv:2209.08673</a> (replaced) [<a href="/pdf/2209.08673" title="Download PDF">pdf</a>, <a href="/format/2209.08673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proofs of Proof-of-Stake with Sublinear Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+S">Shresth Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Neu%2C+J">Joachim Neu</a>, 
<a href="/search/cs?searchtype=author&query=Tas%2C+E+N">Ertem Nusret Tas</a>, 
<a href="/search/cs?searchtype=author&query=Zindros%2C+D">Dionysis Zindros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.09977" title="Abstract">arXiv:2209.09977</a> (replaced) [<a href="/pdf/2209.09977" title="Download PDF">pdf</a>, <a href="/format/2209.09977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Bilinear Models of Actuated Koopman Generators from  Partially-Observed Trajectories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Otto%2C+S+E">Samuel E. Otto</a>, 
<a href="/search/math?searchtype=author&query=Peitz%2C+S">Sebastian Peitz</a>, 
<a href="/search/math?searchtype=author&query=Rowley%2C+C+W">Clarence W. Rowley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Dynamical Systems (math.DS)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Signal Processing (eess.SP); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.15543" title="Abstract">arXiv:2209.15543</a> (replaced) [<a href="/pdf/2209.15543" title="Download PDF">pdf</a>, <a href="/format/2209.15543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Neural Networks for Geothermal Resource Assessment: Prediction  with Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Brown%2C+S">Stephen Brown</a>, 
<a href="/search/physics?searchtype=author&query=Rodi%2C+W+L">William L. Rodi</a>, 
<a href="/search/physics?searchtype=author&query=Seracini%2C+M">Marco Seracini</a>, 
<a href="/search/physics?searchtype=author&query=Gu%2C+C">Chen Gu</a>, 
<a href="/search/physics?searchtype=author&query=Fehler%2C+M">Michael Fehler</a>, 
<a href="/search/physics?searchtype=author&query=Faulds%2C+J">James Faulds</a>, 
<a href="/search/physics?searchtype=author&query=Smith%2C+C+M">Connor M. Smith</a>, 
<a href="/search/physics?searchtype=author&query=Treitel%2C+S">Sven Treitel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.02414" title="Abstract">arXiv:2210.02414</a> (replaced) [<a href="/pdf/2210.02414" title="Download PDF">pdf</a>, <a href="/format/2210.02414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GLM-130B: An Open Bilingual Pre-trained Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+A">Aohan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Z">Zhengxiao Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+H">Hanyu Lai</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Ming Ding</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhuoyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yifan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wendi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+X">Xiao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Tam%2C+W+L">Weng Lam Tam</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zixuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yufei Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+J">Jidong Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenguang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuxiao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.04979" title="Abstract">arXiv:2210.04979</a> (replaced) [<a href="/pdf/2210.04979" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label-free segmentation from cardiac ultrasound using self-supervised  learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ferreira%2C+D+L">Danielle L. Ferreira</a>, 
<a href="/search/eess?searchtype=author&query=Salaymang%2C+Z">Zaynaf Salaymang</a>, 
<a href="/search/eess?searchtype=author&query=Arnaout%2C+R">Rima Arnaout</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 3 Tables, 7 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.07287" title="Abstract">arXiv:2210.07287</a> (replaced) [<a href="/pdf/2210.07287" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Deep Learning Models for Pediatric Low-Grade Glioma Tumors  Molecular Subtype Identification Using 3D Probability Distributions of Tumor  Location
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Namdar%2C+K">Khashayar Namdar</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+M+W">Matthias W. Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Kudus%2C+K">Kareem Kudus</a>, 
<a href="/search/cs?searchtype=author&query=Hawkins%2C+C">Cynthia Hawkins</a>, 
<a href="/search/cs?searchtype=author&query=Tabori%2C+U">Uri Tabori</a>, 
<a href="/search/cs?searchtype=author&query=Ertl-Wagner%2C+B">Brigit Ertl-Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Khalvati%2C+F">Farzad Khalvati</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2207.14776">arXiv:2207.14776</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.08677" title="Abstract">arXiv:2210.08677</a> (replaced) [<a href="/pdf/2210.08677" title="Download PDF">pdf</a>, <a href="/format/2210.08677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regularized Data Programming with Automated Bayesian Prior Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maasch%2C+J+R+M+A">Jacqueline R. M. A. Maasch</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kuleshov%2C+V">Volodymyr Kuleshov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.12089" title="Abstract">arXiv:2210.12089</a> (replaced) [<a href="/pdf/2210.12089" title="Download PDF">pdf</a>, <a href="/format/2210.12089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Graph Counterfactual Explanations: Definitions, Methods,  Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prado-Romero%2C+M+A">Mario Alfonso Prado-Romero</a>, 
<a href="/search/cs?searchtype=author&query=Prenkaj%2C+B">Bardh Prenkaj</a>, 
<a href="/search/cs?searchtype=author&query=Stilo%2C+G">Giovanni Stilo</a>, 
<a href="/search/cs?searchtype=author&query=Giannotti%2C+F">Fosca Giannotti</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ACM Computing Surveys 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.04577" title="Abstract">arXiv:2211.04577</a> (replaced) [<a href="/pdf/2211.04577" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Political Divisiveness using Online Participation data  from the 2022 French and Brazilian Presidential Elections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Navarrete%2C+C">Carlos Navarrete</a>, 
<a href="/search/cs?searchtype=author&query=Macedo%2C+M">Mariana Macedo</a>, 
<a href="/search/cs?searchtype=author&query=Colley%2C+R">Rachael Colley</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingling Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ferrada%2C+N">Nicole Ferrada</a>, 
<a href="/search/cs?searchtype=author&query=Mello%2C+M+E">Maria Eduarda Mello</a>, 
<a href="/search/cs?searchtype=author&query=Lira%2C+R">Rodrigo Lira</a>, 
<a href="/search/cs?searchtype=author&query=Bastos-Filho%2C+C">Carmelo Bastos-Filho</a>, 
<a href="/search/cs?searchtype=author&query=Grandi%2C+U">Umberto Grandi</a>, 
<a href="/search/cs?searchtype=author&query=Lang%2C+J">Jerome Lang</a>, 
<a href="/search/cs?searchtype=author&query=Hidalgo%2C+C+A">C&#xe9;sar A. Hidalgo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages main manuscript with 5 figures. 55 pages of supplementary material
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.04921" title="Abstract">arXiv:2211.04921</a> (replaced) [<a href="/pdf/2211.04921" title="Download PDF">pdf</a>, <a href="/format/2211.04921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global, and Local Optimization Beamforming for Broadband Sources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goudarzi%2C+A">Armin Goudarzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to JASA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09378" title="Abstract">arXiv:2211.09378</a> (replaced) [<a href="/pdf/2211.09378" title="Download PDF">pdf</a>, <a href="/format/2211.09378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outracing Human Racers with Model-based Planning and Control for  Time-trial Racing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+C">Ce Hao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Chen Tang</a>, 
<a href="/search/cs?searchtype=author&query=Bergkvist%2C+E">Eric Bergkvist</a>, 
<a href="/search/cs?searchtype=author&query=Weaver%2C+C">Catherine Weaver</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Liting Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wei Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 13 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09717" title="Abstract">arXiv:2211.09717</a> (replaced) [<a href="/pdf/2211.09717" title="Download PDF">pdf</a>, <a href="/format/2211.09717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UPTON: Preventing Authorship Leakage from Public Text Release via Data  Poisoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Thai Le</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongwon Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.12421" title="Abstract">arXiv:2211.12421</a> (replaced) [<a href="/pdf/2211.12421" title="Download PDF">pdf</a>, <a href="/format/2211.12421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Network Neuroscience: On Data Collection and Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Xu%2C+J">Jiaxing Xu</a>, 
<a href="/search/q-bio?searchtype=author&query=Yang%2C+Y">Yunhan Yang</a>, 
<a href="/search/q-bio?searchtype=author&query=Huang%2C+D+T+J">David Tse Jung Huang</a>, 
<a href="/search/q-bio?searchtype=author&query=Gururajapathy%2C+S+S">Sophi Shilpa Gururajapathy</a>, 
<a href="/search/q-bio?searchtype=author&query=Ke%2C+Y">Yiping Ke</a>, 
<a href="/search/q-bio?searchtype=author&query=Qiao%2C+M">Miao Qiao</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+A">Alan Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Kumar%2C+H">Haribalan Kumar</a>, 
<a href="/search/q-bio?searchtype=author&query=McGeown%2C+J">Josh McGeown</a>, 
<a href="/search/q-bio?searchtype=author&query=Kwon%2C+E">Eryn Kwon</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Neural Information Processing Systems, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.01382" title="Abstract">arXiv:2212.01382</a> (replaced) [<a href="/pdf/2212.01382" title="Download PDF">pdf</a>, <a href="/format/2212.01382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Welfare and Fairness in Multi-objective Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zimeng Fan</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nianli Peng</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+M">Muhang Tian</a>, 
<a href="/search/cs?searchtype=author&query=Fain%2C+B">Brandon Fain</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.01473" title="Abstract">arXiv:2212.01473</a> (replaced) [<a href="/pdf/2212.01473" title="Download PDF">pdf</a>, <a href="/format/2212.01473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallelizing Maximal Clique Enumeration on GPUs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almasri%2C+M">Mohammad Almasri</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yen-Hsiang Chang</a>, 
<a href="/search/cs?searchtype=author&query=Hajj%2C+I+E">Izzat El Hajj</a>, 
<a href="/search/cs?searchtype=author&query=Nagi%2C+R">Rakesh Nagi</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jinjun Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Hwu%2C+W">Wen-mei Hwu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.04407" title="Abstract">arXiv:2212.04407</a> (replaced) [<a href="/pdf/2212.04407" title="Download PDF">pdf</a>, <a href="/format/2212.04407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Decision Frequency with Continuous Options
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karimi%2C+A">Amirmohammad Karimi</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+J">Jun Jin</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+A+R">A. Rupam Mahmood</a>, 
<a href="/search/cs?searchtype=author&query=Jagersand%2C+M">Martin Jagersand</a>, 
<a href="/search/cs?searchtype=author&query=Tosatto%2C+S">Samuele Tosatto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appears in the Proceedings of the 2023 International Conference on Intelligent Robots and Systems (IROS). Source code at <a href="https://github.com/amir-karimi96/continuous-time-continuous-option-policy-gradient.git">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.05946" title="Abstract">arXiv:2212.05946</a> (replaced) [<a href="/pdf/2212.05946" title="Download PDF">pdf</a>, <a href="/format/2212.05946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation and Improvement of Interpretability for Self-Explainable  Part-Prototype Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qihan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+M">Mengqi Xue</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenqi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haofei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jie Song</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+Y">Yongcheng Jing</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Mingli Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.06361" title="Abstract">arXiv:2212.06361</a> (replaced) [<a href="/pdf/2212.06361" title="Download PDF">pdf</a>, <a href="/ps/2212.06361" title="Download PostScript">ps</a>, <a href="/format/2212.06361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical Stability of DeepGOPlus Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pepe%2C+I+G">In&#xe9;s Gonzalez Pepe</a>, 
<a href="/search/cs?searchtype=author&query=Chatelain%2C+Y">Yohan Chatelain</a>, 
<a href="/search/cs?searchtype=author&query=Kiar%2C+G">Gregory Kiar</a>, 
<a href="/search/cs?searchtype=author&query=Glatard%2C+T">Tristan Glatard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 3 figures, 4 tables and 3 figures in Appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09825" title="Abstract">arXiv:2212.09825</a> (replaced) [<a href="/pdf/2212.09825" title="Download PDF">pdf</a>, <a href="/format/2212.09825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What to Read in a Contract? Party-Specific Summarization of Legal  Obligations, Entitlements, and Prohibitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sancheti%2C+A">Abhilasha Sancheti</a>, 
<a href="/search/cs?searchtype=author&query=Garimella%2C+A">Aparna Garimella</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+B+V">Balaji Vasan Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Rudinger%2C+R">Rachel Rudinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09912" title="Abstract">arXiv:2212.09912</a> (replaced) [<a href="/pdf/2212.09912" title="Download PDF">pdf</a>, <a href="/format/2212.09912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tokenization Consistency Matters for Generative Models on Extractive NLP  Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Kaiser Sun</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+P">Peng Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhiheng Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10526" title="Abstract">arXiv:2212.10526</a> (replaced) [<a href="/pdf/2212.10526" title="Download PDF">pdf</a>, <a href="/format/2212.10526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Domain Multi-document Summarization: A Comprehensive Study of Model  Brittleness under Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giorgi%2C+J">John Giorgi</a>, 
<a href="/search/cs?searchtype=author&query=Soldaini%2C+L">Luca Soldaini</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bader%2C+G">Gary Bader</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+K">Kyle Lo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L+L">Lucy Lu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10755" title="Abstract">arXiv:2212.10755</a> (replaced) [<a href="/pdf/2212.10755" title="Download PDF">pdf</a>, <a href="/format/2212.10755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JASMINE: Arabic GPT Models for Few-Shot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nagoudi%2C+E+M+B">El Moatez Billah Nagoudi</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>, 
<a href="/search/cs?searchtype=author&query=Elmadany%2C+A">AbdelRahim Elmadany</a>, 
<a href="/search/cs?searchtype=author&query=Inciarte%2C+A+A">Alcides Alcoba Inciarte</a>, 
<a href="/search/cs?searchtype=author&query=Khondaker%2C+M+T+I">Md Tawkat Islam Khondaker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.11719" title="Abstract">arXiv:2212.11719</a> (replaced) [<a href="/pdf/2212.11719" title="Download PDF">pdf</a>, <a href="/ps/2212.11719" title="Download PostScript">ps</a>, <a href="/format/2212.11719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Markov Categories and Entropy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perrone%2C+P">Paolo Perrone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 54 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Logic in Computer Science (cs.LO); Category Theory (math.CT); Probability (math.PR); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02432" title="Abstract">arXiv:2301.02432</a> (replaced) [<a href="/pdf/2301.02432" title="Download PDF">pdf</a>, <a href="/format/2301.02432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Myths and Legends in High-Performance Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Matsuoka%2C+S">Satoshi Matsuoka</a>, 
<a href="/search/cs?searchtype=author&query=Domke%2C+J">Jens Domke</a>, 
<a href="/search/cs?searchtype=author&query=Wahib%2C+M">Mohamed Wahib</a>, 
<a href="/search/cs?searchtype=author&query=Drozd%2C+A">Aleksandr Drozd</a>, 
<a href="/search/cs?searchtype=author&query=Hoefler%2C+T">Torsten Hoefler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Hardware Architecture (cs.AR); Computers and Society (cs.CY); Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.06796" title="Abstract">arXiv:2301.06796</a> (replaced) [<a href="/pdf/2301.06796" title="Download PDF">pdf</a>, <a href="/format/2301.06796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Database Matching Under Noisy Synchronization Errors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bakirtas%2C+S">Serhat Bakirtas</a>, 
<a href="/search/cs?searchtype=author&query=Erkip%2C+E">Elza Erkip</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.07313" title="Abstract">arXiv:2301.07313</a> (replaced) [<a href="/pdf/2301.07313" title="Download PDF">pdf</a>, <a href="/format/2301.07313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Black-box Checking of Snapshot Isolation in Databases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaile Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Si Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenge Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hengfeng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Basin%2C+D">David Basin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haixiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+A">Anqun Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 15 figures, accepted by PVLDB
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.08110" title="Abstract">arXiv:2301.08110</a> (replaced) [<a href="/pdf/2301.08110" title="Download PDF">pdf</a>, <a href="/format/2301.08110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AtMan: Understanding Transformer Predictions Through Memory Efficient  Attention Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deiseroth%2C+B">Bj&#xf6;rn Deiseroth</a>, 
<a href="/search/cs?searchtype=author&query=Deb%2C+M">Mayukh Deb</a>, 
<a href="/search/cs?searchtype=author&query=Weinbach%2C+S">Samuel Weinbach</a>, 
<a href="/search/cs?searchtype=author&query=Brack%2C+M">Manuel Brack</a>, 
<a href="/search/cs?searchtype=author&query=Schramowski%2C+P">Patrick Schramowski</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.08730" title="Abstract">arXiv:2301.08730</a> (replaced) [<a href="/pdf/2301.08730" title="Download PDF">pdf</a>, <a href="/format/2301.08730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novel-View Acoustic Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Changan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Richard%2C+A">Alexander Richard</a>, 
<a href="/search/cs?searchtype=author&query=Shapovalov%2C+R">Roman Shapovalov</a>, 
<a href="/search/cs?searchtype=author&query=Ithapu%2C+V+K">Vamsi Krishna Ithapu</a>, 
<a href="/search/cs?searchtype=author&query=Neverova%2C+N">Natalia Neverova</a>, 
<a href="/search/cs?searchtype=author&query=Grauman%2C+K">Kristen Grauman</a>, 
<a href="/search/cs?searchtype=author&query=Vedaldi%2C+A">Andrea Vedaldi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CVPR 2023. Project page: <a href="https://vision.cs.utexas.edu/projects/nvas">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11476" title="Abstract">arXiv:2301.11476</a> (replaced) [<a href="/pdf/2301.11476" title="Download PDF">pdf</a>, <a href="/format/2301.11476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Munchausen Reinforcement Learning using Tsallis KL  Divergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lingwei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Schlegel%2C+M">Matthew Schlegel</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+M">Martha White</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12609" title="Abstract">arXiv:2301.12609</a> (replaced) [<a href="/pdf/2301.12609" title="Download PDF">pdf</a>, <a href="/format/2301.12609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Distillation $\approx$ Label Smoothing: Fact or Fallacy?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sultan%2C+M+A">Md Arafat Sultan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00288" title="Abstract">arXiv:2302.00288</a> (replaced) [<a href="/pdf/2302.00288" title="Download PDF">pdf</a>, <a href="/format/2302.00288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoderEval: A Benchmark of Pragmatic Code Generation with Generative  Pre-trained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+B">Bo Shen</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+D">Dezhi Ran</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuchi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+G">Guangtai Liang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Ying Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tao Xie</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICSE (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00441" title="Abstract">arXiv:2302.00441</a> (replaced) [<a href="/pdf/2302.00441" title="Download PDF">pdf</a>, <a href="/format/2302.00441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Laws for Hyperparameter Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kadra%2C+A">Arlind Kadra</a>, 
<a href="/search/cs?searchtype=author&query=Janowski%2C+M">Maciej Janowski</a>, 
<a href="/search/cs?searchtype=author&query=Wistuba%2C+M">Martin Wistuba</a>, 
<a href="/search/cs?searchtype=author&query=Grabocka%2C+J">Josif Grabocka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02285" title="Abstract">arXiv:2302.02285</a> (replaced) [<a href="/pdf/2302.02285" title="Download PDF">pdf</a>, <a href="/format/2302.02285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReDi: Efficient Learning-Free Diffusion Inference via Trajectory  Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kexun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xianjun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08982" title="Abstract">arXiv:2302.08982</a> (replaced) [<a href="/pdf/2302.08982" title="Download PDF">pdf</a>, <a href="/format/2302.08982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> (S)GD over Diagonal Linear Networks: Implicit Regularisation, Large  Stepsizes and Edge of Stability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Even%2C+M">Mathieu Even</a>, 
<a href="/search/cs?searchtype=author&query=Pesme%2C+S">Scott Pesme</a>, 
<a href="/search/cs?searchtype=author&query=Gunasekar%2C+S">Suriya Gunasekar</a>, 
<a href="/search/cs?searchtype=author&query=Flammarion%2C+N">Nicolas Flammarion</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09872" title="Abstract">arXiv:2302.09872</a> (replaced) [<a href="/pdf/2302.09872" title="Download PDF">pdf</a>, <a href="/ps/2302.09872" title="Download PostScript">ps</a>, <a href="/format/2302.09872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel dual-decomposition method for non-convex mixed integer  quadratically constrained quadratic problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Belyak%2C+N">Nikita Belyak</a>, 
<a href="/search/math?searchtype=author&query=Oliveira%2C+F">Fabricio Oliveira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09933" title="Abstract">arXiv:2302.09933</a> (replaced) [<a href="/pdf/2302.09933" title="Download PDF">pdf</a>, <a href="/ps/2302.09933" title="Download PostScript">ps</a>, <a href="/format/2302.09933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mysterious and Manipulative Black Boxes: A Qualitative Analysis of  Perceptions on Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruohonen%2C+J">Jukka Ruohonen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A Working Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13600" title="Abstract">arXiv:2302.13600</a> (replaced) [<a href="/pdf/2302.13600" title="Download PDF">pdf</a>, <a href="/ps/2302.13600" title="Download PostScript">ps</a>, <a href="/format/2302.13600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-place fast polynomial modular remainder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dumas%2C+J">Jean-Guillaume Dumas</a> (CASC), 
<a href="/search/cs?searchtype=author&query=Grenet%2C+B">Bruno Grenet</a> (CASC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13875" title="Abstract">arXiv:2302.13875</a> (replaced) [<a href="/pdf/2302.13875" title="Download PDF">pdf</a>, <a href="/format/2302.13875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Robustness and Uncertainty of Graph Models Under Structural  Distributional Shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bazhenov%2C+G">Gleb Bazhenov</a>, 
<a href="/search/cs?searchtype=author&query=Kuznedelev%2C+D">Denis Kuznedelev</a>, 
<a href="/search/cs?searchtype=author&query=Malinin%2C+A">Andrey Malinin</a>, 
<a href="/search/cs?searchtype=author&query=Babenko%2C+A">Artem Babenko</a>, 
<a href="/search/cs?searchtype=author&query=Prokhorenkova%2C+L">Liudmila Prokhorenkova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14233" title="Abstract">arXiv:2302.14233</a> (replaced) [<a href="/pdf/2302.14233" title="Download PDF">pdf</a>, <a href="/format/2302.14233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Goal Driven Discovery of Distributional Differences via Language  Descriptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+R">Ruiqi Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peter Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Steve Li</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+J">Jinwoo Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+D">Dan Klein</a>, 
<a href="/search/cs?searchtype=author&query=Steinhardt%2C+J">Jacob Steinhardt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03944" title="Abstract">arXiv:2303.03944</a> (replaced) [<a href="/pdf/2303.03944" title="Download PDF">pdf</a>, <a href="/format/2303.03944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Momentum-Based Gradient Methods for Bilevel Optimization with  Nonconvex Lower-Level
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huang%2C+F">Feihu Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In new version of our paper, we relaxed some assumptions, updated our algorithms and added some numerical experiments
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06524" title="Abstract">arXiv:2303.06524</a> (replaced) [<a href="/pdf/2303.06524" title="Download PDF">pdf</a>, <a href="/ps/2303.06524" title="Download PostScript">ps</a>, <a href="/format/2303.06524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Record-Breaking Condorcet Domains on 10 and 11 Alternatives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Riis%2C+S">S&#xf8;ren Riis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07972" title="Abstract">arXiv:2303.07972</a> (replaced) [<a href="/pdf/2303.07972" title="Download PDF">pdf</a>, <a href="/format/2303.07972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GoNet: An Approach-Constrained Generative Grasp Sampling Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weng%2C+Z">Zehang Weng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Haofei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Lundell%2C+J">Jens Lundell</a>, 
<a href="/search/cs?searchtype=author&query=Kragic%2C+D">Danica Kragic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE-RAS International Conference on Humanoid Robots (Humanoids 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09863" title="Abstract">arXiv:2303.09863</a> (replaced) [<a href="/pdf/2303.09863" title="Download PDF">pdf</a>, <a href="/format/2303.09863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Nonparametric Estimation of Intrinsic Data Structures by Chart  Autoencoders: Generalization Error and Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/stat?searchtype=author&query=Havrilla%2C+A">Alex Havrilla</a>, 
<a href="/search/stat?searchtype=author&query=Lai%2C+R">Rongjie Lai</a>, 
<a href="/search/stat?searchtype=author&query=Liao%2C+W">Wenjing Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15090" title="Abstract">arXiv:2303.15090</a> (replaced) [<a href="/pdf/2303.15090" title="Download PDF">pdf</a>, <a href="/ps/2303.15090" title="Download PostScript">ps</a>, <a href="/format/2303.15090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A simplified lower bound for implicational logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Je%C5%99%C3%A1bek%2C+E">Emil Je&#x159;&#xe1;bek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages; switched to Colouring--Cocolouring tautologies
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15556" title="Abstract">arXiv:2303.15556</a> (replaced) [<a href="/pdf/2303.15556" title="Download PDF">pdf</a>, <a href="/format/2303.15556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity of Reconfiguration in Surface Chemical Reaction Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alaniz%2C+R+M">Robert M. Alaniz</a>, 
<a href="/search/cs?searchtype=author&query=Brunner%2C+J">Josh Brunner</a>, 
<a href="/search/cs?searchtype=author&query=Coulombe%2C+M">Michael Coulombe</a>, 
<a href="/search/cs?searchtype=author&query=Demaine%2C+E+D">Erik D. Demaine</a>, 
<a href="/search/cs?searchtype=author&query=Diomidov%2C+J">Jenny Diomidov</a>, 
<a href="/search/cs?searchtype=author&query=Knobel%2C+R">Ryan Knobel</a>, 
<a href="/search/cs?searchtype=author&query=Gomez%2C+T">Timothy Gomez</a>, 
<a href="/search/cs?searchtype=author&query=Grizzell%2C+E">Elise Grizzell</a>, 
<a href="/search/cs?searchtype=author&query=Lynch%2C+J">Jayson Lynch</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+A">Andrew Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Schweller%2C+R">Robert Schweller</a>, 
<a href="/search/cs?searchtype=author&query=Wylie%2C+T">Tim Wylie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16774" title="Abstract">arXiv:2303.16774</a> (replaced) [<a href="/pdf/2303.16774" title="Download PDF">pdf</a>, <a href="/format/2303.16774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polarization and multiscale structural balance in signed networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Talaga%2C+S">Szymon Talaga</a>, 
<a href="/search/physics?searchtype=author&query=Stella%2C+M">Massimo Stella</a>, 
<a href="/search/physics?searchtype=author&query=Swanson%2C+T+J">Trevor James Swanson</a>, 
<a href="/search/physics?searchtype=author&query=Teixeira%2C+A+S">Andreia Sofia Teixeira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages; 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.18128" title="Abstract">arXiv:2303.18128</a> (replaced) [<a href="/pdf/2303.18128" title="Download PDF">pdf</a>, <a href="/format/2303.18128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Age of Incorrect Information With Hybrid ARQ Under a Resource Constraint  for N-ary Symmetric Markov Sources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bountrogiannis%2C+K">Konstantinos Bountrogiannis</a>, 
<a href="/search/cs?searchtype=author&query=Ephremides%2C+A">Anthony Ephremides</a>, 
<a href="/search/cs?searchtype=author&query=Tsakalides%2C+P">Panagiotis Tsakalides</a>, 
<a href="/search/cs?searchtype=author&query=Tzagkarakis%2C+G">George Tzagkarakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 12 figures. This work has been submitted to the IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00083" title="Abstract">arXiv:2304.00083</a> (replaced) [<a href="/pdf/2304.00083" title="Download PDF">pdf</a>, <a href="/format/2304.00083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Generative Framework for Low-Cost Result Validation of Outsourced  Machine Learning Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Abhinav Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Aguilera%2C+M+A+G">Miguel A. Guirao Aguilera</a>, 
<a href="/search/cs?searchtype=author&query=Tourani%2C+R">Reza Tourani</a>, 
<a href="/search/cs?searchtype=author&query=Misra%2C+S">Satyajayant Misra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00170" title="Abstract">arXiv:2304.00170</a> (replaced) [<a href="/pdf/2304.00170" title="Download PDF">pdf</a>, <a href="/format/2304.00170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fixation probability in evolutionary dynamics on switching temporal  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Bhaumik%2C+J">Jnanajyoti Bhaumik</a>, 
<a href="/search/physics?searchtype=author&query=Masuda%2C+N">Naoki Masuda</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J. Math. Biol. 87, 64 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI); Probability (math.PR); Populations and Evolution (q-bio.PE)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00472" title="Abstract">arXiv:2304.00472</a> (replaced) [<a href="/pdf/2304.00472" title="Download PDF">pdf</a>, <a href="/format/2304.00472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Querying Large Language Models with SQL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saeed%2C+M">Mohammed Saeed</a>, 
<a href="/search/cs?searchtype=author&query=De+Cao%2C+N">Nicola De Cao</a>, 
<a href="/search/cs?searchtype=author&query=Papotti%2C+P">Paolo Papotti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at EDBT 2024 as Vision paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00488" title="Abstract">arXiv:2304.00488</a> (replaced) [<a href="/pdf/2304.00488" title="Download PDF">pdf</a>, <a href="/format/2304.00488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Saddle-to-Saddle Dynamics in Diagonal Linear Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pesme%2C+S">Scott Pesme</a>, 
<a href="/search/cs?searchtype=author&query=Flammarion%2C+N">Nicolas Flammarion</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03271" title="Abstract">arXiv:2304.03271</a> (replaced) [<a href="/pdf/2304.03271" title="Download PDF">pdf</a>, <a href="/format/2304.03271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making AI Less &quot;Thirsty&quot;: Uncovering and Addressing the Secret Water  Footprint of AI Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengfei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+A">Mohammad A. Islam</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shaolei Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> New updates include discussion on water withdrawal and water consumption, scope definition for water, and new estimates of GPT-3's water footprint based on Microsoft's new WUE and PUE data. Source codes available at: <a href="https://github.com/Ren-Research/Making-AI-Less-Thirsty">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03510" title="Abstract">arXiv:2304.03510</a> (replaced) [<a href="/pdf/2304.03510" title="Download PDF">pdf</a>, <a href="/format/2304.03510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multispectral Imaging for Differential Face Morphing Attack Detection: A  Preliminary Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramachandra%2C+R">Raghavendra Ramachandra</a>, 
<a href="/search/cs?searchtype=author&query=Venkatesh%2C+S">Sushma Venkatesh</a>, 
<a href="/search/cs?searchtype=author&query=Damer%2C+N">Naser Damer</a>, 
<a href="/search/cs?searchtype=author&query=Vetrekar%2C+N">Narayan Vetrekar</a>, 
<a href="/search/cs?searchtype=author&query=Gad%2C+R">Rajendra Gad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07129" title="Abstract">arXiv:2304.07129</a> (replaced) [<a href="/pdf/2304.07129" title="Download PDF">pdf</a>, <a href="/format/2304.07129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncoordinated Interference Avoidance Between Terrestrial and  Non-Terrestrial Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mismar%2C+F+B">Faris B. Mismar</a>, 
<a href="/search/cs?searchtype=author&query=Kaya%2C+A+O">Aliye Ozge Kaya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, submitted to IEEE International Conference on Communications 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07485" title="Abstract">arXiv:2304.07485</a> (replaced) [<a href="/pdf/2304.07485" title="Download PDF">pdf</a>, <a href="/format/2304.07485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Critical Sampling for Robust Evolution Operator Learning of Unknown  Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Ce Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kailiang Wu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhihai He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Artificial Intelligence (IEEE TAI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08244" title="Abstract">arXiv:2304.08244</a> (replaced) [<a href="/pdf/2304.08244" title="Download PDF">pdf</a>, <a href="/format/2304.08244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Minghao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yingxiu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bowen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+F">Feifan Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hangyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haiyang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhoujun Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08965" title="Abstract">arXiv:2304.08965</a> (replaced) [<a href="/pdf/2304.08965" title="Download PDF">pdf</a>, <a href="/format/2304.08965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PointDC:Unsupervised Semantic Segmentation of 3D Point Clouds via  Cross-modal Distillation and Super-Voxel Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zisheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongbin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weitao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhipeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+H">Haihong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Baigui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xuansong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+W">Wenxiong Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by International Conference on Computer Vision (ICCV) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09097" title="Abstract">arXiv:2304.09097</a> (replaced) [<a href="/pdf/2304.09097" title="Download PDF">pdf</a>, <a href="/format/2304.09097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sheaf Neural Networks for Graph-based Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Purificato%2C+A">Antonio Purificato</a>, 
<a href="/search/cs?searchtype=author&query=Cassar%C3%A0%2C+G">Giulia Cassar&#xe0;</a>, 
<a href="/search/cs?searchtype=author&query=Li%C3%B2%2C+P">Pietro Li&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Silvestri%2C+F">Fabrizio Silvestri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09576" title="Abstract">arXiv:2304.09576</a> (replaced) [<a href="/pdf/2304.09576" title="Download PDF">pdf</a>, <a href="/format/2304.09576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging the two timescale regime to demonstrate convergence of neural  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Marion%2C+P">Pierre Marion</a>, 
<a href="/search/math?searchtype=author&query=Berthier%2C+R">Rapha&#xeb;l Berthier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. 34 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14232" title="Abstract">arXiv:2304.14232</a> (replaced) [<a href="/pdf/2304.14232" title="Download PDF">pdf</a>, <a href="/format/2304.14232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design of Reconfigurable Intelligent Surfaces for Wireless  Communication: A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xiong%2C+R">Rujing Xiong</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+J">Jianan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+F">Fuhai Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zhengyu Wang</a>, 
<a href="/search/eess?searchtype=author&query=Ren%2C+X">Xiang Ren</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Junshuo Liu</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+J">Jialong Lu</a>, 
<a href="/search/eess?searchtype=author&query=Wan%2C+K">Kai Wan</a>, 
<a href="/search/eess?searchtype=author&query=Mi%2C+T">Tiebin Mi</a>, 
<a href="/search/eess?searchtype=author&query=Qiu%2C+R+C">Robert Caiming Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00467" title="Abstract">arXiv:2305.00467</a> (replaced) [<a href="/pdf/2305.00467" title="Download PDF">pdf</a>, <a href="/ps/2305.00467" title="Download PostScript">ps</a>, <a href="/format/2305.00467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The iteration time and the general position number in graph convexities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Araujo%2C+J">Julio Araujo</a>, 
<a href="/search/cs?searchtype=author&query=Dourado%2C+M+C">Mitre C. Dourado</a>, 
<a href="/search/cs?searchtype=author&query=Protti%2C+F">F&#xe1;bio Protti</a>, 
<a href="/search/cs?searchtype=author&query=Sampaio%2C+R">Rudini Sampaio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01094" title="Abstract">arXiv:2305.01094</a> (replaced) [<a href="/pdf/2305.01094" title="Download PDF">pdf</a>, <a href="/format/2305.01094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performative Prediction with Bandit Feedback: Learning through  Reparameterization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yatong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Wei Tang</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+C">Chien-Ju Ho</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02793" title="Abstract">arXiv:2305.02793</a> (replaced) [<a href="/pdf/2305.02793" title="Download PDF">pdf</a>, <a href="/ps/2305.02793" title="Download PostScript">ps</a>, <a href="/format/2305.02793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symbolic Solution of Emerson-Lei Games for Reactive Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hausmann%2C+D">Daniel Hausmann</a>, 
<a href="/search/cs?searchtype=author&query=Lehaut%2C+M">Mathieu Lehaut</a>, 
<a href="/search/cs?searchtype=author&query=Pitermann%2C+N">Nir Pitermann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02906" title="Abstract">arXiv:2305.02906</a> (replaced) [<a href="/pdf/2305.02906" title="Download PDF">pdf</a>, <a href="/format/2305.02906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optics for Premonoidal Categories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hefford%2C+J">James Hefford</a>, 
<a href="/search/math?searchtype=author&query=Rom%C3%A1n%2C+M">Mario Rom&#xe1;n</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Category Theory (math.CT)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04423" title="Abstract">arXiv:2305.04423</a> (replaced) [<a href="/pdf/2305.04423" title="Download PDF">pdf</a>, <a href="/ps/2305.04423" title="Download PostScript">ps</a>, <a href="/format/2305.04423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Power Allocation for UAV-aided ISAC Systems with Uncertain  Location Sensing Errors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sun%2C+J">Junchang Sun</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+S">Shuai Ma</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+R">Ruixin Yang</a>, 
<a href="/search/eess?searchtype=author&query=Tingting%2C+Y">Yang Tingting</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shiyin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04574" title="Abstract">arXiv:2305.04574</a> (replaced) [<a href="/pdf/2305.04574" title="Download PDF">pdf</a>, <a href="/format/2305.04574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TAPS: Connecting Certified and Adversarial Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yuhao Mao</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+M+N">Mark Niklas M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+M">Marc Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Vechev%2C+M">Martin Vechev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeuIPS'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04966" title="Abstract">arXiv:2305.04966</a> (replaced) [<a href="/pdf/2305.04966" title="Download PDF">pdf</a>, <a href="/format/2305.04966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NerfAcc: Efficient Sampling Accelerates NeRFs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ruilong Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Tancik%2C+M">Matthew Tancik</a>, 
<a href="/search/cs?searchtype=author&query=Kanazawa%2C+A">Angjoo Kanazawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website: <a href="https://www.nerfacc.com">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05411" title="Abstract">arXiv:2305.05411</a> (replaced) [<a href="/pdf/2305.05411" title="Download PDF">pdf</a>, <a href="/ps/2305.05411" title="Download PostScript">ps</a>, <a href="/format/2305.05411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 4/3-Approximation of Graphic TSP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%87ivril%2C+A">Ali &#xc7;ivril</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, drastically simplified the algorithm and the analysis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05588" title="Abstract">arXiv:2305.05588</a> (replaced) [<a href="/pdf/2305.05588" title="Download PDF">pdf</a>, <a href="/format/2305.05588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Opper%2C+M">Mattia Opper</a>, 
<a href="/search/cs?searchtype=author&query=Prokhorov%2C+V">Victor Prokhorov</a>, 
<a href="/search/cs?searchtype=author&query=Siddharth%2C+N">N. Siddharth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06343" title="Abstract">arXiv:2305.06343</a> (replaced) [<a href="/pdf/2305.06343" title="Download PDF">pdf</a>, <a href="/format/2305.06343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incorporating Structured Representations into Pretrained Vision &amp;  Language Models Using Scene Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Herzig%2C+R">Roei Herzig</a>, 
<a href="/search/cs?searchtype=author&query=Mendelson%2C+A">Alon Mendelson</a>, 
<a href="/search/cs?searchtype=author&query=Karlinsky%2C+L">Leonid Karlinsky</a>, 
<a href="/search/cs?searchtype=author&query=Arbelle%2C+A">Assaf Arbelle</a>, 
<a href="/search/cs?searchtype=author&query=Feris%2C+R">Rogerio Feris</a>, 
<a href="/search/cs?searchtype=author&query=Darrell%2C+T">Trevor Darrell</a>, 
<a href="/search/cs?searchtype=author&query=Globerson%2C+A">Amir Globerson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10225" title="Abstract">arXiv:2305.10225</a> (replaced) [<a href="/pdf/2305.10225" title="Download PDF">pdf</a>, <a href="/format/2305.10225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New and improved bounds on the contextuality degree of multi-qubit  configurations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Muller%2C+A">Axel Muller</a>, 
<a href="/search/quant-ph?searchtype=author&query=Saniga%2C+M">Metod Saniga</a>, 
<a href="/search/quant-ph?searchtype=author&query=Giorgetti%2C+A">Alain Giorgetti</a>, 
<a href="/search/quant-ph?searchtype=author&query=de+Boutray%2C+H">Henri de Boutray</a>, 
<a href="/search/quant-ph?searchtype=author&query=Holweck%2C+F">Fr&#xe9;d&#xe9;ric Holweck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 5 figures, 2 tables, submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Discrete Mathematics (cs.DM); Symplectic Geometry (math.SG)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10307" title="Abstract">arXiv:2305.10307</a> (replaced) [<a href="/pdf/2305.10307" title="Download PDF">pdf</a>, <a href="/format/2305.10307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FACE: Evaluating Natural Language Generation with Fourier Analysis of  Cross-Entropy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zuhao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yingfang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+S">Shuo Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+H">Huajun Bai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kefan Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10429" title="Abstract">arXiv:2305.10429</a> (replaced) [<a href="/pdf/2305.10429" title="Download PDF">pdf</a>, <a href="/format/2305.10429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+S+M">Sang Michael Xie</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+H">Hieu Pham</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xuanyi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+N">Nan Du</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hanxiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yifeng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+Q+V">Quoc V. Le</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Tengyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+A+W">Adams Wei Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11430" title="Abstract">arXiv:2305.11430</a> (replaced) [<a href="/pdf/2305.11430" title="Download PDF">pdf</a>, <a href="/format/2305.11430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santu%2C+S+K+K">Shubhra Kanti Karmaker Santu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+D">Dongji Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11772" title="Abstract">arXiv:2305.11772</a> (replaced) [<a href="/pdf/2305.11772" title="Download PDF">pdf</a>, <a href="/format/2305.11772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Foundations of Mental Simulation: Future Prediction of Latent  Representations on Dynamic Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nayebi%2C+A">Aran Nayebi</a>, 
<a href="/search/cs?searchtype=author&query=Rajalingham%2C+R">Rishi Rajalingham</a>, 
<a href="/search/cs?searchtype=author&query=Jazayeri%2C+M">Mehrdad Jazayeri</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G+R">Guangyu Robert Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 10 figures, NeurIPS 2023 Camera Ready Version (spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12752" title="Abstract">arXiv:2305.12752</a> (replaced) [<a href="/pdf/2305.12752" title="Download PDF">pdf</a>, <a href="/format/2305.12752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vector Autoregressive Evolution for Dynamic Multi-Objective Optimisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shouyong Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yaru Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shengxiang Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12786" title="Abstract">arXiv:2305.12786</a> (replaced) [<a href="/pdf/2305.12786" title="Download PDF">pdf</a>, <a href="/format/2305.12786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Data Imbalance and Representation Degeneration in  Multilingual Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+W">Wen Lai</a>, 
<a href="/search/cs?searchtype=author&query=Chronopoulou%2C+A">Alexandra Chronopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Fraser%2C+A">Alexander Fraser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023, add statistical significance tests. code available at <a href="https://github.com/lavine-lmu/Bi-ACL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12920" title="Abstract">arXiv:2305.12920</a> (replaced) [<a href="/pdf/2305.12920" title="Download PDF">pdf</a>, <a href="/format/2305.12920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Diachronic Analysis of Paradigm Shifts in NLP Research: When, How, and  Why?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pramanick%2C+A">Aniket Pramanick</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yufang Hou</a>, 
<a href="/search/cs?searchtype=author&query=Mohammad%2C+S+M">Saif M. Mohammad</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13808" title="Abstract">arXiv:2305.13808</a> (replaced) [<a href="/pdf/2305.13808" title="Download PDF">pdf</a>, <a href="/format/2305.13808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asking Clarification Questions to Handle Ambiguity in Open-Domain QA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongryeol Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Segwang Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Minwoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hwanhee Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Joonsuk Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sang-Woo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+K">Kyomin Jung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 4 figures, accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13812" title="Abstract">arXiv:2305.13812</a> (replaced) [<a href="/pdf/2305.13812" title="Download PDF">pdf</a>, <a href="/format/2305.13812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for  Improved Vision-Language Compositionality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+H">Harman Singh</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pengchuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengjiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Wenhan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Jingfei Du</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (long paper, main conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14095" title="Abstract">arXiv:2305.14095</a> (replaced) [<a href="/pdf/2305.14095" title="Download PDF">pdf</a>, <a href="/format/2305.14095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist  Captions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mo%2C+S">Sangwoo Mo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minkyu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kyungmin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jinwoo Shin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14410" title="Abstract">arXiv:2305.14410</a> (replaced) [<a href="/pdf/2305.14410" title="Download PDF">pdf</a>, <a href="/format/2305.14410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Manipulation via Multi-Hop Instructions -- A New Dataset and  Weakly-Supervised Neuro-Symbolic Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+H">Harman Singh</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+P">Poorva Garg</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+M">Mohit Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+K">Kevin Shah</a>, 
<a href="/search/cs?searchtype=author&query=Goswami%2C+A">Ashish Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Modi%2C+S">Satyam Modi</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+A+K">Arnab Kumar Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Khandelwal%2C+D">Dinesh Khandelwal</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+D">Dinesh Garg</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+P">Parag Singla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (long paper, main conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14482" title="Abstract">arXiv:2305.14482</a> (replaced) [<a href="/pdf/2305.14482" title="Download PDF">pdf</a>, <a href="/format/2305.14482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is a Prestigious Job the same as a Prestigious Country? A Case Study on  Multilingual Sentence Embeddings and European Countries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Libovick%C3%BD%2C+J">Jind&#x159;ich Libovick&#xfd;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 1 figure; Findings of EMNLP 2023, camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14583" title="Abstract">arXiv:2305.14583</a> (replaced) [<a href="/pdf/2305.14583" title="Download PDF">pdf</a>, <a href="/format/2305.14583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Language Decompositions of Implicit Content Enable Better Text  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoyle%2C+A">Alexander Hoyle</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+R">Rupak Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+P">Pranav Goel</a>, 
<a href="/search/cs?searchtype=author&query=Resnik%2C+P">Philip Resnik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Main conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15016" title="Abstract">arXiv:2305.15016</a> (replaced) [<a href="/pdf/2305.15016" title="Download PDF">pdf</a>, <a href="/format/2305.15016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Class Separability of Datasets Using Persistent Homology with  Application to LLM Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghalyan%2C+N">Najah Ghalyan</a>, 
<a href="/search/cs?searchtype=author&query=Gourgoulias%2C+K">Kostis Gourgoulias</a>, 
<a href="/search/cs?searchtype=author&query=Satsangi%2C+Y">Yash Satsangi</a>, 
<a href="/search/cs?searchtype=author&query=Moran%2C+S">Sean Moran</a>, 
<a href="/search/cs?searchtype=author&query=Labonne%2C+M">Maxime Labonne</a>, 
<a href="/search/cs?searchtype=author&query=Sabelja%2C+J">Joseph Sabelja</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Rewrite of the manuscript with more baselines, extended related works section, and discussion
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15141" title="Abstract">arXiv:2305.15141</a> (replaced) [<a href="/pdf/2305.15141" title="Download PDF">pdf</a>, <a href="/format/2305.15141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Tempered to Benign Overfitting in ReLU Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kornowski%2C+G">Guy Kornowski</a>, 
<a href="/search/cs?searchtype=author&query=Yehudai%2C+G">Gilad Yehudai</a>, 
<a href="/search/cs?searchtype=author&query=Shamir%2C+O">Ohad Shamir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15352" title="Abstract">arXiv:2305.15352</a> (replaced) [<a href="/pdf/2305.15352" title="Download PDF">pdf</a>, <a href="/format/2305.15352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Rates for Bandit Nonstochastic Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y+J">Y. Jennifer Sun</a>, 
<a href="/search/cs?searchtype=author&query=Newman%2C+S">Stephen Newman</a>, 
<a href="/search/cs?searchtype=author&query=Hazan%2C+E">Elad Hazan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15986" title="Abstract">arXiv:2305.15986</a> (replaced) [<a href="/pdf/2305.15986" title="Download PDF">pdf</a>, <a href="/format/2305.15986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ACAI: Protecting Accelerator Execution with Arm Confidential Computing  Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sridhara%2C+S">Supraja Sridhara</a>, 
<a href="/search/cs?searchtype=author&query=Bertschi%2C+A">Andrin Bertschi</a>, 
<a href="/search/cs?searchtype=author&query=Schl%C3%BCter%2C+B">Benedict Schl&#xfc;ter</a>, 
<a href="/search/cs?searchtype=author&query=Kuhne%2C+M">Mark Kuhne</a>, 
<a href="/search/cs?searchtype=author&query=Aliberti%2C+F">Fabio Aliberti</a>, 
<a href="/search/cs?searchtype=author&query=Shinde%2C+S">Shweta Shinde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of the Usenix Security 2024 paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16475" title="Abstract">arXiv:2305.16475</a> (replaced) [<a href="/pdf/2305.16475" title="Download PDF">pdf</a>, <a href="/ps/2305.16475" title="Download PostScript">ps</a>, <a href="/format/2305.16475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Initialization-Dependent Sample Complexity of Linear Predictors and  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Magen%2C+R">Roey Magen</a>, 
<a href="/search/cs?searchtype=author&query=Shamir%2C+O">Ohad Shamir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16508" title="Abstract">arXiv:2305.16508</a> (replaced) [<a href="/pdf/2305.16508" title="Download PDF">pdf</a>, <a href="/format/2305.16508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Most Neural Networks Are Almost Learnable
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daniely%2C+A">Amit Daniely</a>, 
<a href="/search/cs?searchtype=author&query=Srebro%2C+N">Nathan Srebro</a>, 
<a href="/search/cs?searchtype=author&query=Vardi%2C+G">Gal Vardi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Small fixes after review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16985" title="Abstract">arXiv:2305.16985</a> (replaced) [<a href="/pdf/2305.16985" title="Download PDF">pdf</a>, <a href="/format/2305.16985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse Dynamics Pretraining Learns Good Representations for Multitask  Imitation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brandfonbrener%2C+D">David Brandfonbrener</a>, 
<a href="/search/cs?searchtype=author&query=Nachum%2C+O">Ofir Nachum</a>, 
<a href="/search/cs?searchtype=author&query=Bruna%2C+J">Joan Bruna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17190" title="Abstract">arXiv:2305.17190</a> (replaced) [<a href="/pdf/2305.17190" title="Download PDF">pdf</a>, <a href="/format/2305.17190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiplication-Free Transformer Training via Piecewise Affine Operations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kosson%2C+A">Atli Kosson</a>, 
<a href="/search/cs?searchtype=author&query=Jaggi%2C+M">Martin Jaggi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18097" title="Abstract">arXiv:2305.18097</a> (replaced) [<a href="/pdf/2305.18097" title="Download PDF">pdf</a>, <a href="/ps/2305.18097" title="Download PostScript">ps</a>, <a href="/format/2305.18097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Analysis of Discrete-Phase-Shifter IRS-aided  Amplify-and-Forward Relay Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+R">Rongen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhongyi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+F">Feng Shu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Mengxing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiangzhou Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18258" title="Abstract">arXiv:2305.18258</a> (replaced) [<a href="/pdf/2305.18258" title="Download PDF">pdf</a>, <a href="/format/2305.18258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximize to Explore: One Objective Function Fusing Estimation, Planning,  and Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+M">Miao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Wei Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Han Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shenao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Sirui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhuoran Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoran Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18303" title="Abstract">arXiv:2305.18303</a> (replaced) [<a href="/pdf/2305.18303" title="Download PDF">pdf</a>, <a href="/format/2305.18303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Era of Artificial Intelligence in Education: Towards a Sustainable  Multifaceted Revolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamalov%2C+F">Firuz Kamalov</a>, 
<a href="/search/cs?searchtype=author&query=Calong%2C+D+S">David Santandreu Calong</a>, 
<a href="/search/cs?searchtype=author&query=Gurrib%2C+I">Ikhlaas Gurrib</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The new version is based on the updated version of the paper that was published in Sustainability: <a href="https://doi.org/10.3390/su151612451">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Sustainability, 15(16), 12451 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19254" title="Abstract">arXiv:2305.19254</a> (replaced) [<a href="/pdf/2305.19254" title="Download PDF">pdf</a>, <a href="/format/2305.19254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Can We Learn from Unlearnable Datasets?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sandoval-Segura%2C+P">Pedro Sandoval-Segura</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+V">Vasu Singla</a>, 
<a href="/search/cs?searchtype=author&query=Geiping%2C+J">Jonas Geiping</a>, 
<a href="/search/cs?searchtype=author&query=Goldblum%2C+M">Micah Goldblum</a>, 
<a href="/search/cs?searchtype=author&query=Goldstein%2C+T">Tom Goldstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023. Code available at <a href="https://github.com/psandovalsegura/learn-from-unlearnable">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19706" title="Abstract">arXiv:2305.19706</a> (replaced) [<a href="/pdf/2305.19706" title="Download PDF">pdf</a>, <a href="/format/2305.19706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Necessary and Sufficient Conditions for Optimal Decision Trees using  Dynamic Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+der+Linden%2C+J+G+M">Jacobus G. M. van der Linden</a>, 
<a href="/search/cs?searchtype=author&query=de+Weerdt%2C+M+M">Mathijs M. de Weerdt</a>, 
<a href="/search/cs?searchtype=author&query=Demirovi%C4%87%2C+E">Emir Demirovi&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19809" title="Abstract">arXiv:2305.19809</a> (replaced) [<a href="/pdf/2305.19809" title="Download PDF">pdf</a>, <a href="/format/2305.19809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Diffusion Bridge using Data Consistency for Inverse Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+H">Hyungjin Chung</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jeongsol Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J+C">Jong Chul Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera-ready. 16 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00734" title="Abstract">arXiv:2306.00734</a> (replaced) [<a href="/pdf/2306.00734" title="Download PDF">pdf</a>, <a href="/format/2306.00734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Babel to Boole: The Logical Organization of Information  Decompositions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gutknecht%2C+A+J">Aaron J. Gutknecht</a>, 
<a href="/search/cs?searchtype=author&query=Makkeh%2C+A">Abdullah Makkeh</a>, 
<a href="/search/cs?searchtype=author&query=Wibral%2C+M">Michael Wibral</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01293" title="Abstract">arXiv:2306.01293</a> (replaced) [<a href="/pdf/2306.01293" title="Download PDF">pdf</a>, <a href="/format/2306.01293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoCoOp: Few-Shot Out-of-Distribution Detection via Prompt Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miyai%2C+A">Atsuyuki Miyai</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Irie%2C+G">Go Irie</a>, 
<a href="/search/cs?searchtype=author&query=Aizawa%2C+K">Kiyoharu Aizawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01439" title="Abstract">arXiv:2306.01439</a> (replaced) [<a href="/pdf/2306.01439" title="Download PDF">pdf</a>, <a href="/format/2306.01439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable and Explainable Logical Policies via Neurally Guided  Symbolic Abstraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delfosse%2C+Q">Quentin Delfosse</a>, 
<a href="/search/cs?searchtype=author&query=Shindo%2C+H">Hikaru Shindo</a>, 
<a href="/search/cs?searchtype=author&query=Dhami%2C+D">Devendra Dhami</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 main pages + appendix (19 in total)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Logic in Computer Science (cs.LO); Symbolic Computation (cs.SC)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01755" title="Abstract">arXiv:2306.01755</a> (replaced) [<a href="/pdf/2306.01755" title="Download PDF">pdf</a>, <a href="/format/2306.01755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Priors Predict Text-To-Image Model Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lovering%2C+C">Charles Lovering</a>, 
<a href="/search/cs?searchtype=author&query=Pavlick%2C+E">Ellie Pavlick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02109" title="Abstract">arXiv:2306.02109</a> (replaced) [<a href="/pdf/2306.02109" title="Download PDF">pdf</a>, <a href="/format/2306.02109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Encoding Time-Series Explanations through Self-Supervised Model Behavior  Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Queen%2C+O">Owen Queen</a>, 
<a href="/search/cs?searchtype=author&query=Hartvigsen%2C+T">Thomas Hartvigsen</a>, 
<a href="/search/cs?searchtype=author&query=Koker%2C+T">Teddy Koker</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Huan He</a>, 
<a href="/search/cs?searchtype=author&query=Tsiligkaridis%2C+T">Theodoros Tsiligkaridis</a>, 
<a href="/search/cs?searchtype=author&query=Zitnik%2C+M">Marinka Zitnik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 (spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02858" title="Abstract">arXiv:2306.02858</a> (replaced) [<a href="/pdf/2306.02858" title="Download PDF">pdf</a>, <a href="/format/2306.02858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023's demo track; Code, Pretrained Model, and Dataset: <a href="https://github.com/DAMO-NLP-SG/Video-LLaMA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03022" title="Abstract">arXiv:2306.03022</a> (replaced) [<a href="/pdf/2306.03022" title="Download PDF">pdf</a>, <a href="/format/2306.03022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Alzheimer&#x27;s Disease Classification Via a Contrastive  Diffusion Autoencoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ijishakin%2C+A">Ayodeji Ijishakin</a>, 
<a href="/search/cs?searchtype=author&query=Abdulaal%2C+A">Ahmed Abdulaal</a>, 
<a href="/search/cs?searchtype=author&query=Hadjivasiliou%2C+A">Adamos Hadjivasiliou</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+S">Sophie Martin</a>, 
<a href="/search/cs?searchtype=author&query=Cole%2C+J">James Cole</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICML (2023), 3rd Workshop on Interpretable Machine Learning in
  Healthcare (IMLH)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05501" title="Abstract">arXiv:2306.05501</a> (replaced) [<a href="/pdf/2306.05501" title="Download PDF">pdf</a>, <a href="/format/2306.05501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Framework for Explanation Evaluation in Time Series  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+T">Thu Trang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+L">Thach Le Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Ifrim%2C+G">Georgiana Ifrim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05515" title="Abstract">arXiv:2306.05515</a> (replaced) [<a href="/pdf/2306.05515" title="Download PDF">pdf</a>, <a href="/format/2306.05515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PeFLL: Personalized Federated Learning by Learning to Learn
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scott%2C+J">Jonathan Scott</a>, 
<a href="/search/cs?searchtype=author&query=Zakerinia%2C+H">Hossein Zakerinia</a>, 
<a href="/search/cs?searchtype=author&query=Lampert%2C+C+H">Christoph H. Lampert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08698" title="Abstract">arXiv:2306.08698</a> (replaced) [<a href="/pdf/2306.08698" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phase Transitions of Civil Unrest across Countries and Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Braha%2C+D">Dan Braha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main paper (46 pages) Supporting Information 1 (5 pages) and 2 (138 pages) will be available upon request
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Machine Learning (cs.LG); Adaptation and Self-Organizing Systems (nlin.AO)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09065" title="Abstract">arXiv:2306.09065</a> (replaced) [<a href="/pdf/2306.09065" title="Download PDF">pdf</a>, <a href="/format/2306.09065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Node Cardinality Estimation in a Heterogeneous Wireless Network Deployed  Over a Large Region Using a Mobile Base Station
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kadam%2C+S">Sachin Kadam</a>, 
<a href="/search/cs?searchtype=author&query=Bhargao%2C+K+S">Kaustubh S. Bhargao</a>, 
<a href="/search/cs?searchtype=author&query=Kasbekar%2C+G+S">Gaurav S. Kasbekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the 'Journal of Network and Computer Applications' (<a href="https://www.sciencedirect.com/journal/journal-of-network-and-computer-applications">this https URL</a>). It is an expanded version of the paper, which was presented at the IEEE SPCOM 2020 conference. DOI: 10.1109/SPCOM50965.2020.9179541
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09338" title="Abstract">arXiv:2306.09338</a> (replaced) [<a href="/pdf/2306.09338" title="Download PDF">pdf</a>, <a href="/format/2306.09338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Optimization of Deep Learning via Jacobian Matrix and  Lipschitz Constant
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xianbiao Qi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Digital Economy Academy (IDEA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11974" title="Abstract">arXiv:2306.11974</a> (replaced) [<a href="/pdf/2306.11974" title="Download PDF">pdf</a>, <a href="/format/2306.11974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal adversarial perturbations for multiple classification tasks  with quantum classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Qiu%2C+Y">Yun-Zhong Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12306" title="Abstract">arXiv:2306.12306</a> (replaced) [<a href="/pdf/2306.12306" title="Download PDF">pdf</a>, <a href="/ps/2306.12306" title="Download PostScript">ps</a>, <a href="/format/2306.12306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Deep Ensembles: A Large-Scale Evaluation of Bayesian Deep  Learning under Distribution Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seligmann%2C+F">Florian Seligmann</a>, 
<a href="/search/cs?searchtype=author&query=Becker%2C+P">Philipp Becker</a>, 
<a href="/search/cs?searchtype=author&query=Volpp%2C+M">Michael Volpp</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+G">Gerhard Neumann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code at <a href="https://github.com/Feuermagier/Beyond_Deep_Ensembles">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12747" title="Abstract">arXiv:2306.12747</a> (replaced) [<a href="/pdf/2306.12747" title="Download PDF">pdf</a>, <a href="/format/2306.12747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Don&#x27;t be so Monotone: Relaxing Stochastic Line Search in  Over-Parameterized Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Galli%2C+L">Leonardo Galli</a>, 
<a href="/search/math?searchtype=author&query=Rauhut%2C+H">Holger Rauhut</a>, 
<a href="/search/math?searchtype=author&query=Schmidt%2C+M">Mark Schmidt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12795" title="Abstract">arXiv:2306.12795</a> (replaced) [<a href="/pdf/2306.12795" title="Download PDF">pdf</a>, <a href="/format/2306.12795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Unseen Modality Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunhua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Doughty%2C+H">Hazel Doughty</a>, 
<a href="/search/cs?searchtype=author&query=Snoek%2C+C+G+M">Cees G.M. Snoek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13233" title="Abstract">arXiv:2306.13233</a> (replaced) [<a href="/pdf/2306.13233" title="Download PDF">pdf</a>, <a href="/format/2306.13233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logarithmic Regret for Matrix Games against an Adversary with Noisy  Bandit Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maiti%2C+A">Arnab Maiti</a>, 
<a href="/search/cs?searchtype=author&query=Jamieson%2C+K">Kevin Jamieson</a>, 
<a href="/search/cs?searchtype=author&query=Ratliff%2C+L+J">Lillian J. Ratliff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 68 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13384" title="Abstract">arXiv:2306.13384</a> (replaced) [<a href="/pdf/2306.13384" title="Download PDF">pdf</a>, <a href="/format/2306.13384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch  Diffusion in Histopathology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Aversa%2C+M">Marco Aversa</a>, 
<a href="/search/eess?searchtype=author&query=Nobis%2C+G">Gabriel Nobis</a>, 
<a href="/search/eess?searchtype=author&query=H%C3%A4gele%2C+M">Miriam H&#xe4;gele</a>, 
<a href="/search/eess?searchtype=author&query=Standvoss%2C+K">Kai Standvoss</a>, 
<a href="/search/eess?searchtype=author&query=Chirica%2C+M">Mihaela Chirica</a>, 
<a href="/search/eess?searchtype=author&query=Murray-Smith%2C+R">Roderick Murray-Smith</a>, 
<a href="/search/eess?searchtype=author&query=Alaa%2C+A">Ahmed Alaa</a>, 
<a href="/search/eess?searchtype=author&query=Ruff%2C+L">Lukas Ruff</a>, 
<a href="/search/eess?searchtype=author&query=Ivanova%2C+D">Daniela Ivanova</a>, 
<a href="/search/eess?searchtype=author&query=Samek%2C+W">Wojciech Samek</a>, 
<a href="/search/eess?searchtype=author&query=Klauschen%2C+F">Frederick Klauschen</a>, 
<a href="/search/eess?searchtype=author&query=Sanguinetti%2C+B">Bruno Sanguinetti</a>, 
<a href="/search/eess?searchtype=author&query=Oala%2C+L">Luis Oala</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13992" title="Abstract">arXiv:2306.13992</a> (replaced) [<a href="/pdf/2306.13992" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Sociality in Driving Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiaocong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Meng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15217" title="Abstract">arXiv:2306.15217</a> (replaced) [<a href="/pdf/2306.15217" title="Download PDF">pdf</a>, <a href="/format/2306.15217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Episode Generation for Graph Meta-learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+J">Jihyeong Jung</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+S">Sangwoo Seo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungwon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chanyoung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 12 figures, Preprint version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15969" title="Abstract">arXiv:2306.15969</a> (replaced) [<a href="/pdf/2306.15969" title="Download PDF">pdf</a>, <a href="/format/2306.15969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Separable Physics-Informed Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Junwoo Cho</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+S">Seungtae Nam</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hyunmo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Seok-Bae Yun</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Youngjoon Hong</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+E">Eunbyung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in NeurIPS 2023 (28 pages, 13 figures). arXiv admin note: text overlap with <a href="/abs/2211.08761">arXiv:2211.08761</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01180" title="Abstract">arXiv:2307.01180</a> (replaced) [<a href="/pdf/2307.01180" title="Download PDF">pdf</a>, <a href="/format/2307.01180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PlanE: Representation Learning over Planar Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dimitrov%2C+R">Radoslav Dimitrov</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zeyang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Abboud%2C+R">Ralph Abboud</a>, 
<a href="/search/cs?searchtype=author&query=Ceylan%2C+%C4%B0+%C4%B0">&#x130;smail &#x130;lkan Ceylan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the Thirty-Seventh Annual Conference on Advances in Neural Information Processing Systems (NeurIPS 2023). Code and data available at: <a href="https://github.com/ZZYSonny/PlanE">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03305" title="Abstract">arXiv:2307.03305</a> (replaced) [<a href="/pdf/2307.03305" title="Download PDF">pdf</a>, <a href="/format/2307.03305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Vulnerability of Attribution Methods Using Pre-Softmax Scores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lerma%2C+M">Miguel Lerma</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+M">Mirtha Lucas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03833" title="Abstract">arXiv:2307.03833</a> (replaced) [<a href="/pdf/2307.03833" title="Download PDF">pdf</a>, <a href="/format/2307.03833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhongyu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhuoran Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+W">Wenhao Chai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng-Yen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+J">Jenq-Neng Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05466" title="Abstract">arXiv:2307.05466</a> (replaced) [<a href="/pdf/2307.05466" title="Download PDF">pdf</a>, <a href="/format/2307.05466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Tolling in Arc-based Traffic Assignment Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chiu%2C+C">Chih-Yuan Chiu</a>, 
<a href="/search/eess?searchtype=author&query=Maheshwari%2C+C">Chinmay Maheshwari</a>, 
<a href="/search/eess?searchtype=author&query=Su%2C+P">Pan-Yang Su</a>, 
<a href="/search/eess?searchtype=author&query=Sastry%2C+S">Shankar Sastry</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 4 figures, 2 tables. arXiv admin note: text overlap with <a href="/abs/2304.04705">arXiv:2304.04705</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07314" title="Abstract">arXiv:2307.07314</a> (replaced) [<a href="/pdf/2307.07314" title="Download PDF">pdf</a>, <a href="/format/2307.07314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact Bayesian Inference for Loopy Probabilistic Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klinkenberg%2C+L">Lutz Klinkenberg</a>, 
<a href="/search/cs?searchtype=author&query=Blumenthal%2C+C">Christian Blumenthal</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingshuai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Haase%2C+D">Darion Haase</a>, 
<a href="/search/cs?searchtype=author&query=Katoen%2C+J">Joost-Pieter Katoen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08421" title="Abstract">arXiv:2307.08421</a> (replaced) [<a href="/pdf/2307.08421" title="Download PDF">pdf</a>, <a href="/format/2307.08421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Systematic Comparison of Software Agents and Digital Twins: Differences,  Similarities, and Synergies in Industrial Production
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reinpold%2C+L+M">Lasse Matthias Reinpold</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+L+P">Lukas Peter Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Gehlhoff%2C+F">Felix Gehlhoff</a>, 
<a href="/search/cs?searchtype=author&query=Ramonat%2C+M">Malte Ramonat</a>, 
<a href="/search/cs?searchtype=author&query=Kilthau%2C+M">Maximilian Kilthau</a>, 
<a href="/search/cs?searchtype=author&query=Gill%2C+M+S">Milapji Singh Gill</a>, 
<a href="/search/cs?searchtype=author&query=Reif%2C+J+T">Jonathan Tobias Reif</a>, 
<a href="/search/cs?searchtype=author&query=Henkel%2C+V">Vincent Henkel</a>, 
<a href="/search/cs?searchtype=author&query=Scholz%2C+L">Lena Scholz</a>, 
<a href="/search/cs?searchtype=author&query=Fay%2C+A">Alexander Fay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Manuscript submitted to Journal of Intelligent Manufacturing, Corresponding dataset: <a href="https://doi.org/10.5281/zenodo.8120623">this https URL</a> Additional references in Sec. 1, some other minor changes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08715" title="Abstract">arXiv:2307.08715</a> (replaced) [<a href="/pdf/2307.08715" title="Download PDF">pdf</a>, <a href="/format/2307.08715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MasterKey: Automated Jailbreak Across Multiple Large Language Model  Chatbots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+G">Gelei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuekang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kailong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zefeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The Network and Distributed System Security Symposium (NDSS) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10455" title="Abstract">arXiv:2307.10455</a> (replaced) [<a href="/pdf/2307.10455" title="Download PDF">pdf</a>, <a href="/format/2307.10455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect  Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gharaee%2C+Z">Zahra Gharaee</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Z">ZeMing Gong</a>, 
<a href="/search/cs?searchtype=author&query=Pellegrino%2C+N">Nicholas Pellegrino</a>, 
<a href="/search/cs?searchtype=author&query=Zarubiieva%2C+I">Iuliia Zarubiieva</a>, 
<a href="/search/cs?searchtype=author&query=Haurum%2C+J+B">Joakim Bruslund Haurum</a>, 
<a href="/search/cs?searchtype=author&query=Lowe%2C+S+C">Scott C. Lowe</a>, 
<a href="/search/cs?searchtype=author&query=McKeown%2C+J+T+A">Jaclyn T.A. McKeown</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+C+C+Y">Chris C.Y. Ho</a>, 
<a href="/search/cs?searchtype=author&query=McLeod%2C+J">Joschka McLeod</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y+C">Yi-Yun C Wei</a>, 
<a href="/search/cs?searchtype=author&query=Agda%2C+J">Jireh Agda</a>, 
<a href="/search/cs?searchtype=author&query=Ratnasingham%2C+S">Sujeevan Ratnasingham</a>, 
<a href="/search/cs?searchtype=author&query=Steinke%2C+D">Dirk Steinke</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+A+X">Angel X. Chang</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+G+W">Graham W. Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Fieguth%2C+P">Paul Fieguth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11694" title="Abstract">arXiv:2307.11694</a> (replaced) [<a href="/pdf/2307.11694" title="Download PDF">pdf</a>, <a href="/format/2307.11694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction  and Drug Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Edwards%2C+C">Carl Edwards</a>, 
<a href="/search/cs?searchtype=author&query=Naik%2C+A">Aakanksha Naik</a>, 
<a href="/search/cs?searchtype=author&query=Khot%2C+T">Tushar Khot</a>, 
<a href="/search/cs?searchtype=author&query=Burke%2C+M">Martin Burke</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Hope%2C+T">Tom Hope</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Biomolecules (q-bio.BM); Molecular Networks (q-bio.MN)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12444" title="Abstract">arXiv:2307.12444</a> (replaced) [<a href="/pdf/2307.12444" title="Download PDF">pdf</a>, <a href="/format/2307.12444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proximal Galerkin: A structure-preserving finite element method for  pointwise bound constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Keith%2C+B">Brendan Keith</a>, 
<a href="/search/math?searchtype=author&query=Surowiec%2C+T+M">Thomas M. Surowiec</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13854" title="Abstract">arXiv:2307.13854</a> (replaced) [<a href="/pdf/2307.13854" title="Download PDF">pdf</a>, <a href="/format/2307.13854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WebArena: A Realistic Web Environment for Building Autonomous Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shuyan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+F+F">Frank F. Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+R">Robert Lo</a>, 
<a href="/search/cs?searchtype=author&query=Sridhar%2C+A">Abishek Sridhar</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xianyi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+T">Tianyue Ou</a>, 
<a href="/search/cs?searchtype=author&query=Bisk%2C+Y">Yonatan Bisk</a>, 
<a href="/search/cs?searchtype=author&query=Fried%2C+D">Daniel Fried</a>, 
<a href="/search/cs?searchtype=author&query=Alon%2C+U">Uri Alon</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our code, data, environment reproduction resources, and video demonstrations are publicly available at <a href="https://webarena.dev/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13885" title="Abstract">arXiv:2307.13885</a> (replaced) [<a href="/pdf/2307.13885" title="Download PDF">pdf</a>, <a href="/format/2307.13885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Estimation of Average-Case Robustness for Multi-Class  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tessa Han</a>, 
<a href="/search/cs?searchtype=author&query=Srinivas%2C+S">Suraj Srinivas</a>, 
<a href="/search/cs?searchtype=author&query=Lakkaraju%2C+H">Himabindu Lakkaraju</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14200" title="Abstract">arXiv:2307.14200</a> (replaced) [<a href="/pdf/2307.14200" title="Download PDF">pdf</a>, <a href="/ps/2307.14200" title="Download PostScript">ps</a>, <a href="/format/2307.14200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating functions of non-backtracking walks on weighted digraphs:  radius of convergence and Ihara&#x27;s theorem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Noferini%2C+V">Vanni Noferini</a>, 
<a href="/search/math?searchtype=author&query=Quintana%2C+M+C">Mar&#xed;a C. Quintana</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15776" title="Abstract">arXiv:2307.15776</a> (replaced) [<a href="/pdf/2307.15776" title="Download PDF">pdf</a>, <a href="/format/2307.15776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Select and Augment: Enhanced Dense Retrieval Knowledge Graph  Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abaho%2C+M">Micheal Abaho</a>, 
<a href="/search/cs?searchtype=author&query=Alfaifi%2C+Y+H">Yousef H. Alfaifi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Article has already been puclished to Journal of Artificial Intelligence Research (JAIR)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Artificial Intelligence Research, 78, 2023, 269-285
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15804" title="Abstract">arXiv:2307.15804</a> (replaced) [<a href="/pdf/2307.15804" title="Download PDF">pdf</a>, <a href="/format/2307.15804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Single Index Models beyond Gaussian Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bruna%2C+J">Joan Bruna</a>, 
<a href="/search/cs?searchtype=author&query=Pillaud-Vivien%2C+L">Loucas Pillaud-Vivien</a>, 
<a href="/search/cs?searchtype=author&query=Zweig%2C+A">Aaron Zweig</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16200" title="Abstract">arXiv:2307.16200</a> (replaced) [<a href="/pdf/2307.16200" title="Download PDF">pdf</a>, <a href="/format/2307.16200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue  Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zefa Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+Z">Ziyi Ni</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jing Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shuang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bo Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Machine Intelligence Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16607" title="Abstract">arXiv:2307.16607</a> (replaced) [<a href="/pdf/2307.16607" title="Download PDF">pdf</a>, <a href="/format/2307.16607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $OIDC^2$: Open Identity Certification with OpenID Connect
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Primbs%2C+J">Jonas Primbs</a>, 
<a href="/search/cs?searchtype=author&query=Menth%2C+M">Michael Menth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00605" title="Abstract">arXiv:2308.00605</a> (replaced) [<a href="/pdf/2308.00605" title="Download PDF">pdf</a>, <a href="/format/2308.00605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3:1 Nesting Rules in Redistricting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Donnay%2C+C">Christopher Donnay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 9 figures. For associated code, see <a href="https://github.com/cdonnay/nesting_OH_WI.">this https URL</a> Submitted to Statistics and Public Policy
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Physics and Society (physics.soc-ph); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03688" title="Abstract">arXiv:2308.03688</a> (replaced) [<a href="/pdf/2308.03688" title="Download PDF">pdf</a>, <a href="/format/2308.03688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AgentBench: Evaluating LLMs as Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yifan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+X">Xuanyu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+H">Hanyu Lai</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yu Gu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+H">Hangliang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Men%2C+K">Kaiwen Men</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kejuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shudan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+X">Xiang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+A">Aohan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Z">Zhengxiao Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenhui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Sheng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianjun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuxiao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 55 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06399" title="Abstract">arXiv:2308.06399</a> (replaced) [<a href="/pdf/2308.06399" title="Download PDF">pdf</a>, <a href="/format/2308.06399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via  Mixed-Effect Models and Hierarchical Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Valleggi%2C+L">Lorenzo Valleggi</a>, 
<a href="/search/stat?searchtype=author&query=Scutari%2C+M">Marco Scutari</a>, 
<a href="/search/stat?searchtype=author&query=Stefanini%2C+F+M">Federico Mattia Stefanini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06975" title="Abstract">arXiv:2308.06975</a> (replaced) [<a href="/pdf/2308.06975" title="Download PDF">pdf</a>, <a href="/format/2308.06975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Knowledge Graphs Simplify Text?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Colas%2C+A">Anthony Colas</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Haodi Ma</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xuanli He</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D+Z">Daisy Zhe Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a Main Conference Long Paper at CIKM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08561" title="Abstract">arXiv:2308.08561</a> (replaced) [<a href="/pdf/2308.08561" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implementation of The Future of Drug Discovery: QuantumBased Machine  Learning Simulation (QMLS)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Zhou%2C+Y">Yifan Zhou</a>, 
<a href="/search/q-bio?searchtype=author&query=Wong%2C+Y+K">Yew Kee Wong</a>, 
<a href="/search/q-bio?searchtype=author&query=Liang%2C+Y+S">Yan Shing Liang</a>, 
<a href="/search/q-bio?searchtype=author&query=Qiu%2C+H">Haichuan Qiu</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+Y+X">Yu Xi Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=He%2C+B">Bin He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Computer Science and Mobile Applications,
  Vol 11 Issue 5,May- 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11596" title="Abstract">arXiv:2308.11596</a> (replaced) [<a href="/pdf/2308.11596" title="Download PDF">pdf</a>, <a href="/format/2308.11596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SeamlessM4T: Massively Multilingual &amp; Multimodal Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Communication%2C+S">Seamless Communication</a>, 
<a href="/search/cs?searchtype=author&query=Barrault%2C+L">Lo&#xef;c Barrault</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+Y">Yu-An Chung</a>, 
<a href="/search/cs?searchtype=author&query=Meglioli%2C+M+C">Mariano Cora Meglioli</a>, 
<a href="/search/cs?searchtype=author&query=Dale%2C+D">David Dale</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+N">Ning Dong</a>, 
<a href="/search/cs?searchtype=author&query=Duquenne%2C+P">Paul-Ambroise Duquenne</a>, 
<a href="/search/cs?searchtype=author&query=Elsahar%2C+H">Hady Elsahar</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+H">Hongyu Gong</a>, 
<a href="/search/cs?searchtype=author&query=Heffernan%2C+K">Kevin Heffernan</a>, 
<a href="/search/cs?searchtype=author&query=Hoffman%2C+J">John Hoffman</a>, 
<a href="/search/cs?searchtype=author&query=Klaiber%2C+C">Christopher Klaiber</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Licht%2C+D">Daniel Licht</a>, 
<a href="/search/cs?searchtype=author&query=Maillard%2C+J">Jean Maillard</a>, 
<a href="/search/cs?searchtype=author&query=Rakotoarison%2C+A">Alice Rakotoarison</a>, 
<a href="/search/cs?searchtype=author&query=Sadagopan%2C+K+R">Kaushik Ram Sadagopan</a>, 
<a href="/search/cs?searchtype=author&query=Wenzek%2C+G">Guillaume Wenzek</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+E">Ethan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Akula%2C+B">Bapi Akula</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Peng-Jen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hachem%2C+N+E">Naji El Hachem</a>, 
<a href="/search/cs?searchtype=author&query=Ellis%2C+B">Brian Ellis</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+G+M">Gabriel Mejia Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Haaheim%2C+J">Justin Haaheim</a>, 
<a href="/search/cs?searchtype=author&query=Hansanti%2C+P">Prangthip Hansanti</a>, 
<a href="/search/cs?searchtype=author&query=Howes%2C+R">Russ Howes</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Bernie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+M">Min-Jae Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Inaguma%2C+H">Hirofumi Inaguma</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Somya Jain</a>, 
<a href="/search/cs?searchtype=author&query=Kalbassi%2C+E">Elahe Kalbassi</a>, 
<a href="/search/cs?searchtype=author&query=Kallet%2C+A">Amanda Kallet</a>, 
<a href="/search/cs?searchtype=author&query=Kulikov%2C+I">Ilia Kulikov</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+J">Janice Lam</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Daniel Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xutai Ma</a>, 
<a href="/search/cs?searchtype=author&query=Mavlyutov%2C+R">Ruslan Mavlyutov</a>, 
<a href="/search/cs?searchtype=author&query=Peloquin%2C+B">Benjamin Peloquin</a>, 
<a href="/search/cs?searchtype=author&query=Ramadan%2C+M">Mohamed Ramadan</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+A">Abinesh Ramakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+A">Anna Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+K">Kevin Tran</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+T">Tuan Tran</a>, 
<a href="/search/cs?searchtype=author&query=Tufanov%2C+I">Igor Tufanov</a>, 
<a href="/search/cs?searchtype=author&query=Vogeti%2C+V">Vish Vogeti</a>, 
<a href="/search/cs?searchtype=author&query=Wood%2C+C">Carleigh Wood</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yilin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bokai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Andrews%2C+P">Pierre Andrews</a>, 
<a href="/search/cs?searchtype=author&query=Balioglu%2C+C">Can Balioglu</a>, 
<a href="/search/cs?searchtype=author&query=Costa-juss%C3%A0%2C+M+R">Marta R. Costa-juss&#xe0;</a>,  et al. (16 additional authors not shown)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13369" title="Abstract">arXiv:2308.13369</a> (replaced) [<a href="/pdf/2308.13369" title="Download PDF">pdf</a>, <a href="/format/2308.13369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distribution-Aligned Diffusion for Human Mesh Recovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Foo%2C+L+G">Lin Geng Foo</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+J">Jia Gong</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+H">Hossein Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15772" title="Abstract">arXiv:2308.15772</a> (replaced) [<a href="/pdf/2308.15772" title="Download PDF">pdf</a>, <a href="/format/2308.15772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task-Based MoE for Multitask Multilingual Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pham%2C+H">Hai Pham</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y+J">Young Jin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Subhabrata Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Woodruff%2C+D+P">David P. Woodruff</a>, 
<a href="/search/cs?searchtype=author&query=Poczos%2C+B">Barnabas Poczos</a>, 
<a href="/search/cs?searchtype=author&query=Awadalla%2C+H+H">Hany Hassan Awadalla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16355" title="Abstract">arXiv:2308.16355</a> (replaced) [<a href="/pdf/2308.16355" title="Download PDF">pdf</a>, <a href="/format/2308.16355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Recycling Training Strategy for Medical Image Segmentation with  Diffusion Denoising Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fu%2C+Y">Yunguan Fu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yiwen Li</a>, 
<a href="/search/eess?searchtype=author&query=Saeed%2C+S+U">Shaheer U Saeed</a>, 
<a href="/search/eess?searchtype=author&query=Clarkson%2C+M+J">Matthew J Clarkson</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+Y">Yipeng Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16572" title="Abstract">arXiv:2308.16572</a> (replaced) [<a href="/pdf/2308.16572" title="Download PDF">pdf</a>, <a href="/format/2308.16572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CL-MAE: Curriculum-Learned Masked Autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madan%2C+N">Neelu Madan</a>, 
<a href="/search/cs?searchtype=author&query=Ristea%2C+N">Nicolae-Catalin Ristea</a>, 
<a href="/search/cs?searchtype=author&query=Nasrollahi%2C+K">Kamal Nasrollahi</a>, 
<a href="/search/cs?searchtype=author&query=Moeslund%2C+T+B">Thomas B. Moeslund</a>, 
<a href="/search/cs?searchtype=author&query=Ionescu%2C+R+T">Radu Tudor Ionescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00082" title="Abstract">arXiv:2309.00082</a> (replaced) [<a href="/pdf/2309.00082" title="Download PDF">pdf</a>, <a href="/format/2309.00082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RePo: Resilient Model-Based Reinforcement Learning by Regularizing  Posterior Predictability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chuning Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Simchowitz%2C+M">Max Simchowitz</a>, 
<a href="/search/cs?searchtype=author&query=Gadipudi%2C+S">Siri Gadipudi</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Abhishek Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04579" title="Abstract">arXiv:2309.04579</a> (replaced) [<a href="/pdf/2309.04579" title="Download PDF">pdf</a>, <a href="/format/2309.04579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EGOFALLS: A visual-audio dataset and benchmark for fall detection using  egocentric cameras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xueyi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05117" title="Abstract">arXiv:2309.05117</a> (replaced) [<a href="/pdf/2309.05117" title="Download PDF">pdf</a>, <a href="/format/2309.05117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model discovery for nonautonomous translation-invariant problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhao%2C+H">Hongli Zhao</a>, 
<a href="/search/math?searchtype=author&query=Tartakovsky%2C+D+M">Daniel M. Tartakovsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 21 figures. Replaced title, added references based on received comments and edited introduction. Other edits pending
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06578" title="Abstract">arXiv:2309.06578</a> (replaced) [<a href="/pdf/2309.06578" title="Download PDF">pdf</a>, <a href="/format/2309.06578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models Discern Evidence for Scientific Hypotheses?  Case Studies in the Social Sciences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koneru%2C+S">Sai Koneru</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Rajtmajer%2C+S">Sarah Rajtmajer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09357" title="Abstract">arXiv:2309.09357</a> (replaced) [<a href="/pdf/2309.09357" title="Download PDF">pdf</a>, <a href="/format/2309.09357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Talk2Care: Facilitating Asynchronous Patient-Provider Communication with  Large-Language-Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziqi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xuhai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+B">Bingsheng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Rogers%2C+E">Ethan Rogers</a>, 
<a href="/search/cs?searchtype=author&query=Intille%2C+S">Stephen Intille</a>, 
<a href="/search/cs?searchtype=author&query=Shara%2C+N">Nawar Shara</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+G+G">Guodong Gordon Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dakuo Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under submission to CHI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09725" title="Abstract">arXiv:2309.09725</a> (replaced) [<a href="/pdf/2309.09725" title="Download PDF">pdf</a>, <a href="/ps/2309.09725" title="Download PostScript">ps</a>, <a href="/format/2309.09725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss  with Imbalanced Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hong%2C+W">Wanli Hong</a>, 
<a href="/search/stat?searchtype=author&query=Ling%2C+S">Shuyang Ling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09770" title="Abstract">arXiv:2309.09770</a> (replaced) [<a href="/pdf/2309.09770" title="Download PDF">pdf</a>, <a href="/format/2309.09770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Data in Datathons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mougan%2C+C">Carlos Mougan</a>, 
<a href="/search/cs?searchtype=author&query=Plant%2C+R">Richard Plant</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+C">Clare Teng</a>, 
<a href="/search/cs?searchtype=author&query=Bazzi%2C+M">Marya Bazzi</a>, 
<a href="/search/cs?searchtype=author&query=Cabrejas-Egea%2C+A">Alvaro Cabrejas-Egea</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+R+S">Ryan Sze-Yin Chan</a>, 
<a href="/search/cs?searchtype=author&query=Jasin%2C+D+S">David Salvador Jasin</a>, 
<a href="/search/cs?searchtype=author&query=Stoffel%2C+M">Martin Stoffel</a>, 
<a href="/search/cs?searchtype=author&query=Whitaker%2C+K+J">Kirstie Jane Whitaker</a>, 
<a href="/search/cs?searchtype=author&query=Manser%2C+J">Jules Manser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmark
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10498" title="Abstract">arXiv:2309.10498</a> (replaced) [<a href="/pdf/2309.10498" title="Download PDF">pdf</a>, <a href="/format/2309.10498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Configurable Library for Generating and Manipulating Maze Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ivanitskiy%2C+M+I">Michael Igorevich Ivanitskiy</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+R">Rusheb Shah</a>, 
<a href="/search/cs?searchtype=author&query=Spies%2C+A+F">Alex F. Spies</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%A4uker%2C+T">Tilman R&#xe4;uker</a>, 
<a href="/search/cs?searchtype=author&query=Valentine%2C+D">Dan Valentine</a>, 
<a href="/search/cs?searchtype=author&query=Rager%2C+C">Can Rager</a>, 
<a href="/search/cs?searchtype=author&query=Quirke%2C+L">Lucia Quirke</a>, 
<a href="/search/cs?searchtype=author&query=Mathwin%2C+C">Chris Mathwin</a>, 
<a href="/search/cs?searchtype=author&query=Corlouer%2C+G">Guillaume Corlouer</a>, 
<a href="/search/cs?searchtype=author&query=Behn%2C+C+D">Cecilia Diniz Behn</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+S+W">Samy Wu Fung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures, 1 table. Corresponding author: Michael Ivanitskiy (mivanits@umich.edu). Code available at <a href="https://github.com/understanding-search/maze-dataset">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10790" title="Abstract">arXiv:2309.10790</a> (replaced) [<a href="/pdf/2309.10790" title="Download PDF">pdf</a>, <a href="/format/2309.10790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guide Your Agent with Adaptive Multimodal Rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+C">Changyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+Y">Younggyo Seo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+L">Lisa Lee</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jinwoo Shin</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Honglak Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kimin Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023. Project webpage: <a href="https://sites.google.com/view/2023arp">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12508" title="Abstract">arXiv:2309.12508</a> (replaced) [<a href="/pdf/2309.12508" title="Download PDF">pdf</a>, <a href="/format/2309.12508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Diffusion-Model of Joint Interactive Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niedoba%2C+M">Matthew Niedoba</a>, 
<a href="/search/cs?searchtype=author&query=Lavington%2C+J+W">Jonathan Wilder Lavington</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yunpeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lioutas%2C+V">Vasileios Lioutas</a>, 
<a href="/search/cs?searchtype=author&query=Sefas%2C+J">Justice Sefas</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaoxuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Green%2C+D">Dylan Green</a>, 
<a href="/search/cs?searchtype=author&query=Dabiri%2C+S">Setareh Dabiri</a>, 
<a href="/search/cs?searchtype=author&query=Zwartsenberg%2C+B">Berend Zwartsenberg</a>, 
<a href="/search/cs?searchtype=author&query=Scibior%2C+A">Adam Scibior</a>, 
<a href="/search/cs?searchtype=author&query=Wood%2C+F">Frank Wood</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures. Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13230" title="Abstract">arXiv:2309.13230</a> (replaced) [<a href="/pdf/2309.13230" title="Download PDF">pdf</a>, <a href="/format/2309.13230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unify word-level and span-level tasks: NJUNLP&#x27;s Participation for the  WMT2023 Quality Estimation Shared Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+X">Xiang Geng</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Z">Zhejian Lai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+S">Shimin Tao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shujian Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13472" title="Abstract">arXiv:2309.13472</a> (replaced) [<a href="/pdf/2309.13472" title="Download PDF">pdf</a>, <a href="/format/2309.13472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge Aware Learning for 3D Point Cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CGI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13876" title="Abstract">arXiv:2309.13876</a> (replaced) [<a href="/pdf/2309.13876" title="Download PDF">pdf</a>, <a href="/format/2309.13876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reproducing Whisper-Style Training Using an Open-Source Toolkit and  Publicly Available Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yifan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Jinchuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Brian Yan</a>, 
<a href="/search/cs?searchtype=author&query=Berrebbi%2C+D">Dan Berrebbi</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xuankai Chang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinjian Li</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiatong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Siddhant Arora</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">William Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Roshan Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wangyou Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sudo%2C+Y">Yui Sudo</a>, 
<a href="/search/cs?searchtype=author&query=Shakeel%2C+M">Muhammad Shakeel</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+J">Jee-weon Jung</a>, 
<a href="/search/cs?searchtype=author&query=Maiti%2C+S">Soumi Maiti</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14484" title="Abstract">arXiv:2309.14484</a> (replaced) [<a href="/pdf/2309.14484" title="Download PDF">pdf</a>, <a href="/format/2309.14484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distribution-Agnostic Database De-Anonymization Under Synchronization  Errors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bakirtas%2C+S">Serhat Bakirtas</a>, 
<a href="/search/cs?searchtype=author&query=Erkip%2C+E">Elza Erkip</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15500" title="Abstract">arXiv:2309.15500</a> (replaced) [<a href="/pdf/2309.15500" title="Download PDF">pdf</a>, <a href="/format/2309.15500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaEvo: Edge-Assisted Continuous and Timely DNN Model Evolution for  Mobile Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lehao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhiwen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haoyi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sicong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yaxiong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+B">Bin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yunxin Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Mobile Computing 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16928" title="Abstract">arXiv:2309.16928</a> (replaced) [<a href="/pdf/2309.16928" title="Download PDF">pdf</a>, <a href="/format/2309.16928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Receive Help: Intervention-Aware Concept Embedding Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zarlenga%2C+M+E">Mateo Espinosa Zarlenga</a>, 
<a href="/search/cs?searchtype=author&query=Collins%2C+K+M">Katherine M. Collins</a>, 
<a href="/search/cs?searchtype=author&query=Dvijotham%2C+K">Krishnamurthy Dvijotham</a>, 
<a href="/search/cs?searchtype=author&query=Weller%2C+A">Adrian Weller</a>, 
<a href="/search/cs?searchtype=author&query=Shams%2C+Z">Zohreh Shams</a>, 
<a href="/search/cs?searchtype=author&query=Jamnik%2C+M">Mateja Jamnik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a spotlight at the Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17310" title="Abstract">arXiv:2309.17310</a> (replaced) [<a href="/pdf/2309.17310" title="Download PDF">pdf</a>, <a href="/format/2309.17310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leave-one-out Distinguishability in Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jiayuan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Borovykh%2C+A">Anastasia Borovykh</a>, 
<a href="/search/cs?searchtype=author&query=Hayou%2C+S">Soufiane Hayou</a>, 
<a href="/search/cs?searchtype=author&query=Shokri%2C+R">Reza Shokri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17332" title="Abstract">arXiv:2309.17332</a> (replaced) [<a href="/pdf/2309.17332" title="Download PDF">pdf</a>, <a href="/format/2309.17332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overview of the BioLaySumm 2023 Shared Task on Lay Summarization of  Biomedical Research Articles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goldsack%2C+T">Tomas Goldsack</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zheheng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Scarton%2C+C">Carolina Scarton</a>, 
<a href="/search/cs?searchtype=author&query=Shardlow%2C+M">Matthew Shardlow</a>, 
<a href="/search/cs?searchtype=author&query=Ananiadou%2C+S">Sophia Ananiadou</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at BioNLP@ACL2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 22nd Workshop on Biomedical Natural Language Processing and
  BioNLP Shared Tasks (2023) 468-477
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01037" title="Abstract">arXiv:2310.01037</a> (replaced) [<a href="/pdf/2310.01037" title="Download PDF">pdf</a>, <a href="/format/2310.01037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seismogram Transformer: A generic deep learning backbone network for  multiple earthquake monitoring tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Li%2C+S">Sen Li</a>, 
<a href="/search/physics?searchtype=author&query=Yang%2C+X">Xu Yang</a>, 
<a href="/search/physics?searchtype=author&query=Cao%2C+A">Anye Cao</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+C">Changbin Wang</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+Y">Yaoqi Liu</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+Y">Yapeng Liu</a>, 
<a href="/search/physics?searchtype=author&query=Niu%2C+Q">Qiang Niu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01164" title="Abstract">arXiv:2310.01164</a> (replaced) [<a href="/pdf/2310.01164" title="Download PDF">pdf</a>, <a href="/format/2310.01164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segment Any Building For Remote Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CGI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01552" title="Abstract">arXiv:2310.01552</a> (replaced) [<a href="/pdf/2310.01552" title="Download PDF">pdf</a>, <a href="/format/2310.01552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Ancillary Services: From Grid Codes to Transfer Function-Based  Converter Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=H%C3%A4berle%2C+V">Verena H&#xe4;berle</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+L">Linbin Huang</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+X">Xiuqiang He</a>, 
<a href="/search/eess?searchtype=author&query=Prieto-Araujo%2C+E">Eduardo Prieto-Araujo</a>, 
<a href="/search/eess?searchtype=author&query=D%C3%B6rfler%2C+F">Florian D&#xf6;rfler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02031" title="Abstract">arXiv:2310.02031</a> (replaced) [<a href="/pdf/2310.02031" title="Download PDF">pdf</a>, <a href="/format/2310.02031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OceanGPT: A Large Language Model for Ocean Science Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+Z">Zhen Bi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yida Xue</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+Y">Yixin Ou</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+D">Daxiong Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Guozhou Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress. Project Website: <a href="https://zjunlp.github.io/project/OceanGPT/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02674" title="Abstract">arXiv:2310.02674</a> (replaced) [<a href="/pdf/2310.02674" title="Download PDF">pdf</a>, <a href="/format/2310.02674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Land-cover change detection using paired OpenStreetMap data and optical  high-resolution imagery via object-guided Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongruixuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+C">Cuiling Lan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jian Song</a>, 
<a href="/search/cs?searchtype=author&query=Broni-Bediako%2C+C">Clifford Broni-Bediako</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+J">Junshi Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yokoya%2C+N">Naoto Yokoya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02907" title="Abstract">arXiv:2310.02907</a> (replaced) [<a href="/pdf/2310.02907" title="Download PDF">pdf</a>, <a href="/format/2310.02907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Whole-body MPC for highly redundant legged manipulators: experimental  evaluation with a 37 DoF dual-arm quadruped
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dadiotis%2C+I">Ioannis Dadiotis</a>, 
<a href="/search/cs?searchtype=author&query=Laurenzi%2C+A">Arturo Laurenzi</a>, 
<a href="/search/cs?searchtype=author&query=Tsagarakis%2C+N">Nikos Tsagarakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 2023 IEEE-RAS International Conference on Humanoid Robots (Humanoids 2023), final version with video and acknowledgements
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03368" title="Abstract">arXiv:2310.03368</a> (replaced) [<a href="/pdf/2310.03368" title="Download PDF">pdf</a>, <a href="/format/2310.03368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Hallucinations in Chinese Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qinyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tianxiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siyin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiangyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mozhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junliang He</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Mianqiu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhangyue Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03888" title="Abstract">arXiv:2310.03888</a> (replaced) [<a href="/pdf/2310.03888" title="Download PDF">pdf</a>, <a href="/format/2310.03888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Frequency Domain Analysis of Nonlinear Series Elastic Actuator via  Describing Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hirao%2C+M">Motohiro Hirao</a>, 
<a href="/search/cs?searchtype=author&query=Kurkcu%2C+B">Burak Kurkcu</a>, 
<a href="/search/cs?searchtype=author&query=Ghanbarpour%2C+A">Alireza Ghanbarpour</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by 2023 IEEE ROBIO conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04951" title="Abstract">arXiv:2310.04951</a> (replaced) [<a href="/pdf/2310.04951" title="Download PDF">pdf</a>, <a href="/format/2310.04951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeTransOcean: A Comprehensive Multilingual Benchmark for Code  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+W">Weixiang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuchen Tian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunzhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wen Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05163" title="Abstract">arXiv:2310.05163</a> (replaced) [<a href="/pdf/2310.05163" title="Download PDF">pdf</a>, <a href="/format/2310.05163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Investigation of LLMs&#x27; Inefficacy in Understanding Converse Relations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+C">Chengwen Qi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bowen Li</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+B">Binyuan Hui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bailin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jinwang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Laili%2C+Y">Yuanjun Laili</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05857" title="Abstract">arXiv:2310.05857</a> (replaced) [<a href="/pdf/2310.05857" title="Download PDF">pdf</a>, <a href="/format/2310.05857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Summarization with Human Edits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zonghai Yao</a>, 
<a href="/search/cs?searchtype=author&query=Schloss%2C+B+J">Benjamin J Schloss</a>, 
<a href="/search/cs?searchtype=author&query=Selvaraj%2C+S+P">Sai P. Selvaraj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in proceedings of the Main Conference on Empirical Methods in Natural Language Processing (EMNLP) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05858" title="Abstract">arXiv:2310.05858</a> (replaced) [<a href="/pdf/2310.05858" title="Download PDF">pdf</a>, <a href="/format/2310.05858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DSAC-T: Distributional Soft Actor-Critic with Three Refinements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jingliang Duan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+L">Liming Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jiaxin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+E">Shengbo Eben Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06069" title="Abstract">arXiv:2310.06069</a> (replaced) [<a href="/pdf/2310.06069" title="Download PDF">pdf</a>, <a href="/format/2310.06069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Exploration is no harder than Thompson Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+Z">Zhaoqi Li</a>, 
<a href="/search/stat?searchtype=author&query=Jamieson%2C+K">Kevin Jamieson</a>, 
<a href="/search/stat?searchtype=author&query=Jain%2C+L">Lalit Jain</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06754" title="Abstract">arXiv:2310.06754</a> (replaced) [<a href="/pdf/2310.06754" title="Download PDF">pdf</a>, <a href="/format/2310.06754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Analysis of RIS-assisted MIMO-OFDM Cellular Networks Based  on Matern Cluster Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Guodong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Baccelli%2C+F">Francois Baccelli</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+K">Ke Feng</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+L+U">Luis Uzeda Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Paris%2C+S">Stefano Paris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07235" title="Abstract">arXiv:2310.07235</a> (replaced) [<a href="/pdf/2310.07235" title="Download PDF">pdf</a>, <a href="/format/2310.07235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are GATs Out of Balance?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mustafa%2C+N">Nimrah Mustafa</a>, 
<a href="/search/cs?searchtype=author&query=Bojchevski%2C+A">Aleksandar Bojchevski</a>, 
<a href="/search/cs?searchtype=author&query=Burkholz%2C+R">Rebekka Burkholz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages. To be published in Advances in Neural Information Processing Systems (NeurIPS), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07440" title="Abstract">arXiv:2310.07440</a> (replaced) [<a href="/pdf/2310.07440" title="Download PDF">pdf</a>, <a href="/format/2310.07440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distance Weighted Trans Network for Image Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shamsolmoali%2C+P">Pourya Shamsolmoali</a>, 
<a href="/search/cs?searchtype=author&query=Zareapoor%2C+M">Masoumeh Zareapoor</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huiyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuelong Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yue Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07554" title="Abstract">arXiv:2310.07554</a> (replaced) [<a href="/pdf/2310.07554" title="Download PDF">pdf</a>, <a href="/format/2310.07554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieve Anything To Augment Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peitian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shitao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+J">Jian-Yun Nie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08176" title="Abstract">arXiv:2310.08176</a> (replaced) [<a href="/pdf/2310.08176" title="Download PDF">pdf</a>, <a href="/format/2310.08176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Infinite Width Graph Neural Networks for Node Regression/ Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cobanoglu%2C+Y">Yunus Cobanoglu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 49 Pages, 2 Figures (with subfigures), multiple tables, made table of contents fit to one page and added Derivatives on GAT*NTK and GAT*GP in A.4
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08872" title="Abstract">arXiv:2310.08872</a> (replaced) [<a href="/pdf/2310.08872" title="Download PDF">pdf</a>, <a href="/format/2310.08872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> R&amp;B: Region and Boundary Aware Zero-shot Grounded Text-to-image  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jiayu Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liang Li</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+H">Henglei Lv</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuhui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qingming Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under review. Project page: <a href="https://sagileo.github.io/Region-and-Boundary">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09725" title="Abstract">arXiv:2310.09725</a> (replaced) [<a href="/pdf/2310.09725" title="Download PDF">pdf</a>, <a href="/format/2310.09725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yuyang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shangbin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Balachandran%2C+V">Vidhisha Balachandran</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhaoxuan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+S">Shiqi Lou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianxing He</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10316" title="Abstract">arXiv:2310.10316</a> (replaced) [<a href="/pdf/2310.10316" title="Download PDF">pdf</a>, <a href="/ps/2310.10316" title="Download PostScript">ps</a>, <a href="/format/2310.10316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral representation of two-sided signals from $\ell_\infty$ and  applications to signal processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dokuchaev%2C+N">Nikolai Dokuchaev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Functional Analysis (math.FA)

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10333" title="Abstract">arXiv:2310.10333</a> (replaced) [<a href="/pdf/2310.10333" title="Download PDF">pdf</a>, <a href="/format/2310.10333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Legal NLP Meets MiCAR: Advancing the Analysis of Crypto White Papers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Camassa%2C+C">Carolina Camassa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NLLP23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL); General Finance (q-fin.GN)

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10552" title="Abstract">arXiv:2310.10552</a> (replaced) [<a href="/pdf/2310.10552" title="Download PDF">pdf</a>, <a href="/format/2310.10552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal bounds for POD approximations of infinite horizon control  problems based on time derivatives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=de+Frutos%2C+J">Javier de Frutos</a>, 
<a href="/search/math?searchtype=author&query=Garcia-Archilla%2C+B">Bosco Garcia-Archilla</a>, 
<a href="/search/math?searchtype=author&query=Novo%2C+J">Julia Novo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10692" title="Abstract">arXiv:2310.10692</a> (replaced) [<a href="/pdf/2310.10692" title="Download PDF">pdf</a>, <a href="/format/2310.10692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ACES: Generating Diverse Programming Puzzles with Autotelic Language  Models and Semantic Descriptors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pourcel%2C+J">Julien Pourcel</a>, 
<a href="/search/cs?searchtype=author&query=Colas%2C+C">C&#xe9;dric Colas</a>, 
<a href="/search/cs?searchtype=author&query=Oudeyer%2C+P">Pierre-Yves Oudeyer</a>, 
<a href="/search/cs?searchtype=author&query=Teodorescu%2C+L">Laetitia Teodorescu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11069" title="Abstract">arXiv:2310.11069</a> (replaced) [<a href="/pdf/2310.11069" title="Download PDF">pdf</a>, <a href="/format/2310.11069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waheed%2C+A">Abdul Waheed</a>, 
<a href="/search/cs?searchtype=author&query=Talafha%2C+B">Bashar Talafha</a>, 
<a href="/search/cs?searchtype=author&query=Suvellin%2C+P">Peter Suvellin</a>, 
<a href="/search/cs?searchtype=author&query=Elmadany%2C+A">AbdelRahim Elmadany</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ArabicNLP conference co-located with EMNLP'23. First three authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11518" title="Abstract">arXiv:2310.11518</a> (replaced) [<a href="/pdf/2310.11518" title="Download PDF">pdf</a>, <a href="/format/2310.11518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guarantees for Self-Play in Multiplayer Games via Polymatrix  Decomposability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=MacQueen%2C+R">Revan MacQueen</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+J+R">James R. Wright</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11651" title="Abstract">arXiv:2310.11651</a> (replaced) [<a href="/pdf/2310.11651" title="Download PDF">pdf</a>, <a href="/format/2310.11651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> US Microelectronics Packaging Ecosystem: Challenges and Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Noor%2C+R">Rouhan Noor</a>, 
<a href="/search/eess?searchtype=author&query=Kottur%2C+H+R">Himanandhan Reddy Kottur</a>, 
<a href="/search/eess?searchtype=author&query=Craig%2C+P+J">Patrick J Craig</a>, 
<a href="/search/eess?searchtype=author&query=Biswas%2C+L+K">Liton Kumar Biswas</a>, 
<a href="/search/eess?searchtype=author&query=Khan%2C+M+S+M">M Shafkat M Khan</a>, 
<a href="/search/eess?searchtype=author&query=Varshney%2C+N">Nitin Varshney</a>, 
<a href="/search/eess?searchtype=author&query=Dalir%2C+H">Hamed Dalir</a>, 
<a href="/search/eess?searchtype=author&query=Ak%C3%A7al%C4%B1%2C+E">Elif Ak&#xe7;al&#x131;</a>, 
<a href="/search/eess?searchtype=author&query=Motlagh%2C+B">Bahar Motlagh</a>, 
<a href="/search/eess?searchtype=author&query=Woychik%2C+C">Charles Woychik</a>, 
<a href="/search/eess?searchtype=author&query=Yoon%2C+Y">Yong-Kyu Yoon</a>, 
<a href="/search/eess?searchtype=author&query=Asadizanjani%2C+N">Navid Asadizanjani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11954" title="Abstract">arXiv:2310.11954</a> (replaced) [<a href="/pdf/2310.11954" title="Download PDF">pdf</a>, <a href="/format/2310.11954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MusicAgent: An AI Agent for Music Understanding and Generation with  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dingyao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kaitao Song</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+P">Peiling Lu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianyu He</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wei Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shikun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12127" title="Abstract">arXiv:2310.12127</a> (replaced) [<a href="/pdf/2310.12127" title="Download PDF">pdf</a>, <a href="/format/2310.12127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for  Fairer Instruction-Tuned Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Attanasio%2C+G">Giuseppe Attanasio</a>, 
<a href="/search/cs?searchtype=author&query=Plaza-del-Arco%2C+F+M">Flor Miriam Plaza-del-Arco</a>, 
<a href="/search/cs?searchtype=author&query=Nozza%2C+D">Debora Nozza</a>, 
<a href="/search/cs?searchtype=author&query=Lauscher%2C+A">Anne Lauscher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023. Code and data at <a href="https://github.com/MilaNLProc/interpretability-mt-gender-bias">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12541" title="Abstract">arXiv:2310.12541</a> (replaced) [<a href="/pdf/2310.12541" title="Download PDF">pdf</a>, <a href="/format/2310.12541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model for Multi-objective Evolutionary Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenkun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+S">Shunyu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+X">Xialiang Tong</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+M">Mingxuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingfu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13276" title="Abstract">arXiv:2310.13276</a> (replaced) [<a href="/pdf/2310.13276" title="Download PDF">pdf</a>, <a href="/format/2310.13276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jian%2C+X">Xiangru Jian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yimu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13447" title="Abstract">arXiv:2310.13447</a> (replaced) [<a href="/pdf/2310.13447" title="Download PDF">pdf</a>, <a href="/format/2310.13447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiscale Superpixel Structured Difference Graph Convolutional Network  for VL Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Siyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yeming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Sirui Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yaoru Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+L">Lizhi Bai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13613" title="Abstract">arXiv:2310.13613</a> (replaced) [<a href="/pdf/2310.13613" title="Download PDF">pdf</a>, <a href="/format/2310.13613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hunayn: Elevating Translation Beyond the Literal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almousa%2C+N">Nasser Almousa</a>, 
<a href="/search/cs?searchtype=author&query=Alzamil%2C+N">Nasser Alzamil</a>, 
<a href="/search/cs?searchtype=author&query=Alshehri%2C+A">Abdullah Alshehri</a>, 
<a href="/search/cs?searchtype=author&query=Sait%2C+A">Ahmad Sait</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13788" title="Abstract">arXiv:2310.13788</a> (replaced) [<a href="/pdf/2310.13788" title="Download PDF">pdf</a>, <a href="/ps/2310.13788" title="Download PostScript">ps</a>, <a href="/format/2310.13788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster Integer Points Counting in Parametric Polyhedra
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gribanov%2C+D">D. Gribanov</a>, 
<a href="/search/cs?searchtype=author&query=Malyshev%2C+D">D. Malyshev</a>, 
<a href="/search/cs?searchtype=author&query=Pardalos%2C+P">P. Pardalos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG); Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13896" title="Abstract">arXiv:2310.13896</a> (replaced) [<a href="/pdf/2310.13896" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPTutor: an open-source AI pair programming tool alternative to Copilot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Eason Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ray Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Justa Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Damien Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+P">Pierce Hung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14199" title="Abstract">arXiv:2310.14199</a> (replaced) [<a href="/pdf/2310.14199" title="Download PDF">pdf</a>, <a href="/ps/2310.14199" title="Download PostScript">ps</a>, <a href="/format/2310.14199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partially Explicit Generalized Multiscale Method for Poroelasticity  Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Su%2C+X">Xin Su</a>, 
<a href="/search/math?searchtype=author&query=Leung%2C+W+T">Wing Tat Leung</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+W">Wenyuan Li</a>, 
<a href="/search/math?searchtype=author&query=Pun%2C+S">Sai-Mang Pun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 Pages,61 figures. arXiv admin note: text overlap with <a href="/abs/2208.05542">arXiv:2208.05542</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14455" title="Abstract">arXiv:2310.14455</a> (replaced) [<a href="/pdf/2310.14455" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An International Consortium for Evaluations of Societal-Scale Risks from  Advanced AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gruetzemacher%2C+R">Ross Gruetzemacher</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+A">Alan Chan</a>, 
<a href="/search/cs?searchtype=author&query=Frazier%2C+K">Kevin Frazier</a>, 
<a href="/search/cs?searchtype=author&query=Manning%2C+C">Christy Manning</a>, 
<a href="/search/cs?searchtype=author&query=Los%2C+%C5%A0">&#x160;t&#x11b;p&#xe1;n Los</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+J">James Fox</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Orallo%2C+J">Jos&#xe9; Hern&#xe1;ndez-Orallo</a>, 
<a href="/search/cs?searchtype=author&query=Burden%2C+J">John Burden</a>, 
<a href="/search/cs?searchtype=author&query=Franklin%2C+M">Matija Franklin</a>, 
<a href="/search/cs?searchtype=author&query=Ghuidhir%2C+C+N">Cl&#xed;odhna N&#xed; Ghuidhir</a>, 
<a href="/search/cs?searchtype=author&query=Bailey%2C+M">Mark Bailey</a>, 
<a href="/search/cs?searchtype=author&query=Eth%2C+D">Daniel Eth</a>, 
<a href="/search/cs?searchtype=author&query=Pilditch%2C+T">Toby Pilditch</a>, 
<a href="/search/cs?searchtype=author&query=Kilian%2C+K">Kyle Kilian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14480" title="Abstract">arXiv:2310.14480</a> (replaced) [<a href="/pdf/2310.14480" title="Download PDF">pdf</a>, <a href="/format/2310.14480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention-Enhancing Backdoor Attacks Against BERT-based Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+W">Weimin Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Songzhu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+L">Lu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+H">Haibin Ling</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14484" title="Abstract">arXiv:2310.14484</a> (replaced) [<a href="/pdf/2310.14484" title="Download PDF">pdf</a>, <a href="/format/2310.14484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FlipDyn with Control: Resource Takeover Games with Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Banik%2C+S">Sandeep Banik</a>, 
<a href="/search/eess?searchtype=author&query=Bopardikar%2C+S+D">Shaunak D. Bopardikar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 Pages, 2 figures. Under review at IEEE TAC
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14563" title="Abstract">arXiv:2310.14563</a> (replaced) [<a href="/pdf/2310.14563" title="Download PDF">pdf</a>, <a href="/format/2310.14563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NormDial: A Comparable Bilingual Synthetic Dialog Dataset for Modeling  Social Norm Adherence and Violation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+O">Oliver Li</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+M">Mallika Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Saakyan%2C+A">Arkadiy Saakyan</a>, 
<a href="/search/cs?searchtype=author&query=CH-Wang%2C+S">Sky CH-Wang</a>, 
<a href="/search/cs?searchtype=author&query=Muresan%2C+S">Smaranda Muresan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Conference, Short Paper; Data at <a href="https://github.com/Aochong-Li/NormDial">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14664" title="Abstract">arXiv:2310.14664</a> (replaced) [<a href="/pdf/2310.14664" title="Download PDF">pdf</a>, <a href="/format/2310.14664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Pruning via Moving-one-Sample-out
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+H">Haoru Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Sitong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+F">Fei Du</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yukang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhibin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiaojuan Qi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14939" title="Abstract">arXiv:2310.14939</a> (replaced) [<a href="/pdf/2310.14939" title="Download PDF">pdf</a>, <a href="/format/2310.14939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expressivity and Complexity of the Conjunctive Core of the SIGNAL  Process Query Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kampik%2C+T">Timotheus Kampik</a>, 
<a href="/search/cs?searchtype=author&query=Okulmus%2C+C">Cem Okulmus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15074" title="Abstract">arXiv:2310.15074</a> (replaced) [<a href="/pdf/2310.15074" title="Download PDF">pdf</a>, <a href="/format/2310.15074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MGAS: Multi-Granularity Architecture Search for Effective and Efficient  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoyun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Saxena%2C+D">Divya Saxena</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jiannong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuqing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+P">Penghui Ruan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15205" title="Abstract">arXiv:2310.15205</a> (replaced) [<a href="/pdf/2310.15205" title="Download PDF">pdf</a>, <a href="/format/2310.15205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple  Experts Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiushi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Z">Zefei Long</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xianyin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhongtian Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bingxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiarong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xiang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhongyu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 13 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15298" title="Abstract">arXiv:2310.15298</a> (replaced) [<a href="/pdf/2310.15298" title="Download PDF">pdf</a>, <a href="/format/2310.15298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TaskDiff: A Similarity Metric for Task-Oriented Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhaumik%2C+A">Ankita Bhaumik</a>, 
<a href="/search/cs?searchtype=author&query=Venkateswaran%2C+P">Praveen Venkateswaran</a>, 
<a href="/search/cs?searchtype=author&query=Rizk%2C+Y">Yara Rizk</a>, 
<a href="/search/cs?searchtype=author&query=Isahagian%2C+V">Vatche Isahagian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the main conference at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15340" title="Abstract">arXiv:2310.15340</a> (replaced) [<a href="/pdf/2310.15340" title="Download PDF">pdf</a>, <a href="/format/2310.15340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Calculational Design of [In]Correctness Transformational Program Logics  by Abstract Interpretation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cousot%2C+P">Patrick Cousot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 62 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15421" title="Abstract">arXiv:2310.15421</a> (replaced) [<a href="/pdf/2310.15421" title="Download PDF">pdf</a>, <a href="/format/2310.15421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FANToM: A Benchmark for Stress-testing Machine Theory of Mind in  Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyunwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sclar%2C+M">Melanie Sclar</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Bras%2C+R+L">Ronan Le Bras</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+G">Gunhee Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Sap%2C+M">Maarten Sap</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023. Code and dataset can be found here: <a href="https://hyunw.kim/fantom">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15495" title="Abstract">arXiv:2310.15495</a> (replaced) [<a href="/pdf/2310.15495" title="Download PDF">pdf</a>, <a href="/format/2310.15495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMG: Automated Efficient Approximate Multiplier Generator for FPGAs via  Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lingli Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2023 IEEE International Conference on Field-Programmable Technology (ICFPT)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15556" title="Abstract">arXiv:2310.15556</a> (replaced) [<a href="/pdf/2310.15556" title="Download PDF">pdf</a>, <a href="/format/2310.15556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for  Inference Cost Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+T">Tong Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bowen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yiming Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15578" title="Abstract">arXiv:2310.15578</a> (replaced) [<a href="/pdf/2310.15578" title="Download PDF">pdf</a>, <a href="/ps/2310.15578" title="Download PostScript">ps</a>, <a href="/format/2310.15578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VMAF Re-implementation on PyTorch: Some Experimental Results
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aistov%2C+K">Kirill Aistov</a>, 
<a href="/search/cs?searchtype=author&query=Koroteev%2C+M">Maxim Koroteev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15614" title="Abstract">arXiv:2310.15614</a> (replaced) [<a href="/pdf/2310.15614" title="Download PDF">pdf</a>, <a href="/format/2310.15614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Bayesian neural networks for regression: Tackling overfitting and  computational challenges in uncertainty quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dabiran%2C+N">Nastaran Dabiran</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+B">Brandon Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Sandhu%2C+R">Rimple Sandhu</a>, 
<a href="/search/cs?searchtype=author&query=Khalil%2C+M">Mohammad Khalil</a>, 
<a href="/search/cs?searchtype=author&query=Poirel%2C+D">Dominique Poirel</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Abhijit Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15669" title="Abstract">arXiv:2310.15669</a> (replaced) [<a href="/pdf/2310.15669" title="Download PDF">pdf</a>, <a href="/format/2310.15669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Methods for Multiscale Coarse Approximations of Diffusion Models  in Perforated Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Boutilier%2C+M">Miranda Boutilier</a>, 
<a href="/search/math?searchtype=author&query=Brenner%2C+K">Konstantin Brenner</a>, 
<a href="/search/math?searchtype=author&query=Dolean%2C+V">Victorita Dolean</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 14 figures, submitted to Journal of Computational Physics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15694" title="Abstract">arXiv:2310.15694</a> (replaced) [<a href="/pdf/2310.15694" title="Download PDF">pdf</a>, <a href="/format/2310.15694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COPF: Continual Learning Human Preference through Optimal Policy Fitting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Y">Yuanzhao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15846" title="Abstract">arXiv:2310.15846</a> (replaced) [<a href="/pdf/2310.15846" title="Download PDF">pdf</a>, <a href="/format/2310.15846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Spatial-Temporal Triangulation for Bearing-Only Cooperative  Motion Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C+L">C. L. Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+Y+Z">Y. Z. Mi</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H+Q">H. Q. Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H+B">H. B. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z+Y">Z. Y. Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S+Y">S. Y. Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15952" title="Abstract">arXiv:2310.15952</a> (replaced) [<a href="/pdf/2310.15952" title="Download PDF">pdf</a>, <a href="/format/2310.15952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Robustness and Reliability in Medical Image Classification  with Latent-Guided Diffusion and Nested-Ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xing Shen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hengguan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Nichyporuk%2C+B">Brennan Nichyporuk</a>, 
<a href="/search/cs?searchtype=author&query=Arbel%2C+T">Tal Arbel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15958" title="Abstract">arXiv:2310.15958</a> (replaced) [<a href="/pdf/2310.15958" title="Download PDF">pdf</a>, <a href="/format/2310.15958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparison of Unscented Kalman Filter Design for Agricultural Anaerobic  Digestion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hellmann%2C+S">Simon Hellmann</a>, 
<a href="/search/eess?searchtype=author&query=Wilms%2C+T">Terrance Wilms</a>, 
<a href="/search/eess?searchtype=author&query=Streif%2C+S">Stefan Streif</a>, 
<a href="/search/eess?searchtype=author&query=Weinrich%2C+S">S&#xf6;ren Weinrich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated model equations in appendix. Updated paper title
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15970" title="Abstract">arXiv:2310.15970</a> (replaced) [<a href="/pdf/2310.15970" title="Download PDF">pdf</a>, <a href="/format/2310.15970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accented Speech Recognition With Accent-specific Codebooks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prabhu%2C+D">Darshan Prabhu</a>, 
<a href="/search/cs?searchtype=author&query=Jyothi%2C+P">Preethi Jyothi</a>, 
<a href="/search/cs?searchtype=author&query=Ganapathy%2C+S">Sriram Ganapathy</a>, 
<a href="/search/cs?searchtype=author&query=Unni%2C+V">Vinit Unni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Main Conference (Long Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16042" title="Abstract">arXiv:2310.16042</a> (replaced) [<a href="/pdf/2310.16042" title="Download PDF">pdf</a>, <a href="/format/2310.16042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WebWISE: Web Interface Control and Sequential Exploration with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tao%2C+H">Heyi Tao</a>, 
<a href="/search/cs?searchtype=author&query=V%2C+S+T">Sethuraman T V</a>, 
<a href="/search/cs?searchtype=author&query=Shlapentokh-Rothman%2C+M">Michal Shlapentokh-Rothman</a>, 
<a href="/search/cs?searchtype=author&query=Hoiem%2C+D">Derek Hoiem</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16044" title="Abstract">arXiv:2310.16044</a> (replaced) [<a href="/pdf/2310.16044" title="Download PDF">pdf</a>, <a href="/format/2310.16044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stanford-ORB: A Real-World 3D Object Inverse Rendering Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuang%2C+Z">Zhengfei Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunzhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong-Xing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Agarwala%2C+S">Samir Agarwala</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shangzhe Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiajun Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Datasets and Benchmarks Track. The first two authors contributed equally to this work. Project page: <a href="https://stanfordorb.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item374">Cross-lists</a></li>
<li><a href="#item410">Replacements</a></li>
</ul>
<small>[ total of 670 entries:  <b>1-670</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
